{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    mutation_prompts1 = [\\n        \"Refine and simplify the selected algorithm to improve it.\",  # simplify mutation\\n    ]\\n    mutation_prompts2 = [\\n        \"Generate a new algorithm that is different from the algorithms you have tried before.\", #new random solution\\n    ]\\n    mutation_prompts3 = [\\n        \"Refine and simplify the selected solution to improve it.\",  # simplify mutation\\n        \"Generate a new algorithm that is different from the algorithms you have tried before.\", #new random solution\\n    ]\\n    LLaMEA_method1 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-1\", mutation_prompts=mutation_prompts1, n_parents=4, n_offspring=12, elitism=False) \\n    LLaMEA_method2 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-2\", mutation_prompts=mutation_prompts2, n_parents=4, n_offspring=12, elitism=False) \\n    LLaMEA_method3 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-3\", mutation_prompts=mutation_prompts3, n_parents=4, n_offspring=12, elitism=False) \\n    LLaMEA_method4 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-4\", mutation_prompts=mutation_prompts3, n_parents=1, n_offspring=1, elitism=True) \\n    LLaMEA_method5 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-5\", mutation_prompts=None, adaptive_mutation=True, n_parents=4, n_offspring=12, elitism=False) \\n    LLaMEA_method6 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-6\", mutation_prompts=None, adaptive_mutation=True, n_parents=1, n_offspring=1, elitism=True) \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    mutation_prompts1 = [\n",
    "        \"Refine and simplify the selected algorithm to improve it.\",  # simplify mutation\n",
    "    ]\n",
    "    mutation_prompts2 = [\n",
    "        \"Generate a new algorithm that is different from the algorithms you have tried before.\", #new random solution\n",
    "    ]\n",
    "    mutation_prompts3 = [\n",
    "        \"Refine and simplify the selected solution to improve it.\",  # simplify mutation\n",
    "        \"Generate a new algorithm that is different from the algorithms you have tried before.\", #new random solution\n",
    "    ]\n",
    "    LLaMEA_method1 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-1\", mutation_prompts=mutation_prompts1, n_parents=4, n_offspring=12, elitism=False) \n",
    "    LLaMEA_method2 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-2\", mutation_prompts=mutation_prompts2, n_parents=4, n_offspring=12, elitism=False) \n",
    "    LLaMEA_method3 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-3\", mutation_prompts=mutation_prompts3, n_parents=4, n_offspring=12, elitism=False) \n",
    "    LLaMEA_method4 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-4\", mutation_prompts=mutation_prompts3, n_parents=1, n_offspring=1, elitism=True) \n",
    "    LLaMEA_method5 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-5\", mutation_prompts=None, adaptive_mutation=True, n_parents=4, n_offspring=12, elitism=False) \n",
    "    LLaMEA_method6 = LLaMEA(llm, budget=budget, name=f\"LLaMEA-6\", mutation_prompts=None, adaptive_mutation=True, n_parents=1, n_offspring=1, elitism=True) \n",
    "# \"\"\"\n",
    "\n",
    "# | **Metric**                              | **What it Measures**                                    | **Reference**                          |\n",
    "# | --------------------------------------- | ------------------------------------------------------- | -------------------------------------- |\n",
    "# | **Nearest-Neighbor Distance** (avg NND) | Novelty of new samples (exploration)                    | Distance-based exploration     https://arxiv.org/html/2410.14573#:~:text=removing%20points%20from%20the%20Pareto,various%20optimization%20scenarios%20and%20objectives        |\n",
    "# | **Coverage / Dispersion**               | Fraction of space covered (exploration)                 | Space-filling designs in DOE   https://www.nature.com/articles/s41598-024-68436-1#:~:text=To%20perform%20population%20diversity%20analysis%2C,used%20for%20population%20diversity%20estimation        |\n",
    "# | **Spatial Entropy**                     | Spread/uncertainty of sample distribution               | Entropy for batch diversity    https://www.nature.com/articles/s41598-024-68436-1#:~:text=To%20perform%20population%20diversity%20analysis%2C,used%20for%20population%20diversity%20estimation        |\n",
    "# | **Proximity to Best**                   | Focus on known optima (exploitation)                    | Exploitation = staying in neighborhood https://romisatriawahono.net/lecture/rm/survey/softcomputing/Crepinsek%20-%20Exploration%20and%20Exploitation%20in%20Evolutionary%20Algorithms%20-%202013.pdf#:~:text=space,regions%20of%20a%20search%20space  |\n",
    "# | **Exploration–Exploitation %**          | Trade-off via diversity (time-varying)                  | Pop. diversity ratio                 https://www.nature.com/articles/s41598-024-68436-1#:~:text=%24%24exploration%5C%3B%5Cleft%28iteration%5Cright%29%3D%5Cleft%28%5Cfrac |\n",
    "# | **Success/Improvement Rate**            | Frequency of improving moves (convergence/exploitation) | 1/5th success rule concept           https://www.nature.com/articles/s41598-024-68436-1#:~:text=%24%24exploration%5C%3B%5Cleft%28iteration%5Cright%29%3D%5Cleft%28%5Cfrac  |\n",
    "# | **Area Under Conv. Curve**              | Overall speed of convergence                            | – (common in benchmarking)             |\n",
    "# | **Average Convergence Rate** (ACR)      | Geometric mean error reduction (convergence speed)      | Convergence rate analysis             https://www.nature.com/articles/s41598-024-68436-1#:~:text=%24%24exploration%5C%3B%5Cleft%28iteration%5Cright%29%3D%5Cleft%28%5Cfrac |\n",
    "# | **No-Improvement Streak**               | Longest stagnation period                               | Used as stopping criterion            https://www.sciencedirect.com/science/article/pii/S037722172200159X#:~:text=The%20second%20parameter%20to%20be,In%20this%20case%2C%20we |\n",
    "# | **Last Improvement Fraction**           | Portion of run spent stagnating                         | – (derived from convergence curve)     |\n",
    "# | **Pop. Diversity (to centroid)**        | Dispersion of points in search space                    | “Distance to average” measure         https://www.sciencedirect.com/science/article/pii/S037722172200159X#:~:text=The%20second%20parameter%20to%20be,In%20this%20case%2C%20we |\n",
    "# | **Mean Pairwise Distance**              | Overall pairwise diversity                              | Diversity measures in EAs             https://romisatriawahono.net/lecture/rm/survey/softcomputing/Crepinsek%20-%20Exploration%20and%20Exploitation%20in%20Evolutionary%20Algorithms%20-%202013.pdf#:~:text=match%20at%20L885%20%E2%80%94Distance,used%20type%20of%20diversity%20measure |\n",
    "# | **Coverage Volume**                     | Volume spanned by samples (diversity)                   | DPP-based diversity                   https://romisatriawahono.net/lecture/rm/survey/softcomputing/Crepinsek%20-%20Exploration%20and%20Exploitation%20in%20Evolutionary%20Algorithms%20-%202013.pdf#:~:text=match%20at%20L885%20%E2%80%94Distance,used%20type%20of%20diversity%20measure |\n",
    "# | **Average Step Length**                 | Typical move size (exploration intensity)               | Step size reduction in search         https://stats.stackexchange.com/questions/304813/stopping-criterion-for-nelder-mead#:~:text=Stopping%20criterion%20for%20Nelder%20Mead,than%20some%20tolerance%20TOL |\n",
    "# | **Total Path Length**                   | Total distance traveled                                 | – (trajectory length analysis)         |\n",
    "# | **Path Efficiency**                     | Directness of search path                               | –                                      |\n",
    "# | **Step Length Trend**                   | Change of step size over time (convergence indicator)   | – (implied by 1/5th rule)              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LLaMEA-1', 'LLaMEA-2', 'LLaMEA-3', 'LLaMEA-4', 'LLaMEA-5', 'LLaMEA-6']\n",
      "['BBOB']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "algorithms:   0%|          | 1/3000 [00:18<15:48:04, 18.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 83\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, algo_row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(),  \u001b[38;5;66;03m# <‑‑ your dataframe of algorithms\u001b[39;00m\n\u001b[1;32m     80\u001b[0m                         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df),\n\u001b[1;32m     81\u001b[0m                         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithms\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m         df_runs \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgo_row\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# your function above\u001b[39;00m\n\u001b[1;32m     84\u001b[0m         frames\u001b[38;5;241m.\u001b[39mappend(df_runs)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mprocess_run\u001b[0;34m(row, func_ids, runs_per_func, root)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# load run data for one folder\u001b[39;00m\n\u001b[1;32m     44\u001b[0m df \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mload(monotonic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, include_meta_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_behavior_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     48\u001b[0m f_fid \u001b[38;5;241m=\u001b[39m avg_auc_for_fid(aucs, fid, runs_per_func\u001b[38;5;241m=\u001b[39mruns_per_func, func_ids\u001b[38;5;241m=\u001b[39mfunc_ids)\n",
      "File \u001b[0;32m~/repos/BLADE/iohblade/behaviour_metrics.py:307\u001b[0m, in \u001b[0;36mcompute_behavior_metrics\u001b[0;34m(df, bounds, radius, disp_samples)\u001b[0m\n\u001b[1;32m    304\u001b[0m     radius \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m (bounds[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m bounds[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# time‑series based exploration/exploitation percentages (averaged)\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m avg_exploration_pct, avg_exploitation_pct \u001b[38;5;241m=\u001b[39m \u001b[43mavg_exploration_exploitation_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# improvement stats\u001b[39;00m\n\u001b[1;32m    310\u001b[0m avg_imp, success_rate \u001b[38;5;241m=\u001b[39m improvement_statistics(df)\n",
      "File \u001b[0;32m~/repos/BLADE/iohblade/behaviour_metrics.py:189\u001b[0m, in \u001b[0;36mavg_exploration_exploitation_chunked\u001b[0;34m(df, chunk_size, bounds, rng)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     chunk_div \u001b[38;5;241m=\u001b[39m \u001b[43mpdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    190\u001b[0m     chunk_diversities\u001b[38;5;241m.\u001b[39mappend(chunk_div)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk_diversities:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/iohblade-xXF9vABH-py3.10/lib/python3.10/site-packages/scipy/spatial/distance.py:2322\u001b[0m, in \u001b[0;36mpdist\u001b[0;34m(X, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2321\u001b[0m     pdist_fn \u001b[38;5;241m=\u001b[39m metric_info\u001b[38;5;241m.\u001b[39mpdist_func\n\u001b[0;32m-> 2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2324\u001b[0m     metric_info \u001b[38;5;241m=\u001b[39m _TEST_METRICS\u001b[38;5;241m.\u001b[39mget(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import iohinspector\n",
    "import polars as pl\n",
    "from iohblade.behaviour_metrics import compute_behavior_metrics\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from iohblade.loggers import ExperimentLogger\n",
    "from iohblade import plot_convergence, plot_experiment_CEG, plot_boxplot_fitness_hue, plot_boxplot_fitness, fitness_table\n",
    "import os\n",
    "\n",
    "logger = ExperimentLogger('/data/neocortex/BBOB', True)\n",
    "\n",
    "data = logger.get_data()\n",
    "\n",
    "\n",
    "tqdm.pandas(desc=\"Processing runs\")\n",
    "\n",
    "func_ids        = [1, 3, 6, 8, 10, 13, 15, 17, 21, 23]\n",
    "runs_per_func   = 5          # we know each function has 5 runs\n",
    "\n",
    "def avg_auc_for_fid(aucs: list[float], fid: int, runs_per_func=5, func_ids=[1, 3, 6, 8, 10, 13, 15, 17, 21, 23]) -> float:\n",
    "    \"\"\"\n",
    "    aucs = 50-long list in metadata.\n",
    "    Take the five entries belonging to the requested fid and average them.\n",
    "    \"\"\"\n",
    "    if len(aucs) < 50:\n",
    "        return 0\n",
    "    block = func_ids.index(fid) * runs_per_func\n",
    "    slice_ = aucs[block : block + runs_per_func]\n",
    "    return float(np.mean(slice_))\n",
    "\n",
    "def process_run(row, func_ids= [1, 3, 6, 8, 10, 13, 15, 17, 21, 23], runs_per_func=5, root=\"/data/neocortex/BBOB/ioh/\"):\n",
    "    rows = []\n",
    "    # each algorithm has 50 different runs = 50 different directories we need to process\n",
    "    algid = row['id']\n",
    "    fitness = row['fitness']\n",
    "    aucs = row['aucs_list']\n",
    "\n",
    "    counter = 0\n",
    "    for fid in func_ids:\n",
    "        for run in range(runs_per_func):\n",
    "            \n",
    "            path = f\"{root}{algid}\"\n",
    "            if counter > 0:\n",
    "                path = f\"{root}{algid}-{counter}\"\n",
    "            # each algorithm has 50 different runs = 50 different directories we need to process\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "            manager = iohinspector.DataManager()\n",
    "            manager.add_folder(path)\n",
    "            # load run data for one folder\n",
    "            df = manager.load(monotonic=False, include_meta_data=True)\n",
    "            metrics = compute_behavior_metrics(df)\n",
    "            counter += 1\n",
    "\n",
    "            f_fid = avg_auc_for_fid(aucs, fid, runs_per_func=runs_per_func, func_ids=func_ids)\n",
    "            metrics.update(\n",
    "                {\n",
    "                    \"id\":           algid,\n",
    "                    \"fid\":          fid,\n",
    "                    \"fitness_fid\":  f_fid,\n",
    "                }\n",
    "            )\n",
    "            rows.append(metrics)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n",
    "methods, problems = logger.get_methods_problems()\n",
    "print(methods)\n",
    "print(problems)\n",
    "\n",
    "df = logger.get_problem_data(problem_name='BBOB')\n",
    "\n",
    "def get_aucs(d):\n",
    "    \"\"\"Return the list under key 'aucs' (or [] if anything is wrong).\"\"\"\n",
    "    try:\n",
    "        return d.get(\"aucs\", [])\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "df[\"aucs_list\"] = df[\"metadata\"].apply(get_aucs)\n",
    "\n",
    "\n",
    "frames = []\n",
    "\n",
    "for _, algo_row in tqdm(df.iterrows(),  # <‑‑ your dataframe of algorithms\n",
    "                        total=len(df),\n",
    "                        desc=\"algorithms\"):\n",
    "    try:\n",
    "        df_runs = process_run(algo_row)      # your function above\n",
    "        frames.append(df_runs)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠︎  {algo_row['id']} skipped ({e})\")\n",
    "\n",
    "all_runs = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "print(f\"✅  built dataframe with {len(all_runs)} runs\")\n",
    "\n",
    "print(all_runs.columns)\n",
    "all_runs.to_pickle(\"./BBOB0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fitness</th>\n",
       "      <th>generation</th>\n",
       "      <th>operator</th>\n",
       "      <th>seed</th>\n",
       "      <th>_id</th>\n",
       "      <th>avg_nearest_neighbor_distance</th>\n",
       "      <th>dispersion</th>\n",
       "      <th>avg_exploration_pct</th>\n",
       "      <th>avg_distance_to_best</th>\n",
       "      <th>intensification_ratio</th>\n",
       "      <th>avg_exploitation_pct</th>\n",
       "      <th>average_convergence_rate</th>\n",
       "      <th>avg_improvement</th>\n",
       "      <th>success_rate</th>\n",
       "      <th>longest_no_improvement_streak</th>\n",
       "      <th>last_improvement_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>2689.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-inf</td>\n",
       "      <td>19.380000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>0.327266</td>\n",
       "      <td>7.721254</td>\n",
       "      <td>22.844320</td>\n",
       "      <td>1.674394</td>\n",
       "      <td>0.668318</td>\n",
       "      <td>77.155680</td>\n",
       "      <td>0.992196</td>\n",
       "      <td>2.028162</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>5378.375976</td>\n",
       "      <td>0.467204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>27.119749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.414449</td>\n",
       "      <td>28.870882</td>\n",
       "      <td>0.478168</td>\n",
       "      <td>2.640381</td>\n",
       "      <td>25.009378</td>\n",
       "      <td>2.173379</td>\n",
       "      <td>0.327501</td>\n",
       "      <td>25.009378</td>\n",
       "      <td>0.042587</td>\n",
       "      <td>2.663068</td>\n",
       "      <td>0.040383</td>\n",
       "      <td>2697.266793</td>\n",
       "      <td>0.320008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>1.990900</td>\n",
       "      <td>0.073442</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.135774</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>0.062002</td>\n",
       "      <td>5.769276</td>\n",
       "      <td>2.914043</td>\n",
       "      <td>0.152246</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>65.772685</td>\n",
       "      <td>0.995937</td>\n",
       "      <td>0.694889</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>3285.000000</td>\n",
       "      <td>0.160116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.216620</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>0.162717</td>\n",
       "      <td>8.177932</td>\n",
       "      <td>13.198842</td>\n",
       "      <td>0.825181</td>\n",
       "      <td>0.777700</td>\n",
       "      <td>86.801158</td>\n",
       "      <td>0.995972</td>\n",
       "      <td>1.272730</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>5478.000000</td>\n",
       "      <td>0.485249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.278098</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>74.250000</td>\n",
       "      <td>0.385661</td>\n",
       "      <td>9.560339</td>\n",
       "      <td>34.227315</td>\n",
       "      <td>2.190751</td>\n",
       "      <td>0.956700</td>\n",
       "      <td>97.085957</td>\n",
       "      <td>0.996020</td>\n",
       "      <td>2.296845</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>7624.000000</td>\n",
       "      <td>0.755876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.538004</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>11.074070</td>\n",
       "      <td>16.187526</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>15.266032</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>99.926558</td>\n",
       "      <td>0.996412</td>\n",
       "      <td>42.248496</td>\n",
       "      <td>0.975898</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>0.999000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fitness   generation  operator         seed          _id  \\\n",
       "count  3000.000000  3000.000000       0.0  3000.000000  3000.000000   \n",
       "mean          -inf    19.380000       NaN     2.000000    49.500000   \n",
       "std            NaN    27.119749       NaN     1.414449    28.870882   \n",
       "min           -inf     0.000000       NaN     0.000000     0.000000   \n",
       "25%       0.135774     3.000000       NaN     1.000000    24.750000   \n",
       "50%       0.216620     6.000000       NaN     2.000000    49.500000   \n",
       "75%       0.278098    24.250000       NaN     3.000000    74.250000   \n",
       "max       0.538004    99.000000       NaN     4.000000    99.000000   \n",
       "\n",
       "       avg_nearest_neighbor_distance   dispersion  avg_exploration_pct  \\\n",
       "count                    2689.000000  2689.000000          2689.000000   \n",
       "mean                        0.327266     7.721254            22.844320   \n",
       "std                         0.478168     2.640381            25.009378   \n",
       "min                         0.000832     1.990900             0.073442   \n",
       "25%                         0.062002     5.769276             2.914043   \n",
       "50%                         0.162717     8.177932            13.198842   \n",
       "75%                         0.385661     9.560339            34.227315   \n",
       "max                        11.074070    16.187526           100.000000   \n",
       "\n",
       "       avg_distance_to_best  intensification_ratio  avg_exploitation_pct  \\\n",
       "count           2689.000000            2689.000000           2689.000000   \n",
       "mean               1.674394               0.668318             77.155680   \n",
       "std                2.173379               0.327501             25.009378   \n",
       "min                0.000202               0.000100              0.000000   \n",
       "25%                0.152246               0.451500             65.772685   \n",
       "50%                0.825181               0.777700             86.801158   \n",
       "75%                2.190751               0.956700             97.085957   \n",
       "max               15.266032               0.999300             99.926558   \n",
       "\n",
       "       average_convergence_rate  avg_improvement  success_rate  \\\n",
       "count               2689.000000      2689.000000   2689.000000   \n",
       "mean                   0.992196         2.028162      0.013402   \n",
       "std                    0.042587         2.663068      0.040383   \n",
       "min                    0.003621         0.005470      0.000100   \n",
       "25%                    0.995937         0.694889      0.003104   \n",
       "50%                    0.995972         1.272730      0.005710   \n",
       "75%                    0.996020         2.296845      0.008201   \n",
       "max                    0.996412        42.248496      0.975898   \n",
       "\n",
       "       longest_no_improvement_streak  last_improvement_fraction  \n",
       "count                    2689.000000                2689.000000  \n",
       "mean                     5378.375976                   0.467204  \n",
       "std                      2697.266793                   0.320008  \n",
       "min                         2.000000                   0.000000  \n",
       "25%                      3285.000000                   0.160116  \n",
       "50%                      5478.000000                   0.485249  \n",
       "75%                      7624.000000                   0.755876  \n",
       "max                      9989.000000                   0.999000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.to_pickle(\"./BBOB0.pkl\")\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(\"./BBOB0.pkl\")  \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import seaborn as sns\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0  House‑keeping\n",
    "# ------------------------------------------------------------------\n",
    "# Make a copy we can mangle\n",
    "data = df.copy()\n",
    "\n",
    "# Replace ±∞ with NaN, then drop rows that have no finite fitness at all\n",
    "data.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "data = data.dropna(subset=[\"fitness\"])\n",
    "\n",
    "# Normalise fitness to [0, 1] per algorithm so colour scales are comparable\n",
    "data[\"fitness_norm\"] = (\n",
    "    data.groupby(\"method_name\")[\"fitness\"]\n",
    "        .transform(lambda s: (s - s.min()) / (s.max() - s.min()))\n",
    ")\n",
    "\n",
    "# Some columns you’ll want handy later\n",
    "behaviour_feats = [\n",
    "    \"avg_nearest_neighbor_distance\", \"dispersion\", \"avg_exploration_pct\",\n",
    "    \"avg_distance_to_best\", \"intensification_ratio\", \"avg_exploitation_pct\",\n",
    "    \"average_convergence_rate\", \"avg_improvement\", \"success_rate\",\n",
    "    \"longest_no_improvement_streak\", \"last_improvement_fraction\",\n",
    "]\n",
    "nice_names = {\n",
    "    \"avg_nearest_neighbor_distance\":  \"NN-dist\",\n",
    "    \"dispersion\":                     \"Disp\",\n",
    "    \"avg_exploration_pct\":            \"Expl %\",\n",
    "    \"avg_distance_to_best\":           \"Dist→best\",\n",
    "    \"intensification_ratio\":          \"Inten-ratio\",\n",
    "    \"avg_exploitation_pct\":           \"Explt %\",\n",
    "    \"average_convergence_rate\":       \"Conv-rate\",\n",
    "    \"avg_improvement\":                \"Δ fitness\",\n",
    "    \"success_rate\":                   \"Success %\",\n",
    "    \"longest_no_improvement_streak\":  \"No-imp streak\",\n",
    "    \"last_improvement_fraction\":      \"Last-imp frac\",\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1  Behaviour features over evaluations\n",
    "#     (_id is assumed to be monotonically increasing per seed)\n",
    "# ------------------------------------------------------------------\n",
    "for algo, g in data.groupby(\"method_name\", sort=False):\n",
    "    continue\n",
    "\n",
    "    # Pre‑sort once for nicer lines\n",
    "    g = g.sort_values([\"seed\", \"_id\"])\n",
    "\n",
    "    for feat in behaviour_feats:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        # All individual runs in the background\n",
    "        for _, seed_group in g.groupby(\"seed\"):\n",
    "            plt.plot(\n",
    "                seed_group[\"_id\"],\n",
    "                seed_group[feat],\n",
    "                linewidth=0.6,\n",
    "                alpha=0.3,\n",
    "            )\n",
    "\n",
    "        # Mean trend over runs on top\n",
    "        mean_curve = (\n",
    "            g.groupby(\"_id\")[feat]\n",
    "              .mean()\n",
    "              .reset_index()\n",
    "              .sort_values(\"_id\")\n",
    "        )\n",
    "        plt.plot(\n",
    "            mean_curve[\"_id\"],\n",
    "            mean_curve[feat],\n",
    "            linewidth=2,\n",
    "            label=\"per‑eval mean\",\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{algo}\")\n",
    "        plt.xlabel(\"Evaluations\")\n",
    "        plt.ylabel(feat)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1  Correlation heat‑map \n",
    "# ------------------------------------------------------------------\n",
    "#for algo, g in data.groupby(\"method_name\", sort=False):\n",
    "\n",
    "corr = data[behaviour_feats + [\"fitness_norm\"]].corr()\n",
    "\n",
    "plot_df = corr.rename(columns=nice_names)\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(\n",
    "    plot_df,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws=dict(label=\"Pearson r\"),\n",
    ")\n",
    "plt.title(f\"Behaviour metrics\\ncorrelation matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "# Parallel coordinates for all!\n",
    "pc = data[behaviour_feats + [\"fitness_norm\"]].copy()\n",
    "\n",
    "# 2‑a.  Min‑max scale features (parallel plots hate disparate ranges)\n",
    "pc[behaviour_feats] = (\n",
    "    pc[behaviour_feats] - pc[behaviour_feats].min()\n",
    ") / (pc[behaviour_feats].max() - pc[behaviour_feats].min())\n",
    "\n",
    "quartile_labels = [\"Q1 (low)\", \"Q2\", \"Q3\", \"Q4 (high)\"]\n",
    "quart_cat = pd.CategoricalDtype(categories=quartile_labels, ordered=True)\n",
    "\n",
    "# 2‑b.  Quartile‑bin fitness so we can colour by performance\n",
    "pc[\"fitness_group\"] = pd.qcut(\n",
    "    pc[\"fitness_norm\"], 4, labels=quartile_labels, duplicates=\"drop\"\n",
    ")\n",
    "pc[\"fitness_group\"] = pc[\"fitness_group\"].astype(quart_cat)\n",
    "\n",
    "# ── optional: subsample to keep the plot readable ──────────────\n",
    "#sample = pc.sample(n=min(len(pc), 600), random_state=42)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "plot_df = pc.rename(columns=nice_names)\n",
    "parallel_coordinates(\n",
    "    plot_df, \"fitness_group\", alpha=0.3, linewidth=1.0,\n",
    "    colormap=\"seismic\", ax=ax,\n",
    ")\n",
    "\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, quartile_labels, title=\"fitness quartile\",\n",
    "    loc=\"upper left\", frameon=True)\n",
    "plt.title(f\"Behaviour profile\")\n",
    "plt.ylabel(\"scaled feature value\")\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2  Parallel‑coordinate plot per algorithm\n",
    "# ------------------------------------------------------------------\n",
    "for algo, g in data.groupby(\"method_name\", sort=False):\n",
    "\n",
    "    pc = g[behaviour_feats + [\"fitness_norm\"]].copy()\n",
    "\n",
    "    # 2‑a.  Min‑max scale features (parallel plots hate disparate ranges)\n",
    "    pc[behaviour_feats] = (\n",
    "        pc[behaviour_feats] - pc[behaviour_feats].min()\n",
    "    ) / (pc[behaviour_feats].max() - pc[behaviour_feats].min())\n",
    "\n",
    "    # 2‑b.  Quartile‑bin fitness so we can colour by performance\n",
    "    pc[\"fitness_group\"] = pd.qcut(\n",
    "        pc[\"fitness_norm\"], 4, labels=None, duplicates=\"drop\" #[\"Q1 (low)\", \"Q2\", \"Q3\", \"Q4 (high)\"]\n",
    "    )\n",
    "\n",
    "    # ── optional: subsample to keep the plot readable ──────────────\n",
    "    #sample = pc.sample(n=min(len(pc), 600), random_state=42)\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    parallel_coordinates(\n",
    "        pc, \"fitness_group\", alpha=0.4, linewidth=1.0,\n",
    "        colormap=\"seismic\",\n",
    "    )\n",
    "    plt.title(f\"{algo} — parallel‑coordinate behaviour profile\")\n",
    "    plt.ylabel(\"scaled feature value\")\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0  Aggregate the 50 raw auc‑columns into 10 function‑level scores\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def get_aucs(d):\n",
    "    \"\"\"Return the list under key 'aucs' (or [] if anything is wrong).\"\"\"\n",
    "    try:\n",
    "        return d.get(\"aucs\", [])\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "data[\"aucs_list\"] = data[\"metadata\"].apply(get_aucs)\n",
    "max_len = data[\"aucs_list\"].str.len().max()\n",
    "\n",
    "# One column per AUC position\n",
    "for i in range(max_len):\n",
    "    data[f\"auc_{i}\"] = data[\"aucs_list\"].apply(\n",
    "        lambda xs, i=i: xs[i] if len(xs) > i else np.nan\n",
    "    )\n",
    "\n",
    "func_ids        = [1, 3, 6, 8, 10, 13, 15, 17, 21, 23]\n",
    "runs_per_func   = 5          # we know each function has 5 runs\n",
    "quart_labels    = [\"Q1 (low)\", \"Q2\", \"Q3\", \"Q4 (high)\"]\n",
    "quart_cat       = pd.CategoricalDtype(quart_labels, ordered=True)\n",
    "\n",
    "for j, fid in enumerate(func_ids):\n",
    "    start, end      = j * runs_per_func, (j + 1) * runs_per_func\n",
    "    run_cols        = [f\"auc_{k}\" for k in range(start, end)]\n",
    "    data[f\"auc_f{fid}\"] = data[run_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1  Build a helper with min‑max‑scaled behaviour features\n",
    "# ------------------------------------------------------------------\n",
    "behaviour_feats = [\n",
    "    \"avg_nearest_neighbor_distance\", \"dispersion\", \"avg_exploration_pct\",\n",
    "    \"avg_distance_to_best\", \"intensification_ratio\", \"avg_exploitation_pct\",\n",
    "    \"average_convergence_rate\", \"avg_improvement\", \"success_rate\",\n",
    "    \"longest_no_improvement_streak\", \"last_improvement_fraction\",\n",
    "]\n",
    "\n",
    "nice_names = {\n",
    "    \"avg_nearest_neighbor_distance\":  \"NN-dist\",\n",
    "    \"dispersion\":                     \"Disp\",\n",
    "    \"avg_exploration_pct\":            \"Expl %\",\n",
    "    \"avg_distance_to_best\":           \"Dist→best\",\n",
    "    \"intensification_ratio\":          \"Inten-ratio\",\n",
    "    \"avg_exploitation_pct\":           \"Explt %\",\n",
    "    \"average_convergence_rate\":       \"Conv-rate\",\n",
    "    \"avg_improvement\":                \"Δ fitness\",\n",
    "    \"success_rate\":                   \"Success %\",\n",
    "    \"longest_no_improvement_streak\":  \"No-imp streak\",\n",
    "    \"last_improvement_fraction\":      \"Last-imp frac\",\n",
    "}\n",
    "\n",
    "behav_scaled = (\n",
    "    data[behaviour_feats] - data[behaviour_feats].min()\n",
    ") / (data[behaviour_feats].max() - data[behaviour_feats].min())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2  Parallel‑coordinate plot for each function‑level AUC\n",
    "# ------------------------------------------------------------------\n",
    "from pandas.plotting import parallel_coordinates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for fid in func_ids:\n",
    "    col = f\"auc_f{fid}\"\n",
    "    df_f = data.dropna(subset=[col])\n",
    "\n",
    "    if df_f.empty:       # nothing to draw for this function\n",
    "        continue\n",
    "\n",
    "    # quartile‑bin the aggregated fitness score\n",
    "    q = pd.qcut(df_f[col], 4, labels=quart_labels).astype(quart_cat)\n",
    "\n",
    "    # frame to plot: scaled behaviour + quartile label\n",
    "    plot_df = behav_scaled.loc[df_f.index].copy()\n",
    "    plot_df[\"fitness_quartile\"] = q\n",
    "\n",
    "    # (optional) thin out lines for readability\n",
    "    plot_df = plot_df.sample(n=min(len(plot_df), 2000), random_state=42)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 12))\n",
    "    \n",
    "    plot_df = plot_df.rename(columns=nice_names)\n",
    "    parallel_coordinates(\n",
    "        plot_df, \"fitness_quartile\", ax=ax,\n",
    "        alpha=0.35, linewidth=1.0, colormap=\"seismic\",\n",
    "    )\n",
    "\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, quart_labels, title=\"fitness quartile\",\n",
    "              loc=\"upper left\", frameon=True)\n",
    "\n",
    "    ax.set_title(f\"Behaviour profile for $f_{{{fid}}}$\")\n",
    "    ax.set_ylabel(\"scaled feature value\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K     = 100            # ← change this to whatever \"top‑k\" you want\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1  Gather top‑k rows per function (by aggregated auc_f*)\n",
    "# ---------------------------------------------------------------\n",
    "pieces = []\n",
    "for fid in func_ids:\n",
    "    col = f\"auc_f{fid}\"\n",
    "    if col not in data.columns:\n",
    "        continue                        # skip if no such column\n",
    "\n",
    "    sub = data.dropna(subset=[col])\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    # take the k rows with highest score for this function\n",
    "    topk = sub.nlargest(TOP_K, columns=col)\n",
    "\n",
    "    # scaled behaviour + fid label\n",
    "    frame = behav_scaled.loc[topk.index].copy()\n",
    "    frame[\"fid\"] = f\"fid {fid}\"\n",
    "    pieces.append(frame)\n",
    "\n",
    "# Concatenate every function’s Q4 slice\n",
    "plot_df = pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "# Rename cols for prettier x‑ticks\n",
    "plot_df = plot_df.rename(columns=nice_names)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3  Parallel‑coordinate plot\n",
    "# ------------------------------------------------------------------\n",
    "from pandas.plotting import parallel_coordinates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build a colour list (one per fid) – tab10 has 10 distinct hues\n",
    "cmap   = plt.get_cmap(\"tab10\")\n",
    "colors = [cmap(i) for i in range(len(func_ids))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "parallel_coordinates(\n",
    "    plot_df, \"fid\",\n",
    "    ax=ax,\n",
    "    linewidth=1.1,\n",
    "    alpha=0.8,\n",
    "    color=colors,\n",
    ")\n",
    "\n",
    "ax.set_title(\"Top-quartile behaviour profiles per function id\")\n",
    "ax.set_ylabel(\"scaled feature value\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "ax.legend(title=\"function id\", frameon=True, loc=\"upper left\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the converenge curves per problem and then plot the Code Evolution Graphs to inspect different runs of the otpimzers.\n",
    "\n",
    "plot_convergence(logger, metric=\"AOCC\", save=False, budget=100)\n",
    "plot_experiment_CEG(logger, save=False, budget=100, max_seeds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(logger, metric=\"AOCC\", save=True, budget=100)\n",
    "plot_experiment_CEG(logger, save=True, budget=100, max_seeds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_boxplot_fitness(logger)\n",
    "plot_boxplot_fitness_hue(logger)\n",
    "\n",
    "table_df = fitness_table(logger, alpha=0.05)\n",
    "table_df\n",
    "# Convert to LaTeX; escape=False so we can keep the \\textbf{} markup\n",
    "#latex_str = table_df.to_latex(escape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "df = manager.load(monotonic=True, include_meta_data=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,10))\n",
    "_ = iohinspector.plot.plot_ecdf(df.filter(pl.col(\"dimension\") == 5), y_max=100, y_min=1e-8, ax=ax, scale_xlog=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eaf = iohinspector.metrics.transform_fval(df, 1e-8, 1e2)\n",
    "iohinspector.metrics.get_aocc(df_eaf.filter(pl.col(\"dimension\") == 5), 10000, group_cols=['algorithm_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blade-xXF9vABH-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

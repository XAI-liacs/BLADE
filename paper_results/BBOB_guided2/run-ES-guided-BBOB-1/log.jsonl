{"id": "b86b676b-91f8-4b23-89ea-ffbb21612295", "fitness": 0.42875951190979755, "name": "ACLS", "description": "Adaptive Covariance-Step with L\u00e9vy Escapes (ACLS) \u2014 an evolution-style, gradient-free search that adapts a multivariate covariance, controls step-size by success rate, and uses rare heavy-tailed jumps to escape local minima.", "code": "import numpy as np\n\nclass ACLS:\n    \"\"\"\n    Adaptive Covariance-Step with L\u00e9vy Escapes (ACLS).\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional tuning parameters can be passed as keyword arguments.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None, init_fraction=0.12,\n                 jump_prob=0.06, target_success=0.2, cov_update_rate=None,\n                 sigma_initial=None, stagnation_restart=500):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # meta-parameters (tunable)\n        self.init_fraction = float(init_fraction)   # fraction of budget for initial sampling\n        self.jump_prob = float(jump_prob)           # probability of heavy-tailed jump\n        self.target_success = float(target_success) # target success rate for sigma control\n        # covariance learning rate (smaller for larger dimensions)\n        if cov_update_rate is None:\n            self.c_cov = 0.2 / max(1.0, (self.dim + 2.0))\n        else:\n            self.c_cov = float(cov_update_rate)\n\n        # initial step-size (sigma) can be provided; otherwise f(range)/6 default will be used later\n        self.sigma_initial = sigma_initial\n\n        # stagnation restart threshold (number of evals without improvement)\n        self.stagnation_restart = int(stagnation_restart)\n\n    def _levy_vector(self, size):\n        # heavy-tailed direction: use normalized Cauchy vector (very simple Levy-like)\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        # guard against inf/nan large values: clip and replace\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        norm = np.linalg.norm(z)\n        if norm == 0:\n            return self.rng.standard_normal(size=size)\n        return z / norm\n\n    def __call__(self, func):\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # ensure bounds are vector of length dim\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # determine initial sample count (respect budget)\n        n_init = max(1, int(min(max(10, 2 * dim), max(1, int(budget * self.init_fraction)))))\n        n_init = min(n_init, budget)  # can't exceed budget\n\n        # initial sampling (uniform)\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n\n        # setup covariance and sigma\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma_initial is None:\n            sigma = max(1e-8, 0.25 * range_scale)  # reasonably large initial step\n        else:\n            sigma = float(self.sigma_initial)\n\n        # initialize covariance matrix as diagonal with moderate variance\n        C = np.eye(dim) * ( (range_scale / 4.0) ** 2 + 1e-9 )\n\n        # success tracking for step-size control (exponential moving average)\n        p_succ = 0.0\n        ema_alpha = 0.15  # how fast the EMA updates\n\n        # stagnation detection\n        since_improvement = 0\n\n        # main loop: sample candidates in small batches until budget used\n        while evals < budget:\n            remaining = budget - evals\n            # choose a batch size to amortize cholesky but keep iterations responsive\n            batch = min( max(1, int(8 + dim // 2)), remaining )\n\n            # precompute decomposition for covariance sampling\n            # regularize C to avoid numerical issues\n            reg = 1e-8 * np.eye(dim)\n            try:\n                L = np.linalg.cholesky(C + reg)\n            except np.linalg.LinAlgError:\n                # fallback: add more regularization\n                L = np.linalg.cholesky(C + (1e-6 * np.eye(dim)))\n\n            for _ in range(batch):\n                # choose jump or multivariate normal perturbation\n                if rng.random() < self.jump_prob:\n                    # heavy-tailed jump: normalized cauchy times larger factor\n                    z = self._levy_vector(dim)\n                    dx = L.dot(z) * 3.0  # amplify heavy-tailed jump\n                else:\n                    z = rng.normal(size=dim)\n                    # direction shaped by covariance\n                    dx = L.dot(z)\n\n                x_trial = x_opt + sigma * dx\n                # ensure within bounds by reflecting at boundaries (simple)\n                below_lb = x_trial < lb\n                above_ub = x_trial > ub\n                if np.any(below_lb) or np.any(above_ub):\n                    # reflect\n                    x_trial = np.where(below_lb, lb + (lb - x_trial), x_trial)\n                    x_trial = np.where(above_ub, ub - (x_trial - ub), x_trial)\n                    # after reflection, still clip to be safe\n                    x_trial = np.clip(x_trial, lb, ub)\n\n                f_trial = func(x_trial)\n                evals += 1\n\n                improved = False\n                if f_trial < f_opt:\n                    improved = True\n                    # displacement normalized for covariance update (we used dx as L@z)\n                    # update covariance with outer product of the normalized displacement (dx)\n                    # use rank-one learning similar to CMA ideas\n                    y = dx.reshape(-1, 1)\n                    C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                    # small regularization\n                    C += 1e-12 * np.eye(dim)\n\n                    f_opt = float(f_trial)\n                    x_opt = x_trial.copy()\n                    since_improvement = 0\n                else:\n                    since_improvement += 1\n\n                # update EMA of success rate\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n\n                # step-size control: increase or decrease sigma smoothly\n                # use multiplicative update\n                adjust = np.exp(0.6 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                sigma *= float(np.clip(adjust, 0.6, 1.6))\n\n                # small bounds on sigma to avoid collapse or huge steps\n                sigma = np.clip(sigma, 1e-8 * range_scale + 1e-12, 4.0 * range_scale + 1e-12)\n\n                # early break if budget used\n                if evals >= budget:\n                    break\n\n            # stagnation handling: if no improvement for a while, try a restart-like perturbation\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # Try a random re-location near a random past sample or full random restart\n                # Generate a few random pushes\n                pushes = min(5, budget - evals)\n                for _ in range(pushes):\n                    # sample around a random point in space (diverse exploration)\n                    if rng.random() < 0.5:\n                        center = rng.uniform(lb, ub)\n                    else:\n                        center = x_opt\n                    x_try = center + 2.0 * sigma * rng.normal(size=dim)\n                    x_try = np.clip(x_try, lb, ub)\n                    f_try = func(x_try)\n                    evals += 1\n                    if f_try < f_opt:\n                        f_opt = float(f_try)\n                        x_opt = x_try.copy()\n                        # reset sigma and covariance a bit to encourage local search\n                        sigma = max(sigma * 0.8, 1e-8 * range_scale)\n                        # mix covariance lightly with identity to regain exploration\n                        C = 0.5 * C + 0.5 * np.eye(dim) * ( (range_scale/6.0)**2 )\n                        since_improvement = 0\n                        break\n                    if evals >= budget:\n                        break\n                # escalate exploration if still no improvement: increase jump probability temporarily\n                if since_improvement >= self.stagnation_restart:\n                    self.jump_prob = min(0.5, self.jump_prob * 1.5)\n                    # enlarge sigma a bit to escape\n                    sigma = min(sigma * 2.0, 8.0 * range_scale)\n\n        # final return\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ACLS scored 0.429 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9800553609886713}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9807899891012277}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9771246006680763}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.5108895202342856}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.4842612155538223}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.5336847091317292}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02061456796607708}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.05519412815883784}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.03737340694854718}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.014352957468387273}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.01375243367834078}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.6657695125262966}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.3152729637664715}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.8422073124561928}], "aucs": [0.9800553609886713, 0.9807899891012277, 0.9771246006680763, 0.5108895202342856, 0.4842612155538223, 0.5336847091317292, 0.02061456796607708, 0.05519412815883784, 0.03737340694854718, 0.014352957468387273, 4.999999999999449e-05, 0.01375243367834078, 0.6657695125262966, 0.3152729637664715, 0.8422073124561928]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1289.0, "Edges": 1288.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.9984484096198603, "Degree Variance": 2.1303311844990223, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.770797962648557, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3264650240342042, "Depth Entropy": 2.0901458105837656, "Assortativity": 1.0669148003388695e-08, "Average Eccentricity": 16.30256012412723, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0007757951900698216, "Average Shortest Path": 9.751292590433144, "mean_complexity": 9.0, "total_complexity": 27.0, "mean_token_count": 389.3333333333333, "total_token_count": 1168.0, "mean_parameter_count": 4.666666666666667, "total_parameter_count": 14.0}, "archive_direction": "", "archive_feature": ""}
{"id": "9b57bd18-7a38-4bd7-beb0-51edb67f3b17", "fitness": 0.19450950914073584, "name": "AdaptiveDirectionalSearch", "description": "Adaptive Directional Sampling (ADS) \u2014 a mixed-strategy, gradient-free search that combines global uniform exploration, local Gaussian refinement, and directional recombination along principal directions with adaptive step-size control.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSearch:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n\n    Mixes three mutation strategies:\n      - Global uniform exploration\n      - Local Gaussian refinement around the current best (trust-region style)\n      - Directional recombination / principal-direction probing using archive covariance\n\n    Adapts step-size (sigma) based on success, and adapts sampling probabilities.\n    Works for functions defined on [-5, 5]^dim. Budget is enforced exactly.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # Tunable hyperparameters (sensible defaults)\n        self.init_fraction = 0.12   # fraction of budget used for initial uniform samples\n        self.sigma0 = 1.2           # initial step size (relative to domain width)\n        self.sigma_min = 1e-4\n        self.sigma_max = 5.0\n        self.success_shrink = 0.92  # shrink sigma when improvement\n        self.failure_expand = 1.05  # expand sigma when no improvement\n        self.top_k = 20             # archive top-k for recombination/principal directions\n\n    def __call__(self, func):\n        # Ensure budget is positive\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb, dtype=float)\n        ub = np.full(dim, self.ub, dtype=float)\n        width = ub - lb\n\n        evals = 0\n\n        # Initialize archive lists\n        X_archive = []\n        f_archive = []\n\n        # initial uniform sampling to populate archive\n        n_init = int(max(1, min(budget // 6, max(5, int(self.init_fraction * budget), 2 * dim))))\n        n_init = min(n_init, budget)\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X_archive.append(np.array(x, copy=True))\n            f_archive.append(float(f))\n\n        # convert to arrays for convenience\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # track best\n        best_idx = int(np.argmin(f_archive))\n        x_best = X_archive[best_idx].copy()\n        f_best = float(f_archive[best_idx])\n\n        # adaptive parameters\n        sigma = self.sigma0 * np.mean(width)  # absolute step size\n        sigma = float(np.clip(sigma, self.sigma_min, self.sigma_max))\n        p_explore = 0.45\n        p_local = 0.35\n        p_dir = 0.20\n\n        # helper: choose parent indices weighted by fitness (better = higher prob)\n        def parent_weights(f_vals):\n            # convert to probabilities where lower f is better\n            fvals = np.array(f_vals, dtype=float)\n            fmin = np.min(fvals)\n            # scale by range to avoid overflow or zero division\n            rng_scale = max(np.ptp(fvals), 1e-8)\n            w = np.exp(-(fvals - fmin) / rng_scale)\n            w = w / np.sum(w)\n            return w\n\n        # main loop: one evaluation per iteration (keeps budget exact)\n        while evals < budget:\n            # adapt probabilities slowly: if sigma small -> more exploration; if sigma large -> more local?\n            # Keep simple: reduce exploration probability as search progresses (more evaluations -> exploit more).\n            frac = evals / max(1.0, budget)\n            # linearly move mass from explore to local/dir\n            p_explore = 0.45 * (1 - frac) + 0.05  # remains at least small\n            p_local = 0.4 * (0.5 + frac * 0.5)    # increase slightly over time\n            p_dir = 1.0 - (p_explore + p_local)\n            # numerical safety\n            ps = np.array([p_explore, p_local, p_dir])\n            ps = np.clip(ps, 1e-6, 1.0)\n            ps = ps / np.sum(ps)\n            p_explore, p_local, p_dir = ps\n\n            r = self.rng.random()\n            if r < p_explore:\n                # Global uniform exploration\n                x_candidate = self.rng.uniform(lb, ub)\n            elif r < p_explore + p_local:\n                # Local Gaussian refinement around the current best\n                x_candidate = x_best + sigma * self.rng.normal(size=dim)\n                # Occasionally do anisotropic local moves: scale by empirical std per-dimension\n                if X_archive.shape[0] >= max(4, dim):\n                    # use per-dim std from the archive to scale steps\n                    stds = np.std(X_archive, axis=0)\n                    # avoid too small, keep baseline\n                    stds = np.maximum(stds, 0.05 * np.mean(width))\n                    x_candidate = x_best + sigma * (self.rng.normal(size=dim) * (stds / np.mean(stds)))\n            else:\n                # Directional recombination / principal-direction probing\n                # Use top_k archive members\n                k = min(self.top_k, X_archive.shape[0])\n                if k < 2:\n                    # fallback to uniform if not enough points\n                    x_candidate = self.rng.uniform(lb, ub)\n                else:\n                    # pick parents with weights favoring low f\n                    top_indices = np.argsort(f_archive)[:k]\n                    top_X = X_archive[top_indices]\n                    top_f = f_archive[top_indices]\n                    w = parent_weights(top_f)\n                    # choose two parents\n                    i1 = self.rng.choice(len(top_X), p=w)\n                    i2 = self.rng.choice(len(top_X), p=w)\n                    p1 = top_X[i1]\n                    p2 = top_X[i2]\n                    # recombine plus directional step along difference\n                    alpha = self.rng.uniform(-0.05, 1.05)  # allow slight extrapolation\n                    base = alpha * p1 + (1 - alpha) * p2\n                    # directional magnitude proportional to distance of parents\n                    dir_vec = p1 - p2\n                    dir_norm = np.linalg.norm(dir_vec) + 1e-12\n                    # principal probing: project noise onto dir_vec and top principal components\n                    noise = self.rng.normal(size=dim)\n                    proj = (np.dot(noise, dir_vec) / dir_norm) * (dir_vec / dir_norm)\n                    # estimate principal direction from top_X covariance if enough points\n                    if top_X.shape[0] >= max(3, dim // 2):\n                        try:\n                            # center\n                            C = np.cov(top_X.T)\n                            # SVD for principal directions\n                            U, Svals, Vt = np.linalg.svd(C)\n                            # principal vector\n                            v1 = U[:, 0]\n                            # combine directional projection with principal component perturbation\n                            pc_component = (self.rng.normal() * v1) * (0.5 * sigma / (np.sqrt(Svals[0] + 1e-12)))\n                        except Exception:\n                            pc_component = 0.1 * sigma * noise\n                    else:\n                        pc_component = 0.1 * sigma * noise\n                    dir_component = 0.5 * sigma * proj\n                    x_candidate = base + dir_component + pc_component\n\n            # clip to bounds\n            x_candidate = np.minimum(np.maximum(x_candidate, lb), ub)\n\n            # evaluate\n            f_candidate = float(func(x_candidate))\n            evals += 1\n\n            # update archive\n            X_archive = np.vstack([X_archive, x_candidate])\n            f_archive = np.concatenate([f_archive, [f_candidate]])\n\n            # update best and adapt sigma\n            improved = False\n            if f_candidate < f_best:\n                improved = True\n                f_best = f_candidate\n                x_best = x_candidate.copy()\n\n            # adapt sigma based on success\n            if improved:\n                sigma = max(self.sigma_min, sigma * self.success_shrink)\n            else:\n                sigma = min(self.sigma_max, sigma * self.failure_expand)\n\n            # keep archive size bounded to avoid unbounded memory growth\n            max_archive = max(200, 20 * self.dim)\n            if X_archive.shape[0] > max_archive:\n                # keep best entries + some randoms to maintain diversity\n                best_inds = np.argsort(f_archive)[:max_archive // 2]\n                random_inds = self.rng.choice(np.arange(X_archive.shape[0]), size=max_archive - len(best_inds), replace=False)\n                keep_inds = np.unique(np.concatenate([best_inds, random_inds]))\n                X_archive = X_archive[keep_inds]\n                f_archive = f_archive[keep_inds]\n\n            # Small additional adaptation: if no improvement for long, do a random restart (local reset)\n            # Implemented by occasional large jump when many recent evals had no improvement\n            # Count recent improvements by checking last chunk\n            if evals >= 50:\n                recent = min(50, X_archive.shape[0])\n                if np.min(f_archive[-recent:]) >= f_best and self.rng.random() < 0.02:\n                    # make a larger global jump candidate next iteration by inflating sigma\n                    sigma = min(self.sigma_max, sigma * 2.2)\n\n        # final best\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSearch scored 0.195 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.16204990767356708}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.12707119521251575}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.11285545840806321}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02650813940584884}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.02608400785579401}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0250448579381386}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0045500062813389475}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.02820695650099847}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.7747795275463776}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.7957608256384119}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.834531754649983}], "aucs": [0.16204990767356708, 0.12707119521251575, 0.11285545840806321, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.02650813940584884, 0.02608400785579401, 0.0250448579381386, 4.999999999999449e-05, 0.0045500062813389475, 0.02820695650099847, 0.7747795275463776, 0.7957608256384119, 0.834531754649983]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1449.0, "Edges": 1448.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9986197377501724, "Degree Variance": 2.184953236353002, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.356374807987711, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3174340416677857, "Depth Entropy": 2.2869195410711374, "Assortativity": 9.46433329584136e-09, "Average Eccentricity": 17.73982056590752, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0006901311249137336, "Average Shortest Path": 9.929635221852372, "mean_complexity": 6.0, "total_complexity": 18.0, "mean_token_count": 415.6666666666667, "total_token_count": 1247.0, "mean_parameter_count": 2.3333333333333335, "total_parameter_count": 7.0}, "archive_direction": "", "archive_feature": ""}
{"id": "4751d8ed-f9f8-472b-beaa-438078438529", "fitness": 0.10880757218006136, "name": "APDLS", "description": "Adaptive Population Directional & L\u00e9vy Search (APDLS) \u2014 a small evolving population with adaptive step-sizes that mixes L\u00e9vy-style long jumps, directional local probes and recombination to balance exploration and exploitation.", "code": "import numpy as np\n\nclass APDLS:\n    \"\"\"\n    Adaptive Population Directional & L\u00e9vy Search (APDLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: population size (auto tuned from budget if None)\n    - init_step: initial step size (scale of exploratory moves)\n    - seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, pop_size=None, init_step=1.0, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        self.init_step = float(init_step)\n        self.seed = seed\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # ensure lb/ub shape\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n\n        # adaptive population size: small to moderate depending on budget\n        if self.pop_size is None:\n            # use between 4 and 20 or budget/10 whichever smaller\n            self.pop_size = int(max(3, min(20, max(3, self.budget // 10))))\n        pop_size = min(self.pop_size, max(1, self.budget))  # cannot exceed budget\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initialize population uniformly\n        pop_x = rng.uniform(lb, ub, size=(pop_size, self.dim))\n        pop_f = np.empty(pop_size, dtype=float)\n        pop_step = np.full(pop_size, self.init_step, dtype=float)  # per-individual step scale\n        stagn = np.zeros(pop_size, dtype=int)  # stagnation counters\n\n        # initial evaluations (ensure not to exceed budget)\n        for i in range(pop_size):\n            if evals >= self.budget:\n                pop_f[i] = np.inf\n                continue\n            xi = pop_x[i]\n            fi = func(xi)\n            evals += 1\n            pop_f[i] = fi\n            if fi < f_opt:\n                f_opt = fi\n                x_opt = xi.copy()\n\n        # Main loop: produce candidates for each individual until budget exhausted\n        cycle = 0\n        while evals < self.budget:\n            cycle += 1\n            # Shuffle order to avoid bias\n            order = rng.permutation(pop_size)\n            # Try to produce a few candidates per individual\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                xi = pop_x[idx]\n                si = pop_step[idx]\n\n                # Candidate generation strategies (mix)\n                # 1) L\u00e9vy/Cauchy long jump\n                levy = rng.standard_cauchy(self.dim)\n                cand1 = xi + si * levy\n\n                # 2) Directional probe: random direction scaled by step\n                d = rng.normal(size=self.dim)\n                nd = np.linalg.norm(d)\n                if nd == 0:\n                    d = rng.normal(size=self.dim)\n                    nd = np.linalg.norm(d) + 1e-12\n                d /= nd\n                cand2 = xi + si * d\n                cand3 = xi - si * d\n\n                # 3) Recombination with another random individual (differential-like)\n                j = rng.integers(pop_size)\n                if j == idx:\n                    j = (j + 1) % pop_size\n                beta = rng.random()\n                cand4 = xi + beta * (pop_x[j] - xi)\n\n                # 4) Small Gaussian local refinement around current if stagnating\n                if stagn[idx] >= 3:\n                    cand5 = xi + (si * 0.2) * rng.normal(size=self.dim)\n                else:\n                    # otherwise a small random candidate to maintain diversity\n                    cand5 = xi + (si * 0.6) * rng.normal(size=self.dim)\n\n                # Collect candidates; evaluate sequentially while budget remains\n                candidates = [cand1, cand2, cand3, cand4, cand5]\n                for cand in candidates:\n                    if evals >= self.budget:\n                        break\n                    # enforce bounds\n                    cand_clipped = np.minimum(np.maximum(cand, lb), ub)\n                    # Evaluate\n                    try:\n                        f_cand = func(cand_clipped)\n                    except Exception:\n                        # if the function misbehaves, skip\n                        f_cand = np.inf\n                    evals += 1\n\n                    # If candidate improves the individual, adopt it\n                    if f_cand < pop_f[idx]:\n                        pop_x[idx] = cand_clipped\n                        pop_f[idx] = f_cand\n                        pop_step[idx] = min(pop_step[idx] * 1.2 + 1e-12, 5.0 * (ub - lb).max())  # enlarge step on success\n                        stagn[idx] = 0\n                        # update global best\n                        if f_cand < f_opt:\n                            f_opt = f_cand\n                            x_opt = cand_clipped.copy()\n                    else:\n                        # penalize unsuccessful attempts by shrinking step moderately\n                        pop_step[idx] = max(pop_step[idx] * 0.85, 1e-8)\n                        stagn[idx] += 1\n\n                # Mid-cycle replacement: if an individual stagnates too long, replace with sample around best\n                if stagn[idx] >= 8 and evals < self.budget:\n                    # Replace with a focused sample around the global best to regain progress\n                    rscale = max(0.05, 0.5 * (0.9 ** cycle))\n                    new_x = x_opt + rscale * (ub - lb) * rng.normal(size=self.dim)\n                    new_x = np.minimum(np.maximum(new_x, lb), ub)\n                    try:\n                        new_f = func(new_x)\n                    except Exception:\n                        new_f = np.inf\n                    evals += 1\n                    pop_x[idx] = new_x\n                    pop_f[idx] = new_f\n                    pop_step[idx] = max(self.init_step * 0.5, pop_step[idx])\n                    stagn[idx] = 0\n                    if new_f < f_opt:\n                        f_opt = new_f\n                        x_opt = new_x.copy()\n\n            # Periodic global diversification / pruning\n            if evals < self.budget and cycle % 6 == 0:\n                # Keep top half, replace bottom half with perturbations of top individuals\n                ranks = np.argsort(pop_f)\n                topk = ranks[: max(1, pop_size // 2)]\n                bottomk = ranks[max(1, pop_size // 2):]\n                for b in bottomk:\n                    if evals >= self.budget:\n                        break\n                    donor = topk[rng.integers(len(topk))]\n                    # create a new individual by perturbing donor\n                    perturb = 0.8 * (0.9 ** cycle) * (ub - lb) * rng.normal(size=self.dim)\n                    new_x = pop_x[donor] + perturb\n                    new_x = np.minimum(np.maximum(new_x, lb), ub)\n                    try:\n                        new_f = func(new_x)\n                    except Exception:\n                        new_f = np.inf\n                    evals += 1\n                    pop_x[b] = new_x\n                    pop_f[b] = new_f\n                    pop_step[b] = pop_step[donor] * 0.9\n                    stagn[b] = 0\n                    if new_f < f_opt:\n                        f_opt = new_f\n                        x_opt = new_x.copy()\n\n            # If global progress stalls for many cycles, force a wide restart of a few individuals\n            if cycle % 20 == 0:\n                # detect global stagnation\n                best_recent = pop_f.min()\n                if best_recent >= f_opt - 1e-12:\n                    # inject wide random samples (but don't exceed budget)\n                    replacements = min(pop_size // 3, max(1, (self.budget - evals) // 5))\n                    for _ in range(replacements):\n                        if evals >= self.budget:\n                            break\n                        idx = rng.integers(pop_size)\n                        new_x = rng.uniform(lb, ub)\n                        try:\n                            new_f = func(new_x)\n                        except Exception:\n                            new_f = np.inf\n                        evals += 1\n                        pop_x[idx] = new_x\n                        pop_f[idx] = new_f\n                        pop_step[idx] = self.init_step\n                        stagn[idx] = 0\n                        if new_f < f_opt:\n                            f_opt = new_f\n                            x_opt = new_x.copy()\n\n        # Final return\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm APDLS scored 0.109 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.41242663147854874}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.3073859488296714}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.3017409818625706}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02877314975623979}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0357446263288288}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.03195315362262008}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.028146608864107803}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.045217616661909266}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.13337009545029832}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.16669274316287552}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.14046202668325025}], "aucs": [0.41242663147854874, 0.3073859488296714, 0.3017409818625706, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.02877314975623979, 0.0357446263288288, 0.03195315362262008, 0.028146608864107803, 0.045217616661909266, 4.999999999999449e-05, 0.13337009545029832, 0.16669274316287552, 0.14046202668325025]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1477.0, "Edges": 1476.0, "Max Degree": 22.0, "Min Degree": 1.0, "Mean Degree": 1.998645903859174, "Degree Variance": 2.0094768394094236, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.36390977443609, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3090411510403759, "Depth Entropy": 2.064731955113055, "Assortativity": 0.0, "Average Eccentricity": 16.272173324306024, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0006770480704129993, "Average Shortest Path": 9.754244394170414, "mean_complexity": 17.5, "total_complexity": 35.0, "mean_token_count": 602.0, "total_token_count": 1204.0, "mean_parameter_count": 4.0, "total_parameter_count": 8.0}, "archive_direction": "", "archive_feature": ""}
{"id": "b4f78011-40d8-486f-a970-ac4eb735d830", "fitness": "-inf", "name": "AdaptiveDirectionalGaussianSearch", "description": "Adaptive Directional Gaussian Search (ADGS) \u2014 combine covariance-informed Gaussian sampling, principal-direction line probes, adaptive step-size and occasional restarts to efficiently explore/exploit in the [-5,5]^d box.", "code": "import numpy as np\n\nclass AdaptiveDirectionalGaussianSearch:\n    \"\"\"\n    Adaptive Directional Gaussian Search (ADGS)\n\n    Main ideas:\n    - Maintain an archive of evaluated points.\n    - Use covariance of the elite points to form an anisotropic multivariate normal\n      for sampling around the current best (exploitation with learned directions).\n    - Perform short directional line probes along top principal components (exploit\n      promising directions).\n    - Adapt the global step-size (sigma) using success-rate rules.\n    - Do occasional restarts when stagnation is detected.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng_seed=None,\n                 init_samples=None, pop_size=None, sigma0=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng_seed = rng_seed\n        self.rng = np.random.default_rng(rng_seed)\n        # sensible defaults depending on budget and dimension\n        self.init_samples = int(init_samples) if init_samples is not None else max(20, min(200, self.budget // 20))\n        self.pop_size = int(pop_size) if pop_size is not None else max(10, min(200, self.budget // 50))\n        self.sigma0 = float(sigma0) if sigma0 is not None else 0.3  # initial normalized step (relative to search width)\n        # parameters\n        self.succ_target = 0.2\n        self.adapt_up = 1.2\n        self.adapt_down = 0.85\n        self.patience_restart = 10\n        self.min_sigma = 1e-6\n        self.max_sigma = 5.0\n\n    def __call__(self, func):\n        # Establish bounds (support scalar or vector bounds on func.bounds)\n        lb, ub = -5.0, 5.0\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            # try multiple patterns (scalars or arrays)\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb_v = b.lb\n                ub_v = b.ub\n                # if scalars or length-1, broadcast\n                try:\n                    lb_arr = np.asarray(lb_v, dtype=float)\n                    ub_arr = np.asarray(ub_v, dtype=float)\n                    if lb_arr.size == 1:\n                        lb = float(lb_arr) * np.ones(self.dim)\n                    else:\n                        lb = lb_arr.reshape(-1)[:self.dim]\n                except Exception:\n                    lb = -5.0\n                try:\n                    ub_arr = np.asarray(ub_v, dtype=float)\n                    if ub_arr.size == 1:\n                        ub = float(ub_arr) * np.ones(self.dim)\n                    else:\n                        ub = ub_arr.reshape(-1)[:self.dim]\n                except Exception:\n                    ub = 5.0\n        # If lb/ub are scalars make arrays\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # normalized width used to scale sigma to actual coordinate range\n        width = ub - lb\n        # initialize archive\n        X = []\n        F = []\n        evals = 0\n\n        def eval_x(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise RuntimeError(\"Budget exceeded\")\n            # ensure shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy())\n            F.append(float(f))\n            return f, x\n\n        # Initial random samples (global exploration)\n        n_init = min(self.init_samples, self.budget)\n        for _ in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            eval_x(x0)\n            if evals >= self.budget:\n                break\n\n        # ensure we have at least one point\n        if len(F) == 0:\n            x0 = self.rng.uniform(lb, ub)\n            eval_x(x0)\n\n        # bookkeeping\n        X = np.array(X)  # shape (n, dim)\n        F = np.array(F)\n        best_idx = int(np.argmin(F))\n        x_best = X[best_idx].copy()\n        f_best = float(F[best_idx])\n\n        sigma = self.sigma0 * np.linalg.norm(width) / np.sqrt(self.dim)  # absolute sigma in coordinate space\n        sigma = float(np.clip(sigma, self.min_sigma, self.max_sigma))\n        no_improve = 0\n        success_window = []\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # compute elites\n            n_points = X.shape[0]\n            k_elite = max(2, min(n_points, max(5, n_points // 6)))\n            elite_idx = np.argsort(F)[:k_elite]\n            elites = X[elite_idx]\n            # compute mean and covariance of elites\n            if elites.shape[0] >= 2:\n                mean_elite = elites.mean(axis=0)\n                # rowvar=False: observations in rows -> shape (dim,dim)\n                cov = np.cov(elites, rowvar=False)\n                # regularize\n                cov = cov + 1e-8 * np.eye(self.dim)\n            else:\n                mean_elite = elites[0].copy()\n                cov = np.eye(self.dim) * 1e-6\n\n            # blend covariance with isotropic component to keep exploration\n            cov_scale = max(1e-12, np.linalg.norm(width)**2 / (self.dim**2))\n            C = cov + 0.05 * cov_scale * np.eye(self.dim)\n\n            # sample a population around current best using anisotropic gaussian\n            pop = min(self.pop_size, remaining)\n            # build sampling covariance matrix\n            sample_cov = (sigma**2) * C\n            # ensure positive-definite symmetric\n            sample_cov = (sample_cov + sample_cov.T) / 2.0\n            # do Cholesky with fallback to eigen\n            try:\n                L = np.linalg.cholesky(sample_cov)\n            except np.linalg.LinAlgError:\n                # use eigen decomposition\n                vals, vecs = np.linalg.eigh(sample_cov)\n                vals = np.clip(vals, 1e-12, None)\n                L = vecs @ np.diag(np.sqrt(vals))\n\n            # keep one candidate per iteration that is pure perturbation around x_best\n            n_eval_this_round = 0\n            improved = False\n            for i in range(pop):\n                if evals >= self.budget:\n                    break\n                z = self.rng.normal(size=self.dim)\n                cand = x_best + L @ z\n                # clip\n                cand = np.minimum(np.maximum(cand, lb), ub)\n                f_cand, x_cand = eval_x(cand)\n                n_eval_this_round += 1\n                # update arrays X,F already appended in eval_x; update local copies\n                if f_cand < f_best:\n                    f_best = f_cand\n                    x_best = x_cand.copy()\n                    improved = True\n                    no_improve = 0\n                # avoid too many samples at once if budget small\n                if evals >= self.budget:\n                    break\n\n            # directional line probes along top principal components of C\n            # compute eigenpairs\n            try:\n                eigvals, eigvecs = np.linalg.eigh(C)\n            except np.linalg.LinAlgError:\n                eigvals = np.ones(self.dim)\n                eigvecs = np.eye(self.dim)\n            # sort descending\n            order = np.argsort(-eigvals)\n            eigvals = eigvals[order]\n            eigvecs = eigvecs[:, order]\n            n_dir = min(self.dim, 3, eigvecs.shape[1])\n            for j in range(n_dir):\n                if evals >= self.budget:\n                    break\n                direction = eigvecs[:, j]\n                # scale for probing: combine eigenvalue and sigma\n                step_len = sigma * (1.0 + np.sqrt(max(eigvals[j], 1e-12)))\n                # try a short sequence of probes: positive and negative\n                for sgn in (+1.0, -1.0):\n                    if evals >= self.budget:\n                        break\n                    probe = x_best + sgn * direction * step_len\n                    probe = np.minimum(np.maximum(probe, lb), ub)\n                    f_probe, x_probe = eval_x(probe)\n                    if f_probe < f_best:\n                        f_best = f_probe\n                        x_best = x_probe.copy()\n                        improved = True\n                        no_improve = 0\n\n            # adapt sigma based on success in this batch\n            success = 1.0 if improved else 0.0\n            success_window.append(success)\n            if len(success_window) > 20:\n                success_window.pop(0)\n            succ_rate = np.mean(success_window) if len(success_window) > 0 else 0.0\n            if succ_rate > self.succ_target:\n                sigma = min(self.max_sigma, sigma * self.adapt_down)  # shrink when many successes\n            else:\n                sigma = min(self.max_sigma, sigma * self.adapt_up)  # grow when few successes\n\n            sigma = float(np.clip(sigma, self.min_sigma, self.max_sigma))\n            if improved:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # occasional restart if stuck\n            if no_improve >= self.patience_restart and evals < self.budget:\n                # pick a new center by sampling uniformly or from a random elite\n                if self.rng.random() < 0.6 and X.shape[0] > 0:\n                    idx = self.rng.integers(0, X.shape[0])\n                    x_restart = X[idx] + self.rng.normal(scale=0.5 * sigma, size=self.dim)\n                else:\n                    x_restart = self.rng.uniform(lb, ub)\n                x_restart = np.minimum(np.maximum(x_restart, lb), ub)\n                # evaluate restart point\n                try:\n                    f_r, x_r = eval_x(x_restart)\n                except RuntimeError:\n                    break\n                if f_r < f_best:\n                    f_best = f_r\n                    x_best = x_r.copy()\n                    no_improve = 0\n                else:\n                    # if restart didn't help, reset sigma moderately\n                    sigma = max(self.sigma0 * np.linalg.norm(width) / np.sqrt(self.dim), self.min_sigma)\n                    no_improve = 0\n                # continue main loop\n\n            # update X,F arrays and best (they were already appended by eval_x)\n            X = np.array(X)\n            F = np.array(F)\n            best_idx = int(np.argmin(F))\n            x_best = X[best_idx].copy()\n            f_best = float(F[best_idx])\n\n        # final return best found\n        self.f_opt = float(f_best)\n        self.x_opt = x_best.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 86, in eval_x, the following error occurred:\nAttributeError: 'numpy.ndarray' object has no attribute 'append'\nOn line: X.append(x.copy())", "error": "In the code, line 86, in eval_x, the following error occurred:\nAttributeError: 'numpy.ndarray' object has no attribute 'append'\nOn line: X.append(x.copy())", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1894.0, "Edges": 1893.0, "Max Degree": 36.0, "Min Degree": 1.0, "Mean Degree": 1.9989440337909188, "Degree Variance": 2.348467733932197, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.7053254437869825, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.311003564901304, "Depth Entropy": 1.9930895035450038, "Assortativity": 7.470803634954267e-09, "Average Eccentricity": 16.029039070749736, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0005279831045406547, "Average Shortest Path": 9.505040802244249, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 550.0, "total_token_count": 1650.0, "mean_parameter_count": 3.3333333333333335, "total_parameter_count": 10.0}, "archive_direction": "", "archive_feature": ""}
{"id": "0e0f14b7-0341-4b06-a151-5442f5c3b114", "fitness": 0.5584758686406592, "name": "AdaptiveSubspaceExploratorySearch", "description": "Adaptive Subspace Exploratory Search (ASES) \u2014 perform randomized low-dimensional subspace explorations inside an adaptive trust region around the current best, shrink/expand the region by success/failure, and occasionally diversify with global samples and opposites.", "code": "import numpy as np\n\nclass AdaptiveSubspaceExploratorySearch:\n    \"\"\"\n    Adaptive Subspace Exploratory Search (ASES)\n\n    Main idea:\n      - Keep a current best point and an adaptive trust radius.\n      - Repeatedly explore random low-dimensional subspaces around the best point\n        with Gaussian perturbations scaled by the trust radius.\n      - Expand the radius when improvements are found, shrink when not.\n      - If stagnation occurs, try opposite point and limited global random diversification.\n      - All evaluations strictly bounded by the provided budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, init_radius=2.5, min_radius=1e-6,\n                 max_radius=2.5, stagnation_patience=6, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          init_radius: starting trust-region radius (default 2.5, half of [-5,5] range)\n          min_radius: minimum radius before forced diversification\n          max_radius: maximum radius allowed\n          stagnation_patience: iterations without improvement before diversification\n          rng: optional numpy.random.Generator instance (for reproducibility)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        # Try to read bounds from func.bounds if available; otherwise assume [-5,5]\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        # convert scalars to arrays\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():  # scalar\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():  # scalar\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using at most self.budget function evaluations.\n        The func should accept a numpy array of shape (dim,) and return a scalar.\n        Returns the best function value and the corresponding x found: (f_opt, x_opt)\n        \"\"\"\n        lb, ub = self._get_bounds(func)\n        # safety: ensure bounds length matches dim\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        # bookkeeping\n        evals = 0\n        # initial sample\n        x_best = lb + self.rng.random(self.dim) * (ub - lb)\n        f_best = float(func(x_best))\n        evals += 1\n\n        # state\n        radius = float(self.init_radius)\n        stagnation = 0\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # Subspace dimension selection: favor small subspaces but sometimes larger\n            # Use a beta distribution skewed towards small k\n            raw = self.rng.beta(1.8, 5.0)\n            k = int(np.clip(np.ceil(raw * self.dim), 1, self.dim))\n\n            # batch size depends on radius and remaining budget\n            # more exploration (bigger batches) when radius is larger\n            approx_batch = int(np.clip(6 * (1.0 + (radius / self.max_radius)), 1, 50))\n            batch = max(1, min(remaining, approx_batch))\n\n            improved_in_batch = False\n            # For numerical stability scale sigma inversely with sqrt(k) so overall perturbation magnitude is ~radius\n            sigma = max(radius / np.sqrt(k), 1e-12)\n\n            for _ in range(batch):\n                if evals >= self.budget:\n                    break\n\n                # choose k coordinates to perturb\n                if k == self.dim:\n                    indices = np.arange(self.dim)\n                else:\n                    indices = self.rng.choice(self.dim, size=k, replace=False)\n\n                # generate perturbation in subspace\n                step = self.rng.normal(loc=0.0, scale=sigma, size=k)\n                x_candidate = x_best.copy()\n                x_candidate[indices] += step\n                # enforce bounds\n                x_candidate = np.minimum(np.maximum(x_candidate, lb), ub)\n\n                f_candidate = float(func(x_candidate))\n                evals += 1\n\n                # accept improvement\n                if f_candidate < f_best:\n                    f_best = f_candidate\n                    x_best = x_candidate\n                    improved_in_batch = True\n                    stagnation = 0\n                    # expand the trust region slightly on success\n                    radius = min(self.max_radius, radius * 1.15 + 1e-12)\n                    # small immediate exploitation: try a short greedy step along same direction\n                    if evals < self.budget:\n                        # a scaled greedy probe\n                        probe = x_candidate.copy()\n                        probe[indices] += 0.5 * step\n                        probe = np.minimum(np.maximum(probe, lb), ub)\n                        f_probe = float(func(probe))\n                        evals += 1\n                        if f_probe < f_best:\n                            f_best = f_probe\n                            x_best = probe\n                            radius = min(self.max_radius, radius * 1.2)\n                    # continue exploring with updated center if budget allows\n                # else continue sampling others\n            # end batch\n\n            if not improved_in_batch:\n                stagnation += 1\n                # shrink radius on failure\n                radius *= 0.6\n            # if improved, stagnation already reset\n\n            # occasional opposite-point check (cheap diversification)\n            if evals < self.budget and (stagnation >= 2 and self.rng.random() < 0.25):\n                x_opp = lb + ub - x_best  # mirror inside bounds\n                x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                f_opp = float(func(x_opp))\n                evals += 1\n                if f_opp < f_best:\n                    f_best = f_opp\n                    x_best = x_opp\n                    radius = min(self.max_radius, radius * 1.2)\n                    stagnation = 0\n\n            # forced diversification after patience exceeded\n            if stagnation >= self.stagnation_patience:\n                # try a few global random samples and one \"near-random\" jump\n                num_global = min(5, max(1, (self.dim // 2)))\n                for _ in range(num_global):\n                    if evals >= self.budget:\n                        break\n                    x_rand = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_rand = float(func(x_rand))\n                    evals += 1\n                    if f_rand < f_best:\n                        f_best = f_rand\n                        x_best = x_rand\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n                        break\n                # if still no improvement, perform a near-random jump centered around a random point\n                if stagnation >= self.stagnation_patience and evals < self.budget:\n                    jump_scale = 0.5 * (ub - lb)\n                    x_jump = x_best + self.rng.normal(0, 1.0, size=self.dim) * jump_scale\n                    x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                    f_jump = float(func(x_jump))\n                    evals += 1\n                    if f_jump < f_best:\n                        f_best = f_jump\n                        x_best = x_jump\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n                    else:\n                        # if even the jump fails, reset radius to a moderate value to encourage exploration\n                        radius = max(self.min_radius, self.init_radius * 0.6)\n                        stagnation = 0  # give another cycle after diversification\n\n            # ensure radius not too small\n            if radius < self.min_radius:\n                # tiny radius => reinitialize radius and try a moderate random move\n                radius = max(self.min_radius, self.init_radius * 0.5)\n                if evals < self.budget:\n                    x_try = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_try = float(func(x_try))\n                    evals += 1\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n\n        # end main loop\n        return f_best, x_best", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSubspaceExploratorySearch scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9681815939867833}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9731760185653048}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9743664652621153}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.5361481234381055}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.20401741084311897}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.32490611280501536}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.32744181280072104}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.2868192079170686}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.4143379053219237}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.14090924371560554}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.13971854034602382}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.12170784852344918}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9873154262585652}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9896534746427401}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9884388451833469}], "aucs": [0.9681815939867833, 0.9731760185653048, 0.9743664652621153, 0.5361481234381055, 0.20401741084311897, 0.32490611280501536, 0.32744181280072104, 0.2868192079170686, 0.4143379053219237, 0.14090924371560554, 0.13971854034602382, 0.12170784852344918, 0.9873154262585652, 0.9896534746427401, 0.9884388451833469]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1307.0, "Edges": 1306.0, "Max Degree": 16.0, "Min Degree": 1.0, "Mean Degree": 1.9984697781178271, "Degree Variance": 1.775055041741573, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.959527824620573, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3394105433768826, "Depth Entropy": 2.051252263875764, "Assortativity": 2.5001505345065247e-08, "Average Eccentricity": 15.28538638102525, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0007651109410864575, "Average Shortest Path": 9.932911604495056, "mean_complexity": 10.666666666666666, "total_complexity": 32.0, "mean_token_count": 376.3333333333333, "total_token_count": 1129.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}, "archive_direction": "", "archive_feature": ""}
{"id": "59af599d-0632-4845-b16d-cfcb095e7b04", "fitness": 0.21458564160611127, "name": "AdaptiveSpiralLevy", "description": "Adaptive Spiral-Levy Population (ASLP) \u2014 a population-based directional spiral search toward the best solution combined with occasional Levy jumps, opposition sampling, and a final mesh-adaptive local refinement.", "code": "import numpy as np\n\nclass AdaptiveSpiralLevy:\n    \"\"\"\n    Adaptive Spiral-Levy Population (ASLP)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional parameters may be passed to tune behavior (kept simple by defaults).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, levy_prob=0.12, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.levy_prob = float(levy_prob)\n        self.rng = np.random.default_rng(seed)\n        # Choose a reasonable population size relative to budget and dimension\n        if pop_size is None:\n            self.pop_size = max(8, min(40, int(max(10, np.sqrt(self.budget) * 2))))\n        else:\n            self.pop_size = int(pop_size)\n\n    def _get_bounds(self, func):\n        # attempt to use func.bounds if available, otherwise default to [-5, 5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # If scalar bounds provided, broadcast\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n\n        evals = 0\n        # initialize population uniformly\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # identify best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        # parameters for adaptive moves\n        alpha0 = 0.6 * np.linalg.norm(ub - lb)  # initial directional step scale\n        sigma0 = 0.2 * np.linalg.norm(ub - lb)  # gaussian perturbation scale\n        no_improve = 0\n        max_no_improve = max(20, self.pop_size * 3)\n\n        # main population iterations: continue until budget exhausted\n        while evals < self.budget:\n            # adapt parameters with remaining budget (more exploitation when near end)\n            frac = evals / max(1, self.budget)\n            alpha = alpha0 * (1 - frac) + 1e-6\n            sigma = sigma0 * (1 - 0.8 * frac) + 1e-8\n\n            improved_in_loop = False\n\n            for i in range(self.pop_size):\n                if evals >= self.budget: break\n\n                xi = pop[i].copy()\n                fi = pop_f[i]\n\n                # directional vector toward best\n                d = x_best - xi\n                norm_d = np.linalg.norm(d) + 1e-12\n                dir_vec = d / norm_d\n\n                # small randomized rotation in a random 2D subspace to create a spiral\n                if dim > 1:\n                    a, b = self.rng.choice(dim, size=2, replace=False)\n                    theta = self.rng.uniform(-np.pi/3, np.pi/3)\n                    ca, cb = np.cos(theta), np.sin(theta)\n                    # rotate the component of d in the (a,b) plane\n                    da, db = d[a], d[b]\n                    d_rot_a = ca * da - cb * db\n                    d_rot_b = cb * da + ca * db\n                    d_rot = d.copy()\n                    d_rot[a], d_rot[b] = d_rot_a, d_rot_b\n                    # new direction from rotated vector\n                    dir_vec = d_rot / (np.linalg.norm(d_rot) + 1e-12)\n\n                # base spiral move toward best with random scaling\n                move = alpha * dir_vec * (0.5 + self.rng.random())\n\n                # gaussian local perturbation\n                move += sigma * self.rng.standard_normal(dim)\n\n                # occasional Levy jump to escape basins\n                if self.rng.random() < self.levy_prob:\n                    # use Cauchy-like heavy tail scaled to problem size but clipped\n                    levy_scale = 0.25 * np.linalg.norm(ub - lb)\n                    # sample from standard Cauchy via tan(pi*(u-0.5))\n                    u = self.rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    move += levy_scale * cauchy\n                    # clip large leaps\n                    move = np.clip(move, -np.linalg.norm(ub - lb), np.linalg.norm(ub - lb))\n\n                # candidate position\n                x_new = xi + move\n                # opposition-based candidate (cheap diversification): candidate symmetric around center\n                if self.rng.random() < 0.08:\n                    x_opp = lb + ub - xi\n                    x_opp = np.clip(x_opp, lb, ub)\n                    if evals < self.budget:\n                        f_opp = float(func(x_opp))\n                        evals += 1\n                        if f_opp < fi:\n                            pop[i] = x_opp\n                            pop_f[i] = f_opp\n                            if f_opp < f_best:\n                                f_best = f_opp; x_best = x_opp.copy()\n                                improved_in_loop = True\n                            continue  # proceed to next particle\n\n                # ensure bounds\n                x_new = np.clip(x_new, lb, ub)\n\n                # evaluate candidate\n                f_new = float(func(x_new))\n                evals += 1\n\n                # greedy replacement, but allow occasional uphill acceptance to keep diversity\n                if f_new < fi or (self.rng.random() < 0.01):\n                    pop[i] = x_new\n                    pop_f[i] = f_new\n\n                # update global best\n                if f_new < f_best:\n                    f_best = f_new\n                    x_best = x_new.copy()\n                    improved_in_loop = True\n\n            if improved_in_loop:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # if stuck, re-seed half of population around best + some random individuals\n            if no_improve >= max_no_improve and evals < self.budget:\n                no_improve = 0\n                n_reset = max(1, self.pop_size // 2)\n                for j in range(n_reset):\n                    if evals >= self.budget: break\n                    idx = self.rng.integers(0, self.pop_size)\n                    # place near best with small gaussian\n                    newp = x_best + 0.1 * (ub - lb) * self.rng.standard_normal(dim)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    pop[idx] = newp\n                    pop_f[idx] = f_new\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # small random restarts when budget allows: inject one random sample per major loop\n            if (self.rng.random() < 0.03) and (evals < self.budget):\n                r = self.rng.uniform(lb, ub)\n                fr = float(func(r))\n                evals += 1\n                # replace worst if better\n                idx_worst = int(np.argmax(pop_f))\n                if fr < pop_f[idx_worst]:\n                    pop[idx_worst] = r\n                    pop_f[idx_worst] = fr\n                    if fr < f_best:\n                        f_best = fr; x_best = r.copy()\n\n            # If very low remaining budget, break to local refine\n            if self.budget - evals < max(10, dim * 3):\n                break\n\n        # Final mesh-adaptive local refinement (pattern search style)\n        # Start mesh size as a fraction of range, shrink when no improvement\n        mesh = 0.2 * np.linalg.norm(ub - lb)\n        mesh_min = 1e-6 * np.linalg.norm(ub - lb)\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # try positive direction\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    # continue exploring same coordinate in same direction\n                    continue\n                # try negative direction\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5  # reduce mesh and try again\n\n        # store results\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSpiralLevy scored 0.215 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.08971415063129407}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.10031439797137698}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.09611059102059738}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.001368210397262204}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0008753545182020872}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.002009399322147165}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9406773850574086}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9951797345954874}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.992235400577893}], "aucs": [0.08971415063129407, 0.10031439797137698, 0.09611059102059738, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.001368210397262204, 0.0008753545182020872, 0.002009399322147165, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.9406773850574086, 0.9951797345954874, 0.992235400577893]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1749.0, "Edges": 1748.0, "Max Degree": 22.0, "Min Degree": 1.0, "Mean Degree": 1.9988564894225271, "Degree Variance": 1.8959392298335305, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.9949685534591195, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3322762956729532, "Depth Entropy": 1.9912185655063326, "Assortativity": 0.0, "Average Eccentricity": 18.31160663236135, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0005717552887364208, "Average Shortest Path": 10.085671707795106, "mean_complexity": 14.0, "total_complexity": 42.0, "mean_token_count": 484.3333333333333, "total_token_count": 1453.0, "mean_parameter_count": 3.3333333333333335, "total_parameter_count": 10.0}, "archive_direction": "", "archive_feature": ""}
{"id": "0228629a-a7a4-4d75-8105-fe131f1f6e92", "fitness": 0.20726123009321723, "name": "MSDirectionalSubspaceSearch", "description": "Multi-Scale Directional Subspace Search (MS-DSS) \u2014 low-discrepancy seeding followed by adaptive directional line searches in random low-dimensional subspaces with step shrinking and occasional recombination.", "code": "import numpy as np\n\nclass MSDirectionalSubspaceSearch:\n    \"\"\"\n    Multi-Scale Directional Subspace Search (MS-DSS)\n\n    Main idea:\n    1) Use a compact Latin-Hypercube-like initial scan to get a global view.\n    2) Select several best seeds.\n    3) For each seed perform adaptive directional line searches in randomly chosen low-dimensional\n       subspaces. Use greedy moves along directions with step shrinking.\n    4) Occasional recombination among top seeds to escape local traps.\n    Works with a strict limit on the number of function evaluations (budget).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng_seed=None,\n                 init_frac=0.15, max_seeds=12, subspace_scale='sqrt'):\n        \"\"\"\n        budget: total allowed function evaluations\n        dim: problem dimensionality\n        rng_seed: optional integer seed for reproducibility\n        init_frac: fraction of budget used for initial global scan (0..1)\n        max_seeds: maximum number of local seeds to refine\n        subspace_scale: 'sqrt' (use ceil(sqrt(dim))) or integer for subspace dimension\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(rng_seed)\n        self.init_frac = float(init_frac)\n        self.max_seeds = int(max_seeds)\n        self.subspace_scale = subspace_scale\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # try to use provided bounds, otherwise default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # Ensure 1D arrays of correct length\n        lb = lb.reshape(-1)[:self.dim]\n        ub = ub.reshape(-1)[:self.dim]\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _lhs(self, n_samples, lb, ub):\n        # Simple Latin Hypercube Sampling\n        n = int(n_samples)\n        dim = self.dim\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.random((n, dim))\n        pts = np.empty((n, dim))\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            pts[:, j] = (perm + u[:, j]) / n\n        # scale to bounds\n        return lb + pts * (ub - lb)\n\n    def __call__(self, func):\n        # internal call counter and eval wrapper\n        calls_made = 0\n        B = self.budget\n\n        def eval_f(x):\n            nonlocal calls_made\n            if calls_made >= B:\n                raise StopIteration(\"Budget exhausted\")\n            # ensure numpy array\n            x = np.asarray(x, dtype=float)\n            calls_made += 1\n            return float(func(x))\n\n        lb, ub = self._get_bounds(func)\n        # Initialize best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Determine initial sampling size\n        init_samples = max(10 * self.dim, 50)\n        init_samples = int(min(init_samples, max(1, int(self.init_frac * B))))\n        # ensure at least a few samples but leave budget for refinement\n        init_samples = max(1, min(init_samples, max(1, B - min(50, B//4))))\n        # Evaluate initial LHS sample\n        try:\n            X0 = self._lhs(init_samples, lb, ub)\n            F0 = np.empty(init_samples, dtype=float)\n            for i in range(init_samples):\n                fval = eval_f(X0[i])\n                F0[i] = fval\n                if fval < self.f_opt:\n                    self.f_opt = fval\n                    self.x_opt = X0[i].copy()\n        except StopIteration:\n            # budget exhausted during initial scan\n            return self.f_opt, self.x_opt\n\n        # select seeds\n        n_seeds = min(self.max_seeds, max(1, int(np.sqrt(init_samples))))\n        # Also limit seeds so we have at least a few evaluations per seed\n        n_seeds = min(n_seeds, max(1, (B - calls_made) // 10))\n        if n_seeds < 1:\n            n_seeds = 1\n\n        idx_sorted = np.argsort(F0)\n        seeds_X = [X0[i].copy() for i in idx_sorted[:n_seeds]]\n        seeds_F = [float(F0[i]) for i in idx_sorted[:n_seeds]]\n\n        # Per-seed trust radius initial: fraction of bound width\n        width = ub - lb\n        initial_radius = 0.25 * np.linalg.norm(width) / np.sqrt(self.dim)  # scale sensibly\n\n        # Subspace dimension\n        if isinstance(self.subspace_scale, int):\n            sdim = max(1, min(self.dim, int(self.subspace_scale)))\n        else:\n            sdim = max(1, min(self.dim, int(np.ceil(np.sqrt(self.dim)))))\n\n        # Refinement loop: iterate through seeds making greedy directional moves\n        # Stopping when budget exhausted or no improvement for many rounds\n        no_improve_rounds = 0\n        max_no_improve = 6 + self.dim // 5\n        try:\n            while calls_made < B and no_improve_rounds < max_no_improve:\n                improved_any = False\n                # iterate seeds in order of current quality\n                order = np.argsort(seeds_F)\n                for idx in order:\n                    if calls_made >= B:\n                        break\n                    x_curr = seeds_X[idx]\n                    f_curr = seeds_F[idx]\n\n                    # pick a random subspace of coordinates\n                    if sdim < self.dim:\n                        sub_idx = self.rng.choice(self.dim, size=sdim, replace=False)\n                    else:\n                        sub_idx = np.arange(self.dim)\n\n                    # create a random direction in the subspace (unit)\n                    dir_sub = self.rng.normal(size=sub_idx.shape[0])\n                    norm = np.linalg.norm(dir_sub)\n                    if norm == 0:\n                        dir_sub = np.ones_like(dir_sub) / np.sqrt(len(dir_sub))\n                    else:\n                        dir_sub = dir_sub / norm\n                    direction = np.zeros(self.dim, dtype=float)\n                    direction[sub_idx] = dir_sub\n\n                    # adaptive step search along + and - direction with shrinking\n                    step = initial_radius\n                    step_shrink = 0.5\n                    min_step = 1e-6 * (ub - lb).mean() + 1e-9\n                    improved_local = False\n                    # limit inner iterations to avoid starving budget\n                    max_inner = 8 + int(np.ceil(np.log2((initial_radius / (min_step + 1e-12)) + 1)))\n                    inner = 0\n                    while inner < max_inner and calls_made < B and step > min_step:\n                        inner += 1\n                        # try positive direction\n                        x_plus = np.clip(x_curr + step * direction, lb, ub)\n                        f_plus = eval_f(x_plus)\n                        if f_plus < f_curr:\n                            x_curr = x_plus\n                            f_curr = f_plus\n                            improved_local = True\n                            if f_plus < self.f_opt:\n                                self.f_opt = f_plus\n                                self.x_opt = x_plus.copy()\n                            # continue exploring in same direction (keep step)\n                            continue\n                        # try negative direction\n                        x_minus = np.clip(x_curr - step * direction, lb, ub)\n                        f_minus = eval_f(x_minus)\n                        if f_minus < f_curr:\n                            x_curr = x_minus\n                            f_curr = f_minus\n                            improved_local = True\n                            if f_minus < self.f_opt:\n                                self.f_opt = f_minus\n                                self.x_opt = x_minus.copy()\n                            continue\n                        # no improvement, shrink step\n                        step *= step_shrink\n\n                    # after exploring direction for this seed, maybe try a coordinate-wise jitter\n                    if calls_made < B:\n                        # small local random perturbation try\n                        jitter_scale = 0.02 * (ub - lb)\n                        x_j = np.clip(x_curr + self.rng.normal(scale=jitter_scale), lb, ub)\n                        f_j = eval_f(x_j)\n                        if f_j < f_curr:\n                            x_curr = x_j\n                            f_curr = f_j\n                            improved_local = True\n                            if f_j < self.f_opt:\n                                self.f_opt = f_j\n                                self.x_opt = x_j.copy()\n\n                    # update seed\n                    seeds_X[idx] = x_curr\n                    seeds_F[idx] = f_curr\n                    if improved_local:\n                        improved_any = True\n\n                # occasional recombination: create new candidates from weighted average of best seeds\n                if calls_made < B:\n                    # perform recombination with small probability if stagnating\n                    if (not improved_any) or self.rng.random() < 0.3:\n                        # pick two best and combine\n                        best_idx = np.argsort(seeds_F)[:min(3, len(seeds_F))]\n                        parents = [seeds_X[i] for i in best_idx]\n                        # weighted average + direction noise\n                        weights = self.rng.random(len(parents))\n                        weights = weights / weights.sum()\n                        child = np.zeros(self.dim)\n                        for w, p in zip(weights, parents):\n                            child += w * p\n                        # add scaled directional noise\n                        noise = self.rng.normal(scale=0.05 * (ub - lb))\n                        child = np.clip(child + noise, lb, ub)\n                        # evaluate child if budget allows\n                        f_child = eval_f(child)\n                        # if child better than worst seed, replace\n                        worst = np.argmax(seeds_F)\n                        if f_child < seeds_F[worst]:\n                            seeds_X[worst] = child\n                            seeds_F[worst] = f_child\n                            if f_child < self.f_opt:\n                                self.f_opt = f_child\n                                self.x_opt = child.copy()\n                            improved_any = True\n\n                if improved_any:\n                    no_improve_rounds = 0\n                else:\n                    no_improve_rounds += 1\n\n        except StopIteration:\n            # budget exhausted - return best found so far\n            pass\n\n        # final attempt: if budget left, do a few global jittered samples around best\n        try:\n            extra = max(0, B - calls_made)\n            extra_trials = min(10, extra)\n            for _ in range(extra_trials):\n                jitter_scale = 0.02 * (ub - lb)\n                x_try = np.clip((self.x_opt if self.x_opt is not None else 0.5 * (lb + ub))\n                                + self.rng.normal(scale=jitter_scale), lb, ub)\n                f_try = eval_f(x_try)\n                if f_try < self.f_opt:\n                    self.f_opt = f_try\n                    self.x_opt = x_try.copy()\n        except StopIteration:\n            pass\n\n        return self.f_opt, (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))", "configspace": "", "generation": 0, "feedback": "The algorithm MSDirectionalSubspaceSearch scored 0.207 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.21872556510139007}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.28962032293303697}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.2752671961495713}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.03735676732663473}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03691339854227149}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.05164185432991175}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.031196268421255113}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.037639413769299024}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.022325526692390296}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.7399969734062732}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.7580369512484595}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.610048213477765}], "aucs": [0.21872556510139007, 0.28962032293303697, 0.2752671961495713, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.03735676732663473, 0.03691339854227149, 0.05164185432991175, 0.031196268421255113, 0.037639413769299024, 0.022325526692390296, 0.7399969734062732, 0.7580369512484595, 0.610048213477765]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1859.0, "Edges": 1858.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9989241527703066, "Degree Variance": 1.9429789393789894, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.318840579710145, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3290099620903484, "Depth Entropy": 2.199779827762776, "Assortativity": 0.0, "Average Eccentricity": 17.706831629908553, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0005379236148466917, "Average Shortest Path": 10.83371096072926, "mean_complexity": 9.6, "total_complexity": 48.0, "mean_token_count": 318.0, "total_token_count": 1590.0, "mean_parameter_count": 3.2, "total_parameter_count": 16.0}, "archive_direction": "", "archive_feature": ""}
{"id": "301b461d-2f9e-49ea-ae17-7d69bab7e52f", "fitness": 0.24227907497481785, "name": "AdaptiveRotationalGaussianSearch", "description": "Adaptive Rotational Gaussian Search (ARGS) \u2014 a population-free adaptive covariance sampler that rotates and scales Gaussian proposals using recent successful steps, with adaptive step-size and occasional restarts to efficiently explore bounded continuous spaces.", "code": "import numpy as np\n\nclass AdaptiveRotationalGaussianSearch:\n    \"\"\"\n    Adaptive Rotational Gaussian Search (ARGS)\n\n    Key ideas:\n    - Maintain a single center (current best) and sample Gaussian candidates around it.\n    - Adapt the covariance matrix using a lightweight rank-1 update from recent successful steps,\n      effectively rotating the sampling distribution toward productive directions.\n    - Adapt the global step-size (sigma) multiplicatively on success/failure (trust-region-like).\n    - Occasionally sample along principal directions of the accumulated covariance and perform restarts if stagnation occurs.\n    - Enforce bounds by clipping.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_samples=None, rng=None):\n        \"\"\"\n        budget: total number of function evaluations allowed\n        dim: dimensionality of the problem\n        init_samples: optional number of initial random samples (defaults to ~3*dim or small fraction of budget)\n        rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = init_samples\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n        # internal results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds if provided, otherwise use -5..5\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub =  5.0 * np.ones(self.dim)\n\n        # Ensure shape\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # helper: clip into bounds\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # Budget tracking\n        evals = 0\n        remaining = lambda: self.budget - evals\n\n        # initial sampling count\n        if self.init_samples is None:\n            init_samples = min(max(5, 3 * self.dim), max(5, self.budget // 20))\n        else:\n            init_samples = int(self.init_samples)\n            init_samples = min(init_samples, max(1, self.budget // 5))\n\n        # initial random samples (uniform)\n        for _ in range(init_samples):\n            if evals >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # If we didn't get any initial sample (budget maybe small), create one point\n        if self.x_opt is None:\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            self.f_opt = float(f)\n            self.x_opt = x.copy()\n\n        # Initialize strategy parameters\n        # global step-size relative to bounds size (vector magnitude scale)\n        span = ub - lb\n        initial_sigma = 0.2 * np.linalg.norm(span) / max(1.0, np.sqrt(self.dim))\n        sigma = max(1e-6, initial_sigma)\n\n        # covariance (normalized so trace ~= dim), start as identity\n        C = np.eye(self.dim)\n        eps = 1e-8\n\n        # success history for covariance updates\n        success_steps = []\n        max_success_history = min(40, 6 * self.dim + 10)\n\n        # adaptation parameters\n        c_cov = 0.2  # covariance learning rate\n        c_success = 1.2  # sigma multiplier on success\n        c_failure = 0.85  # sigma multiplier on failure\n        stagnation_limit = max(8, 40 // (self.dim // 5 + 1))\n        stagnation_counter = 0\n\n        # main loop: sample in mini-batches\n        while evals < self.budget:\n            rem = self.budget - evals\n            # batch size adjusted by dimension and remaining budget\n            batch = min(rem, max(1, int(min(20, 4 + np.sqrt(self.dim) * 2))))\n            # Occasionally do a targeted principal-direction exploration\n            do_pc_explore = (self.rng.random() < 0.12) and (len(success_steps) >= 2)\n\n            # prepare sqrt of covariance (Cholesky) with jitter\n            # ensure positive definite\n            C_pd = C + eps * np.eye(self.dim)\n            try:\n                B = np.linalg.cholesky(C_pd)\n            except np.linalg.LinAlgError:\n                # fallback: eigen-based sqrt\n                vals, vecs = np.linalg.eigh(C_pd)\n                vals = np.clip(vals, 1e-12, None)\n                B = vecs @ np.diag(np.sqrt(vals))\n            # scale by sigma\n            B = sigma * B\n\n            # generate candidate batch\n            candidates = []\n            for k in range(batch):\n                if do_pc_explore and k == 0:\n                    # principal component exploration: build PC from success steps\n                    S = np.array(success_steps)\n                    if S.size == 0:\n                        z = self.rng.standard_normal(self.dim)\n                    else:\n                        # covariance of recent success steps\n                        M = np.cov(S.T) if S.shape[0] > 1 else np.atleast_2d(S[0]).T @ np.atleast_2d(S[0])\n                        # get top eigenvector\n                        vals, vecs = np.linalg.eigh(M + 1e-12 * np.eye(self.dim))\n                        pc = vecs[:, -1]\n                        # sample longer step along pc and small orthogonal noise\n                        long_step = 2.0 * sigma * pc * self.rng.normal()\n                        z = self.rng.standard_normal(self.dim) * 0.2 + long_step / (sigma + 1e-12)\n                else:\n                    z = self.rng.standard_normal(self.dim)\n                y = self.x_opt + B.dot(z)\n                y = clip(y)\n                candidates.append(y)\n\n            # Evaluate candidates in sequence (stop if budget hits)\n            best_candidate = None\n            best_candidate_f = np.inf\n            for y in candidates:\n                if evals >= self.budget:\n                    break\n                f_y = func(y)\n                evals += 1\n                if f_y < best_candidate_f:\n                    best_candidate_f = float(f_y)\n                    best_candidate = y.copy()\n\n            # If no candidate evaluated (shouldn't happen), break\n            if best_candidate is None:\n                break\n\n            # Decide acceptance\n            if best_candidate_f < self.f_opt - 1e-15:\n                # success: update best, record step\n                step = best_candidate - self.x_opt\n                norm_step = np.linalg.norm(step)\n                if norm_step > 0:\n                    s_norm = step / norm_step\n                    success_steps.append(step)\n                    if len(success_steps) > max_success_history:\n                        success_steps.pop(0)\n\n                    # rank-1 update of covariance using normalized step direction (outer product)\n                    v = (step / (norm_step + 1e-12))\n                    outer = np.outer(v, v)\n                    C = (1.0 - c_cov) * C + c_cov * (outer * (norm_step**2) / ( (np.trace(C)/self.dim) + 1e-12))\n                    # keep covariance trace stable (~dim)\n                    tr = np.trace(C)\n                    if tr > 0:\n                        C = C * (self.dim / tr)\n\n                # move center to new best\n                self.x_opt = best_candidate.copy()\n                self.f_opt = best_candidate_f\n                sigma = max(1e-12, sigma * c_success)\n                stagnation_counter = 0\n            else:\n                # failure to improve\n                sigma = max(1e-12, sigma * c_failure)\n                stagnation_counter += 1\n                # slightly increase isotropic exploration if many failures\n                if stagnation_counter > stagnation_limit:\n                    # random jump (small restart), expand sigma temporarily\n                    jump_scale = 0.6 + 0.6 * self.rng.random()\n                    trial = self.x_opt + jump_scale * sigma * (self.rng.standard_normal(self.dim))\n                    trial = clip(trial)\n                    if evals < self.budget:\n                        f_trial = func(trial)\n                        evals += 1\n                        if f_trial < self.f_opt:\n                            self.x_opt = trial.copy()\n                            self.f_opt = float(f_trial)\n                            sigma *= 1.5\n                            stagnation_counter = 0\n                        else:\n                            # nudge covariance isotropically to escape\n                            C = (1 - 0.05) * C + 0.05 * np.eye(self.dim)\n                    else:\n                        # no budget to try trial\n                        pass\n\n            # small regularization to ensure PD\n            C = 0.999 * C + 0.001 * np.eye(self.dim)\n\n        # Final ensure x_opt is clipped\n        if self.x_opt is not None:\n            self.x_opt = clip(self.x_opt)\n\n        return float(self.f_opt), np.array(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveRotationalGaussianSearch scored 0.242 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9300014323633204}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9203727327187958}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9350445116789752}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.01522756069787956}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.044571986592281565}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.05705349883881061}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.04335527031542952}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.017032796195959432}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0269807989755374}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.10550338524504899}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.1239351922869728}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.4149569587132571}], "aucs": [0.9300014323633204, 0.9203727327187958, 0.9350445116789752, 4.999999999999449e-05, 4.999999999999449e-05, 0.01522756069787956, 0.044571986592281565, 0.05705349883881061, 4.999999999999449e-05, 0.04335527031542952, 0.017032796195959432, 0.0269807989755374, 0.10550338524504899, 0.1239351922869728, 0.4149569587132571]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1493.0, "Edges": 1492.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.998660415271266, "Degree Variance": 1.8807751646554205, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.20262390670554, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3154324989881885, "Depth Entropy": 2.2005538697961966, "Assortativity": 1.4883972501912283e-08, "Average Eccentricity": 17.16075016744809, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0006697923643670462, "Average Shortest Path": 10.115394629809531, "mean_complexity": 11.333333333333334, "total_complexity": 34.0, "mean_token_count": 428.3333333333333, "total_token_count": 1285.0, "mean_parameter_count": 2.6666666666666665, "total_parameter_count": 8.0}, "archive_direction": "", "archive_feature": ""}
{"id": "7f89b7f6-cf89-4f1b-8225-f8daa36aa4da", "fitness": "-inf", "name": "DGES_ALR", "description": "Dense-Graph Edge Sampling with Adaptive Local Radii (DGES-ALR) \u2014 maintain an archive of elite points, form a dense implicit graph by connecting each node to many neighbors, and generate many edge-based interpolation/extrapolation proposals (plus barycentric and occasional heavy-tailed jumps) while adapting local search radii and pruning the archive to keep a diverse, high-edge-density search structure.", "code": "import numpy as np\n\nclass DGES_ALR:\n    \"\"\"\n    Dense-Graph Edge Sampling with Adaptive Local Radii (DGES-ALR).\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional tuning parameters can be passed as keyword arguments.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_pop=None, k_neighbors=12, proposals_per_iter=24,\n                 init_frac=0.12, heavy_prob=0.07, stagnation_restart=None,\n                 max_archive=120):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # initialization/population size\n        if init_pop is None:\n            # scale with dimension and budget but keep reasonable\n            self.init_pop = max(6, min(40, int(2 * dim + np.sqrt(budget) // 2)))\n        else:\n            self.init_pop = int(init_pop)\n\n        # how many neighbors each node connects to when sampling edges\n        self.k_neighbors = int(k_neighbors)\n\n        # how many candidate proposals to evaluate per main iteration\n        self.proposals_per_iter = int(proposals_per_iter)\n\n        # fraction of budget to reserve for initial sampling (not strict)\n        self.init_frac = float(init_frac)\n\n        # probability of generating a heavy-tailed global jump instead of edge sampling\n        self.heavy_prob = float(heavy_prob)\n\n        # stagnation restart threshold: default proportion of budget\n        if stagnation_restart is None:\n            self.stagnation_restart = max(20, int(0.15 * self.budget))\n        else:\n            self.stagnation_restart = int(stagnation_restart)\n\n        # maximum archive size (prune to keep graph manageable)\n        self.max_archive = int(max(10, max_archive))\n\n    def _reflect_clip(self, x, lb, ub):\n        # Reflect when outside bounds, then clip to be safe\n        # reflection: for each coordinate below lb -> lb + (lb - v); above ub -> ub - (v - ub)\n        x = x.copy()\n        below = x < lb\n        if np.any(below):\n            x[below] = lb[below] + (lb[below] - x[below])\n        above = x > ub\n        if np.any(above):\n            x[above] = ub[above] - (x[above] - ub[above])\n        # one more clip to ensure numerical safety\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        # extract bounds from func (assumed to provide func.bounds.lb / ub)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        evals = 0\n        # compute a range scale used for radii defaults\n        range_vec = ub - lb\n        mean_range = float(np.mean(range_vec))\n\n        # Determine initial number of samples (cannot exceed budget)\n        n_init = min(self.init_pop, max(1, int(max(6, budget * self.init_frac))))\n        n_init = min(n_init, budget)\n\n        # archive: maintain arrays for coordinates, objective, local radius, success age\n        xs = []\n        fs = []\n        radii = []\n        ages = []\n\n        # initial sampling: uniform diverse seeding\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            xs.append(x.copy())\n            fs.append(f)\n            radii.append(max(1e-8, 0.25 * mean_range))  # start with moderate radius\n            ages.append(0)\n\n        xs = np.asarray(xs, dtype=float)\n        fs = np.asarray(fs, dtype=float)\n        radii = np.asarray(radii, dtype=float)\n        ages = np.asarray(ages, dtype=int)\n\n        # track best-so-far\n        best_idx = int(np.argmin(fs))\n        f_best = float(fs[best_idx])\n        x_best = xs[best_idx].copy()\n\n        since_improvement = 0\n\n        # main loop: propose candidates until budget exhausted\n        while evals < budget:\n            remaining = budget - evals\n            # number of proposals this cycle\n            proposals = min(remaining, self.proposals_per_iter)\n\n            n_archive = xs.shape[0]\n\n            # If archive too small, fall back to local/global gaussian & heavy jumps\n            if n_archive <= 1:\n                for _ in range(proposals):\n                    if rng.random() < self.heavy_prob:\n                        # heavy-tailed global jump around best\n                        z = rng.standard_cauchy(size=dim)\n                        # scale and normalize heavy jump\n                        z = np.where(np.isfinite(z), z, rng.standard_normal(size=dim))\n                        z = z / (np.linalg.norm(z) + 1e-12)\n                        x_try = x_best + 3.0 * mean_range * z * rng.random()\n                    else:\n                        # gaussian local around best\n                        x_try = x_best + 0.5 * mean_range * rng.normal(size=dim)\n                    x_try = self._reflect_clip(x_try, lb, ub)\n                    f_try = float(func(x_try))\n                    evals += 1\n                    # add to archive\n                    xs = np.vstack([xs, x_try])\n                    fs = np.append(fs, f_try)\n                    radii = np.append(radii, max(1e-8, 0.25 * mean_range))\n                    ages = np.append(ages, 0)\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                        since_improvement = 0\n                    else:\n                        since_improvement += 1\n                    if evals >= budget:\n                        break\n                # prune archive if needed\n                if xs.shape[0] > self.max_archive:\n                    keep = np.argsort(fs)[:self.max_archive]\n                    xs = xs[keep]\n                    fs = fs[keep]\n                    radii = radii[keep]\n                    ages = ages[keep]\n                continue\n\n            # Precompute neighbor lists (k nearest by Euclidean distance)\n            # compute pairwise distances from each node to others\n            # Use efficient broadcasting\n            diff = xs.reshape(n_archive, 1, dim) - xs.reshape(1, n_archive, dim)\n            dists = np.sqrt(np.sum(diff * diff, axis=2))\n            # set self-distance to large to avoid selecting itself\n            np.fill_diagonal(dists, np.inf)\n            k = min(self.k_neighbors, n_archive - 1)\n            neighbors = np.argpartition(dists, kth=k, axis=1)[:, :k]  # unsorted k neighbors\n\n            # We will sample edges from this dense neighborhood structure\n            proposals_made = 0\n            # create a randomized order of source nodes to diversify edges\n            node_order = rng.permutation(n_archive)\n\n            # prepare list of candidate indices (source, neighbor) random picks until proposals met\n            # because edges are many, we'll random-sample pairs on the fly\n            while proposals_made < proposals and evals < budget:\n                i = int(rng.choice(node_order))\n                # neighbor selection biased towards nearer ones by using distances as weights\n                neigh_idx = neighbors[i]\n                # compute selection probabilities inversely proportional to distance (add small eps)\n                neigh_d = dists[i, neigh_idx] + 1e-12\n                inv = 1.0 / neigh_d\n                probs = inv / np.sum(inv)\n                # pick neighbor j\n                j = int(rng.choice(neigh_idx, p=probs))\n\n                # Decide proposal type:\n                r = rng.random()\n                if r < self.heavy_prob:\n                    # heavy-tailed jump anchored at best\n                    z = rng.standard_cauchy(size=dim)\n                    z = np.where(np.isfinite(z), z, rng.standard_normal(size=dim))\n                    # scale by combined local radius and mean_range\n                    scale = (radii[i] + radii[j]) * 1.2 + 0.5 * mean_range\n                    x_try = x_best + scale * (z / (np.linalg.norm(z) + 1e-12)) * rng.random()\n                elif r < 0.4:\n                    # barycentric interpolation from a small clique (triangle) to create internal points\n                    # pick a third node k distinct from i,j\n                    choice_pool = list(range(n_archive))\n                    choice_pool.remove(i)\n                    if j in choice_pool:\n                        choice_pool.remove(j)\n                    if len(choice_pool) == 0:\n                        k_idx = i\n                    else:\n                        k_idx = rng.choice(choice_pool)\n                    w = rng.random(3)\n                    w = w / np.sum(w)\n                    x_try = w[0] * xs[i] + w[1] * xs[j] + w[2] * xs[k_idx]\n                    # add small gaussian perturbation scaled by mean of local radii\n                    x_try = x_try + 0.5 * np.mean([radii[i], radii[j], radii[k_idx]]) * rng.normal(size=dim)\n                else:\n                    # edge interpolation/extrapolation + local gaussian noise\n                    direction = xs[j] - xs[i]\n                    # extrapolate or interpolate: alpha centered >0.5 to bias toward neighbor\n                    alpha = rng.normal(loc=0.6, scale=0.45)\n                    alpha = float(np.clip(alpha, -1.5, 2.5))\n                    local_noise_scale = 0.8 * radii[i]\n                    x_try = xs[i] + alpha * direction + local_noise_scale * rng.normal(size=dim)\n\n                # ensure inside bounds (reflective)\n                x_try = self._reflect_clip(x_try, lb, ub)\n\n                # evaluate\n                f_try = float(func(x_try))\n                evals += 1\n                proposals_made += 1\n\n                # Add to archive adaptively:\n                added = False\n                # if better than best, accept strongly\n                if f_try < f_best:\n                    f_best = f_try\n                    x_best = x_try.copy()\n                    # decrease radii near success to refine local search\n                    radii[i] = max(1e-12, radii[i] * 0.85)\n                    if j != i:\n                        radii[j] = max(1e-12, radii[j] * 0.95)\n                    # push new successful point into archive\n                    xs = np.vstack([xs, x_try])\n                    fs = np.append(fs, f_try)\n                    radii = np.append(radii, max(1e-12, 0.5 * radii[i]))\n                    ages = np.append(ages, 0)\n                    added = True\n                    since_improvement = 0\n                else:\n                    since_improvement += 1\n                    # small chance to insert to keep diversity (if it's not almost duplicate)\n                    # check distance to nearest archive point\n                    dd = np.linalg.norm(xs - x_try, axis=1)\n                    if np.min(dd) > 1e-6 * mean_range and rng.random() < 0.12:\n                        xs = np.vstack([xs, x_try])\n                        fs = np.append(fs, f_try)\n                        # set radius proportional to distance to nearest neighbor (diverse point)\n                        radii = np.append(radii, max(1e-12, 0.5 * np.min(dd)))\n                        ages = np.append(ages, 0)\n                        added = True\n\n                # if not added, still update local ages and maybe adjust radii modestly\n                if not added:\n                    # age the source & neighbor (they were \"tested\")\n                    ages[i] += 1\n                    ages[j] += 1\n                    # slowly increase radii for nodes not producing improvements to encourage escape\n                    radii[i] = min(4.0 * mean_range, radii[i] * (1.0 + 0.002 + 0.002 * (ages[i] // 10)))\n                    radii[j] = min(4.0 * mean_range, radii[j] * (1.0 + 0.001 + 0.001 * (ages[j] // 10)))\n\n                # prune archive if it grows too big\n                if xs.shape[0] > self.max_archive:\n                    # keep best fraction plus some diverse representatives\n                    n_keep_best = max(6, int(0.5 * self.max_archive))\n                    best_idx_order = np.argsort(fs)\n                    keep_best = best_idx_order[:n_keep_best]\n\n                    # fill remaining slots by max-min diversity selection from remaining points\n                    remaining_idx = best_idx_order[n_keep_best:]\n                    if remaining_idx.size > 0:\n                        # compute distances among remaining to best set and pick those farthest from best set\n                        rem_xs = xs[remaining_idx]\n                        best_xs = xs[keep_best]\n                        # min distance of each remaining to the best set\n                        dmin = np.min(np.linalg.norm(rem_xs[:, None, :] - best_xs[None, :, :], axis=2), axis=1)\n                        # choose top (max) by distance\n                        n_fill = self.max_archive - n_keep_best\n                        pick_indices = np.argsort(-dmin)[:n_fill]\n                        keep_diverse = remaining_idx[pick_indices]\n                        keep_idx = np.concatenate([keep_best, keep_diverse])\n                    else:\n                        keep_idx = keep_best\n                    xs = xs[keep_idx]\n                    fs = fs[keep_idx]\n                    radii = radii[keep_idx]\n                    ages = ages[keep_idx]\n\n                # update best_idx variable if needed\n                best_idx = int(np.argmin(fs))\n                x_best = xs[best_idx].copy()\n                f_best = float(fs[best_idx])\n\n                # minor adaptive dynamics: if we are improving frequently, slightly shrink radii to exploit\n                # else gradually expand some radii to explore\n                if since_improvement == 0:\n                    # on immediate success, shrink global small fraction\n                    radii = np.maximum(1e-12, radii * 0.995)\n                else:\n                    # small expansion proportional to stagnation\n                    if since_improvement > 20:\n                        radii = np.minimum(4.0 * mean_range, radii * (1.0 + 0.002))\n\n                # break if budget is exhausted\n                if evals >= budget:\n                    break\n\n            # stagnation handling: larger perturbation if no improvement for a while\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # perform a burst of diverse global probes\n                bursts = min(12, budget - evals)\n                for _ in range(bursts):\n                    if rng.random() < 0.6:\n                        # sample uniformly across domain\n                        x_try = rng.uniform(lb, ub)\n                    else:\n                        # perturb around a random archive point heavily\n                        idx = rng.integers(0, xs.shape[0])\n                        x_try = xs[idx] + 1.5 * mean_range * rng.normal(size=dim)\n                        x_try = self._reflect_clip(x_try, lb, ub)\n                    f_try = float(func(x_try))\n                    evals += 1\n                    # add if better or diverse\n                    dd = np.linalg.norm(xs - x_try, axis=1)\n                    if f_try < f_best or np.min(dd) > 0.05 * mean_range:\n                        xs = np.vstack([xs, x_try])\n                        fs = np.append(fs, f_try)\n                        radii = np.append(radii, 0.5 * mean_range)\n                        ages = np.append(ages, 0)\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                        since_improvement = 0\n                        break\n                    if evals >= budget:\n                        break\n                # if still stagnating, amplify neighborhood radii to encourage escapes\n                if since_improvement >= self.stagnation_restart:\n                    radii = np.minimum(6.0 * mean_range, radii * 1.8)\n                    # also increase probability of heavy jumps slightly for future iterations\n                    self.heavy_prob = min(0.5, self.heavy_prob * 1.5)\n\n        # final return best value and corresponding point\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 1, "feedback": "In the code, line 211, in __call__, the following error occurred:\nIndexError: index 120 is out of bounds for axis 0 with size 120\nOn line: direction = xs[j] - xs[i]", "error": "In the code, line 211, in __call__, the following error occurred:\nIndexError: index 120 is out of bounds for axis 0 with size 120\nOn line: direction = xs[j] - xs[i]", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2652.0, "Edges": 2651.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9992458521870287, "Degree Variance": 2.12594211602729, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.710132890365449, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3250522798879358, "Depth Entropy": 2.1973558875770487, "Assortativity": 1.451914207911595e-08, "Average Eccentricity": 19.469834087481146, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.0003770739064856712, "Average Shortest Path": 10.670177962953165, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 767.6666666666666, "total_token_count": 2303.0, "mean_parameter_count": 5.666666666666667, "total_parameter_count": 17.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "e2057573-e866-4dbd-afa2-84a34af4dde0", "fitness": "-inf", "name": "EdgeOrientedDirectionalAnnealing", "description": "Edge-Oriented Directional Annealing (EODA) \u2014 focus search on box edges/corners and principal directions using adaptive 1-D directional probes, elite recombination and boundary probes to increase \"edge density\" exploration.", "code": "import numpy as np\n\nclass EdgeOrientedDirectionalAnnealing:\n    \"\"\"\n    Edge-Oriented Directional Annealing (EODA)\n\n    Main idea:\n      - Seed an elite set with a stratified initial sample.\n      - Maintain a pool of direction templates emphasizing axis directions,\n        corner/diagonal directions and a few random orthonormal directions.\n      - Perform adaptive 1-D directional probes (small budget line-search-like sampling)\n        from elite centers along chosen directions; expand step-size on success,\n        shrink on failure.\n      - Explicitly probe boundary (edge/corner) points by projecting along directions\n        to the box limits to increase exploration near edges.\n      - Periodically recombine elites (convex/differential-style) and use opposition sampling\n        as diversification. All calls strictly counted and limited by budget.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, rng=None,\n                 init_samples=None, elite_size=6,\n                 max_dir_random=5, init_radius=None,\n                 min_radius=1e-6, max_radius=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          rng: optional numpy.random.Generator\n          init_samples: number of initial stratified samples (defaults adaptively)\n          elite_size: number of elite points to keep\n          max_dir_random: number of random orthonormal directions to include in pool\n          init_radius: starting step length (if None uses half-range)\n          min_radius: minimum step before reset\n          max_radius: maximum step (if None uses range)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.elite_size = int(elite_size)\n        self.max_dir_random = int(max_dir_random)\n        self.min_radius = float(min_radius)\n        self.init_samples = init_samples if init_samples is not None else max(8, min(40, self.budget // 10))\n        self.init_radius = init_radius\n        self.max_radius = max_radius\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def _latin_hypercube(self, n, lb, ub):\n        # simple latin hypercube sampling in [lb,ub]\n        d = self.dim\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        u = rng.random((n, d))\n        pts = np.empty((n, d), dtype=float)\n        for j in range(d):\n            perm = rng.permutation(n)\n            pts[:, j] = (cut[:-1] + (perm + u[:, j]) / n)[:n]\n        # map to bounds\n        return lb + pts * (ub - lb)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        range_vec = ub - lb\n        if self.init_radius is None:\n            radius = 0.5 * np.min(range_vec)  # start at half of smallest side\n        else:\n            radius = float(self.init_radius)\n        max_radius = float(self.max_radius) if self.max_radius is not None else float(np.min(range_vec))\n        radius = float(np.clip(radius, self.min_radius, max_radius))\n\n        evals = 0\n        cache = {}  # map tuple(x) -> f to avoid duplicate calls\n\n        def eval_x(x):\n            nonlocal evals\n            key = tuple(float(np.round(xi, 12)) for xi in x)  # rounding for stable hashing\n            if key in cache:\n                return cache[key]\n            if evals >= self.budget:\n                # budget exhausted: return inf sentinel\n                return np.inf\n            f = float(func(x))\n            cache[key] = f\n            evals += 1\n            return f\n\n        # INITIALIZATION: stratified sampling (LHS) + opposition of best\n        n0 = max(2, min(self.init_samples, self.budget - 1))\n        xs = self._latin_hypercube(n0, lb, ub)\n        fs = []\n        for i in range(xs.shape[0]):\n            if evals >= self.budget:\n                break\n            fs.append(eval_x(xs[i]))\n        if evals < self.budget:\n            # ensure we have at least one evaluation (should be true)\n            pass\n\n        # Build elite list (tuples f,x)\n        entries = list(zip(fs, xs[:len(fs)]))\n        if len(entries) == 0:\n            # fallback random initialization\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            f0 = eval_x(x0)\n            entries = [(f0, x0)]\n        entries.sort(key=lambda t: t[0])\n        # opposition check of best\n        best_f, best_x = entries[0]\n        if evals < self.budget:\n            x_opp = lb + ub - best_x\n            x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n            f_opp = eval_x(x_opp)\n            if f_opp < best_f:\n                best_f, best_x = f_opp, x_opp\n        elite = [(best_f, best_x.copy())]\n        # add a few more from entries\n        for f, x in entries[1:self.elite_size]:\n            elite.append((f, x.copy()))\n        # if not enough, fill with random samples until either elite_size or budget\n        while len(elite) < self.elite_size and evals < self.budget:\n            x_r = lb + self.rng.random(self.dim) * (ub - lb)\n            f_r = eval_x(x_r)\n            elite.append((f_r, x_r.copy()))\n        elite.sort(key=lambda t: t[0])\n        # keep only top elite_size\n        elite = elite[:self.elite_size]\n\n        f_best, x_best = elite[0]\n\n        # build direction pool: axis \u00b1, some corner/diagonal, some random orthonormal\n        def build_directions():\n            dirs = []\n            # axis \u00b1 unit vectors\n            for i in range(self.dim):\n                e = np.zeros(self.dim, float)\n                e[i] = 1.0\n                dirs.append(e.copy())\n                dirs.append(-e.copy())\n            # some diagonal/corner directions (random \u00b11)\n            num_corners = min(2 * self.dim, max(4, self.dim))\n            for _ in range(num_corners):\n                v = self.rng.choice([-1.0, 1.0], size=self.dim)\n                v = v / np.linalg.norm(v)\n                dirs.append(v)\n            # random orthonormal directions via QR\n            R = self.rng.normal(size=(self.dim, max(2, self.max_dir_random)))\n            try:\n                Q, _ = np.linalg.qr(R)\n                for j in range(min(Q.shape[1], self.max_dir_random)):\n                    dirs.append(Q[:, j].copy())\n            except np.linalg.LinAlgError:\n                # fallback to random normalized vectors\n                for _ in range(self.max_dir_random):\n                    v = self.rng.normal(size=self.dim)\n                    v /= max(1e-12, np.linalg.norm(v))\n                    dirs.append(v)\n            # normalize and unique-ish\n            normed = []\n            for d in dirs:\n                nrm = np.linalg.norm(d)\n                if nrm > 0:\n                    normed.append(d / nrm)\n            return normed\n\n        directions = build_directions()\n\n        # per-elite radius state\n        elite_radii = [radius for _ in range(len(elite))]\n\n        # main loop: alternate directional probes and recombination until budget exhausted\n        while evals < self.budget:\n            # choose which elite center to work from (probabilistic: prefer better elites)\n            probs = np.array([1.0 / (1.0 + 5.0 * i) for i in range(len(elite))], dtype=float)\n            probs /= probs.sum()\n            idx = self.rng.choice(len(elite), p=probs)\n            f_center, x_center = elite[idx]\n            r_center = elite_radii[idx]\n\n            # select direction: with some probability pick boundary-biased (diagonal/corner), else axis or random\n            if self.rng.random() < 0.25:\n                # pick a diagonal/corner-like (last chunk)\n                d = directions[self.dim * 2 + (self.rng.integers(0, max(1, self.dim)))]\n            elif self.rng.random() < 0.4:\n                # axis direction\n                d = directions[self.rng.integers(0, min(len(directions), 2 * self.dim))]\n            else:\n                d = directions[self.rng.integers(0, len(directions))]\n            # ensure d normalized\n            d = d / max(1e-12, np.linalg.norm(d))\n\n            # compute max step before hitting bounds (both directions)\n            # find t such that x_center + t*d hits a bound first\n            t_plus = np.inf\n            t_minus = -np.inf\n            # positive direction bounds\n            mask_pos = d > 1e-12\n            if np.any(mask_pos):\n                t_vals = (ub[mask_pos] - x_center[mask_pos]) / d[mask_pos]\n                if t_vals.size:\n                    t_plus = np.min(t_vals)\n            mask_neg = d < -1e-12\n            if np.any(mask_neg):\n                t_vals = (lb[mask_neg] - x_center[mask_neg]) / d[mask_neg]\n                if t_vals.size:\n                    t_plus = min(t_plus, np.min(t_vals))\n            # negative bound\n            if np.any(mask_pos):\n                t_vals = (lb[mask_pos] - x_center[mask_pos]) / d[mask_pos]\n                if t_vals.size:\n                    t_minus = max(t_minus, np.max(t_vals))\n            if np.any(mask_neg):\n                t_vals = (ub[mask_neg] - x_center[mask_neg]) / d[mask_neg]\n                if t_vals.size:\n                    t_minus = max(t_minus, np.max(t_vals))\n\n            # ensure finite bounds\n            if not np.isfinite(t_plus):\n                t_plus = max_radius\n            if not np.isfinite(t_minus):\n                t_minus = -max_radius\n\n            # choose sampling t-points relative to current radius and bounds\n            local_r = float(np.clip(r_center, self.min_radius, max_radius))\n            # candidate t-values: centered around 0, include extremes hitting bound, and a couple inner probes\n            t_candidates = []\n            scales = [ -1.0, -0.5, -0.15, 0.15, 0.5, 1.0 ]\n            for s in scales:\n                t = s * local_r\n                # clip to physical allowable interval [t_minus, t_plus]\n                t_clipped = float(np.clip(t, t_minus, t_plus))\n                if abs(t_clipped) > 1e-15:\n                    t_candidates.append(t_clipped)\n            # include extreme bound points explicitly to raise edge density\n            t_candidates.append(t_plus)\n            t_candidates.append(t_minus)\n            # unique and sort by magnitude (probe close points first)\n            t_candidates = list(dict.fromkeys([float(np.round(t, 12)) for t in t_candidates]))\n            t_candidates = sorted(t_candidates, key=lambda tt: abs(tt))\n\n            improved = False\n            # small local best to compare within this probe\n            local_best_f = f_center\n            local_best_x = x_center.copy()\n\n            # limit how many point probes we take along a direction (control budget)\n            max_probes = min(6, max(1, int(6 * (1 + local_r / max_radius))))\n            probes = t_candidates[:max_probes]\n\n            for t in probes:\n                if evals >= self.budget:\n                    break\n                if abs(t) < 1e-14:\n                    continue\n                x_probe = x_center + t * d\n                # clip to bounds\n                x_probe = np.minimum(np.maximum(x_probe, lb), ub)\n                f_probe = eval_x(x_probe)\n                if f_probe < local_best_f:\n                    local_best_f = f_probe\n                    local_best_x = x_probe.copy()\n                    improved = True\n                    # early accept and break to allow step expansion\n                    break\n\n            if improved:\n                # promote to elite and expand radius\n                # update selected elite entry\n                elite[idx] = (local_best_f, local_best_x.copy())\n                elite_radii[idx] = min(max_radius, r_center * 1.35 + 1e-12)\n                # if new global best, update and maybe perform a short greedy push toward boundary\n                if local_best_f < f_best:\n                    f_best, x_best = local_best_f, local_best_x.copy()\n                    # greedy probe toward nearest boundary corner\n                    if evals < self.budget:\n                        # direction to nearest corner (choose corner that reduces value measured by moving toward extremes)\n                        corner = np.where(x_best - (lb + ub)/2.0 >= 0, ub, lb)\n                        dir_to_corner = corner - x_best\n                        if np.linalg.norm(dir_to_corner) > 1e-12:\n                            dir_to_corner /= np.linalg.norm(dir_to_corner)\n                            t_try = 0.4 * np.min(range_vec)\n                            x_try = np.minimum(np.maximum(x_best + t_try * dir_to_corner, lb), ub)\n                            f_try = eval_x(x_try)\n                            if f_try < f_best:\n                                f_best, x_best = f_try, x_try.copy()\n                                # encourage larger radius\n                                elite_radii[idx] = min(max_radius, elite_radii[idx] * 1.5)\n                # maintain elite ordering\n                elite.sort(key=lambda t: t[0])\n            else:\n                # shrink radius for that elite\n                elite_radii[idx] = max(self.min_radius, r_center * 0.6)\n\n                # small chance to probe opposite/mirror or extreme corner from center to escape local flatness\n                if self.rng.random() < 0.18 and evals < self.budget:\n                    x_mirror = lb + ub - x_center\n                    x_mirror = np.minimum(np.maximum(x_mirror, lb), ub)\n                    f_mirror = eval_x(x_mirror)\n                    if f_mirror < f_center:\n                        elite[idx] = (f_mirror, x_mirror.copy())\n                        elite_radii[idx] = min(max_radius, r_center * 1.2)\n                        if f_mirror < f_best:\n                            f_best, x_best = f_mirror, x_mirror.copy()\n                        continue\n\n            # Periodically attempt recombination / differential moves between elites\n            if evals < self.budget and self.rng.random() < 0.22 and len(elite) >= 2:\n                # pick two distinct elites\n                i1 = self.rng.integers(0, len(elite))\n                i2 = self.rng.integers(0, len(elite))\n                tries = 0\n                while i2 == i1 and tries < 8:\n                    i2 = self.rng.integers(0, len(elite))\n                    tries += 1\n                f1, x1 = elite[i1]\n                f2, x2 = elite[i2]\n                # produce differential/convex hybrid\n                F = self.rng.uniform(0.6, 1.2)\n                mix = self.rng.uniform(0.0, 1.0)\n                x_new = x1 + F * (x1 - x2) * mix\n                # nudging toward boundary with some probability to increase edge visits\n                if self.rng.random() < 0.4:\n                    corner_mask = self.rng.choice([-1.0, 1.0], size=self.dim)\n                    corner_point = np.where(corner_mask > 0, ub, lb)\n                    x_new = 0.6 * x_new + 0.4 * corner_point\n                x_new = np.minimum(np.maximum(x_new, lb), ub)\n                f_new = eval_x(x_new)\n                if f_new < f_best:\n                    f_best, x_best = f_new, x_new.copy()\n                # insert into elites if good\n                if f_new < elite[-1][0]:\n                    elite.append((f_new, x_new.copy()))\n                    elite.sort(key=lambda t: t[0])\n                    elite = elite[:self.elite_size]\n                    # sync radii length\n                    while len(elite_radii) < len(elite):\n                        elite_radii.append(radius)\n                    elite_radii = elite_radii[:len(elite)]\n\n            # If budget is low, do some targeted exploitation around global best\n            if evals < self.budget and (self.budget - evals) <= max(5, self.dim // 2):\n                # tight gaussian sampling around best\n                tries = min(self.budget - evals, 6)\n                for _ in range(tries):\n                    sigma = 0.5 * max(self.min_radius, np.min(range_vec) * 0.02)\n                    xg = x_best + self.rng.normal(scale=sigma, size=self.dim)\n                    xg = np.minimum(np.maximum(xg, lb), ub)\n                    fg = eval_x(xg)\n                    if fg < f_best:\n                        f_best, x_best = fg, xg.copy()\n\n            # occasional full re-build of directions to avoid stagnation\n            if self.rng.random() < 0.05:\n                directions = build_directions()\n\n            # maintain elite list size consistent\n            elite.sort(key=lambda t: t[0])\n            elite = elite[:self.elite_size]\n            # make sure radii aligns with elites\n            elite_radii = (elite_radii[:len(elite)] +\n                           [radius] * max(0, len(elite) - len(elite_radii)))\n            # update global best\n            if elite and elite[0][0] < f_best:\n                f_best, x_best = elite[0][0], elite[0][1].copy()\n\n        # end main loop\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3285.0, "Edges": 3284.0, "Max Degree": 41.0, "Min Degree": 1.0, "Mean Degree": 1.9993911719939117, "Degree Variance": 2.169253815021001, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.09419795221843, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3227408779232732, "Depth Entropy": 2.07119260759015, "Assortativity": 7.890648057400595e-09, "Average Eccentricity": 17.60304414003044, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00030441400304414006, "Average Shortest Path": 10.411725315491188, "mean_complexity": 13.833333333333334, "total_complexity": 83.0, "mean_token_count": 476.8333333333333, "total_token_count": 2861.0, "mean_parameter_count": 3.1666666666666665, "total_parameter_count": 19.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "c8a93a65-df7b-4857-861c-4fb3773a5e6e", "fitness": 0.22657911007941955, "name": "AdaptiveRotationalGaussianSearchV2", "description": "Adaptive Rotational Gaussian Search with Elite Edge Recombination \u2014 mixes rotated Gaussian sampling, dense edge-based recombination among a small elite archive, principled covariance updates from both successful steps and elite edges, and occasional L\u00e9vy jumps with reflection-bound handling to increase connectivity (edges) and escape local minima.", "code": "import numpy as np\n\nclass AdaptiveRotationalGaussianSearchV2:\n    \"\"\"\n    Adaptive Rotational Gaussian Search V2 (ARGSv2)\n\n    Improvements over ARGS:\n    - Maintain a small elite archive and explicitly sample \"edge\" recombinations between elites\n      (increasing the effective graph/edge density of explored solutions).\n    - Covariance is adapted using both successful steps and weighted elite-edge directions,\n      producing richer rotational updates.\n    - Sigma adapts using a success-rate controller (like CSA), and also reacts to stagnation.\n    - Mix of proposal types per batch: rotated Gaussian, principal-direction probe, edge-recombine,\n      and occasional L\u00e9vy (Cauchy) long jumps. Reflection boundary handling keeps trajectories informative.\n    - Replace/climb strategy: best-so-far is always tracked, but elites can be updated by promising\n      candidates even if they don't beat the global best \u2014 this encourages denser connections.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng=None, init_samples=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_samples = init_samples\n\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # Determine bounds if provided by func, else default\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub =  5.0 * np.ones(self.dim)\n\n        # normalize shapes\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # reflection-based boundary projection to preserve directional structure\n        def reflect(x):\n            # reflect multiple times if necessary\n            x = np.array(x, dtype=float, copy=True)\n            for i in range(self.dim):\n                lo, hi = lb[i], ub[i]\n                if x[i] < lo or x[i] > hi:\n                    width = hi - lo\n                    if width <= 0:\n                        x[i] = lo\n                    else:\n                        # reflect into [lo,hi] using modular reflection\n                        v = (x[i] - lo) / width\n                        # map real line -> [0,1] with reflection\n                        v = abs((v + 1) % 2 - 1)\n                        x[i] = lo + v * width\n            return x\n\n        evals = 0\n        remaining = lambda: self.budget - evals\n\n        # initial sampling: mix of uniform points (to seed elites) and a center point\n        if self.init_samples is None:\n            init_samples = min(max(6, 4 * self.dim), max(6, self.budget // 10))\n        else:\n            init_samples = int(self.init_samples)\n            init_samples = min(init_samples, max(1, self.budget // 5))\n\n        # Elite archive: keep small set of best distinct points\n        max_elites = max(3, min(8, self.dim + 1))\n        elites = []   # list of (x, f)\n        def add_elite(x, f):\n            # maintain sorted elites by f (ascending)\n            nonlocal elites\n            x = np.array(x, dtype=float)\n            pair = (x.copy(), float(f))\n            elites.append(pair)\n            elites.sort(key=lambda p: p[1])\n            # prune to unique-ish and limit size\n            pruned = []\n            for xi, fi in elites:\n                too_close = False\n                for xj, fj in pruned:\n                    if np.linalg.norm(xi - xj) < 1e-8 or np.linalg.norm((xi - xj) / (ub - lb + 1e-12)) < 1e-6:\n                        too_close = True\n                        break\n                if not too_close:\n                    pruned.append((xi, fi))\n                if len(pruned) >= max_elites:\n                    break\n            elites = pruned\n\n        # seed with some uniform samples\n        for _ in range(init_samples):\n            if evals >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            add_elite(x, f)\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # if nothing, at least one\n        if self.x_opt is None:\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            add_elite(x, f)\n            self.f_opt = float(f)\n            self.x_opt = x.copy()\n\n        # covariance / step-size initialization\n        span = ub - lb\n        initial_sigma = 0.25 * np.linalg.norm(span) / max(1.0, np.sqrt(self.dim))\n        sigma = max(1e-8, initial_sigma)\n        C = np.eye(self.dim)  # covariance-like matrix (normalized trace ~ dim)\n        eps = 1e-10\n\n        # adaptation controls\n        success_window = []\n        window_size = max(30, 8 * self.dim)\n        target_success_rate = 0.2\n        c_cov = 0.25\n        edge_cov_weight = 0.35  # how much edges contribute in covariance update\n        stagnation = 0\n        stagnation_restart = max(8, 30 // (1 + self.dim // 5))\n\n        # Limits\n        sigma_min, sigma_max = 1e-12, 5.0 * np.linalg.norm(span)  # avoid runaway\n        # batch size heuristic\n        def batch_for_dim_and_budget(rem):\n            return int(min(rem, max(1, min(30, 6 + int(np.sqrt(self.dim) * 3)))))\n\n        # helper: compute principal components of recent success steps and elites\n        def top_eigenvectors(matrix, k=2):\n            # symmetric matrix expected\n            vals, vecs = np.linalg.eigh(matrix)\n            idx = np.argsort(vals)[::-1]\n            vecs = vecs[:, idx]\n            vals = vals[idx]\n            k = min(k, vecs.shape[1])\n            return vals[:k], vecs[:, :k]\n\n        # main loop\n        while evals < self.budget:\n            rem = self.budget - evals\n            batch = batch_for_dim_and_budget(rem)\n\n            # compute cholesky-like factor for sampling\n            C_pd = C + (eps + 1e-12 * np.eye(self.dim))\n            try:\n                L = np.linalg.cholesky(C_pd)\n            except np.linalg.LinAlgError:\n                vals, vecs = np.linalg.eigh(C_pd)\n                vals = np.clip(vals, 1e-12, None)\n                L = vecs @ np.diag(np.sqrt(vals))\n\n            # scale by sigma\n            B = sigma * L\n\n            candidates = []\n            cand_sources = []  # tag type for how generated\n            # generate a mix of proposals to increase connectivity\n            for k in range(batch):\n                r = self.rng.random()\n                # type distribution:\n                # 0-0.45 : rotated Gaussian about global best\n                # 0.45-0.7: edge recombination between elites\n                # 0.7-0.85: principal-direction probe\n                # 0.85-0.95: isotropic small gaussian around random elite\n                # 0.95-1.0: L\u00e9vy/Cauchy jump\n                if r < 0.45 or len(elites) < 2:\n                    # rotated gaussian around current best\n                    z = self.rng.standard_normal(self.dim)\n                    y = self.x_opt + B.dot(z)\n                    src = \"gauss\"\n                elif r < 0.7 and len(elites) >= 2:\n                    # Edge recombination: choose two distinct elites and sample along the connecting edge\n                    i, j = self.rng.integers(0, len(elites), size=2)\n                    # ensure distinct\n                    if i == j:\n                        j = (j + 1) % len(elites)\n                    xi, fi = elites[i]\n                    xj, fj = elites[j]\n                    alpha = self.rng.random()\n                    # biased toward better elite\n                    w = 0.7\n                    if fi < fj:\n                        alpha = alpha * w\n                    else:\n                        alpha = 1 - alpha * w\n                    base = alpha * xi + (1 - alpha) * xj\n                    # add anisotropic noise preferentially along edge direction\n                    edge_dir = xi - xj\n                    if np.linalg.norm(edge_dir) > 1e-12:\n                        edge_dir = edge_dir / (np.linalg.norm(edge_dir))\n                        # sample scalar along edge, plus small orthogonal gaussian\n                        edge_scale = (0.7 + 1.2 * self.rng.random()) * sigma\n                        orth = self.rng.standard_normal(self.dim) * (0.15 * sigma)\n                        y = base + edge_dir * (self.rng.normal() * edge_scale) + orth\n                    else:\n                        y = base + B.dot(self.rng.standard_normal(self.dim))\n                    src = \"edge\"\n                elif r < 0.85:\n                    # principal direction probe: use top eigenvector of C\n                    vals, vecs = top_eigenvectors(C_pd, k=min(3, self.dim))\n                    if vecs.shape[1] == 0:\n                        z = self.rng.standard_normal(self.dim)\n                        y = self.x_opt + B.dot(z)\n                    else:\n                        v = vecs[:, 0]\n                        step_len = sigma * (1.0 + self.rng.random() * 2.0)\n                        sign = 1 if self.rng.random() < 0.5 else -1\n                        y = self.x_opt + sign * step_len * v + 0.1 * B.dot(self.rng.standard_normal(self.dim))\n                    src = \"pc\"\n                elif r < 0.95:\n                    # small gaussian around a random elite to densify local exploration\n                    idx = self.rng.integers(0, len(elites))\n                    x_e, f_e = elites[idx]\n                    y = x_e + 0.3 * sigma * self.rng.standard_normal(self.dim)\n                    src = \"local_elite\"\n                else:\n                    # L\u00e9vy/Cauchy long jump (heavy tail) from best\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    # scale heavy tail by sigma and span\n                    jump = cauch * (0.8 + 1.5 * self.rng.random()) * sigma\n                    y = self.x_opt + jump\n                    src = \"levy\"\n\n                # enforce bounds by reflection\n                y = reflect(y)\n                candidates.append(y)\n                cand_sources.append(src)\n\n            # evaluate candidates (sequentially) but do not exceed budget\n            best_cand = None\n            best_cand_f = np.inf\n            best_idx = None\n            cand_results = []\n            for idx, y in enumerate(candidates):\n                if evals >= self.budget:\n                    break\n                f_y = func(y)\n                evals += 1\n                cand_results.append((y.copy(), float(f_y), cand_sources[idx]))\n                if f_y < best_cand_f:\n                    best_cand_f = float(f_y)\n                    best_cand = y.copy()\n                    best_idx = idx\n\n            # No candidates evaluated\n            if best_cand is None:\n                break\n\n            # Acceptance and archive updates:\n            improved = False\n            if best_cand_f < self.f_opt - 1e-15:\n                # strict improvement to global best\n                step = best_cand - self.x_opt\n                norm_step = np.linalg.norm(step)\n                # update covariance with rank-1 and elite-edge contributions\n                if norm_step > 0:\n                    v = step / (norm_step + 1e-12)\n                    rank_outer = np.outer(v, v) * (norm_step**2)\n                else:\n                    rank_outer = np.zeros((self.dim, self.dim))\n\n                # gather edge outer products from top few elites to increase edge density\n                edge_outer = np.zeros((self.dim, self.dim))\n                if len(elites) >= 2:\n                    # accumulate pairwise differences (weighted by pair quality)\n                    pairs = []\n                    for i in range(len(elites)):\n                        for j in range(i+1, len(elites)):\n                            xi, fi = elites[i]\n                            xj, fj = elites[j]\n                            d = xi - xj\n                            nd = np.linalg.norm(d)\n                            if nd > 1e-12:\n                                weight = 1.0 / (1.0 + fi + fj - 2 * self.f_opt)  # encourages edges near best\n                                pairs.append((d / nd, nd, max(0.01, weight)))\n                    # pick a subset to limit cost\n                    self.rng.shuffle(pairs)\n                    for k, (dirv, nd, w) in enumerate(pairs[:min(6, len(pairs))]):\n                        edge_outer += w * np.outer(dirv, dirv) * (nd**2)\n                    # normalize edge_outer\n                    if np.sum(edge_outer) != 0:\n                        edge_outer = edge_outer / (np.trace(edge_outer) / (self.dim + 1e-12))\n\n                # combine updates\n                C = (1.0 - c_cov) * C + c_cov * (0.7 * rank_outer + edge_cov_weight * edge_outer + 0.3 * np.eye(self.dim))\n                # stabilize trace\n                tr = np.trace(C)\n                if tr > 0:\n                    C = C * (self.dim / tr)\n\n                # update best\n                self.x_opt = best_cand.copy()\n                self.f_opt = best_cand_f\n                add_elite(self.x_opt, self.f_opt)\n                improved = True\n                # success window\n                success_window.append(1)\n                if len(success_window) > window_size:\n                    success_window.pop(0)\n                # increase sigma a bit on success\n                sigma = min(sigma_max, sigma * (1.2 + 0.2 * self.rng.random()))\n                stagnation = 0\n            else:\n                # no global improvement\n                success_window.append(0)\n                if len(success_window) > window_size:\n                    success_window.pop(0)\n                stagnation += 1\n                # though candidate might still be good for elites: update archive if better than worst elite\n                if len(cand_results) > 0:\n                    # find best candidate overall among evaluated candidates\n                    local_best = min(cand_results, key=lambda p: p[1])\n                    cand_x, cand_f, src = local_best\n                    # if it's better than worst elite, replace worst elite (grow edge density)\n                    if len(elites) < max_elites or cand_f < elites[-1][1] - 1e-12:\n                        add_elite(cand_x, cand_f)\n                        # nudge covariance slightly toward this candidate displacement from a random elite\n                        if len(elites) >= 2:\n                            # choose the elite it is replacing or closest\n                            # compute difference to current best elite\n                            d = cand_x - elites[0][0]\n                            nd = np.linalg.norm(d)\n                            if nd > 1e-12:\n                                v = d / nd\n                                C = (1.0 - 0.03) * C + 0.03 * np.outer(v, v) * (nd**2)\n                                tr = np.trace(C)\n                                if tr > 0:\n                                    C = C * (self.dim / tr)\n\n                # adapt sigma down after failure\n                sigma = max(sigma_min, sigma * (0.85 - 0.05 * (self.rng.random())))\n                # on stagnation trigger a directional restart/jump\n                if stagnation >= stagnation_restart:\n                    # construct a multi-edge informed jump: linear comb of elites plus noise\n                    if len(elites) >= 2:\n                        # choose several elites and form barycentric perturbation\n                        ksel = min(len(elites), 3 + self.rng.integers(0, 3))\n                        idxs = self.rng.choice(len(elites), size=ksel, replace=False)\n                        weights = self.rng.random(ksel)\n                        weights = weights / (np.sum(weights) + 1e-12)\n                        base = sum(weights[i] * elites[idxs[i]][0] for i in range(ksel))\n                        jump = base + (1.0 + self.rng.random()) * sigma * self.rng.standard_normal(self.dim)\n                    else:\n                        jump = self.x_opt + (1.0 + self.rng.random()) * sigma * self.rng.standard_normal(self.dim)\n                    jump = reflect(jump)\n                    if evals < self.budget:\n                        f_jump = func(jump)\n                        evals += 1\n                        if f_jump < self.f_opt:\n                            self.x_opt = jump.copy()\n                            self.f_opt = float(f_jump)\n                            add_elite(self.x_opt, self.f_opt)\n                            sigma = min(sigma_max, sigma * 1.5)\n                        else:\n                            # diversify covariance isotropically\n                            C = 0.97 * C + 0.03 * np.eye(self.dim)\n                    stagnation = 0\n\n            # success-rate based sigma adaptation (control)\n            if len(success_window) >= max(5, window_size // 6):\n                sr = sum(success_window) / len(success_window)\n                if sr > target_success_rate:\n                    sigma = min(sigma_max, sigma * (1.08 + 0.06 * self.rng.random()))\n                else:\n                    sigma = max(sigma_min, sigma * (0.92 - 0.04 * self.rng.random()))\n\n            # small regularization on covariance\n            C = 0.997 * C + 0.003 * np.eye(self.dim)\n            # maintain trace\n            tr = np.trace(C)\n            if tr > 0:\n                C = C * (self.dim / tr)\n\n        # final clip/reflect ensure feasibility\n        if self.x_opt is not None:\n            self.x_opt = reflect(self.x_opt)\n\n        return float(self.f_opt), np.array(self.x_opt)", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveRotationalGaussianSearchV2 scored 0.227 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["301b461d-2f9e-49ea-ae17-7d69bab7e52f"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8456124508728993}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8208445685897705}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8367077256514802}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.025777512285672488}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.0057129894465904885}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06065876954271865}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08039816645775466}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.059028162922477034}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.04251722300107774}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.02767924604976746}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.03242562931854864}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.12561842831065972}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.16985470457328844}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.2658010741685882}], "aucs": [0.8456124508728993, 0.8208445685897705, 0.8367077256514802, 0.025777512285672488, 0.0057129894465904885, 4.999999999999449e-05, 0.06065876954271865, 0.08039816645775466, 0.059028162922477034, 0.04251722300107774, 0.02767924604976746, 0.03242562931854864, 0.12561842831065972, 0.16985470457328844, 0.2658010741685882]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3202.0, "Edges": 3201.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9993753903810119, "Degree Variance": 1.9219234074893072, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 9.297205180640763, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3234940689044512, "Depth Entropy": 2.3557265193897567, "Assortativity": 0.0, "Average Eccentricity": 21.074953154278575, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.0003123048094940662, "Average Shortest Path": 11.747612053619253, "mean_complexity": 11.833333333333334, "total_complexity": 71.0, "mean_token_count": 457.5, "total_token_count": 2745.0, "mean_parameter_count": 2.1666666666666665, "total_parameter_count": 13.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "f3dbca1a-0785-4ec0-801b-db9d3b46d02a", "fitness": "-inf", "name": "GraphGuidedEdgeExpansion", "description": "Graph-Guided Edge Expansion (GGEE) \u2014 build a dynamic k-NN graph of evaluated points and repeatedly expand along promising edges (midpoints, extrapolations, orthogonal probes and convex recombinations) to increase \"edge density\" and exploit connections between good samples for efficient global-to-local search.", "code": "import numpy as np\n\nclass GraphGuidedEdgeExpansion:\n    \"\"\"\n    Graph-Guided Edge Expansion (GGEE)\n\n    Main idea:\n      - Seed with a space-filling initial sample.\n      - Maintain a pool of evaluated points (nodes) and a k-NN graph (edges).\n      - Repeatedly evaluate midpoints of promising edges, extrapolate beyond better endpoints,\n        and perform small orthogonal probes \u2014 adding successful points to the pool.\n      - Occasionally perform convex recombinations among top nodes and global heavy-tailed jumps.\n      - The iterative edge-centric expansion increases the \"edge density\" of explored regions,\n        enabling traversals across basins and fine local refinement while respecting a strict budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng_seed=None,\n                 init_frac=0.12, k_edges=6, max_nodes=None):\n        \"\"\"\n        budget: total allowed function evaluations\n        dim: problem dimensionality\n        rng_seed: optional RNG seed\n        init_frac: fraction of budget used to set a soft cap for initial sampling\n        k_edges: number of neighbors per node used to construct edges\n        max_nodes: soft cap on stored nodes (older/worse nodes may be pruned)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(rng_seed)\n        self.init_frac = float(init_frac)\n        self.k_edges = int(k_edges)\n        self.max_nodes = None if max_nodes is None else int(max_nodes)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        lb = lb.reshape(-1)[:self.dim]\n        ub = ub.reshape(-1)[:self.dim]\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _lhs(self, n, lb, ub):\n        # Simple Latin-Hypercube-like sampling\n        n = int(max(1, n))\n        dim = self.dim\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.random((n, dim))\n        pts = np.empty((n, dim))\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            pts[:, j] = (perm + u[:, j]) / n\n        return lb + pts * (ub - lb)\n\n    def __call__(self, func):\n        calls = 0\n        B = self.budget\n\n        def eval_f(x):\n            nonlocal calls\n            if calls >= B:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            calls += 1\n            return float(func(x))\n\n        lb, ub = self._get_bounds(func)\n        width = ub - lb\n        avg_width = float(width.mean())\n\n        # initialize pool\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # initial sample size: balanced to leave room for many edge evaluations\n        init_samples = int(min(max(5 * self.dim, 50), max(10, int(self.init_frac * B))))\n        init_samples = max(10, init_samples)\n        init_samples = min(init_samples, B // 4)  # leave budget for expansions\n\n        X_pool = []\n        F_pool = []\n\n        try:\n            X0 = self._lhs(init_samples, lb, ub)\n            for i in range(X0.shape[0]):\n                x = X0[i]\n                f = eval_f(x)\n                X_pool.append(np.asarray(x, dtype=float))\n                F_pool.append(float(f))\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n        except StopIteration:\n            # budget exhausted during initialization\n            return self.f_opt, (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))\n\n        X_pool = np.vstack(X_pool)\n        F_pool = np.asarray(F_pool, dtype=float)\n\n        # helper to add a new point if it's not essentially duplicate\n        def try_add_point(x_new, f_new, tol_rel=1e-8):\n            nonlocal X_pool, F_pool\n            x_new = np.asarray(x_new, dtype=float)\n            # check duplicate-ish by distance\n            if X_pool.shape[0] > 0:\n                dists = np.linalg.norm(X_pool - x_new, axis=1)\n                if np.min(dists) < 1e-12 * (1.0 + np.linalg.norm(x_new)):\n                    return False\n            X_pool = np.vstack([X_pool, x_new])\n            F_pool = np.concatenate([F_pool, [float(f_new)]])\n            return True\n\n        # iteration parameters\n        max_no_improve = 8 + self.dim // 4\n        no_improve = 0\n        round_idx = 0\n\n        try:\n            while calls < B and no_improve < max_no_improve:\n                round_idx += 1\n                n_nodes = X_pool.shape[0]\n                improved_in_round = False\n\n                # Build k-NN edges (brute force distances; small n relative to budget)\n                edges = set()\n                if n_nodes > 1:\n                    dmat = np.linalg.norm(X_pool[:, None, :] - X_pool[None, :, :], axis=2)\n                    for i in range(n_nodes):\n                        k = min(self.k_edges, n_nodes - 1)\n                        if k <= 0:\n                            continue\n                        neigh_idx = np.argsort(dmat[i])[:k + 1]  # include self at index 0\n                        for j in neigh_idx:\n                            if i == j:\n                                continue\n                            e = (i, j) if i < j else (j, i)\n                            edges.add(e)\n\n                # score edges by min objective of endpoints (prefer edges touching good points)\n                edge_list = []\n                for (i, j) in edges:\n                    score = min(F_pool[i], F_pool[j])\n                    length = np.linalg.norm(X_pool[i] - X_pool[j])\n                    # prefer shorter edges among good ones (normalize by avg_width)\n                    edge_list.append((score + 0.01 * (length / (avg_width + 1e-12)), i, j, length))\n                # sort ascending by score (better edges first)\n                edge_list.sort(key=lambda x: x[0])\n\n                # cap number of edges to process this round\n                max_edges = max(6, 8 + self.dim)\n                edges_to_process = [e for e in edge_list[:max_edges]]\n\n                # Also ensure we always consider at least some random edges if few computed\n                if len(edges_to_process) < max_edges and len(edge_list) > 0:\n                    # add a couple random edges\n                    extra = self.rng.choice(len(edge_list), size=min(len(edge_list), max_edges - len(edges_to_process)), replace=False)\n                    for idx in extra:\n                        edges_to_process.append(edge_list[idx])\n\n                # Process each edge by midpoint + extrapolation + orthogonal probe\n                for (_, i, j, length) in edges_to_process:\n                    if calls >= B:\n                        break\n\n                    xi = X_pool[i]\n                    xj = X_pool[j]\n                    fi = F_pool[i]\n                    fj = F_pool[j]\n\n                    # order endpoints: better and worse\n                    if fi <= fj:\n                        best_end = xi.copy()\n                        best_f = fi\n                        worse_end = xj.copy()\n                    else:\n                        best_end = xj.copy()\n                        best_f = fj\n                        worse_end = xi.copy()\n\n                    # midpoint\n                    m = 0.5 * (xi + xj)\n                    m = np.clip(m, lb, ub)\n                    fm = eval_f(m)\n                    if fm < self.f_opt:\n                        self.f_opt = fm\n                        self.x_opt = m.copy()\n                    added = try_add_point(m, fm)\n                    if fm < min(fi, fj):\n                        improved_in_round = True\n                        # try extrapolation beyond the better endpoint in direction of edge\n                        d = best_end - worse_end\n                        norm_d = np.linalg.norm(d)\n                        if norm_d > 0:\n                            d_unit = d / norm_d\n                            # several scaled extrapolations (small and moderate)\n                            for beta in (0.3, 0.7, 1.5):\n                                if calls >= B:\n                                    break\n                                x_ext = np.clip(best_end + beta * norm_d * d_unit, lb, ub)\n                                f_ext = eval_f(x_ext)\n                                if f_ext < self.f_opt:\n                                    self.f_opt = f_ext\n                                    self.x_opt = x_ext.copy()\n                                try_add_point(x_ext, f_ext)\n                                if f_ext < best_f:\n                                    best_f = f_ext\n                                    best_end = x_ext.copy()\n                                    improved_in_round = True\n                        # also try small orthogonal probe in subspace spanned by random vector\n                        if calls < B and norm_d > 0:\n                            # random vector and orthogonalize with respect to d\n                            rnd = self.rng.normal(size=self.dim)\n                            proj = rnd - (np.dot(rnd, d_unit) * d_unit)\n                            nrm = np.linalg.norm(proj)\n                            if nrm > 0:\n                                orth = proj / nrm\n                                scale = 0.05 * avg_width * (0.5 + self.rng.random())\n                                x_ort = np.clip(best_end + scale * orth, lb, ub)\n                                f_ort = eval_f(x_ort)\n                                if f_ort < self.f_opt:\n                                    self.f_opt = f_ort\n                                    self.x_opt = x_ort.copy()\n                                try_add_point(x_ort, f_ort)\n                                if f_ort < best_f:\n                                    best_f = f_ort\n                                    best_end = x_ort.copy()\n                                    improved_in_round = True\n                    else:\n                        # midpoint not improving endpoints: try a directional golden-ish probe from midpoint toward better endpoint\n                        if calls < B:\n                            d = best_end - m\n                            nrm = np.linalg.norm(d)\n                            if nrm > 0:\n                                # try a point pulled to better endpoint (fractional)\n                                frac = 0.25 + 0.5 * self.rng.random()\n                                x_pull = np.clip(m + frac * d, lb, ub)\n                                f_pull = eval_f(x_pull)\n                                if f_pull < self.f_opt:\n                                    self.f_opt = f_pull\n                                    self.x_opt = x_pull.copy()\n                                try_add_point(x_pull, f_pull)\n                                if f_pull < min(fi, fj):\n                                    improved_in_round = True\n\n                # Edge recombination among top nodes: convex combinations + heavy-tailed jumps\n                if calls < B:\n                    ntop = min(6 + self.dim // 2, X_pool.shape[0])\n                    top_idx = np.argsort(F_pool)[:ntop]\n                    # create a handful of recombined children\n                    n_recomb = min(6, max(2, ntop // 2))\n                    for _ in range(n_recomb):\n                        if calls >= B:\n                            break\n                        k = self.rng.integers(2, min(5, ntop) + 1)\n                        picks = self.rng.choice(top_idx, size=k, replace=False)\n                        weights = self.rng.random(k)\n                        weights = weights / weights.sum()\n                        child = np.zeros(self.dim)\n                        for w, p in zip(weights, picks):\n                            child += w * X_pool[p]\n                        # add small directional noise proportional to local spread of picks\n                        spread = np.std(X_pool[picks], axis=0)\n                        noise_scale = 0.03 * (0.5 * (spread + 0.2 * width))\n                        child = np.clip(child + self.rng.normal(scale=noise_scale), lb, ub)\n                        f_child = eval_f(child)\n                        if f_child < self.f_opt:\n                            self.f_opt = f_child\n                            self.x_opt = child.copy()\n                        try_add_point(child, f_child)\n                        if f_child < np.max(F_pool):\n                            improved_in_round = True\n\n                # If stagnating, perform a few heavy-tailed jumps from best\n                if not improved_in_round:\n                    if calls < B:\n                        n_heavy = min(6, max(1, (B - calls) // 50))\n                        for _ in range(n_heavy):\n                            if calls >= B:\n                                break\n                            # Cauchy-like heavy tail scaled to domain\n                            scale = 0.5 * avg_width\n                            jump = self.rng.standard_cauchy(size=self.dim)\n                            # clip extreme outliers to keep inside reasonable range\n                            jump = np.tanh(jump)  # compress tails to [-1,1]\n                            x_jump = np.clip((self.x_opt if self.x_opt is not None else 0.5 * (lb + ub))\n                                             + scale * jump, lb, ub)\n                            f_jump = eval_f(x_jump)\n                            if f_jump < self.f_opt:\n                                self.f_opt = f_jump\n                                self.x_opt = x_jump.copy()\n                            try_add_point(x_jump, f_jump)\n                            if f_jump < np.min(F_pool):\n                                improved_in_round = True\n\n                # prune pool if too large: keep best plus diversity\n                if self.max_nodes is not None and X_pool.shape[0] > self.max_nodes:\n                    keep = min(self.max_nodes, X_pool.shape[0])\n                    idx_keep = np.argsort(F_pool)[:keep]\n                    X_pool = X_pool[idx_keep]\n                    F_pool = F_pool[idx_keep]\n\n                # small final local Gaussian refinement around current best if budget allows\n                if calls < B and improved_in_round:\n                    local_trials = min(5, (B - calls))\n                    for _ in range(local_trials):\n                        if calls >= B:\n                            break\n                        scale = 0.02 * avg_width * (0.5 + self.rng.random())\n                        x_local = np.clip(self.x_opt + self.rng.normal(scale=scale, size=self.dim), lb, ub)\n                        f_local = eval_f(x_local)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local.copy()\n                        try_add_point(x_local, f_local)\n\n                if improved_in_round:\n                    no_improve = 0\n                else:\n                    no_improve += 1\n\n        except StopIteration:\n            # budget exhausted; fall through to return best found\n            pass\n\n        # final exploitation: use remaining budget for small random local probes around best\n        try:\n            extra = max(0, B - calls)\n            extra_trials = min(12, extra)\n            for _ in range(extra_trials):\n                scale = 0.01 * avg_width * (0.2 + self.rng.random())\n                x_try = np.clip((self.x_opt if self.x_opt is not None else 0.5 * (lb + ub))\n                                + self.rng.normal(scale=scale, size=self.dim), lb, ub)\n                f_try = eval_f(x_try)\n                if f_try < self.f_opt:\n                    self.f_opt = f_try\n                    self.x_opt = x_try.copy()\n        except StopIteration:\n            pass\n\n        return self.f_opt, (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["0228629a-a7a4-4d75-8105-fe131f1f6e92"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2763.0, "Edges": 2762.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.9992761491132827, "Degree Variance": 1.8313422194347546, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 9.137207425343018, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.337333103548655, "Depth Entropy": 2.28096113222367, "Assortativity": 0.0, "Average Eccentricity": 18.842200506695622, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003619254433586681, "Average Shortest Path": 11.492210478645744, "mean_complexity": 12.833333333333334, "total_complexity": 77.0, "mean_token_count": 396.8333333333333, "total_token_count": 2381.0, "mean_parameter_count": 3.1666666666666665, "total_parameter_count": 19.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "b2f7fc94-fa92-4b90-84f5-1b43111990f3", "fitness": "-inf", "name": "EdgeAugmentedAdaptiveGraphSearch", "description": "Edge-Augmented Adaptive Graph Search (EAAGS) \u2014 build a dense implicit graph among elite samples and propose candidates along graph edges, midpoints and anisotropic Gaussian directions; adapt step-sizes by success-rate and restart when stuck.", "code": "import numpy as np\n\nclass EdgeAugmentedAdaptiveGraphSearch:\n    \"\"\"\n    Edge-Augmented Adaptive Graph Search (EAAGS)\n\n    Key ideas:\n    - Maintain an archive of evaluated points (lists -> arrays on demand).\n    - Build a dense local graph among elite points (k-NN / pair edges).\n    - Propose candidates along edges: midpoints, extrapolations, and noisy edge directions,\n      scaled by edge length to increase 'edge density' of search moves.\n    - Combine edge proposals with anisotropic Gaussian sampling using elite covariance.\n    - Adaptive sigma via success-window (like 1/5-ish rule), occasional stochastic restarts.\n    - Works fully within budget and box [-5,5] (or func.bounds if provided).\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, rng_seed=None,\n                 init_samples=None, pop_size=None, sigma0=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(rng_seed)\n        # sensible defaults scaled by budget/dimension\n        self.init_samples = int(init_samples) if init_samples is not None else max(12, min(200, self.budget // 20))\n        self.pop_size = int(pop_size) if pop_size is not None else max(8, min(200, self.budget // 50))\n        # sigma0: normalized relative step (fraction of box diameter)\n        self.sigma0 = float(sigma0) if sigma0 is not None else 0.25\n        # adaptation params\n        self.success_target = 0.2\n        self.adapt_up = 1.3\n        self.adapt_down = 0.85\n        self.min_sigma = 1e-6\n        self.max_sigma = 5.0\n        self.patience_restart = max(6, self.budget // 200)\n        # internal results\n        self.x_opt = None\n        self.f_opt = None\n\n    def __call__(self, func):\n        # parse bounds: default [-5,5] scalar or per-dim arrays if func exposes .bounds\n        lb, ub = -5.0, 5.0\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            # try multiple common patterns\n            try:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb_v = np.asarray(b.lb, dtype=float)\n                    ub_v = np.asarray(b.ub, dtype=float)\n                    if lb_v.size == 1:\n                        lb = float(lb_v) * np.ones(self.dim)\n                    else:\n                        lb = lb_v.reshape(-1)[:self.dim]\n                    if ub_v.size == 1:\n                        ub = float(ub_v) * np.ones(self.dim)\n                    else:\n                        ub = ub_v.reshape(-1)[:self.dim]\n                elif isinstance(b, (list, tuple, np.ndarray)) and len(b) == 2:\n                    lb_v = np.asarray(b[0], dtype=float)\n                    ub_v = np.asarray(b[1], dtype=float)\n                    if lb_v.size == 1:\n                        lb = float(lb_v) * np.ones(self.dim)\n                    else:\n                        lb = lb_v.reshape(-1)[:self.dim]\n                    if ub_v.size == 1:\n                        ub = float(ub_v) * np.ones(self.dim)\n                    else:\n                        ub = ub_v.reshape(-1)[:self.dim]\n            except Exception:\n                lb, ub = -5.0, 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        width = ub - lb\n        diameter = np.linalg.norm(width)\n        # archive as Python lists to allow append safely\n        X_list = []\n        F_list = []\n        evals = 0\n\n        def eval_x(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise RuntimeError(\"Budget exceeded\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            evals += 1\n            X_list.append(x.copy())\n            F_list.append(float(f))\n            return float(f), x.copy()\n\n        # initial sampling (global)\n        n_init = min(self.init_samples, self.budget)\n        for _ in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            eval_x(x0)\n            if evals >= self.budget:\n                break\n\n        # ensure at least one point\n        if len(F_list) == 0:\n            x0 = self.rng.uniform(lb, ub)\n            eval_x(x0)\n\n        # convert to arrays for convenience\n        X = np.array(X_list)\n        F = np.array(F_list)\n        best_idx = int(np.argmin(F))\n        x_best = X[best_idx].copy()\n        f_best = float(F[best_idx])\n\n        # absolute sigma in coordinate space\n        sigma = self.sigma0 * (diameter / np.sqrt(max(1, self.dim)))\n        sigma = float(np.clip(sigma, self.min_sigma, self.max_sigma))\n\n        success_window = []\n        no_improve = 0\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            n_points = X.shape[0]\n\n            # choose elites: top k (but at least 2)\n            k_elite = max(2, min(n_points, max(6, n_points // 6)))\n            elite_idx = np.argsort(F)[:k_elite]\n            elites = X[elite_idx]  # shape (k_elite, dim)\n\n            # compute covariance from elites for anisotropic proposals\n            if elites.shape[0] >= 2:\n                mean_elite = np.mean(elites, axis=0)\n                cov = np.cov(elites, rowvar=False)\n                # regularize small eigenvalues\n                cov = cov + 1e-8 * np.eye(self.dim)\n            else:\n                mean_elite = elites[0].copy()\n                cov = np.eye(self.dim) * 1e-6\n\n            # blend isotropic to keep exploration\n            cov_scale = max(1e-12, (diameter**2) / (self.dim**2))\n            C = cov + 0.04 * cov_scale * np.eye(self.dim)\n\n            # prepare sampling matrix (Cholesky or eigen fallback)\n            sample_cov = (sigma**2) * C\n            sample_cov = (sample_cov + sample_cov.T) / 2.0\n            try:\n                L = np.linalg.cholesky(sample_cov)\n            except np.linalg.LinAlgError:\n                vals, vecs = np.linalg.eigh(sample_cov)\n                vals = np.clip(vals, 1e-12, None)\n                L = vecs @ np.diag(np.sqrt(vals))\n\n            improved_in_batch = False\n            batch_proposals = 0\n\n            # build dense edge list among elites (k-NN style: for each node, connect to up to m nearest)\n            k = elites.shape[0]\n            if k >= 2:\n                # pairwise differences and distances\n                diffs = elites[:, None, :] - elites[None, :, :]  # (k,k,dim)\n                dists = np.linalg.norm(diffs, axis=2) + np.eye(k) * 1e12  # large on diagonal to ignore\n                # for each node pick up to m nearest neighbors\n                m = min(k - 1, 4)  # connectivity per node\n                edges = []\n                for i in range(k):\n                    nn = np.argsort(dists[i])[:m]\n                    for j in nn:\n                        edges.append((i, int(j), dists[i, int(j)], diffs[i, int(j)]))\n                # deduplicate directional edges (keep as is; order keeps orientation)\n            else:\n                edges = []\n\n            # determine how many proposals this iteration: limit to pop_size but also remaining budget\n            pop = min(self.pop_size, remaining)\n\n            # Strategy mix weights: anisotropic gaussian (0.4), edge-midpoint (0.3), edge-extrapolate (0.2), differential-edge (0.1)\n            # choose strategies per-proposal\n            strat_probs = np.array([0.4, 0.3, 0.2, 0.1])\n            strat_probs = strat_probs / strat_probs.sum()\n\n            for _ in range(pop):\n                if evals >= self.budget:\n                    break\n\n                r = self.rng.random()\n                # sample strategy\n                s = np.searchsorted(np.cumsum(strat_probs), r)\n                # Strategy 0: anisotropic gaussian around best (or mean elite)\n                if s == 0 or len(edges) == 0:\n                    z = self.rng.normal(size=self.dim)\n                    cand = x_best + L @ z\n                # Strategy 1: edge-midpoint + small noise (encourage dense mid-edge exploration)\n                elif s == 1 and len(edges) > 0:\n                    ei = self.rng.integers(0, len(edges))\n                    i, j, dist_ij, diff_ij = edges[ei]\n                    midpoint = 0.5 * (elites[i] + elites[j])\n                    # noise scaled by edge length and sigma\n                    local_scale = max(1e-8, dist_ij)\n                    noise = self.rng.normal(scale=0.12 * sigma * (1.0 + local_scale / (diameter + 1e-12)), size=self.dim)\n                    cand = midpoint + noise\n                # Strategy 2: edge-extrapolate (xi + rho * (xi - xj)), rho random (can be >1)\n                elif s == 2 and len(edges) > 0:\n                    ei = self.rng.integers(0, len(edges))\n                    i, j, dist_ij, diff_ij = edges[ei]\n                    direction = diff_ij  # xi - xj\n                    # scale direction to unit\n                    norm_dir = np.linalg.norm(direction)\n                    if norm_dir < 1e-12:\n                        direction_unit = self.rng.normal(size=self.dim)\n                        direction_unit /= np.linalg.norm(direction_unit)\n                    else:\n                        direction_unit = direction / norm_dir\n                    # rho drawn from a heavy-tailed-ish distribution (normal * folded Cauchy-like factor)\n                    rho = (1.0 + 2.0 * self.rng.standard_cauchy()) if self.rng.random() < 0.1 else (1.0 + self.rng.normal(scale=0.6))\n                    rho = float(np.clip(rho, -3.0, 5.0))\n                    cand = elites[i] + rho * norm_dir * direction_unit * (0.5 / max(1.0, self.dim / 10.0))\n                    # add small gaussian\n                    cand += self.rng.normal(scale=0.08 * sigma, size=self.dim)\n                # Strategy 3: differential-edge recombination (like DE but using edges)\n                else:\n                    # pick two random edges (or fall back)\n                    if len(edges) >= 2:\n                        e1 = edges[self.rng.integers(0, len(edges))]\n                        e2 = edges[self.rng.integers(0, len(edges))]\n                        _, _, d1, diff1 = e1\n                        _, _, d2, diff2 = e2\n                        # candidate = best + F*(diff1 - diff2) + noise\n                        Fscale = 0.7 * (0.5 + self.rng.random())\n                        cand = x_best + Fscale * (diff1 - diff2)\n                        cand += self.rng.normal(scale=0.06 * sigma, size=self.dim)\n                    else:\n                        # fallback to gaussian\n                        z = self.rng.normal(size=self.dim)\n                        cand = x_best + L @ z\n\n                # clip candidate to bounds\n                cand = np.minimum(np.maximum(cand, lb), ub)\n\n                try:\n                    f_cand, x_cand = eval_x(cand)\n                except RuntimeError:\n                    break\n\n                batch_proposals += 1\n                X = np.array(X_list)\n                F = np.array(F_list)\n\n                if f_cand < f_best:\n                    f_best = f_cand\n                    x_best = x_cand.copy()\n                    improved_in_batch = True\n                    no_improve = 0\n                # small early exit if we reached function optimum (best possible heuristically)\n                # but we do not assume known optimum value, so we just continue until budget.\n\n            # Additional local manifold probes along top eigenvectors of C\n            if evals < self.budget:\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                except np.linalg.LinAlgError:\n                    eigvals = np.ones(self.dim)\n                    eigvecs = np.eye(self.dim)\n                order = np.argsort(-eigvals)\n                top_k = min(3, self.dim)\n                for idx in range(top_k):\n                    if evals >= self.budget:\n                        break\n                    v = eigvecs[:, order[idx]]\n                    # step length scaled by sqrt(eigval) and sigma\n                    step_len = sigma * (1.0 + np.sqrt(max(0.0, eigvals[order[idx]])))\n                    for sgn in (+1.0, -1.0):\n                        if evals >= self.budget:\n                            break\n                        probe = x_best + sgn * v * step_len * (0.6 + 0.4 * self.rng.random())\n                        probe = np.minimum(np.maximum(probe, lb), ub)\n                        try:\n                            f_p, x_p = eval_x(probe)\n                        except RuntimeError:\n                            break\n                        X = np.array(X_list)\n                        F = np.array(F_list)\n                        if f_p < f_best:\n                            f_best = f_p\n                            x_best = x_p.copy()\n                            improved_in_batch = True\n                            no_improve = 0\n\n            # adapt sigma using success window\n            success = 1.0 if improved_in_batch else 0.0\n            success_window.append(success)\n            if len(success_window) > 25:\n                success_window.pop(0)\n            succ_rate = float(np.mean(success_window)) if success_window else 0.0\n            if succ_rate > self.success_target:\n                # many successes -> shrink to exploit\n                sigma = max(self.min_sigma, sigma * self.adapt_down)\n            else:\n                # few successes -> grow to explore\n                sigma = min(self.max_sigma, sigma * self.adapt_up)\n            sigma = float(np.clip(sigma, self.min_sigma, self.max_sigma))\n\n            if improved_in_batch:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # occasional restart if stuck\n            if no_improve >= self.patience_restart and evals < self.budget:\n                # propose a randomized restart: either uniform or perturb a random elite heavily\n                if self.rng.random() < 0.5 and X.shape[0] > 0:\n                    idx = int(self.rng.integers(0, X.shape[0]))\n                    x_restart = X[idx] + self.rng.normal(scale=1.2 * sigma, size=self.dim)\n                else:\n                    x_restart = self.rng.uniform(lb, ub)\n                x_restart = np.minimum(np.maximum(x_restart, lb), ub)\n                try:\n                    f_r, x_r = eval_x(x_restart)\n                except RuntimeError:\n                    break\n                X = np.array(X_list)\n                F = np.array(F_list)\n                if f_r < f_best:\n                    f_best = f_r\n                    x_best = x_r.copy()\n                    no_improve = 0\n                else:\n                    # reset sigma moderately to escape local basin\n                    sigma = self.sigma0 * (diameter / np.sqrt(max(1, self.dim)))\n                    sigma = float(np.clip(sigma, self.min_sigma, self.max_sigma))\n                    no_improve = 0\n\n            # refresh arrays and best\n            X = np.array(X_list)\n            F = np.array(F_list)\n            best_idx = int(np.argmin(F))\n            x_best = X[best_idx].copy()\n            f_best = float(F[best_idx])\n\n        # store and return best found\n        self.x_opt = x_best.copy()\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["b4f78011-40d8-486f-a970-ac4eb735d830"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2851.0, "Edges": 2850.0, "Max Degree": 35.0, "Min Degree": 1.0, "Mean Degree": 1.9992984917572783, "Degree Variance": 2.106628760779907, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.865474339035769, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.325297316030386, "Depth Entropy": 2.2826252147822146, "Assortativity": 0.0, "Average Eccentricity": 18.934408979305505, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00035075412136092597, "Average Shortest Path": 11.164573587599303, "mean_complexity": 20.0, "total_complexity": 60.0, "mean_token_count": 843.0, "total_token_count": 2529.0, "mean_parameter_count": 3.3333333333333335, "total_parameter_count": 10.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "09ee62dd-1db4-40bf-b90a-82a108feee3a", "fitness": 0.11581316873808226, "name": "EDAGS", "description": "Edge-Density Adaptive Graph Search (EDAGS) \u2014 maintain a small population as a graph, preferentially sample along edges with large objective differences to densify promising edges while using local anisotropic proposals and occasional L\u00e9vy escapes for global moves.", "code": "import numpy as np\n\nclass EDAGS:\n    \"\"\"\n    Edge-Density Adaptive Graph Search (EDAGS)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional keyword arguments tune exploration/exploitation behavior.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_pool=None, k_neighbors=None,\n                 edge_priority=0.6,    # probability to do an edge-based proposal\n                 local_priority=0.3,   # probability to do local anisotropic proposal\n                 levy_prob=0.06,       # occasional heavy-tailed jump\n                 stagnation_restart=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # pool size: modestly scales with dimension but remains small to keep graph dense\n        if init_pool is None:\n            self.pool_size = int(np.clip(12 + 2 * self.dim, 8, 60))\n        else:\n            self.pool_size = int(init_pool)\n\n        # number of neighbors to build local covariance\n        if k_neighbors is None:\n            self.k_neighbors = int(np.clip(2 + self.dim // 3, 2, min(12, self.pool_size - 1)))\n        else:\n            self.k_neighbors = int(k_neighbors)\n\n        self.edge_priority = float(edge_priority)\n        self.local_priority = float(local_priority)\n        # remaining probability is for uniform/global proposals\n        self.levy_prob = float(levy_prob)\n        self.stagnation_restart = int(stagnation_restart)\n\n    def _levy_dir(self, size):\n        # simple normalized Cauchy vector as a heavy-tailed direction\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        nrm = np.linalg.norm(z)\n        if nrm == 0:\n            return self.rng.standard_normal(size=size)\n        return z / nrm\n\n    def __call__(self, func):\n        # fetch bounds from func; fall back to [-5,5] if absent\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        evals = 0\n        # initial pool size must not exceed budget\n        n_init = min(self.pool_size, max(1, budget))\n        positions = np.empty((0, dim), dtype=float)\n        values = np.empty((0,), dtype=float)\n\n        # initial uniform sampling (seed the graph)\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            positions = np.vstack([positions, x])\n            values = np.append(values, f)\n            if evals >= budget:\n                break\n\n        # if budget exhausted quickly, return best found\n        best_idx = int(np.argmin(values))\n        f_best = float(values[best_idx])\n        x_best = positions[best_idx].copy()\n\n        # basic scales\n        box_range = ub - lb\n        mean_range = float(np.mean(box_range))\n        # small scale used to control local gaussian std\n        base_sigma = max(1e-8, 0.1 * mean_range)\n\n        # stagnation counter\n        since_improve = 0\n        best_seen = f_best\n\n        # helper: reflect-and-clip boundary handling\n        def reflect_clip(x):\n            below = x < lb\n            above = x > ub\n            if np.any(below):\n                x = np.where(below, lb + (lb - x), x)\n            if np.any(above):\n                x = np.where(above, ub - (x - ub), x)\n            return np.clip(x, lb, ub)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n\n            # recompute best and worst\n            best_idx = int(np.argmin(values))\n            worst_idx = int(np.argmax(values))\n            f_best = float(values[best_idx])\n            x_best = positions[best_idx].copy()\n\n            # build distance matrix once per outer iteration\n            n_nodes = positions.shape[0]\n            if n_nodes >= 2:\n                diffs = positions[:, None, :] - positions[None, :, :]  # (n,n,dim)\n                dists = np.linalg.norm(diffs, axis=2) + np.eye(n_nodes) * 1e12\n            else:\n                dists = np.array([[1e12]])\n\n            # choose operator\n            p = rng.random()\n            op = None\n            if p < self.edge_priority and n_nodes >= 2:\n                op = \"edge\"\n            elif p < self.edge_priority + self.local_priority and n_nodes >= 2:\n                op = \"local\"\n            elif rng.random() < self.levy_prob:\n                op = \"levy\"\n            else:\n                op = \"global\"\n\n            # prepare one candidate per loop (we'll evaluate one by one)\n            x_trial = None\n\n            if op == \"edge\":\n                # pick an edge weighted by objective difference and length\n                # compute upper triangle potentials\n                idxs = np.triu_indices(n_nodes, k=1)\n                fi = values[idxs[0]]\n                fj = values[idxs[1]]\n                edge_diff = np.abs(fi - fj)\n                # length from distances matrix\n                edge_len = dists[idxs]\n                # potential weights prefer large objective differences and moderate lengths\n                pot = edge_diff + 0.1 * (edge_len / (np.mean(edge_len) + 1e-12))\n                # avoid zeros\n                pot_sum = np.sum(pot)\n                if pot_sum == 0:\n                    # fallback to uniform random pair\n                    a, b = rng.integers(0, n_nodes, size=2)\n                    while b == a:\n                        b = rng.integers(0, n_nodes)\n                else:\n                    # sample an edge index\n                    edge_idx = rng.choice(len(pot), p=pot / pot_sum)\n                    a = idxs[0][edge_idx]\n                    b = idxs[1][edge_idx]\n\n                # designate better end and worse end\n                if values[a] <= values[b]:\n                    better, worse = a, b\n                else:\n                    better, worse = b, a\n\n                xb = positions[better]\n                xw = positions[worse]\n                vec = xw - xb\n                ell = np.linalg.norm(vec) + 1e-12\n\n                # sample interpolation biased towards better end\n                # mostly interior sampling (Beta), occasional extrapolation outward\n                if rng.random() < 0.08:\n                    alpha = rng.uniform(1.0, 2.0)  # extrapolate occasionally\n                else:\n                    alpha = rng.beta(2.0, 5.0)\n                # add small perpendicular / isotropic noise proportional to edge length\n                noise_scale = max(base_sigma * 0.15, 0.12 * ell)\n                x_trial = xb + alpha * vec + rng.normal(scale=noise_scale, size=dim)\n\n                x_trial = reflect_clip(x_trial)\n\n            elif op == \"local\":\n                # build local covariance from best's k nearest neighbors\n                if n_nodes <= 1:\n                    # fallback to uniform\n                    x_trial = rng.uniform(lb, ub)\n                else:\n                    # find k nearest to best (excluding itself)\n                    d_to_best = dists[best_idx].copy()\n                    d_to_best[best_idx] = 1e12\n                    k = min(self.k_neighbors, n_nodes - 1)\n                    nbr_idx = np.argpartition(d_to_best, k)[:k]\n                    rel = positions[nbr_idx] - x_best  # (k, dim)\n                    # weight neighbors by relative improvement (lower is better)\n                    f_nbr = values[nbr_idx]\n                    # smaller f_nbr -> higher weight; use softmax-like weighting\n                    delta = f_nbr - f_best\n                    # scale to avoid overflow\n                    s = np.std(delta) if np.std(delta) > 0 else 1.0\n                    weights = np.exp(-np.clip(delta / (s + 1e-12), -10, 10))\n                    weights = weights / (np.sum(weights) + 1e-12)\n                    # covariance estimate (anisotropic along directions to neighbors)\n                    cov = (rel.T * weights) @ rel + 1e-8 * np.eye(dim)\n                    # scale sigma according to typical neighbor distance\n                    avg_len = np.mean(np.linalg.norm(rel, axis=1)) + 1e-12\n                    sigma_loc = max(base_sigma * 0.5, 0.6 * avg_len)\n                    # sample from multivariate normal with cov but controlled magnitude\n                    try:\n                        L = np.linalg.cholesky(cov + 1e-12 * np.eye(dim))\n                        z = rng.normal(size=dim)\n                        step = L.dot(z)\n                    except np.linalg.LinAlgError:\n                        # fallback isotropic\n                        step = rng.normal(scale=sigma_loc, size=dim)\n                    # normalize step magnitude to be comparable to sigma_loc\n                    step = step / (np.linalg.norm(step) + 1e-12) * (rng.normal(loc=0.6, scale=0.2) * sigma_loc)\n                    x_trial = x_best + step\n                    x_trial = reflect_clip(x_trial)\n\n            elif op == \"levy\":\n                # heavy-tailed global jump around a random center (sometimes around best)\n                if rng.random() < 0.5:\n                    center = x_best\n                else:\n                    center = rng.uniform(lb, ub)\n                dirc = self._levy_dir(dim)\n                # step length scaled to box range\n                scale = mean_range * (0.5 + rng.random() * 2.0)\n                x_trial = center + dirc * scale\n                x_trial = reflect_clip(x_trial)\n\n            else:  # global uniform sampling\n                x_trial = rng.uniform(lb, ub)\n\n            # Evaluate candidate if we still have budget\n            if x_trial is None:\n                # safety fallback\n                x_trial = rng.uniform(lb, ub)\n\n            # Avoid duplicate re-evaluation of an identical point to reduce wasted evals:\n            # If x_trial is extremely close to an existing node, nudge slightly\n            if positions.shape[0] > 0:\n                dmin = np.min(np.linalg.norm(positions - x_trial, axis=1))\n                if dmin < 1e-12:\n                    x_trial = x_trial + rng.normal(scale=1e-6, size=dim)\n                    x_trial = reflect_clip(x_trial)\n\n            f_trial = float(func(x_trial))\n            evals += 1\n\n            # Insert or replace logic:\n            # If better than worst, replace worst. Otherwise, with small probability replace a random node\n            if f_trial < values[worst_idx]:\n                positions[worst_idx] = x_trial\n                values[worst_idx] = f_trial\n            else:\n                # occasionally accept for diversity (simulated steady-state)\n                if rng.random() < 0.12 and positions.shape[0] >= 1:\n                    # replace a random non-best node\n                    replace_idx = rng.integers(0, positions.shape[0])\n                    # don't overwrite the best if possible\n                    if replace_idx == best_idx and positions.shape[0] > 1:\n                        replace_idx = (replace_idx + 1) % positions.shape[0]\n                    positions[replace_idx] = x_trial\n                    values[replace_idx] = f_trial\n                else:\n                    # if pool not yet full, append\n                    if positions.shape[0] < self.pool_size:\n                        positions = np.vstack([positions, x_trial])\n                        values = np.append(values, f_trial)\n\n            # update best tracking\n            cur_best_idx = int(np.argmin(values))\n            cur_best_val = float(values[cur_best_idx])\n            if cur_best_val < best_seen - 1e-12:\n                best_seen = cur_best_val\n                since_improve = 0\n            else:\n                since_improve += 1\n\n            # adaptive escalation on stagnation: densify top edges and increase randomization\n            if since_improve > self.stagnation_restart and evals < budget:\n                # densify: create midpoints of top-K edges (by potential) and evaluate a few\n                n_nodes = positions.shape[0]\n                if n_nodes >= 2:\n                    idxs = np.triu_indices(n_nodes, k=1)\n                    fi = values[idxs[0]]\n                    fj = values[idxs[1]]\n                    edge_diff = np.abs(fi - fj)\n                    edge_len = dists[idxs]\n                    pot = edge_diff + 0.05 * (edge_len / (np.mean(edge_len) + 1e-12))\n                    if np.sum(pot) > 0:\n                        # select up to 4 top edges\n                        topk = min(4, len(pot))\n                        top_idx = np.argpartition(-pot, topk - 1)[:topk]\n                        pushes = min(topk, budget - evals)\n                        for ii in range(pushes):\n                            e = top_idx[ii]\n                            a = idxs[0][e]; b = idxs[1][e]\n                            mid = 0.5 * (positions[a] + positions[b])\n                            # small random perturbation around midpoint\n                            mm = reflect_clip(mid + rng.normal(scale=0.08 * mean_range, size=dim))\n                            f_mm = float(func(mm))\n                            evals += 1\n                            # replace worst if improvement\n                            if f_mm < values[worst_idx]:\n                                positions[worst_idx] = mm\n                                values[worst_idx] = f_mm\n                                worst_idx = int(np.argmax(values))\n                                # update best_seen if necessary\n                                if f_mm < best_seen:\n                                    best_seen = f_mm\n                                    since_improve = 0\n                            if evals >= budget:\n                                break\n                # escalate randomness a bit to escape\n                self.levy_prob = min(0.5, self.levy_prob * 1.4)\n                # reset stagnation counter to allow further progress before next escalation\n                since_improve = 0\n\n        # return best found\n        final_best_idx = int(np.argmin(values))\n        return float(values[final_best_idx]), np.asarray(positions[final_best_idx], dtype=float)", "configspace": "", "generation": 1, "feedback": "The algorithm EDAGS scored 0.116 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.371909930659285}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.3527510177113853}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.33891223520511493}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06788549600777027}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0013263166187318376}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.021167542846817433}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.008803650252148132}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.024526503201566463}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0041615367212283205}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.130195413990835}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.16823060658397904}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.24717728127237237}], "aucs": [0.371909930659285, 0.3527510177113853, 0.33891223520511493, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.06788549600777027, 0.0013263166187318376, 0.021167542846817433, 0.008803650252148132, 0.024526503201566463, 0.0041615367212283205, 0.130195413990835, 0.16823060658397904, 0.24717728127237237]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2467.0, "Edges": 2466.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.999189298743413, "Degree Variance": 2.0583698332377325, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.535299374441466, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3225825946213905, "Depth Entropy": 2.176474615201532, "Assortativity": 1.597286881742356e-08, "Average Eccentricity": 17.667207134171058, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00040535062829347385, "Average Shortest Path": 10.618772172235554, "mean_complexity": 12.75, "total_complexity": 51.0, "mean_token_count": 536.25, "total_token_count": 2145.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "c62e9cae-8abe-4542-ae2c-11adf1434b0a", "fitness": 0.009277899778133588, "name": "MSDirectionalGraphSearch", "description": "Multi-Scale Directional Graph Search (MS-DGS) \u2014 combine Latin-hypercube seeding, a k-nearest \"edge\" graph between promising seeds for efficient line-parabolic probes, PCA-informed low-dimensional subspace directional searches, adaptive per-seed step sizes, and frequent recombinations to increase connectivity and escape traps.", "code": "import numpy as np\n\nclass MSDirectionalGraphSearch:\n    \"\"\"\n    Multi-Scale Directional Graph Search (MS-DGS)\n\n    Key ideas:\n    - Use a compact Latin-Hypercube initialization to get diverse seeds.\n    - Build a k-nearest graph among top seeds and probe edges with 1D parabolic interpolation (cheap line searches).\n    - For each seed do PCA-informed low-dimensional directional searches (subspace), using parabolic interpolation along directions.\n    - Maintain per-seed adaptive step sizes: increase on success, shrink on failure.\n    - Frequent recombinations (convex + extrapolated) and local jittering.\n    - Maintain archive of visited points to avoid duplicated evaluations and enable local PCA.\n    - Carefully budget all function evaluations.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng_seed=None,\n                 init_frac=0.12, max_seeds=14, subspace_scale='sqrt'):\n        \"\"\"\n        budget: allowed function evaluations\n        dim: dimensionality\n        rng_seed: int seed\n        init_frac: fraction of budget used for initial sampling (0..1)\n        max_seeds: maximum number of seeds to refine\n        subspace_scale: 'sqrt' or integer subspace dimension\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(rng_seed)\n        self.init_frac = float(init_frac)\n        self.max_seeds = int(max_seeds)\n        self.subspace_scale = subspace_scale\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        lb = lb.reshape(-1)[:self.dim]\n        ub = ub.reshape(-1)[:self.dim]\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _lhs(self, n_samples, lb, ub):\n        n = int(n_samples)\n        dim = self.dim\n        # Simple LHS with random permutation per dimension\n        u = self.rng.random((n, dim))\n        pts = np.empty((n, dim))\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            pts[:, j] = (perm + u[:, j]) / n\n        return lb + pts * (ub - lb)\n\n    def __call__(self, func):\n        calls_made = 0\n        B = self.budget\n\n        def eval_f(x):\n            nonlocal calls_made\n            if calls_made >= B:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            calls_made += 1\n            return float(func(x))\n\n        lb, ub = self._get_bounds(func)\n        width = ub - lb\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # initial sampling size (keep some budget for refinement)\n        init_budget = max(1, int(min(max(50, 10 * self.dim), max(1, int(self.init_frac * B)))))\n        init_budget = min(init_budget, max(1, B - max(10, self.dim * 2)))  # reserve some evals\n        try:\n            X0 = self._lhs(init_budget, lb, ub)\n            F0 = np.empty(init_budget, dtype=float)\n            for i in range(init_budget):\n                fval = eval_f(X0[i])\n                F0[i] = fval\n                if fval < self.f_opt:\n                    self.f_opt = fval\n                    self.x_opt = X0[i].copy()\n        except StopIteration:\n            # out of budget during initialization\n            return self.f_opt, (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))\n\n        # choose seeds: top-performing unique points (prune near-duplicates)\n        sorted_idx = np.argsort(F0)\n        seeds_X = []\n        seeds_F = []\n        used = []\n        min_sep = 1e-6 + 1e-3 * np.linalg.norm(width)  # minimal separation to treat distinct\n        for idx in sorted_idx:\n            x = X0[idx].copy()\n            if all(np.linalg.norm(x - u) > min_sep for u in used):\n                seeds_X.append(x)\n                seeds_F.append(float(F0[idx]))\n                used.append(x)\n            if len(seeds_X) >= self.max_seeds:\n                break\n        if len(seeds_X) == 0:\n            seeds_X = [X0[sorted_idx[0]].copy()]\n            seeds_F = [float(F0[sorted_idx[0]])]\n\n        n_seeds = len(seeds_X)\n        # per-seed step sizes (scalar)\n        base_step = 0.2 * np.linalg.norm(width) / np.sqrt(self.dim)\n        step_sizes = np.full(n_seeds, base_step)\n\n        # determine subspace dimension\n        if isinstance(self.subspace_scale, int):\n            sdim = max(1, min(self.dim, int(self.subspace_scale)))\n        else:\n            sdim = max(1, min(self.dim, int(np.ceil(np.sqrt(self.dim)))))\n\n        # Archive to avoid duplicate evaluations and to provide local PCA\n        # key by tuple of rounded coords for stability\n        archive = {}\n        def arch_key(x):\n            return tuple(np.round(x, 10))\n        # populate archive with initial samples\n        for i in range(init_budget):\n            archive[arch_key(X0[i])] = float(F0[i])\n\n        # helper: safe evaluate that checks archive first\n        def eval_cached(x):\n            k = arch_key(x)\n            if k in archive:\n                return archive[k]\n            fv = eval_f(x)\n            archive[k] = fv\n            return fv\n\n        # refinement iterations\n        no_improve = 0\n        max_no_improve = 8 + self.dim // 4\n        # desired k-neighbors for edge graph\n        k_neigh = min(3, max(1, n_seeds - 1))\n\n        try:\n            while calls_made < B and no_improve < max_no_improve:\n                improved_round = False\n\n                # 1) Build k-nearest graph (distance on coordinates)\n                pts = np.vstack(seeds_X)\n                dists = np.linalg.norm(pts[:, None, :] - pts[None, :, :], axis=2)\n                # for each seed, get k nearest neighbors (exclude self)\n                neighbors = []\n                for i in range(n_seeds):\n                    order = np.argsort(dists[i])\n                    nb = [j for j in order if j != i][:k_neigh]\n                    neighbors.append(nb)\n\n                # 2) Edge probing: for each edge (i,j) with i<j evaluate midpoint and possibly parabolic optimum\n                visited_edges = set()\n                for i in range(n_seeds):\n                    for j in neighbors[i]:\n                        if calls_made >= B:\n                            break\n                        if i == j:\n                            continue\n                        a, b = (i, j) if i < j else (j, i)\n                        if (a, b) in visited_edges:\n                            continue\n                        visited_edges.add((a, b))\n                        xi = seeds_X[a]\n                        xj = seeds_X[b]\n                        fi = seeds_F[a]\n                        fj = seeds_F[b]\n                        # midpoint\n                        xm = np.clip(0.5 * (xi + xj), lb, ub)\n                        try:\n                            fm = eval_cached(xm)\n                        except StopIteration:\n                            raise\n                        # if midpoint improves over endpoints, attempt parabolic interpolation in t in [0,1]\n                        best_f = fi\n                        best_x = xi.copy()\n                        if fj < best_f:\n                            best_f = fj\n                            best_x = xj.copy()\n                        if fm < best_f:\n                            best_f = fm\n                            best_x = xm.copy()\n                        # if midpoint is better than endpoints, try quadratic fit using t=0,0.5,1\n                        if (fm < fi) or (fm < fj):\n                            # solve quadratic for t using f0=fi, f05=fm, f1=fj\n                            f0, f05, f1 = fi, fm, fj\n                            # fit quadratic coefficients for t in [0,1]\n                            # f(t) = A t^2 + B t + C; f(0)=C=f0; f(1)=A+B+C=f1; f(0.5)=A/4 + B/2 + C = f05\n                            C = f0\n                            B_plus_A = f1 - C\n                            # From f05: A/4 + B/2 = f05 - C\n                            rhs = f05 - C\n                            # Let A = s, B = B_plus_A - s\n                            # then s/4 + (B_plus_A - s)/2 = rhs => s/4 + B_plus_A/2 - s/2 = rhs\n                            # => (-s/4) + B_plus_A/2 = rhs => s/4 = B_plus_A/2 - rhs => s = 4*(B_plus_A/2 - rhs)\n                            s = 4.0 * (B_plus_A * 0.5 - rhs)\n                            A = s\n                            B = B_plus_A - A\n                            # if parabola opens up (A>0) compute minimizer t* = -B/(2A)\n                            if A > 1e-12:\n                                t_star = -B / (2.0 * A)\n                                if 0.0 < t_star < 1.0:\n                                    x_star = np.clip(xi + t_star * (xj - xi), lb, ub)\n                                    try:\n                                        f_star = eval_cached(x_star)\n                                    except StopIteration:\n                                        raise\n                                    if f_star < best_f:\n                                        best_f = f_star\n                                        best_x = x_star.copy()\n                        # If we found improvement along edge, replace worse endpoint with best_x\n                        if best_f < min(fi, fj) - 1e-12:\n                            # pick which endpoint to replace (the worse one)\n                            if fi > fj:\n                                replace_idx = a\n                            else:\n                                replace_idx = b\n                            seeds_X[replace_idx] = best_x.copy()\n                            seeds_F[replace_idx] = float(best_f)\n                            # update step size for that seed: increase modestly\n                            step_sizes[replace_idx] = max(step_sizes[replace_idx], base_step) * 1.15\n                            if best_f < self.f_opt:\n                                self.f_opt = best_f\n                                self.x_opt = best_x.copy()\n                            improved_round = True\n\n                # 3) Per-seed PCA-informed subspace directional searches\n                for idx in np.argsort(seeds_F):\n                    if calls_made >= B:\n                        break\n                    x_curr = seeds_X[idx].copy()\n                    f_curr = seeds_F[idx]\n                    # find nearest archived points for local PCA\n                    # use up to M nearest archive points\n                    if len(archive) >= 3:\n                        keys = np.array(list(archive.keys()))\n                        pts_arr = np.array(keys, dtype=float)\n                        if pts_arr.shape[0] >= 3:\n                            dists_archive = np.linalg.norm(pts_arr - x_curr, axis=1)\n                            nearest_idx = np.argsort(dists_archive)[:max(3, min(50, pts_arr.shape[0]))]\n                            local_pts = pts_arr[nearest_idx]\n                            diffs = local_pts - x_curr\n                            # if there is any variation, compute PCA directions\n                            if diffs.shape[0] >= 2 and np.any(np.linalg.norm(diffs, axis=1) > 0):\n                                # center diffs and compute covariance\n                                D = diffs - diffs.mean(axis=0, keepdims=True)\n                                cov = D.T @ D\n                                try:\n                                    eigvals, eigvecs = np.linalg.eigh(cov)\n                                    # take top sdim directions corresponding to largest eigenvalues\n                                    order = np.argsort(eigvals)[::-1]\n                                    dirs = eigvecs[:, order[:min(sdim, eigvecs.shape[1])]]\n                                except Exception:\n                                    dirs = None\n                            else:\n                                dirs = None\n                        else:\n                            dirs = None\n                    else:\n                        dirs = None\n                    # fallback: random orthonormal directions in full or subspace dims\n                    if dirs is None:\n                        # create random basis of sdim vectors (orthonormal)\n                        A = self.rng.normal(size=(self.dim, sdim))\n                        Q, _ = np.linalg.qr(A)\n                        dirs = Q[:, :sdim]\n\n                    # for each direction vector, do a symmetric parabolic probe around x_curr\n                    local_improved = False\n                    for d in range(dirs.shape[1]):\n                        if calls_made >= B:\n                            break\n                        direction = dirs[:, d]\n                        direction = direction / (np.linalg.norm(direction) + 1e-12)\n                        step = step_sizes[idx]\n                        # points: x - step*dir, x, x + step*dir\n                        xa = np.clip(x_curr - step * direction, lb, ub)\n                        xb = x_curr.copy()  # already known value f_curr\n                        xc = np.clip(x_curr + step * direction, lb, ub)\n                        # evaluate xa and xc (xb value is f_curr)\n                        try:\n                            fa = eval_cached(xa)\n                        except StopIteration:\n                            raise\n                        try:\n                            fc = eval_cached(xc)\n                        except StopIteration:\n                            raise\n                        # pick best among three\n                        f_best = f_curr\n                        x_best = x_curr.copy()\n                        if fa < f_best:\n                            f_best = fa\n                            x_best = xa.copy()\n                        if fc < f_best:\n                            f_best = fc\n                            x_best = xc.copy()\n                        # if best is middle already, but sides lower, try parabolic interpolation along t in [-1,0,1] param\n                        # convert to t in [-1,0,1] with xb at 0: fa at -1, fc at +1\n                        # fit quadratic on (-1,fa),(0,f_curr),(1,fc): f(t)=A t^2 + B t + C; C=f_curr\n                        if (fa < f_curr) or (fc < f_curr):\n                            C = f_curr\n                            # using symmetric points gives B = (fc - fa)/4, A = (fa + fc - 2*C)/2\n                            B = (fc - fa) * 0.25\n                            A = 0.5 * (fa + fc - 2.0 * C)\n                            if A > 1e-12:\n                                t_star = -B / (2.0 * A)\n                                if -1.0 < t_star < 1.0:\n                                    x_star = np.clip(x_curr + t_star * step * direction, lb, ub)\n                                    try:\n                                        f_star = eval_cached(x_star)\n                                    except StopIteration:\n                                        raise\n                                    if f_star < f_best:\n                                        f_best = f_star\n                                        x_best = x_star.copy()\n                        # accept improvement\n                        if f_best < f_curr - 1e-12:\n                            # update seed\n                            seeds_X[idx] = x_best.copy()\n                            seeds_F[idx] = float(f_best)\n                            x_curr = x_best.copy()\n                            f_curr = float(f_best)\n                            # boost step size on success\n                            step_sizes[idx] = step_sizes[idx] * 1.2 + 1e-12\n                            if f_best < self.f_opt:\n                                self.f_opt = f_best\n                                self.x_opt = x_best.copy()\n                            local_improved = True\n                            improved_round = True\n                        else:\n                            # shrink step if no improvement\n                            step_sizes[idx] = max(step_sizes[idx] * 0.75, 1e-8 * np.linalg.norm(width))\n                    # small local jitter attempt\n                    if calls_made < B:\n                        jitter_scale = 0.015 * width\n                        xj = np.clip(x_curr + self.rng.normal(scale=jitter_scale), lb, ub)\n                        try:\n                            fj = eval_cached(xj)\n                        except StopIteration:\n                            raise\n                        if fj < f_curr - 1e-12:\n                            seeds_X[idx] = xj.copy()\n                            seeds_F[idx] = float(fj)\n                            step_sizes[idx] = max(step_sizes[idx], base_step) * 1.1\n                            if fj < self.f_opt:\n                                self.f_opt = fj\n                                self.x_opt = xj.copy()\n                            improved_round = True\n\n                # 4) Recombination and extrapolation: combine top seeds to create new candidates\n                if calls_made < B:\n                    # pick top 3 seeds and produce few children\n                    top_k = min(4, n_seeds)\n                    best_order = np.argsort(seeds_F)[:top_k]\n                    parents = [seeds_X[i] for i in best_order]\n                    for _ in range(2):  # up to 2 recombinations per round\n                        if calls_made >= B:\n                            break\n                        weights = self.rng.random(len(parents))\n                        weights = weights / (weights.sum() + 1e-12)\n                        child = np.zeros(self.dim)\n                        for w, p in zip(weights, parents):\n                            child += w * p\n                        # extrapolate a bit along the weighted sum of (best - mean)\n                        mean_par = np.mean(parents, axis=0)\n                        bestpar = parents[0]\n                        extrap = (bestpar - mean_par)\n                        child = child + 0.08 * extrap + self.rng.normal(scale=0.03 * width)\n                        child = np.clip(child, lb, ub)\n                        try:\n                            fchild = eval_cached(child)\n                        except StopIteration:\n                            raise\n                        # if child improves replace worst seed\n                        worst_idx = int(np.argmax(seeds_F))\n                        if fchild < seeds_F[worst_idx] - 1e-12:\n                            seeds_X[worst_idx] = child.copy()\n                            seeds_F[worst_idx] = float(fchild)\n                            step_sizes[worst_idx] = base_step\n                            if fchild < self.f_opt:\n                                self.f_opt = fchild\n                                self.x_opt = child.copy()\n                            improved_round = True\n\n                # update no_improve counter\n                if improved_round:\n                    no_improve = 0\n                else:\n                    no_improve += 1\n\n        except StopIteration:\n            # budget exhausted; fall through to finalization\n            pass\n\n        # final exploitation: spend small remaining budget sampling around current best and along elite edges\n        try:\n            remaining = max(0, B - calls_made)\n            extra = min(20, remaining)\n            for _ in range(extra):\n                if self.x_opt is None:\n                    sample = lb + self.rng.random(self.dim) * (ub - lb)\n                else:\n                    sample = np.clip(self.x_opt + self.rng.normal(scale=0.01 * width), lb, ub)\n                fv = eval_f(sample)\n                if fv < self.f_opt:\n                    self.f_opt = fv\n                    self.x_opt = sample.copy()\n        except StopIteration:\n            pass\n\n        return self.f_opt, (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))", "configspace": "", "generation": 1, "feedback": "The algorithm MSDirectionalGraphSearch scored 0.009 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0228629a-a7a4-4d75-8105-fe131f1f6e92"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.058770391333148875}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.025660050006189206}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.046232354256257935}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.0}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.0}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.0}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.0}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.0}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.008505701076407801}], "aucs": [0.058770391333148875, 0.025660050006189206, 0.046232354256257935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008505701076407801]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3112.0, "Edges": 3111.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.999357326478149, "Degree Variance": 1.9215934173049347, "Transitivity": 0.0, "Max Depth": 19.0, "Min Depth": 2.0, "Mean Depth": 9.34149855907781, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3329391875168657, "Depth Entropy": 2.3064273622415343, "Assortativity": 0.0, "Average Eccentricity": 20.806233933161955, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.00032133676092544985, "Average Shortest Path": 11.714522190518924, "mean_complexity": 12.571428571428571, "total_complexity": 88.0, "mean_token_count": 380.42857142857144, "total_token_count": 2663.0, "mean_parameter_count": 2.5714285714285716, "total_parameter_count": 18.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "73167bb0-c570-47b5-8cb5-a9c5446068ba", "fitness": 0.1323511225502636, "name": "GAES", "description": "Graph-Adaptive Edge Sampling (GAES) \u2014 maintain a compact population, build a dense k-nearest neighbor graph every generation and generate many edge-based recombinations + locally-covariant samples (plus occasional L\u00e9vy escapes), adapting per-individual step-sizes by success-rate to produce a high \"edge density\" of candidate proposals.", "code": "import numpy as np\n\nclass GAES:\n    \"\"\"\n    Graph-Adaptive Edge Sampling (GAES)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional keyword args:\n      pop_size: population size (default heuristic from dim and budget)\n      k_neighbors: neighbors per node in graph (default dense)\n      cand_per_edge: number of candidates generated per node-edge\n      init_rand: number of initial uniform samples (overrides pop_size if given)\n      levy_prob: probability of a global L\u00e9vy escape each trial\n      stagnation_restart: number of generations without improvement before stronger diversification\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 pop_size=None, k_neighbors=None, cand_per_node=None,\n                 init_rand=None, levy_prob=0.04, stagnation_restart=30):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # population size heuristic: limited but scales with dim\n        if pop_size is None:\n            self.pop_size = int(np.clip(2 * self.dim + 4, 8, 40))\n            # also constrain by budget (need space to create candidates)\n            self.pop_size = int(min(self.pop_size, max(4, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n\n        # graph neighbors: dense graph -> edge density up\n        if k_neighbors is None:\n            # choose quite dense: half the population\n            self.k_neighbors = max(2, min(self.pop_size - 1, self.pop_size // 2))\n        else:\n            self.k_neighbors = int(min(k_neighbors, self.pop_size - 1))\n\n        # candidates per node per generation\n        if cand_per_node is None:\n            self.cand_per_node = max(2, min(8, 1 + self.dim // 2))\n        else:\n            self.cand_per_node = int(cand_per_node)\n\n        # number of initial random samples (if provided) or equal to pop_size\n        self.init_rand = None if init_rand is None else int(init_rand)\n\n        self.levy_prob = float(levy_prob)\n        self.stagnation_restart = int(stagnation_restart)\n\n        # per-individual adaptation inertia\n        self.ema_alpha = 0.12\n        self.target_success = 0.2\n        # small regularizer for covariances\n        self._cov_reg = 1e-8\n\n    def _levy_vector(self, size):\n        # simple heavy-tailed direction (Cauchy with clipping)\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        # clip extremes to avoid infinities\n        z = np.clip(z, -1e6, 1e6)\n        n = np.linalg.norm(z)\n        if n == 0:\n            z = self.rng.standard_normal(size=size)\n            n = np.linalg.norm(z)\n            if n == 0:\n                return np.ones(size) / np.sqrt(size)\n        return z / n\n\n    def __call__(self, func):\n        # read bounds from func.bounds.lb/ub (support scalar or vector)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        # bookkeeping\n        evals = 0\n        # initial sample count: try to fill population but don't exceed budget\n        if self.init_rand is None:\n            n_init = min(self.pop_size, max(1, budget // 20))  # some budget reserved for search\n            n_init = max(n_init, 4)\n        else:\n            n_init = min(self.init_rand, budget)\n        # prepare population containers\n        pop_size = min(self.pop_size, budget)  # cannot have more individuals than budget\n        X = np.zeros((pop_size, dim), dtype=float)\n        F = np.full(pop_size, np.inf, dtype=float)\n\n        # initial uniform sampling to populate\n        for i in range(pop_size):\n            if evals >= budget:\n                break\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            X[i] = x\n            F[i] = f\n\n        # If some slots remain and budget still available, fill them\n        if evals < budget:\n            for i in range(pop_size):\n                if np.isfinite(F[i]):\n                    continue\n                if evals >= budget:\n                    break\n                x = rng.uniform(lb, ub)\n                f = float(func(x))\n                evals += 1\n                X[i] = x\n                F[i] = f\n\n        # sort population by fitness ascending (best first)\n        order = np.argsort(F)\n        X = X[order]\n        F = F[order]\n\n        # per-individual step-sizes and success EMA\n        range_scale = float(np.mean(ub - lb))\n        sigma = np.full(pop_size, max(1e-8, 0.3 * range_scale / max(1.0, np.sqrt(dim))))  # small adaptive\n        p_succ = np.zeros(pop_size, dtype=float)\n\n        # per-individual covariance approximations (diagonal initially)\n        covs = np.array([np.eye(dim) * ((range_scale / 6.0) ** 2 + 1e-9) for _ in range(pop_size)])\n\n        # best so far\n        f_best = float(F[0])\n        x_best = X[0].copy()\n        gens_since_improve = 0\n\n        # main loop: generate candidates via dense graph edges until budget expended\n        # We'll iterate generation-by-generation; each generation we create up to pop_size * cand_per_node candidates\n        gen = 0\n        while evals < budget:\n            gen += 1\n            # build distance matrix and k-nearest neighbors (dense graph)\n            # compute pairwise squared distances\n            diffs = X[:, None, :] - X[None, :, :]  # shape (pop, pop, dim)\n            d2 = np.einsum('ijk,ijk->ij', diffs, diffs)  # squared distances\n            # avoid self by setting big distance\n            np.fill_diagonal(d2, np.inf)\n            # neighbors indices for each node\n            k = min(self.k_neighbors, pop_size - 1)\n            neigh_idx = np.argpartition(d2, kth=k, axis=1)[:, :k]  # unsorted k nearest\n            # For reproducibility we can sort them by distance\n            # get sorted neighbor lists\n            sorted_neigh = np.zeros_like(neigh_idx)\n            for i in range(pop_size):\n                idx = neigh_idx[i]\n                sort_idx = idx[np.argsort(d2[i, idx])]\n                sorted_neigh[i] = sort_idx\n\n            # candidate generation loop: iterate nodes in shuffled order for fairness\n            nodes = np.arange(pop_size)\n            rng.shuffle(nodes)\n            # each node produces up to cand_per_node candidates (edge-based + local PCA samples)\n            candidates = []\n            cand_from = []  # which node produced\n            for i in nodes:\n                if evals + len(candidates) >= budget:\n                    break\n                neighbors = sorted_neigh[i]\n                if neighbors.size == 0:\n                    continue\n\n                # Compute local neighborhood statistics once\n                local_pts = X[np.concatenate(([i], neighbors))]  # include self and neighbors\n                local_mean = local_pts.mean(axis=0)\n                # local displacements for covariance/pca\n                disps = (local_pts - local_mean)\n                # local covariance (small regularization)\n                cov_local = (disps.T @ disps) / max(1.0, disps.shape[0]) + self._cov_reg * np.eye(dim)\n                # SVD/PCA to get principal directions (for subspace sampling)\n                try:\n                    U, S, _ = np.linalg.svd(cov_local, full_matrices=False)\n                except np.linalg.LinAlgError:\n                    U = np.eye(dim)\n                    S = np.ones(dim) * ( (range_scale/6.0) ** 2 )\n\n                top_k = max(1, min(dim, 1 + dim // 4))\n                principal = U[:, :top_k]\n                principal_var = S[:top_k]\n\n                # number of edge neighbors to use: take a subset to encourage diversity\n                use_neighbors = neighbors if neighbors.size <= 6 else neighbors[rng.choice(neighbors.size, size=6, replace=False)]\n\n                # generate edge-based candidates (convex combos + local noise)\n                for nb in use_neighbors:\n                    if len(candidates) + evals >= budget:\n                        break\n                    # alpha biased towards small moves (exploitation) but sometimes uniform\n                    if rng.random() < 0.7:\n                        alpha = rng.beta(1.2, 3.0)  # more weight near 0\n                    else:\n                        alpha = rng.random()\n                    x_i = X[i]\n                    x_j = X[nb]\n                    edge_mid = x_i + alpha * (x_j - x_i)\n                    # shape noise: combine local covariance and per-individual sigma\n                    # draw gaussian in principal subspace to favor principal modes\n                    coeffs = rng.normal(scale=np.sqrt(principal_var))\n                    noise_sub = principal @ coeffs\n                    # small isotropic component\n                    noise_iso = rng.normal(size=dim) * (0.4 * sigma[i])\n                    x_cand = edge_mid + sigma[i] * 0.8 * noise_sub + noise_iso\n\n                    # occasional slight perpendicular perturbation using neighbor difference\n                    if rng.random() < 0.15:\n                        perp = (x_j - x_i) - np.dot(x_j - x_i, principal[:, 0]) * principal[:, 0]\n                        if np.linalg.norm(perp) > 0:\n                            perp = perp / (np.linalg.norm(perp) + 1e-12)\n                            x_cand = x_cand + 0.5 * sigma[i] * perp * rng.normal()\n\n                    # reflection + clip into bounds\n                    x_cand = np.where(x_cand < lb, lb + (lb - x_cand), x_cand)\n                    x_cand = np.where(x_cand > ub, ub - (x_cand - ub), x_cand)\n                    x_cand = np.clip(x_cand, lb, ub)\n\n                    candidates.append(x_cand)\n                    cand_from.append(i)\n\n                # also generate a local PCA-dominated sample around current node\n                if len(candidates) + evals >= budget:\n                    break\n                # sample in top-2 principal subspace with amplified variance\n                mix = np.zeros(dim)\n                # sample coefficients proportional to principal variances\n                coeffs2 = rng.normal(scale=np.sqrt(principal_var))\n                mix += principal @ coeffs2\n                # isotropic part\n                mix += rng.normal(size=dim) * (0.3 * sigma[i])\n                x_local = X[i] + 0.9 * sigma[i] * mix\n                x_local = np.clip(x_local, lb, ub)\n                candidates.append(x_local)\n                cand_from.append(i)\n\n                # occasionally produce a small opposite-sample to increase edge coverage\n                if rng.random() < 0.12 and len(candidates) + evals < budget:\n                    opp = lb + ub - X[i] + rng.normal(scale=0.5 * sigma[i], size=dim)\n                    opp = np.clip(opp, lb, ub)\n                    candidates.append(opp)\n                    cand_from.append(i)\n\n            # Add occasional global L\u00e9vy escapes (not many, but increases long edges)\n            if rng.random() < self.levy_prob and evals + len(candidates) < budget:\n                lv = self._levy_vector(dim) * (2.5 * range_scale)\n                cand = x_best + lv\n                cand = np.clip(cand, lb, ub)\n                candidates.append(cand)\n                cand_from.append(0)\n\n            # Evaluate candidate batch (in order), respecting budget\n            # Convert to array for efficient processing\n            if len(candidates) == 0:\n                # fallback: random uniform exploration\n                if evals < budget:\n                    x_try = rng.uniform(lb, ub)\n                    f_try = float(func(x_try))\n                    evals += 1\n                    # replace worst if better\n                    if f_try < F[-1]:\n                        X[-1] = x_try\n                        F[-1] = f_try\n                        # keep sorted\n                        order = np.argsort(F)\n                        X = X[order]\n                        F = F[order]\n                        # update best\n                        if F[0] < f_best:\n                            f_best = float(F[0])\n                            x_best = X[0].copy()\n                            gens_since_improve = 0\n                    continue\n                else:\n                    break\n\n            cand_arr = np.array(candidates, dtype=float)\n            n_cand = cand_arr.shape[0]\n            # ensure we don't exceed budget\n            max_eval = min(n_cand, budget - evals)\n            # evaluate sequentially with possibility to break early\n            for idx in range(max_eval):\n                x_try = cand_arr[idx]\n                f_try = float(func(x_try))\n                evals += 1\n                src = cand_from[idx]\n                # Replacement policy: if better than worst, insert and keep elitism\n                if f_try < F[-1]:\n                    X[-1] = x_try\n                    F[-1] = f_try\n                    # sort to maintain best-first ordering\n                    order = np.argsort(F)\n                    X = X[order]\n                    F = F[order]\n                    # update per-individual sigma and success EMA for source individual\n                    # find new index of src after sorting: assume best positions changed, but we update by matching by proximity\n                    # Simpler: update sigma and EMA for the source index slot (src might have moved), so we update the source's tracked sigma/p_succ\n                    # But since we keep sigma/p_succ arrays aligned with X rows, we need to also reorder them on population sorts.\n                    # To keep consistency, maintain sigma and p_succ alongside X and reorder now.\n                    # Build arrays if not matching shapes\n                    # Reorder sigma and p_succ in same way\n                    if 'order' in locals():\n                        sigma = sigma[order]\n                        p_succ = p_succ[order]\n                        covs = covs[order]\n                    # mark success for the producing individual (we keep producers approx by index src if src < pop_size)\n                    # To be robust, map producer to nearest current individual\n                    # We'll treat producer success as global boost to sigma of best individuals\n                    p_succ[0] = (1.0 - self.ema_alpha) * p_succ[0] + self.ema_alpha * 1.0\n                    sigma[0] = min(sigma[0] * 1.15, 8.0 * range_scale)\n                else:\n                    # unsuccessful: decay some of the producing individual's sigma slowly\n                    if 0 <= src < pop_size:\n                        p_succ[src] = (1.0 - self.ema_alpha) * p_succ[src] + self.ema_alpha * 0.0\n                        sigma[src] = max(sigma[src] * 0.985, 1e-10)\n\n                # Update global best if improved\n                if f_try < f_best:\n                    f_best = float(f_try)\n                    x_best = x_try.copy()\n                    gens_since_improve = 0\n                # break if budget exhausted\n                if evals >= budget:\n                    break\n\n            # adapt sigmas for all individuals based on EMA success (multiplicative update towards target)\n            # moderate scaling factor based on dimension\n            scale = np.exp(0.7 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n            scale = np.clip(scale, 0.7, 1.5)\n            sigma = np.clip(sigma * scale, 1e-12, 12.0 * range_scale)\n\n            # Occasionally (if stagnation) perform stronger diversification using random restarts\n            if f_best < np.inf:\n                if gens_since_improve >= self.stagnation_restart:\n                    # perform a few random global samples\n                    restarts = min(6, max(1, (budget - evals) // 4))\n                    for _ in range(restarts):\n                        if evals >= budget:\n                            break\n                        if rng.random() < 0.5:\n                            x_r = rng.uniform(lb, ub)\n                        else:\n                            # sample near a random individual\n                            center = X[rng.integers(pop_size)]\n                            x_r = center + rng.normal(scale=1.5 * np.mean(sigma))  # larger spread\n                        x_r = np.clip(x_r, lb, ub)\n                        f_r = float(func(x_r))\n                        evals += 1\n                        if f_r < F[-1]:\n                            X[-1] = x_r\n                            F[-1] = f_r\n                            order = np.argsort(F)\n                            X = X[order]\n                            F = F[order]\n                            sigma = sigma[order]\n                            p_succ = p_succ[order]\n                            covs = covs[order]\n                        if f_r < f_best:\n                            f_best = float(f_r)\n                            x_best = x_r.copy()\n                            gens_since_improve = 0\n                    # escalate jump probability temporarily by injecting a L\u00e9vy sample near best\n                    if evals < budget:\n                        lv = self._levy_vector(dim) * (3.5 * range_scale)\n                        cand = x_best + lv\n                        cand = np.clip(cand, lb, ub)\n                        f_lv = float(func(cand))\n                        evals += 1\n                        if f_lv < F[-1]:\n                            X[-1] = cand\n                            F[-1] = f_lv\n                            order = np.argsort(F)\n                            X = X[order]\n                            F = F[order]\n                            sigma = sigma[order]\n                            p_succ = p_succ[order]\n                            covs = covs[order]\n                        if f_lv < f_best:\n                            f_best = float(f_lv)\n                            x_best = cand.copy()\n                            gens_since_improve = 0\n                    # reset some sigmas to be more explorative after heavy diversification\n                    sigma = np.clip(sigma * 1.4, 1e-12, 20.0 * range_scale)\n                    gens_since_improve = 0\n                else:\n                    gens_since_improve += 1\n\n            # small covariance adaptation: for top individuals, shrink covariance around them from local neighborhoods\n            for top_i in range(min(4, pop_size)):\n                # local covariance from neighbors\n                neighbors = sorted_neigh[top_i][:max(1, min(6, sorted_neigh.shape[1]))]\n                pts = X[np.concatenate(([top_i], neighbors))]\n                cov_local = np.cov(pts.T) + 1e-12 * np.eye(dim)\n                # update stored cov with small learning rate\n                covs[top_i] = 0.85 * covs[top_i] + 0.15 * cov_local\n\n            # if budget nearly exhausted, break\n            if evals >= budget:\n                break\n\n        # return best found\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 1, "feedback": "The algorithm GAES scored 0.132 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5385913284495948}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.42946237328653947}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.47564692416760745}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.031111846891000994}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.04236087085980522}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.026792849510937078}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.023615870405811035}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.030419838769014107}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.036204522919862536}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.12153932490623143}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.14135862288216505}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.08801246520538464}], "aucs": [0.5385913284495948, 0.42946237328653947, 0.47564692416760745, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.031111846891000994, 0.04236087085980522, 0.026792849510937078, 0.023615870405811035, 0.030419838769014107, 0.036204522919862536, 0.12153932490623143, 0.14135862288216505, 0.08801246520538464]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3123.0, "Edges": 3122.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9993595901376882, "Degree Variance": 2.1869992696702774, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.453781512605042, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3294687281677462, "Depth Entropy": 2.108141360222697, "Assortativity": 0.0, "Average Eccentricity": 18.45244956772334, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003202049311559398, "Average Shortest Path": 10.625536025311163, "mean_complexity": 19.666666666666668, "total_complexity": 59.0, "mean_token_count": 882.3333333333334, "total_token_count": 2647.0, "mean_parameter_count": 4.666666666666667, "total_parameter_count": 14.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "475bfe2b-859b-4964-b643-0bb3e2f0745a", "fitness": 0.09040223831155535, "name": "ACLS_EdgeFusion", "description": "Adaptive Covariance-Step with Edge-Dense Recombination (ACLS-EdgeFusion) \u2014 maintain a compact elite archive and densely sample along edges between elites (convex/extended combinations) while mixing covariance-shaped mutations, L\u00e9vy escapes, and adaptive step-size/covariance updates to increase graph connectivity and escape local minima.", "code": "import numpy as np\n\nclass ACLS_EdgeFusion:\n    \"\"\"\n    Adaptive Covariance-Step with Edge-Dense Recombination (ACLS-EdgeFusion).\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional keyword arguments tune behaviour (see defaults).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_fraction=0.12, jump_prob=0.04, target_success=0.2,\n                 cov_update_rate=None, sigma_initial=None,\n                 archive_size=None, edge_samples=6, global_prob=0.03,\n                 stagnation_restart=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # meta-parameters\n        self.init_fraction = float(init_fraction)\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n\n        # covariance learning rate (smaller for larger dims)\n        if cov_update_rate is None:\n            self.c_cov = 0.25 / max(1.0, (self.dim + 2.0))\n        else:\n            self.c_cov = float(cov_update_rate)\n\n        self.sigma_initial = sigma_initial\n        self.edge_samples = int(edge_samples)\n        self.global_prob = float(global_prob)\n\n        if archive_size is None:\n            self.archive_size = max(6, min(40, 4 + int(self.dim)))\n        else:\n            self.archive_size = int(archive_size)\n\n        if stagnation_restart is None:\n            self.stagnation_restart = max(200, int(5 * self.dim))\n        else:\n            self.stagnation_restart = int(stagnation_restart)\n\n    def _levy_vector(self, size):\n        # heavy-tailed direction (Cauchy-like normalized)\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        nrm = np.linalg.norm(z)\n        if nrm == 0:\n            return self.rng.standard_normal(size=size)\n        return z / nrm\n\n    def _reflect(self, x, lb, ub):\n        # simple reflection for bound handling\n        below = x < lb\n        x = np.where(below, lb + (lb - x), x)\n        above = x > ub\n        x = np.where(above, ub - (x - ub), x)\n        # final clip for numerical safety\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        # get bounds (support scalar or vector)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial sampling\n        n_init = max(1, int(min(max(12, 3 * dim), max(1, int(budget * self.init_fraction)))))\n        n_init = min(n_init, budget)\n        archive_x = []\n        archive_f = []\n\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            # maintain best\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n            # archive insert\n            archive_x.append(x.copy())\n            archive_f.append(float(f))\n\n        # turn archive into arrays sorted by fitness\n        if len(archive_f) > 0:\n            order = np.argsort(archive_f)\n            archive_x = [archive_x[i] for i in order]\n            archive_f = [archive_f[i] for i in order]\n            # keep limited size\n            archive_x = archive_x[:self.archive_size]\n            archive_f = archive_f[:self.archive_size]\n        else:\n            archive_x = []\n            archive_f = []\n\n        # scale and initial sigma\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma_initial is None:\n            sigma = max(1e-9, 0.2 * range_scale)\n        else:\n            sigma = float(self.sigma_initial)\n\n        # covariance initialization (diagonal moderate)\n        C = np.eye(dim) * ((range_scale / 4.0) ** 2 + 1e-9)\n\n        # success EMA for step-size adaptation\n        p_succ = 0.0\n        ema_alpha = 0.12\n\n        since_improvement = 0\n        iter_count = 0\n\n        # helper to insert into archive\n        def archive_insert(x, f):\n            nonlocal archive_x, archive_f\n            # if archive not full, append\n            if len(archive_f) < self.archive_size:\n                archive_x.append(x.copy())\n                archive_f.append(float(f))\n            else:\n                # if better than worst, replace and keep sorted\n                worst_idx = int(np.argmax(archive_f))\n                if f < archive_f[worst_idx]:\n                    archive_x[worst_idx] = x.copy()\n                    archive_f[worst_idx] = float(f)\n            # sort archive by fitness ascending\n            if len(archive_f) > 1:\n                order = np.argsort(archive_f)\n                archive_x = [archive_x[i] for i in order]\n                archive_f = [archive_f[i] for i in order]\n\n        # main loop: generate candidate-by-candidate ensuring not to exceed budget\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n\n            # precompute cholesky of C with regularization\n            reg = 1e-8 * np.eye(dim)\n            try:\n                L = np.linalg.cholesky(C + reg)\n            except np.linalg.LinAlgError:\n                L = np.linalg.cholesky(C + (1e-6 * np.eye(dim)))\n\n            # choose operation probabilities adaptively:\n            # - edge recombination if archive has at least 2 entries\n            # - covariance mutation (from best) if we have best\n            # - Levy jumps occasionally\n            # - global random occasionally\n            # - opposite sampling occasionally\n            # We'll produce up to a small batch per loop but guard with remaining.\n            batch = min(max(1, 6 + dim // 3), remaining)\n            for _ in range(batch):\n                if remaining <= 0:\n                    break\n\n                op_r = rng.random()\n                x_trial = None\n                parent = None\n\n                # global random exploration\n                if op_r < self.global_prob:\n                    x_trial = rng.uniform(lb, ub)\n                    parent = None\n\n                # heavy-tailed Levy escapes (rare)\n                elif op_r < self.global_prob + self.jump_prob:\n                    z = self._levy_vector(dim)\n                    # amplify and shape a bit by covariance L\n                    dx = L.dot(z) * (2.5 + rng.random() * 2.0)\n                    if x_opt is None:\n                        center = rng.uniform(lb, ub)\n                    else:\n                        center = x_opt\n                    x_trial = self._reflect(center + sigma * dx, lb, ub)\n                    parent = center.copy()\n\n                # edge-dense recombination (if archive has enough)\n                elif len(archive_x) >= 2 and op_r < 0.75:\n                    # pick two elites with bias towards better ones\n                    # use fitness-based probability: translate to weights\n                    n_el = len(archive_x)\n                    idxs = rng.choice(n_el, size=2, replace=False, p=None)\n                    a = np.asarray(archive_x[idxs[0]])\n                    b = np.asarray(archive_x[idxs[1]])\n                    # sample along extended edge (alpha in [-0.3, 1.3])\n                    alpha = rng.uniform(-0.3, 1.3)\n                    edge_point = a + alpha * (b - a)\n                    # add covariance-shaped noise but smaller than pure mutation to stay on edge\n                    z = rng.normal(size=dim)\n                    dx = L.dot(z) * (0.6 + 0.8 * rng.random())\n                    # also add small directional push along edge direction to increase connectivity\n                    edge_dir = b - a\n                    ed_norm = np.linalg.norm(edge_dir)\n                    if ed_norm > 0:\n                        edge_dir = edge_dir / ed_norm\n                        dx += edge_dir * (0.2 * rng.normal())\n                    x_trial = self._reflect(edge_point + sigma * dx, lb, ub)\n                    parent = (a.copy(), b.copy())\n\n                # covariance-shaped local mutation around best / elites\n                else:\n                    # choose center: mostly x_opt, sometimes a random elite\n                    if x_opt is None or (len(archive_x) > 0 and rng.random() < 0.15):\n                        center = rng.choice(archive_x) if len(archive_x) > 0 else rng.uniform(lb, ub)\n                    else:\n                        center = x_opt\n                    z = rng.normal(size=dim)\n                    dx = L.dot(z)\n                    # small elongation toward best direction if archive exists\n                    if x_opt is not None and center is not None:\n                        center = np.asarray(center)\n                        # occasionally bias toward best\n                        if rng.random() < 0.25:\n                            bias = (np.asarray(x_opt) - center)\n                            bnorm = np.linalg.norm(bias)\n                            if bnorm > 0:\n                                dx += (bias / bnorm) * (0.2 * rng.random())\n                    x_trial = self._reflect(center + sigma * dx, lb, ub)\n                    parent = center.copy() if center is not None else None\n\n                # one eval if still allowed\n                if evals >= budget:\n                    break\n                f_trial = func(x_trial)\n                evals += 1\n                remaining -= 1\n\n                improved = False\n                # update best/archive\n                if f_trial < f_opt:\n                    improved = True\n                    f_opt = float(f_trial)\n                    x_opt = x_trial.copy()\n                    since_improvement = 0\n                else:\n                    since_improvement += 1\n\n                archive_insert(x_trial, f_trial)\n\n                # covariance & C update based on successful step if parent known\n                if improved and parent is not None:\n                    # compute displacement y in normalized form\n                    if isinstance(parent, tuple):\n                        # parent was edge (a,b): use displacement relative to mid of parents\n                        center_parent = 0.5 * (parent[0] + parent[1])\n                    else:\n                        center_parent = np.asarray(parent)\n                    y = (x_trial - center_parent).reshape(-1, 1)\n                    # rank-one update from successful displacement (like CMA)\n                    C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                    # tiny regularization\n                    C += 1e-12 * np.eye(dim)\n                else:\n                    # small covariance diffusion to avoid degeneracy\n                    C += 1e-14 * np.eye(dim)\n\n                # occasionally mix in covariance built from elite-edge differences to increase edge density\n                if iter_count % max(8, dim // 2) == 0 and len(archive_x) >= 3:\n                    # collect a few random differences among elites\n                    sample_pairs = min(40, len(archive_x) * (len(archive_x) - 1) // 2)\n                    diffs = []\n                    idxs = np.arange(len(archive_x))\n                    # sample random pairs\n                    for _ in range(sample_pairs):\n                        i, j = rng.choice(idxs, size=2, replace=False)\n                        diffs.append(np.asarray(archive_x[i]) - np.asarray(archive_x[j]))\n                    diffs = np.array(diffs)\n                    if diffs.size > 0:\n                        cov_edges = np.cov(diffs.T) if diffs.shape[0] > 1 else np.atleast_2d(diffs).T @ np.atleast_2d(diffs) / diffs.shape[0]\n                        mix = min(0.3, 0.15 + 0.1 * rng.random())\n                        C = (1.0 - mix) * C + mix * (cov_edges + 1e-12 * np.eye(dim))\n\n                # EMA success\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n\n                # adaptive step-size multiplicative control with gentle clipping\n                adjust = np.exp(0.7 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                sigma *= float(np.clip(adjust, 0.7, 1.45))\n                sigma = np.clip(sigma, 1e-10 * range_scale + 1e-12, 6.0 * range_scale + 1e-12)\n\n                # if improved frequently, slightly reduce covariance learning rate to stabilize\n                if improved:\n                    self.c_cov = max(1e-4, self.c_cov * 0.995)\n                else:\n                    self.c_cov = min(0.5, self.c_cov * 1.001)\n\n                # Early small local injection: if archive top two are close but stagnating, sample midpoints\n                if (iter_count % 25 == 0) and (len(archive_x) >= 2) and since_improvement > max(5, dim):\n                    a = np.asarray(archive_x[0])\n                    b = np.asarray(archive_x[1])\n                    mid = 0.5 * (a + b)\n                    z = rng.normal(size=dim)\n                    dx = L.dot(z) * 0.5\n                    x_m = self._reflect(mid + sigma * dx, lb, ub)\n                    if evals < budget:\n                        f_m = func(x_m)\n                        evals += 1\n                        archive_insert(x_m, f_m)\n                        if f_m < f_opt:\n                            f_opt = float(f_m)\n                            x_opt = x_m.copy()\n                            since_improvement = 0\n                            p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha\n                            # modest covariance update\n                            y = (x_m - mid).reshape(-1,1)\n                            C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n\n                # safety break if budget used\n                if evals >= budget:\n                    break\n\n            # stagnation handling: if prolonged no improvements, intensify edge sampling and escapes\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # temporarily increase jump prob and sample many edges / global points\n                old_jump = self.jump_prob\n                self.jump_prob = min(0.6, self.jump_prob * 2.5)\n                pushes = min(8 + dim // 2, budget - evals)\n                for _ in range(pushes):\n                    # sample an extended-edge heavy candidate\n                    if len(archive_x) >= 2 and rng.random() < 0.8:\n                        i, j = rng.choice(len(archive_x), size=2, replace=False)\n                        a = np.asarray(archive_x[i]); b = np.asarray(archive_x[j])\n                        alpha = rng.uniform(-0.6, 1.6)\n                        base = a + alpha * (b - a)\n                        z = self._levy_vector(dim)\n                        dx = L.dot(z) * (1.8 + rng.random() * 2.0)\n                        x_try = self._reflect(base + sigma * dx, lb, ub)\n                    else:\n                        # broad random restart around best\n                        center = x_opt if x_opt is not None and rng.random() < 0.8 else rng.uniform(lb, ub)\n                        z = rng.normal(size=dim)\n                        x_try = self._reflect(center + (2.0 * sigma) * (L.dot(z)), lb, ub)\n\n                    if evals >= budget:\n                        break\n                    f_try = func(x_try)\n                    evals += 1\n                    archive_insert(x_try, f_try)\n                    if f_try < f_opt:\n                        f_opt = float(f_try)\n                        x_opt = x_try.copy()\n                        since_improvement = 0\n                        # cool down jump prob and shrink sigma a bit\n                        self.jump_prob = max(0.01, old_jump)\n                        sigma = max(1e-9, sigma * 0.7)\n                        break\n                # restore jump prob (or keep moderated)\n                self.jump_prob = max(0.01, min(self.jump_prob, 0.5, old_jump * 1.3))\n\n                # expand covariance slightly to encourage escape\n                C = C + (0.5 * np.eye(dim) * ((range_scale / 6.0) ** 2))\n\n                # if still no improvement, perform a directed recombination of many edges\n                if since_improvement >= self.stagnation_restart and evals < budget:\n                    edge_budget = min(budget - evals, 4 + self.edge_samples)\n                    # sample many quick edge-midpoints with small perturbation\n                    for _ in range(edge_budget):\n                        i, j = rng.choice(len(archive_x), size=2, replace=False)\n                        a = np.asarray(archive_x[i]); b = np.asarray(archive_x[j])\n                        alpha = rng.uniform(-0.2, 1.2)\n                        base = a + alpha * (b - a)\n                        z = rng.normal(size=dim)\n                        x_try = self._reflect(base + 0.8 * sigma * L.dot(z), lb, ub)\n                        if evals >= budget:\n                            break\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_insert(x_try, f_try)\n                        if f_try < f_opt:\n                            f_opt = float(f_try)\n                            x_opt = x_try.copy()\n                            since_improvement = 0\n                            break\n\n        # final result\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 2, "feedback": "The algorithm ACLS_EdgeFusion scored 0.090 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.3294450539792103}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.3802383012402075}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.24265024949879965}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.07121068710194611}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.034567768104786456}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0008397539647019281}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0015194967274811955}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.049589454472090844}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.11101208594705025}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.07273842209926784}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.062022301537788316}], "aucs": [0.3294450539792103, 0.3802383012402075, 0.24265024949879965, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.07121068710194611, 0.034567768104786456, 0.0008397539647019281, 0.0015194967274811955, 4.999999999999449e-05, 0.049589454472090844, 0.11101208594705025, 0.07273842209926784, 0.062022301537788316]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3131.0, "Edges": 3130.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9993612264452252, "Degree Variance": 2.106036002060968, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.991602519244227, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3298098716671576, "Depth Entropy": 2.2786288177029577, "Assortativity": 0.0, "Average Eccentricity": 19.286170552539126, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00031938677738741617, "Average Shortest Path": 11.350972599063473, "mean_complexity": 14.6, "total_complexity": 73.0, "mean_token_count": 556.6, "total_token_count": 2783.0, "mean_parameter_count": 4.6, "total_parameter_count": 23.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "a42735ff-2377-459b-a076-b96c4c1a1637", "fitness": 0.11617362417628509, "name": "EdgeDenseMSDirectionalSubspaceSearch", "description": "Edge-Dense Multi-Scale Directional Subspace Search (ED-MS-DSS) \u2014 combine multi-scale directional local refinements with a dense graph of edge-based recombinations and covariant edge-aligned proposals to raise edge density and improve escape/connection between basins.", "code": "import numpy as np\n\nclass EdgeDenseMSDirectionalSubspaceSearch:\n    \"\"\"\n    Edge-Dense Multi-Scale Directional Subspace Search (ED-MS-DSS)\n\n    Main ideas:\n    - Low-discrepancy LHS initialization to seed a compact elite set.\n    - Per-seed adaptive directional line-searches in random low-dimensional subspaces.\n    - Maintain per-seed trust radii adapted by success/failure rates.\n    - Build a k-nearest-neighbor graph on seeds and generate many edge-based recombinations:\n      midpoints, convex mixes, extrapolations, and Gaussian perturbations aligned with edges.\n    - Maintain a small archive of recent successful steps to form a low-rank covariance for local proposals.\n    - When stagnation occurs, perform aggressive edge-dense recombination among many seed pairs.\n    - All proposals respect box bounds via clipping and budget is strictly enforced.\n    \"\"\"\n\n    def __init__(self, budget, dim, rng_seed=None,\n                 init_frac=0.12, max_seeds=16, subspace_scale='sqrt',\n                 k_neighbors=3, edge_offspring=6, success_archive=32):\n        \"\"\"\n        budget: total allowed function evaluations (int)\n        dim: problem dimensionality (int)\n        rng_seed: RNG seed\n        init_frac: fraction of budget devoted to initialization sampling (0..1)\n        max_seeds: max number of seeds to refine\n        subspace_scale: 'sqrt' or int subspace dimension\n        k_neighbors: number of neighbors per seed for edge graph\n        edge_offspring: number of offspring per edge when doing edge recombination\n        success_archive: how many recent successful deltas to retain for covariance\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(rng_seed)\n        self.init_frac = float(init_frac)\n        self.max_seeds = int(max_seeds)\n        self.subspace_scale = subspace_scale\n        self.k_neighbors = max(1, int(k_neighbors))\n        self.edge_offspring = max(1, int(edge_offspring))\n        self.success_archive = max(1, int(success_archive))\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        lb = lb.reshape(-1)[:self.dim]\n        ub = ub.reshape(-1)[:self.dim]\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _lhs(self, n, lb, ub):\n        # simple LHS with random jitter (low-discrepancy-ish)\n        n = max(1, int(n))\n        dim = self.dim\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.random((n, dim))\n        pts = np.zeros((n, dim))\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            pts[:, j] = (perm + u[:, j]) / n\n        return lb + pts * (ub - lb)\n\n    def __call__(self, func):\n        calls_made = 0\n        B = self.budget\n\n        def eval_f(x):\n            nonlocal calls_made\n            if calls_made >= B:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            calls_made += 1\n            return float(func(x))\n\n        lb, ub = self._get_bounds(func)\n        width = ub - lb\n        mean_scale = width.mean()\n\n        # Initialize best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # initial sampling: reserve a fraction of budget but at least dim*6 samples\n        init_budget = max(10 * self.dim, int(self.init_frac * B))\n        init_budget = min(init_budget, B // 3)  # leave room for refinement\n        init_budget = max(init_budget, min(B, 6 * self.dim, 200))\n        init_budget = int(min(init_budget, B - 1))\n        try:\n            X0 = self._lhs(init_budget, lb, ub)\n            F0 = np.empty(init_budget, dtype=float)\n            for i in range(init_budget):\n                fval = eval_f(X0[i])\n                F0[i] = fval\n                if fval < self.f_opt:\n                    self.f_opt = fval\n                    self.x_opt = X0[i].copy()\n        except StopIteration:\n            return self.f_opt, (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))\n\n        # build seeds: pick best distinct seeds (avoid duplicates by threshold)\n        idx_sorted = np.argsort(F0)\n        seeds = []\n        seen = []\n        dist_tol = 1e-6 * mean_scale + 1e-8\n        for i in idx_sorted:\n            x = X0[i].copy()\n            if all(np.linalg.norm(x - s) > dist_tol for s in seen):\n                seeds.append((x, float(F0[i])))\n                seen.append(x)\n            if len(seeds) >= self.max_seeds:\n                break\n        if len(seeds) == 0:\n            # fallback\n            seeds = [(X0[idx_sorted[0]].copy(), float(F0[idx_sorted[0]]))]\n\n        seeds_X = [s[0] for s in seeds]\n        seeds_F = [s[1] for s in seeds]\n        n_seeds = len(seeds_X)\n\n        # per-seed trust radii\n        base_radius = 0.25 * np.linalg.norm(width) / np.sqrt(self.dim)\n        radii = np.full(n_seeds, base_radius, dtype=float)\n        # per-seed success counters for adaptive radius\n        success_counts = np.zeros(n_seeds, dtype=int)\n        fail_counts = np.zeros(n_seeds, dtype=int)\n\n        # subspace dimension\n        if isinstance(self.subspace_scale, int):\n            sdim = max(1, min(self.dim, int(self.subspace_scale)))\n        else:\n            sdim = max(1, min(self.dim, int(np.ceil(np.sqrt(self.dim)))))\n\n        # success archive for covariance (store deltas)\n        success_deltas = []\n\n        # stagnation control\n        no_improve_rounds = 0\n        max_no_improve = 8 + self.dim // 4\n\n        try:\n            while calls_made < B and no_improve_rounds < max_no_improve:\n                improved_any = False\n\n                # 1) Per-seed directional subspace refinement (greedy)\n                order = np.argsort(seeds_F)\n                for idx in order:\n                    if calls_made >= B:\n                        break\n                    x_curr = seeds_X[idx].copy()\n                    f_curr = seeds_F[idx]\n                    radius = radii[idx]\n\n                    # select subspace\n                    if sdim < self.dim:\n                        sub_idx = self.rng.choice(self.dim, size=sdim, replace=False)\n                    else:\n                        sub_idx = np.arange(self.dim)\n                    # try multiple orthogonal random directions in subspace\n                    n_dirs = 1 + int(np.ceil(np.log2(1 + radius / (1e-8 + mean_scale * 1e-6))))\n                    improved_local = False\n                    for d_iter in range(n_dirs):\n                        if calls_made >= B:\n                            break\n                        v = self.rng.normal(size=sub_idx.shape[0])\n                        nv = np.linalg.norm(v)\n                        if nv == 0:\n                            v = np.ones_like(v) / np.sqrt(len(v))\n                        else:\n                            v = v / nv\n                        direction = np.zeros(self.dim, dtype=float)\n                        direction[sub_idx] = v\n\n                        # adaptive line search: try multiple step scales\n                        step = radius\n                        shrink = 0.5\n                        min_step = 1e-6 * mean_scale + 1e-9\n                        inner = 0\n                        max_inner = 6\n                        while inner < max_inner and calls_made < B and step > min_step:\n                            inner += 1\n                            # propose + and - along direction\n                            for sign in (+1.0, -1.0):\n                                cand = np.clip(x_curr + sign * step * direction, lb, ub)\n                                f_cand = eval_f(cand)\n                                if f_cand < f_curr - 1e-12:\n                                    # success: accept and continue from new center\n                                    delta = cand - x_curr\n                                    x_curr = cand\n                                    f_curr = f_cand\n                                    improved_local = True\n                                    # record success delta\n                                    if len(success_deltas) >= self.success_archive:\n                                        success_deltas.pop(0)\n                                    success_deltas.append(delta.copy())\n                                    # update global best\n                                    if f_cand < self.f_opt:\n                                        self.f_opt = f_cand\n                                        self.x_opt = cand.copy()\n                                    # enlarge radius gently\n                                    radii[idx] = min(radii[idx] * 1.15 + 1e-12, max(5.0 * base_radius, 2.0 * radii[idx]))\n                                    success_counts[idx] += 1\n                                    fail_counts[idx] = 0\n                                    break  # continue exploring same step\n                            else:\n                                # no improvement on both signs\n                                step *= shrink\n                                fail_counts[idx] += 1\n                                # slight shrink of radius on failures\n                                radii[idx] = max(radii[idx] * (0.98 if fail_counts[idx] > 2 else 0.995), 1e-12)\n                                continue\n                            # if we had a break (success), continue with same step (do not shrink)\n                            continue\n\n                    # small local covariance-based perturbation using success archive (if available)\n                    if len(success_deltas) >= 2 and calls_made < B:\n                        # build low-rank covariance aligned with recent deltas\n                        D = np.vstack(success_deltas[-min(len(success_deltas), 8):])\n                        # covariance in full space: scale from empirical deltas\n                        cov = np.cov(D.T) if D.shape[0] > 1 else np.atleast_2d(D.T @ D)\n                        # regularize\n                        cov = cov + np.eye(self.dim) * (1e-8 * mean_scale**2)\n                        # sample a proposal limited in magnitude by current radius\n                        try:\n                            z = self.rng.multivariate_normal(np.zeros(self.dim), cov * (0.5 * radii[idx]**2))\n                        except Exception:\n                            z = self.rng.normal(scale=0.5 * radii[idx], size=self.dim)\n                        cand = np.clip(x_curr + z, lb, ub)\n                        f_cand = eval_f(cand)\n                        if f_cand < f_curr:\n                            delta = cand - x_curr\n                            if len(success_deltas) >= self.success_archive:\n                                success_deltas.pop(0)\n                            success_deltas.append(delta.copy())\n                            x_curr = cand\n                            f_curr = f_cand\n                            improved_local = True\n                            if f_cand < self.f_opt:\n                                self.f_opt = f_cand\n                                self.x_opt = cand.copy()\n\n                    # update seed record\n                    seeds_X[idx] = x_curr\n                    seeds_F[idx] = f_curr\n                    if improved_local:\n                        improved_any = True\n\n                if calls_made >= B:\n                    break\n\n                # 2) Edge-based recombination: build K-NN graph among seeds and sample along edges\n                # compute pairwise distances\n                Xmat = np.vstack(seeds_X)\n                # compute squared distances efficiently\n                diff = Xmat[:, None, :] - Xmat[None, :, :]\n                d2 = np.sum(diff**2, axis=2)\n                # for each seed pick neighbors\n                for i in range(n_seeds):\n                    if calls_made >= B:\n                        break\n                    # get neighbor indices sorted (exclude self)\n                    neighs = np.argsort(d2[i])\n                    # skip first (self)\n                    neighs = [j for j in neighs if j != i][:self.k_neighbors]\n                    for j in neighs:\n                        if calls_made >= B:\n                            break\n                        a = seeds_X[i]\n                        b = seeds_X[j]\n                        fab = (seeds_F[i] + seeds_F[j]) / 2.0\n                        edge_dir = b - a\n                        norm_edge = np.linalg.norm(edge_dir)\n                        if norm_edge == 0:\n                            # small random edge\n                            edge_dir = self.rng.normal(scale=mean_scale * 0.01, size=self.dim)\n                            norm_edge = np.linalg.norm(edge_dir)\n                        u = edge_dir / (norm_edge + 1e-12)\n                        # produce several offspring along this edge\n                        offspring_per_edge = min(self.edge_offspring, max(1, (B - calls_made) // 5))\n                        for _ in range(offspring_per_edge):\n                            if calls_made >= B:\n                                break\n                            choice = self.rng.random()\n                            if choice < 0.4:\n                                # midpoint + orthogonal small noise\n                                base = 0.5 * (a + b)\n                                orth_noise = self.rng.normal(scale=0.03 * mean_scale, size=self.dim)\n                                # project orth_noise to orthogonal complement of u to create edge-aligned midpoint noise\n                                orth_noise = orth_noise - (orth_noise @ u) * u\n                                child = np.clip(base + orth_noise, lb, ub)\n                            elif choice < 0.7:\n                                # convex combination biased towards better endpoint\n                                # weight favoring best of a,b\n                                if seeds_F[i] < seeds_F[j]:\n                                    better, worse = a, b\n                                else:\n                                    better, worse = b, a\n                                w = 0.6 + 0.4 * self.rng.random()\n                                child = np.clip(w * better + (1 - w) * worse + self.rng.normal(scale=0.02 * mean_scale, size=self.dim), lb, ub)\n                            else:\n                                # extrapolation beyond better endpoint with gaussian along edge\n                                if seeds_F[i] < seeds_F[j]:\n                                    base = a\n                                else:\n                                    base = b\n                                ext = 0.1 + 0.4 * self.rng.random()\n                                child = np.clip(base + ext * norm_edge * u + self.rng.normal(scale=0.03 * mean_scale, size=self.dim), lb, ub)\n                            # evaluate child\n                            f_child = eval_f(child)\n                            # replace worst seed if better\n                            worst_idx = int(np.argmax(seeds_F))\n                            if f_child < seeds_F[worst_idx]:\n                                seeds_X[worst_idx] = child.copy()\n                                seeds_F[worst_idx] = f_child\n                                # inherit radius from parents (average)\n                                radii[worst_idx] = 0.5 * (radii[i] + radii[j])\n                                if f_child < self.f_opt:\n                                    self.f_opt = f_child\n                                    self.x_opt = child.copy()\n                                improved_any = True\n\n                # 3) Global edge-density burst when stagnating: generate many pairwise edge combos\n                if not improved_any or self.rng.random() < 0.25:\n                    # pick pairs among top seeds\n                    top_k = max(2, min(n_seeds, 6))\n                    best_idxs = np.argsort(seeds_F)[:top_k]\n                    pairings = []\n                    # create many unique pairs\n                    for _p in range(min(20, B - calls_made)):  # limit by budget\n                        i, j = self.rng.choice(best_idxs, size=2, replace=False)\n                        pairings.append((i, j))\n                    for (i, j) in pairings:\n                        if calls_made >= B:\n                            break\n                        a = seeds_X[i]\n                        b = seeds_X[j]\n                        edge_dir = b - a\n                        norm_edge = np.linalg.norm(edge_dir)\n                        if norm_edge == 0:\n                            continue\n                        u = edge_dir / (norm_edge + 1e-12)\n                        # create a batch of children for this pair\n                        for _c in range(2):\n                            if calls_made >= B:\n                                break\n                            t = self.rng.random()\n                            # weighted mix with small extrapolation\n                            w = 0.3 + 0.7 * t\n                            base = np.clip(w * a + (1 - w) * b, lb, ub)\n                            noise = self.rng.normal(scale=0.04 * radii[i], size=self.dim)\n                            # align a fraction of noise along edge direction\n                            noise = (noise @ u) * u + 0.3 * (noise - (noise @ u) * u)\n                            child = np.clip(base + noise, lb, ub)\n                            f_child = eval_f(child)\n                            worst_idx = int(np.argmax(seeds_F))\n                            if f_child < seeds_F[worst_idx]:\n                                seeds_X[worst_idx] = child.copy()\n                                seeds_F[worst_idx] = f_child\n                                radii[worst_idx] = 0.5 * (radii[i] + radii[j])\n                                if f_child < self.f_opt:\n                                    self.f_opt = f_child\n                                    self.x_opt = child.copy()\n                                improved_any = True\n\n                # adaptive radius adjustment across seeds based on relative success\n                # seeds with repeated success get slightly larger radii, failing ones shrink\n                for i in range(n_seeds):\n                    if success_counts[i] > 1:\n                        radii[i] = min(radii[i] * (1.05 ** success_counts[i]), base_radius * 5.0)\n                        success_counts[i] = max(0, success_counts[i] - 1)\n                    if fail_counts[i] > 4:\n                        radii[i] = max(radii[i] * 0.85, 1e-12)\n                        fail_counts[i] = max(0, fail_counts[i] - 2)\n\n                if improved_any:\n                    no_improve_rounds = 0\n                else:\n                    no_improve_rounds += 1\n\n        except StopIteration:\n            # budget exhausted\n            pass\n\n        # final exploitation: spend leftover budget on focused local jitter around best and best edges\n        try:\n            extra = max(0, B - calls_made)\n            # try opposites and small Gaussians around current best and around midpoints of best pairs\n            attempts = min(20, extra)\n            best_point = (self.x_opt if self.x_opt is not None else 0.5 * (lb + ub))\n            for t in range(attempts):\n                if calls_made >= B:\n                    break\n                scale = (0.015 + 0.015 * (t / max(1, attempts - 1))) * mean_scale\n                if self.rng.random() < 0.5:\n                    # jitter around best\n                    cand = np.clip(best_point + self.rng.normal(scale=scale, size=self.dim), lb, ub)\n                else:\n                    # midpoint of two random seeds + jitter\n                    if n_seeds >= 2:\n                        i, j = self.rng.choice(n_seeds, size=2, replace=False)\n                        cand = np.clip(0.5 * (seeds_X[i] + seeds_X[j]) + self.rng.normal(scale=scale, size=self.dim), lb, ub)\n                    else:\n                        cand = np.clip(best_point + self.rng.normal(scale=scale, size=self.dim), lb, ub)\n                f_cand = eval_f(cand)\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = cand.copy()\n        except StopIteration:\n            pass\n\n        return self.f_opt, (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))", "configspace": "", "generation": 2, "feedback": "The algorithm EdgeDenseMSDirectionalSubspaceSearch scored 0.116 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0228629a-a7a4-4d75-8105-fe131f1f6e92"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.24954707738365134}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.2607275861094036}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.26873888870867335}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.0006584206997880537}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.05634114725349482}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.04333071781080011}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.05824446831746499}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.02553068681045534}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.032996296169172434}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.03187626254654219}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.16092852611720354}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.21473424397776075}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.3388500407398658}], "aucs": [0.24954707738365134, 0.2607275861094036, 0.26873888870867335, 4.999999999999449e-05, 0.0006584206997880537, 4.999999999999449e-05, 0.05634114725349482, 0.04333071781080011, 0.05824446831746499, 0.02553068681045534, 0.032996296169172434, 0.03187626254654219, 0.16092852611720354, 0.21473424397776075, 0.3388500407398658]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3458.0, "Edges": 3457.0, "Max Degree": 35.0, "Min Degree": 1.0, "Mean Degree": 1.9994216310005783, "Degree Variance": 1.9543085145349919, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 9.69968253968254, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3297915232974191, "Depth Entropy": 2.4390453057521997, "Assortativity": 2.177967296199994e-08, "Average Eccentricity": 20.413533834586467, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.0002891844997108155, "Average Shortest Path": 12.48491280045868, "mean_complexity": 17.0, "total_complexity": 85.0, "mean_token_count": 600.8, "total_token_count": 3004.0, "mean_parameter_count": 3.8, "total_parameter_count": 19.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "71e01481-ec8a-45bb-86e2-7696794eb7a1", "fitness": 0.09928589672464559, "name": "ECAGS", "description": "Edge-Clustered Archive Graph Search (ECAGS) \u2014 maintain a compact elite population plus a bounded archive of all evaluated points, build very dense graphs over the archive to create many virtual edges (including long inter-cluster edges), and sample richly along edges with perpendicular cross-section perturbations and small directional line-searches; adapt per-elite step-sizes by success, and occasionally inject L\u00e9vy escapes.", "code": "import numpy as np\n\nclass ECAGS:\n    \"\"\"\n    Edge-Clustered Archive Graph Search (ECAGS)\n\n    - budget: maximum number of function evaluations\n    - dim: dimensionality\n    Optional kwargs:\n      pop_size: elite population size\n      k_neighbors: neighbors per node when building archive graph\n      cand_per_edge: candidates generated per selected edge\n      archive_cap: maximum archive size to keep for graph construction\n      levy_prob: occasional heavy-tailed jump probability per generation\n      stagnation_restart: gens without improvement before global diversification\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 pop_size=None, k_neighbors=None, cand_per_edge=None,\n                 archive_cap=None, levy_prob=0.05, stagnation_restart=40):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # sensible default elite size (small, focuses resources)\n        if pop_size is None:\n            self.pop_size = int(np.clip(3 + self.dim // 2, 6, 40))\n            self.pop_size = min(self.pop_size, max(4, self.budget // 12))\n        else:\n            self.pop_size = int(pop_size)\n\n        # neighbors to use from archive graph (dense-ish)\n        if k_neighbors is None:\n            self.k_neighbors = max(3, min(30, self.pop_size * 2))\n        else:\n            self.k_neighbors = int(k_neighbors)\n\n        if cand_per_edge is None:\n            self.cand_per_edge = max(2, min(10, 2 + self.dim // 3))\n        else:\n            self.cand_per_edge = int(cand_per_edge)\n\n        # archive cap: keep a bounded history of evaluated points to build dense graphs\n        if archive_cap is None:\n            # allow archive up to a fraction of budget but cap to keep costs manageable\n            self.archive_cap = int(min(800, max(100, self.budget // 6)))\n        else:\n            self.archive_cap = int(archive_cap)\n\n        self.levy_prob = float(levy_prob)\n        self.stagnation_restart = int(stagnation_restart)\n\n        # adaptation hyperparams\n        self.ema_alpha = 0.14\n        self.target_success = 0.25\n        self._cov_reg = 1e-10\n\n    def _levy_vector(self, size):\n        z = self.rng.standard_cauchy(size=size)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        z = np.clip(z, -1e6, 1e6)\n        n = np.linalg.norm(z)\n        if n == 0:\n            z = self.rng.standard_normal(size=size)\n            n = np.linalg.norm(z)\n            if n == 0:\n                return np.ones(size) / np.sqrt(size)\n        return z / n\n\n    def __call__(self, func):\n        # bounds support (scalar or vector)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        # bookkeeping\n        evals = 0\n\n        # initialize elite population with uniform samples (as many as pop_size or until budget)\n        pop_size = min(self.pop_size, max(1, budget))\n        X = np.zeros((pop_size, dim), dtype=float)\n        F = np.full(pop_size, np.inf, dtype=float)\n\n        for i in range(pop_size):\n            if evals >= budget:\n                break\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            X[i] = x\n            F[i] = f\n\n        # sort elite\n        order = np.argsort(F)\n        X = X[order]\n        F = F[order]\n\n        # adaptive sigmas per-elite and success EMA\n        range_scale = float(np.mean(ub - lb))\n        sigma = np.full(pop_size, max(1e-9, 0.35 * range_scale / max(1.0, np.sqrt(dim))))\n        p_succ = np.zeros(pop_size, dtype=float)\n\n        # archive: keep all evaluated points up to archive_cap (includes elite)\n        X_arch = list(X.copy())\n        F_arch = list(F.copy())\n\n        # best so far\n        if np.isfinite(F[0]):\n            f_best = float(F[0])\n            x_best = X[0].copy()\n        else:\n            f_best = np.inf\n            x_best = rng.uniform(lb, ub)\n\n        gens_since_improve = 0\n        gen = 0\n\n        # main generation loop\n        while evals < budget:\n            gen += 1\n\n            # Build graph over archive (if archive small, use all; else use best+recent)\n            M = len(X_arch)\n            if M < 2:\n                # sample random to expand archive\n                x_rand = rng.uniform(lb, ub)\n                f_rand = float(func(x_rand))\n                evals += 1\n                X_arch.append(x_rand.copy())\n                F_arch.append(f_rand)\n                if f_rand < f_best:\n                    f_best = float(f_rand)\n                    x_best = x_rand.copy()\n                    gens_since_improve = 0\n                continue\n\n            # limit nodes used to build graph to manageable subset: combine best and recent\n            use_top = min(120, M)  # keep good nodes to ensure exploitation\n            # take indices of best points in archive\n            arch_F_arr = np.array(F_arch)\n            best_idx = np.argsort(arch_F_arr)[:use_top]\n            # also take some recent ones if archive is bigger\n            if M > use_top:\n                recent_take = min(use_top, M - use_top)\n                recent_idx = np.arange(max(0, M - recent_take), M)\n                node_idx = np.unique(np.concatenate((best_idx, recent_idx)))\n            else:\n                node_idx = best_idx\n            node_idx = np.array(node_idx, dtype=int)\n            nodes = np.array([X_arch[i] for i in node_idx])\n            NN = nodes.shape[0]\n\n            # Compute pairwise squared distances among nodes (archive-subgraph)\n            diffs = nodes[:, None, :] - nodes[None, :, :]\n            d2 = np.einsum('ijk,ijk->ij', diffs, diffs)\n            np.fill_diagonal(d2, np.inf)\n            k = min(self.k_neighbors, max(1, NN - 1))\n            neigh_idx = np.argpartition(d2, kth=k, axis=1)[:, :k]\n\n            # For each node in the elite population, we pick some archive neighbors to form edges.\n            # Map elite positions to nearest node index in the archive-subgraph for edge generation\n            X_arch_arr = np.array(X_arch)\n            # nearest node in node_idx for each elite\n            # compute distances from elite to nodes (NN manageable)\n            dist_e2n = np.sum((X[:, None, :] - nodes[None, :, :]) ** 2, axis=2)\n            nearest_node = np.argmin(dist_e2n, axis=1)\n\n            # Build list of candidate edge pairs (archive node idx a, archive node idx b, producer elite index p)\n            edge_pairs = []\n            edges_per_elite = max(2, min(8, int(self.cand_per_edge)))\n            beta_long_bias = 1.8  # bias towards selecting longer edges\n            for p in range(pop_size):\n                a = nearest_node[p]\n                # choose neighbor candidates from the k-nearest to node a\n                nb = neigh_idx[a]\n                # compute lengths and sample neighbors biased by length\n                lens = np.sqrt(d2[a, nb])\n                probs = lens ** beta_long_bias\n                if probs.sum() <= 0 or np.isnan(probs).any():\n                    probs = np.ones_like(probs)\n                probs = probs / probs.sum()\n                # pick up to edges_per_elite unique neighbors\n                pick_count = min(edges_per_elite, nb.size)\n                picks = rng.choice(nb, size=pick_count, replace=(pick_count > nb.size), p=probs)\n                for b in picks:\n                    edge_pairs.append((node_idx[a], node_idx[b], p))\n\n            # Additionally include some purely-archive long edges (to densify graph)\n            # sample random pairs among the node_idx with bias to long distances\n            extra_pairs = max(0, min(40, NN // 2))\n            tri = []\n            if extra_pairs > 0:\n                # flatten upper triangle indices and sample\n                # to avoid heavy memory, sample random i and pick j from its long neighbors\n                for _ in range(extra_pairs):\n                    ia = rng.integers(0, NN)\n                    candidates_j = neigh_idx[ia]\n                    if candidates_j.size == 0:\n                        continue\n                    lens = np.sqrt(d2[ia, candidates_j])\n                    probs = (lens + 1e-12) ** beta_long_bias\n                    probs = probs / probs.sum()\n                    jb = rng.choice(candidates_j, p=probs)\n                    tri.append((node_idx[ia], node_idx[jb], None))  # None producer\n                edge_pairs.extend(tri)\n\n            # Shuffle edges for fairness\n            rng.shuffle(edge_pairs)\n\n            # Candidate generation from edges\n            candidates = []\n            cand_meta = []  # tuple (producer_elite_index or None, a_idx_in_Xarch, b_idx_in_Xarch)\n            for (ia, ib, prod_p) in edge_pairs:\n                if evals + len(candidates) >= budget:\n                    break\n                xa = X_arch[ia]\n                xb = X_arch[ib]\n                vec = xb - xa\n                edge_len = max(1e-12, np.linalg.norm(vec))\n                u = vec / edge_len\n                # sample a few alpha along the edge; mixture generates endpoints and midpoints\n                # create a small set of alphas skewed to both endpoints and mid region\n                m = self.cand_per_edge\n                r = rng.random(m)\n                # mixture: some near endpoints, some uniform, some beta near middle\n                alphas = np.empty(m)\n                mask1 = r < 0.35\n                mask2 = (r >= 0.35) & (r < 0.7)\n                mask3 = r >= 0.7\n                if mask1.any():\n                    alphas[mask1] = rng.beta(2.5, 1.2, size=mask1.sum())  # near 1\n                if mask2.any():\n                    alphas[mask2] = rng.uniform(0.15, 0.85, size=mask2.sum())\n                if mask3.any():\n                    alphas[mask3] = rng.beta(1.2, 2.5, size=mask3.sum())  # near 0\n                # for each alpha create perpendicular perturbation basis\n                for alpha in alphas:\n                    if evals + len(candidates) >= budget:\n                        break\n                    base = xa + alpha * vec\n                    # perpendicular random vector: sample gaussian and orthogonalize to u\n                    z = rng.normal(size=dim)\n                    z -= np.dot(z, u) * u\n                    znorm = np.linalg.norm(z)\n                    if znorm < 1e-12:\n                        # fallback: create any orthonormal vector\n                        # build basis by replacing one coordinate\n                        z = np.zeros(dim)\n                        z[rng.integers(dim)] = 1.0\n                        z -= np.dot(z, u) * u\n                        znorm = max(1e-12, np.linalg.norm(z))\n                    z = z / znorm\n                    # perpendicular amplitude scales with edge length and producer sigma if any\n                    if prod_p is None:\n                        sigma_factor = np.mean(sigma)  # archive-only edge, use mean sigma\n                    else:\n                        sigma_factor = sigma[prod_p]\n                    perp_scale = (0.08 + rng.random() * 0.4) * edge_len * (0.6 + 0.8 * (sigma_factor / (sigma_factor + range_scale)))\n                    # cross-section Gaussian\n                    perp = z * rng.normal(scale=perp_scale)\n                    # small isotropic jitter proportional to sigma\n                    iso = rng.normal(scale=0.25 * sigma_factor, size=dim)\n                    x_cand = base + perp + iso\n\n                    # small guided directional move: step toward better endpoint with some probability\n                    if rng.random() < 0.25:\n                        # prefer direction to better of endpoints (by archive fitness)\n                        if F_arch[ia] < F_arch[ib]:\n                            target_dir = xa - base\n                        else:\n                            target_dir = xb - base\n                        # small move along target_dir scaled\n                        td_norm = np.linalg.norm(target_dir)\n                        if td_norm > 1e-12:\n                            x_cand = x_cand + (rng.random() * 0.6 * sigma_factor) * (target_dir / td_norm)\n\n                    # reflection and clip into bounds with mirror reflection to preserve energy\n                    x_cand = np.where(x_cand < lb, lb + (lb - x_cand), x_cand)\n                    x_cand = np.where(x_cand > ub, ub - (x_cand - ub), x_cand)\n                    x_cand = np.clip(x_cand, lb, ub)\n\n                    candidates.append(x_cand)\n                    cand_meta.append((prod_p, ia, ib))\n\n                # also attempt a short 1D line probing from one endpoint into the edge direction (cheap local line-search surrogate)\n                if evals + len(candidates) >= budget:\n                    break\n                if rng.random() < 0.18:\n                    # pick a direction near u but with small noise and few step points\n                    start = xa if rng.random() < 0.5 else xb\n                    step_dir = u * (0.8 * edge_len) + rng.normal(scale=0.2 * edge_len, size=dim)\n                    step_dir = step_dir / (np.linalg.norm(step_dir) + 1e-12)\n                    # probe 2 points along this ray with different step lengths\n                    for step_frac in (0.12, 0.35):\n                        if evals + len(candidates) >= budget:\n                            break\n                        x_probe = start + step_frac * edge_len * step_dir\n                        x_probe += rng.normal(scale=0.15 * (sigma[prod_p] if prod_p is not None else np.mean(sigma)), size=dim)\n                        x_probe = np.clip(x_probe, lb, ub)\n                        candidates.append(x_probe)\n                        cand_meta.append((prod_p, ia, ib))\n\n            # occasional global L\u00e9vy escape to diversify long edges\n            if rng.random() < self.levy_prob and evals + len(candidates) < budget:\n                lv = self._levy_vector(dim) * (2.5 * range_scale)\n                lev_c = x_best + lv\n                lev_c = np.clip(lev_c, lb, ub)\n                candidates.append(lev_c)\n                cand_meta.append((None, -1, -1))\n\n            # Evaluate candidates sequentially up to budget\n            if len(candidates) == 0:\n                # fallback: sample uniform\n                if evals < budget:\n                    x_r = rng.uniform(lb, ub)\n                    f_r = float(func(x_r))\n                    evals += 1\n                    X_arch.append(x_r.copy())\n                    F_arch.append(f_r)\n                    # maybe replace worst elite\n                    if f_r < F[-1]:\n                        X[-1] = x_r.copy()\n                        F[-1] = f_r\n                        # reorder and align per-elite arrays\n                        order = np.argsort(F)\n                        X = X[order]\n                        F = F[order]\n                        sigma = sigma[order]\n                        p_succ = p_succ[order]\n                    if f_r < f_best:\n                        f_best = float(f_r)\n                        x_best = x_r.copy()\n                        gens_since_improve = 0\n                    # keep archive bounded\n                    if len(X_arch) > self.archive_cap:\n                        # remove worst recent entries by F_arch (prefer to keep best + recent)\n                        # simple policy: drop oldest beyond cap\n                        del X_arch[0:len(X_arch) - self.archive_cap]\n                        del F_arch[0:len(F_arch) - self.archive_cap]\n                    continue\n                else:\n                    break\n\n            cand_arr = np.array(candidates, dtype=float)\n            max_eval = min(len(cand_arr), budget - evals)\n            for idx in range(max_eval):\n                x_try = cand_arr[idx]\n                f_try = float(func(x_try))\n                evals += 1\n\n                # append to archive and trim if needed\n                X_arch.append(x_try.copy())\n                F_arch.append(f_try)\n                if len(X_arch) > self.archive_cap:\n                    # remove oldest (FIFO) to maintain recent+best tendency\n                    del X_arch[0]\n                    del F_arch[0]\n\n                prod_p, a_idx, b_idx = cand_meta[idx]\n                improved_elite = False\n\n                # If candidate improves worst elite, insert it\n                if f_try < F[-1]:\n                    X[-1] = x_try.copy()\n                    F[-1] = f_try\n                    # reorder and align per-elite arrays\n                    order = np.argsort(F)\n                    X = X[order]\n                    F = F[order]\n                    sigma = sigma[order]\n                    p_succ = p_succ[order]\n                    improved_elite = True\n                    # update success statistics: if produced by an elite, credit it; else credit top elite\n                    if prod_p is not None and 0 <= prod_p < pop_size:\n                        # find where the producing elite is now (approx by nearest)\n                        # we'll credit by nearest elite index to the candidate\n                        d2_e = np.sum((X - x_try) ** 2, axis=1)\n                        nearest_el = int(np.argmin(d2_e))\n                        p_succ[nearest_el] = (1.0 - self.ema_alpha) * p_succ[nearest_el] + self.ema_alpha * 1.0\n                        sigma[nearest_el] = min(sigma[nearest_el] * 1.12, 12.0 * range_scale)\n                    else:\n                        # credit current best elite\n                        p_succ[0] = (1.0 - self.ema_alpha) * p_succ[0] + self.ema_alpha * 1.0\n                        sigma[0] = min(sigma[0] * 1.10, 12.0 * range_scale)\n                else:\n                    # unsuccessful -> penalize producing elite mildly\n                    if prod_p is not None and 0 <= prod_p < pop_size:\n                        p_succ[prod_p] = (1.0 - self.ema_alpha) * p_succ[prod_p] + self.ema_alpha * 0.0\n                        sigma[prod_p] = max(sigma[prod_p] * 0.985, 1e-12)\n\n                # Update global best\n                if f_try < f_best:\n                    f_best = float(f_try)\n                    x_best = x_try.copy()\n                    gens_since_improve = 0\n                # stop if budget used up\n                if evals >= budget:\n                    break\n\n            # adapt sigmas multiplicatively according to EMA success\n            # scaling factor modest and dimension-aware\n            scale = np.exp(0.9 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n            scale = np.clip(scale, 0.75, 1.45)\n            sigma = np.clip(sigma * scale, 1e-12, 20.0 * range_scale)\n\n            # stagnation handling\n            if f_best < np.inf:\n                if gens_since_improve >= self.stagnation_restart:\n                    # perform a stronger diversification: sample near several archive nodes and a Levy jump\n                    divers = min(8, max(2, (budget - evals) // 6))\n                    for _ in range(divers):\n                        if evals >= budget:\n                            break\n                        if rng.random() < 0.6:\n                            # sample near random good archive node\n                            cand_idx = int(rng.choice(np.argsort(np.array(F_arch))[:min(len(F_arch), 50)]))\n                            center = np.array(X_arch[cand_idx])\n                            x_r = center + rng.normal(scale=1.2 * np.mean(sigma), size=dim)\n                        else:\n                            x_r = rng.uniform(lb, ub)\n                        x_r = np.clip(x_r, lb, ub)\n                        f_r = float(func(x_r))\n                        evals += 1\n                        X_arch.append(x_r.copy())\n                        F_arch.append(f_r)\n                        if len(X_arch) > self.archive_cap:\n                            del X_arch[0]\n                            del F_arch[0]\n                        if f_r < F[-1]:\n                            X[-1] = x_r.copy()\n                            F[-1] = f_r\n                            order = np.argsort(F)\n                            X = X[order]\n                            F = F[order]\n                            sigma = sigma[order]\n                            p_succ = p_succ[order]\n                        if f_r < f_best:\n                            f_best = float(f_r)\n                            x_best = x_r.copy()\n                            gens_since_improve = 0\n                    # one Levy jump near best\n                    if evals < budget:\n                        lv = self._levy_vector(dim) * (3.0 * range_scale)\n                        cand = x_best + lv\n                        cand = np.clip(cand, lb, ub)\n                        f_lv = float(func(cand))\n                        evals += 1\n                        X_arch.append(cand.copy())\n                        F_arch.append(f_lv)\n                        if len(X_arch) > self.archive_cap:\n                            del X_arch[0]\n                            del F_arch[0]\n                        if f_lv < F[-1]:\n                            X[-1] = cand\n                            F[-1] = f_lv\n                            order = np.argsort(F)\n                            X = X[order]\n                            F = F[order]\n                            sigma = sigma[order]\n                            p_succ = p_succ[order]\n                        if f_lv < f_best:\n                            f_best = float(f_lv)\n                            x_best = cand.copy()\n                            gens_since_improve = 0\n                    # increase exploration breadth temporarily\n                    sigma = np.clip(sigma * 1.6, 1e-12, 30.0 * range_scale)\n                    gens_since_improve = 0\n                else:\n                    gens_since_improve += 1\n\n            # small local covariance update for top elites by recomputing local neighborhood in archive\n            topk_cov = min(3, pop_size)\n            arch_arr = np.array(X_arch)\n            for top_i in range(topk_cov):\n                # find nearest archive neighbors\n                d2top = np.sum((arch_arr - X[top_i]) ** 2, axis=1)\n                nb_idx = np.argsort(d2top)[:min(6, len(arch_arr))]\n                pts = arch_arr[nb_idx]\n                # update a rough diagonal covariance to inform perturbation magnitude (not stored for now)\n                # This is just to keep a small adaptation effect on sigma\n                local_var = np.mean(np.var(pts, axis=0))\n                # nudge sigma toward local variance scale\n                sigma[top_i] = 0.9 * sigma[top_i] + 0.1 * (np.sqrt(local_var + 1e-12) + 1e-12)\n\n            # end of generation loop; break if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 2, "feedback": "The algorithm ECAGS scored 0.099 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["73167bb0-c570-47b5-8cb5-a9c5446068ba"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5513654903308531}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.3053647024926015}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.37959683806388267}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06536798786620257}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.007493078594380864}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.03951748627495799}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.01593488726454706}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.017633556352046775}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.03766249677408451}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.024996145760953548}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.04415578109517304}], "aucs": [0.5513654903308531, 0.3053647024926015, 0.37959683806388267, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.06536798786620257, 0.007493078594380864, 0.03951748627495799, 0.01593488726454706, 0.017633556352046775, 4.999999999999449e-05, 0.03766249677408451, 0.024996145760953548, 0.04415578109517304]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3963.0, "Edges": 3962.0, "Max Degree": 44.0, "Min Degree": 1.0, "Mean Degree": 1.9994953318193287, "Degree Variance": 2.3113800127841633, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.62751677852349, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3300817534486884, "Depth Entropy": 2.1069089302679433, "Assortativity": 0.0, "Average Eccentricity": 18.52132223063336, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00025233409033560434, "Average Shortest Path": 10.618550466117492, "mean_complexity": 25.0, "total_complexity": 75.0, "mean_token_count": 1143.6666666666667, "total_token_count": 3431.0, "mean_parameter_count": 4.666666666666667, "total_parameter_count": 14.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "f5713cbd-1130-4e8d-abdc-3c3ca8966b71", "fitness": 0.3884038477171859, "name": "AdaptiveSpiralLevyEdgeGraph", "description": "Adaptive Spiral-Levy with Dense Edge Graph Recombination \u2014 combine spiral directional moves and occasional L\u00e9vy jumps with a high \"edge density\" strategy that builds a k-NN edge graph among a compact population and generates many edge-based recombinations (midpoints, extrapolations, perturbations) plus adaptive per-individual step sizes and an elite-driven archive to increase connectivity and escape basins.", "code": "import numpy as np\n\nclass AdaptiveSpiralLevyEdgeGraph:\n    \"\"\"\n    Adaptive Spiral-Levy with Dense Edge Graph Recombination (ASL-EdgeGraph)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional args: pop_size, elite_frac, k_edges, levy_prob, seed\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.25,\n                 k_edges=None, proposals_per_edge=2, levy_prob=0.08, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.levy_prob = float(levy_prob)\n        self.elite_frac = float(elite_frac)\n        self.proposals_per_edge = int(proposals_per_edge)\n\n        if pop_size is None:\n            # keep a compact but edge-rich population, scaled by dimension and budget\n            base = int(max(12, min(80, np.clip(np.sqrt(self.budget) * 1.5, 12, 80))))\n            self.pop_size = max(base, dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        if k_edges is None:\n            self.k_edges = max(2, min(self.pop_size - 1, int(np.clip(self.dim, 2, 12))))\n        else:\n            self.k_edges = int(k_edges)\n\n        # adaptive parameters per individual\n        self.sigma_init = 0.12  # fraction of range\n        self.alpha_init = 0.45  # fraction of range for directed spiral\n        self.success_increase = 1.20\n        self.failure_decrease = 0.85\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        range_norm = np.linalg.norm(ub - lb)\n        sigma_scale = self.sigma_init * range_norm\n        alpha_scale = self.alpha_init * range_norm\n\n        evals = 0\n\n        # Initialize population uniformly in bounds\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # per-individual adaptive step parameters\n        sigma = np.full(self.pop_size, sigma_scale)\n        alpha = np.full(self.pop_size, alpha_scale)\n        success_counts = np.zeros(self.pop_size, dtype=int)\n\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        no_improve = 0\n        max_no_improve = max(20, self.pop_size * 3)\n\n        # helper: build kNN edges biased to elites for higher edge density between good individuals\n        def build_edges():\n            # compute pairwise squared distances\n            X = pop\n            # efficient pairwise distances using broadcasting\n            dif = X[:, None, :] - X[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            edges = []\n            # select elites (top fraction)\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count]\n            # For each elite, connect to k nearest neighbors (excluding self)\n            for i in elites:\n                neigh = np.argsort(dist2[i])  # includes self at position 0\n                # take next k_edges neighbors\n                added = 0\n                for j in neigh:\n                    if j == i: continue\n                    edges.append((int(i), int(j)))\n                    added += 1\n                    if added >= self.k_edges:\n                        break\n            # also add some random edges among the whole pop to densify\n            extra = max(0, self.pop_size // 3)\n            for _ in range(extra):\n                a = int(rng.integers(0, self.pop_size))\n                b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges.append((a, b))\n            return edges\n\n        # main loop: emphasize edge-based recombination first to increase \"edge density\"\n        while evals < self.budget:\n            rem = self.budget - evals\n            improved_in_loop = False\n\n            # Build dense edges\n            edges = build_edges()\n\n            # iterate edges and produce recombination proposals\n            for (i, j) in edges:\n                if evals >= self.budget: break\n                # endpoints\n                xi, xj = pop[i].copy(), pop[j].copy()\n                fi, fj = pop_f[i], pop_f[j]\n                # generate several proposals per edge\n                edge_vec = xj - xi\n                edge_len = np.linalg.norm(edge_vec) + 1e-12\n\n                for _ in range(self.proposals_per_edge):\n                    if evals >= self.budget: break\n\n                    # choose recombination style: midpoint, biased toward better, extrapolation\n                    style = rng.random()\n                    if style < 0.4:\n                        # midpoint with small perturbation\n                        w = 0.5\n                    elif style < 0.8:\n                        # bias toward better endpoint\n                        if fi < fj:\n                            w = rng.uniform(0.1, 0.6)  # closer to i\n                        else:\n                            w = rng.uniform(0.4, 0.9)  # closer to j\n                    else:\n                        # extrapolate beyond better endpoint\n                        if fi < fj:\n                            w = rng.uniform(-0.6, 0.3)  # extrapolate from i towards j or beyond\n                        else:\n                            w = rng.uniform(0.7, 1.6)\n\n                    x_prop = (1.0 - w) * xi + w * xj\n\n                    # perturbation scaled to edge length and local sigma\n                    local_scale = (sigma[i] + sigma[j]) * 0.5\n                    noise = local_scale * rng.standard_normal(dim)\n                    x_prop = x_prop + noise\n\n                    # occasional small rotational spiral around midpoint for diversity\n                    if dim > 1 and rng.random() < 0.25:\n                        a, b = rng.choice(dim, size=2, replace=False)\n                        theta = rng.uniform(-np.pi / 4, np.pi / 4)\n                        ca, sb = np.cos(theta), np.sin(theta)\n                        # apply to edge direction components\n                        da, db = edge_vec[a], edge_vec[b]\n                        rda = ca * da - sb * db\n                        rdb = sb * da + ca * db\n                        rot = np.zeros(dim)\n                        rot[a], rot[b] = rda - da, rdb - db\n                        x_prop += 0.08 * rot\n\n                    # occasional Levy jump on edge-based proposals (rare but powerful)\n                    if rng.random() < self.levy_prob * 0.7:\n                        u = rng.random(dim)\n                        cauchy = np.tan(np.pi * (u - 0.5))\n                        levy_scale = 0.12 * range_norm\n                        x_prop += levy_scale * cauchy\n\n                    # ensure inside bounds\n                    x_prop = np.clip(x_prop, lb, ub)\n\n                    # evaluate\n                    f_prop = float(func(x_prop))\n                    evals += 1\n\n                    # decide where to place candidate: replace worse endpoint if better, else maybe replace global worst occasionally\n                    if f_prop < fi or f_prop < fj:\n                        # find worse endpoint index\n                        if fi >= fj:\n                            replace_idx = i\n                        else:\n                            replace_idx = j\n                        pop[replace_idx] = x_prop\n                        pop_f[replace_idx] = f_prop\n                        # adapt sigma for that individual (success)\n                        sigma[replace_idx] = sigma[replace_idx] * self.success_increase\n                        # small alpha increase for directional moves\n                        alpha[replace_idx] = alpha[replace_idx] * 1.05\n                        success_counts[replace_idx] += 1\n                        if f_prop < f_best:\n                            f_best = f_prop; x_best = x_prop.copy(); improved_in_loop = True\n                    else:\n                        # occasionally replace worst global to maintain exploration\n                        if rng.random() < 0.005:\n                            idx_worst = int(np.argmax(pop_f))\n                            if f_prop < pop_f[idx_worst]:\n                                pop[idx_worst] = x_prop\n                                pop_f[idx_worst] = f_prop\n                                sigma[idx_worst] = sigma[idx_worst] * 1.05\n                                if f_prop < f_best:\n                                    f_best = f_prop; x_best = x_prop.copy(); improved_in_loop = True\n                        # failure shrinkage for local sigmas around the edge endpoints\n                        sigma[i] *= self.failure_decrease\n                        sigma[j] *= self.failure_decrease\n\n            # after edge-based proposals, apply spiral directional moves per individual\n            for i in range(self.pop_size):\n                if evals >= self.budget: break\n                xi = pop[i].copy()\n                fi = pop_f[i]\n\n                # direction to best\n                d = x_best - xi\n                nd = np.linalg.norm(d) + 1e-12\n                dir_vec = d / nd\n\n                # small randomized rotation for spiral\n                if dim > 1 and rng.random() < 0.6:\n                    a, b = rng.choice(dim, size=2, replace=False)\n                    theta = rng.uniform(-np.pi/3, np.pi/3)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    da, db = d[a], d[b]\n                    ra = ca * da - sb * db\n                    rb = sb * da + ca * db\n                    d_rot = d.copy()\n                    d_rot[a], d_rot[b] = ra, rb\n                    dir_vec = d_rot / (np.linalg.norm(d_rot) + 1e-12)\n\n                # directional scaling adaptive\n                mov_scale = alpha[i] * (0.6 + rng.random() * 0.8)\n                move = mov_scale * dir_vec\n\n                # gaussian local perturbation with per-individual sigma\n                move += sigma[i] * rng.standard_normal(dim)\n\n                # occasional Levy jump for individual\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    move += 0.18 * range_norm * cauchy\n\n                x_new = np.clip(xi + move, lb, ub)\n\n                # opposition sampling small chance\n                if rng.random() < 0.06:\n                    x_opp = lb + ub - xi\n                    x_opp = np.clip(x_opp, lb, ub)\n                    f_opp = float(func(x_opp))\n                    evals += 1\n                    if f_opp < fi:\n                        pop[i] = x_opp; pop_f[i] = f_opp\n                        sigma[i] *= self.success_increase\n                        if f_opp < f_best:\n                            f_best = f_opp; x_best = x_opp.copy(); improved_in_loop = True\n                        continue\n\n                # evaluate\n                f_new = float(func(x_new))\n                evals += 1\n\n                # replacement rule: greedy with tiny uphill acceptance to maintain diversity\n                if f_new < fi or rng.random() < 0.007:\n                    pop[i] = x_new\n                    pop_f[i] = f_new\n                    # success adaptation\n                    sigma[i] *= self.success_increase\n                    alpha[i] *= 1.02\n                    success_counts[i] += 1\n                else:\n                    # failure adaptation\n                    sigma[i] *= self.failure_decrease\n                    alpha[i] *= 0.985\n\n                if f_new < f_best:\n                    f_best = f_new; x_best = x_new.copy(); improved_in_loop = True\n\n            # global maintenance: clamp sigmas and alphas to reasonable ranges\n            sigma = np.clip(sigma, 1e-8 * range_norm, 2.0 * range_norm)\n            alpha = np.clip(alpha, 1e-8 * range_norm, 1.5 * range_norm)\n\n            if improved_in_loop:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # stagnation handling: reseed half population around best and keep some random individuals\n            if no_improve >= max_no_improve and evals < self.budget:\n                no_improve = 0\n                n_reset = max(1, self.pop_size // 2)\n                for _ in range(n_reset):\n                    if evals >= self.budget: break\n                    idx = int(rng.integers(0, self.pop_size))\n                    # generate either near-best or random\n                    if rng.random() < 0.75:\n                        newp = x_best + 0.06 * (ub - lb) * rng.standard_normal(dim)\n                    else:\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    pop[idx] = newp; pop_f[idx] = f_new\n                    sigma[idx] = sigma_scale\n                    alpha[idx] = alpha_scale\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # occasional random injection\n            if rng.random() < 0.04 and evals < self.budget:\n                r = rng.uniform(lb, ub)\n                fr = float(func(r))\n                evals += 1\n                idx_worst = int(np.argmax(pop_f))\n                if fr < pop_f[idx_worst]:\n                    pop[idx_worst] = r; pop_f[idx_worst] = fr\n                    sigma[idx_worst] = sigma_scale\n                    if fr < f_best:\n                        f_best = fr; x_best = r.copy()\n\n            # if low remaining budget, break into local refinement\n            if self.budget - evals < max(10, dim * 3):\n                break\n\n        # Final mesh-adaptive local refinement (coordinate pattern search)\n        mesh = 0.15 * range_norm\n        mesh_min = 1e-6 * range_norm\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                # negative\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # store results\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveSpiralLevyEdgeGraph scored 0.388 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59af599d-0632-4845-b16d-cfcb095e7b04"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9129198772628673}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8954948216130091}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8988388038216445}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.04754744641170727}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10249899254372052}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.07132852495436559}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.036905158759182766}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.05023318180376557}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.047718329065428056}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.8872834828057611}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9262097389629332}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.948929357753404}], "aucs": [0.9129198772628673, 0.8954948216130091, 0.8988388038216445, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.04754744641170727, 0.10249899254372052, 0.07132852495436559, 0.036905158759182766, 0.05023318180376557, 0.047718329065428056, 0.8872834828057611, 0.9262097389629332, 0.948929357753404]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2946.0, "Edges": 2945.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9993211133740665, "Degree Variance": 2.025797230898421, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.337037037037037, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3327277936394362, "Depth Entropy": 2.0464127016356968, "Assortativity": 0.0, "Average Eccentricity": 18.778343516632724, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00033944331296673454, "Average Shortest Path": 10.56606350644366, "mean_complexity": 17.25, "total_complexity": 69.0, "mean_token_count": 617.75, "total_token_count": 2471.0, "mean_parameter_count": 3.25, "total_parameter_count": 13.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "e513ab61-961c-43f3-98a2-7cbf094d03bc", "fitness": 0.26075842408191935, "name": "SpiralEdgeLevy", "description": "Spiral-Edge-Levy Adaptive Network Search (SELAN) \u2014 combine directional spiral moves toward the best with dense edge-based recombinations among an evolving elite graph, per-individual adaptive step-sizes, and occasional L\u00e9vy escapes to increase \"edge density\" and promote connected exploration across basins.", "code": "import numpy as np\n\nclass SpiralEdgeLevy:\n    \"\"\"\n    Spiral-Edge-Levy Adaptive Network Search (SELAN)\n\n    Key ideas:\n    - Maintain a compact population and an elite archive.\n    - For each individual, perform spiral-style directional proposals toward the best,\n      plus edge-based recombinations with its k-nearest neighbors (sample along edges,\n      midpoints, extrapolations, and orthogonal jitter).\n    - Track per-individual step sizes and success rates to adapt exploration/exploitation.\n    - Occasionally perform dense 'edge bursts' sampling many candidates along elite edges\n      to raise the effective edge density (many edge-derived proposals), and rare Levy jumps\n      to escape basins.\n    - Final mesh-adaptive local refinement.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_size=None, k_edges=3,\n                 levy_prob=0.10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.levy_prob = float(levy_prob)\n        self.k_edges = max(1, int(k_edges))\n        # Choose population & elite sizes relative to budget and dim\n        if pop_size is None:\n            self.pop_size = int(np.clip(12 + 2 * np.sqrt(self.budget / max(1, dim)),\n                                        8, 80))\n        else:\n            self.pop_size = int(pop_size)\n        if elite_size is None:\n            self.elite_size = max(3, int(min(self.pop_size // 3, 12)))\n        else:\n            self.elite_size = int(elite_size)\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        range_norm = np.linalg.norm(ub - lb)\n        evals = 0\n\n        # initialize population uniformly with some opposition diversity\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, dim))\n        # add a few opposite points for diversity if budget allows\n        pop[:min(self.pop_size, 4)] = (pop[:min(self.pop_size, 4)] + (lb + ub - pop[:min(self.pop_size, 4)])) / 2.0\n\n        pop_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # best so far\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        # per-individual adaptive step sizes and success counters\n        alpha = np.full(self.pop_size, 0.3 * range_norm)   # directional scale\n        sigma = np.full(self.pop_size, 0.12 * range_norm)  # orthogonal jitter\n        succ = np.zeros(self.pop_size, dtype=int)\n        fail = np.zeros(self.pop_size, dtype=int)\n\n        # main loop\n        gen = 0\n        no_improve = 0\n        max_no_improve = max(15, self.pop_size * 2)\n        while evals < self.budget:\n            gen += 1\n            # create elite archive (sorted)\n            order = np.argsort(pop_f)\n            elites_idx = order[:self.elite_size]\n            elites = pop[elites_idx]\n            elites_f = pop_f[elites_idx]\n\n            # compute k-nearest neighbors for each elite (simple distance)\n            # used to derive edges; for small elite sizes this is cheap\n            if self.elite_size > 1:\n                dists = np.linalg.norm(elites[:, None, :] - elites[None, :, :], axis=2)\n                # set diagonals large\n                np.fill_diagonal(dists, np.inf)\n                # neighbors indices per elite (within elite-index space)\n                knn = np.argsort(dists, axis=1)[:, :min(self.k_edges, self.elite_size - 1)]\n            else:\n                knn = np.zeros((self.elite_size, 0), dtype=int)\n\n            improved_in_gen = False\n\n            # For each member, produce a small portfolio of candidates:\n            for i in range(self.pop_size):\n                if evals >= self.budget: break\n\n                xi = pop[i].copy()\n                fi = pop_f[i]\n\n                # directional spiral toward global best\n                d = x_best - xi\n                norm_d = np.linalg.norm(d) + 1e-12\n                dir_unit = d / norm_d\n                # small random rotation in a random 2D subspace to create spiral curvature\n                if dim > 1:\n                    a, b = self.rng.choice(dim, size=2, replace=False)\n                    theta = self.rng.uniform(-np.pi/4, np.pi/4)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    da, db = d[a], d[b]\n                    d_rot = d.copy()\n                    d_rot[a] = ca * da - sb * db\n                    d_rot[b] = sb * da + ca * db\n                    dir_unit = d_rot / (np.linalg.norm(d_rot) + 1e-12)\n\n                # base spiral move scaled by adaptive alpha[i]\n                t_scale = 0.4 + 0.6 * self.rng.random()\n                base_move = alpha[i] * t_scale * dir_unit\n\n                # edge-based recombination partner selection: pick among nearest elites to pop[i]\n                # choose a partner by nearest distance in global pop\n                # compute distances to elites\n                if self.elite_size > 0:\n                    ep_dists = np.linalg.norm(elites - xi, axis=1)\n                    partner_local_idx = int(np.argmin(ep_dists))\n                    # choose one of the partner's neighbors if available (to create edges)\n                    if knn.shape[1] > 0 and self.rng.random() < 0.8:\n                        # choose neighbor index in elite space\n                        neighbor_idx_in_elite_space = knn[partner_local_idx, self.rng.integers(0, knn.shape[1])]\n                        partner = elites[neighbor_idx_in_elite_space]\n                    else:\n                        partner = elites[partner_local_idx]\n                else:\n                    partner = xi + self.rng.standard_normal(dim) * range_norm * 0.1\n\n                # build several edge-derived candidates but evaluate a limited subset (budget-aware)\n                candidates = []\n\n                # candidate 1: along edge (interpolation) with small orthogonal jitter\n                t = self.rng.uniform(0.0, 1.0)\n                edge_point = (1 - t) * xi + t * partner\n                # orthogonal jitter: project random Gaussian and remove parallel component\n                v = self.rng.standard_normal(dim)\n                edge_dir = partner - xi\n                proj = (np.dot(v, edge_dir) / (np.dot(edge_dir, edge_dir) + 1e-12)) * edge_dir if np.linalg.norm(edge_dir) > 1e-12 else v\n                orth = v - proj\n                if np.linalg.norm(orth) > 1e-12:\n                    orth = orth / np.linalg.norm(orth)\n                orth_scale = sigma[i] * 0.6 * self.rng.random()\n                candidates.append(np.clip(edge_point + orth * orth_scale, lb, ub))\n\n                # candidate 2: midpoint-extrapolation toward best (edge extrapolation encourages connections)\n                mid = 0.5 * (xi + partner)\n                extrap = mid + 0.5 * (mid - xi) * self.rng.uniform(-0.4, 1.0)\n                candidates.append(np.clip(extrap + self.rng.standard_normal(dim) * (sigma[i] * 0.3), lb, ub))\n\n                # candidate 3: base spiral move added to xi\n                cand_spiral = xi + base_move + self.rng.standard_normal(dim) * (sigma[i] * 0.5)\n                candidates.append(np.clip(cand_spiral, lb, ub))\n\n                # candidate 4: directed sample along best-edge (toward best from partner)\n                if self.rng.random() < 0.6:\n                    direction_to_best = x_best - partner\n                    c = partner + 0.3 * (direction_to_best) * self.rng.uniform(-0.3, 1.2)\n                    candidates.append(np.clip(c + self.rng.standard_normal(dim) * (sigma[i] * 0.4), lb, ub))\n\n                # occasionally include a heavier orthogonal displacement (increase edge density)\n                if self.rng.random() < 0.18:\n                    # sample random point uniformly on the segment extended a bit\n                    t2 = self.rng.uniform(-0.5, 1.5)\n                    p_ext = np.clip((1 - t2) * xi + t2 * partner, lb, ub)\n                    p_ext += self.rng.standard_normal(dim) * (sigma[i] * 0.8)\n                    candidates.append(np.clip(p_ext, lb, ub))\n\n                # occasionally try a Levy jump for this candidate set\n                if self.rng.random() < self.levy_prob:\n                    u = self.rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    levy_scale = 0.15 * range_norm\n                    lw = xi + levy_scale * cauchy\n                    candidates.append(np.clip(lw, lb, ub))\n\n                # Evaluate candidates in order until we find improvement or run out of budget\n                # Limit evaluations per individual per generation to keep fair budget usage\n                max_eval_per_ind = max(1, min(6, (self.budget - evals) // (self.pop_size or 1)))\n                evals_for_this_ind = 0\n                best_local_candidate = None\n                best_local_val = fi\n                for cand in candidates:\n                    if evals >= self.budget or evals_for_this_ind >= max_eval_per_ind:\n                        break\n                    f_cand = float(func(cand))\n                    evals += 1\n                    evals_for_this_ind += 1\n                    # greedy acceptance\n                    if f_cand < best_local_val:\n                        best_local_val = f_cand\n                        best_local_candidate = cand.copy()\n                        # small immediate acceptance to save remaining per-individual budget\n                        # but continue to try a couple more (allow better candidate)\n                        if self.rng.random() < 0.7:\n                            break\n\n                # Apply best found among candidates\n                if best_local_candidate is not None and best_local_val < fi:\n                    # success: replace and update adaptives\n                    pop[i] = best_local_candidate\n                    pop_f[i] = best_local_val\n                    succ[i] += 1\n                    fail[i] = 0\n                    # increase step sizes moderately to encourage exploitation along promising directions\n                    alpha[i] = min(alpha[i] * 1.08, range_norm)\n                    sigma[i] = min(sigma[i] * 1.05, range_norm * 0.8)\n                    if best_local_val < f_best:\n                        f_best = best_local_val\n                        x_best = best_local_candidate.copy()\n                        improved_in_gen = True\n                else:\n                    # no improvement among candidates: slight decay of step sizes\n                    fail[i] += 1\n                    succ[i] = max(0, succ[i] - 0)\n                    alpha[i] = max(alpha[i] * 0.96, 1e-8 * range_norm)\n                    sigma[i] = max(sigma[i] * 0.96, 1e-8 * range_norm)\n\n                # occasional small random replacement to keep diversity\n                if self.rng.random() < 0.004 and evals < self.budget:\n                    r = self.rng.uniform(lb, ub)\n                    fr = float(func(r))\n                    evals += 1\n                    # replace worst if better\n                    idx_worst = int(np.argmax(pop_f))\n                    if fr < pop_f[idx_worst]:\n                        pop[idx_worst] = r\n                        pop_f[idx_worst] = fr\n                        if fr < f_best:\n                            f_best = fr; x_best = r.copy()\n                            improved_in_gen = True\n\n            # End per-individual proposals\n\n            if improved_in_gen:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # If stuck for several generations, perform a dense \"edge burst\": sample many points along elite edges\n            if no_improve >= max_no_improve and evals < self.budget:\n                no_improve = 0\n                # build edge list among elites (all pairs)\n                edges = []\n                for a in range(self.elite_size):\n                    for b in range(a + 1, self.elite_size):\n                        edges.append((a, b))\n                # shuffle edges to randomize sampling\n                self.rng.shuffle(edges)\n                # number of samples per edge depends on remaining budget\n                remaining = self.budget - evals\n                # allocate a chunk to edge burst but keep some for later\n                burst_budget = max(1, min(remaining // 2, 100 + dim * 10))\n                samples_per_edge = max(1, burst_budget // max(1, len(edges)))\n                # sample along edges\n                for (a, b) in edges:\n                    if evals >= self.budget: break\n                    pa = elites[a]; pb = elites[b]\n                    edge_vec = pb - pa\n                    edge_len = np.linalg.norm(edge_vec) + 1e-12\n                    for s in range(samples_per_edge):\n                        if evals >= self.budget: break\n                        t = self.rng.random()\n                        p = pa + t * edge_vec\n                        # orthogonal perturbation proportional to edge length\n                        w = self.rng.standard_normal(dim)\n                        proj = (np.dot(w, edge_vec) / (edge_len ** 2)) * edge_vec\n                        orth = w - proj\n                        if np.linalg.norm(orth) > 1e-12:\n                            orth = orth / np.linalg.norm(orth)\n                        orth_scale = 0.5 * (edge_len) * self.rng.random() * (0.2 + 0.8 * (1.0 - t))\n                        candidate = np.clip(p + orth * orth_scale, lb, ub)\n                        f_c = float(func(candidate))\n                        evals += 1\n                        # place into population replacing worst if better\n                        if f_c < np.max(pop_f):\n                            idx_w = int(np.argmax(pop_f))\n                            pop[idx_w] = candidate\n                            pop_f[idx_w] = f_c\n                            if f_c < f_best:\n                                f_best = f_c; x_best = candidate.copy()\n                # after burst slightly enlarge step sizes to allow exploration\n                alpha *= 1.06\n                sigma *= 1.06\n\n            # occasional reseeding: replace some failing individuals with samples around elites\n            if self.rng.random() < 0.06 and evals < self.budget:\n                n_replace = max(1, self.pop_size // 8)\n                for _ in range(n_replace):\n                    if evals >= self.budget: break\n                    idx = int(self.rng.integers(0, self.pop_size))\n                    # sample near a random elite\n                    eidx = int(self.rng.integers(0, self.elite_size))\n                    newp = elites[eidx] + self.rng.standard_normal(dim) * (0.08 * range_norm)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    pop[idx] = newp\n                    pop_f[idx] = f_new\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # If remaining budget low, break to local refinement\n            if self.budget - evals < max(12, dim * 4):\n                break\n\n        # Final local mesh-adaptive refinement (coordinate pattern search)\n        mesh = 0.15 * range_norm\n        mesh_min = 1e-6 * range_norm\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                # negative\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # store results\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SpiralEdgeLevy scored 0.261 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59af599d-0632-4845-b16d-cfcb095e7b04"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.28764682501127725}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.29006989567157093}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.3030775957763677}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.04244664871989645}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.04335379762122027}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.060330685112760296}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.03180039045354388}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.029303352318008402}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.023278369212461292}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9157333120989359}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9463142958561196}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9378711933766283}], "aucs": [0.28764682501127725, 0.29006989567157093, 0.3030775957763677, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.04244664871989645, 0.04335379762122027, 0.060330685112760296, 0.03180039045354388, 0.029303352318008402, 0.023278369212461292, 0.9157333120989359, 0.9463142958561196, 0.9378711933766283]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3131.0, "Edges": 3130.0, "Max Degree": 38.0, "Min Degree": 1.0, "Mean Degree": 1.9993612264452252, "Degree Variance": 2.179494960860073, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.607192254495159, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3257299718878766, "Depth Entropy": 2.0796953715066877, "Assortativity": 4.4109834268686215e-09, "Average Eccentricity": 19.11370169274992, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00031938677738741617, "Average Shortest Path": 10.620124224109517, "mean_complexity": 21.0, "total_complexity": 63.0, "mean_token_count": 882.6666666666666, "total_token_count": 2648.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "aaf6761e-5db8-4d7d-953f-b374c3ab29dd", "fitness": "-inf", "name": "EdgeDenseGraphSampler", "description": "Edge-Dense Graph Sampling (EDGS) \u2014 maintain a compact elite archive, build many virtual edges between elites and actively sample along and around these edges (including slight extrapolations and orthogonal jitter) to increase the \"edge density\" of explored connections, combined with multi-scale local refinements around improved nodes.", "code": "import numpy as np\n\nclass EdgeDenseGraphSampler:\n    \"\"\"\n    Edge-Dense Graph Sampling (EDGS)\n\n    Main idea:\n    - Keep a small elite archive of best points.\n    - Treat archive as nodes of a complete graph, and actively sample along edges\n      (interpolations, slight extrapolations) with orthogonal jitter and multi-scale\n      noise to increase edge coverage and connectivity.\n    - Replace worst elites with better edge-derived candidates; when a node improves,\n      do a short local directional refinement.\n    - Strictly respects evaluation budget via the eval_f wrapper.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng_seed=None,\n                 archive_size=12, init_frac=0.12, edge_batch=8, explore_prob=0.35):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimensionality\n        rng_seed: seed for RNG\n        archive_size: number of elite nodes maintained\n        init_frac: fraction of budget to spend on initial global sampling (0..1)\n        edge_batch: nominal number of edges to sample per main iteration\n        explore_prob: probability to prefer exploring novel edges vs exploitation\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(rng_seed)\n        self.archive_size = max(2, int(archive_size))\n        self.init_frac = float(init_frac)\n        self.edge_batch = max(1, int(edge_batch))\n        self.explore_prob = float(np.clip(explore_prob, 0.0, 1.0))\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        lb = lb.reshape(-1)[:self.dim]\n        ub = ub.reshape(-1)[:self.dim]\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _lhs(self, n_samples, lb, ub):\n        # simple Latin-Hypercube-like sampling\n        n = max(1, int(n_samples))\n        dim = self.dim\n        u = self.rng.random((n, dim))\n        pts = np.empty((n, dim))\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            pts[:, j] = (perm + u[:, j]) / n\n        return lb + pts * (ub - lb)\n\n    def __call__(self, func):\n        calls_made = 0\n        B = self.budget\n\n        def eval_f(x):\n            nonlocal calls_made\n            if calls_made >= B:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            calls_made += 1\n            return float(func(x))\n\n        lb, ub = self._get_bounds(func)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # INITIAL GLOBAL SCAN\n        init_budget = max( min(int(self.init_frac * B), B//2), min(50, self.dim * 8) )\n        init_budget = int(max(10, min(init_budget, B-1)))\n        try:\n            X0 = self._lhs(init_budget, lb, ub)\n            F0 = np.empty(init_budget, dtype=float)\n            for i in range(init_budget):\n                fval = eval_f(X0[i])\n                F0[i] = fval\n                if fval < self.f_opt:\n                    self.f_opt = fval\n                    self.x_opt = X0[i].copy()\n        except StopIteration:\n            return self.f_opt, (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))\n\n        # initialize elite archive with best points\n        idx_sorted = np.argsort(F0)\n        archive = []\n        for i in idx_sorted[:self.archive_size]:\n            archive.append((X0[i].copy(), float(F0[i])))\n        # If not enough, add random points from bounds\n        while len(archive) < self.archive_size and calls_made < B:\n            try:\n                x = lb + self.rng.random(self.dim) * (ub - lb)\n                f = eval_f(x)\n                archive.append((x.copy(), float(f)))\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n            except StopIteration:\n                break\n\n        # convert to arrays for easier ops\n        def archive_arrays(archive_list):\n            X = np.vstack([a[0] for a in archive_list])\n            F = np.array([a[1] for a in archive_list], dtype=float)\n            return X, F\n\n        X_arch, F_arch = archive_arrays(archive)\n\n        # bookkeeping for edges visited (encourage sampling many different edges)\n        visited_edges = set()\n\n        # helper to insert candidate into archive (keep best archive_size)\n        def try_insert_candidate(x_cand, f_cand):\n            nonlocal X_arch, F_arch, visited_edges\n            if np.isnan(f_cand):\n                return False\n            if f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = x_cand.copy()\n            if len(F_arch) < self.archive_size:\n                X_arch = np.vstack([X_arch, x_cand.copy()]) if X_arch.size else x_cand.copy().reshape(1, self.dim)\n                F_arch = np.concatenate([F_arch, [f_cand]])\n                return True\n            worst_idx = int(np.argmax(F_arch))\n            if f_cand < F_arch[worst_idx]:\n                # Replace worst; remove related visited edges to allow re-exploration in new graph\n                # We'll clear edges touching the replaced node to not penalize new topology\n                removed_node = worst_idx\n                # remove edges touching removed node from visited set (safe removal)\n                to_remove = [e for e in visited_edges if removed_node in e]\n                for e in to_remove:\n                    visited_edges.discard(e)\n                X_arch[worst_idx] = x_cand.copy()\n                F_arch[worst_idx] = float(f_cand)\n                return True\n            return False\n\n        # Precompute some scale measures\n        span = ub - lb\n        avg_span = span.mean()\n        min_sigma = 1e-8 + 1e-4 * avg_span\n\n        # MAIN EDGE-SAMPLING LOOP\n        try:\n            while calls_made < B:\n                n_nodes = X_arch.shape[0]\n                if n_nodes < 2:\n                    # generate random sample if not enough nodes\n                    x_r = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_r = eval_f(x_r)\n                    try_insert_candidate(x_r, f_r)\n                    continue\n\n                # build all unique edges (i<j)\n                edges = []\n                dists = []\n                for i in range(n_nodes):\n                    for j in range(i+1, n_nodes):\n                        d = np.linalg.norm(X_arch[j] - X_arch[i])\n                        edges.append((i, j))\n                        dists.append(d + 1e-12)\n                edges = np.array(edges, dtype=int)\n                dists = np.array(dists, dtype=float)\n\n                # scoring edges to choose which to sample: prefer longer edges and unvisited ones\n                edge_scores = dists.copy()\n                # boost unvisited edges with a random exploration component\n                for k, (i, j) in enumerate(edges):\n                    if (i, j) not in visited_edges:\n                        edge_scores[k] *= 1.0 + 0.8 * self.rng.random()\n                    else:\n                        edge_scores[k] *= 0.5 + 0.5 * self.rng.random()\n                    # slight weighting by disparity of node qualities (encourage bridging)\n                    quality_gap = abs(F_arch[i] - F_arch[j])\n                    edge_scores[k] *= 1.0 + 0.2 * (quality_gap / (1.0 + abs(self.f_opt)))\n                # choose how many edges this iteration:\n                remaining = B - calls_made\n                # leave some budget for node jitter and local refinements\n                per_iter_budget = min(max(4, remaining // 10), remaining)\n                m_edges = min(len(edges), max(1, min(self.edge_batch, per_iter_budget // 2)))\n                # choose edges by probability proportional to score but sometimes force novel edges\n                probs = edge_scores / (edge_scores.sum() + 1e-12)\n                chosen_idx = self.rng.choice(len(edges), size=m_edges, replace=False, p=probs)\n                # sample along each chosen edge at multi-scale pattern\n                for ei in chosen_idx:\n                    if calls_made >= B:\n                        break\n                    i, j = int(edges[ei,0]), int(edges[ei,1])\n                    visited = (i, j) in visited_edges\n                    # Mark visited now (we will sample)\n                    visited_edges.add((i, j))\n\n                    xi = X_arch[i]\n                    xj = X_arch[j]\n                    d = xj - xi\n                    L = np.linalg.norm(d) + 1e-12\n                    dir_unit = d / L\n\n                    # choose sampling mode: interpolation, extrapolation, midpoint multi-scale\n                    mode = self.rng.choice(['interp','mid_multi','extrap'], p=[0.6, 0.25, 0.15])\n                    if mode == 'interp':\n                        # sample t biased to interior but sometimes near ends\n                        t = np.clip(self.rng.normal(0.5, 0.18), 0.0, 1.0)\n                        base = (1.0 - t) * xi + t * xj\n                        noise_scale = 0.06 * L + 0.02 * avg_span\n                    elif mode == 'mid_multi':\n                        t = 0.5\n                        base = 0.5 * (xi + xj)\n                        # multi-scale jitter: try small and medium perturbs\n                        noise_scale = (0.03 + 0.03 * self.rng.random()) * (L + avg_span)\n                    else:  # extrapolate small\n                        t = np.clip(self.rng.normal(0.5, 0.25), -0.2, 1.2)\n                        base = (1.0 - t) * xi + t * xj\n                        noise_scale = 0.08 * (L + avg_span)\n\n                    # generate orthogonal noise: sample gaussian then remove projection on dir_unit\n                    z = self.rng.normal(scale=noise_scale, size=self.dim)\n                    proj = np.dot(z, dir_unit) * dir_unit\n                    orth = z - proj\n                    # also add a small parallel jitter\n                    parallel = dir_unit * self.rng.normal(scale=0.01 * (L + avg_span))\n                    candidate = base + orth + parallel\n                    # clip to bounds\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                    # evaluate candidate\n                    f_c = eval_f(candidate)\n                    inserted = try_insert_candidate(candidate, f_c)\n\n                    # If candidate improved a node, perform short directional polish along direction of improvement\n                    if inserted and calls_made < B:\n                        # find index of the (newly) best node in archive\n                        best_idx = int(np.argmin(F_arch))\n                        xbest = X_arch[best_idx].copy()\n                        fbest = float(F_arch[best_idx])\n                        # small directional exploration along random low-dim subspace around xbest\n                        # pick a random direction biased towards the edge direction\n                        rand_dir = self.rng.normal(size=self.dim)\n                        # bias it slightly toward dir_unit\n                        rand_dir = 0.7 * rand_dir + 0.3 * dir_unit\n                        nd = np.linalg.norm(rand_dir)\n                        if nd == 0:\n                            rand_dir = np.ones(self.dim) / np.sqrt(self.dim)\n                        else:\n                            rand_dir = rand_dir / nd\n                        # try a few step sizes decreasing\n                        step0 = max(1e-3 * avg_span, 0.12 * L + 0.02 * avg_span)\n                        steps = [step0 * (0.5**k) for k in range(4)]\n                        for s in steps:\n                            if calls_made >= B:\n                                break\n                            xp = np.clip(xbest + s * rand_dir, lb, ub)\n                            fp = eval_f(xp)\n                            if fp < fbest:\n                                fbest = fp\n                                xbest = xp.copy()\n                                try_insert_candidate(xp, fp)\n                                # continue exploring in same direction (small greedy)\n                                continue\n                            xm = np.clip(xbest - s * rand_dir, lb, ub)\n                            fm = eval_f(xm)\n                            if fm < fbest:\n                                fbest = fm\n                                xbest = xm.copy()\n                                try_insert_candidate(xm, fm)\n                                continue\n                        # optionally update archived arrays reference\n                        if X_arch.shape[0] > 0:\n                            # refresh local arrays from archive (we maintain them on insertion)\n                            pass\n\n                # after edge sampling, do some node-level jittering around top nodes (small budget)\n                # choose top few nodes\n                if calls_made >= B:\n                    break\n                n_jit = min(3, X_arch.shape[0])\n                top_idx = np.argsort(F_arch)[:n_jit]\n                for idx in top_idx:\n                    if calls_made >= B:\n                        break\n                    center = X_arch[idx]\n                    # jitter scales: small and medium\n                    for scale_mult in (0.015, 0.04):\n                        if calls_made >= B:\n                            break\n                        sigma = scale_mult * (span)\n                        cand = np.clip(center + self.rng.normal(scale=sigma), lb, ub)\n                        f_c = eval_f(cand)\n                        try_insert_candidate(cand, f_c)\n\n                # refresh archive arrays in case we mutated them\n                # (For simplicity keep arrays consistent by re-building from current arrays)\n                # Here try_insert_candidate modified X_arch and F_arch in-place, so keep them as is.\n\n                # If no improvement for long, try a random global restart (single sample)\n                if calls_made < B:\n                    # small chance to sample a purely global random point to avoid missing basins\n                    if self.rng.random() < 0.08:\n                        xr = lb + self.rng.random(self.dim) * (ub - lb)\n                        fr = eval_f(xr)\n                        try_insert_candidate(xr, fr)\n\n                # break condition: if almost out of budget, exit loop\n                if B - calls_made < 6:\n                    break\n\n        except StopIteration:\n            # budget exhausted\n            pass\n\n        # Final polishing: use remaining budget to sample small jitter around current best\n        try:\n            while calls_made < B:\n                jitter_scale = 0.01 * (span)\n                x0 = (self.x_opt if self.x_opt is not None else 0.5 * (lb + ub))\n                x_try = np.clip(x0 + self.rng.normal(scale=jitter_scale), lb, ub)\n                f_try = eval_f(x_try)\n                if f_try < self.f_opt:\n                    self.f_opt = f_try\n                    self.x_opt = x_try.copy()\n        except StopIteration:\n            pass\n\n        return float(self.f_opt), (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))", "configspace": "", "generation": 2, "feedback": "In the code, line 894, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities do not sum to 1. See Notes section of docstring for more information.", "error": "In the code, line 894, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities do not sum to 1. See Notes section of docstring for more information.", "parent_ids": ["0228629a-a7a4-4d75-8105-fe131f1f6e92"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2581.0, "Edges": 2580.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.9992251065478497, "Degree Variance": 1.9597049400283206, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.592943201376936, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3307854650168136, "Depth Entropy": 2.166561473272298, "Assortativity": 0.0, "Average Eccentricity": 18.122045718713675, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0003874467260751647, "Average Shortest Path": 10.935686846934516, "mean_complexity": 9.142857142857142, "total_complexity": 64.0, "mean_token_count": 325.0, "total_token_count": 2275.0, "mean_parameter_count": 2.857142857142857, "total_parameter_count": 20.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "701bfc7e-c1fd-40fc-8533-beb7ddd599a9", "fitness": 0.3202278574639151, "name": "EdgeEnrichedAdaptiveGraphSearch", "description": "Edge-Enriched Adaptive Graph Search (EEAGS) \u2014 build a compact elite graph (edges between good solutions) and actively sample/interpolate along and around those edges (with orthogonal perturbations, adaptive local refinement and occasional global injection) to increase the \"edge density\" of explored solutions and exploit promising connections.", "code": "import numpy as np\n\nclass EdgeEnrichedAdaptiveGraphSearch:\n    \"\"\"\n    Edge-Enriched Adaptive Graph Search (EEAGS)\n\n    Key ideas:\n      - Maintain a compact elite population (archive) and an explicit edge graph (k-NN edges).\n      - Sample candidates primarily by interpolating along edges and perturbing orthogonally to the edge\n        (increases exploration *between* known good points -> higher edge density).\n      - Complement edge sampling with local Gaussian refinement, directed mutations along edges,\n        and occasional global/opposite injections to avoid stagnation.\n      - Adapt step-sizes by success history and refresh graph periodically.\n    Works for functions defined on [-5,5]^dim. Enforces exact evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 pop_size=32, knn=6, init_frac=0.12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # population/graph params\n        self.pop_size = max(6, int(pop_size))\n        self.knn = max(1, int(knn))\n        self.init_frac = float(init_frac)\n\n        # step-size / adaptation\n        self.sigma0 = 0.6 * (self.ub - self.lb)  # absolute initial sigma\n        self.sigma = float(self.sigma0)\n        self.sigma_min = 1e-5\n        self.sigma_max = 5.0\n        self.success_shrink = 0.90\n        self.failure_expand = 1.04\n\n        # graph refresh frequency (evaluations)\n        self.edge_refresh = 12\n\n        # other\n        self.max_archive = max(400, 30 * self.dim)\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb)\n        ub = np.full(dim, self.ub)\n        width = ub - lb\n\n        evals = 0\n\n        # initialize with uniform samples\n        n_init = int(min(budget, max(6, int(self.init_frac * budget), 4 * dim)))\n        X_archive = []\n        f_archive = []\n\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            X_archive.append(np.array(x, copy=True))\n            f_archive.append(float(f))\n            evals += 1\n            if evals >= budget:\n                break\n\n        if len(X_archive) == 0:\n            # fallback: sample one point\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_archive.append(x0.copy())\n            f_archive.append(float(f0))\n            evals = 1\n\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # initial population = best pop_size\n        def build_population(Xa, fa):\n            order = np.argsort(fa)\n            k = min(self.pop_size, len(order))\n            inds = order[:k]\n            return Xa[inds].copy(), fa[inds].copy()\n\n        pop_X, pop_f = build_population(X_archive, f_archive)\n        best_idx = int(np.argmin(pop_f))\n        x_best = pop_X[best_idx].copy()\n        f_best = float(pop_f[best_idx])\n\n        # build edge list (unique pairs) from k-NN\n        edges = []       # list of (i,j)\n        edge_weights = None\n\n        def refresh_graph():\n            nonlocal edges, edge_weights, pop_X, pop_f\n            n = pop_X.shape[0]\n            if n < 2:\n                edges = []\n                edge_weights = np.array([])\n                return\n            # pairwise distances\n            D = np.linalg.norm(pop_X[:, None, :] - pop_X[None, :, :], axis=2)\n            # for each node connect to knn nearest neighbors (excluding self)\n            kn = min(self.knn, n - 1)\n            pairs = set()\n            for i in range(n):\n                neigh = np.argsort(D[i])[1:1 + kn]\n                for j in neigh:\n                    a, b = (i, j) if i < j else (j, i)\n                    pairs.add((a, b))\n            edges = sorted(list(pairs))\n            if len(edges) == 0:\n                edge_weights = np.array([])\n                return\n            # compute edge weights: better edges connecting low f get higher weight\n            fmin, fmax = np.min(pop_f), np.max(pop_f)\n            rng_scale = max(fmax - fmin, 1e-9)\n            w = []\n            for (i, j) in edges:\n                # prefer edges where at least one endpoint is good (min f lower)\n                score = (fmin - min(pop_f[i], pop_f[j])) / (rng_scale + 1e-12)\n                # shift and exponentiate to get positive probabilities; add small term for diversity\n                w.append(np.exp(-score * -2.5) + 1e-3)  # edges between good points get larger exp value\n            edge_weights = np.array(w, dtype=float)\n            edge_weights = edge_weights / np.sum(edge_weights)\n\n        refresh_graph()\n        iter_since_improve = 0\n        total_iters = 0\n\n        # helper: pick edge index weighted\n        def choose_edge_idx():\n            if len(edges) == 0:\n                return None\n            # numerically safe weights\n            probs = edge_weights.copy()\n            probs = np.clip(probs, 1e-12, None)\n            probs = probs / probs.sum()\n            return self.rng.choice(len(edges), p=probs)\n\n        # main loop - one evaluation per iteration until budget exhausted\n        while evals < budget:\n            total_iters += 1\n            # adapt strategy probabilities over time\n            frac = evals / max(1.0, budget)\n            p_edge = 0.55 * (1 - frac) + 0.35   # edge sampling stays important\n            p_mut = 0.15 + 0.1 * frac\n            p_local = 0.18 + 0.45 * frac\n            p_global = 1.0 - (p_edge + p_mut + p_local)\n            # safety normalize\n            ps = np.array([p_edge, p_mut, p_local, p_global])\n            ps = np.clip(ps, 1e-6, 1.0)\n            ps = ps / np.sum(ps)\n            p_edge, p_mut, p_local, p_global = ps\n\n            r = self.rng.random()\n\n            if r < p_edge and len(edges) > 0:\n                # Edge interpolation + orthogonal perturbation\n                eidx = choose_edge_idx()\n                if eidx is None:\n                    # fallback\n                    x_candidate = self.rng.uniform(lb, ub)\n                else:\n                    i, j = edges[eidx]\n                    xi = pop_X[i]\n                    xj = pop_X[j]\n                    dir_vec = xj - xi\n                    dir_norm = np.linalg.norm(dir_vec) + 1e-12\n                    u = dir_vec / dir_norm\n                    # sample interpolation coefficient biased toward midpoint but allow extrapolation\n                    alpha = self.rng.beta(2.0, 2.0)  # more mass near 0.5\n                    # small extrapolation\n                    extrap = self.rng.uniform(-0.12, 0.12)\n                    alpha = np.clip(alpha + extrap, -0.3, 1.3)\n                    base = alpha * xi + (1 - alpha) * xj\n                    # orthogonal noise\n                    noise = self.rng.normal(size=dim)\n                    # project noise orthogonal to u\n                    proj_along = np.dot(noise, u) * u\n                    orth = noise - proj_along\n                    orth_norm = np.linalg.norm(orth)\n                    if orth_norm > 1e-12:\n                        orth = orth / orth_norm\n                    else:\n                        # generate random orthogonal by random normal and Gram-Schmidt-like step\n                        orth = noise\n                        orth = orth / (np.linalg.norm(orth) + 1e-12)\n                    # magnitude selection: along-edge small gaussian, orth component scaled by sigma and edge length\n                    along_mag = (alpha - 0.5) * dir_norm * self.rng.normal(loc=1.0, scale=0.3)\n                    orth_mag = self.sigma * (0.6 + 1.6 * self.rng.random()) * (min(dir_norm / (np.mean(width) + 1e-12), 1.2))\n                    x_candidate = base + along_mag * u + orth_mag * orth\n\n            elif r < p_edge + p_mut and pop_X.shape[0] >= 2:\n                # Directed mutation along a chosen edge: parent + fraction*(neighbor-parent) + gaussian\n                # pick a random node weighted by fitness (better nodes more likely)\n                # compute weights (lower f -> higher weight)\n                pf = pop_f.copy()\n                fmin, fmax = pf.min(), pf.max()\n                scale = max(fmax - fmin, 1e-9)\n                w_nodes = np.exp(-(pf - fmin) / scale)\n                w_nodes /= w_nodes.sum()\n                i = self.rng.choice(pop_X.shape[0], p=w_nodes)\n                # choose neighbor j among knn or random\n                if len(edges) > 0:\n                    # find edges that include i\n                    cand_edges_idx = [idx for idx, (a, b) in enumerate(edges) if a == i or b == i]\n                    if len(cand_edges_idx) > 0:\n                        eidx = self.rng.choice(cand_edges_idx)\n                        a, b = edges[eidx]\n                        j = b if a == i else a\n                    else:\n                        j = self.rng.integers(0, pop_X.shape[0])\n                else:\n                    j = self.rng.integers(0, pop_X.shape[0])\n                xi = pop_X[i]\n                xj = pop_X[j]\n                frac_dir = self.rng.uniform(-0.2, 1.2)\n                base = xi + frac_dir * (xj - xi)\n                x_candidate = base + self.sigma * self.rng.normal(size=dim)\n\n            elif r < p_edge + p_mut + p_local:\n                # Local refinement around best (adaptive gaussian)\n                local_sigma = self.sigma * (0.5 + 0.6 * self.rng.random())\n                # anisotropic scale using population std if available\n                if pop_X.shape[0] >= max(3, dim // 2):\n                    stds = np.std(pop_X, axis=0)\n                    # avoid extremely small stds\n                    stds = np.maximum(stds, 0.02 * np.mean(width))\n                    anis = stds / np.mean(stds)\n                    x_candidate = x_best + local_sigma * (self.rng.normal(size=dim) * anis)\n                else:\n                    x_candidate = x_best + local_sigma * self.rng.normal(size=dim)\n\n            else:\n                # Global exploration or opposite sampling\n                if self.rng.random() < 0.6:\n                    # opposite sampling: opposite of a random elite plus small noise\n                    idx = self.rng.integers(0, pop_X.shape[0])\n                    x0 = pop_X[idx]\n                    opp = lb + ub - x0\n                    x_candidate = opp + 0.3 * self.sigma * self.rng.normal(size=dim)\n                else:\n                    x_candidate = self.rng.uniform(lb, ub)\n\n            # clip to bounds\n            x_candidate = np.minimum(np.maximum(x_candidate, lb), ub)\n\n            # evaluate\n            f_candidate = float(func(x_candidate))\n            evals += 1\n\n            # update global archive\n            X_archive = np.vstack([X_archive, x_candidate])\n            f_archive = np.concatenate([f_archive, [f_candidate]])\n\n            # update best\n            improved = False\n            if f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = x_candidate.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # update sigma\n            if improved:\n                self.sigma = max(self.sigma_min, self.sigma * self.success_shrink)\n            else:\n                self.sigma = min(self.sigma_max, self.sigma * self.failure_expand)\n\n            # maintain/populate compact population (replace worst if candidate is better)\n            if pop_X.shape[0] < self.pop_size:\n                # add candidate\n                pop_X = np.vstack([pop_X, x_candidate])\n                pop_f = np.concatenate([pop_f, [f_candidate]])\n            else:\n                # if candidate better than worst, replace one\n                worst_idx = int(np.argmax(pop_f))\n                if f_candidate < pop_f[worst_idx]:\n                    pop_X[worst_idx] = x_candidate\n                    pop_f[worst_idx] = f_candidate\n                else:\n                    # occasionally replace a random element to inject diversity\n                    if self.rng.random() < 0.02:\n                        ridx = self.rng.integers(0, pop_X.shape[0])\n                        pop_X[ridx] = x_candidate\n                        pop_f[ridx] = f_candidate\n\n            # bound archive size\n            if X_archive.shape[0] > self.max_archive:\n                keep_best = int(self.max_archive * 0.6)\n                best_inds = np.argsort(f_archive)[:keep_best]\n                other_inds = self.rng.choice(np.arange(X_archive.shape[0]), size=self.max_archive - keep_best, replace=False)\n                keep = np.unique(np.concatenate([best_inds, other_inds]))\n                X_archive = X_archive[keep]\n                f_archive = f_archive[keep]\n\n            # periodically rebuild population and refresh graph for updated edge weights\n            if (evals % self.edge_refresh) == 0:\n                pop_X, pop_f = build_population(X_archive, f_archive)\n                refresh_graph()\n\n            # emergency diversification: long stagnation -> inject several global samples but evaluate one now\n            if iter_since_improve > max(80, 5 * dim) and self.rng.random() < 0.18:\n                # do a more exploratory candidate next: increase sigma temporarily\n                self.sigma = min(self.sigma_max, self.sigma * (1.6 + 0.8 * self.rng.random()))\n                # chance to replace a worst pop member with a global sample\n                if self.rng.random() < 0.5:\n                    xg = self.rng.uniform(lb, ub)\n                    fg = float(func(xg))\n                    evals += 1\n                    X_archive = np.vstack([X_archive, xg])\n                    f_archive = np.concatenate([f_archive, [fg]])\n                    if fg < pop_f.max():\n                        worst_idx = int(np.argmax(pop_f))\n                        pop_X[worst_idx] = xg\n                        pop_f[worst_idx] = fg\n                    if fg < f_best:\n                        f_best = fg\n                        x_best = xg.copy()\n                        iter_since_improve = 0\n                    # ensure we don't exceed budget in outer loop\n                    if evals >= budget:\n                        break\n\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 2, "feedback": "The algorithm EdgeEnrichedAdaptiveGraphSearch scored 0.320 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9b57bd18-7a38-4bd7-beb0-51edb67f3b17"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8215148441275653}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8167878440362702}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7585807149916985}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0497759713251007}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0456920975217634}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.05388351842404282}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.014340987849823406}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07822186822298094}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.05889721021815675}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.6345273035934385}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8142244417300535}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.6568210599178327}], "aucs": [0.8215148441275653, 0.8167878440362702, 0.7585807149916985, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.0497759713251007, 0.0456920975217634, 0.05388351842404282, 0.014340987849823406, 0.07822186822298094, 0.05889721021815675, 0.6345273035934385, 0.8142244417300535, 0.6568210599178327]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2652.0, "Edges": 2651.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9992458521870287, "Degree Variance": 2.1274504116532325, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.414309484193012, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3242292167494378, "Depth Entropy": 2.174250074865403, "Assortativity": 0.0, "Average Eccentricity": 17.88838612368024, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0003770739064856712, "Average Shortest Path": 10.376595416624706, "mean_complexity": 9.0, "total_complexity": 45.0, "mean_token_count": 464.4, "total_token_count": 2322.0, "mean_parameter_count": 2.2, "total_parameter_count": 11.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "7f2e70a9-363e-4c06-a682-9efcf4327cbc", "fitness": "-inf", "name": "EdgeDensityAmplifiedGraphSearch", "description": "Edge-Density Amplified Graph Search (EDAGS) \u2014 maintain a compact elite set, build a k-NN graph, and actively sample along, across and around graph edges (with orthogonal perturbations and targeted densification) to create many useful connections while mixing in local and global refinements.", "code": "import numpy as np\n\nclass EdgeDensityAmplifiedGraphSearch:\n    \"\"\"\n    Edge-Density Amplified Graph Search (EDAGS)\n\n    Main ideas:\n      - Maintain a compact elite archive (best solutions).\n      - Build a k-NN graph on the elite periodically and score edges by\n        their geometric & fitness promise.\n      - Sample candidates along edges (interpolation, slight extrapolation),\n        and add orthogonal perturbations to create new connections (increase edge density).\n      - Occasionally \"densify\" a promising edge by sampling multiple points along it.\n      - Mix edge-based proposals with local Gaussian refinement around best and\n        occasional global uniform exploration.\n      - Adaptive step sizes and simple success-based adaptation steer sampling scale.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 elite_size=24, k_neighbors=6,\n                 edge_samples=1, densify_samples=6,\n                 sigma0=0.8, sigma_min=1e-4, sigma_max=6.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # algorithm params\n        self.elite_size = int(max(6, elite_size))\n        self.k_neighbors = int(max(1, k_neighbors))\n        self.edge_samples = int(max(1, edge_samples))\n        self.densify_samples = int(max(2, densify_samples))\n\n        # adaptive step control\n        self.sigma0 = float(sigma0) * (self.ub - self.lb)\n        self.sigma = float(sigma0) * (self.ub - self.lb)\n        self.sigma_min = float(sigma_min)\n        self.sigma_max = float(sigma_max)\n\n        # probabilities for strategies (will be adapted)\n        self.p_edge = 0.55\n        self.p_local = 0.30\n        self.p_global = 0.15\n\n        # diagnostics for adaptation\n        self.edge_success_window = []\n        self.local_success_window = []\n        self.window_size = 40\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = int(self.dim)\n        lb = np.full(dim, self.lb, dtype=float)\n        ub = np.full(dim, self.ub, dtype=float)\n        width = ub - lb\n\n        evals = 0\n\n        # Start with an initial Latin-hypercube like uniform seeding (cheap)\n        n_init = int(min(max(12, self.elite_size * 2, int(0.08 * budget)), budget // 3))\n        X = []\n        F = []\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, copy=True))\n            F.append(f)\n            if evals >= budget:\n                break\n\n        X = np.array(X)\n        F = np.array(F)\n\n        # If budget left but no initial points (shouldn't happen), add one\n        if X.shape[0] == 0 and evals < budget:\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            X = np.array([x])\n            F = np.array([f])\n\n        # Build elite archive: keep best up to elite_size\n        def rebuild_elite(X_all, F_all):\n            k = min(self.elite_size, X_all.shape[0])\n            inds = np.argsort(F_all)[:k]\n            return X_all[inds].copy(), F_all[inds].copy()\n\n        X_elite, F_elite = rebuild_elite(X, F)\n\n        best_idx = int(np.argmin(F_elite))\n        x_best = X_elite[best_idx].copy()\n        f_best = float(F_elite[best_idx])\n\n        # ensure sigma scaled to domain\n        self.sigma = float(np.clip(self.sigma0, self.sigma_min, self.sigma_max))\n\n        # helper for kNN graph (returns list of edges (i,j,dist))\n        def knn_edges(Xnodes, k):\n            n = Xnodes.shape[0]\n            if n < 2:\n                return []\n            # pairwise distances (small n so ok)\n            D = np.linalg.norm(Xnodes[:, None, :] - Xnodes[None, :, :], axis=2)\n            edges = []\n            for i in range(n):\n                # pick k nearest excluding self\n                idxs = np.argsort(D[i])[: min(k + 1, n)]  # includes self maybe\n                for j in idxs:\n                    if j == i:\n                        continue\n                    if i < j:\n                        edges.append((i, j, D[i, j]))\n            return edges\n\n        # score edge: combine fitness contrast and geometric proximity and absolute fitness\n        def edge_score(i, j, dist, Fnodes):\n            fi = Fnodes[i]\n            fj = Fnodes[j]\n            mean_f = 0.5 * (fi + fj)\n            diff = abs(fi - fj)\n            # prefer edges with some fitness gradient and moderate distance\n            score = (diff + 1e-9) * (1.0 / (1.0 + dist)) * np.exp(-mean_f / (1.0 + abs(mean_f) + 1e-9))\n            return max(score, 1e-12)\n\n        # generate an edge-based candidate\n        def sample_along_edge(a, b, dist, sigma, allow_extrapolate=0.2):\n            # interpolation coefficient near 0.5, allow small extrapolation\n            alpha = self.rng.normal(loc=0.5, scale=0.16)\n            alpha = np.clip(alpha, -allow_extrapolate, 1.0 + allow_extrapolate)\n            base = alpha * a + (1.0 - alpha) * b\n            # directional component along edge (small)\n            dir_vec = a - b\n            dir_norm = np.linalg.norm(dir_vec) + 1e-12\n            unit_dir = dir_vec / dir_norm\n            # noise vector\n            noise = self.rng.normal(size=dim)\n            # project noise perpendicular to edge to encourage bridging (increase edge density)\n            proj_along = np.dot(noise, unit_dir) * unit_dir\n            orth = noise - proj_along\n            # scale orthogonal perturbation proportional to edge length and sigma\n            orth_scale = (0.6 * sigma) * (dist / (np.mean(width) + 1e-12))\n            # small along-edge jitter\n            along_scale = 0.3 * sigma * (self.rng.normal() * 0.5)\n            candidate = base + along_scale * unit_dir + orth_scale * (orth / (np.linalg.norm(orth) + 1e-12))\n            return candidate\n\n        # main loop: produce one evaluated candidate per iteration and update archive\n        while evals < budget:\n            remaining = budget - evals\n\n            # adapt probabilities based on recent success in windows and remaining budget\n            frac = evals / max(1.0, budget)\n            # tend to use edge strategy more early-to-mid, local more late\n            p_edge = max(0.2, self.p_edge * (1.0 - frac * 0.6))\n            p_local = min(0.6, self.p_local * (0.6 + frac * 0.8))\n            p_global = max(0.02, 1.0 - (p_edge + p_local))\n            ps = np.array([p_edge, p_local, p_global])\n            ps = np.clip(ps, 1e-6, 1.0)\n            ps = ps / np.sum(ps)\n            p_edge, p_local, p_global = ps\n\n            choice = self.rng.random()\n            candidate = None\n            strategy_used = \"global\"\n\n            # Rebuild elite occasionally from full archive to keep it fresh\n            if self.rng.random() < 0.06:\n                X_elite, F_elite = rebuild_elite(np.vstack([X, X_elite]) if X.size else X_elite, np.concatenate([F, F_elite]) if F.size else F_elite)\n                best_idx = int(np.argmin(F_elite))\n                x_best = X_elite[best_idx].copy()\n                f_best = float(F_elite[best_idx])\n\n            if choice < p_edge and X_elite.shape[0] >= 2:\n                # Edge-based: build kNN graph on elite and sample along an edge\n                edges = knn_edges(X_elite, min(self.k_neighbors, X_elite.shape[0]-1))\n                if len(edges) == 0:\n                    # fallback\n                    candidate = self.rng.uniform(lb, ub)\n                    strategy_used = \"global-fallback\"\n                else:\n                    # score edges\n                    scores = np.array([edge_score(i, j, d, F_elite) for (i, j, d) in edges], dtype=float)\n                    # sample an edge proportionally to score\n                    probs = scores / (np.sum(scores) + 1e-12)\n                    idx = self.rng.choice(len(edges), p=probs)\n                    i, j, d = edges[idx]\n                    a = X_elite[i]\n                    b = X_elite[j]\n                    # decide densify occasionally if edge looks promising and budget allows\n                    densify_prob = 0.10 + 0.25 * (scores[idx] / (np.max(scores) + 1e-12))\n                    if self.rng.random() < densify_prob and remaining > 2:\n                        # densify: generate multiple samples along edge (but cap by remaining)\n                        n_densify = min(self.densify_samples, remaining)\n                        for s in range(n_densify):\n                            cand = sample_along_edge(a, b, d, self.sigma)\n                            # clip and evaluate\n                            cand = np.minimum(np.maximum(cand, lb), ub)\n                            f_c = float(func(cand))\n                            evals += 1\n                            # append to global archive arrays\n                            X = np.vstack([X, cand]) if X.size else np.array([cand])\n                            F = np.concatenate([F, [f_c]]) if F.size else np.array([f_c])\n                            # update elite if good\n                            if f_c < np.max(F_elite) or X_elite.shape[0] < self.elite_size:\n                                X_elite, F_elite = rebuild_elite(X, F)\n                                best_idx = int(np.argmin(F_elite))\n                                x_best = X_elite[best_idx].copy()\n                                f_best = float(F_elite[best_idx])\n                            # adapt sigma by local success\n                            if f_c < f_best:\n                                self.edge_success_window.append(1)\n                                if len(self.edge_success_window) > self.window_size:\n                                    self.edge_success_window.pop(0)\n                                f_best = f_c\n                                x_best = cand.copy()\n                            else:\n                                self.edge_success_window.append(0)\n                                if len(self.edge_success_window) > self.window_size:\n                                    self.edge_success_window.pop(0)\n                            # stop early if budget exhausted\n                            if evals >= budget:\n                                break\n                        # proceed to next main loop iteration\n                        continue\n                    else:\n                        # single edge sample\n                        candidate = sample_along_edge(a, b, d, self.sigma)\n                        strategy_used = \"edge\"\n            elif choice < p_edge + p_local:\n                # Local Gaussian refinement around best (trust-region style)\n                strategy_used = \"local\"\n                # anisotropic local steps: scale by per-dim spread of elite if available\n                if X_elite.shape[0] >= max(4, dim):\n                    stds = np.std(X_elite, axis=0)\n                    baseline = max(1e-3, np.mean(stds))\n                    scales = np.maximum(stds, 0.08 * baseline)\n                    # random normal scaled per-dim\n                    candidate = x_best + self.rng.normal(size=dim) * (self.sigma * (scales / baseline))\n                else:\n                    candidate = x_best + self.rng.normal(size=dim) * self.sigma\n            else:\n                # Global uniform exploration\n                strategy_used = \"global\"\n                candidate = self.rng.uniform(lb, ub)\n\n            # Safety: ensure candidate is set\n            if candidate is None:\n                candidate = self.rng.uniform(lb, ub)\n                strategy_used = \"global-fallback-2\"\n\n            # Clip to bounds\n            candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n            # Evaluate candidate\n            f_cand = float(func(candidate))\n            evals += 1\n\n            # Append to global archive\n            X = np.vstack([X, candidate]) if X.size else np.array([candidate])\n            F = np.concatenate([F, [f_cand]]) if F.size else np.array([f_cand])\n\n            # Update elite if warranted\n            replace = False\n            if X_elite.shape[0] < self.elite_size:\n                replace = True\n            else:\n                worst_elite_val = np.max(F_elite)\n                if f_cand < worst_elite_val:\n                    replace = True\n\n            if replace:\n                X_elite, F_elite = rebuild_elite(X, F)\n                best_idx = int(np.argmin(F_elite))\n                x_best = X_elite[best_idx].copy()\n                f_best = float(F_elite[best_idx])\n\n            # Update sigma adaptively based on recent success per strategy\n            if strategy_used.startswith(\"edge\"):\n                succ = 1 if f_cand < f_best else 0\n                self.edge_success_window.append(succ)\n                if len(self.edge_success_window) > self.window_size:\n                    self.edge_success_window.pop(0)\n                # adjust sigma: if too many failures, increase exploration scale slightly to cross gaps\n                rate = np.mean(self.edge_success_window) if self.edge_success_window else 0.0\n                if rate > 0.2:\n                    self.sigma = max(self.sigma_min, self.sigma * 0.92)\n                else:\n                    self.sigma = min(self.sigma_max, self.sigma * 1.03)\n            elif strategy_used.startswith(\"local\"):\n                succ = 1 if f_cand < f_best else 0\n                self.local_success_window.append(succ)\n                if len(self.local_success_window) > self.window_size:\n                    self.local_success_window.pop(0)\n                rate = np.mean(self.local_success_window) if self.local_success_window else 0.0\n                if rate > 0.2:\n                    self.sigma = max(self.sigma_min, self.sigma * 0.90)\n                else:\n                    self.sigma = min(self.sigma_max, self.sigma * 1.02)\n            else:\n                # global attempts cause small sigma increase if failing often\n                # track via combined small metric\n                pass\n\n            # If candidate improved global best, update best (we updated f_best earlier from elite rebuild, but ensure)\n            if f_cand < f_best:\n                f_best = f_cand\n                x_best = candidate.copy()\n\n            # Keep global archive size bounded to avoid memory blowup\n            max_archive = max(600, 40 * dim)\n            if X.shape[0] > max_archive:\n                keep_k = max(self.elite_size, max_archive // 3)\n                best_inds = np.argsort(F)[:keep_k]\n                # fill remaining with uniform random subsample of others\n                rem = max_archive - len(best_inds)\n                other_inds = np.setdiff1d(np.arange(X.shape[0]), best_inds)\n                if len(other_inds) > 0 and rem > 0:\n                    rand_inds = self.rng.choice(other_inds, size=min(rem, len(other_inds)), replace=False)\n                    keep = np.unique(np.concatenate([best_inds, rand_inds]))\n                else:\n                    keep = best_inds\n                X = X[keep]\n                F = F[keep]\n\n            # occasional small random restart: inject a few global points if stagnation\n            if evals > 50 and self.rng.random() < 0.007:\n                # generate a small local cluster around a random elite member or best\n                center = x_best if self.rng.random() < 0.7 else X_elite[self.rng.integers(0, X_elite.shape[0])]\n                n_cluster = min(3, budget - evals)\n                for _ in range(n_cluster):\n                    cand = center + self.rng.normal(size=dim) * (1.5 * self.sigma)\n                    cand = np.minimum(np.maximum(cand, lb), ub)\n                    f_c = float(func(cand))\n                    evals += 1\n                    X = np.vstack([X, cand])\n                    F = np.concatenate([F, [f_c]])\n                    if f_c < f_best:\n                        f_best = f_c\n                        x_best = cand.copy()\n                    if evals >= budget:\n                        break\n\n        # Final best (ensure returned best is from all evaluated points)\n        if F.size > 0:\n            overall_best_idx = int(np.argmin(F))\n            return float(F[overall_best_idx]), np.array(X[overall_best_idx], copy=True)\n        else:\n            return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 2, "feedback": "In the code, line 894, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities do not sum to 1. See Notes section of docstring for more information.", "error": "In the code, line 894, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities do not sum to 1. See Notes section of docstring for more information.", "parent_ids": ["9b57bd18-7a38-4bd7-beb0-51edb67f3b17"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2930.0, "Edges": 2929.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9993174061433447, "Degree Variance": 2.098293049423989, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.358433734939759, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3295053238076935, "Depth Entropy": 2.1850625671711703, "Assortativity": 0.0, "Average Eccentricity": 17.625597269624574, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00034129692832764505, "Average Shortest Path": 10.599783266546027, "mean_complexity": 10.666666666666666, "total_complexity": 64.0, "mean_token_count": 438.3333333333333, "total_token_count": 2630.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 26.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "cb7e7d29-2d16-4500-91f4-8190a6f331d3", "fitness": 0.018772046857237292, "name": "EdgeEnrichedRotationalSearch", "description": "Edge-Enriched Rotational Adaptive Search (EERAS) \u2014 maintain a compact elite archive and actively increase \"edge density\" by sampling along, extrapolating from, and perturbing edges between good solutions while combining rotational Gaussian proposals and adaptive covariance updates to bias search toward productive directions.", "code": "import numpy as np\n\nclass EdgeEnrichedRotationalSearch:\n    \"\"\"\n    Edge-Enriched Rotational Adaptive Search (EERAS)\n\n    Main ideas:\n    - Maintain a small elite archive of best found points.\n    - Increase \"edge density\" by proposing candidates along edges between elites:\n      midpoints, extrapolations and perpendicular-perturbed edge samples.\n    - Combine edge proposals with rotational Gaussian proposals around the best,\n      principal-component long-step probes, opposition samples and occasional global restarts.\n    - Adapt a global covariance (rotational) using recent successful steps (rank-1-like)\n      and adapt sigma by success/failure.\n    - Keep all points clipped to bounds and ensure function is called at most `budget` times.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng=None, elite_size=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.elite_size = int(elite_size) if elite_size is not None else min(8, max(4, self.dim + 1))\n\n        # bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds detection (fallback to [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub =  5.0 * np.ones(self.dim)\n\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        budget = self.budget\n\n        # initial sampling to populate archive\n        init_samples = min(max(6, 3 * self.dim), max(6, budget // 10))\n        archive = []  # list of tuples (f, x)\n        for _ in range(min(init_samples, budget)):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            archive.append((float(f), x.copy()))\n            if evals >= budget:\n                break\n\n        if len(archive) == 0:\n            # one fallback sample\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            archive.append((float(f), x.copy()))\n\n        # sort and keep elites\n        archive.sort(key=lambda t: t[0])\n        archive = archive[: self.elite_size]\n        self.f_opt, self.x_opt = archive[0][0], archive[0][1].copy()\n\n        # init covariance and sigma\n        span = ub - lb\n        initial_sigma = 0.18 * np.linalg.norm(span) / max(1.0, np.sqrt(self.dim))\n        sigma = max(1e-8, initial_sigma)\n        C = np.eye(self.dim)  # normalized covariance (trace ~ dim)\n        eps = 1e-8\n\n        # history of successful steps for PC exploration and covariance updates\n        success_steps = []\n        max_success_history = min(50, 8 * self.dim + 10)\n\n        # adaptation parameters\n        c_cov = 0.18\n        c_success = 1.25\n        c_failure = 0.86\n\n        stagnation = 0\n        stagnation_limit = max(8, 30 // max(1, self.dim // 5))\n\n        # helper: get current elite points and fitness arrays\n        def elite_arrays():\n            fs = np.array([t[0] for t in archive])\n            xs = np.array([t[1] for t in archive])\n            return fs, xs\n\n        # helper: add candidate to archive (keeps sorted)\n        def archive_add(f, x):\n            nonlocal archive, self, sigma\n            t = (float(f), x.copy())\n            archive.append(t)\n            archive.sort(key=lambda s: s[0])\n            # keep unique-ish by closeness: if near an existing point, keep better one only\n            # prune duplicates\n            pruned = []\n            seen = []\n            for ff, xx in archive:\n                too_close = False\n                for yy in seen:\n                    if np.linalg.norm(xx - yy) < 1e-8:\n                        too_close = True\n                        break\n                if not too_close:\n                    pruned.append((ff, xx))\n                    seen.append(xx)\n                if len(pruned) >= 3 * self.elite_size:\n                    break\n            archive = pruned[: self.elite_size]\n            # update global best\n            if f < self.f_opt - 1e-15:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # main loop: propose candidates until budget exhausted\n        while evals < budget:\n            rem = budget - evals\n            # determine a small batch size to allow more archive updates\n            batch = min(rem, max(1, int(min(12, 3 + np.sqrt(self.dim)))))\n            # recompute elites\n            fs, xs = elite_arrays()\n            num_elite = xs.shape[0]\n\n            # small randomness in selecting candidate types\n            # weights: edge (0.45), rotational (0.25), elite_mix (0.12), pc_explore (0.08), opposition (0.05), random global (0.05)\n            for _ in range(batch):\n                if evals >= budget:\n                    break\n\n                # choose proposal type\n                r = self.rng.random()\n                if num_elite >= 2 and r < 0.45:\n                    # Edge-based proposal\n                    # select two elites, prefer nearer fitness but include random\n                    if num_elite > 2 and self.rng.random() < 0.6:\n                        # tournament between random subsets biased to better fitness\n                        idx = self.rng.integers(0, num_elite, size=min(4, num_elite))\n                        idxs = sorted(idx, key=lambda i: fs[i])[:2]\n                        i1, i2 = idxs[0], idxs[1]\n                    else:\n                        i1, i2 = self.rng.choice(num_elite, size=2, replace=False)\n                    a = xs[i1]\n                    b = xs[i2]\n                    v = b - a\n                    norm_v = np.linalg.norm(v)\n                    if norm_v < 1e-12:\n                        # degenerate edge -> small random perturbation around a\n                        base = a\n                        alpha = 0.0\n                    else:\n                        base = a\n                        # allow slight extrapolation beyond edge endpoints\n                        alpha = self.rng.uniform(-0.35, 1.35)\n                    # take a point along the edge (interpolate/extrapolate)\n                    along = base + alpha * v\n\n                    # orthogonal perturbation relative to edge\n                    if norm_v > 1e-12:\n                        uv = v / (norm_v + 1e-16)\n                        z = self.rng.normal(size=self.dim)\n                        # remove component along uv to get orthogonal noise\n                        z -= np.dot(z, uv) * uv\n                        nz = np.linalg.norm(z)\n                        if nz < 1e-12:\n                            # fallback to isotropic noise\n                            perp = self.rng.normal(scale=1.0, size=self.dim)\n                        else:\n                            perp = z / nz\n                        # scale perpendicular noise by factor related to edge length and sigma\n                        perp_scale = sigma * (0.5 + 0.8 * self.rng.random()) * (1.0 + 0.5 * norm_v / (np.linalg.norm(span) + 1e-12))\n                        candidate = along + perp * perp_scale\n                    else:\n                        # small isotropic around base\n                        candidate = along + sigma * self.rng.normal(size=self.dim)\n\n                    # small chance to bias towards midpoint instead (densify center of edge)\n                    if self.rng.random() < 0.12:\n                        candidate = (a + b) * 0.5 + 0.3 * sigma * self.rng.normal(size=self.dim)\n\n                elif r < 0.45 + 0.25:\n                    # Rotational Gaussian around best (ARGS style)\n                    # build sqrt of C\n                    C_pd = C + eps * np.eye(self.dim)\n                    try:\n                        B = np.linalg.cholesky(C_pd)\n                    except np.linalg.LinAlgError:\n                        vals, vecs = np.linalg.eigh(C_pd)\n                        vals = np.clip(vals, 1e-12, None)\n                        B = vecs @ np.diag(np.sqrt(vals))\n                    B = sigma * B\n                    z = self.rng.normal(size=self.dim)\n                    candidate = self.x_opt + B.dot(z)\n\n                elif r < 0.45 + 0.25 + 0.12:\n                    # Elite mix: weighted average of 2-3 elites plus small noise\n                    k = min(num_elite, 1 + self.rng.integers(1, min(4, num_elite) + 1))\n                    ids = self.rng.choice(num_elite, size=k, replace=False)\n                    weights = self.rng.random(size=k)\n                    weights /= weights.sum()\n                    mix = np.sum(xs[ids] * weights[:, None], axis=0)\n                    # bias toward better elites by adding a shift toward global best occasionally\n                    if self.rng.random() < 0.35:\n                        mix = 0.6 * mix + 0.4 * self.x_opt\n                    candidate = mix + 0.6 * sigma * self.rng.normal(size=self.dim)\n\n                elif r < 0.45 + 0.25 + 0.12 + 0.08 and len(success_steps) >= 2:\n                    # principal component long step\n                    S = np.array(success_steps[-min(len(success_steps), max(3, self.dim)):])\n                    if S.shape[0] >= 2:\n                        M = np.cov(S.T)\n                        vals, vecs = np.linalg.eigh(M + 1e-12 * np.eye(self.dim))\n                        pc = vecs[:, -1]\n                        longue = (1.8 + 1.2 * self.rng.random()) * sigma * (self.rng.normal() * pc)\n                        ort = 0.3 * sigma * self.rng.normal(size=self.dim)\n                        candidate = self.x_opt + longue + ort\n                    else:\n                        candidate = self.x_opt + sigma * self.rng.normal(size=self.dim)\n\n                elif r < 0.45 + 0.25 + 0.12 + 0.08 + 0.05:\n                    # opposition sampling around the best\n                    # reflect best across center of bounds and add small noise\n                    center = 0.5 * (lb + ub)\n                    opp = center + (center - self.x_opt)\n                    candidate = opp + 0.6 * sigma * self.rng.normal(size=self.dim)\n\n                else:\n                    # global random (diversify)\n                    candidate = self.rng.uniform(lb, ub)\n\n                # clip candidate into bounds\n                candidate = clip(candidate)\n\n                # Evaluate candidate\n                f_c = func(candidate)\n                evals += 1\n\n                # If improvement, register success and update archive and covariance\n                if f_c < self.f_opt - 1e-15:\n                    # compute step and update covariance (rank-1-ish)\n                    step = candidate - self.x_opt\n                    step_norm = np.linalg.norm(step)\n                    if step_norm > 1e-12:\n                        success_steps.append(step.copy())\n                        if len(success_steps) > max_success_history:\n                            success_steps.pop(0)\n                        v = step / (step_norm + 1e-12)\n                        outer = np.outer(v, v)\n                        # scale update by step length relative to trace to keep scale meaningful\n                        tr = np.trace(C) if np.trace(C) > 1e-12 else float(self.dim)\n                        C = (1.0 - c_cov) * C + c_cov * outer * (step_norm ** 2) / (tr / self.dim + 1e-12)\n                        # normalize trace to ~dim\n                        tr2 = np.trace(C)\n                        if tr2 > 0:\n                            C = C * (self.dim / tr2)\n                    else:\n                        # extremely small step: nudge isotropically\n                        C = (1 - 0.02) * C + 0.02 * np.eye(self.dim)\n\n                    # move center and update sigma\n                    self.x_opt = candidate.copy()\n                    self.f_opt = float(f_c)\n                    sigma = max(1e-12, sigma * c_success)\n                    stagnation = 0\n                    archive_add(f_c, candidate)\n\n                else:\n                    # failure\n                    sigma = max(1e-12, sigma * c_failure)\n                    stagnation += 1\n                    # occasionally accept using simulated annealing acceptance to keep edges dense (rare)\n                    if self.rng.random() < 0.01:\n                        T = 1e-3 + 0.2 * (1.0 - evals / max(1, budget))\n                        if self.rng.random() < np.exp(-(f_c - self.f_opt) / (T + 1e-16)):\n                            # soft acceptance to encourage connecting edges\n                            step = candidate - self.x_opt\n                            if np.linalg.norm(step) > 1e-12:\n                                success_steps.append(step.copy())\n                                if len(success_steps) > max_success_history:\n                                    success_steps.pop(0)\n                                v = step / (np.linalg.norm(step) + 1e-12)\n                                outer = np.outer(v, v)\n                                C = (1.0 - 0.5 * c_cov) * C + 0.5 * c_cov * outer\n                                tr2 = np.trace(C)\n                                if tr2 > 0:\n                                    C = C * (self.dim / tr2)\n                            # but do not update x_opt/f_opt unless better\n                            archive_add(f_c, candidate)\n\n                # stagnation handling: try directed edge densification or small restart\n                if stagnation > stagnation_limit:\n                    stagnation = 0\n                    # perform a small burst of edge densification: sample several midpoints with perpendicular noise\n                    bursts = min(3, max(1, self.elite_size // 2))\n                    for _b in range(bursts):\n                        if evals >= budget:\n                            break\n                        # pick two different elites, biased to better ones\n                        i1, i2 = self.rng.choice(num_elite, size=2, replace=False)\n                        a = xs[i1]\n                        b = xs[i2]\n                        v = b - a\n                        norm_v = np.linalg.norm(v)\n                        if norm_v < 1e-12:\n                            mid = a\n                        else:\n                            mid = 0.5 * (a + b)\n                        perp = sigma * (0.4 + self.rng.random() * 0.8) * self.rng.normal(size=self.dim)\n                        if norm_v > 1e-12:\n                            # make perp orthogonal to v\n                            uv = v / (norm_v + 1e-16)\n                            perp -= np.dot(perp, uv) * uv\n                        trial = clip(mid + perp)\n                        f_t = func(trial)\n                        evals += 1\n                        if f_t < self.f_opt - 1e-15:\n                            step = trial - self.x_opt\n                            if np.linalg.norm(step) > 1e-12:\n                                success_steps.append(step.copy())\n                                if len(success_steps) > max_success_history:\n                                    success_steps.pop(0)\n                                vdir = step / (np.linalg.norm(step) + 1e-12)\n                                outer = np.outer(vdir, vdir)\n                                C = (1.0 - c_cov) * C + c_cov * outer * (np.linalg.norm(step) ** 2)\n                                tr2 = np.trace(C)\n                                if tr2 > 0:\n                                    C = C * (self.dim / tr2)\n                            self.x_opt = trial.copy()\n                            self.f_opt = float(f_t)\n                            sigma *= 1.4\n                            archive_add(f_t, trial)\n                        else:\n                            archive_add(f_t, trial)\n                    # also nudge covariance towards isotropic to regain exploration\n                    C = 0.96 * C + 0.04 * np.eye(self.dim)\n\n                # minor regularization of covariance to maintain PD and correct scaling\n                C = 0.997 * C + 0.003 * np.eye(self.dim)\n\n            # end of batch\n\n        # final clip and return\n        if self.x_opt is not None:\n            self.x_opt = clip(self.x_opt)\n        return float(self.f_opt), np.array(self.x_opt)", "configspace": "", "generation": 3, "feedback": "The algorithm EdgeEnrichedRotationalSearch scored 0.019 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["301b461d-2f9e-49ea-ae17-7d69bab7e52f"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.0984394909278522}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.05557740502422859}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.07592462833084668}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.017536449166720036}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.025130335117100056}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.008522394291811897}], "aucs": [0.0984394909278522, 0.05557740502422859, 0.07592462833084668, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.017536449166720036, 0.025130335117100056, 0.008522394291811897]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2961.0, "Edges": 2960.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.999324552516042, "Degree Variance": 1.8318131202651238, "Transitivity": 0.0, "Max Depth": 19.0, "Min Depth": 2.0, "Mean Depth": 9.954478707782673, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3194742988330295, "Depth Entropy": 2.4395829015972077, "Assortativity": 0.0, "Average Eccentricity": 20.82134414049308, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.00033772374197906115, "Average Shortest Path": 12.010282318792957, "mean_complexity": 12.4, "total_complexity": 62.0, "mean_token_count": 526.4, "total_token_count": 2632.0, "mean_parameter_count": 2.0, "total_parameter_count": 10.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "2c409501-52e9-4505-8ad4-bcf4ade87ef5", "fitness": 0.32267532573135327, "name": "DenseEdgeTriangulationGraph", "description": "Dense Edge Triangulation Graph Walks \u2014 build an ultra-dense elite-focused edge/triangle graph, create many barycentric (triangle) and edge-walk recombinations, adaptively weight successful edges and momentum to concentrate search along productive connections while preserving exploration with L\u00e9vy-like jumps and periodic random injections.", "code": "import numpy as np\n\nclass DenseEdgeTriangulationGraph:\n    \"\"\"\n    Dense Edge Triangulation Graph Walks (DET-Graph)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional args:\n      pop_size: initial population size (will be clipped to budget)\n      elite_frac: fraction of population considered elite for dense graph\n      k_neighbors: local neighbor count for extra edges\n      proposals_per_cycle: number of recombination proposals per main cycle\n      tri_prob: probability to propose triangle barycentric recombination vs edge recombination\n      walk_prob: probability to do graph-walk momentum proposals\n      levy_prob: probability of occasional heavy-tailed jump\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.25,\n                 k_neighbors=6, proposals_per_cycle=60, tri_prob=0.45,\n                 walk_prob=0.35, levy_prob=0.06, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.tri_prob = float(tri_prob)\n        self.walk_prob = float(walk_prob)\n        self.levy_prob = float(levy_prob)\n\n        # sensible default population based on dimension and budget but not larger than budget\n        if pop_size is None:\n            base = int(max(12, min(120, np.clip(np.sqrt(self.budget) * 1.2, 12, 120))))\n            self.pop_size = max(base, self.dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        # will ensure pop_size <= budget in __call__\n        # adaptation constants\n        self.sigma_init = 0.10\n        self.success_boost = 1.25\n        self.failure_shrink = 0.80\n        self.edge_gain = 1.18  # increase edge weight on success\n        self.edge_decay = 0.995  # small decay to forget stale edges\n\n    def _get_bounds(self, func):\n        # try to get bounds from func object if provided, otherwise use [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_norm = np.linalg.norm(ub - lb)\n        sigma_scale = self.sigma_init * range_norm\n\n        # ensure population is not larger than budget (we must evaluate at least initial pop)\n        pop_size = min(self.pop_size, max(2, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population uniformly\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # per-individual sigma and momentum\n        sigma = np.full(self.pop_size, sigma_scale)\n        momentum = np.zeros((self.pop_size, dim), dtype=float)\n\n        # edge weights dictionary keyed by sorted tuple (i,j)\n        edge_weights = {}\n        # helper to get or create weight\n        def get_edge_weight(a, b):\n            key = (int(min(a, b)), int(max(a, b)))\n            return edge_weights.get(key, 1.0)\n\n        def inc_edge_weight(a, b, factor):\n            key = (int(min(a, b)), int(max(a, b)))\n            edge_weights[key] = edge_weights.get(key, 1.0) * factor\n\n        # current best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        # function to build dense edge set emphasizing elites: return list of (i,j)\n        def build_dense_graph():\n            nonlocal edge_weights\n            # soft decay of all edge weights\n            if edge_weights:\n                for k in list(edge_weights.keys()):\n                    edge_weights[k] *= self.edge_decay\n\n            # compute pairwise squared distances\n            X = pop\n            dif = X[:, None, :] - X[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count].tolist()\n\n            edges = []\n            # connect elites into a near-complete subgraph (dense core)\n            for i_idx in range(len(elites)):\n                for j_idx in range(i_idx + 1, len(elites)):\n                    a = elites[i_idx]; b = elites[j_idx]\n                    edges.append((a, b))\n                    # ensure an initial weight exists\n                    key = (min(a, b), max(a, b))\n                    if key not in edge_weights:\n                        edge_weights[key] = 1.0\n\n            # for each elite, add k nearest neighbors (excluding itself)\n            for i in elites:\n                neigh = np.argsort(dist2[i])\n                added = 0\n                for j in neigh:\n                    if j == i: continue\n                    edges.append((i, int(j)))\n                    added += 1\n                    if added >= self.k_neighbors:\n                        break\n\n            # add cross edges between high-ranked non-elites to raise density\n            topk = max(elite_count, min(self.pop_size, elite_count * 2))\n            top_indices = np.argsort(pop_f)[:topk]\n            for i in top_indices:\n                # connect to several random nodes to create diversity\n                for _ in range(2):\n                    j = int(rng.integers(0, self.pop_size))\n                    if j != i:\n                        edges.append((int(i), j))\n\n            # add a batch of random edges to ensure connectivity\n            for _ in range(max(4, self.pop_size // 2)):\n                a = int(rng.integers(0, self.pop_size))\n                b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges.append((a, b))\n\n            # deduplicate\n            edges = list({(min(a, b), max(a, b)) for (a, b) in edges})\n            return edges\n\n        # main loop: propose batches of recombinations that exploit dense edges and triangles\n        cycle = 0\n        while evals < self.budget:\n            remaining = self.budget - evals\n            cycle += 1\n            improved = False\n\n            # build graph every cycle\n            edges = build_dense_graph()\n\n            # precompute adjacency for random-walks\n            adjacency = [[] for _ in range(self.pop_size)]\n            for (a, b) in edges:\n                adjacency[a].append(b)\n                adjacency[b].append(a)\n\n            # proposals_per_cycle but limited by remaining budget\n            proposals = min(self.proposals_per_cycle, remaining)\n            for p in range(proposals):\n                if evals >= self.budget: break\n\n                # choose strategy: triangle barycenter recombination, edge recombination, or graph-walk\n                r = rng.random()\n                if r < self.tri_prob and len(edges) >= 2 and self.pop_size >= 3:\n                    # triangle recombination: pick three distinct nodes biased to elites\n                    # sample from top fraction with some chance\n                    if rng.random() < 0.75:\n                        elite_count = max(3, int(np.ceil(self.elite_frac * self.pop_size)))\n                        candidates = np.argsort(pop_f)[:elite_count]\n                        trip = rng.choice(candidates, size=3, replace=False)\n                    else:\n                        trip = rng.choice(self.pop_size, size=3, replace=False)\n                    a, b, c = int(trip[0]), int(trip[1]), int(trip[2])\n                    Xa, Xb, Xc = pop[a], pop[b], pop[c]\n                    # barycentric weights sampled from a Dirichlet (concentration <1 favors corners)\n                    alpha_dir = 0.7\n                    w = rng.gamma(alpha_dir, 1.0, size=3)\n                    w = w / np.sum(w)\n                    x_prop = w[0] * Xa + w[1] * Xb + w[2] * Xc\n\n                    # occasionally extrapolate outside triangle\n                    if rng.random() < 0.16:\n                        expo = rng.uniform(-0.6, 1.4)\n                        centroid = (Xa + Xb + Xc) / 3.0\n                        x_prop = centroid + expo * (centroid - x_prop)\n\n                    # local noise scaled by local sigmas\n                    local_sigma = (sigma[a] + sigma[b] + sigma[c]) / 3.0\n                    x_prop = x_prop + local_sigma * rng.standard_normal(dim) * 0.6\n\n                    # small orthogonal perturbation within triangle plane for diversity\n                    if dim > 1 and rng.random() < 0.35:\n                        # compute two edge directions\n                        e1 = Xb - Xa\n                        e2 = Xc - Xa\n                        # sample small linear combo orthogonal-ish to centroid\n                        coeffs = rng.standard_normal(2)\n                        ortho = coeffs[0] * e1 + coeffs[1] * e2\n                        x_prop += 0.05 * ortho\n\n                elif r < self.tri_prob + self.walk_prob and any(len(nei) > 0 for nei in adjacency):\n                    # graph-walk: start at a random node biased to elites, walk few steps along weighted edges using momentum\n                    if rng.random() < 0.8:\n                        start_candidates = np.argsort(pop_f)[:max(2, int(self.elite_frac * self.pop_size))]\n                        cur = int(rng.choice(start_candidates))\n                    else:\n                        cur = int(rng.integers(0, self.pop_size))\n                    steps = rng.integers(1, 4)  # 1-3 steps\n                    cur_pos = pop[cur].copy()\n                    cur_idx = cur\n                    for s in range(steps):\n                        neigh = adjacency[cur_idx]\n                        if not neigh:\n                            break\n                        # select neighbor weighted by edge success weights and by quality\n                        weights = np.array([get_edge_weight(cur_idx, nb) for nb in neigh], dtype=float)\n                        # incorporate neighbor quality (better f increases chance)\n                        qualities = np.array([np.exp(-(pop_f[nb] - f_best) / (1e-8 + np.std(pop_f) + 1.0)) for nb in neigh])\n                        probs = weights * qualities\n                        probs = probs / (np.sum(probs) + 1e-12)\n                        nb_idx = int(rng.choice(neigh, p=probs))\n                        direction = pop[nb_idx] - pop[cur_idx]\n                        # step proportionally to sigma and edge weight\n                        step_scale = 0.6 * (sigma[cur_idx] + sigma[nb_idx]) * 0.5\n                        # include momentum influence\n                        mom = momentum[cur_idx] * 0.5\n                        cur_pos = cur_pos + 0.8 * step_scale * (direction / (np.linalg.norm(direction) + 1e-12)) + mom\n                        # move to neighbor index for next hop\n                        cur_idx = nb_idx\n                    x_prop = cur_pos + 0.02 * range_norm * rng.standard_normal(dim)\n\n                else:\n                    # edge recombination: pick an edge (a,b) biased by edge weight and elite status\n                    if rng.random() < 0.85 and edges:\n                        # pick edge with probability proportional to weight\n                        keys = edges\n                        weights = np.array([get_edge_weight(a, b) for (a, b) in keys], dtype=float)\n                        if np.sum(weights) <= 0:\n                            idx = int(rng.integers(0, len(keys)))\n                        else:\n                            probs = weights / np.sum(weights)\n                            idx = int(rng.choice(len(keys), p=probs))\n                        a, b = keys[idx]\n                    else:\n                        a = int(rng.integers(0, self.pop_size)); b = int(rng.integers(0, self.pop_size))\n                        if a == b:\n                            b = (a + 1) % self.pop_size\n                    Xa, Xb = pop[a], pop[b]\n                    # choose interpolation/extrapolation coefficient\n                    style = rng.random()\n                    if style < 0.45:\n                        t = rng.uniform(0.3, 0.7)  # near midpoint\n                    elif style < 0.9:\n                        # biased to better endpoint\n                        if pop_f[a] < pop_f[b]:\n                            t = rng.uniform(0.0, 0.6)\n                        else:\n                            t = rng.uniform(0.4, 1.0)\n                    else:\n                        # extrapolate\n                        t = rng.uniform(-0.8, 1.8)\n                    x_prop = (1 - t) * Xa + t * Xb\n                    # add correlated perturbation along edge plus orthogonal gaussian\n                    edge_dir = Xb - Xa\n                    if np.linalg.norm(edge_dir) > 1e-12:\n                        edge_unit = edge_dir / (np.linalg.norm(edge_dir) + 1e-12)\n                        x_prop += 0.08 * (sigma[a] + sigma[b]) * rng.standard_normal() * edge_unit\n                    x_prop += 0.6 * (sigma[a] + sigma[b]) * 0.5 * rng.standard_normal(dim)\n\n                # occasional Levy-like heavy tail\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    x_prop += 0.14 * range_norm * cauchy\n\n                # clip to bounds\n                x_prop = np.clip(x_prop, lb, ub)\n\n                # evaluate candidate\n                f_prop = float(func(x_prop))\n                evals += 1\n\n                # replacement policy: attempt to insert into population by replacing worst of parents or global worst\n                # find three closest population indices to x_prop (by Euclidean distance)\n                dists = np.sum((pop - x_prop) ** 2, axis=1)\n                nearest = np.argsort(dists)[:max(1, min(3, self.pop_size))]\n                # choose replacement target among nearest that is worse than f_prop if possible\n                replaced = False\n                for idx in nearest:\n                    if f_prop < pop_f[idx]:\n                        # replace\n                        pop[idx] = x_prop\n                        old = pop_f[idx]\n                        pop_f[idx] = f_prop\n                        # adapt sigma and momentum for this individual\n                        sigma[idx] = max(1e-12 * range_norm, sigma[idx] * self.success_boost)\n                        momentum[idx] = 0.7 * momentum[idx] + 0.3 * (x_prop - pop[idx])  # note pop[idx] already set to x_prop -> small effect\n                        replaced = True\n                        # increase weights of edges involved between idx and its nearest neighbors\n                        for nb in nearest:\n                            if nb != idx:\n                                inc_edge_weight(idx, nb, self.edge_gain)\n                        break\n                if not replaced:\n                    # maybe replace global worst with small probability if it's better\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        pop[idx_worst] = x_prop\n                        pop_f[idx_worst] = f_prop\n                        sigma[idx_worst] = max(1e-12 * range_norm, sigma[idx_worst] * self.success_boost)\n                        momentum[idx_worst] = 0.2 * (x_prop - pop[idx_worst])\n                        replaced = True\n\n                # if replacement occurred, reward edge(s) that produced candidate\n                if replaced:\n                    improved = True\n                    # reward edges between involved parents if applicable\n                    # try to attribute to closest two population points used to form candidate\n                    if len(nearest) >= 2:\n                        inc_edge_weight(nearest[0], nearest[1], self.edge_gain)\n\n                # global best update\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n                    improved = True\n\n                # small failure penalty for sigmas of the nearest nodes if not replaced\n                if not replaced:\n                    for idx in nearest:\n                        sigma[idx] *= self.failure_shrink\n\n                # periodically re-evaluate elite_count to update graph in next cycle\n                if evals >= self.budget:\n                    break\n\n            # end proposals in cycle\n\n            # adaptive housekeeping: clamp sigma, momentum dampening\n            sigma = np.clip(sigma, 1e-10 * range_norm, 2.0 * range_norm)\n            momentum *= 0.94  # slow decay\n\n            # occasional stochastic injection (diversify)\n            if rng.random() < 0.05 and evals < self.budget:\n                newp = rng.uniform(lb, ub)\n                f_new = float(func(newp))\n                evals += 1\n                idx_w = int(np.argmax(pop_f))\n                if f_new < pop_f[idx_w]:\n                    pop[idx_w] = newp\n                    pop_f[idx_w] = f_new\n                    sigma[idx_w] = sigma_scale\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # stagnation handling: if no improvement for some cycles, reseed some population around best and random\n            if not improved:\n                # small chance to reseed a few\n                if rng.random() < 0.18 and evals < self.budget:\n                    n_reset = max(1, self.pop_size // 6)\n                    for _ in range(n_reset):\n                        if evals >= self.budget: break\n                        idx = int(rng.integers(0, self.pop_size))\n                        if rng.random() < 0.8:\n                            candidate = x_best + 0.06 * (ub - lb) * rng.standard_normal(dim)\n                        else:\n                            candidate = rng.uniform(lb, ub)\n                        candidate = np.clip(candidate, lb, ub)\n                        f_c = float(func(candidate))\n                        evals += 1\n                        if f_c < pop_f[idx]:\n                            pop[idx] = candidate\n                            pop_f[idx] = f_c\n                            sigma[idx] = sigma_scale\n                            momentum[idx] = 0.0\n                            if f_c < f_best:\n                                f_best = f_c; x_best = candidate.copy()\n\n            # if budget low, break to local refinement\n            if self.budget - evals < max(8, dim * 2):\n                break\n\n        # final local refinement: pattern search around best with decreasing step\n        step = 0.12 * range_norm\n        step_min = 1e-6 * range_norm\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # positive direction\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                # negative direction\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n\n        # store results\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm DenseEdgeTriangulationGraph scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5713cbd-1130-4e8d-abdc-3c3ca8966b71"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8012198876217059}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7875704963852296}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7913862683279855}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08556223462107615}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06944837989454267}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08468239724465854}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.08229733288353203}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0663032402302608}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0656919455269972}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.6551885851669486}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.5714727983345744}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.7791563197327878}], "aucs": [0.8012198876217059, 0.7875704963852296, 0.7913862683279855, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.08556223462107615, 0.06944837989454267, 0.08468239724465854, 0.08229733288353203, 0.0663032402302608, 0.0656919455269972, 0.6551885851669486, 0.5714727983345744, 0.7791563197327878]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3583.0, "Edges": 3582.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9994418085403294, "Degree Variance": 2.0122799005350487, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 9.037469287469287, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3280834231403214, "Depth Entropy": 2.289129798402576, "Assortativity": 1.3875993833718095e-08, "Average Eccentricity": 20.873011442924923, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.00027909572983533354, "Average Shortest Path": 11.448847799016168, "mean_complexity": 15.333333333333334, "total_complexity": 92.0, "mean_token_count": 522.3333333333334, "total_token_count": 3134.0, "mean_parameter_count": 3.3333333333333335, "total_parameter_count": 20.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "a6bac7d7-5d8f-49d5-aaf6-24ab3df4da15", "fitness": 0.20291776655710578, "name": "EdgeAugmentedAdaptiveRotationalGaussian", "description": "Edge-Augmented Adaptive Rotational Gaussian (EA-ARGS) \u2014 maintain a compact elite archive, build a dense edge graph among elites and repeatedly sample midpoints, extrapolations and multi-edge mixes (with orthogonal perturbations) combined with a rotational Gaussian; adapt a global covariance and per-elite/edge weights from successes to densify productive edges and improve connectivity.", "code": "import numpy as np\n\nclass EdgeAugmentedAdaptiveRotationalGaussian:\n    \"\"\"\n    Edge-Augmented Adaptive Rotational Gaussian Search (EA-ARGS)\n\n    Key ideas:\n    - Maintain a compact elite archive of good solutions.\n    - Build a dense edge graph among elites (k-NN + selected pairs).\n    - Sample heavily along edges: midpoints, extrapolations, edge-intersections (mix of edges),\n      and add orthogonal perturbations sampled from an adaptive rotational Gaussian.\n    - Keep a global covariance C adapted from successful steps (rank-1 updates) and a global sigma.\n    - Track per-elite/edge successes to bias sampling toward productive connections (increase \"edge density\").\n    - Use reflection for bound handling to avoid boundary pile-up.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, archive_size=None, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = archive_size if archive_size is not None else max(6, min(2 * self.dim, 14))\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds if provided by func, otherwise use [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub =  5.0 * np.ones(self.dim)\n\n        # ensure shapes\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        span = ub - lb\n        span = np.where(span <= 0, 1.0, span)\n\n        # bounds reflection function (vectorized) \u2014 keeps points inside [lb,ub] by reflecting across boundaries\n        def reflect(x):\n            # map to [0, 2*span) relative coordinate\n            t = (x - lb) / span\n            t_mod = np.mod(t, 2.0)\n            inside = np.where(t_mod <= 1.0, t_mod, 2.0 - t_mod)\n            return lb + inside * span\n\n        # clipping fallback\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        budget = self.budget\n\n        # initial sampling: small set to seed archive\n        n_init = min(max(6, 4 * self.dim), max(6, self.budget // 10))\n        archive = []  # list of dicts: {'x':..., 'f':..., 'succ':int}\n        for _ in range(n_init):\n            if evals >= budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            entry = {'x': x.copy(), 'f': float(f), 'succ': 0}\n            archive.append(entry)\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n        if len(archive) == 0:\n            # last resort\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            self.f_opt = float(f)\n            self.x_opt = x.copy()\n            archive.append({'x': self.x_opt.copy(), 'f': self.f_opt, 'succ': 0})\n\n        # sort and trim archive\n        archive = sorted(archive, key=lambda e: e['f'])\n        archive = archive[: self.archive_size]\n        # ensure best included\n        best_entry = archive[0]\n        self.x_opt = best_entry['x'].copy()\n        self.f_opt = float(best_entry['f'])\n\n        # global adaptive parameters\n        initial_sigma = 0.18 * np.linalg.norm(span) / max(1.0, np.sqrt(self.dim))\n        sigma = max(1e-8, initial_sigma)\n        C = np.eye(self.dim)  # covariance (normalized)\n        eps = 1e-8\n        c_cov = 0.18  # covariance learning rate\n        c_succ = 1.16\n        c_fail = 0.88\n\n        # structures to encourage edge density\n        edge_success = {}  # key (i,j) sorted indices -> success count\n        max_success_history = min(50, 8 * self.dim + 20)\n        recent_success_steps = []\n\n        stagnation = 0\n        stagnation_limit = max(8, 40 // (self.dim // 5 + 1))\n\n        # helper: compute B = sqrt(C)*sigma (matrix) robustly\n        def cov_sqrt_mul_sigma(Cmat, sig):\n            Cpd = Cmat + eps * np.eye(self.dim)\n            try:\n                B = np.linalg.cholesky(Cpd)\n            except np.linalg.LinAlgError:\n                vals, vecs = np.linalg.eigh(Cpd)\n                vals = np.clip(vals, 1e-12, None)\n                B = vecs @ np.diag(np.sqrt(vals))\n            return sig * B\n\n        # helper: rebuild edges among current archive\n        def build_edges(archive_list, k=3, max_pairs=200):\n            n = len(archive_list)\n            if n < 2:\n                return []\n            X = np.vstack([e['x'] for e in archive_list])\n            # pairwise distances\n            d2 = np.sum((X[:, None, :] - X[None, :, :]) ** 2, axis=2)\n            # avoid self pairs\n            edges = set()\n            # k-NN edges\n            for i in range(n):\n                neigh = np.argsort(d2[i])  # ascending (includes self)\n                count = 0\n                for j in neigh:\n                    if j == i:\n                        continue\n                    a, b = (i, j) if i < j else (j, i)\n                    edges.add((a, b))\n                    count += 1\n                    if count >= k:\n                        break\n            # add some random extra pairs boosting density among top elites\n            topn = min(n, max(3, n // 2))\n            for i in range(topn):\n                for j in range(i + 1, min(n, i + 4)):\n                    edges.add((i, j))\n                    if len(edges) >= max_pairs:\n                        break\n                if len(edges) >= max_pairs:\n                    break\n            edges = list(edges)\n            # shuffle edges to randomize sampling order\n            self.rng.shuffle(edges)\n            return edges\n\n        # main loop\n        while evals < budget:\n            rem = budget - evals\n            batch = min(rem, max(1, int(min(28, 6 + np.sqrt(self.dim) * 2))))\n            # rebuild edges every loop\n            edges = build_edges(archive, k=3, max_pairs=300)\n            # generate candidates\n            candidates = []\n            strategies = []\n            B = cov_sqrt_mul_sigma(C, sigma)\n\n            for b in range(batch):\n                roll = self.rng.random()\n                # with moderate probability sample along edges / mixes, otherwise global Gaussian\n                if len(edges) > 0 and roll < 0.68:\n                    # choose an edge with weighting by past successes and inverse length (favor closer edges)\n                    edge_idx = self.rng.integers(0, len(edges))\n                    i, j = edges[edge_idx]\n                    xi = archive[i]['x']\n                    xj = archive[j]['x']\n                    vec = xj - xi\n                    L = np.linalg.norm(vec) + 1e-12\n                    u = vec / L\n                    # edge weight influences alpha scale\n                    key = (i, j) if i < j else (j, i)\n                    esucc = edge_success.get(key, 0.5)\n                    # draw one of three edge-driven types: midpoint perturbation, extrapolation, multi-edge mix\n                    t = self.rng.random()\n                    if t < 0.45:\n                        # midpoint with directed displacement proportional to edge and sigma\n                        alpha = self.rng.normal(0.0, 0.4 + 0.6 * (1.0 / (1.0 + np.exp(- (esucc - 1.0)))))  # adaptive\n                        base = 0.5 * (xi + xj)\n                        # orthogonal noise\n                        z = self.rng.standard_normal(self.dim)\n                        z_perp = z - np.dot(z, u) * u\n                        if np.linalg.norm(z_perp) < 1e-12:\n                            z_perp = self.rng.standard_normal(self.dim)\n                        z_perp = z_perp / (np.linalg.norm(z_perp) + 1e-12)\n                        orth_scale = sigma * (0.6 + 0.8 * (1.0 / (1.0 + esucc)))\n                        y = base + alpha * (vec) + orth_scale * z_perp * self.rng.normal()\n                    elif t < 0.82:\n                        # extrapolation from better endpoint toward worse and beyond\n                        # identify better endpoint\n                        if archive[i]['f'] <= archive[j]['f']:\n                            better = xi\n                            worse = xj\n                        else:\n                            better = xj\n                            worse = xi\n                        # extrapolate from better toward worse with noise\n                        beta = 0.15 + 0.6 * self.rng.random()\n                        direction = worse - better\n                        y = better + (1.0 + beta) * direction + B.dot(self.rng.standard_normal(self.dim)) * 0.4\n                    else:\n                        # multi-edge mix: pick another random edge that shares a node if possible\n                        other_edge = edges[self.rng.integers(0, len(edges))]\n                        a, b = other_edge\n                        # choose three unique elites if possible\n                        idxs = list({i, j, a, b})\n                        if len(idxs) < 2:\n                            base = 0.5 * (xi + xj)\n                        else:\n                            weights = np.array(self.rng.random(len(idxs)))\n                            weights = weights / np.sum(weights)\n                            X = np.vstack([archive[k]['x'] for k in idxs])\n                            base = weights.dot(X)\n                        # small directed step along combined edge directions plus gaussian\n                        y = base + 0.6 * sigma * self.rng.standard_normal(self.dim) + 0.6 * B.dot(self.rng.standard_normal(self.dim))\n                    # reflect into bounds\n                    y = reflect(y)\n                    candidates.append(y)\n                    strategies.append(('edge', key))\n                else:\n                    # global rotational-gaussian step around current best (exploit)\n                    z = self.rng.standard_normal(self.dim)\n                    y = self.x_opt + B.dot(z)\n                    # occasional opposition sampling (try opposite of current best)\n                    if self.rng.random() < 0.06:\n                        opp = lb + ub - self.x_opt\n                        y = 0.5 * (y + opp) + 0.2 * B.dot(self.rng.standard_normal(self.dim))\n                    y = reflect(y)\n                    candidates.append(y)\n                    strategies.append(('global', None))\n\n            # Evaluate candidates in sequence, keep best of batch\n            best_cand = None\n            best_cand_f = np.inf\n            best_strategy = None\n            best_key = None\n            for idx, y in enumerate(candidates):\n                if evals >= budget:\n                    break\n                f_y = func(y)\n                evals += 1\n                if f_y < best_cand_f:\n                    best_cand_f = float(f_y)\n                    best_cand = y.copy()\n                    best_strategy = strategies[idx][0]\n                    best_key = strategies[idx][1]\n\n            if best_cand is None:\n                break\n\n            # Acceptance / adaptation\n            if best_cand_f + 1e-15 < self.f_opt:\n                # success\n                step = best_cand - self.x_opt\n                norm_step = np.linalg.norm(step)\n                # record step\n                if norm_step > 0:\n                    recent_success_steps.append(step)\n                    if len(recent_success_steps) > max_success_history:\n                        recent_success_steps.pop(0)\n                    # rank-1 covariance update using normalized step direction scaled by step length\n                    v = step / (norm_step + 1e-12)\n                    outer = np.outer(v, v)\n                    # scale outer by magnitude effect normalized by current trace\n                    tr = np.trace(C)\n                    scale = (norm_step ** 2) / ( (tr / self.dim) + 1e-12)\n                    C = (1.0 - c_cov) * C + c_cov * outer * scale\n                    # maintain trace ~ dim\n                    tr2 = np.trace(C)\n                    if tr2 > 0:\n                        C = C * (self.dim / tr2)\n                # update best\n                prev_best = self.x_opt.copy()\n                self.x_opt = best_cand.copy()\n                self.f_opt = float(best_cand_f)\n                sigma = max(1e-12, sigma * c_succ)\n                stagnation = 0\n                # update archive: insert and keep sorted unique-ish\n                archive.append({'x': self.x_opt.copy(), 'f': self.f_opt, 'succ': 1})\n                # trim duplicate-like entries: keep unique by ~distance\n                keep = []\n                pts = []\n                for e in sorted(archive, key=lambda e: e['f']):\n                    x = e['x']\n                    # if too close to an already kept point, skip (but allow a few)\n                    too_close = False\n                    for p in pts:\n                        if np.linalg.norm(x - p) < 1e-6 * np.linalg.norm(span) + 1e-12:\n                            too_close = True\n                            break\n                    if not too_close:\n                        keep.append(e)\n                        pts.append(x)\n                    if len(keep) >= self.archive_size:\n                        break\n                archive = keep\n                # reward edge if candidate was from edge strategy\n                if best_strategy == 'edge' and best_key is not None:\n                    edge_success[best_key] = edge_success.get(best_key, 0) + 1\n                    # cap\n                    if edge_success[best_key] > 200:\n                        edge_success[best_key] = 200\n                # slightly nudge covariance towards edges connectivity by adding small isotropic blend\n                C = 0.995 * C + 0.005 * np.eye(self.dim)\n            else:\n                # no improvement\n                sigma = max(1e-12, sigma * c_fail)\n                stagnation += 1\n                # penalize edge if used\n                if best_strategy == 'edge' and best_key is not None:\n                    edge_success[best_key] = max(0.0, edge_success.get(best_key, 0.5) - 0.5)\n                # if stagnation persists, densify sampling along edges and occasionally inject new random elites\n                if stagnation > stagnation_limit:\n                    # try a bridge candidate (random edge midpoint perturbation) immediately if budget allows\n                    if len(edges) > 0 and evals < budget:\n                        e_idx = self.rng.integers(0, len(edges))\n                        i, j = edges[e_idx]\n                        xi = archive[i]['x']\n                        xj = archive[j]['x']\n                        mid = 0.5 * (xi + xj)\n                        cand = mid + 1.2 * sigma * self.rng.standard_normal(self.dim)\n                        cand = reflect(cand)\n                        f_c = func(cand)\n                        evals += 1\n                        if f_c < self.f_opt:\n                            self.x_opt = cand.copy()\n                            self.f_opt = float(f_c)\n                            sigma *= 1.3\n                            stagnation = 0\n                            # store success\n                            k = (i, j) if i < j else (j, i)\n                            edge_success[k] = edge_success.get(k, 0) + 1\n                            archive.append({'x': self.x_opt.copy(), 'f': self.f_opt, 'succ': 1})\n                            archive = sorted(archive, key=lambda e: e['f'])[: self.archive_size]\n                        else:\n                            # nudge covariance isotropically to escape\n                            C = 0.96 * C + 0.04 * np.eye(self.dim)\n                    # occasionally inject a fresh random point to increase graph nodes\n                    if self.rng.random() < 0.6 and evals < budget:\n                        randp = self.rng.uniform(lb, ub)\n                        f_r = func(randp)\n                        evals += 1\n                        archive.append({'x': randp.copy(), 'f': float(f_r), 'succ': 0})\n                        archive = sorted(archive, key=lambda e: e['f'])[: self.archive_size]\n                        if f_r < self.f_opt:\n                            self.x_opt = randp.copy()\n                            self.f_opt = float(f_r)\n                            stagnation = 0\n                    # reset stagnation occasionally\n                    if stagnation > stagnation_limit * 3:\n                        stagnation = 0\n                        sigma = max(initial_sigma * 0.5, sigma * 0.9)\n                        C = np.eye(self.dim)\n\n            # small regularization to keep pd\n            C = 0.9995 * C + 0.0005 * np.eye(self.dim)\n\n        # final ensure clipped/reflected\n        if self.x_opt is not None:\n            self.x_opt = reflect(self.x_opt)\n\n        return float(self.f_opt), np.array(self.x_opt)", "configspace": "", "generation": 3, "feedback": "The algorithm EdgeAugmentedAdaptiveRotationalGaussian scored 0.203 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["301b461d-2f9e-49ea-ae17-7d69bab7e52f"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9012808373927268}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9054354481706401}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8953353445417651}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.0002631296359896229}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02909891606715498}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03540909665192371}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.03654422835537863}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0073951826430782575}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0002461103612596549}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.008433803414811702}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.08475363523530055}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.07058121211134616}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.06888955377521178}], "aucs": [0.9012808373927268, 0.9054354481706401, 0.8953353445417651, 4.999999999999449e-05, 4.999999999999449e-05, 0.0002631296359896229, 0.02909891606715498, 0.03540909665192371, 0.03654422835537863, 0.0073951826430782575, 0.0002461103612596549, 0.008433803414811702, 0.08475363523530055, 0.07058121211134616, 0.06888955377521178]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3078.0, "Edges": 3077.0, "Max Degree": 37.0, "Min Degree": 1.0, "Mean Degree": 1.999350227420403, "Degree Variance": 2.143599317886563, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.913963328631876, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.324443274448531, "Depth Entropy": 2.265770288793316, "Assortativity": 0.0, "Average Eccentricity": 19.504223521767383, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0003248862897985705, "Average Shortest Path": 11.10520381889738, "mean_complexity": 11.5, "total_complexity": 69.0, "mean_token_count": 450.8333333333333, "total_token_count": 2705.0, "mean_parameter_count": 2.3333333333333335, "total_parameter_count": 14.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "f991f47c-f205-45b5-abb3-b9e355c8a42b", "fitness": 0.22681655618993446, "name": "GraphAugmentedEdgeDensification", "description": "Graph-Augmented Edge Densification with Mesh Bridging (GED-MB) \u2014 actively densify and \"fill\" promising edges by creating small adaptive meshes and orthogonal bridges between good points, prioritizing long-but-good edges, adaptively splitting edges that yield improvements, and balancing local refinement with global injections to increase edge density and discover connecting valleys.", "code": "import numpy as np\n\nclass GraphAugmentedEdgeDensification:\n    \"\"\"\n    Graph-Augmented Edge Densification with Mesh Bridging (GED-MB)\n\n    Main ideas:\n      - Maintain a compact archive and build a k-NN edge graph among top solutions.\n      - Prioritize edges that connect relatively good endpoints and have useful length\n        (long edges help bridge basins). Track how often an edge was sampled and\n        penalize over-sampled edges to force densification elsewhere.\n      - For a selected edge, generate a small adaptive mesh of candidates along the\n        edge (multiple t positions) plus orthogonal bridge perturbations using a\n        small orthonormal subspace (derived by projecting random vectors off the\n        edge vector). Evaluate candidates sequentially until budget is reached.\n      - If a midpoint improves substantially, split the edge (add node) to increase\n        local edge density. Also perform local Gaussian refinement around best and\n        occasional global/opposite sampling.\n      - Adaptive sigma shrinks on success and grows on failure.\n    Works in [-5,5]^dim. Enforces exact evaluation budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 pop_size=28, knn=5, init_frac=0.10,\n                 densify_points=3, mesh_orth=2, rebuild_every=10):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # population/graph params\n        self.pop_size = max(6, int(pop_size))\n        self.knn = max(1, int(knn))\n        self.init_frac = float(init_frac)\n\n        # mesh densification params\n        self.densify_points = max(1, int(densify_points))  # number of base positions along edge\n        self.mesh_orth = max(0, int(mesh_orth))  # number of orthogonal perturbation directions per base\n        self.rebuild_every = max(1, int(rebuild_every))\n\n        # step-size / adaptation\n        self.sigma0 = 0.5 * (self.ub - self.lb)\n        self.sigma = float(self.sigma0)\n        self.sigma_min = 1e-6\n        self.sigma_max = 6.0\n        self.success_shrink = 0.85\n        self.failure_expand = 1.06\n\n        # archive cap\n        self.max_archive = max(400, 40 * self.dim)\n\n        # small epsilon\n        self._eps = 1e-12\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb)\n        ub = np.full(dim, self.ub)\n        width = ub - lb\n\n        evals = 0\n\n        # initial sampling: a modest spread\n        n_init = int(min(budget, max(8, int(self.init_frac * budget), 6 * dim)))\n        X_archive = []\n        f_archive = []\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            X_archive.append(np.array(x, copy=True))\n            f_archive.append(float(f))\n            evals += 1\n            if evals >= budget:\n                break\n\n        if len(X_archive) == 0:\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_archive.append(x0.copy())\n            f_archive.append(float(f0))\n            evals = 1\n\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # helper: get compact population (best)\n        def build_population(Xa, fa):\n            order = np.argsort(fa)\n            k = min(self.pop_size, len(order))\n            inds = order[:k]\n            return Xa[inds].copy(), fa[inds].copy(), inds\n\n        pop_X, pop_f, pop_inds = build_population(X_archive, f_archive)\n        best_idx = int(np.argmin(pop_f))\n        x_best = pop_X[best_idx].copy()\n        f_best = float(pop_f[best_idx])\n\n        # build graph: edges as list of tuples on pop indices (0..n-1)\n        edges = []\n        edge_weights = np.array([])\n        edge_counts = np.array([])  # sampling counts per edge\n\n        def refresh_graph():\n            nonlocal edges, edge_weights, edge_counts, pop_X, pop_f\n            n = pop_X.shape[0]\n            edges = []\n            if n < 2:\n                edge_weights = np.array([])\n                edge_counts = np.array([])\n                return\n            # pairwise distances\n            D = np.linalg.norm(pop_X[:, None, :] - pop_X[None, :, :], axis=2)\n            kn = min(self.knn, n - 1)\n            pairs = set()\n            for i in range(n):\n                neigh = np.argsort(D[i])[1:1 + kn]\n                for j in neigh:\n                    a, b = (i, j) if i < j else (j, i)\n                    pairs.add((a, b))\n            edges = sorted(list(pairs))\n            if len(edges) == 0:\n                edge_weights = np.array([])\n                edge_counts = np.array([])\n                return\n            # compute weights favoring good endpoints and giving some bonus to moderate/long edges\n            fmin, fmax = np.min(pop_f), np.max(pop_f)\n            rng_scale = max(fmax - fmin, 1e-9)\n            weights = []\n            for (i, j) in edges:\n                meanf = 0.5 * (pop_f[i] + pop_f[j])\n                # normalized quality (0 best -> 1 worst)\n                q = (meanf - fmin) / (rng_scale + self._eps)\n                # length score normalized by domain diagonal\n                length = D[i, j]\n                length_norm = length / (np.linalg.norm(width) + self._eps)\n                # weight: prefer lower q (good) and moderately long edges (encourage bridging)\n                w = np.exp(-4.0 * q) * (1.0 + 1.8 * np.tanh(2.5 * length_norm))\n                weights.append(w + 1e-8)\n            edge_weights = np.array(weights, dtype=float)\n            # initialize counts to zero when rebuilding\n            edge_counts = np.zeros(len(edges), dtype=float)\n            # normalize weights\n            edge_weights = np.clip(edge_weights, 1e-12, None)\n            edge_weights = edge_weights / edge_weights.sum()\n\n        refresh_graph()\n        iter_since_improve = 0\n        total_iters = 0\n        last_rebuild_at = evals\n\n        # helper: choose edge idx with penalty by sampling counts\n        def choose_edge_idx():\n            nonlocal edge_weights, edge_counts\n            if len(edges) == 0:\n                return None\n            # penalize frequently sampled edges\n            penal = 1.0 / (1.0 + 0.9 * edge_counts)\n            probs = edge_weights * penal\n            probs = np.clip(probs, 1e-12, None)\n            probs = probs / probs.sum()\n            return self.rng.choice(len(edges), p=probs)\n\n        # helper: generate orthonormal directions orthogonal to u (at most mesh_orth)\n        def orthonormal_basis(u, k):\n            # u: normalized vector\n            basis = []\n            if k <= 0:\n                return np.array(basis)\n            # produce k random vectors and orthogonalize\n            for _ in range(k):\n                v = self.rng.normal(size=dim)\n                # make v orthogonal to u\n                v = v - np.dot(v, u) * u\n                norm = np.linalg.norm(v)\n                if norm < 1e-12:\n                    # fallback: random and try again\n                    attempts = 0\n                    while norm < 1e-12 and attempts < 5:\n                        v = self.rng.normal(size=dim)\n                        v = v - np.dot(v, u) * u\n                        norm = np.linalg.norm(v)\n                        attempts += 1\n                    if norm < 1e-12:\n                        # give up, return whatever we have\n                        break\n                basis.append(v / (norm + self._eps))\n            if len(basis) == 0:\n                return np.zeros((0, dim))\n            return np.vstack(basis)\n\n        # main loop\n        while evals < budget:\n            total_iters += 1\n            frac = evals / max(1.0, budget)\n            # strategy mix evolves: early focus on edges, later refine local\n            p_edge = 0.6 * (1.0 - frac) + 0.25\n            p_split = 0.12 + 0.08 * frac\n            p_local = 0.15 + 0.5 * frac\n            p_global = max(0.0, 1.0 - (p_edge + p_split + p_local))\n            ps = np.array([p_edge, p_split, p_local, p_global])\n            ps = np.clip(ps, 1e-8, None)\n            ps = ps / ps.sum()\n            p_edge, p_split, p_local, p_global = ps\n\n            r = self.rng.random()\n\n            # Candidate batch (a small list of candidates evaluated sequentially)\n            candidates = []\n\n            if r < p_edge and len(edges) > 0:\n                # Edge mesh densification: choose an edge and build a small mesh along it\n                eidx = choose_edge_idx()\n                if eidx is None:\n                    # fallback global\n                    candidates.append(self.rng.uniform(lb, ub))\n                else:\n                    i, j = edges[eidx]\n                    xi = pop_X[i]\n                    xj = pop_X[j]\n                    dir_vec = xj - xi\n                    edge_len = np.linalg.norm(dir_vec) + self._eps\n                    u = dir_vec / (edge_len + self._eps)\n\n                    # adaptive number of base t positions (longer edges -> more positions)\n                    base_n = min(self.densify_points, max(1, int(np.ceil(1.0 + 2.0 * (edge_len / (np.linalg.norm(width) + self._eps))))))\n                    # select t values biased to inner points, jittered\n                    base_t = np.linspace(0.2, 0.8, base_n)\n                    jitter = (self.rng.random(size=base_n) - 0.5) * 0.18\n                    base_t = np.clip(base_t + jitter, 0.02, 0.98)\n\n                    # produce orthonormal basis for perturbations\n                    ortho_basis = orthonormal_basis(u, self.mesh_orth)\n                    # magnitude scale: edge_len controls bridge thickness; sigma adds stochasticity\n                    base_scale = edge_len * 0.5\n                    mag_scale = min(base_scale + 0.5 * self.sigma, 1.2 * np.linalg.norm(width))\n\n                    # create mesh: for each t, add center and orth perturbations\n                    for t in base_t:\n                        base = (1.0 - t) * xi + t * xj\n                        # center candidate with slight random offset along edge\n                        along_offset = u * ((self.rng.normal(scale=0.12) * edge_len) * 0.08)\n                        center = base + along_offset\n                        candidates.append(center)\n                        # orthogonal bridge perturbations\n                        for ob in ortho_basis:\n                            # magnitude proportional to mag_scale and shrunk as we approach endpoints (encourage interior bridging)\n                            endpoint_factor = 1.0 - abs(2.0 * t - 1.0)\n                            mag = (0.08 + 0.9 * self.rng.random()) * mag_scale * endpoint_factor\n                            candidates.append(base + ob * mag)\n                    # small extrapolation candidates near endpoints occasionally\n                    if self.rng.random() < 0.18:\n                        ext = 0.12 * edge_len\n                        candidates.append(xi - u * ext)\n                        candidates.append(xj + u * ext)\n                    # increment sampling count bias (we will increment after evaluating one or more candidates)\n                    edge_counts[eidx] += 0.5  # soft credit for building mesh\n\n            elif r < p_edge + p_split and len(edges) > 0:\n                # targeted split attempt: sample midpoint + small orth perturbation and if better, add to pop\n                eidx = choose_edge_idx()\n                if eidx is None:\n                    candidates.append(self.rng.uniform(lb, ub))\n                else:\n                    i, j = edges[eidx]\n                    xi = pop_X[i]\n                    xj = pop_X[j]\n                    mid = 0.5 * (xi + xj)\n                    # small orth perturbation from mid (a few)\n                    u = (xj - xi)\n                    edge_len = np.linalg.norm(u) + self._eps\n                    u = u / (edge_len + self._eps)\n                    ortho_basis = orthonormal_basis(u, max(1, self.mesh_orth))\n                    mag = 0.6 * self.sigma * (0.5 + self.rng.random())\n                    candidates.append(mid)\n                    for ob in ortho_basis:\n                        candidates.append(mid + ob * mag)\n                    edge_counts[eidx] += 1.0\n\n            elif r < p_edge + p_split + p_local:\n                # Local Gaussian refinement around best with anisotropic scaling\n                local_sigma = self.sigma * (0.35 + 0.9 * self.rng.random())\n                if pop_X.shape[0] >= max(3, dim // 2):\n                    stds = np.std(pop_X, axis=0)\n                    stds = np.maximum(stds, 0.03 * np.mean(width))\n                    anis = stds / np.mean(stds)\n                    for _ in range(3):\n                        candidates.append(x_best + local_sigma * (self.rng.normal(size=dim) * anis))\n                else:\n                    for _ in range(3):\n                        candidates.append(x_best + local_sigma * self.rng.normal(size=dim))\n\n            else:\n                # Global exploration: opposite sampling and pure random\n                if self.rng.random() < 0.6 and pop_X.shape[0] > 0:\n                    idx = self.rng.integers(0, pop_X.shape[0])\n                    x0 = pop_X[idx]\n                    opp = lb + ub - x0\n                    mag = 0.35 * self.sigma\n                    candidates.append(opp + mag * self.rng.normal(size=dim))\n                    # also try random mid-global\n                    candidates.append(self.rng.uniform(lb, ub))\n                else:\n                    candidates.append(self.rng.uniform(lb, ub))\n\n            # Evaluate candidates sequentially but don't exceed budget\n            for x_c in candidates:\n                if evals >= budget:\n                    break\n                # clip and evaluate\n                x_c = np.minimum(np.maximum(x_c, lb), ub)\n                f_c = float(func(x_c))\n                evals += 1\n\n                # append to archive\n                X_archive = np.vstack([X_archive, x_c])\n                f_archive = np.concatenate([f_archive, [f_c]])\n\n                improved = False\n                if f_c < f_best:\n                    f_best = float(f_c)\n                    x_best = x_c.copy()\n                    improved = True\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n\n                # adapt sigma\n                if improved:\n                    self.sigma = max(self.sigma_min, self.sigma * self.success_shrink)\n                else:\n                    self.sigma = min(self.sigma_max, self.sigma * self.failure_expand)\n\n                # maintain compact pop\n                if pop_X.shape[0] < self.pop_size:\n                    pop_X = np.vstack([pop_X, x_c])\n                    pop_f = np.concatenate([pop_f, [f_c]])\n                else:\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_c < pop_f[worst_idx]:\n                        pop_X[worst_idx] = x_c\n                        pop_f[worst_idx] = f_c\n                    else:\n                        # small chance to inject diversity\n                        if self.rng.random() < 0.03:\n                            ridx = self.rng.integers(0, pop_X.shape[0])\n                            pop_X[ridx] = x_c\n                            pop_f[ridx] = f_c\n\n                # if candidate was produced by edge mesh and beat both endpoints, encourage split by immediate insertion\n                # Check whether newly added x_c is close to some edge interior and improves both endpoints average\n                # We'll do a simple heuristic: if there exist endpoints i,j within pop_X such that x_c is near their midpoint and has better f than both,\n                # then replace worst pop member with x_c to densify.\n                if pop_X.shape[0] > 1:\n                    # find nearest neighbor in pop to x_c\n                    dists = np.linalg.norm(pop_X - x_c, axis=1)\n                    nearest = np.argsort(dists)[:2]\n                    if len(nearest) >= 2:\n                        i0, i1 = nearest[0], nearest[1]\n                        # check interior closeness and improvement\n                        mid = 0.5 * (pop_X[i0] + pop_X[i1])\n                        if np.linalg.norm(x_c - mid) < 0.6 * np.linalg.norm(pop_X[i0] - pop_X[i1]) + 1e-12:\n                            if f_c + 1e-12 < 0.999 * (0.5 * (pop_f[i0] + pop_f[i1]) if pop_f.shape[0] > max(i0, i1) else np.inf):\n                                # replace worst in pop to densify around this new node\n                                worst_idx = int(np.argmax(pop_f))\n                                pop_X[worst_idx] = x_c\n                                pop_f[worst_idx] = f_c\n\n                # cap archive size occasionally\n                if X_archive.shape[0] > self.max_archive:\n                    keep_best = int(self.max_archive * 0.6)\n                    best_inds = np.argsort(f_archive)[:keep_best]\n                    other_needed = self.max_archive - keep_best\n                    remaining = np.setdiff1d(np.arange(X_archive.shape[0]), best_inds, assume_unique=False)\n                    if remaining.size > 0 and other_needed > 0:\n                        other_inds = self.rng.choice(remaining, size=min(other_needed, remaining.size), replace=False)\n                        keep = np.unique(np.concatenate([best_inds, other_inds]))\n                        X_archive = X_archive[keep]\n                        f_archive = f_archive[keep]\n                    else:\n                        # fallback keep only best\n                        keep = np.argsort(f_archive)[:self.max_archive]\n                        X_archive = X_archive[keep]\n                        f_archive = f_archive[keep]\n\n            # Periodically rebuild pop and refresh graph (recompute weights and reset counts)\n            if (evals - last_rebuild_at) >= self.rebuild_every or evals >= budget:\n                pop_X, pop_f, pop_inds = build_population(X_archive, f_archive)\n                refresh_graph()\n                last_rebuild_at = evals\n\n            # emergency diversification if stagnating\n            if iter_since_improve > max(60, 6 * dim) and self.rng.random() < 0.2:\n                # inject one larger global sample (but still respect budget)\n                if evals < budget:\n                    xg = self.rng.uniform(lb, ub)\n                    fg = float(func(xg))\n                    evals += 1\n                    X_archive = np.vstack([X_archive, xg])\n                    f_archive = np.concatenate([f_archive, [fg]])\n                    if fg < f_best:\n                        f_best = fg\n                        x_best = xg.copy()\n                        iter_since_improve = 0\n                    # replace a random or worst pop member\n                    if pop_X.shape[0] < self.pop_size:\n                        pop_X = np.vstack([pop_X, xg])\n                        pop_f = np.concatenate([pop_f, [fg]])\n                    else:\n                        if fg < pop_f.max() or self.rng.random() < 0.4:\n                            worst_idx = int(np.argmax(pop_f))\n                            pop_X[worst_idx] = xg\n                            pop_f[worst_idx] = fg\n                    # expand sigma to escape\n                    self.sigma = min(self.sigma_max, self.sigma * (1.4 + self.rng.random()))\n\n            # safety: ensure sigma stays in bounds\n            self.sigma = float(np.clip(self.sigma, self.sigma_min, self.sigma_max))\n\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 3, "feedback": "The algorithm GraphAugmentedEdgeDensification scored 0.227 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["701bfc7e-c1fd-40fc-8533-beb7ddd599a9"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.4139873723352897}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.3789727508577251}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.3876253125058434}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02799203154861185}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.017290709083315092}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.012491639940362398}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.011733077893390842}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.024566724498202852}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.008140070196356652}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.7197487935348433}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.7770227495339823}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.6225271109210937}], "aucs": [0.4139873723352897, 0.3789727508577251, 0.3876253125058434, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.02799203154861185, 0.017290709083315092, 0.012491639940362398, 0.011733077893390842, 0.024566724498202852, 0.008140070196356652, 0.7197487935348433, 0.7770227495339823, 0.6225271109210937]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3506.0, "Edges": 3505.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9994295493439818, "Degree Variance": 2.0616083454360203, "Transitivity": 0.0, "Max Depth": 19.0, "Min Depth": 2.0, "Mean Depth": 8.910971786833855, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3291802666281882, "Depth Entropy": 2.2974334672311003, "Assortativity": 0.0, "Average Eccentricity": 21.439247005134057, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.0002852253280091272, "Average Shortest Path": 11.44051371482187, "mean_complexity": 10.833333333333334, "total_complexity": 65.0, "mean_token_count": 514.8333333333334, "total_token_count": 3089.0, "mean_parameter_count": 2.6666666666666665, "total_parameter_count": 16.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "7122544b-51dc-41b4-8c0b-a820ef136bc1", "fitness": 0.23886705259450297, "name": "EdgeDenseAdaptiveSubspaceNetwork", "description": "Edge-Dense Adaptive Subspace Network (EDASN) \u2014 maintain a compact elite archive, build many dense interpolations/extrapolations along edges between elites and perform constrained subspace perturbations (aligned with edge directions plus orthogonal modes) to rapidly increase \"edge density\" of explored solutions and exploit discovered connections.", "code": "import numpy as np\n\nclass EdgeDenseAdaptiveSubspaceNetwork:\n    \"\"\"\n    Edge-Dense Adaptive Subspace Network (EDASN)\n\n    Main idea:\n      - Maintain a small elite archive of best points found so far.\n      - Form a dense set of edges between elites and actively sample many points\n        along, slightly beyond, and around those edges.\n      - For each sampled point, perturb inside a low-dimensional subspace spanned\n        by the edge direction and a few orthogonal modes (via QR-based basis).\n      - Adapt per-elite trust radii based on success; densify edges where improvements\n        occur and occasionally inject global randomization to avoid stagnation.\n      - Strictly respect the evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, elite_size=8, init_samples=None,\n                 init_radius=1.2, min_radius=1e-6, max_radius=2.5,\n                 max_edges_per_iter=30, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations (must be integer)\n          dim: problem dimensionality\n          elite_size: number of elites to keep in the archive\n          init_samples: how many random samples to seed (default ~ min(budget/10, 20))\n          init_radius: initial per-elite radius (controls perturbation scale)\n          min_radius, max_radius: radius bounds\n          max_edges_per_iter: how many distinct edges to explore per main iteration\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_size = int(elite_size)\n        if init_samples is None:\n            self.init_samples = max(2, min(20, self.budget // 10))\n        else:\n            self.init_samples = int(init_samples)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_edges_per_iter = int(max_edges_per_iter)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _orthonormal_basis(self, primary_vec, k):\n        \"\"\"\n        Build an orthonormal basis with first vector aligned to primary_vec.\n        Returns matrix of shape (dim, k).\n        \"\"\"\n        # ensure primary_vec is not zero\n        v = np.asarray(primary_vec, dtype=float)\n        norm = np.linalg.norm(v)\n        if norm < 1e-12:\n            # fallback to random basis\n            A = self.rng.normal(size=(self.dim, k))\n            Q, _ = np.linalg.qr(A)\n            return Q[:, :k]\n        # create matrix with primary vec as first column and random other columns\n        A = self.rng.normal(size=(self.dim, k))\n        A[:, 0] = v / norm\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        # seed initial random samples\n        elites_x = []\n        elites_f = []\n        per_elite_radius = []\n\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            elites_x.append(x)\n            elites_f.append(f)\n            per_elite_radius.append(self.init_radius)\n\n        # If no evaluations happened (budget small), at least one eval\n        if len(elites_x) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            elites_x.append(x)\n            elites_f.append(f)\n            per_elite_radius.append(self.init_radius)\n\n        # Keep best record\n        elites = list(zip(elites_f, elites_x, per_elite_radius))\n        elites.sort(key=lambda t: t[0])\n        # trim to elite_size\n        elites = elites[:self.elite_size]\n\n        # main loop: build and explore edges until budget exhausted\n        no_improve_iters = 0\n        iters = 0\n        while evals < self.budget:\n            iters += 1\n            # refresh elites lists\n            elites_f = [e[0] for e in elites]\n            elites_x = [e[1] for e in elites]\n            per_elite_radius = [e[2] for e in elites]\n            n_el = len(elites_x)\n\n            # Compute all possible edges (pairs) indices\n            edges = []\n            for i in range(n_el):\n                for j in range(i + 1, n_el):\n                    # priority preferring edges that connect good elites (smaller avg f)\n                    priority = (elites_f[i] + elites_f[j]) / 2.0\n                    edges.append((priority, i, j))\n            # sort edges by priority ascending (better edges first)\n            edges.sort(key=lambda t: t[0])\n\n            # choose a subset of edges to explore: prefer best edges but include some random ones\n            num_edges = min(len(edges), self.max_edges_per_iter if self.max_edges_per_iter > 0 else len(edges))\n            chosen_edges = []\n            # take a mix: top half from best, other half random\n            top_take = int(np.ceil(num_edges * 0.6))\n            top_take = min(top_take, len(edges))\n            chosen_edges.extend(edges[:top_take])\n            remaining_choices = edges[top_take:]\n            # random picks (without replacement)\n            if remaining_choices:\n                r_take = num_edges - len(chosen_edges)\n                idxs = self.rng.choice(len(remaining_choices), size=min(r_take, len(remaining_choices)), replace=False)\n                for idx in idxs:\n                    chosen_edges.append(remaining_choices[idx])\n\n            # candidate pool to evaluate (collected, then evaluated respecting budget)\n            # But we will evaluate on the fly, checking budget\n            improved = False\n            candidates_tried = 0\n\n            for _, i, j in chosen_edges:\n                if evals >= self.budget:\n                    break\n                xi = elites_x[i]\n                xj = elites_x[j]\n                fi = elites_f[i]\n                fj = elites_f[j]\n\n                # compute edge-based parameters\n                edge_vec = xj - xi\n                edge_len = np.linalg.norm(edge_vec) + 1e-12\n                edge_dir = edge_vec / edge_len\n                # combine radius from both endpoints scaled by edge length (so long edges get slightly larger exploration)\n                rad_i = per_elite_radius[i]\n                rad_j = per_elite_radius[j]\n                edge_radius = min(self.max_radius, max(self.min_radius, 0.7 * (rad_i + rad_j) * (1.0 + 0.2 * edge_len)))\n\n                # number of slots to sample along this edge - proportional to dimensionality but limited\n                slots = int(np.clip(2 + self.dim // 6, 2, 8))\n                # pick random seeds along edge positions (not deterministic equally spaced) to encourage dense coverage\n                t_positions = self.rng.random(slots)\n                # also add a midpoint and two extrapolation points (slightly beyond endpoints)\n                extras = [0.5, -0.15, 1.15]\n                t_positions = np.concatenate([t_positions, np.array(extras)])\n                # define low-dimensional subspace size: include edge direction plus up to 3 orthogonal modes\n                k_sub = min(self.dim, 1 + min(3, max(1, int(np.ceil(self.dim / 6)))))  # typically 2-4\n                basis = self._orthonormal_basis(edge_dir, k_sub)  # shape (dim, k_sub)\n\n                for t in t_positions:\n                    if evals >= self.budget:\n                        break\n                    # base point on edge (can be slightly outside because of extras)\n                    x_base = xi * (1.0 - t) + xj * t\n                    # perturb coefficients sampled from normal with scale proportional to edge_radius\n                    # the first coefficient controls movement along the edge (small), others are orthogonal\n                    coeffs = self.rng.normal(loc=0.0, scale=edge_radius, size=k_sub)\n                    # reduce the along-edge coefficient to focus on \"between\" connectivity rather than long jumps\n                    coeffs[0] *= 0.5\n                    x_cand = x_base + basis.dot(coeffs)\n                    # clip\n                    x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n\n                    # Evaluate candidate\n                    f_cand = float(func(x_cand))\n                    evals += 1\n                    candidates_tried += 1\n\n                    # Accept and integrate into elite archive if good\n                    if f_cand < elites[-1][0] or len(elites) < self.elite_size:\n                        # add to elites and re-sort/trim\n                        per_e = edge_radius * (1.0 if f_cand < min(fi, fj) else 0.8)\n                        elites.append((f_cand, x_cand.copy(), per_e))\n                        elites.sort(key=lambda t: t[0])\n                        elites = elites[:self.elite_size]\n                        improved = True\n                        no_improve_iters = 0\n                    # If candidate better than global best, expand both endpoint radii a bit\n                    if f_cand < elites[0][0]:\n                        # update best directly inside elites list (it was inserted above)\n                        # expand endpoint radii heuristically\n                        # find endpoints by matching fi/fj positions -- safer to adapt all radii modestly\n                        new_list = []\n                        for (f_old, x_old, r_old) in elites:\n                            r_new = min(self.max_radius, r_old * (1.0 + 0.12))\n                            new_list.append((f_old, x_old, r_new))\n                        elites = new_list\n\n                # small local exploitation along the edge: attempt a greedy step from the best endpoint towards the other\n                if evals < self.budget:\n                    # choose direction from better to worse\n                    if fi <= fj:\n                        base_idx, other_idx = i, j\n                    else:\n                        base_idx, other_idx = j, i\n                    base_x = elites_x[base_idx]\n                    other_x = elites_x[other_idx]\n                    step_vec = (other_x - base_x) * 0.15  # short step\n                    probe = base_x + step_vec\n                    probe = np.minimum(np.maximum(probe, lb), ub)\n                    f_probe = float(func(probe))\n                    evals += 1\n                    if f_probe < elites[-1][0] or len(elites) < self.elite_size:\n                        elites.append((f_probe, probe.copy(), max(self.min_radius, (per_elite_radius[base_idx] * 0.9))))\n                        elites.sort(key=lambda t: t[0])\n                        elites = elites[:self.elite_size]\n                        improved = True\n                        no_improve_iters = 0\n\n            # End edge exploration for this outer iteration\n\n            # If no improvement in this round, slightly shrink radii for all elites to focus exploitation\n            if not improved:\n                no_improve_iters += 1\n                new_elites = []\n                for (f_old, x_old, r_old) in elites:\n                    r_new = max(self.min_radius, r_old * 0.85)\n                    new_elites.append((f_old, x_old, r_new))\n                elites = new_elites\n            else:\n                # on improvement, gently expand radii of successful elites (promotes more edge growth)\n                new_elites = []\n                for (f_old, x_old, r_old) in elites:\n                    r_new = min(self.max_radius, r_old * 1.05)\n                    new_elites.append((f_old, x_old, r_new))\n                elites = new_elites\n\n            # Occasional global diversification if stagnation persists\n            if no_improve_iters >= 6 and evals < self.budget:\n                n_global = min(4, max(1, self.dim // 4))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg))\n                    evals += 1\n                    if fg < elites[-1][0] or len(elites) < self.elite_size:\n                        elites.append((fg, xg.copy(), self.init_radius))\n                        elites.sort(key=lambda t: t[0])\n                        elites = elites[:self.elite_size]\n                        improved = True\n                        no_improve_iters = 0\n                # if still no improvement, perform a larger radius random jump centered at a random elite\n                if not improved and evals < self.budget:\n                    center_idx = self.rng.integers(0, len(elites))\n                    center = elites[center_idx][1]\n                    jump_scale = 0.6 * (ub - lb)\n                    xj = center + self.rng.normal(0, 1.0, size=self.dim) * jump_scale\n                    xj = np.minimum(np.maximum(xj, lb), ub)\n                    fj = float(func(xj))\n                    evals += 1\n                    if fj < elites[-1][0] or len(elites) < self.elite_size:\n                        elites.append((fj, xj.copy(), self.init_radius))\n                        elites.sort(key=lambda t: t[0])\n                        elites = elites[:self.elite_size]\n                        improved = True\n                        no_improve_iters = 0\n                    else:\n                        # small reset of radii to encourage fresh exploration\n                        new_elites = []\n                        for (f_old, x_old, r_old) in elites:\n                            new_elites.append((f_old, x_old, max(self.min_radius, self.init_radius * 0.6)))\n                        elites = new_elites\n                        no_improve_iters = 0  # give another cycle after forced diversification\n\n            # defensive: if all radii collapse, re-inflate them a bit\n            avg_rad = np.mean([e[2] for e in elites])\n            if avg_rad < 1e-7:\n                new_elites = []\n                for (f_old, x_old, r_old) in elites:\n                    new_elites.append((f_old, x_old, max(self.min_radius, self.init_radius * 0.7)))\n                elites = new_elites\n\n            # safety break if iterations too many (logical safety)\n            if iters > max(10000, self.budget * 10):\n                break\n\n        # finalize best\n        elites.sort(key=lambda t: t[0])\n        f_best, x_best, _ = elites[0]\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 3, "feedback": "The algorithm EdgeDenseAdaptiveSubspaceNetwork scored 0.239 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.33648616545569565}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.2882100291870282}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.2927033201265907}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.056976098297305544}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.048107145821190356}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.04856392665493858}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.035633879488671805}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.022732602260868928}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.02129086834392857}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.7557052208637175}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8169473850582841}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.8594991473593241}], "aucs": [0.33648616545569565, 0.2882100291870282, 0.2927033201265907, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.056976098297305544, 0.048107145821190356, 0.04856392665493858, 0.035633879488671805, 0.022732602260868928, 0.02129086834392857, 0.7557052208637175, 0.8169473850582841, 0.8594991473593241]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2510.0, "Edges": 2509.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.999203187250996, "Degree Variance": 2.0629475722607578, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.306266548984995, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3342340444805363, "Depth Entropy": 2.1550271349671024, "Assortativity": 0.0, "Average Eccentricity": 17.593625498007967, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00039840637450199205, "Average Shortest Path": 10.518862612523204, "mean_complexity": 14.25, "total_complexity": 57.0, "mean_token_count": 540.25, "total_token_count": 2161.0, "mean_parameter_count": 4.25, "total_parameter_count": 17.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "e6c23af3-912d-41e1-ac28-7c0c38b76d67", "fitness": 0.21838283904112393, "name": "AdaptiveRotationalGaussianSearchEdge", "description": "Adaptive Rotational Gaussian Search with Edge-Enriched Recombination (ARGS-Edge) \u2014 rotate & adapt Gaussian proposals around the best while building a small elite archive and actively sampling/interpolating along edges between elites (midpoints, extrapolations and orthogonal perturbations) to increase edge-density and exploit promising connections.", "code": "import numpy as np\n\nclass AdaptiveRotationalGaussianSearchEdge:\n    \"\"\"\n    Adaptive Rotational Gaussian Search with Edge-Enriched Recombination (ARGS-Edge)\n\n    Main ideas:\n    - Maintain a single current best and a small elite archive (best visited points).\n    - Adapt a covariance matrix C (trace ~ dim) and scalar step-size sigma using rank-1 updates\n      from successful steps (rotational adaptation).\n    - Build edges among elite archive members and actively sample along these edges:\n      midpoints, extrapolations, and orthogonal perturbations to increase \"edge density\".\n    - Mix Gaussian proposals, edge recombinations, principal-component bursts, and occasional\n      global/opposition samples. Adapt mixing ratios when stagnating and perform small restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_samples=None, elite_size=None,\n                 k_neighbors=None, rng=None):\n        \"\"\"\n        budget: total number of func evaluations allowed\n        dim: problem dimensionality\n        init_samples: number of initial uniform samples (default ~3*dim or budget//20)\n        elite_size: maximum number of elite archive members (default depends on dim)\n        k_neighbors: number of neighbors to consider when building edges (default min(5,elite-1))\n        rng: numpy.random.Generator or None\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = init_samples\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.elite_size = elite_size if elite_size is not None else min(12, max(4, 2 * self.dim))\n        self.k_neighbors = k_neighbors  # will be set later relative to elite\n        # placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds if possible, else default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub =  5.0 * np.ones(self.dim)\n\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n\n        # initial sampling\n        if self.init_samples is None:\n            init_samples = int(min(max(5, 3 * self.dim), max(5, self.budget // 20)))\n        else:\n            init_samples = int(self.init_samples)\n            init_samples = min(init_samples, max(1, self.budget // 5))\n\n        # elite archive: list of points and fitnesses (kept sorted ascending by fitness)\n        elites = []\n        elites_f = []\n\n        for _ in range(init_samples):\n            if evals >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            # insert into elites\n            if len(elites) < self.elite_size or f < elites_f[-1]:\n                # insert keeping sorted order\n                pos = np.searchsorted(elites_f, f)\n                elites.insert(pos, x.copy())\n                elites_f.insert(pos, f)\n                if len(elites) > self.elite_size:\n                    elites.pop()\n                    elites_f.pop()\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # Ensure at least one point\n        if self.x_opt is None:\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            self.f_opt = f\n            self.x_opt = x.copy()\n            elites = [self.x_opt.copy()]\n            elites_f = [self.f_opt]\n\n        # Strategy parameters\n        span = ub - lb\n        initial_sigma = 0.18 * np.linalg.norm(span) / max(1.0, np.sqrt(self.dim))\n        sigma = max(1e-8, initial_sigma)\n\n        # covariance start as identity scaled to have trace ~ dim\n        C = np.eye(self.dim)\n        # ensure trace dims\n        C = C * (self.dim / (np.trace(C) + 1e-12))\n\n        eps = 1e-9\n        c_cov = 0.18\n        c_success = 1.2\n        c_failure = 0.86\n        stagnation_counter = 0\n        stagnation_limit = max(8, 40 // (self.dim // 5 + 1))\n\n        max_success_history = min(60, 8 * self.dim + 20)\n        success_steps = []\n\n        # Edge data structure: list of tuples (i, j, weight, vec_ij, dist)\n        edges = []\n\n        def recompute_edges():\n            nonlocal edges, self\n            m = len(elites)\n            edges = []\n            if m < 2:\n                return\n            # compute qualities: higher is better\n            f_arr = np.array(elites_f)\n            # convert to positive quality: (max_f - f) so smaller f => larger quality\n            maxf = np.max(f_arr)\n            qual = (maxf - f_arr) + 1e-12\n            # neighbor count\n            kn = self.k_neighbors if self.k_neighbors is not None else min(5, max(1, m - 1))\n            kn = min(kn, m - 1)\n            # pairwise distances\n            X = np.vstack(elites)\n            dists = np.linalg.norm(X[:, None, :] - X[None, :, :], axis=2)\n            for i in range(m):\n                # find nearest neighbors of i (excluding itself)\n                neighs = np.argsort(dists[i])[1: kn + 1]\n                for j in neighs:\n                    if j <= i:\n                        continue  # only keep i<j to avoid duplicates\n                    weight = (qual[i] + qual[j])  # simple additive weight\n                    vec = X[j] - X[i]\n                    dist = np.linalg.norm(vec) + 1e-12\n                    edges.append((i, j, weight, vec / dist, dist))\n            # normalize weights\n            if len(edges) > 0:\n                w = np.array([e[2] for e in edges], dtype=float)\n                if w.sum() > 0:\n                    w = w / w.sum()\n                    edges = [(e[0], e[1], float(wi), e[3], e[4]) for e, wi in zip(edges, w)]\n                else:\n                    nw = 1.0 / len(edges)\n                    edges = [(e[0], e[1], float(nw), e[3], e[4]) for e in edges]\n\n        recompute_edges()\n\n        # mixing ratios for candidate generation\n        p_edge = 0.32\n        p_gauss = 0.48\n        p_pc = 0.10\n        p_global = 0.10\n\n        # main loop: sample until budget exhausted\n        while evals < self.budget:\n            rem = self.budget - evals\n            # choose batch size adaptively: small for low budget\n            batch = min(rem, max(1, int(min(28, 6 + np.sqrt(self.dim) * 2))))\n            # if stagnated, bias toward edge sampling and PCA bursts\n            if stagnation_counter > stagnation_limit:\n                p_edge_loc = min(0.7, p_edge + 0.25)\n                p_gauss_loc = max(0.15, p_gauss - 0.2)\n                p_pc_loc = min(0.25, p_pc + 0.1)\n                p_global_loc = max(0.05, p_global - 0.05)\n            else:\n                p_edge_loc = p_edge\n                p_gauss_loc = p_gauss\n                p_pc_loc = p_pc\n                p_global_loc = p_global\n\n            # compute sqrt of covariance for Gaussian proposals\n            C_pd = C + eps * np.eye(self.dim)\n            try:\n                B = np.linalg.cholesky(C_pd)\n            except np.linalg.LinAlgError:\n                vals, vecs = np.linalg.eigh(C_pd)\n                vals = np.clip(vals, 1e-14, None)\n                B = vecs @ np.diag(np.sqrt(vals))\n            B = sigma * B\n\n            candidates = []\n            # sometimes do a denser edge sweep to increase edge density if stagnating\n            do_edge_sweep = (stagnation_counter > stagnation_limit) and (len(edges) > 0) and (self.rng.random() < 0.6)\n\n            if do_edge_sweep:\n                # generate multiple points along top edges\n                # pick top-K edges by weight\n                K = min(len(edges), max(3, int(len(edges) * 0.4)))\n                edges_sorted = sorted(edges, key=lambda e: -e[2])[:K]\n                for (i, j, w, dirvec, dist) in edges_sorted:\n                    if len(candidates) >= batch:\n                        break\n                    xi = elites[i]\n                    xj = elites[j]\n                    # sample midpoint variant\n                    alpha = self.rng.uniform(-0.6, 0.6)\n                    base = 0.5 * (xi + xj) + alpha * (xj - xi)\n                    # orthogonal perturbation\n                    z = self.rng.standard_normal(self.dim)\n                    proj = np.dot(z, dirvec) * dirvec\n                    orth = z - proj\n                    if np.linalg.norm(orth) > 1e-12:\n                        orth = orth / (np.linalg.norm(orth) + 1e-12)\n                    scale = 0.3 * sigma * (0.5 + self.rng.random())\n                    cand = base + scale * orth\n                    candidates.append(clip(cand))\n                # fill remaining with regular mixed proposals\n            # generate batch candidates by mixture\n            for k in range(len(candidates), batch):\n                r = self.rng.random()\n                if (r < p_edge_loc) and (len(edges) > 0):\n                    # edge-based recombination\n                    # select one edge by weight\n                    ws = np.array([e[2] for e in edges], dtype=float)\n                    if ws.sum() <= 0:\n                        idx = self.rng.integers(0, len(edges))\n                    else:\n                        idx = self.rng.choice(len(edges), p=ws)\n                    i, j, w, dirvec, dist = edges[idx]\n                    xi = elites[i]\n                    xj = elites[j]\n                    # choose mode: midpoint / extrapolate / biased toward best\n                    mode = self.rng.random()\n                    if mode < 0.5:\n                        # midpoint with small offset\n                        alpha = self.rng.uniform(-0.6, 0.6)\n                        base = 0.5 * (xi + xj) + alpha * (xj - xi)\n                    elif mode < 0.8:\n                        # extrapolate from better of the two towards the other\n                        if elites_f[i] < elites_f[j]:\n                            better, other = xi, xj\n                        else:\n                            better, other = xj, xi\n                        beta = self.rng.uniform(0.0, 0.9)\n                        base = better + beta * (better - other)\n                    else:\n                        # along edge but biased toward global best\n                        base = 0.5 * (xi + xj) + 0.5 * (self.x_opt - 0.5 * (xi + xj))\n\n                    # orthogonal noise\n                    z = self.rng.standard_normal(self.dim)\n                    proj = np.dot(z, dirvec) * dirvec\n                    orth = z - proj\n                    if np.linalg.norm(orth) > 1e-12:\n                        orth = orth / (np.linalg.norm(orth) + 1e-12)\n                    # scale orth noise relative to distance and sigma\n                    orth_scale = sigma * (0.1 + 0.6 * self.rng.random()) * min(1.0, dist / (np.linalg.norm(span) + 1e-12))\n                    cand = base + orth_scale * orth\n                    # small isotropic gaussian around candidate\n                    cand = cand + 0.15 * (B @ self.rng.standard_normal(self.dim))\n                    candidates.append(clip(cand))\n                elif r < p_edge_loc + p_gauss_loc:\n                    # gaussian rotation sample around current best\n                    z = self.rng.standard_normal(self.dim)\n                    cand = self.x_opt + B.dot(z)\n                    candidates.append(clip(cand))\n                elif r < p_edge_loc + p_gauss_loc + p_pc:\n                    # principal component exploration using recent successful steps\n                    if len(success_steps) >= 2:\n                        S = np.array(success_steps)\n                        # covariance of success steps\n                        M = np.cov(S.T) if S.shape[0] > 1 else np.atleast_2d(S[0]).T @ np.atleast_2d(S[0])\n                        vals, vecs = np.linalg.eigh(M + 1e-12 * np.eye(self.dim))\n                        pc = vecs[:, -1]\n                        long_fac = 1.8 * (1.0 + self.rng.random())\n                        long_step = long_fac * sigma * pc * self.rng.normal()\n                        orth_noise = 0.25 * (B @ self.rng.standard_normal(self.dim))\n                        cand = self.x_opt + long_step + orth_noise\n                    else:\n                        cand = self.x_opt + B.dot(self.rng.standard_normal(self.dim))\n                    candidates.append(clip(cand))\n                else:\n                    # global uniform / opposition sampling to diversify\n                    if self.rng.random() < 0.6:\n                        # opposition of best\n                        opp = lb + ub - self.x_opt\n                        perturb = 0.25 * span * (self.rng.random(self.dim) - 0.5)\n                        cand = opp + perturb\n                    else:\n                        cand = self.rng.uniform(lb, ub)\n                    candidates.append(clip(cand))\n\n            # Evaluate candidates sequentially until budget or done\n            best_candidate = None\n            best_candidate_f = np.inf\n            best_idx = -1\n            for y in candidates:\n                if evals >= self.budget:\n                    break\n                f_y = float(func(y))\n                evals += 1\n                if f_y < best_candidate_f:\n                    best_candidate_f = f_y\n                    best_candidate = y.copy()\n                # also maintain elites on the fly: insert promising ones\n                if len(elites) < self.elite_size or f_y < elites_f[-1]:\n                    pos = np.searchsorted(elites_f, f_y)\n                    elites.insert(pos, y.copy())\n                    elites_f.insert(pos, f_y)\n                    if len(elites) > self.elite_size:\n                        elites.pop()\n                        elites_f.pop()\n                    # recompute edges lazily later\n            # after batch, recompute edges if elites changed\n            recompute_edges()\n\n            if best_candidate is None:\n                break\n\n            # Acceptance and updates\n            if best_candidate_f < self.f_opt - 1e-15:\n                # success\n                step = best_candidate - self.x_opt\n                norm_step = np.linalg.norm(step)\n                if norm_step > 0:\n                    success_steps.append(step.copy())\n                    if len(success_steps) > max_success_history:\n                        success_steps.pop(0)\n                    v = step / (norm_step + 1e-12)\n                    outer = np.outer(v, v)\n                    # scale outer by step magnitude relative to trace\n                    traceC = np.trace(C) / max(1, self.dim)\n                    scale = (norm_step**2) / (traceC + 1e-12)\n                    C = (1.0 - c_cov) * C + c_cov * (outer * scale)\n                    # normalize trace back to ~dim\n                    tr = np.trace(C)\n                    if tr > 0:\n                        C = C * (self.dim / tr)\n                self.x_opt = best_candidate.copy()\n                self.f_opt = float(best_candidate_f)\n                sigma = max(1e-12, sigma * c_success)\n                stagnation_counter = 0\n                # Insert/update elite if not already\n                if len(elites) < self.elite_size or self.f_opt < elites_f[-1]:\n                    pos = np.searchsorted(elites_f, self.f_opt)\n                    elites.insert(pos, self.x_opt.copy())\n                    elites_f.insert(pos, self.f_opt)\n                    if len(elites) > self.elite_size:\n                        elites.pop()\n                        elites_f.pop()\n                    recompute_edges()\n            else:\n                # failure to improve\n                sigma = max(1e-12, sigma * c_failure)\n                stagnation_counter += 1\n                # small isotropic inflation of covariance occasionally\n                if stagnation_counter % (stagnation_limit + 3) == 0:\n                    C = 0.95 * C + 0.05 * np.eye(self.dim)\n                # if very stagnated, attempt a diversity injection by sampling along top edges\n                if stagnation_counter > 2 * stagnation_limit and len(edges) > 0 and evals < self.budget:\n                    # produce a trial from a top edge extrapolation\n                    top_edges = sorted(edges, key=lambda e: -e[2])[:min(6, len(edges))]\n                    ei = top_edges[self.rng.integers(0, len(top_edges))]\n                    i, j, w, dirvec, dist = ei\n                    xi = elites[i]; xj = elites[j]\n                    # aggressive extrapolation\n                    beta = 0.8 + 0.8 * self.rng.random()\n                    trial = 0.5 * (xi + xj) + beta * (xj - xi) * (0.2 + 0.8 * self.rng.random())\n                    trial = clip(trial + 0.2 * (B @ self.rng.standard_normal(self.dim)))\n                    if evals < self.budget:\n                        f_trial = float(func(trial)); evals += 1\n                        if f_trial < self.f_opt:\n                            self.x_opt = trial.copy(); self.f_opt = f_trial\n                            sigma *= 1.4\n                            stagnation_counter = 0\n                            # update elites\n                            pos = np.searchsorted(elites_f, f_trial)\n                            elites.insert(pos, trial.copy())\n                            elites_f.insert(pos, f_trial)\n                            if len(elites) > self.elite_size:\n                                elites.pop(); elites_f.pop()\n                            recompute_edges()\n                        else:\n                            # nudge covariance to isotropic to escape\n                            C = 0.92 * C + 0.08 * np.eye(self.dim)\n\n            # regularize covariance to stay PD\n            C = 0.998 * C + 0.002 * np.eye(self.dim)\n\n        # final clip\n        if self.x_opt is not None:\n            self.x_opt = clip(self.x_opt)\n\n        return float(self.f_opt), np.array(self.x_opt)", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveRotationalGaussianSearchEdge scored 0.218 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["301b461d-2f9e-49ea-ae17-7d69bab7e52f"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9192688178940085}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.904700743096251}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8919795235396323}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.006459660429616654}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.011752495430062848}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.02179028134635641}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.03167001429633087}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0006470159353130223}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.013308943863576284}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.07461948351057002}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.16366792548881037}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.23572768078633166}], "aucs": [0.9192688178940085, 0.904700743096251, 0.8919795235396323, 0.006459660429616654, 4.999999999999449e-05, 4.999999999999449e-05, 0.011752495430062848, 0.02179028134635641, 4.999999999999449e-05, 0.03167001429633087, 0.0006470159353130223, 0.013308943863576284, 0.07461948351057002, 0.16366792548881037, 0.23572768078633166]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3445.0, "Edges": 3444.0, "Max Degree": 35.0, "Min Degree": 1.0, "Mean Degree": 1.9994194484760524, "Degree Variance": 2.000580214483876, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 9.045165394402035, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3220508351835936, "Depth Entropy": 2.2640202888488936, "Assortativity": 0.0, "Average Eccentricity": 18.845573294629897, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00029027576197387516, "Average Shortest Path": 11.439114406072528, "mean_complexity": 18.5, "total_complexity": 74.0, "mean_token_count": 751.75, "total_token_count": 3007.0, "mean_parameter_count": 2.5, "total_parameter_count": 10.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "a5046abc-901e-41cf-93ab-3615b17e34b3", "fitness": 0.21035286153070476, "name": "EdgeFocusedGraphGuidedSampler", "description": "Edge-Focused Graph-Guided Sampling (EFGGS) \u2014 explicitly build and adapt a dense elite-edge graph and bias diverse proposals (edge midpoints, extrapolations, orthogonal probes, barycentric recombinations and local PC samplers) along high-value edges to rapidly increase edge density around promising basins while keeping occasional global diversification.", "code": "import numpy as np\n\nclass EdgeFocusedGraphGuidedSampler:\n    \"\"\"\n    Edge-Focused Graph-Guided Sampler (EFGGS)\n\n    Main ideas:\n    - Maintain a compact elite set and an explicit weighted edge graph between elites.\n    - Sample heavily along edges (midpoints, biased toward better end, extrapolations),\n      plus orthogonal perturbations to create many informative \"edge-neighbors\".\n    - Keep per-elite short success memory to estimate local principal directions and local covariances.\n    - Adapt edge weights by reward signals when samples near an edge produce improvements.\n    - Occasionally perform barycentric (multi-elite) recombination and global uniform restarts.\n    - Reflection boundary handling to preserve directional info.\n    \"\"\"\n\n    def __init__(self, budget, dim, rng=None, init_elites=8, max_elites=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_elites = int(init_elites)\n        if max_elites is None:\n            self.max_elites = max(4, min(14, self.dim + 4))\n        else:\n            self.max_elites = int(max_elites)\n\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # determine bounds if available, else use [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub =  5.0 * np.ones(self.dim)\n\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # reflection handler (keeps directional info)\n        def reflect(x):\n            x = np.array(x, dtype=float, copy=True)\n            for i in range(self.dim):\n                lo, hi = lb[i], ub[i]\n                if lo == hi:\n                    x[i] = lo\n                    continue\n                width = hi - lo\n                # fold into [0,1] with reflection\n                v = (x[i] - lo) / width\n                # handle negative and large v\n                v = np.abs(((v + 1) % 2) - 1)\n                x[i] = lo + np.clip(v, 0.0, 1.0) * width\n            return x\n\n        evals = 0\n        remaining = lambda: self.budget - evals\n\n        # helper for uniqueness\n        def is_far_from_elites(x, elites, tol=1e-6):\n            x = np.asarray(x)\n            for xi, fi, mem in elites:\n                if np.linalg.norm((xi - x) / (ub - lb + 1e-12)) < tol:\n                    return False\n            return True\n\n        # Elite structure: list of tuples (x, f, memory)\n        # memory: dict with 'steps' list for recent successful steps (used for local PCA/cov)\n        elites = []\n\n        # Edge structure: dictionary key (i,j) with weight and last_reward time\n        edges = {}  # (i,j) sorted indices -> {'w':..., 'last': iter_count}\n\n        iter_count = 0\n\n        # seeding\n        init_n = min(self.init_elites, max(1, self.budget // 20))\n        init_n = max(init_n, 4)\n        for _ in range(init_n):\n            if evals >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            mem = {'steps': []}\n            elites.append((x.copy(), float(f), mem))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # ensure at least one point\n        if len(elites) == 0 and evals < self.budget:\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            elites.append((x.copy(), float(f), {'steps': []}))\n            self.x_opt = x.copy()\n            self.f_opt = float(f)\n\n        # initialize edges between all elite pairs with small weight\n        def rebuild_edges():\n            edges.clear()\n            n = len(elites)\n            for i in range(n):\n                for j in range(i+1, n):\n                    edges[(i,j)] = {'w': 1.0, 'last': iter_count}\n        rebuild_edges()\n\n        # parameters\n        max_batch = max(3, min(40, 6 + int(np.sqrt(self.dim) * 3)))\n        sigma0 = 0.2 * np.linalg.norm(ub - lb) / max(1.0, np.sqrt(self.dim))\n        sigma_global = max(1e-8, sigma0)\n        sigma_min = 1e-12\n        sigma_max = 5.0 * np.linalg.norm(ub - lb)\n\n        edge_decay = 0.995  # multiplicative decay per iteration\n        edge_boost = 3.0    # boost multiplier when an edge yields improvement\n        local_mem_limit = max(6, 4 + self.dim // 4)\n        triangle_rate = 0.12  # chance to do barycentric recombination\n\n        # helper: get local covariance approximate from memory or isotropic\n        def local_covariance(mem):\n            steps = mem.get('steps', [])\n            if len(steps) >= 2:\n                S = np.stack(steps, axis=0)  # k x dim\n                C = np.cov(S.T, bias=True)\n                # regularize\n                C = C + 1e-6 * np.eye(self.dim)\n                # scale to typical magnitude of steps\n                mean_norm = np.mean(np.linalg.norm(S, axis=1)) + 1e-12\n                C = C * (mean_norm**2 / (np.trace(C)/self.dim + 1e-12))\n                return C\n            else:\n                return np.eye(self.dim)\n\n        # candidate generators\n        def gen_edge_midpoint(i, j):\n            xi, fi, mi = elites[i]\n            xj, fj, mj = elites[j]\n            # bias toward better elite\n            if fi < fj:\n                bias = 0.7\n                base = bias * xi + (1 - bias) * xj\n            else:\n                bias = 0.7\n                base = bias * xj + (1 - bias) * xi\n            # sample along edge with a small extrapolation possibility\n            t = self.rng.random()\n            t = (t - 0.5) * 1.4 + 0.5  # slight bias to interior but allow outside\n            point = base + (t - 0.5) * (xi - xj)\n            # orthogonal perturbation: build orthonormal complement approx\n            d = xi - xj\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                ort = self.rng.standard_normal(self.dim)\n            else:\n                u = d / nd\n                # random vector then remove component u\n                r = self.rng.standard_normal(self.dim)\n                ort = r - np.dot(r, u) * u\n            ort = ort / (np.linalg.norm(ort) + 1e-12)\n            scale = (0.06 + 0.3 * self.rng.random()) * nd + 0.3 * sigma_global\n            y = point + ort * (self.rng.normal() * scale)\n            return reflect(y)\n\n        def gen_edge_extrapolate(i, j):\n            xi, fi, mi = elites[i]\n            xj, fj, mj = elites[j]\n            # extrapolate beyond better elite\n            if fi < fj:\n                base = xi\n                dirv = xi - xj\n            else:\n                base = xj\n                dirv = xj - xi\n            nd = np.linalg.norm(dirv) + 1e-12\n            diru = dirv / nd\n            length = (0.6 + 1.6 * self.rng.random()) * nd + 0.4 * sigma_global\n            y = base + diru * (length * (0.6 + 0.8 * self.rng.random()))\n            y = y + 0.15 * sigma_global * self.rng.standard_normal(self.dim)\n            return reflect(y)\n\n        def gen_local_gauss(idx):\n            x, f, mem = elites[idx]\n            C = local_covariance(mem)\n            # draw from gaussian with covariance C scaled by sigma_global\n            try:\n                L = np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                vals, vecs = np.linalg.eigh(C)\n                vals = np.clip(vals, 1e-12, None)\n                L = vecs @ np.diag(np.sqrt(vals))\n            z = L.dot(self.rng.standard_normal(self.dim))\n            y = x + sigma_global * z * (0.9 + 0.4 * self.rng.random())\n            return reflect(y)\n\n        def gen_barycentric(k=3):\n            # choose k elites and sampling weights, then add small PCA-based perturbation in their span\n            if len(elites) < 2:\n                return reflect(self.x_opt + sigma_global * self.rng.standard_normal(self.dim))\n            k = min(k, len(elites))\n            idxs = self.rng.choice(len(elites), size=k, replace=False)\n            pts = np.stack([elites[i][0] for i in idxs], axis=0)  # k x dim\n            weights = self.rng.random(k)\n            weights = weights / (np.sum(weights) + 1e-12)\n            base = weights.dot(pts)\n            # compute small PCA in the local subspace\n            S = pts - base\n            if np.linalg.norm(S) < 1e-12:\n                y = base + sigma_global * self.rng.standard_normal(self.dim)\n            else:\n                # covariance in the small set\n                C = np.cov(S.T, bias=True) + 1e-6 * np.eye(self.dim)\n                try:\n                    vals, vecs = np.linalg.eigh(C)\n                except Exception:\n                    vals = np.ones(self.dim)\n                    vecs = np.eye(self.dim)\n                # pick top direction\n                idx = np.argsort(vals)[::-1]\n                v = vecs[:, idx[0]]\n                # combine subspace perturbation and orthogonal gaussian\n                span_scale = (0.4 + 1.6 * self.rng.random()) * sigma_global\n                ort = 0.12 * sigma_global * self.rng.standard_normal(self.dim)\n                y = base + span_scale * ( (self.rng.normal() * v) ) + ort\n            return reflect(y)\n\n        def gen_global_random():\n            return self.rng.uniform(lb, ub)\n\n        # main optimization loop\n        while evals < self.budget:\n            iter_count += 1\n            # small decay on edge weights\n            for k in list(edges.keys()):\n                edges[k]['w'] *= edge_decay\n            # batch size heuristic\n            rem = self.budget - evals\n            batch = int(min(rem, max(1, min(max_batch, 4 + int(np.sqrt(self.dim))))))\n\n            candidates = []\n            cand_meta = []  # metadata to help update edges/elites\n            # draw many proposals; bias heavily toward edges to increase edge density\n            for b in range(batch):\n                r = self.rng.random()\n                # have four main modes: edge-driven (~55%), local Gaussian (~20%), barycentric (~10%), global random (~15%)\n                if len(edges) > 0 and r < 0.55:\n                    # sample an edge proportional to weight\n                    keys, ws = zip(*[(k, v['w']) for k, v in edges.items()])\n                    ws = np.array(ws, dtype=float)\n                    if np.sum(ws) <= 0 or np.any(np.isnan(ws)):\n                        probs = None\n                    else:\n                        probs = ws / ws.sum()\n                    # sample index\n                    try:\n                        ei = self.rng.choice(len(keys), p=probs)\n                    except Exception:\n                        ei = self.rng.integers(0, len(keys))\n                    i, j = keys[ei]\n                    # choose generation style along edge\n                    s = self.rng.random()\n                    if s < 0.55:\n                        y = gen_edge_midpoint(i, j)\n                        style = 'edge_mid'\n                    elif s < 0.85:\n                        y = gen_edge_extrapolate(i, j)\n                        style = 'edge_ext'\n                    else:\n                        # orthogonal probe to edge midpoint\n                        y = gen_edge_midpoint(i, j) + 0.25 * sigma_global * self.rng.standard_normal(self.dim)\n                        y = reflect(y)\n                        style = 'edge_orth'\n                    candidates.append(y)\n                    cand_meta.append({'type': style, 'edge': (i, j), 'edge_key': (min(i,j), max(i,j))})\n                elif r < 0.75 and len(elites) > 0:\n                    # local gaussian around a random elite\n                    idx = self.rng.integers(0, len(elites))\n                    y = gen_local_gauss(idx)\n                    candidates.append(y)\n                    cand_meta.append({'type': 'local', 'elite': idx})\n                elif r < 0.75 + triangle_rate and len(elites) >= 3:\n                    # barycentric recombination\n                    y = gen_barycentric(k=min(3, len(elites)))\n                    candidates.append(y)\n                    cand_meta.append({'type': 'bary'})\n                else:\n                    # global random\n                    y = gen_global_random()\n                    candidates.append(y)\n                    cand_meta.append({'type': 'global'})\n\n            # evaluate sequentially until budget exhausted\n            results = []\n            for k, y in enumerate(candidates):\n                if evals >= self.budget:\n                    break\n                fy = func(y)\n                evals += 1\n                results.append((y.copy(), float(fy), cand_meta[k]))\n\n                # immediate check for global improvement\n                if fy < self.f_opt - 1e-15:\n                    # update best\n                    prev_best = self.f_opt\n                    self.f_opt = float(fy)\n                    self.x_opt = y.copy()\n                    # add to elites\n                    mem = {'steps': []}\n                    elites.append((y.copy(), float(fy), mem))\n                    # create edges connecting to existing elites\n                    n = len(elites)\n                    # rebuild edges with a cap to avoid explosion: keep up to max_elites elites by score\n                    elites.sort(key=lambda p: p[1])\n                    if len(elites) > self.max_elites:\n                        elites = elites[:self.max_elites]\n                    # rebuild edges to include new pairs (cheap if small)\n                    rebuild_edges()\n                    # reward the edge that generated the sample if applicable\n                    meta = cand_meta[k]\n                    if meta.get('edge') is not None:\n                        ek = meta.get('edge_key')\n                        if ek in edges:\n                            edges[ek]['w'] = edges[ek].get('w', 1.0) * edge_boost\n                            edges[ek]['last'] = iter_count\n                    # update per-elite memory: record step from previous best if exists\n                    if prev_best < np.inf:\n                        step = y - self.x_opt  # small step (zero here), fallback: store small step anyway\n                        # record to the new elite memory to help local covariance later (no-op if zero)\n                        elites[0][2]['steps'].append(step)\n                        # trim\n                        elites[0][2]['steps'] = elites[0][2]['steps'][-local_mem_limit:]\n                    # continue (do not immediate sigma/gamma adaptation here)\n            # process evaluated results to update edges and elites\n            for y, fy, meta in results:\n                # try to insert into elites if it's competitive\n                # We keep elites sorted by fitness ascending, small set\n                inserted = False\n                if len(elites) == 0:\n                    elites.append((y.copy(), float(fy), {'steps': []}))\n                    inserted = True\n                else:\n                    # if better than worst or limit not reached\n                    elites_sorted = sorted(elites, key=lambda p: p[1])\n                    worst_f = elites_sorted[-1][1]\n                    if len(elites) < self.max_elites or fy < worst_f - 1e-12:\n                        # add\n                        elites.append((y.copy(), float(fy), {'steps': []}))\n                        # keep only top max_elites\n                        elites.sort(key=lambda p: p[1])\n                        if len(elites) > self.max_elites:\n                            elites = elites[:self.max_elites]\n                        inserted = True\n\n                # if candidate was generated from an edge, reward that edge slightly if candidate was good\n                if meta.get('edge') is not None:\n                    ek = meta.get('edge_key')\n                    if ek in edges:\n                        # reward proportionally to improvement over local average\n                        delta = max(0.0, max(0.0, (max(0.0, self.f_opt - fy)) ))\n                        # simpler: if candidate is within top-k elites or improved best, boost\n                        if fy <= self.f_opt + 1e-12:\n                            edges[ek]['w'] = edges[ek]['w'] * (1.0 + 0.5 * self.rng.random()) + edge_boost\n                        else:\n                            # small boost if candidate is better than average elite\n                            mean_elite = np.mean([e[1] for e in elites])\n                            if fy < mean_elite:\n                                edges[ek]['w'] += 0.5 * self.rng.random()\n\n                # if candidate was inserted as elite, create edges to neighbours and update local memories\n                if inserted:\n                    # ensure elites are sorted and unique-ish by proximity\n                    # prune near duplicates\n                    pruned = []\n                    for xi, fi, mem in elites:\n                        too_close = False\n                        for xj, fj, mm in pruned:\n                            if np.linalg.norm((xi - xj) / (ub - lb + 1e-12)) < 1e-8:\n                                too_close = True\n                                break\n                        if not too_close:\n                            pruned.append((xi, fi, mem))\n                    elites = pruned[:self.max_elites]\n                    # rebuild edges\n                    rebuild_edges()\n\n                # update per-elite memory for local covariance: find closest elite and record step\n                if len(elites) > 0:\n                    dists = [np.linalg.norm(y - e[0]) for e in elites]\n                    idx_min = int(np.argmin(dists))\n                    e_x, e_f, e_mem = elites[idx_min]\n                    step = y - e_x\n                    # store recent step\n                    e_mem.setdefault('steps', []).append(step)\n                    if len(e_mem['steps']) > local_mem_limit:\n                        e_mem['steps'] = e_mem['steps'][-local_mem_limit:]\n\n            # periodically prune and renormalize edge weights, also remove edges with tiny weight\n            if iter_count % 7 == 0:\n                # remove tiny edges and renormalize\n                total_w = 0.0\n                to_del = []\n                for k, v in edges.items():\n                    if v['w'] < 1e-4:\n                        to_del.append(k)\n                    else:\n                        total_w += v['w']\n                for k in to_del:\n                    del edges[k]\n                # if no edges, rebuild\n                if len(edges) == 0 and len(elites) >= 2:\n                    rebuild_edges()\n                # normalize to reasonable scale\n                if len(edges) > 0:\n                    meanw = np.mean([v['w'] for v in edges.values()])\n                    if meanw > 0:\n                        for v in edges.values():\n                            v['w'] = max(1e-6, v['w'] / (meanw + 1e-12))  # normalize around 1\n\n            # adaptive global sigma: small increase if recent improvements, decrease otherwise\n            # simple heuristic: if best updated recently (iter_count small since last update) keep sigma larger\n            # compute success indicator from elites: fraction of evaluations in this outer loop that improved best\n            # We approximate: if many elites improved (inserted) this iteration, increase sigma a bit\n            if any((r[1] < self.f_opt + 1e-12) for r in results):\n                sigma_global = min(sigma_max, sigma_global * (1.03 + 0.02 * self.rng.random()))\n            else:\n                sigma_global = max(sigma_min, sigma_global * (0.95 - 0.03 * self.rng.random()))\n\n            # occasional global diversification if stagnation (no improvement for many iterations)\n            if iter_count % max(20, 3*self.dim) == 0 and self.rng.random() < 0.25:\n                if evals < self.budget:\n                    z = self.rng.uniform(lb, ub)\n                    fz = func(z)\n                    evals += 1\n                    if fz < self.f_opt:\n                        self.f_opt = float(fz)\n                        self.x_opt = z.copy()\n                    # consider adding to elites\n                    if len(elites) < self.max_elites or fz < max(e[1] for e in elites):\n                        elites.append((z.copy(), float(fz), {'steps': []}))\n                        elites.sort(key=lambda p: p[1])\n                        if len(elites) > self.max_elites:\n                            elites = elites[:self.max_elites]\n                        rebuild_edges()\n\n        # final ensure best is reflected inside bounds\n        if self.x_opt is not None:\n            self.x_opt = reflect(self.x_opt)\n\n        return float(self.f_opt), np.array(self.x_opt)", "configspace": "", "generation": 3, "feedback": "The algorithm EdgeFocusedGraphGuidedSampler scored 0.210 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c8a93a65-df7b-4857-861c-4fb3773a5e6e"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8476415674643657}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9135141415695955}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8945335789552868}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.04143969801791225}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06258311047988974}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.015746870224949583}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.01313790752908739}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.013342435342996395}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.12528503881206876}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.10668964605688291}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.12117892850753631}], "aucs": [0.8476415674643657, 0.9135141415695955, 0.8945335789552868, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.04143969801791225, 0.06258311047988974, 0.015746870224949583, 0.01313790752908739, 4.999999999999449e-05, 0.013342435342996395, 0.12528503881206876, 0.10668964605688291, 0.12117892850753631]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3862.0, "Edges": 3861.0, "Max Degree": 36.0, "Min Degree": 1.0, "Mean Degree": 1.9994821336095288, "Degree Variance": 1.802174770654381, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.743281875357347, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3354859144744533, "Depth Entropy": 2.171485526897974, "Assortativity": 0.0, "Average Eccentricity": 19.983687208700154, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.0002589331952356292, "Average Shortest Path": 11.454851399439695, "mean_complexity": 9.0, "total_complexity": 99.0, "mean_token_count": 313.90909090909093, "total_token_count": 3453.0, "mean_parameter_count": 1.7272727272727273, "total_parameter_count": 19.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "ad6ffb9d-0131-4f63-8be9-2d0ea40ef28f", "fitness": 0.20347718589292574, "name": "EdgeDenseManifoldWalk", "description": "Edge-Dense Manifold Walk (EDMW) \u2014 build and densify a compact elite graph, create virtual interpolated nodes (midpoints and PCA-based manifold samples) to raise edge density, and perform short graph-walks plus local-subspace sampling to efficiently explore and exploit the space.", "code": "import numpy as np\n\nclass EdgeDenseManifoldWalk:\n    \"\"\"\n    Edge-Dense Manifold Walk (EDMW)\n\n    Main ideas:\n      - Maintain a compact elite archive and an explicit k-NN edge graph.\n      - Densify the graph by generating *virtual nodes* (interpolated midpoints and PCA-based manifold samples)\n        for promising edges to increase the \"edge density\" between good solutions without immediate evaluation.\n      - Candidate generation modes:\n         * virtual-node activation (evaluate a virtual node created on an edge),\n         * graph-walk aggregation (short random walk across edges -> sample around walk-centroid),\n         * PCA-subspace sampling around an elite neighborhood,\n         * focused local search around current best,\n         * global/opposite injection for diversification.\n      - Adapt proposal scale via success statistics and replace worst elites when improvements arrive.\n      - Strictly enforce exact evaluation budget.\n    Works on [-5, 5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget, dim, rng=None,\n                 pop_size=30, knn=5, init_samples_frac=0.08,\n                 sigma0=None, virtual_per_edge=2, edge_refresh=10):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # archive / population\n        self.pop_size = max(6, int(pop_size))\n        self.knn = max(1, int(knn))\n        self.init_samples_frac = float(init_samples_frac)\n\n        # proposal scale\n        if sigma0 is None:\n            # relative to domain width\n            self.sigma0 = 0.35 * (self.ub - self.lb)\n        else:\n            self.sigma0 = float(sigma0)\n        self.sigma = float(self.sigma0)\n        self.sigma_min = 1e-5\n        self.sigma_max = 5.0\n\n        # virtual node creation\n        self.virtual_per_edge = max(1, int(virtual_per_edge))\n        self.edge_refresh = int(edge_refresh)\n\n        # archive limits\n        self.max_archive = max(500, 40 * self.dim)\n\n        # adaptation counters\n        self.success_window = []\n        self.window_size = 30\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb)\n        ub = np.full(dim, self.ub)\n        width = ub - lb\n\n        evals = 0\n\n        # initial sampling: uniform random (ensuring at least a few points)\n        n_init = int(min(budget, max(6, int(self.init_samples_frac * budget), 6 + dim)))\n        X_archive = []\n        f_archive = []\n\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            fx = float(func(x))\n            X_archive.append(x.copy())\n            f_archive.append(float(fx))\n            evals += 1\n            if evals >= budget:\n                break\n\n        if len(X_archive) == 0:\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_archive.append(x0.copy())\n            f_archive.append(float(f0))\n            evals = 1\n\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # build initial elite population: best pop_size\n        def build_elite(Xa, fa):\n            order = np.argsort(fa)\n            k = min(self.pop_size, len(order))\n            inds = order[:k]\n            return Xa[inds].copy(), fa[inds].copy()\n\n        pop_X, pop_f = build_elite(X_archive, f_archive)\n        best_idx = int(np.argmin(pop_f))\n        x_best = pop_X[best_idx].copy()\n        f_best = float(pop_f[best_idx])\n\n        # graph structure\n        edges = []           # list of tuples (i,j)\n        edge_weights = np.array([])\n        # virtual nodes: list of dicts {coords: np.array, source_edge: (i,j), kind: 'mid'/'orth'/'pca'}\n        virtual_nodes = []\n\n        def compute_pairwise_distances(A):\n            # returns pairwise L2 distances (n,n)\n            if A.shape[0] < 2:\n                return np.zeros((A.shape[0], A.shape[0]))\n            dif = A[:, None, :] - A[None, :, :]\n            return np.linalg.norm(dif, axis=2)\n\n        def rebuild_graph_and_virtuals():\n            nonlocal edges, edge_weights, virtual_nodes, pop_X, pop_f\n            n = pop_X.shape[0]\n            virtual_nodes = []\n            if n < 2:\n                edges = []\n                edge_weights = np.array([])\n                return\n\n            D = compute_pairwise_distances(pop_X)\n            kn = min(self.knn, n - 1)\n            pairs = set()\n            for i in range(n):\n                neigh = np.argsort(D[i])[1:1 + kn]\n                for j in neigh:\n                    a, b = (i, j) if i < j else (j, i)\n                    pairs.add((a, b))\n            edges = sorted(list(pairs))\n\n            if len(edges) == 0:\n                edge_weights = np.array([])\n                return\n\n            # edge weighting: prioritize edges connecting good solutions, but also encourage medium-long edges\n            fmin, fmax = float(pop_f.min()), float(pop_f.max())\n            rng_scale = max(fmax - fmin, 1e-9)\n            weights = []\n            for (i, j) in edges:\n                avg_f = 0.5 * (pop_f[i] + pop_f[j])\n                quality_factor = np.exp(-(avg_f - fmin) / rng_scale)  # higher for better edges\n                length = D[i, j]\n                length_norm = length / (np.mean(width) + 1e-12)\n                length_factor = 1.0 + 0.9 * np.tanh(length_norm)  # prefer non-trivial inter-node distances\n                w = quality_factor * length_factor + 1e-6\n                weights.append(w)\n            weights = np.array(weights, dtype=float)\n            weights /= weights.sum()\n            edge_weights = weights\n\n            # generate a small pool of virtual nodes for the top edges (densify)\n            # pick some top edges by weight\n            topk = max(1, int(min(len(edges), self.virtual_per_edge * 4)))\n            top_indices = np.argsort(-edge_weights)[:topk]\n            for idx in top_indices:\n                i, j = edges[idx]\n                xi = pop_X[i]\n                xj = pop_X[j]\n                # create a midpoint virtual node\n                mid = 0.5 * (xi + xj)\n                # small orthogonal perturbation\n                dir_vec = xj - xi\n                norm_dir = np.linalg.norm(dir_vec) + 1e-12\n                if norm_dir > 1e-12:\n                    u = dir_vec / norm_dir\n                else:\n                    u = np.zeros_like(dir_vec)\n                # random orth vector via normal then remove projection\n                noise = self.rng.normal(size=dim)\n                proj = np.dot(noise, u) * u\n                orth = noise - proj\n                onorm = np.linalg.norm(orth)\n                if onorm > 1e-12:\n                    orth = orth / onorm\n                else:\n                    orth = np.zeros_like(orth)\n                orth_scale = (0.06 + 0.3 * self.rng.random()) * min(norm_dir, np.mean(width))\n                mid_orth = np.clip(mid + orth_scale * orth, lb, ub)\n                # PCA-based virtual sample: compute local neighbors and sample a low-dim manifold point\n                # gather neighbors for node i and j\n                neighbors_idx = [i, j]\n                # include up to knn nearest indices\n                # compute neighbors based on D for nodes i and j\n                neigh_i = np.argsort(D[i])[1:1 + kn]\n                neigh_j = np.argsort(D[j])[1:1 + kn]\n                for nid in neigh_i:\n                    if nid not in neighbors_idx:\n                        neighbors_idx.append(nid)\n                for nid in neigh_j:\n                    if nid not in neighbors_idx:\n                        neighbors_idx.append(nid)\n                neigh_coords = pop_X[neighbors_idx]\n                # center and PCA\n                cen = neigh_coords.mean(axis=0)\n                C = neigh_coords - cen\n                # small SVD\n                try:\n                    U, S, Vt = np.linalg.svd(C, full_matrices=False)\n                except Exception:\n                    U = None\n                if U is not None and U.shape[0] >= 2:\n                    # choose 1 or 2 dims for PCA sample\n                    kdim = min(2, U.shape[1])\n                    coeffs = (0.12 + 0.4 * self.rng.random()) * S[:kdim] * self.rng.normal(size=kdim)\n                    pca_sample = cen + (coeffs @ Vt[:kdim, :])\n                    pca_sample = np.clip(pca_sample, lb, ub)\n                else:\n                    pca_sample = mid.copy()\n\n                # register a few virtual nodes (midpoint, orth-perturbed, pca)\n                virtual_nodes.append({'coords': mid.copy(), 'edge': (i, j), 'kind': 'mid'})\n                virtual_nodes.append({'coords': mid_orth.copy(), 'edge': (i, j), 'kind': 'orth'})\n                virtual_nodes.append({'coords': pca_sample.copy(), 'edge': (i, j), 'kind': 'pca'})\n\n            # shuffle virtual nodes for randomness\n            self.rng.shuffle(virtual_nodes)\n\n        rebuild_graph_and_virtuals()\n\n        iter_since_improve = 0\n        total_iters = 0\n\n        # helper: pick an edge index weighted\n        def choose_edge_index():\n            if len(edges) == 0:\n                return None\n            probs = edge_weights.copy()\n            probs = np.clip(probs, 1e-12, None)\n            probs = probs / probs.sum()\n            return self.rng.choice(len(edges), p=probs)\n\n        # helper: sample by graph-walk aggregation\n        def graph_walk_sample(walk_len=3):\n            # pick a start node biased to elites (better f -> higher prob)\n            pf = pop_f.copy()\n            fmin, fmax = float(pf.min()), float(pf.max())\n            scale = max(fmax - fmin, 1e-9)\n            node_weights = np.exp(-(pf - fmin) / scale)\n            node_weights /= node_weights.sum()\n            current = self.rng.choice(pop_X.shape[0], p=node_weights)\n            walk_nodes = [current]\n            # perform short random walk across edges\n            for _ in range(walk_len - 1):\n                # find neighbors from edges\n                neighs = []\n                for (a, b) in edges:\n                    if a == current:\n                        neighs.append(b)\n                    elif b == current:\n                        neighs.append(a)\n                if len(neighs) == 0:\n                    break\n                # choose neighbor weighted by its fitness\n                neigh_weights = []\n                for nb in neighs:\n                    neigh_weights.append(node_weights[nb])\n                neigh_weights = np.array(neigh_weights)\n                neigh_weights = np.clip(neigh_weights, 1e-12, None)\n                neigh_weights = neigh_weights / neigh_weights.sum()\n                current = neighs[self.rng.choice(len(neighs), p=neigh_weights)]\n                walk_nodes.append(current)\n            # aggregate walk coords\n            coords = pop_X[walk_nodes]\n            centroid = coords.mean(axis=0)\n            # sample around centroid with anisotropic covariance derived from walk\n            cov = np.cov(coords.T) if coords.shape[0] > 1 else np.eye(dim) * (np.mean(width) * 0.02)**2\n            # stabilize cov diagonal\n            cov_diag = np.diag(cov)\n            cov_diag = np.maximum(cov_diag, (0.02 * np.mean(width))**2)\n            # sample in principal directions defined by cov diag (independent dims)\n            scale = 1.0 + 0.6 * (len(walk_nodes) / max(1.0, self.knn))\n            sample = centroid + self.sigma * scale * (self.rng.normal(size=dim) * np.sqrt(cov_diag + 1e-12) / (np.mean(np.sqrt(cov_diag)) + 1e-12))\n            return sample\n\n        # helper: PCA neighborhood sampling around a chosen elite index\n        def pca_neighborhood_sample(idx):\n            # find neighbors (based on euclidean)\n            Dp = compute_pairwise_distances(pop_X)\n            neighs = np.argsort(Dp[idx])[1:1 + max(2, self.knn)]\n            coords = pop_X[np.concatenate(([idx], neighs))]\n            cen = coords.mean(axis=0)\n            C = coords - cen\n            try:\n                U, S, Vt = np.linalg.svd(C, full_matrices=False)\n            except Exception:\n                U = None\n            if U is None or Vt.shape[0] == 0:\n                return cen + self.sigma * self.rng.normal(size=dim)\n            kdim = min(2, Vt.shape[0])\n            coeffs = (0.06 + 0.9 * self.rng.random()) * (S[:kdim] + 1e-9) * self.rng.normal(size=kdim)\n            sample = cen + coeffs @ Vt[:kdim, :]\n            # small isotropic jitter\n            sample += 0.12 * self.sigma * self.rng.normal(size=dim)\n            return np.clip(sample, lb, ub)\n\n        # main loop: generate exactly one candidate per iteration and evaluate until budget exhausted\n        while evals < budget:\n            total_iters += 1\n\n            # adapt exploration probabilities over time\n            frac = evals / max(1.0, budget)\n            p_virtual = 0.40 * (1.0 - frac) + 0.18  # virtual activation initially important\n            p_walk = 0.22 + 0.18 * frac\n            p_pca = 0.18 + 0.12 * frac\n            p_local = 0.12 + 0.08 * (1.0 - frac)\n            p_global = 1.0 - (p_virtual + p_walk + p_pca + p_local)\n            ps = np.array([p_virtual, p_walk, p_pca, p_local, p_global])\n            ps = np.clip(ps, 1e-8, None)\n            ps = ps / ps.sum()\n            p_virtual, p_walk, p_pca, p_local, p_global = ps\n\n            r = self.rng.random()\n\n            candidate = None\n            candidate_origin = None\n\n            # Mode 1: activate and evaluate a virtual node (if any)\n            if r < p_virtual and len(virtual_nodes) > 0:\n                # choose a virtual node biased towards those from top-weighted edges\n                # we prefer virtual nodes whose source edge has higher edge weight\n                # compute per-virtual weights\n                v_weights = []\n                for v in virtual_nodes:\n                    # find edge index\n                    try:\n                        e_idx = edges.index(v['edge'])\n                        v_weights.append(edge_weights[e_idx])\n                    except ValueError:\n                        v_weights.append(1e-6)\n                v_weights = np.array(v_weights, dtype=float)\n                v_weights = np.clip(v_weights, 1e-12, None)\n                if v_weights.sum() <= 0:\n                    v_probs = None\n                    vidx = self.rng.integers(0, len(virtual_nodes))\n                else:\n                    v_probs = v_weights / v_weights.sum()\n                    vidx = self.rng.choice(len(virtual_nodes), p=v_probs)\n                vnode = virtual_nodes.pop(vidx)\n                candidate = vnode['coords'].copy()\n                candidate_origin = ('virtual', vnode.get('kind', 'mid'), vnode.get('edge', None))\n\n            elif r < p_virtual + p_walk and len(edges) > 0:\n                # graph-walk aggregated sample\n                walk_len = 2 + self.rng.integers(0, 3)  # 2-4 steps\n                candidate = graph_walk_sample(walk_len=walk_len)\n                candidate_origin = ('walk', walk_len)\n\n            elif r < p_virtual + p_walk + p_pca and pop_X.shape[0] >= 3:\n                # PCA neighborhood sample around a chosen elite (better elites more likely)\n                pf = pop_f.copy()\n                fmin, fmax = float(pf.min()), float(pf.max())\n                scale = max(fmax - fmin, 1e-9)\n                weights = np.exp(-(pf - fmin) / scale)\n                weights /= weights.sum()\n                idx = self.rng.choice(pop_X.shape[0], p=weights)\n                candidate = pca_neighborhood_sample(idx)\n                candidate_origin = ('pca', idx)\n\n            elif r < p_virtual + p_walk + p_pca + p_local:\n                # focused local search around best\n                local_sigma = self.sigma * (0.4 + 0.9 * self.rng.random())\n                if pop_X.shape[0] >= max(3, dim // 2):\n                    stds = np.std(pop_X, axis=0)\n                    stds = np.maximum(stds, 0.03 * np.mean(width))\n                    anis = stds / np.mean(stds)\n                    candidate = x_best + local_sigma * (self.rng.normal(size=dim) * anis)\n                else:\n                    candidate = x_best + local_sigma * self.rng.normal(size=dim)\n                candidate_origin = ('local',)\n\n            else:\n                # global/opposite injection\n                if self.rng.random() < 0.7:\n                    # opposite of a random elite with jitter\n                    idx = self.rng.integers(0, pop_X.shape[0])\n                    x0 = pop_X[idx]\n                    opp = lb + ub - x0\n                    candidate = opp + (0.25 * self.sigma) * self.rng.normal(size=dim)\n                    candidate_origin = ('opposite', idx)\n                else:\n                    candidate = self.rng.uniform(lb, ub)\n                    candidate_origin = ('global',)\n\n            # ensure candidate exists\n            if candidate is None:\n                candidate = self.rng.uniform(lb, ub)\n                candidate_origin = ('fallback',)\n\n            # ensure bounds\n            candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n            # Evaluate (one evaluation)\n            f_candidate = float(func(candidate))\n            evals += 1\n\n            # incorporate into archive\n            X_archive = np.vstack([X_archive, candidate])\n            f_archive = np.concatenate([f_archive, [f_candidate]])\n\n            # check improvement\n            improved = False\n            if f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = candidate.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # update success window and sigma adaptation\n            self.success_window.append(1 if improved else 0)\n            if len(self.success_window) > self.window_size:\n                self.success_window.pop(0)\n            # adapt sigma based on recent success ratio\n            sr = (sum(self.success_window) / max(1, len(self.success_window)))\n            # if success rate high -> shrink, if low -> expand\n            if sr > 0.22:\n                self.sigma = max(self.sigma_min, self.sigma * (0.92 - 0.08 * sr))\n            elif sr < 0.06:\n                self.sigma = min(self.sigma_max, self.sigma * (1.08 - 0.02 * sr))\n            # clamp sigma\n            self.sigma = float(np.clip(self.sigma, self.sigma_min, self.sigma_max))\n\n            # maintain/populate elite population\n            if pop_X.shape[0] < self.pop_size:\n                pop_X = np.vstack([pop_X, candidate])\n                pop_f = np.concatenate([pop_f, [f_candidate]])\n            else:\n                # if candidate better than the worst, replace worst\n                worst_idx = int(np.argmax(pop_f))\n                if f_candidate < pop_f[worst_idx]:\n                    pop_X[worst_idx] = candidate.copy()\n                    pop_f[worst_idx] = f_candidate\n                else:\n                    # small chance to replace a random member to increase diversity\n                    if self.rng.random() < 0.015:\n                        ridx = self.rng.integers(0, pop_X.shape[0])\n                        pop_X[ridx] = candidate.copy()\n                        pop_f[ridx] = f_candidate\n\n            # periodically refresh graph and virtual nodes\n            if (evals % self.edge_refresh) == 0:\n                pop_X, pop_f = build_elite(X_archive, f_archive)\n                rebuild_graph_and_virtuals()\n\n            # also, if we just used up many virtual nodes, rebuild to replenish them\n            if len(virtual_nodes) < max(3, self.virtual_per_edge):\n                # rebuild virtuals without changing pop (cheap)\n                pop_X, pop_f = build_elite(X_archive, f_archive)\n                rebuild_graph_and_virtuals()\n\n            # archive trimming to keep memory bounded\n            if X_archive.shape[0] > self.max_archive:\n                keep_best = int(self.max_archive * 0.65)\n                best_inds = np.argsort(f_archive)[:keep_best]\n                # fill remaining with a random sample of others\n                others = np.setdiff1d(np.arange(X_archive.shape[0]), best_inds)\n                if len(others) > 0:\n                    choose_k = self.max_archive - keep_best\n                    choose_k = min(choose_k, len(others))\n                    rnd_inds = self.rng.choice(others, size=choose_k, replace=False)\n                    keep = np.concatenate([best_inds, rnd_inds])\n                else:\n                    keep = best_inds\n                keep = np.unique(keep)\n                X_archive = X_archive[keep]\n                f_archive = f_archive[keep]\n\n            # emergency diversification on long stagnation\n            if iter_since_improve > max(60, 5 * dim) and self.rng.random() < 0.20:\n                # boost sigma and force one expensive global sample (counts as next loop's eval)\n                self.sigma = min(self.sigma_max, self.sigma * (1.8 + 0.6 * self.rng.random()))\n                # create a strong global candidate and evaluate immediately if budget permits\n                if evals < budget:\n                    xg = self.rng.uniform(lb, ub)\n                    fg = float(func(xg))\n                    evals += 1\n                    X_archive = np.vstack([X_archive, xg])\n                    f_archive = np.concatenate([f_archive, [fg]])\n                    # maybe insert into population\n                    if pop_X.shape[0] < self.pop_size:\n                        pop_X = np.vstack([pop_X, xg])\n                        pop_f = np.concatenate([pop_f, [fg]])\n                    else:\n                        worst_idx = int(np.argmax(pop_f))\n                        if fg < pop_f[worst_idx]:\n                            pop_X[worst_idx] = xg\n                            pop_f[worst_idx] = fg\n                    if fg < f_best:\n                        f_best = fg\n                        x_best = xg.copy()\n                        iter_since_improve = 0\n\n            # small safeguard: ensure pop_f and pop_X sizes consistent\n            if pop_X.shape[0] != pop_f.shape[0]:\n                pop_X, pop_f = build_elite(X_archive, f_archive)\n\n            # safety rebuild occasionally if too many iterations\n            if total_iters % (5 * self.edge_refresh) == 0:\n                pop_X, pop_f = build_elite(X_archive, f_archive)\n                rebuild_graph_and_virtuals()\n\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 3, "feedback": "The algorithm EdgeDenseManifoldWalk scored 0.203 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["701bfc7e-c1fd-40fc-8533-beb7ddd599a9"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.22909497306232474}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.22440330994418867}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.22394561078831576}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.01856544837747387}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.02193947729299195}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.01121198309763427}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.009966185008843942}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.007815205515958823}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.011745029342726032}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.6607262026566807}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8936620839021667}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.7389322794045805}], "aucs": [0.22909497306232474, 0.22440330994418867, 0.22394561078831576, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.01856544837747387, 0.02193947729299195, 0.01121198309763427, 0.009966185008843942, 0.007815205515958823, 0.011745029342726032, 0.6607262026566807, 0.8936620839021667, 0.7389322794045805]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4187.0, "Edges": 4186.0, "Max Degree": 37.0, "Min Degree": 1.0, "Mean Degree": 1.9995223310246, "Degree Variance": 2.3057079160883807, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.32010582010582, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3193982928066883, "Depth Entropy": 2.1042846725903237, "Assortativity": 1.1600116239148993e-08, "Average Eccentricity": 19.860759493670887, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.0002388344877000239, "Average Shortest Path": 10.721418227259289, "mean_complexity": 9.375, "total_complexity": 75.0, "mean_token_count": 463.25, "total_token_count": 3706.0, "mean_parameter_count": 2.125, "total_parameter_count": 17.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "dad14baa-7d46-4686-8ce1-6aaea4db1ad0", "fitness": 0.2501229988025785, "name": "EdgeTriangulationAdaptivePrune", "description": "Edge-Triangle Adaptive Subspace Pruning (ETASP) \u2014 build a compact dense elite edge+triangle graph, propose barycentric/edge moves projected into short adaptive trust-subspaces, prune low-value edges, and adapt a single trust radius to concentrate search while using occasional Cauchy escapes and focused local refinements.", "code": "import numpy as np\n\nclass EdgeTriangulationAdaptivePrune:\n    \"\"\"\n    Edge-Triangle Adaptive Subspace Pruning (ETASP)\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional args:\n      pop_size: initial population size (clipped to budget)\n      elite_frac: fraction of population considered elite\n      k_neighbors: how many nearest neighbors to connect per elite\n      proposals_per_cycle: attempts per main cycle\n      tri_prob: probability to use triangle barycentric proposals\n      levy_prob: probability for heavy-tailed Cauchy jump on a proposal\n      trust_init: initial trust radius fraction of search range\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.25,\n                 k_neighbors=5, proposals_per_cycle=50, tri_prob=0.45,\n                 levy_prob=0.05, trust_init=0.18, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.tri_prob = float(tri_prob)\n        self.levy_prob = float(levy_prob)\n        self.trust_init = float(trust_init)\n\n        # population sizing\n        if pop_size is None:\n            base = int(np.clip(np.sqrt(self.budget) * 1.1, 8, 120))\n            self.pop_size = max(base, self.dim + 4)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation constants (kept simple and shallow)\n        self.trust_expand = 1.18\n        self.trust_shrink = 0.70\n        self.edge_inc = 1.25\n        self.edge_decay = 0.97\n        self.min_trust = 1e-5\n        self.max_trust = 5.0\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        # scale range and initial trust radius\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        trust = float(self.trust_init) * range_norm\n\n        # ensure pop_size <= budget and at least 2\n        pop_size = min(self.pop_size, max(2, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population uniformly in bounds and evaluate\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # edge weight matrix (symmetric), start neutral (1.0)\n        edge_w = np.ones((self.pop_size, self.pop_size), dtype=float)\n        np.fill_diagonal(edge_w, 0.0)\n\n        # bookkeeping\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        cycles_no_improve = 0\n        cycle = 0\n\n        # main search loop: cycles proposing many candidates per cycle\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            proposals = min(self.proposals_per_cycle, remaining)\n            improved_this_cycle = False\n\n            # decay edge weights a bit to forget old signals (shallow)\n            edge_w *= self.edge_decay\n            edge_w[edge_w < 1e-12] = 1e-12\n\n            # compute elites and neighbor structure (vectorized for clarity)\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            order = np.argsort(pop_f)\n            elites = order[:elite_count]\n            # pairwise squared distances (pop_size small enough typically)\n            dif = pop[:, None, :] - pop[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n\n            # build a compact set of edges: connect elites densely + k nearest neighbors per elite + a few random edges\n            edges_set = set()\n            # dense elites clique\n            for i in range(len(elites)):\n                for j in range(i+1, len(elites)):\n                    a = int(elites[i]); b = int(elites[j])\n                    edges_set.add((min(a,b), max(a,b)))\n            # k nearest neighbors per elite\n            for a in elites:\n                neigh = np.argsort(dist2[a])  # includes itself first\n                cnt = 0\n                for nb in neigh:\n                    if nb == a: continue\n                    edges_set.add((min(a,int(nb)), max(a,int(nb))))\n                    cnt += 1\n                    if cnt >= self.k_neighbors:\n                        break\n            # a few random cross edges to keep connectivity\n            rand_pairs = max(2, self.pop_size // 3)\n            for _ in range(rand_pairs):\n                a = int(rng.integers(0, self.pop_size)); b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges_set.add((min(a,b), max(a,b)))\n\n            edges = list(edges_set)\n            if len(edges) == 0:\n                # fallback: connect first two\n                edges = [(0, 1)]\n\n            # precompute sum of pop_f for quality normalization\n            pop_f_std = float(np.std(pop_f)) + 1e-8\n\n            # proposals loop\n            for _ in range(proposals):\n                if evals >= self.budget:\n                    break\n\n                # choose proposal type: triangle barycentric or edge interpolation\n                if rng.random() < self.tri_prob and len(edges) >= 2 and self.pop_size >= 3:\n                    # sample triangle from elites with bias\n                    if rng.random() < 0.8:\n                        idxs = rng.choice(elites, size=3, replace=False)\n                    else:\n                        idxs = rng.choice(self.pop_size, size=3, replace=False)\n                    a, b, c = int(idxs[0]), int(idxs[1]), int(idxs[2])\n                    Xa, Xb, Xc = pop[a], pop[b], pop[c]\n                    # Dirichlet-like barycentric weights (favor corners)\n                    w = rng.gamma(0.8, 1.0, size=3)\n                    w = w / (np.sum(w) + 1e-12)\n                    x_prop = w[0]*Xa + w[1]*Xb + w[2]*Xc\n                    # modest extrapolation toward triangle centroid controlled by trust\n                    if rng.random() < 0.18:\n                        centroid = (Xa + Xb + Xc) / 3.0\n                        factor = rng.uniform(-0.5, 1.0)\n                        x_prop = centroid + factor * (centroid - x_prop)\n                    # add gaussian noise scaled by trust and triangle spread\n                    tri_spread = (np.linalg.norm(Xa-Xb) + np.linalg.norm(Xa-Xc) + np.linalg.norm(Xb-Xc)) / (3.0 + 1e-12)\n                    noise_scale = 0.7 * max(trust, 0.5 * tri_spread)\n                    x_prop = x_prop + noise_scale * rng.standard_normal(dim) * 0.9\n\n                else:\n                    # edge interpolation/extrapolation\n                    # pick edge with probability proportional to weight and favoring endpoints with better quality\n                    e_idx = int(rng.choice(len(edges), p=np.array([edge_w[a,b] for (a,b) in edges]) / np.sum([edge_w[a,b] for (a,b) in edges])))\n                    a, b = edges[e_idx]\n                    Xa, Xb = pop[a], pop[b]\n                    # choose t with bias to better endpoint\n                    if pop_f[a] < pop_f[b]:\n                        t = rng.beta(2.0, 1.3)  # biased toward b end but flexible\n                    else:\n                        t = 1.0 - rng.beta(2.0, 1.3)\n                    # allow occasional extrapolation\n                    if rng.random() < 0.08:\n                        t = rng.uniform(-0.6, 1.6)\n                    x_prop = (1 - t) * Xa + t * Xb\n                    # perturb along edge and orthogonal gaussian scaled by trust\n                    edge_dir = Xb - Xa\n                    norm_edge = np.linalg.norm(edge_dir)\n                    if norm_edge > 1e-12:\n                        unit = edge_dir / norm_edge\n                        x_prop += 0.06 * trust * rng.standard_normal() * unit\n                    x_prop += 0.6 * trust * rng.standard_normal(dim)\n\n                # occasional Levy-like heavy jump\n                if rng.random() < self.levy_prob:\n                    # Generator.standard_cauchy exists; fall back to tan if necessary\n                    try:\n                        cauch = rng.standard_cauchy(dim)\n                    except AttributeError:\n                        u = rng.random(dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                    x_prop += 0.14 * range_norm * cauch\n\n                # clip and evaluate\n                x_prop = np.clip(x_prop, lb, ub)\n                f_prop = float(func(x_prop))\n                evals += 1\n\n                # choose replacement target: nearest neighbor among k nearest, prefer worse\n                d2 = np.sum((pop - x_prop)**2, axis=1)\n                nearest = np.argsort(d2)[:min(4, self.pop_size)]\n                # attempt to replace a worse neighbor; else maybe replace global worst if better\n                replaced_idx = -1\n                # find the worst among nearest that is worse than f_prop\n                candidate_idxs = nearest[np.argsort(pop_f[nearest])[::-1]]  # descending by fitness among nearest\n                for idx in candidate_idxs:\n                    if f_prop < pop_f[idx]:\n                        replaced_idx = int(idx)\n                        break\n                if replaced_idx >= 0:\n                    pop[replaced_idx] = x_prop.copy()\n                    pop_f[replaced_idx] = f_prop\n                    # increase weights of edges between replaced and its neighbors in nearest\n                    for nb in nearest:\n                        if nb == replaced_idx: continue\n                        edge_w[min(replaced_idx, int(nb)), max(replaced_idx, int(nb))] *= self.edge_inc\n                        edge_w[max(replaced_idx, int(nb)), min(replaced_idx, int(nb))] = edge_w[min(replaced_idx, int(nb)), max(replaced_idx, int(nb))]\n                    # adapt trust upward mildly on success (promote exploitation)\n                    trust = min(self.max_trust, trust * self.trust_expand)\n                    improved_this_cycle = True\n                else:\n                    # maybe replace global worst\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        pop[idx_worst] = x_prop.copy()\n                        pop_f[idx_worst] = f_prop\n                        trust = min(self.max_trust, trust * self.trust_expand)\n                        improved_this_cycle = True\n                    else:\n                        # unsuccessful proposal shrinks trust a bit\n                        trust = max(self.min_trust, trust * self.trust_shrink)\n\n                # update best\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n                    improved_this_cycle = True\n\n                # inexpensive maintenance: keep edge matrix symmetric and non-zero diagonal zero\n                np.fill_diagonal(edge_w, 0.0)\n                # small floor to avoid zeros\n                edge_w[edge_w < 1e-12] = 1e-12\n\n                # if budget nearly exhausted, break to final refine\n                if evals >= self.budget:\n                    break\n\n            # end proposals in cycle\n\n            # control stagnation: diversify if no improvement recently\n            if improved_this_cycle:\n                cycles_no_improve = 0\n            else:\n                cycles_no_improve += 1\n                if cycles_no_improve >= 3 and evals < self.budget:\n                    # inject a few focused candidates around current best\n                    n_inject = min(3, self.budget - evals)\n                    for _ in range(n_inject):\n                        cand = x_best + trust * rng.standard_normal(dim) * 0.8\n                        cand = np.clip(cand, lb, ub)\n                        f_c = float(func(cand))\n                        evals += 1\n                        # replace a random worse individual if better\n                        idx_w = int(np.argmax(pop_f))\n                        if f_c < pop_f[idx_w]:\n                            pop[idx_w] = cand\n                            pop_f[idx_w] = f_c\n                            if f_c < f_best:\n                                f_best = f_c; x_best = cand.copy()\n                    # shrink trust modestly to focus local search after injections\n                    trust = max(self.min_trust, trust * 0.85)\n                    cycles_no_improve = 0  # reset\n\n            # if remaining small, break main loop to do final refinement\n            if self.budget - evals <= max(6, dim):\n                break\n\n        # final local refinement: simple coordinate pattern search with reducing step (kept shallow)\n        step = 0.18 * range_norm\n        step_min = max(1e-8, 1e-6 * range_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # try positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                # try negative\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n\n        # store results\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EdgeTriangulationAdaptivePrune scored 0.250 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2c409501-52e9-4505-8ad4-bcf4ade87ef5"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.7512892923288773}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7502988808362294}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7403576012433791}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08893109212843175}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08668476324335261}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08783042289822551}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.058709698845282765}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.045248082308364745}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.05040704927679596}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.6013401019825808}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.3085195401588051}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.18207845678835233}], "aucs": [0.7512892923288773, 0.7502988808362294, 0.7403576012433791, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.08893109212843175, 0.08668476324335261, 0.08783042289822551, 0.058709698845282765, 0.045248082308364745, 0.05040704927679596, 0.6013401019825808, 0.3085195401588051, 0.18207845678835233]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2621.0, "Edges": 2620.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9992369324685235, "Degree Variance": 2.2205259343246615, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.419949706621962, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3254917914526994, "Depth Entropy": 2.201929317162554, "Assortativity": 1.8947690326113097e-08, "Average Eccentricity": 18.29797787104159, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00038153376573826786, "Average Shortest Path": 10.56708208218412, "mean_complexity": 19.333333333333332, "total_complexity": 58.0, "mean_token_count": 764.3333333333334, "total_token_count": 2293.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "mean depth"}
{"id": "e2df9c09-fa47-4fcb-915c-5728a2f0c9b2", "fitness": 0.21796441121556906, "name": "EdgeRepelledAdaptiveGraphSearch", "description": "Edge-Repelled Adaptive Graph Search (ERAGS) \u2014 build an elite edge graph but actively \"repel\" search away from stagnant/deep basins using multi-edge wedge escapes, heavy-tailed L\u00e9vy jumps, centroid reflections and adaptive multi-scale edge sampling to reduce mean solution depth and improve basin escape.", "code": "import numpy as np\n\nclass EdgeRepelledAdaptiveGraphSearch:\n    \"\"\"\n    Edge-Repelled Adaptive Graph Search (ERAGS)\n\n    Main ideas:\n      - Maintain a compact elite archive and k-NN edge graph.\n      - Track simple per-node stagnation/visit/success stats and a recent success window.\n      - Prefer escape-minded moves when stagnation is detected:\n          * Wedge-escape: combine two edge directions at a stagnant hub to push outward.\n          * Edge-orthogonal perturbation with magnitude scaled by stagnation.\n          * L\u00e9vy/Cauchy heavy-tailed jumps centered on elites (scale grows with stagnation).\n          * Centroid reflection: reflect population centroid about a node to propose diverse escapes.\n      - Adapt local sigma by a short success-rate window instead of only multiplicative shrink/expand.\n      - Ensure strict adherence to the evaluation budget.\n      - Works on [-5,5]^dim.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 pop_size=28, knn=6, init_frac=0.10,\n                 edge_refresh=10, max_archive_factor=30):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # population/graph params\n        self.pop_size = max(6, int(pop_size))\n        self.knn = max(1, int(knn))\n        self.init_frac = float(init_frac)\n        self.edge_refresh = int(edge_refresh)\n\n        # sigma and adaptation\n        self.sigma0 = 0.5 * (self.ub - self.lb)\n        self.sigma = float(self.sigma0)\n        self.sigma_min = 1e-5\n        self.sigma_max = 5.0\n        self.window_success = 40  # sliding window length for success rate\n        self.success_history = []  # recent boolean successes\n\n        # archive limits\n        self.max_archive = max(400, max(100, max_archive_factor * self.dim))\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb)\n        ub = np.full(dim, self.ub)\n        width = ub - lb\n\n        evals = 0\n\n        # initialization: diverse uniform samples\n        n_init = int(min(budget, max(6, int(self.init_frac * budget), 4 * dim)))\n        X_archive = []\n        f_archive = []\n\n        while evals < n_init:\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            X_archive.append(np.array(x, copy=True))\n            f_archive.append(float(f))\n            evals += 1\n\n        if len(X_archive) == 0:\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_archive.append(x0.copy())\n            f_archive.append(float(f0))\n            evals = 1\n\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # helper: build compact population (best K)\n        def build_population(Xa, fa):\n            order = np.argsort(fa)\n            k = min(self.pop_size, len(order))\n            inds = order[:k]\n            return Xa[inds].copy(), fa[inds].copy()\n\n        pop_X, pop_f = build_population(X_archive, f_archive)\n        best_idx = int(np.argmin(pop_f))\n        x_best = pop_X[best_idx].copy()\n        f_best = float(pop_f[best_idx])\n\n        # per-node simple stats (aligned with pop_X indices)\n        def init_node_stats(n):\n            # visits, last_improve_eval, successes\n            visits = np.ones(n, dtype=int)\n            last_improve = np.zeros(n, dtype=int)  # mark as 0 initially; will be updated\n            successes = np.zeros(n, dtype=int)\n            return visits, last_improve, successes\n\n        visits, last_improve, successes = init_node_stats(pop_X.shape[0])\n        # set last_improve for current best node to current evals to reduce immediate escape\n        last_improve[np.argmin(pop_f)] = evals\n\n        # edges and edge weights\n        edges = []\n        edge_weights = np.array([])\n\n        def refresh_graph_and_weights(current_eval):\n            nonlocal edges, edge_weights, pop_X, pop_f, visits, last_improve, successes\n            n = pop_X.shape[0]\n            if n < 2:\n                edges = []\n                edge_weights = np.array([])\n                return\n            # pairwise distances\n            D = np.linalg.norm(pop_X[:, None, :] - pop_X[None, :, :], axis=2)\n            kn = min(self.knn, n - 1)\n            pairs = set()\n            for i in range(n):\n                neigh = np.argsort(D[i])[1:1 + kn]\n                for j in neigh:\n                    a, b = (i, j) if i < j else (j, i)\n                    pairs.add((a, b))\n            edges = sorted(list(pairs))\n\n            if len(edges) == 0:\n                edge_weights = np.array([])\n                return\n\n            fmin, fmax = np.min(pop_f), np.max(pop_f)\n            frange = max(fmax - fmin, 1e-9)\n\n            # compute node stagnations and normalize\n            stagn = np.maximum(0, current_eval - last_improve)\n            if stagn.max() <= 0:\n                stagn_norm = stagn.astype(float)\n            else:\n                stagn_norm = stagn / (1.0 + stagn.max())\n\n            w = []\n            for (i, j) in edges:\n                # base preference: edges connecting different-fitness endpoints are interesting\n                df = abs(pop_f[i] - pop_f[j]) / frange\n                # stagnation factor: edges touching highly stagnant nodes get emphasis (we try escapes)\n                stagn_edge = max(stagn_norm[i], stagn_norm[j])\n                # local geometric factor: prefer medium-length edges (not too tiny, not spanning whole domain)\n                length = D[i, j] / (np.mean(width) + 1e-12)\n                length_factor = np.tanh(length * 1.2)\n                # combine: encourage escaping from stagnation and exploring fitness gradients\n                score = 0.6 * stagn_edge + 0.25 * df + 0.15 * length_factor\n                # small positive baseline for exploration diversity\n                w.append(np.clip(score, 1e-4, None))\n            edge_weights = np.array(w, dtype=float)\n            s = edge_weights.sum()\n            if s <= 0:\n                edge_weights = np.ones(len(edges), dtype=float) / len(edges)\n            else:\n                edge_weights = edge_weights / s\n\n        refresh_graph_and_weights(evals)\n\n        iter_since_improve = 0\n\n        total_iters = 0\n\n        # helper to choose edge index by weight\n        def choose_edge_idx():\n            if len(edges) == 0:\n                return None\n            probs = edge_weights.copy()\n            probs = np.clip(probs, 1e-12, None)\n            probs = probs / probs.sum()\n            return self.rng.choice(len(edges), p=probs)\n\n        # helper to ensure we don't exceed budget when attempting multi-eval probes\n        def can_evaluate(additional=1):\n            return evals + additional <= budget\n\n        # main loop\n        while evals < budget:\n            total_iters += 1\n            # adapt sampling probabilities based on stagnation and remaining budget\n            frac = evals / max(1.0, budget)\n            # base strategy weights\n            p_edge = 0.45 * (1 - frac) + 0.28  # edge-based proposals (interpolation + orth)\n            p_wedge = 0.12 + 0.18 * min(1.0, iter_since_improve / (20 + dim))\n            p_local = 0.18 + 0.35 * (1 - min(1.0, iter_since_improve / (50 + 5 * dim)))\n            p_global = 1.0 - (p_edge + p_wedge + p_local)\n            ps = np.array([p_edge, p_wedge, p_local, p_global])\n            ps = np.clip(ps, 1e-6, None)\n            ps = ps / ps.sum()\n            p_edge, p_wedge, p_local, p_global = ps\n\n            r = self.rng.random()\n\n            x_candidate = None\n\n            # choose modes\n            if r < p_edge and len(edges) > 0:\n                # Edge interpolation + adaptive orthogonal perturbation (magnitude increases with stagnation)\n                eidx = choose_edge_idx()\n                if eidx is None:\n                    x_candidate = self.rng.uniform(lb, ub)\n                else:\n                    i, j = edges[eidx]\n                    xi = pop_X[i]\n                    xj = pop_X[j]\n                    dir_vec = xj - xi\n                    dir_norm = np.linalg.norm(dir_vec) + 1e-12\n                    u = dir_vec / dir_norm\n                    # sample alpha biased toward midpoint but with extrapolation\n                    alpha = self.rng.beta(2.0, 2.0)\n                    alpha += self.rng.uniform(-0.15, 0.15)\n                    alpha = np.clip(alpha, -0.4, 1.4)\n                    base = alpha * xi + (1 - alpha) * xj\n                    # orthogonal noise\n                    noise = self.rng.normal(size=dim)\n                    proj = np.dot(noise, u) * u\n                    orth = noise - proj\n                    orth_norm = np.linalg.norm(orth)\n                    if orth_norm > 1e-12:\n                        orth = orth / orth_norm\n                    else:\n                        orth = self.rng.normal(size=dim)\n                        orth /= (np.linalg.norm(orth) + 1e-12)\n                    # stagnation-based scaling\n                    stagn_i = max(0, evals - last_improve[i])\n                    stagn_j = max(0, evals - last_improve[j])\n                    stagn_scale = 1.0 + 2.2 * max(stagn_i, stagn_j) / (1.0 + stagn_i + stagn_j)\n                    orth_mag = self.sigma * stagn_scale * (0.6 + 1.6 * self.rng.random()) * (min(dir_norm / (np.mean(width) + 1e-12), 1.3))\n                    along_mag = (alpha - 0.5) * dir_norm * (0.8 + 0.6 * self.rng.normal())\n                    x_candidate = base + along_mag * u + orth_mag * orth\n\n            elif r < p_edge + p_wedge and pop_X.shape[0] >= 3:\n                # Wedge escape: pick a hub node (prefer stagnant) and two neighbors to form a wedge and push outward\n                # choose hub by stagnation-weighted sampling\n                stagn = np.maximum(0, evals - last_improve)\n                if stagn.sum() <= 0:\n                    hub_probs = np.ones(pop_X.shape[0]) / pop_X.shape[0]\n                else:\n                    hub_probs = (1.0 + stagn) / (np.sum(1.0 + stagn))\n                hub = self.rng.choice(pop_X.shape[0], p=hub_probs)\n                # neighbors of hub from edges\n                neighbor_idxs = []\n                for (a, b) in edges:\n                    if a == hub:\n                        neighbor_idxs.append(b)\n                    elif b == hub:\n                        neighbor_idxs.append(a)\n                if len(neighbor_idxs) >= 2:\n                    j, k = self.rng.choice(np.array(neighbor_idxs), size=2, replace=False)\n                elif len(neighbor_idxs) == 1:\n                    j = neighbor_idxs[0]\n                    k = self.rng.integers(0, pop_X.shape[0])\n                else:\n                    j = self.rng.integers(0, pop_X.shape[0])\n                    k = self.rng.integers(0, pop_X.shape[0])\n                xh = pop_X[hub]\n                xj = pop_X[j]\n                xk = pop_X[k]\n                # directions away from neighbors\n                d1 = xh - xj\n                d2 = xh - xk\n                # canonical escape direction\n                dir_escape = d1 + d2\n                dn = np.linalg.norm(dir_escape)\n                if dn < 1e-12:\n                    # fall back to orthogonal random\n                    noise = self.rng.normal(size=dim)\n                    dir_escape = noise / (np.linalg.norm(noise) + 1e-12)\n                else:\n                    dir_escape = dir_escape / dn\n                # magnitude: combine sigma and wedge strength (if neighbors are similar, push more)\n                wedge_strength = 1.0 + (np.linalg.norm(d1) + np.linalg.norm(d2)) / (2.0 * np.mean(width) + 1e-12)\n                stagn_hub = max(0, evals - last_improve[hub])\n                mag = self.sigma * (1.0 + 1.8 * (stagn_hub / (1 + stagn_hub))) * wedge_strength\n                # small orthogonal blur\n                orth = self.rng.normal(size=dim)\n                orth = orth - np.dot(orth, dir_escape) * dir_escape\n                orth = orth / (np.linalg.norm(orth) + 1e-12)\n                x_candidate = xh + mag * dir_escape + 0.3 * mag * orth * self.rng.normal()\n\n            elif r < p_edge + p_wedge + p_local:\n                # Local around best or local around a good node with anisotropy\n                if pop_X.shape[0] >= max(3, dim // 2) and self.rng.random() < 0.8:\n                    # pick a good node (biased by fitness and recency)\n                    pf = pop_f.copy()\n                    fmin, fmax = pf.min(), pf.max()\n                    scale = max(fmax - fmin, 1e-9)\n                    weights = np.exp(-(pf - fmin) / (scale + 1e-12))\n                    # combine with recency preference: prefer nodes with more recent improvements\n                    recency = 1.0 / (1.0 + np.maximum(0, evals - last_improve))\n                    weights = weights * (0.5 + 0.5 * (recency / (recency.sum() + 1e-12)))\n                    weights = weights / weights.sum()\n                    idx = self.rng.choice(pop_X.shape[0], p=weights)\n                    center = pop_X[idx]\n                    # anisotropic: scale along dimensions with larger variance in pop\n                    stds = np.std(pop_X, axis=0)\n                    stds = np.maximum(stds, 0.02 * np.mean(width))\n                    anis = stds / np.mean(stds)\n                    local_sigma = self.sigma * (0.4 + 0.8 * self.rng.random())\n                    x_candidate = center + local_sigma * (self.rng.normal(size=dim) * anis)\n                else:\n                    # pure best-centered local\n                    local_sigma = self.sigma * (0.3 + 0.9 * self.rng.random())\n                    x_candidate = x_best + local_sigma * self.rng.normal(size=dim)\n\n            else:\n                # Global / heavy-tailed exploration modes\n                if self.rng.random() < 0.55:\n                    # Cauchy (L\u00e9vy-like) heavy tail around a randomly chosen elite (prefer the best)\n                    if self.rng.random() < 0.7:\n                        center = x_best\n                    else:\n                        idx = self.rng.integers(0, pop_X.shape[0])\n                        center = pop_X[idx]\n                    # scale grows with stagnation of entire population (escape pressure)\n                    global_stagn = np.mean(np.maximum(0, evals - last_improve))\n                    scale_factor = 0.6 + 1.8 * min(1.0, global_stagn / (10 + dim))\n                    # Cauchy vector\n                    # rng.standard_cauchy exists on Generator\n                    cauchy = self.rng.standard_cauchy(size=dim)\n                    # scale down very large ones for numerical stability\n                    cauchy = np.tanh(cauchy) * 3.0\n                    levy_vec = cauchy / (np.linalg.norm(cauchy) + 1e-12)\n                    mag = np.abs(self.rng.standard_cauchy())  # heavy-tailed magnitude\n                    mag = np.clip(mag, 0.2, 5.0)\n                    x_candidate = center + (mag * scale_factor * 0.8 * np.mean(width)) * levy_vec\n                else:\n                    # centroid reflection: reflect centroid of elites across a randomly selected elite\n                    centroid = pop_X.mean(axis=0)\n                    idx = self.rng.integers(0, pop_X.shape[0])\n                    xnode = pop_X[idx]\n                    reflection = 2 * xnode - centroid\n                    # small noise\n                    x_candidate = reflection + 0.5 * self.sigma * self.rng.normal(size=dim)\n\n            # Clip candidate\n            if x_candidate is None:\n                x_candidate = self.rng.uniform(lb, ub)\n            x_candidate = np.minimum(np.maximum(x_candidate, lb), ub)\n\n            # Evaluate candidate if budget allows\n            if not can_evaluate(1):\n                break\n            f_candidate = float(func(x_candidate))\n            evals += 1\n\n            # Update archive\n            X_archive = np.vstack([X_archive, x_candidate])\n            f_archive = np.concatenate([f_archive, [f_candidate]])\n\n            # is it improvement globally?\n            improved = False\n            if f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = x_candidate.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # record success history and adapt sigma by success window\n            self.success_history.append(bool(improved))\n            if len(self.success_history) > self.window_success:\n                self.success_history.pop(0)\n            succ_rate = sum(self.success_history) / max(1, len(self.success_history))\n            # adjust sigma: encourage exploration when success low, refine when success high\n            if succ_rate > 0.22:\n                self.sigma = max(self.sigma_min, self.sigma * 0.92)\n            elif succ_rate < 0.08:\n                self.sigma = min(self.sigma_max, self.sigma * 1.12)\n            else:\n                # small random jitter to avoid lock\n                self.sigma = np.clip(self.sigma * (1.0 + (self.rng.random() - 0.5) * 0.06), self.sigma_min, self.sigma_max)\n\n            # Update population: greedy replacement of worst or add if not full\n            if pop_X.shape[0] < self.pop_size:\n                pop_X = np.vstack([pop_X, x_candidate])\n                pop_f = np.concatenate([pop_f, [f_candidate]])\n                # expand stats arrays\n                visits = np.concatenate([visits, np.array([1])])\n                last_improve = np.concatenate([last_improve, np.array([evals if improved else 0])])\n                successes = np.concatenate([successes, np.array([1 if improved else 0])])\n            else:\n                worst_idx = int(np.argmax(pop_f))\n                if f_candidate < pop_f[worst_idx]:\n                    # replace worst\n                    pop_X[worst_idx] = x_candidate\n                    pop_f[worst_idx] = f_candidate\n                    visits[worst_idx] += 1\n                    if improved:\n                        last_improve[worst_idx] = evals\n                        successes[worst_idx] += 1\n                    else:\n                        # no improvement but visited\n                        pass\n                else:\n                    # sometimes inject diversity by replacing a random mid-ranked member\n                    if self.rng.random() < 0.025:\n                        ridx = self.rng.integers(0, pop_X.shape[0])\n                        pop_X[ridx] = x_candidate\n                        pop_f[ridx] = f_candidate\n                        visits[ridx] += 1\n                        if improved:\n                            last_improve[ridx] = evals\n                            successes[ridx] += 1\n\n            # Update node last_improve roughly: if candidate equals some pop node we mark last_improve\n            if pop_X.shape[0] > 0:\n                # ensure arrays consistent lengths\n                if len(visits) != pop_X.shape[0]:\n                    visits = np.ones(pop_X.shape[0], dtype=int)\n                    last_improve = np.zeros(pop_X.shape[0], dtype=int)\n                    successes = np.zeros(pop_X.shape[0], dtype=int)\n\n            # Keep archive bounded\n            if X_archive.shape[0] > self.max_archive:\n                keep_best = int(self.max_archive * 0.6)\n                best_inds = np.argsort(f_archive)[:keep_best]\n                rem = X_archive.shape[0] - keep_best\n                if rem > 0:\n                    other_inds = self.rng.choice(np.arange(X_archive.shape[0])[keep_best:], size=self.max_archive - keep_best, replace=False)\n                    keep = np.unique(np.concatenate([best_inds, other_inds]))\n                else:\n                    keep = best_inds\n                X_archive = X_archive[keep]\n                f_archive = f_archive[keep]\n\n            # Periodic refresh: rebuild population and recalc graph with updated stats\n            if (evals % self.edge_refresh) == 0 or total_iters % (2 * self.edge_refresh) == 0:\n                pop_X, pop_f = build_population(X_archive, f_archive)\n                # Reinitialize node stats conservatively: keep previous info for matching points if possible (approx)\n                n_new = pop_X.shape[0]\n                old_n = len(visits)\n                new_visits = np.ones(n_new, dtype=int)\n                new_last_improve = np.zeros(n_new, dtype=int)\n                new_successes = np.zeros(n_new, dtype=int)\n                # Try to inherit stats by matching identical rows (rare) otherwise default\n                for ii in range(n_new):\n                    row = pop_X[ii]\n                    # search in previous pop for close match\n                    if old_n > 0:\n                        dists = np.linalg.norm(pop_X[ii] - pop_X, axis=1)  # note: this is same array but safe fallback\n                    # we do not have exact mapping; keep default conservative values\n                    new_visits[ii] = 1\n                    new_last_improve[ii] = 0\n                    new_successes[ii] = 0\n                # make best node appear recently improved\n                if n_new > 0:\n                    bestloc = int(np.argmin(pop_f))\n                    new_last_improve[bestloc] = evals\n                visits, last_improve, successes = new_visits, new_last_improve, new_successes\n                refresh_graph_and_weights(evals)\n\n            # Emergency diversification if long stagnation: perform a guaranteed heavy exploration step (but respect budget)\n            if iter_since_improve > max(90, 6 * dim):\n                if can_evaluate(1) and self.rng.random() < 0.22:\n                    # scale up sigma momentarily\n                    self.sigma = min(self.sigma_max, self.sigma * (1.8 + self.rng.random()))\n                    # do one heavy cauchy jump around random elite\n                    center = pop_X[self.rng.integers(0, pop_X.shape[0])]\n                    c = self.rng.standard_cauchy(size=dim)\n                    c = np.tanh(c) * 3.0\n                    vec = c / (np.linalg.norm(c) + 1e-12)\n                    mag = 0.8 * np.mean(width) * (1.0 + 2.0 * self.rng.random())\n                    xg = center + mag * vec\n                    xg = np.minimum(np.maximum(xg, lb), ub)\n                    fg = float(func(xg))\n                    evals += 1\n                    X_archive = np.vstack([X_archive, xg])\n                    f_archive = np.concatenate([f_archive, [fg]])\n                    # possibly update pop\n                    if fg < pop_f.max():\n                        worst_idx = int(np.argmax(pop_f))\n                        pop_X[worst_idx] = xg\n                        pop_f[worst_idx] = fg\n                    if fg < f_best:\n                        f_best = fg\n                        x_best = xg.copy()\n                        iter_since_improve = 0\n                    # refresh graph weights\n                    refresh_graph_and_weights(evals)\n                    # continue main loop\n\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 4, "feedback": "The algorithm EdgeRepelledAdaptiveGraphSearch scored 0.218 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["701bfc7e-c1fd-40fc-8533-beb7ddd599a9"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.26117789067710795}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.314588726372395}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.31685921729374344}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.01821740137032668}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.010604267400207923}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.024943390280467992}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.01368381941471486}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.015598282315209877}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.012960819994402462}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.703541248892992}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.7789691494262883}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.7981719547956796}], "aucs": [0.26117789067710795, 0.314588726372395, 0.31685921729374344, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.01821740137032668, 0.010604267400207923, 0.024943390280467992, 0.01368381941471486, 0.015598282315209877, 0.012960819994402462, 0.703541248892992, 0.7789691494262883, 0.7981719547956796]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4267.0, "Edges": 4266.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.9995312866182329, "Degree Variance": 2.2441994522083992, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.93200408997955, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3164307428074091, "Depth Entropy": 2.2190356167839393, "Assortativity": 0.0, "Average Eccentricity": 19.499648464963673, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.00023435669088352472, "Average Shortest Path": 10.976981294644373, "mean_complexity": 9.142857142857142, "total_complexity": 64.0, "mean_token_count": 540.1428571428571, "total_token_count": 3781.0, "mean_parameter_count": 2.2857142857142856, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "mean depth"}
{"id": "0756d491-38eb-4333-bb81-555f6c07329f", "fitness": 0.21274128070241022, "name": "HierarchicalDirectionalProbing", "description": "Hierarchical Directional Probing (HDP) \u2014 aggressive exponential line-searches along a small adaptive set of orthogonal directions, combined with opposite-point injections, directional memory, and occasional L\u00e9vy escapes to reach good basins quickly and reduce solution depth.", "code": "import numpy as np\n\nclass HierarchicalDirectionalProbing:\n    \"\"\"\n    Hierarchical Directional Probing (HDP)\n\n    Key ideas:\n    - Keep a small adaptive memory of successful directions (orthonormalized).\n    - Perform aggressive exponential (geometric) line-searches along a mix of memory directions and fresh random directions\n      (both + and -), allowing long leaps that quickly cross basins (reduces mean depth).\n    - Use opposition-based samples (opposite of best or random elites) to probe far-away symmetric regions cheaply.\n    - Occasional L\u00e9vy-like heavy jumps to escape deep traps.\n    - Update direction memory from successful long displacements and use simple local Gaussian sampling around the best when promising.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 n_init=None,\n                 n_directions=6,\n                 max_line_evals=6,\n                 opp_prob=0.10,\n                 levy_prob=0.04,\n                 memory_size=6,\n                 sigma_scale=0.25,\n                 growth=2.0,\n                 stagnation_restart=150):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # initialization sizes\n        if n_init is None:\n            # use a small but diverse initial sample proportional to dim and budget\n            self.n_init = int(min(max(8, 3 * dim), max(8, int(0.06 * budget))))\n        else:\n            self.n_init = int(n_init)\n\n        # how many directions to probe per main iteration\n        self.n_directions = int(max(2, n_directions))\n        # max evaluations along each sign for the exponential line search\n        self.max_line_evals = int(max(1, max_line_evals))\n\n        # probabilities for opposite sampling and levy jumps\n        self.opp_prob = float(opp_prob)\n        self.levy_prob = float(levy_prob)\n\n        # directional memory\n        self.memory_size = int(max(1, memory_size))\n\n        # initial sigma relative to search range (will be scaled by range_scale)\n        self.sigma_scale = float(sigma_scale)\n\n        # geometric growth factor for line search (exponential exploration)\n        self.growth = float(growth)\n\n        # stagnation restart threshold\n        self.stagnation_restart = int(stagnation_restart)\n\n    def _get_bounds(self, func):\n        # Try to detect bounds, otherwise default to [-5,5] per problem statement\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            try:\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            except Exception:\n                lb = None\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        return lb, ub\n\n    def _normalize(self, v):\n        v = np.asarray(v, dtype=float).reshape(-1)\n        n = np.linalg.norm(v)\n        if n == 0 or not np.isfinite(n):\n            # fallback to random direction\n            rd = self.rng.normal(size=v.shape)\n            rn = np.linalg.norm(rd)\n            return rd / (rn + 1e-12)\n        return v / n\n\n    def _orthonormalize_set(self, vecs):\n        # Gram-Schmidt orthonormalization, returns list of orthonormal vectors\n        out = []\n        for v in vecs:\n            w = v.copy().astype(float)\n            for u in out:\n                w -= np.dot(u, w) * u\n            n = np.linalg.norm(w)\n            if n > 1e-12:\n                out.append(w / n)\n        return out\n\n    def _reflect_clip(self, x, lb, ub):\n        # Simple reflection at boundaries then clip as a safeguard\n        below = x < lb\n        above = x > ub\n        if np.any(below) or np.any(above):\n            x = np.where(below, lb + (lb - x), x)\n            x = np.where(above, ub - (x - ub), x)\n            x = np.clip(x, lb, ub)\n        return x\n\n    def _levy_dir(self):\n        # Cauchy-based heavy-tailed direction\n        z = self.rng.standard_cauchy(size=self.dim).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=self.dim))\n        return self._normalize(z)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        dim = self.dim\n        budget = int(self.budget)\n\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        sigma = max(1e-12, self.sigma_scale * range_scale)\n\n        evals = 0\n        x_best = None\n        f_best = np.inf\n\n        # directional memory (store normalized direction vectors)\n        dir_memory = []\n\n        # small elite list to enable opposition sampling from elites\n        elites = []\n\n        # initial diversified sampling (uniform)\n        n_init = min(self.n_init, budget)\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n            elites.append((f, x.copy()))\n            if evals >= budget:\n                break\n\n        # keep best K elites sorted\n        elites = sorted(elites, key=lambda t: t[0])[:max(3, dim)]\n\n        since_improvement = 0\n\n        # Main loop\n        # Each outer iteration selects a small set of directions and performs\n        # geometric/exponential line-searches along each sign with limited evals.\n        while evals < budget:\n            remaining = budget - evals\n            # If no best found (should not happen), sample a random point\n            if x_best is None:\n                x_best = rng.uniform(lb, ub)\n                f_best = float(func(x_best))\n                evals += 1\n                elites = sorted(elites + [(f_best, x_best.copy())], key=lambda t: t[0])[:max(3, dim)]\n\n            # Opposition-based quick probe: opposite of best relative to center of bounds\n            if rng.random() < self.opp_prob and evals < budget:\n                center = (lb + ub) / 2.0\n                x_opp = center + (center - x_best)  # opposite relative to center\n                x_opp = self._reflect_clip(x_opp, lb, ub)\n                f_opp = float(func(x_opp))\n                evals += 1\n                if f_opp < f_best:\n                    f_best = f_opp\n                    x_best = x_opp.copy()\n                    since_improvement = 0\n                    dir = self._normalize(x_best - center)\n                    dir_memory.insert(0, dir)\n                else:\n                    since_improvement += 1\n                elites = sorted(elites + [(f_opp, x_opp.copy())], key=lambda t: t[0])[:max(3, dim)]\n                if evals >= budget:\n                    break\n\n            # Build candidate directions:\n            # - include top memory directions\n            # - include a few orthonormal random directions\n            base_dirs = []\n            # keep only unique memory directions\n            for d in dir_memory[:self.memory_size]:\n                base_dirs.append(np.array(d, dtype=float))\n            # fill remaining with random directions orthonormalized\n            rand_try = [rng.normal(size=dim) for _ in range(max(0, self.n_directions - len(base_dirs) + 4))]\n            rand_try = [self._normalize(v) for v in rand_try]\n            # orthonormalize memory + randoms and pick required number\n            all_dirs = self._orthonormalize_set(base_dirs + rand_try)\n            # ensure we have at least some directions\n            if len(all_dirs) < 1:\n                all_dirs = [self._normalize(rng.normal(size=dim))]\n            # select up to n_directions directions (diversify by picking spread)\n            n_sel = min(self.n_directions, len(all_dirs))\n            # simple selection: pick first n_sel (memory are earlier)\n            directions = all_dirs[:n_sel]\n\n            # For each direction, perform exponential line search for both signs\n            for d in directions:\n                if evals >= budget:\n                    break\n\n                # choose a small initial step and then grow geometrically\n                # initial step scaled towards range to allow long jumps quickly\n                step0 = sigma\n                # randomly jitter direction a little to avoid stagnation on exact axes\n                d_probe = self._normalize(d + 1e-3 * rng.normal(size=dim))\n\n                improved_on_dir = False\n                # try both signs: +1 then -1 (order randomized to avoid bias)\n                signs = [+1, -1] if rng.random() < 0.5 else [-1, +1]\n                for s in signs:\n                    if evals >= budget:\n                        break\n                    step = step0\n                    prev_success = False\n                    # perform up to max_line_evals trials along this sign.\n                    for k in range(self.max_line_evals):\n                        # If remaining budget low, break to preserve ability to explore other directions\n                        if evals >= budget:\n                            break\n                        # propose\n                        x_try = x_best + s * step * d_probe\n                        x_try = self._reflect_clip(x_try, lb, ub)\n                        f_try = float(func(x_try))\n                        evals += 1\n\n                        if f_try < f_best:\n                            # found improvement: accept and continue growing (aggressive)\n                            disp = x_try - x_best\n                            x_best = x_try.copy()\n                            f_best = f_try\n                            since_improvement = 0\n                            improved_on_dir = True\n                            prev_success = True\n                            # incorporate normalized displacement into memory (favor long displacements)\n                            dir_new = self._normalize(disp)\n                            dir_memory.insert(0, dir_new)\n                            # trim memory\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory = dir_memory[:self.memory_size]\n                            # grow step to try to leap further\n                            step *= self.growth\n                            # also consider refining sigma down a bit to focus local search after improvement\n                            sigma = max(1e-12, sigma * 0.95)\n                        else:\n                            # no improvement: if we just had a success in previous step, try one more (possible monotone),\n                            # otherwise back-off and stop exploring this sign\n                            if prev_success:\n                                # try one more with same sign at intermediate (smaller) step to refine near boundary\n                                step = max(1e-12, step / (self.growth * 1.5))\n                                prev_success = False\n                                # continue with reduced step (allow one extra refinement)\n                                continue\n                            else:\n                                # no improvement and no recent success: stop this sign\n                                since_improvement += 1\n                                break\n\n                # end for signs\n                # If we improved significantly along this direction, slightly increase sigma (encourage exploitation)\n                if improved_on_dir:\n                    sigma = min(8.0 * range_scale, sigma * 1.06)\n                else:\n                    # if direction failed, slightly reduce sigma to try smaller local adjustments\n                    sigma = max(1e-12, sigma * 0.98)\n\n                if evals >= budget:\n                    break\n\n            # occasional local Gaussian refinement around best using covariance from memory\n            if evals < budget and len(dir_memory) >= 2:\n                # build simple diagonal-like covariance along memory directions\n                mem_mat = np.stack(dir_memory[:min(len(dir_memory), self.memory_size)])\n                # approximate directional spread scalars\n                scales = np.maximum(0.05 * sigma, 0.5 * sigma * (1.0 / (1.0 + np.arange(mem_mat.shape[0]))))\n                # sample one or two local candidates\n                local_tries = min(2, budget - evals)\n                for i in range(local_tries):\n                    # mix memory directions with random normal noise\n                    coeffs = rng.normal(scale=scales[:mem_mat.shape[0]])\n                    delta = (coeffs.reshape(1, -1) @ mem_mat[:coeffs.size]).reshape(-1)\n                    delta += 0.3 * sigma * rng.normal(size=dim)  # isotropic jitter\n                    x_local = x_best + delta\n                    x_local = self._reflect_clip(x_local, lb, ub)\n                    f_local = float(func(x_local))\n                    evals += 1\n                    if f_local < f_best:\n                        disp = x_local - x_best\n                        x_best = x_local.copy()\n                        f_best = f_local\n                        since_improvement = 0\n                        dir_memory.insert(0, self._normalize(disp))\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory = dir_memory[:self.memory_size]\n                    else:\n                        since_improvement += 1\n                    if evals >= budget:\n                        break\n\n            # occasional Levy escape if stagnating or by random chance\n            if (since_improvement > 0 and since_improvement % max(10, self.stagnation_restart // 5) == 0 and evals < budget) \\\n               or (rng.random() < self.levy_prob and evals < budget):\n                # heavy escape: sample a levy-like direction from best amplified to a significant fraction of range\n                ld = self._levy_dir()\n                # amplify to random magnitude between 0.5*range and 1.5*range (cap to bounds)\n                mag = range_scale * (0.5 + rng.random() * 1.0)\n                x_jump = x_best + ld * mag\n                x_jump = self._reflect_clip(x_jump, lb, ub)\n                f_jump = float(func(x_jump))\n                evals += 1\n                if f_jump < f_best:\n                    disp = x_jump - x_best\n                    x_best = x_jump.copy()\n                    f_best = f_jump\n                    since_improvement = 0\n                    dir_memory.insert(0, self._normalize(disp))\n                    if len(dir_memory) > self.memory_size:\n                        dir_memory = dir_memory[:self.memory_size]\n                    # after successful jump, slightly reduce probability of levy\n                    self.levy_prob = max(0.01, self.levy_prob * 0.7)\n                else:\n                    since_improvement += 1\n                    # if unsuccessful, mildly increase levy probability to encourage future escapes\n                    self.levy_prob = min(0.5, self.levy_prob * 1.08)\n                elites = sorted(elites + [(f_jump, x_jump.copy())], key=lambda t: t[0])[:max(3, dim)]\n\n            # update elites with current best\n            elites = sorted(elites + [(f_best, x_best.copy())], key=lambda t: t[0])[:max(3, dim)]\n\n            # stagnation restart: if no improvement for a while, inject global diversification\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # try a few opposite / elite-based relocations\n                tries = min(6, budget - evals)\n                for t in range(tries):\n                    if rng.random() < 0.6 and len(elites) > 0:\n                        # sample around a random elite (broad)\n                        _, center = elites[rng.integers(0, len(elites))]\n                        perturb = (0.5 + rng.random()) * range_scale * rng.normal(size=dim)\n                        x_new = center + perturb\n                    else:\n                        # global uniform relocation\n                        x_new = rng.uniform(lb, ub)\n                    x_new = self._reflect_clip(x_new, lb, ub)\n                    f_new = float(func(x_new))\n                    evals += 1\n                    if f_new < f_best:\n                        disp = x_new - x_best\n                        x_best = x_new.copy()\n                        f_best = f_new\n                        since_improvement = 0\n                        dir_memory.insert(0, self._normalize(disp))\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory = dir_memory[:self.memory_size]\n                        # shrink sigma to focus local search after hitting a new good area\n                        sigma = max(1e-12, sigma * 0.7)\n                        break\n                    else:\n                        since_improvement += 1\n                    if evals >= budget:\n                        break\n                # after restart attempts, encourage more levy escapes if still stuck\n                if since_improvement >= self.stagnation_restart:\n                    self.levy_prob = min(0.6, self.levy_prob * 1.3)\n                    # enlarge sigma to allow bigger leaps\n                    sigma = min(8.0 * range_scale, sigma * 1.8)\n                    # slightly randomize memory to avoid repeating same directions\n                    if len(dir_memory) > 0:\n                        for i in range(len(dir_memory)):\n                            dir_memory[i] = self._normalize(dir_memory[i] + 0.2 * rng.normal(size=dim))\n\n            # safety: keep sigma reasonable relative to range\n            sigma = float(np.clip(sigma, 1e-12, 8.0 * range_scale))\n\n            # small book-keeping: if we had a recent improvement reset counter appropriately\n            if since_improvement < 0:\n                since_improvement = 0\n\n            # break if budget exhausted\n            if evals >= budget:\n                break\n\n            # If no improvement for extremely long, force a heavy jump to a random elite/opposite\n            if since_improvement > 5 * max(100, self.stagnation_restart) and evals < budget:\n                x_forced = rng.uniform(lb, ub)\n                f_forced = float(func(x_forced))\n                evals += 1\n                if f_forced < f_best:\n                    x_best = x_forced.copy()\n                    f_best = f_forced\n                    since_improvement = 0\n                    dir_memory.insert(0, self._normalize(x_best - x_forced))\n                    if len(dir_memory) > self.memory_size:\n                        dir_memory = dir_memory[:self.memory_size]\n                else:\n                    since_improvement += 1\n\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 4, "feedback": "The algorithm HierarchicalDirectionalProbing scored 0.213 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9616688904923573}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9482623422219503}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9489809415685752}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.038200437092008044}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.026384757236823098}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.011288195651194988}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.08015375053937002}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.09297112403610774}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.08290877169776778}], "aucs": [0.9616688904923573, 0.9482623422219503, 0.9489809415685752, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.038200437092008044, 0.026384757236823098, 4.999999999999449e-05, 0.011288195651194988, 4.999999999999449e-05, 0.08015375053937002, 0.09297112403610774, 0.08290877169776778]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2824.0, "Edges": 2823.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9992917847025495, "Degree Variance": 1.9603394417738687, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.260663507109005, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3334469238280173, "Depth Entropy": 2.1559510143585263, "Assortativity": 0.0, "Average Eccentricity": 17.750354107648725, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0003541076487252125, "Average Shortest Path": 10.902169200988642, "mean_complexity": 10.571428571428571, "total_complexity": 74.0, "mean_token_count": 350.85714285714283, "total_token_count": 2456.0, "mean_parameter_count": 3.7142857142857144, "total_parameter_count": 26.0}, "archive_direction": "decrease", "archive_feature": "mean depth"}
{"id": "16e78c81-11a6-405a-ad52-ea8cb358d551", "fitness": "-inf", "name": "EdgeEnrichedAdaptiveGraphSearch", "description": "Edge-Enriched Adaptive Graph Search v2 \u2014 build and exploit a layered elite edge graph with shortcut recombinations (multi-hop teleportation), local linear descent surrogates, and adaptive step-size memory to shorten solution \"depth\" and reach good basins faster.", "code": "import numpy as np\n\nclass EdgeEnrichedAdaptiveGraphSearch:\n    \"\"\"\n    Refined Edge-Enriched Adaptive Graph Search (EEAGS v2)\n\n    Improvements over baseline EEAGS:\n      - Shortcut recombinations: perform short random-walk / multi-hop sampling on the elite edge graph\n        to create \"teleport\" recombinations that bridge distant good nodes (reduces mean search depth).\n      - Local linear surrogate descent: fit a simple linear model on neighbors to estimate a descent\n        direction and take an adaptive step (fast directed progress).\n      - Success-memory-based sigma adaptation (bandit-like): track recent success rate to adapt step-size.\n      - Slightly more aggressive edge weighting that favors short, high-quality edges and edges that\n        often participate in successful shortcuts.\n      - Careful budget accounting and conservative extra-evaluations in diversification.\n    Works on functions defined on [-5,5]^dim. Enforces exact evaluation budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 pop_size=32, knn=6, init_frac=0.12, edge_refresh=8,\n                 success_window=20):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # graph/population parameters\n        self.pop_size = max(6, int(pop_size))\n        self.knn = max(1, int(knn))\n        self.init_frac = float(init_frac)\n        self.edge_refresh = int(edge_refresh)\n\n        # sigma / adaptation\n        self.sigma0 = 0.4 * (self.ub - self.lb)\n        self.sigma = float(self.sigma0)\n        self.sigma_min = 1e-5\n        self.sigma_max = 5.0\n        self.success_window = int(max(5, success_window))\n        self.success_history = []  # recent successes (1/0) to adapt sigma\n\n        # archive and sizes\n        self.max_archive = max(400, 30 * self.dim)\n        # internal counters of edges success frequency to bias shortcuts\n        self.edge_success = {}\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb)\n        ub = np.full(dim, self.ub)\n        width = ub - lb\n\n        evals = 0\n\n        # initialize with uniform samples\n        n_init = int(min(budget, max(6, int(self.init_frac * budget), 4 * dim)))\n        X_archive = []\n        f_archive = []\n\n        for _ in range(n_init):\n            if evals >= budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            X_archive.append(np.array(x, copy=True))\n            f_archive.append(float(f))\n            evals += 1\n\n        if len(X_archive) == 0:\n            # fallback: sample one point\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_archive.append(x0.copy())\n            f_archive.append(float(f0))\n            evals = 1\n\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # helper: build compact population (best)\n        def build_population(Xa, fa):\n            order = np.argsort(fa)\n            k = min(self.pop_size, len(order))\n            inds = order[:k]\n            return Xa[inds].copy(), fa[inds].copy()\n\n        pop_X, pop_f = build_population(X_archive, f_archive)\n        best_idx = int(np.argmin(pop_f))\n        x_best = pop_X[best_idx].copy()\n        f_best = float(pop_f[best_idx])\n\n        # graph edges: list of (i,j), weights (probabilities)\n        edges = []\n        edge_weights = np.array([])\n\n        def refresh_graph():\n            nonlocal edges, edge_weights, pop_X, pop_f, edge_success\n            n = pop_X.shape[0]\n            edges = []\n            if n < 2:\n                edge_weights = np.array([])\n                edge_success = {}\n                return\n            # distance matrix\n            D = np.linalg.norm(pop_X[:, None, :] - pop_X[None, :, :], axis=2) + 1e-12\n            kn = min(self.knn, n - 1)\n            pairs = set()\n            for i in range(n):\n                neigh = np.argsort(D[i])[1:1 + kn]\n                for j in neigh:\n                    a, b = (i, j) if i < j else (j, i)\n                    pairs.add((a, b))\n            edges = sorted(list(pairs))\n            if len(edges) == 0:\n                edge_weights = np.array([])\n                edge_success = {}\n                return\n            # compute edge weights favoring short edges and good endpoints\n            fmin, fmax = np.min(pop_f), np.max(pop_f)\n            rng_scale = max(fmax - fmin, 1e-9)\n            w = []\n            for (i, j) in edges:\n                # endpoints quality term (prefer edges that touch good nodes)\n                qual = ( (fmin - min(pop_f[i], pop_f[j])) / (rng_scale + 1e-12) )\n                # length term (prefer shorter edges, but allow medium length)\n                length = D[i, j]\n                length_term = np.exp(-length / (0.5 * np.mean(width) + 1e-12))\n                # historical success boosting (edge shortcuts that helped previously)\n                key = (i, j)\n                succ = edge_success.get(key, 0.0)\n                boost = 1.0 + 0.9 * succ\n                # combined\n                score = (1.0 + 0.6 * qual) * length_term * boost + 1e-6\n                w.append(score)\n            edge_weights = np.array(w, dtype=float)\n            edge_weights = edge_weights / np.sum(edge_weights)\n            # normalize success counts to prevent blowup\n            # keep keys only for existing edges\n            edge_success = {e: edge_success.get(e, 0.0) for e in edges}\n\n        refresh_graph()\n\n        iter_since_improve = 0\n        total_iters = 0\n\n        # helper to choose edge index by weights\n        def choose_edge_idx():\n            if len(edges) == 0:\n                return None\n            probs = edge_weights.copy()\n            probs = np.clip(probs, 1e-12, None)\n            probs = probs / probs.sum()\n            return self.rng.choice(len(edges), p=probs)\n\n        # helper: perform a short random walk on the edge graph starting from a node\n        # returns endpoint node index and list of traversed edge indices\n        def random_walk(start_idx, max_hops=3):\n            if len(edges) == 0:\n                return start_idx, []\n            current = start_idx\n            path_edges = []\n            for _ in range(max_hops):\n                # list neighbors of current\n                neighs = []\n                eidxs = []\n                for idx, (a, b) in enumerate(edges):\n                    if a == current:\n                        neighs.append(b)\n                        eidxs.append(idx)\n                    elif b == current:\n                        neighs.append(a)\n                        eidxs.append(idx)\n                if len(neighs) == 0:\n                    break\n                # choose neighbor with probability proportional to edge weight\n                w = edge_weights[eidxs].copy()\n                w = np.clip(w, 1e-12, None)\n                w = w / w.sum()\n                pick = self.rng.choice(len(neighs), p=w)\n                current = neighs[pick]\n                path_edges.append(eidxs[pick])\n                # occasionally stop early to encourage diversity\n                if self.rng.random() < 0.3:\n                    break\n            return current, path_edges\n\n        # helper: local linear surrogate descent around a node idx\n        def surrogate_descent(node_idx):\n            # gather neighbors (k nearest by Euclidean in pop_X)\n            if pop_X.shape[0] < 3:\n                return None\n            X0 = pop_X[node_idx]\n            f0 = pop_f[node_idx]\n            # distances to others\n            Dn = np.linalg.norm(pop_X - X0[None, :], axis=1)\n            order = np.argsort(Dn)\n            # take up to m neighbors excluding self\n            m = min(max(3, dim), pop_X.shape[0] - 1)\n            neigh_inds = [int(i) for i in order[1:1 + m]]\n            A = pop_X[neigh_inds] - X0[None, :]\n            b = pop_f[neigh_inds] - f0\n            # if A has very small variance, bail out\n            if np.all(np.linalg.norm(A, axis=1) < 1e-12):\n                return None\n            # solve least-squares A g = b for gradient g\n            try:\n                g, *_ = np.linalg.lstsq(A, b, rcond=None)\n                if np.linalg.norm(g) < 1e-12:\n                    return None\n                # normalize gradient and choose step length adaptive to sigma and local scale\n                gnorm = g / (np.linalg.norm(g) + 1e-12)\n                # step size proportional to current sigma and neighbor spread\n                local_spread = np.mean(np.linalg.norm(A, axis=1))\n                step = 1.2 * self.sigma * (0.8 + 0.8 * self.rng.random()) * (1.0 + local_spread / (np.mean(width) + 1e-12))\n                x_new = X0 - step * gnorm\n                return x_new\n            except Exception:\n                return None\n\n        # main loop\n        while evals < budget:\n            total_iters += 1\n            frac = evals / max(1.0, budget)\n            # strategy mix: maintain edge sampling but add shortcut and surrogate\n            p_shortcut = 0.30 + 0.25 * (1 - frac)   # shortcut multi-hop recombination\n            p_surrogate = 0.18 + 0.20 * frac        # surrogate descent increases later\n            p_edge = 0.25 * (1 - frac) + 0.12      # single-edge interpolation\n            p_local = 0.15 + 0.25 * frac\n            p_global = 1.0 - (p_shortcut + p_surrogate + p_edge + p_local)\n            ps = np.array([p_shortcut, p_surrogate, p_edge, p_local, p_global])\n            ps = np.clip(ps, 1e-8, 1.0)\n            ps = ps / ps.sum()\n            p_shortcut, p_surrogate, p_edge, p_local, p_global = ps\n\n            r = self.rng.random()\n            x_candidate = None\n\n            # Strategy: Shortcut recombination (multi-hop teleport between elites)\n            if r < p_shortcut and pop_X.shape[0] >= 3 and len(edges) > 0:\n                # pick a start node biased towards good fitness\n                pf = pop_f.copy()\n                fmin, fmax = pf.min(), pf.max()\n                scale = max(fmax - fmin, 1e-9)\n                w_nodes = np.exp(-(pf - fmin) / scale)\n                w_nodes /= w_nodes.sum()\n                start = int(self.rng.choice(pop_X.shape[0], p=w_nodes))\n                # do a short random walk to get endpoint\n                endpoint, path_eidxs = random_walk(start, max_hops=1 + int(self.rng.random() * 3))\n                # choose how to combine start and endpoint: convex combination biased toward better of two\n                xs = pop_X[start]\n                xe = pop_X[endpoint]\n                # prefer midpoint but shift toward the better endpoint\n                if pop_f[start] < pop_f[endpoint]:\n                    bias = 0.6\n                else:\n                    bias = 0.4\n                alpha = np.clip(self.rng.normal(loc=bias, scale=0.18), -0.2, 1.2)\n                base = alpha * xs + (1 - alpha) * xe\n                # add orthogonal small noise scaled by sigma and relative distance\n                dir_vec = xe - xs\n                dist = np.linalg.norm(dir_vec) + 1e-12\n                u = dir_vec / dist if dist > 1e-12 else np.zeros(dim)\n                noise = self.rng.normal(size=dim)\n                proj = np.dot(noise, u) * u\n                orth = noise - proj\n                orth_norm = np.linalg.norm(orth)\n                if orth_norm > 1e-12:\n                    orth = orth / orth_norm\n                else:\n                    orth = self.rng.normal(size=dim)\n                    orth = orth / (np.linalg.norm(orth) + 1e-12)\n                orth_mag = self.sigma * (0.4 + 1.2 * self.rng.random()) * (min(dist / (np.mean(width) + 1e-12), 1.5))\n                along = (alpha - 0.5) * dist * self.rng.normal(loc=1.0, scale=0.35)\n                x_candidate = base + along * (u if np.linalg.norm(u) > 1e-12 else 0.0) + orth_mag * orth\n                # record used edges for potential boosting upon success\n                used_path = path_eidxs\n\n            # Strategy: surrogate local linear descent\n            elif r < p_shortcut + p_surrogate and pop_X.shape[0] >= 4:\n                # pick candidate node: prefer best and some random elites\n                pick_prob = 0.7\n                if self.rng.random() < pick_prob:\n                    node = int(np.argmin(pop_f))\n                else:\n                    # choose a node weighted by fitness\n                    pf = pop_f.copy()\n                    fmin, fmax = pf.min(), pf.max()\n                    scale = max(fmax - fmin, 1e-9)\n                    w_nodes = np.exp(-(pf - fmin) / scale)\n                    w_nodes /= w_nodes.sum()\n                    node = int(self.rng.choice(pop_X.shape[0], p=w_nodes))\n                x_s = surrogate_descent(node)\n                if x_s is None:\n                    # fallback to small local gaussian around best\n                    x_candidate = x_best + self.sigma * 0.8 * self.rng.normal(size=dim)\n                    used_path = []\n                else:\n                    # add a little jitter\n                    x_candidate = x_s + 0.2 * self.sigma * self.rng.normal(size=dim)\n                    used_path = []  # surrogate doesn't map to edges\n\n            # Strategy: single-edge interpolation (as in baseline but polished)\n            elif r < p_shortcut + p_surrogate + p_edge and len(edges) > 0:\n                eidx = choose_edge_idx()\n                if eidx is None:\n                    x_candidate = self.rng.uniform(lb, ub)\n                    used_path = []\n                else:\n                    i, j = edges[eidx]\n                    xi = pop_X[i]\n                    xj = pop_X[j]\n                    dir_vec = xj - xi\n                    dist = np.linalg.norm(dir_vec) + 1e-12\n                    u = dir_vec / dist if dist > 1e-12 else np.zeros(dim)\n                    # sample alpha with beta to favor interior but allow slight extrapolation\n                    alpha = np.clip(self.rng.beta(2.2, 2.2) + self.rng.uniform(-0.15, 0.15), -0.25, 1.25)\n                    base = alpha * xi + (1 - alpha) * xj\n                    # orthogonal noise with scale depending on sigma and edge length\n                    noise = self.rng.normal(size=dim)\n                    proj = np.dot(noise, u) * u\n                    orth = noise - proj\n                    orth_norm = np.linalg.norm(orth)\n                    if orth_norm > 1e-12:\n                        orth = orth / orth_norm\n                    else:\n                        orth = self.rng.normal(size=dim)\n                        orth = orth / (np.linalg.norm(orth) + 1e-12)\n                    orth_mag = self.sigma * (0.5 + 1.0 * self.rng.random()) * (min(dist / (np.mean(width) + 1e-12), 1.4))\n                    along_mag = (alpha - 0.5) * dist * self.rng.normal(loc=1.0, scale=0.35)\n                    x_candidate = base + along_mag * u + orth_mag * orth\n                    used_path = [eidx]\n\n            # Strategy: local gaussian around best\n            elif r < p_shortcut + p_surrogate + p_edge + p_local:\n                local_sigma = self.sigma * (0.4 + 0.9 * self.rng.random())\n                if pop_X.shape[0] >= max(4, dim // 2):\n                    stds = np.std(pop_X, axis=0)\n                    stds = np.maximum(stds, 0.02 * np.mean(width))\n                    anis = stds / np.mean(stds)\n                    x_candidate = x_best + local_sigma * (self.rng.normal(size=dim) * anis)\n                else:\n                    x_candidate = x_best + local_sigma * self.rng.normal(size=dim)\n                used_path = []\n\n            # Strategy: global exploration or opposite sampling\n            else:\n                if self.rng.random() < 0.65 and pop_X.shape[0] > 0:\n                    idx = int(self.rng.integers(0, pop_X.shape[0]))\n                    x0 = pop_X[idx]\n                    opp = lb + ub - x0\n                    x_candidate = opp + 0.25 * self.sigma * self.rng.normal(size=dim)\n                else:\n                    x_candidate = self.rng.uniform(lb, ub)\n                used_path = []\n\n            # Clip to bounds\n            x_candidate = np.minimum(np.maximum(x_candidate, lb), ub)\n\n            # Evaluate candidate if budget allows\n            if evals >= budget:\n                break\n            f_candidate = float(func(x_candidate))\n            evals += 1\n\n            # update archive\n            X_archive = np.vstack([X_archive, x_candidate])\n            f_archive = np.concatenate([f_archive, [f_candidate]])\n\n            # determine success\n            improved = False\n            if f_candidate < f_best:\n                improved = True\n                f_best = float(f_candidate)\n                x_best = x_candidate.copy()\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # update success history and sigma (bandit-like)\n            self.success_history.append(1 if improved else 0)\n            if len(self.success_history) > self.success_window:\n                self.success_history.pop(0)\n            # compute empirical success rate\n            succ_rate = (sum(self.success_history) + 1e-12) / max(1, len(self.success_history))\n            # adjust sigma: if success low and not improving, increase exploration; else shrink\n            if succ_rate < 0.15:\n                self.sigma = min(self.sigma_max, self.sigma * (1.12 + 0.18 * (0.15 - succ_rate)))\n            else:\n                self.sigma = max(self.sigma_min, self.sigma * (0.90 - 0.2 * (succ_rate - 0.15)))\n            # clamp sigma\n            self.sigma = float(np.clip(self.sigma, self.sigma_min, self.sigma_max))\n\n            # if candidate replaced a member in population, update pop\n            if pop_X.shape[0] < self.pop_size:\n                pop_X = np.vstack([pop_X, x_candidate])\n                pop_f = np.concatenate([pop_f, [f_candidate]])\n            else:\n                # replace worst if candidate better\n                worst_idx = int(np.argmax(pop_f))\n                if f_candidate < pop_f[worst_idx]:\n                    pop_X[worst_idx] = x_candidate\n                    pop_f[worst_idx] = f_candidate\n                else:\n                    # occasional random replacement for exploration\n                    if self.rng.random() < 0.02:\n                        ridx = int(self.rng.integers(0, pop_X.shape[0]))\n                        pop_X[ridx] = x_candidate\n                        pop_f[ridx] = f_candidate\n\n            # when a shortcut recombination used some path_edges and improved, boost those edges\n            if 'used_path' in locals() and len(used_path) > 0:\n                if improved:\n                    for eidx in used_path:\n                        e = edges[eidx]\n                        # increment success tracker (bounded)\n                        self.edge_success[e] = min(self.edge_success.get(e, 0.0) + 1.0, 7.0)\n                else:\n                    for eidx in used_path:\n                        e = edges[eidx]\n                        # slight decay on failure\n                        self.edge_success[e] = max(self.edge_success.get(e, 0.0) - 0.03, 0.0)\n\n            # bound archive size\n            if X_archive.shape[0] > self.max_archive:\n                keep_best = int(self.max_archive * 0.6)\n                best_inds = np.argsort(f_archive)[:keep_best]\n                other_pool = np.setdiff1d(np.arange(X_archive.shape[0]), best_inds)\n                other_keep = max(0, self.max_archive - keep_best)\n                if other_pool.size > 0 and other_keep > 0:\n                    other_inds = self.rng.choice(other_pool, size=other_keep, replace=False)\n                    keep = np.concatenate([best_inds, other_inds])\n                else:\n                    keep = best_inds\n                keep = np.unique(keep)\n                X_archive = X_archive[keep]\n                f_archive = f_archive[keep]\n\n            # periodic population rebuild and graph refresh\n            if (evals % self.edge_refresh) == 0:\n                pop_X, pop_f = build_population(X_archive, f_archive)\n                refresh_graph()\n\n            # emergency diversification if stuck\n            if iter_since_improve > max(60, 4 * dim) and self.rng.random() < 0.25:\n                # do one strong global injection (but ensure budget)\n                if evals < budget:\n                    xg = self.rng.uniform(lb, ub)\n                    fg = float(func(xg))\n                    evals += 1\n                    X_archive = np.vstack([X_archive, xg])\n                    f_archive = np.concatenate([f_archive, [fg]])\n                    # possibly replace worst and update best\n                    if pop_X.shape[0] > 0 and fg < pop_f.max():\n                        worst_idx = int(np.argmax(pop_f))\n                        pop_X[worst_idx] = xg\n                        pop_f[worst_idx] = fg\n                    if fg < f_best:\n                        f_best = float(fg)\n                        x_best = xg.copy()\n                        iter_since_improve = 0\n                    # after strong injection, slightly increase sigma to explore\n                    self.sigma = min(self.sigma_max, self.sigma * (1.6 + 0.6 * self.rng.random()))\n                    # refresh graph now if we can\n                    if evals < budget:\n                        pop_X, pop_f = build_population(X_archive, f_archive)\n                        refresh_graph()\n\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 4, "feedback": "In the code, line 228, in evaluate, the following error occurred:\nSyntaxError: no binding for nonlocal 'edge_success' found\nOn line: p_edge = 0.25 * (1 - frac) + 0.12      # single-edge interpolation", "error": "In the code, line 228, in evaluate, the following error occurred:\nSyntaxError: no binding for nonlocal 'edge_success' found\nOn line: p_edge = 0.25 * (1 - frac) + 0.12      # single-edge interpolation", "parent_ids": ["701bfc7e-c1fd-40fc-8533-beb7ddd599a9"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4075.0, "Edges": 4074.0, "Max Degree": 34.0, "Min Degree": 1.0, "Mean Degree": 1.9995092024539878, "Degree Variance": 2.2164414769091803, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.744886975242196, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3202459760349523, "Depth Entropy": 2.2402618899991618, "Assortativity": 1.2739470759705033e-08, "Average Eccentricity": 19.58601226993865, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.000245398773006135, "Average Shortest Path": 11.022049868837549, "mean_complexity": 10.857142857142858, "total_complexity": 76.0, "mean_token_count": 520.1428571428571, "total_token_count": 3641.0, "mean_parameter_count": 2.2857142857142856, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "mean depth"}
{"id": "3312bdb3-08e0-43c3-bb16-8cd9f982bb1a", "fitness": 0.2499582484507627, "name": "RapidMultiScaleLocalModeling", "description": "Rapid Multi-Scale Local Modeling (RMSLM) \u2014 fast shallow search using a Latin-Hypercube global cover, compact diagonal-quadratic local surrogates around elites to propose direct minimizers, and quick multi-scale coordinate probes with aggressive early sampling to reduce solution depth.", "code": "import numpy as np\n\nclass RapidMultiScaleLocalModeling:\n    \"\"\"\n    Rapid Multi-Scale Local Modeling (RMSLM)\n\n    Main ideas:\n      - Spend an early portion of the budget on a Latin-Hypercube (LHS) global cover to reduce mean search depth.\n      - Maintain a small elite set. For each elite, fit a compact diagonal-quadratic surrogate\n        (1 + d linear + d quadratic diag terms) using nearby samples and analytically propose a local minimizer.\n      - Perform quick coordinate-wise probes around elites at multiple scales (radius), expanding on success and shrinking on failure.\n      - Periodically re-evaluate elite composition from the sample archive; perform small randomized re-seeding when stagnation occurs.\n      - All function evaluations strictly accounted for and bounded by budget.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, rng=None,\n                 init_scale=0.5, min_radius=1e-6, max_radius=None,\n                 initial_frac=0.30, elite_size=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          rng: numpy.random.Generator or None\n          init_scale: initial radius fraction of (ub-lb) range (default 0.5 -> fairly large)\n          min_radius: minimum radius for local probes\n          max_radius: maximum radius (if None, set to full range)\n          initial_frac: fraction of budget used for initial LHS coverage (default 0.30)\n          elite_size: how many elites to keep (default: min(8, max(2, dim)))\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_scale = float(init_scale)\n        self.min_radius = float(min_radius)\n        self.initial_frac = float(initial_frac)\n        if elite_size is None:\n            self.elite_size = min(8, max(2, self.dim))\n        else:\n            self.elite_size = int(elite_size)\n        self.max_radius = max_radius  # will be set relative to bounds in __call__\n\n    def _lhs(self, n, lb, ub):\n        # Simple Latin Hypercube Sampling in [0,1]^d then scale to bounds\n        n = int(n)\n        d = self.dim\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.random((n, d))\n        points = np.zeros((n, d))\n        for j in range(d):\n            perm = self.rng.permutation(n)\n            points[:, j] = (cut[:-1] + (perm + u[:, j]) / n)  # scrambled strata\n        # scale to bounds\n        return lb + points * (ub - lb)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize black-box function func(x) where x is shape (dim,).\n        Returns (f_best, x_best).\n        \"\"\"\n        # bounds\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        lb_arr = lb_arr.astype(float)\n        ub_arr = ub_arr.astype(float)\n\n        if self.max_radius is None:\n            max_radius = np.max(ub_arr - lb_arr)\n        else:\n            max_radius = float(self.max_radius)\n\n        # bookkeeping\n        evals = 0\n        budget = self.budget\n\n        # initial Latin Hypercube samples: allocate initial_frac of budget (but at least some)\n        n_init = int(np.clip(np.round(budget * self.initial_frac), a_min=min( max(20, 5*self.dim), budget-1 ), a_max=budget-1))\n        # ensure at least 10 samples\n        n_init = max(10, min(n_init, budget-1))\n        X = self._lhs(n_init, lb_arr, ub_arr)\n        f_vals = np.zeros(n_init, dtype=float)\n        for i in range(n_init):\n            f_vals[i] = float(func(X[i]))\n            evals += 1\n            if evals >= budget:\n                break\n\n        # set best\n        best_idx = np.argmin(f_vals)\n        f_best = float(f_vals[best_idx])\n        x_best = X[best_idx].copy()\n\n        # dynamic radius (absolute scale)\n        domain_scale = ub_arr - lb_arr\n        radius = self.init_scale * np.linalg.norm(domain_scale) / np.sqrt(self.dim)  # a multi-dim scale\n        radius = float(np.clip(radius, self.min_radius, max_radius))\n\n        stagnation_cycles = 0\n        no_improve_cycles_thresh = 6  # after this, do re-seed\n        # main iterative local-modeling cycles\n        # Convert X/f_vals to lists for easy append\n        X_list = [x.copy() for x in X]\n        f_list = [float(v) for v in f_vals]\n\n        # helper to add sample\n        def add_sample(x, fx):\n            nonlocal X_list, f_list, f_best, x_best\n            X_list.append(x.copy())\n            f_list.append(float(fx))\n            if fx < f_best:\n                f_best = float(fx)\n                x_best = x.copy()\n\n        # function to get nearest neighbors indices in archive to a center\n        def knn_indices(center, k):\n            if len(X_list) == 0:\n                return []\n            arr = np.vstack(X_list)\n            dif = arr - center.reshape(1, -1)\n            dists = np.sum(dif * dif, axis=1)\n            idx = np.argsort(dists)[:k]\n            return idx\n\n        # compute elites\n        def current_elites():\n            arr_f = np.array(f_list)\n            idx = np.argsort(arr_f)[:self.elite_size]\n            return idx\n\n        # primary loop: iterate while budget remains; each \"cycle\" touches elites once\n        cycle = 0\n        while evals < budget:\n            cycle += 1\n            improved_in_cycle = False\n\n            elites_idx = current_elites()\n            # iterate through elites (random order to avoid bias)\n            self.rng.shuffle(elites_idx)\n\n            for eidx in elites_idx:\n                if evals >= budget:\n                    break\n                center = np.array(X_list[eidx])\n                center_f = float(f_list[eidx])\n\n                # --- Build compact diagonal-quadratic surrogate around center ---\n                # choose up to k neighbors (including center)\n                k_nn = min(max(6, 4 + 2*self.dim), len(X_list))\n                nn = knn_indices(center, k_nn)\n                # assemble regression matrix: columns [1, x_rel (d), x_rel^2 (d)]\n                X_nn = np.vstack([X_list[i] for i in nn])\n                y_nn = np.array([f_list[i] for i in nn])\n                x_rel = X_nn - center.reshape(1, -1)  # relative coords\n                # small regularization to stabilize\n                # design matrix\n                ones = np.ones((x_rel.shape[0], 1))\n                lin = x_rel  # (n_samples, d)\n                quad = x_rel * x_rel  # diag quadratic terms\n                A = np.hstack([ones, lin, quad])  # shape (n_samples, 1+d+d)\n                # solve least squares with small ridge\n                try:\n                    # regularization lambda scaled by variance\n                    lam = 1e-8 + 1e-6 * np.var(y_nn) if y_nn.size > 1 else 1e-8\n                    # solve (A^T A + lam I) p = A^T y\n                    ATA = A.T @ A\n                    ATA_reg = ATA + lam * np.eye(ATA.shape[0])\n                    rhs = A.T @ y_nn\n                    p = np.linalg.solve(ATA_reg, rhs)\n                    # extract parameters\n                    a0 = p[0]\n                    b = p[1:1+self.dim]\n                    q = p[1+self.dim:1+2*self.dim]\n                    # compute analytic minimizer of surrogate in relative coordinates: grad = b + 2*q * x = 0 => x = -b/(2*q)\n                    # handle small q by clamping\n                    q_safe = np.where(np.abs(q) < 1e-12, np.sign(q)*1e-12 + 1e-12, q)\n                    x_rel_pred = -0.5 * (b / q_safe)\n                    # limit shift magnitude to radius in Euclidean norm and also per-dimension clamp relative to domain\n                    shift_norm = np.linalg.norm(x_rel_pred)\n                    if shift_norm > 0:\n                        max_shift = radius\n                        if shift_norm > max_shift:\n                            x_rel_pred = x_rel_pred * (max_shift / shift_norm)\n                    x_model = center + x_rel_pred\n                    # ensure bounds\n                    x_model = np.minimum(np.maximum(x_model, lb_arr), ub_arr)\n                    # evaluate model suggestion\n                    if evals < budget:\n                        f_model = float(func(x_model))\n                        evals += 1\n                        add_sample(x_model, f_model)\n                        if f_model < center_f:\n                            improved_in_cycle = True\n                            # on success, increase radius moderately\n                            radius = min(max_radius, radius * 1.25)\n                except np.linalg.LinAlgError:\n                    # fallback to skip model proposal\n                    pass\n\n                if evals >= budget:\n                    break\n\n                # --- Quick multi-scale coordinate probes (very cheap) ---\n                # choose a small set of probe dims (prefer dims with largest domain variance seen in neighbors)\n                # compute per-dim std from neighbor x_rel to pick active dims\n                try:\n                    per_dim_std = np.std(x_rel, axis=0)\n                    probe_dims = np.argsort(-per_dim_std)[:min(self.dim, max(2, int(np.sqrt(self.dim)*2)))]\n                except Exception:\n                    probe_dims = self.rng.choice(self.dim, size=min(self.dim, max(2, int(np.sqrt(self.dim)*2))), replace=False)\n\n                # do probes at 2 scales: radius and radius*0.3\n                scales = [1.0, 0.3]\n                for s in scales:\n                    step_scale = s * radius\n                    for j in probe_dims:\n                        if evals >= budget:\n                            break\n                        # plus probe\n                        x_p = center.copy()\n                        step = step_scale * (domain_scale[j] / np.linalg.norm(domain_scale) if np.linalg.norm(domain_scale) > 0 else 1.0)\n                        x_p[j] = np.clip(x_p[j] + step, lb_arr[j], ub_arr[j])\n                        f_p = float(func(x_p))\n                        evals += 1\n                        add_sample(x_p, f_p)\n                        if f_p < center_f:\n                            improved_in_cycle = True\n                            center_f = f_p\n                            center = x_p.copy()\n                            # small radius expansion on success\n                            radius = min(max_radius, radius * 1.20)\n                            # early accept: update the corresponding sample (but we just appended)\n                        if evals >= budget:\n                            break\n                        # minus probe\n                        x_m = center.copy()\n                        x_m[j] = np.clip(x_m[j] - step, lb_arr[j], ub_arr[j])\n                        f_m = float(func(x_m))\n                        evals += 1\n                        add_sample(x_m, f_m)\n                        if f_m < center_f:\n                            improved_in_cycle = True\n                            center_f = f_m\n                            center = x_m.copy()\n                            radius = min(max_radius, radius * 1.20)\n                        if evals >= budget:\n                            break\n                    if evals >= budget:\n                        break\n\n                # small opposition check per elite with low probability when stagnating\n                if evals < budget and stagnation_cycles > 1 and self.rng.random() < 0.15:\n                    x_opp = lb_arr + ub_arr - center\n                    x_opp = np.minimum(np.maximum(x_opp, lb_arr), ub_arr)\n                    f_opp = float(func(x_opp))\n                    evals += 1\n                    add_sample(x_opp, f_opp)\n                    if f_opp < center_f:\n                        improved_in_cycle = True\n                        radius = min(max_radius, radius * 1.3)\n\n            # end for each elite\n\n            # update global best from sample archive (already maintained in add_sample)\n            # manage radius adjustments if no improvement in cycle\n            if improved_in_cycle:\n                stagnation_cycles = 0\n            else:\n                stagnation_cycles += 1\n                # shrink radius on no improvement to focus search\n                radius = max(self.min_radius, radius * 0.6)\n\n            # occasional re-seed: if long stagnation, do a small LHS re-injection around best or global LHS of small size\n            if stagnation_cycles >= no_improve_cycles_thresh and evals < budget:\n                # small re-seeding size\n                n_reseed = min( max(6, self.dim*2), budget - evals )\n                # sample in a box around current best (multiplier)\n                spread = np.maximum(domain_scale * 0.15, 0.02*(ub_arr-lb_arr))\n                # create random points around best\n                for _ in range(n_reseed):\n                    if evals >= budget:\n                        break\n                    perturb = self.rng.normal(0.0, 1.0, size=self.dim) * spread\n                    x_r = np.minimum(np.maximum(x_best + perturb, lb_arr), ub_arr)\n                    f_r = float(func(x_r))\n                    evals += 1\n                    add_sample(x_r, f_r)\n                    if f_r < f_best:\n                        stagnation_cycles = 0\n                        radius = min(max_radius, radius * 1.5)\n                        break\n                # if still stagnating, try a tiny global LHS of very few points\n                if stagnation_cycles >= no_improve_cycles_thresh and evals < budget:\n                    small_lhs = min(8, budget - evals)\n                    Xsmall = self._lhs(small_lhs, lb_arr, ub_arr)\n                    for i in range(small_lhs):\n                        if evals >= budget:\n                            break\n                        fx = float(func(Xsmall[i]))\n                        evals += 1\n                        add_sample(Xsmall[i], fx)\n                        if fx < f_best:\n                            stagnation_cycles = 0\n                            radius = min(max_radius, self.init_scale * np.linalg.norm(domain_scale))\n                            break\n                    # after reseed, reduce stagnation counter a bit to allow cycles to act\n                    stagnation_cycles = 0\n\n            # safe-guard: if radius becomes extremely tiny, re-expand moderately\n            if radius < self.min_radius * 10:\n                radius = max(self.min_radius, self.init_scale * np.linalg.norm(domain_scale) * 0.1)\n\n            # limit radius to domain\n            radius = min(radius, max_radius)\n\n            # break if no budget left (loop top already checks)\n        # end while\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 4, "feedback": "The algorithm RapidMultiScaleLocalModeling scored 0.250 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5547797502199159}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.5891791834778805}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.43297787199978144}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.0}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0466487541396744}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03910747407046056}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.032273986315939696}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0262293263142912}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.017765813079436366}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.027024879397564483}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.6896976373197299}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.6834233252593184}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.6101657251674484}], "aucs": [0.5547797502199159, 0.5891791834778805, 0.43297787199978144, 0.0, 4.999999999999449e-05, 4.999999999999449e-05, 0.0466487541396744, 0.03910747407046056, 0.032273986315939696, 0.0262293263142912, 0.017765813079436366, 0.027024879397564483, 0.6896976373197299, 0.6834233252593184, 0.6101657251674484]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2317.0, "Edges": 2316.0, "Max Degree": 36.0, "Min Degree": 1.0, "Mean Degree": 1.9991368148467847, "Degree Variance": 2.2641339117952928, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.360498561840844, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3287873101216356, "Depth Entropy": 2.1928172161477506, "Assortativity": 0.0, "Average Eccentricity": 18.772119119551142, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00043159257660768235, "Average Shortest Path": 10.367040042697104, "mean_complexity": 9.666666666666666, "total_complexity": 58.0, "mean_token_count": 332.3333333333333, "total_token_count": 1994.0, "mean_parameter_count": 3.1666666666666665, "total_parameter_count": 19.0}, "archive_direction": "decrease", "archive_feature": "mean depth"}
{"id": "837b02f8-241c-429e-a4aa-a4ddbc7b8e89", "fitness": 0.13296491806370297, "name": "BroadScoutShallowBasinHopper", "description": "BroadScout Shallow Basin Hopper \u2014 bandit-driven many shallow radial \"arms\" (centers) that probe promising regions broadly and only take short local dives, prioritizing breadth (many shallow searches) over deep exploitation to reduce mean solution depth.", "code": "import numpy as np\n\nclass BroadScoutShallowBasinHopper:\n    \"\"\"\n    BroadScout Shallow Basin Hopper (BSSBH)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional args:\n      arms: number of shallow \"search arms\" (centers)\n      init_frac: fraction of range used to initialize radial probing radius\n      radius_decay: factor to shrink radius after failures\n      radius_grow: factor to grow radius after successes\n      max_depth: max shallow probes per arm selection (keeps searches shallow)\n      global_prob: probability of taking a uniform-global probe instead of radial\n      seed: rng seed\n    Strategy:\n      - Create many diverse arms (centers) via random sampling.\n      - Use an UCB-like bandit to select which arm to probe next.\n      - For the selected arm perform a small number (max_depth) of radial probes:\n        sample a direction, probe at current radius, accept improvements, adapt radius.\n      - Replace stagnated arms with fresh samples (diversification).\n      - Final lightweight coordinate refinement around best if budget remains.\n    \"\"\"\n    def __init__(self, budget, dim, arms=None, init_frac=0.28,\n                 radius_decay=0.7, radius_grow=1.25, max_depth=3,\n                 global_prob=0.06, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_frac = float(init_frac)\n        self.radius_decay = float(radius_decay)\n        self.radius_grow = float(radius_grow)\n        self.max_depth = int(max_depth)\n        self.global_prob = float(global_prob)\n\n        # determine number of arms: keep many arms but bounded by budget\n        if arms is None:\n            # scale with dimension but limited by budget to ensure at least some probes per arm\n            approx = max(8, min(60, 4 * self.dim))\n            # ensure that we can at least evaluate each arm once and do a few probes: approx * 4 <= budget\n            approx = int(min(approx, max(4, self.budget // 4)))\n            self.arms = max(4, approx)\n        else:\n            self.arms = int(max(1, arms))\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        evals = 0\n\n        # problem scale\n        range_vec = ub - lb\n        range_norm = float(np.linalg.norm(range_vec))\n        # initial radial probe radius (fraction of full range)\n        radius_init = max(1e-8, self.init_frac * range_norm)\n\n        # initialize arms by sampling uniformly\n        n_arms = min(self.arms, max(1, self.budget // 3))  # ensure we don't create more arms than reasonable\n        arms_x = np.empty((n_arms, dim), dtype=float)\n        arms_f = np.full(n_arms, np.inf, dtype=float)\n        for i in range(n_arms):\n            if evals >= self.budget: break\n            x = rng.uniform(lb, ub)\n            f = float(func(x)); evals += 1\n            arms_x[i] = x\n            arms_f[i] = f\n\n        # If budget exhausted during init, return best\n        idx_best = int(np.argmin(arms_f))\n        x_best = arms_x[idx_best].copy()\n        f_best = float(arms_f[idx_best])\n\n        # per-arm statistics for bandit selection\n        pulls = np.ones(n_arms, dtype=int)  # start with 1 to avoid div-by-zero; we've evaluated each arm once\n        total_improve = np.zeros(n_arms, dtype=float)  # cumulative (positive) improvements\n        successes = np.zeros(n_arms, dtype=int)\n        radii = np.full(n_arms, radius_init, dtype=float)\n        stagn = np.zeros(n_arms, dtype=int)  # stagnation counter\n\n        total_pulls = int(np.sum(pulls))\n\n        # cap some values\n        max_radius = max(1e-8, 2.0 * range_norm)\n        min_radius = max(1e-12 * range_norm, 1e-12)\n\n        # helper: replace an arm with a fresh attempt near best or random\n        def replace_arm(idx, near_best_prob=0.75):\n            nonlocal evals, x_best, f_best\n            if evals >= self.budget:\n                return\n            if rng.random() < near_best_prob and evals + 1 < self.budget:\n                # sample near best to keep diversity\n                perturb = (rng.normal(size=dim) * 0.08) * range_vec\n                x_new = np.clip(x_best + perturb, lb, ub)\n            else:\n                x_new = rng.uniform(lb, ub)\n            f_new = float(func(x_new)); evals += 1\n            arms_x[idx] = x_new\n            arms_f[idx] = f_new\n            pulls[idx] = 1\n            total_improve[idx] = 0.0\n            successes[idx] = 0\n            radii[idx] = radius_init\n            stagn[idx] = 0\n            if f_new < f_best:\n                f_best = f_new; x_best = x_new.copy()\n\n        # main loop: bandit-driven shallow probing\n        while evals < self.budget:\n            total_pulls = int(np.sum(pulls))\n            # compute UCB-like score: arms with little pulls or high mean improvement are prioritized\n            mean_imp = total_improve / pulls\n            # encourage arms with very small pulls\n            exploration = np.sqrt(np.log(1 + total_pulls) / pulls)\n            # scale constants adapt to problem scale\n            c_explore = 0.9 * range_norm if range_norm > 0 else 1.0\n            ucb = mean_imp + c_explore * exploration\n\n            # prefer better arms slightly by boosting those near current best\n            dist_to_best = np.linalg.norm(arms_x - x_best[None, :], axis=1)\n            # smaller distance -> small boost to encourage local polishing, but not too much\n            ucb += 0.02 * (range_norm - dist_to_best)\n\n            # pick arm with maximum ucb\n            arm_idx = int(np.argmax(ucb))\n\n            # perform a shallow sequence of probes (keeps each arm's depth small)\n            probes = self.max_depth\n            for _ in range(probes):\n                if evals >= self.budget: break\n\n                pulls[arm_idx] += 1\n                total_pulls += 1\n\n                x_center = arms_x[arm_idx].copy()\n                f_center = arms_f[arm_idx]\n\n                # choose probe type: global random or radial\n                if rng.random() < self.global_prob:\n                    x_prop = rng.uniform(lb, ub)\n                else:\n                    # radial probe: random direction normalized\n                    dir_vec = rng.normal(size=dim)\n                    nd = np.linalg.norm(dir_vec)\n                    if nd == 0:\n                        dir_vec = np.ones(dim)\n                        nd = np.linalg.norm(dir_vec)\n                    dir_vec = dir_vec / nd\n                    # sample a fractional distance from center in [0.5, 1.5]*radius\n                    frac = rng.uniform(0.4, 1.6)\n                    step = frac * radii[arm_idx]\n                    # add small gaussian jitter proportional to radius\n                    jitter = 0.22 * radii[arm_idx] * rng.normal(size=dim)\n                    x_prop = x_center + step * dir_vec + jitter\n\n                x_prop = np.clip(x_prop, lb, ub)\n\n                # evaluate\n                f_prop = float(func(x_prop))\n                evals += 1\n\n                # improvement measure (positive if improvement)\n                improvement = max(0.0, f_center - f_prop)\n\n                if improvement > 0.0:\n                    # accept and move center shallowly\n                    arms_x[arm_idx] = x_prop\n                    arms_f[arm_idx] = f_prop\n                    total_improve[arm_idx] += improvement\n                    successes[arm_idx] += 1\n                    radii[arm_idx] = min(max_radius, radii[arm_idx] * self.radius_grow)\n                    stagn[arm_idx] = 0\n                    # update global best quickly\n                    if f_prop < f_best:\n                        f_best = f_prop\n                        x_best = x_prop.copy()\n                else:\n                    # failure: shrink radius for that arm\n                    radii[arm_idx] = max(min_radius, radii[arm_idx] * self.radius_decay)\n                    stagn[arm_idx] += 1\n                    # occasionally accept a slightly worse point to keep diversity (tiny probability)\n                    if rng.random() < 0.005:\n                        arms_x[arm_idx] = x_prop\n                        arms_f[arm_idx] = f_prop\n\n                # small maintenance: if arm stagnates for long, replace it\n                if stagn[arm_idx] > max(6, 2 * dim) and evals < self.budget:\n                    replace_arm(arm_idx)\n                    break\n\n                # if we've found a strong improvement, break early to let bandit reallocate\n                if improvement > 1e-8 * (1.0 + abs(f_best)):\n                    break\n\n            # occasional diversification: replace some worst-performing arms by sampling biased to best region\n            if rng.random() < 0.08 and evals < self.budget:\n                # find the worst arm by f and/or low success\n                worst_idx = int(np.argmax(arms_f + 0.2 * (1.0 / (1 + successes))))\n                replace_arm(worst_idx)\n\n            # if remaining budget is small, break to final refinement\n            if self.budget - evals < max(8, dim * 2):\n                break\n\n        # Final lightweight coordinate pattern search (shallow refinement)\n        mesh = 0.08 * range_norm\n        mesh_min = max(1e-9 * range_norm, 1e-8)\n        while evals < self.budget and mesh > mesh_min:\n            improved_any = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # try plus\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_any = True\n                    continue\n                # try minus\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_any = True\n            if not improved_any:\n                mesh *= 0.5\n\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm BroadScoutShallowBasinHopper scored 0.133 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5713cbd-1130-4e8d-abdc-3c3ca8966b71"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.2333368136084507}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.23471175148673384}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.24111599499666314}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.034542806232826795}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.05065806672202011}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.028507829583038213}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.029694184036614057}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.025863383144027696}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0269729609126933}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.21985877867442327}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.2526813848849524}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.6163798166731009}], "aucs": [0.2333368136084507, 0.23471175148673384, 0.24111599499666314, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.034542806232826795, 0.05065806672202011, 0.028507829583038213, 0.029694184036614057, 0.025863383144027696, 0.0269729609126933, 0.21985877867442327, 0.2526813848849524, 0.6163798166731009]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1786.0, "Edges": 1785.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9988801791713326, "Degree Variance": 2.296751265598176, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.401234567901234, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.315464252494821, "Depth Entropy": 1.9077808788109383, "Assortativity": 4.510972154533596e-09, "Average Eccentricity": 16.070548712206048, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0005599104143337066, "Average Shortest Path": 9.485462090771358, "mean_complexity": 9.75, "total_complexity": 39.0, "mean_token_count": 386.0, "total_token_count": 1544.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "mean depth"}
{"id": "dd6f5888-84ed-4a8f-b38d-b11bbe4c079e", "fitness": "-inf", "name": "EdgeDenseAdaptiveSubspaceNetwork", "description": "Edge-Dense Adaptive Subspace Network (Refined) \u2014 combine prioritized dense edge sampling with cheap quasi-linear gradient estimates and short backtracking line-searches in adaptive low-dimensional subspaces to accelerate improvements and reduce search \"depth\".", "code": "import numpy as np\n\nclass EdgeDenseAdaptiveSubspaceNetwork:\n    \"\"\"\n    Refined Edge-Dense Adaptive Subspace Network (EDASN v2)\n\n    Key refinements versus prior version:\n      - Prioritize edges not only by quality but by recent success counts (bandit-like),\n        so productive edges get explored earlier (reduces mean depth).\n      - Use compact, adaptive sampling per-edge (1-3 focused probes) rather than many random slots.\n      - Build cheap local linear models (least-squares gradient estimate) from elites and perform\n        a short backtracking line-search along the estimated negative gradient (few evals).\n      - Quick, low-cost extrapolations (momentum-like) from top elites.\n      - Maintain per-elite success counters and adaptive radii; allocate budget more conservatively.\n      - Strict budget enforcement.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, elite_size=8, init_samples=None,\n                 init_radius=1.0, min_radius=1e-6, max_radius=2.5,\n                 max_edges_per_iter=24, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_size = int(elite_size)\n        if init_samples is None:\n            self.init_samples = max(2, min(24, int(self.budget // 12)))\n        else:\n            self.init_samples = int(init_samples)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_edges_per_iter = int(max_edges_per_iter)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _orthonormal_basis(self, primary_vec, k):\n        v = np.asarray(primary_vec, dtype=float)\n        norm = np.linalg.norm(v)\n        if norm < 1e-12:\n            A = self.rng.normal(size=(self.dim, k))\n            Q, _ = np.linalg.qr(A)\n            return Q[:, :k]\n        A = self.rng.normal(size=(self.dim, k))\n        A[:, 0] = v / norm\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n\n        # seed initial sampling: a small spread to populate elites\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        xs = []\n        fs = []\n        radii = []\n        succ = []  # per-elite recent success counts (for prioritization)\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            xs.append(x)\n            fs.append(f)\n            radii.append(self.init_radius)\n            succ.append(0)\n\n        if len(xs) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            xs.append(x)\n            fs.append(f)\n            radii.append(self.init_radius)\n            succ.append(0)\n\n        # Pack elites as arrays, keep sorted by f\n        def trim_and_sort(xs, fs, radii, succ):\n            idx = np.argsort(fs)\n            xs = [xs[i] for i in idx][:self.elite_size]\n            fs = [fs[i] for i in idx][:self.elite_size]\n            radii = [radii[i] for i in idx][:self.elite_size]\n            succ = [succ[i] for i in idx][:self.elite_size]\n            return xs, fs, radii, succ\n\n        xs, fs, radii, succ = trim_and_sort(xs, fs, radii, succ)\n\n        iters = 0\n        no_improve_iters = 0\n\n        # Helper to evaluate candidate with budget check\n        def try_eval(xcand):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            xc = np.minimum(np.maximum(xcand, lb), ub)\n            fv = float(func(xc))\n            evals += 1\n            return xc, fv\n\n        # small utility: gradient estimate from top few elites via least squares:\n        def estimate_grad(x0, f0, neighbors_x, neighbors_f):\n            # neighbors: list of arrays\n            if len(neighbors_x) < 2:\n                return None\n            # build differences\n            Xd = np.array(neighbors_x) - x0  # m x dim\n            y = np.array(neighbors_f) - f0   # m\n            # solve Xd @ g \u2248 y  (least squares). We want g approx gradient (\u2202f/\u2202x).\n            # Note: solve for g in least-squares sense; if Xd has low rank, lstsq will regularize.\n            try:\n                g, *_ = np.linalg.lstsq(Xd, y, rcond=None)\n                # If gradient is NaN or inf, bail\n                if not np.all(np.isfinite(g)):\n                    return None\n                return g\n            except Exception:\n                return None\n\n        # Main loop\n        while evals < self.budget:\n            iters += 1\n            n_el = len(xs)\n            # store baseline best\n            best_idx = int(np.argmin(fs))\n            f_best = fs[best_idx]\n            x_best = xs[best_idx].copy()\n\n            # Build list of edges with a priority score that discounts successes (bandit-style)\n            edges = []\n            for i in range(n_el):\n                for j in range(i+1, n_el):\n                    avg_f = 0.5 * (fs[i] + fs[j])\n                    # success bonus reduces priority (we prefer lower scores)\n                    score = avg_f - 0.2 * (succ[i] + succ[j])\n                    edges.append((score, i, j))\n            edges.sort(key=lambda t: t[0])\n\n            if len(edges) == 0:\n                # nothing to do but diversify\n                if evals >= self.budget:\n                    break\n                xg = lb + self.rng.random(self.dim) * (ub - lb)\n                rval = try_eval(xg)\n                if rval is None:\n                    break\n                xg, fg = rval\n                xs.append(xg); fs.append(fg); radii.append(self.init_radius); succ.append(0)\n                xs, fs, radii, succ = trim_and_sort(xs, fs, radii, succ)\n                continue\n\n            # pick subset of edges to explore this iteration (mostly top ones + a few random)\n            num_edges = min(len(edges), max(1, self.max_edges_per_iter))\n            top_count = int(max(1, np.ceil(num_edges * 0.7)))\n            chosen = edges[:top_count]\n            if num_edges > top_count:\n                remaining = edges[top_count:]\n                rcount = num_edges - top_count\n                picks = self.rng.choice(len(remaining), size=min(rcount, len(remaining)), replace=False)\n                for p in picks:\n                    chosen.append(remaining[p])\n\n            improved = False\n\n            # Explore each chosen edge compactly\n            for _, i, j in chosen:\n                if evals >= self.budget:\n                    break\n\n                xi = xs[i]; xj = xs[j]; fi = fs[i]; fj = fs[j]\n                edge_vec = xj - xi\n                edge_len = np.linalg.norm(edge_vec) + 1e-12\n                edge_dir = edge_vec / edge_len\n\n                # adaptive edge radius based on endpoint radii and edge length\n                rad = np.clip(0.6 * (radii[i] + radii[j]) * (1.0 + 0.15 * edge_len),\n                              self.min_radius, self.max_radius)\n\n                # number of probes: small (1-3); focus on midpoint and one extrapolation if budget allows\n                probes = [0.5]\n                if self.rng.random() < 0.4:\n                    probes.append(0.5 + self.rng.normal(0, 0.18))\n                if self.rng.random() < 0.15:\n                    probes.append(-0.12 + 1.27 * self.rng.random())\n\n                # low-dimensional subspace: edge dir + up to 2 orthogonal modes (keep small)\n                k_sub = min(self.dim, 1 + min(2, max(1, int(self.dim / 8))))\n                basis = self._orthonormal_basis(edge_dir, k_sub)  # (dim, k_sub)\n\n                local_success = 0\n                # For each probe, make a focused perturbation and evaluate\n                for t in probes:\n                    if evals >= self.budget:\n                        break\n                    x_base = xi * (1.0 - t) + xj * t\n                    coeffs = self.rng.normal(scale=rad, size=k_sub)\n                    coeffs[0] *= 0.35  # damp along-edge to favor local connectivity\n                    x_cand = x_base + basis.dot(coeffs)\n                    rval = try_eval(x_cand)\n                    if rval is None:\n                        break\n                    x_cand, f_cand = rval\n\n                    # Accept candidate into elites if good\n                    worst_f = max(fs) if len(fs) > 0 else float(\"inf\")\n                    if (len(fs) < self.elite_size) or (f_cand < worst_f):\n                        # insert and trim\n                        xs.append(x_cand.copy()); fs.append(f_cand); radii.append(max(self.min_radius, rad * (1.0 if f_cand < min(fi, fj) else 0.8))); succ.append(1)\n                        xs, fs, radii, succ = trim_and_sort(xs, fs, radii, succ)\n                        local_success += 1\n                        improved = True\n                        no_improve_iters = 0\n                    else:\n                        # small decay of success indicator for endpoints\n                        pass\n\n                # After sampling along the edge, attempt a cheap directed local search from the better endpoint:\n                if evals >= self.budget:\n                    break\n                # choose base as better endpoint among the pair\n                base_idx = i if fi <= fj else j\n                base_x = xs[base_idx]; base_f = fs[base_idx]\n                # choose up to M neighbors (other elites) to estimate gradient\n                M = min(len(xs) - 1, 6)\n                if M >= 2:\n                    # pick nearest elites in Euclidean distance as neighbors (exclude base itself)\n                    dists = [np.linalg.norm(xs[k] - base_x) if k != base_idx else np.inf for k in range(len(xs))]\n                    nn_idx = np.argsort(dists)[:M]\n                    neigh_x = [xs[k] for k in nn_idx]\n                    neigh_f = [fs[k] for k in nn_idx]\n                    g = estimate_grad(base_x, base_f, neigh_x, neigh_f)\n                    if g is not None and np.linalg.norm(g) > 1e-12:\n                        # propose a short step along negative gradient direction with backtracking\n                        g_dir = g / (np.linalg.norm(g) + 1e-12)\n                        # initial step proportional to radius and typical distance among neighbors\n                        typical_scale = np.median(np.maximum(1e-6, np.linalg.norm(np.array(neigh_x) - base_x, axis=1))))\n                        step = 0.9 * (radii[base_idx] + 0.5 * typical_scale)\n                        # clip step to box size\n                        step = min(step, np.linalg.norm(ub - lb) * 0.5)\n                        improved_line = False\n                        for _bs in range(4):  # up to 4 backtracking reductions\n                            if evals >= self.budget:\n                                break\n                            probe = base_x - step * g_dir\n                            rval = try_eval(probe)\n                            if rval is None:\n                                break\n                            probe, f_probe = rval\n                            if (len(fs) < self.elite_size) or (f_probe < max(fs)):\n                                xs.append(probe.copy()); fs.append(f_probe); radii.append(max(self.min_radius, radii[base_idx] * 0.9)); succ.append(2)\n                                xs, fs, radii, succ = trim_and_sort(xs, fs, radii, succ)\n                                improved = True\n                                local_success += 1\n                                improved_line = True\n                                no_improve_iters = 0\n                                break\n                            else:\n                                # reduce step\n                                step *= 0.5\n                        # if line search failed, penalize success count slightly to deprioritize this base\n                        if not improved_line:\n                            succ[base_idx] = max(0, succ[base_idx] - 1)\n\n                # quick 1-step extrapolation from best and second-best (momentum-like)\n                if evals < self.budget and len(xs) >= 2:\n                    b0 = 0  # since arrays are sorted best-first\n                    b1 = 1 if len(xs) > 1 else 0\n                    extrap = xs[b0] + 0.15 * (xs[b0] - xs[b1])\n                    rval = try_eval(extrap)\n                    if rval is not None:\n                        extrap_x, extrap_f = rval\n                        if (len(fs) < self.elite_size) or (extrap_f < max(fs)):\n                            xs.append(extrap_x.copy()); fs.append(extrap_f); radii.append(self.init_radius); succ.append(1)\n                            xs, fs, radii, succ = trim_and_sort(xs, fs, radii, succ)\n                            improved = True\n                            no_improve_iters = 0\n                            local_success += 1\n\n                # update endpoint success counters from local_success (reward)\n                if local_success > 0:\n                    succ[i] = max(0, succ[i]) + local_success\n                    succ[j] = max(0, succ[j]) + local_success\n\n                # if early improvement found and budget tight, break to re-prioritize\n                if improved and (self.budget - evals) < max(4, self.dim // 2):\n                    # allow a short re-evaluation of priorities\n                    break\n\n            # adjust radii globally based on whether we improved\n            if improved:\n                no_improve_iters = 0\n                # gentle expansion for successful elites\n                for idx in range(len(radii)):\n                    radii[idx] = min(self.max_radius, radii[idx] * 1.06)\n            else:\n                no_improve_iters += 1\n                for idx in range(len(radii)):\n                    radii[idx] = max(self.min_radius, radii[idx] * 0.82)\n\n            # occasional diversification when stagnating\n            if no_improve_iters >= 5 and evals < self.budget:\n                n_global = min(3, max(1, self.dim // 6))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    # opposite-based sample for best (reflect across center)\n                    center = 0.5 * (lb + ub)\n                    best = xs[0]\n                    opp = center + (center - best) + self.rng.normal(scale=0.15 * (ub - lb), size=self.dim)\n                    rval = try_eval(opp)\n                    if rval is None:\n                        break\n                    xg, fg = rval\n                    if (len(fs) < self.elite_size) or (fg < max(fs)):\n                        xs.append(xg.copy()); fs.append(fg); radii.append(self.init_radius); succ.append(1)\n                        xs, fs, radii, succ = trim_and_sort(xs, fs, radii, succ)\n                        improved = True\n                        no_improve_iters = 0\n                # if still no improvement, perform a focused large jump from a random elite\n                if (not improved) and evals < self.budget:\n                    center_idx = int(self.rng.integers(0, len(xs)))\n                    center = xs[center_idx]\n                    jump_scale = 0.5 * (ub - lb)\n                    xj = center + self.rng.normal(0, 1.0, size=self.dim) * jump_scale\n                    rval = try_eval(xj)\n                    if rval is not None:\n                        xj, fj = rval\n                        if (len(fs) < self.elite_size) or (fj < max(fs)):\n                            xs.append(xj.copy()); fs.append(fj); radii.append(self.init_radius); succ.append(1)\n                            xs, fs, radii, succ = trim_and_sort(xs, fs, radii, succ)\n                            improved = True\n                            no_improve_iters = 0\n                        else:\n                            # shrink radii a bit to refocus\n                            for idx in range(len(radii)):\n                                radii[idx] = max(self.min_radius, self.init_radius * 0.6)\n                            no_improve_iters = 0\n\n            # safety: if average radius collapsed to near-zero, gently re-inflate\n            if np.mean(radii) < 1e-8:\n                for idx in range(len(radii)):\n                    radii[idx] = max(self.min_radius, self.init_radius * 0.5)\n\n            # safety break to avoid infinite loops in degenerate cases\n            if iters > max(10000, self.budget * 10):\n                break\n\n        # finalize best\n        best_idx = int(np.argmin(fs))\n        return float(fs[best_idx]), np.array(xs[best_idx], dtype=float)", "configspace": "", "generation": 4, "feedback": "In the code, line 228, in evaluate, the following error occurred:\nSyntaxError: unmatched ')'\nOn line: xs.append(x_cand.copy()); fs.append(f_cand); radii.append(max(self.min_radius, rad * (1.0 if f_cand < min(fi, fj) else 0.8))); succ.append(1)", "error": "In the code, line 228, in evaluate, the following error occurred:\nSyntaxError: unmatched ')'\nOn line: xs.append(x_cand.copy()); fs.append(f_cand); radii.append(max(self.min_radius, rad * (1.0 if f_cand < min(fi, fj) else 0.8))); succ.append(1)", "parent_ids": ["7122544b-51dc-41b4-8c0b-a820ef136bc1"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {}, "archive_direction": "decrease", "archive_feature": "mean depth"}
{"id": "0c98b60a-b401-4b27-b5de-4c183326b288", "fitness": 0.3259137506385941, "name": "AdaptiveLowRankQuadraticSurrogates", "description": "Adaptive Low-Rank Quadratic Surrogates (ALRQS) \u2014 build many small low-dimensional quadratic surrogates in random subspaces around elite centers, solve the surrogate minima in-subspace as trust-region proposals, adapt trust sizes with predicted/actual agreement, and inject global diversification to find good solutions quickly.", "code": "import numpy as np\n\nclass AdaptiveLowRankQuadraticSurrogates:\n    \"\"\"\n    Adaptive Low-Rank Quadratic Surrogates (ALRQS)\n\n    Main idea (short):\n      - Maintain an archive of evaluated points.\n      - Repeatedly select a promising center, choose a small random low-dimensional\n        subspace around it, fit a quadratic surrogate (a + b^T s + 0.5 s^T H s)\n        using nearby archive points projected into the subspace, solve the surrogate\n        minimizer in that subspace, propose the mapped point (trust-region limited),\n        evaluate it and update trust radius by comparing predicted vs actual reduction.\n      - Use many cheap, local surrogate solves (instead of high-dimensional sampling),\n        which often produces strong early improvements (reduces mean depth).\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_samples=None,\n                 elite_k=6, init_radius=1.0, min_radius=1e-6, max_radius=5.0,\n                 max_subdim=4, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total function evaluations allowed (int)\n          dim: problem dimensionality\n          init_samples: number of initial random samples (default ~ min(50, budget//10 or 10))\n          elite_k: number of promising centers to sample from by weight\n          init_radius: initial trust radius for centers\n          min_radius, max_radius: bounds on trust radius\n          max_subdim: maximum low-subspace dimension to fit (<= dim)\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if init_samples is None:\n            self.init_samples = int(min(50, max(10, self.budget // 10)))\n        else:\n            self.init_samples = int(init_samples)\n        self.elite_k = int(elite_k)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_subdim = int(min(max_subdim, dim))\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _orthonormal_basis(self, k):\n        # simple random orthonormal basis of dimension k in R^dim\n        A = self.rng.normal(size=(self.dim, k))\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def _build_phi_and_map(self, S):\n        # S: (m, k) array of subspace coordinates s\n        # Build design matrix Phi with columns: [1, s1..sk, 0.5*s1^2, s1*s2, ..., 0.5*sk^2]\n        m, k = S.shape\n        # p = 1 + k + k*(k+1)//2\n        cols = [np.ones((m, 1))]\n        cols.append(S)  # linear\n        # quadratic terms\n        quad_cols = []\n        for i in range(k):\n            quad_cols.append(0.5 * (S[:, i:i+1] ** 2))  # diag entries as 0.5*s_i^2\n        for i in range(k):\n            for j in range(i+1, k):\n                quad_cols.append((S[:, i] * S[:, j])[:, None])\n        cols.extend(quad_cols)\n        Phi = np.hstack(cols)\n        return Phi\n\n    def _theta_to_quad(self, theta, k):\n        # theta vector length p => extract a, b (k), and H (k,k) symmetric\n        a = float(theta[0])\n        b = np.array(theta[1:1 + k], dtype=float)\n        H = np.zeros((k, k), dtype=float)\n        idx = 1 + k\n        # diag entries\n        for i in range(k):\n            H[i, i] = float(theta[idx])\n            idx += 1\n        # off-diagonals\n        for i in range(k):\n            for j in range(i + 1, k):\n                H[i, j] = float(theta[idx])\n                H[j, i] = H[i, j]\n                idx += 1\n        return a, b, H\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []  # list of np arrays\n        F = []  # list of floats\n        radii = []  # trust radius per archived point\n\n        # initial random seeding (diverse)\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n            radii.append(self.init_radius)\n\n        # ensure at least one sample\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n            radii.append(self.init_radius)\n\n        # convert to arrays for some ops (we'll keep lists but convert when needed)\n        best_idx = int(np.argmin(F))\n        best_f = F[best_idx]\n        best_x = X[best_idx].copy()\n\n        stagnation = 0\n        iter_count = 0\n\n        # main loop: propose surrogate minima until budget exhausted\n        while evals < self.budget:\n            iter_count += 1\n            n_archive = len(X)\n            # pick center indices from top-k with bias to better ones\n            top_k = min(self.elite_k, n_archive)\n            # sort archive indices by F ascending\n            idx_sorted = np.argsort(F)\n            # compute selection weights on top_k (softmax of negative f)\n            top_indices = idx_sorted[:top_k]\n            top_fs = np.array([F[i] for i in top_indices])\n            # stabilize weights\n            w = np.exp(-(top_fs - top_fs.min()) / (1e-8 + max(1.0, np.std(top_fs))))\n            probs = w / w.sum()\n            # pick one center\n            center_choice = self.rng.choice(top_indices, p=probs)\n            center_x = X[center_choice].copy()\n            center_f = F[center_choice]\n            center_r = radii[center_choice]\n\n            # choose subspace dimension k_sub (1..max_subdim but <= dim)\n            k_sub = int(self.rng.integers(1, min(self.max_subdim, self.dim) + 1))\n            basis = self._orthonormal_basis(k_sub)  # (dim, k_sub)\n\n            # collect nearby samples in subspace coordinates\n            X_arr = np.vstack(X)\n            # distances in ambient\n            dists = np.linalg.norm(X_arr - center_x[None, :], axis=1)\n            # search radius for neighborhood: use up to 2*center_r or some percentile\n            neigh_radius = max(center_r * 1.8, np.percentile(dists, min(80, max(20, int(100 * (1 - 1.0 / max(1, n_archive)))))) if n_archive > 3 else center_r * 1.8)\n            # pick indices within neigh_radius, but ensure at least p samples\n            within = np.where(dists <= neigh_radius)[0].tolist()\n            p_min = 1 + k_sub + (k_sub * (k_sub + 1)) // 2\n            if len(within) < p_min:\n                # add nearest points until reach p_min\n                order = np.argsort(dists)\n                for idx in order:\n                    if idx not in within:\n                        within.append(int(idx))\n                    if len(within) >= p_min:\n                        break\n            # build S matrix (projected coordinates)\n            X_neigh = X_arr[within]\n            S = (X_neigh - center_x[None, :]) @ basis  # (m, k_sub)\n\n            # Fit quadratic surrogate: a + b^T s + 0.5 s^T H s\n            Phi = self._build_phi_and_map(S)  # (m, p)\n            y = np.array([F[idx] for idx in within], dtype=float)\n            # regularization proportional to variance and p\n            p = Phi.shape[1]\n            reg = 1e-8 * max(1.0, np.var(y))\n            # Solve ridge least squares: (Phi^T Phi + reg I) theta = Phi^T y\n            try:\n                A = Phi.T @ Phi\n                A[np.diag_indices_from(A)] += reg + 1e-12  # tiny diag stabilization\n                rhs = Phi.T @ y\n                theta = np.linalg.solve(A, rhs)\n            except np.linalg.LinAlgError:\n                # fallback: use least squares solver\n                try:\n                    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n                except Exception:\n                    theta = None\n\n            # If fitting failed, perform a random local perturbation\n            if theta is None or np.any(np.isnan(theta)):\n                # random perturbation inside trust radius\n                if evals >= self.budget:\n                    break\n                step = self.rng.normal(size=self.dim)\n                step = step / (np.linalg.norm(step) + 1e-12) * center_r * self.rng.random()\n                x_cand = np.clip(center_x + step, lb, ub)\n                f_cand = float(func(x_cand))\n                evals += 1\n                X.append(x_cand.copy()); F.append(f_cand); radii.append(self.init_radius)\n                # update best\n                if f_cand < best_f:\n                    best_f = f_cand; best_x = x_cand.copy(); stagnation = 0\n                else:\n                    stagnation += 1\n                continue  # next loop\n\n            # extract surrogate parameters\n            a_hat, b_hat, H_hat = self._theta_to_quad(theta, k_sub)\n\n            # solve H s = -b for s*, regularize until stable\n            s_star = None\n            lam = 1e-8\n            # We will try to ensure H + lam*I is invertible; escalate lam if necessary\n            for attempt in range(6):\n                H_reg = H_hat + lam * np.eye(k_sub)\n                try:\n                    s_star = -np.linalg.solve(H_reg, b_hat)\n                    break\n                except np.linalg.LinAlgError:\n                    lam = max(1e-6, lam * 10.0)\n            # If failed to get s*, fallback to negative gradient (b_hat) direction\n            if s_star is None or not np.all(np.isfinite(s_star)):\n                # direction = -b_hat\n                if np.linalg.norm(b_hat) < 1e-12:\n                    # trivial fallback random small step\n                    s_dir = self.rng.normal(size=k_sub)\n                else:\n                    s_dir = -b_hat\n                # scale to a fraction of trust radius\n                norm_dir = np.linalg.norm(s_dir)\n                if norm_dir < 1e-12:\n                    s_star = s_dir\n                else:\n                    s_star = s_dir / norm_dir * (0.75 * center_r)\n\n            # map s_star back to ambient space\n            x_prop = center_x + basis @ s_star\n            # enforce trust-region step size limit: ||x_prop - center|| <= 1.5 * center_r\n            disp = x_prop - center_x\n            disp_norm = np.linalg.norm(disp)\n            max_step = max(1e-12, 1.5 * center_r)\n            if disp_norm > max_step:\n                x_prop = center_x + disp * (max_step / disp_norm)\n            x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n\n            # Predict surrogate value at s_star\n            pred_val = float(a_hat + b_hat.dot(s_star) + 0.5 * float(s_star.T @ (H_hat @ s_star)))\n            predicted_reduction = center_f - pred_val\n\n            # If predicted reduction is non-positive, move along -b_hat in small step instead\n            if predicted_reduction <= 1e-12:\n                # small step opposite linear term\n                dir_s = -b_hat\n                if np.linalg.norm(dir_s) < 1e-12:\n                    # random small\n                    dir_s = self.rng.normal(size=k_sub)\n                step_len = min(center_r * 0.7, max(1e-6, np.linalg.norm(dir_s)))\n                s_try = dir_s / (np.linalg.norm(dir_s) + 1e-12) * step_len\n                x_prop = center_x + basis @ s_try\n                x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n\n            # Evaluate candidate (check budget)\n            if evals >= self.budget:\n                break\n            f_prop = float(func(x_prop))\n            evals += 1\n\n            # Add to archive\n            X.append(x_prop.copy())\n            F.append(f_prop)\n            # initial radius for new point is inherited or init radius\n            new_r = max(self.min_radius, min(self.max_radius, center_r * 0.9))\n            radii.append(new_r)\n\n            # compute actual reduction and ratio\n            actual_reduction = center_f - f_prop\n            denom = max(1e-12, predicted_reduction)\n            rho = actual_reduction / denom if denom != 0.0 else (1.0 if actual_reduction > 0 else 0.0)\n\n            # Trust-region radius update for the center (and candidate)\n            if rho > 0.75:\n                # very successful: expand\n                radii[center_choice] = min(self.max_radius, center_r * 1.6)\n                radii[-1] = min(self.max_radius, radii[-1] * 1.2)\n            elif rho > 0.25:\n                # moderate success: slight expand\n                radii[center_choice] = min(self.max_radius, center_r * 1.15)\n            else:\n                # failure: shrink\n                radii[center_choice] = max(self.min_radius, center_r * 0.5)\n\n            # update best if improved\n            if f_prop < best_f:\n                best_f = f_prop\n                best_x = x_prop.copy()\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # Occasionally do a line of quick greedy short probes along best-direction in subspace\n            if evals < self.budget and self.rng.random() < 0.12:\n                # try small interpolation between best and center\n                alpha = self.rng.random() * 0.5\n                x_probe = np.clip(alpha * center_x + (1 - alpha) * best_x, lb, ub)\n                f_probe = float(func(x_probe))\n                evals += 1\n                X.append(x_probe.copy()); F.append(f_probe); radii.append(self.init_radius * 0.8)\n                if f_probe < best_f:\n                    best_f = f_probe; best_x = x_probe.copy(); stagnation = 0\n\n            # If stagnation grows, inject global random samples or larger jumps\n            if stagnation >= 8 and evals < self.budget:\n                n_global = min(4, max(1, self.dim // 4))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg))\n                    evals += 1\n                    X.append(xg.copy()); F.append(fg); radii.append(self.init_radius)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagnation = 0\n                # if still no improvement, perform a larger Gaussian jump around best_x\n                if stagnation >= 8 and evals < self.budget:\n                    jump_scale = 0.6 * (ub - lb)\n                    xj = best_x + self.rng.normal(0, 1.0, size=self.dim) * jump_scale\n                    xj = np.minimum(np.maximum(xj, lb), ub)\n                    fj = float(func(xj))\n                    evals += 1\n                    X.append(xj.copy()); F.append(fj); radii.append(self.init_radius)\n                    if fj < best_f:\n                        best_f = fj; best_x = xj.copy(); stagnation = 0\n                    else:\n                        # mild radius reset to encourage fresh local fits\n                        radii = [max(self.min_radius, 0.6 * self.init_radius) for _ in radii]\n                        stagnation = 0\n\n            # Trim archive size moderately to keep operations cheap: keep up to e.g. 6*dim + 100\n            max_archive = max(200, 6 * self.dim + 50)\n            if len(X) > max_archive:\n                # keep best ones plus some random diversity\n                idx_sorted = np.argsort(F)\n                keep_best = int(max(30, 3 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                # add some random survivors\n                survivors = list(keep_set)\n                # add by probability favoring better ones\n                probs = np.exp(-(np.array(F) - np.min(F)) / (1e-8 + max(1.0, np.std(F))))\n                probs /= probs.sum()\n                extra = self.rng.choice(len(X), size=min(len(X) - keep_best, max_archive - keep_best), replace=False, p=probs)\n                for e in extra:\n                    keep_set.add(int(e))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                radii = [radii[i] for i in keep_list]\n\n        # finalize\n        best_i = int(np.argmin(F))\n        return float(F[best_i]), np.array(X[best_i], dtype=float)", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveLowRankQuadraticSurrogates scored 0.326 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7122544b-51dc-41b4-8c0b-a820ef136bc1"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.7196228166351077}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.713404027711203}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.6844687825292618}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.01682550343877187}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.935708267844229}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.925143471816697}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.8931333896036423}], "aucs": [0.7196228166351077, 0.713404027711203, 0.6844687825292618, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.01682550343877187, 4.999999999999449e-05, 0.935708267844229, 0.925143471816697, 0.8931333896036423]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3187.0, "Edges": 3186.0, "Max Degree": 60.0, "Min Degree": 1.0, "Mean Degree": 1.9993724505804833, "Degree Variance": 2.623783729181412, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 7.901122019635344, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3180890629464552, "Depth Entropy": 2.0203453454928106, "Assortativity": 0.0, "Average Eccentricity": 18.917477251333544, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.00031377470975839345, "Average Shortest Path": 10.083627164735269, "mean_complexity": 11.666666666666666, "total_complexity": 70.0, "mean_token_count": 460.6666666666667, "total_token_count": 2764.0, "mean_parameter_count": 3.5, "total_parameter_count": 21.0}, "archive_direction": "decrease", "archive_feature": "mean depth"}
{"id": "ad9a18eb-f0d3-4acb-9a5d-923e1cac7b4b", "fitness": 0.21415355858836335, "name": "EdgeMeshAdaptiveManifoldWalks", "description": "Edge-Mesh Adaptive Manifold Walks (EMAM) \u2014 build a dense elite edge+triangle mesh, perform intensive barycentric and manifold-aware orthogonal sampling across edges/faces, and run short adaptive \"edge-walks\" along productive edges to rapidly increase edge density and exploit connected low-cost corridors.", "code": "import numpy as np\n\nclass EdgeMeshAdaptiveManifoldWalks:\n    \"\"\"\n    Edge-Mesh Adaptive Manifold Walks (EMAM)\n\n    Main ideas:\n      - Maintain a compact elite archive (population). Build a dense edge graph (k-NN) and enumerate small\n        triangles (faces) formed by mutual neighbors to create an \"edge+face mesh\".\n      - Sample heavily from edges and triangle interiors (barycentric sampling) with orthogonal perturbations\n        projected into local manifold directions (computed with PCA of neighbors) to increase sampling density\n        between known good points.\n      - Adaptively intensify sampling on edges that produced improvements (edge-walks): short directed walks\n        along the edge with shrinking orthogonal noise to follow corridors between basins.\n      - Complement mesh sampling with local anisotropic refinement around elites and occasional global/opposite\n        injections for diversification.\n      - Strictly enforce the evaluation budget.\n    Works on search domain [-5,5]^dim.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, rng=None,\n                 pop_size=36, knn=7, init_frac=0.12,\n                 edge_refresh=10, max_archive=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # population / graph params\n        self.pop_size = max(6, int(pop_size))\n        self.knn = max(1, int(knn))\n        self.init_frac = float(init_frac)\n        self.edge_refresh = max(4, int(edge_refresh))\n\n        # step-size / adaptation\n        span = self.ub - self.lb\n        self.sigma0 = 0.45 * span\n        self.sigma = float(self.sigma0)\n        self.sigma_min = 1e-6\n        self.sigma_max = 6.0\n        self.success_shrink = 0.85\n        self.failure_expand = 1.05\n\n        # archive limits\n        if max_archive is None:\n            self.max_archive = max(400, 30 * self.dim)\n        else:\n            self.max_archive = int(max_archive)\n\n        # internal state across calls (optional)\n        self.reset_state()\n\n    def reset_state(self):\n        # dynamic graph structures (recomputed during run)\n        self.edges = []        # list of (i,j) indices into pop_X\n        self.edge_weights = np.array([])    # sampling probabilities\n        self.edge_scores = None  # int improvement counts per edge\n        self.triangles = []     # list of triplets (i,j,k)\n        # active \"edge-walk\" state\n        self.walk_state = None  # dict with keys: edge_idx, steps_left, last_dir_sign\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb)\n        ub = np.full(dim, self.ub)\n        width = ub - lb\n\n        evals = 0\n        rng = self.rng\n\n        # INITIALIZATION\n        n_init = int(min(budget, max(6, int(self.init_frac * budget), 4 * dim)))\n        X_archive = []\n        f_archive = []\n\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            X_archive.append(np.array(x, copy=True))\n            f_archive.append(float(f))\n            evals += 1\n            if evals >= budget:\n                break\n\n        if len(X_archive) == 0:\n            x0 = rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_archive.append(np.array(x0.copy()))\n            f_archive.append(float(f0))\n            evals = 1\n\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # build compact elite population\n        def build_population(Xa, fa):\n            order = np.argsort(fa)\n            k = min(self.pop_size, len(order))\n            inds = order[:k]\n            return Xa[inds].copy(), fa[inds].copy(), inds\n\n        pop_X, pop_f, pop_inds = build_population(X_archive, f_archive)\n        best_idx = int(np.argmin(pop_f))\n        x_best = pop_X[best_idx].copy()\n        f_best = float(pop_f[best_idx])\n\n        self.reset_state()\n\n        # helper: refresh edge graph and triangles, recompute weights\n        def refresh_graph():\n            self.edges = []\n            self.triangles = []\n            n = pop_X.shape[0]\n            if n < 2:\n                self.edge_weights = np.array([])\n                self.edge_scores = np.zeros(0, dtype=float)\n                return\n            # pairwise distances\n            D = np.linalg.norm(pop_X[:, None, :] - pop_X[None, :, :], axis=2)\n            kn = min(self.knn, n - 1)\n            pairs = set()\n            for i in range(n):\n                neigh = np.argsort(D[i])[1:1 + kn]\n                for j in neigh:\n                    a, b = (i, j) if i < j else (j, i)\n                    pairs.add((a, b))\n            self.edges = sorted(list(pairs))\n            # build triangles: for each pair, find common neighbors\n            if len(self.edges) > 0:\n                neighbor_sets = [set(np.argsort(D[i])[1:1 + kn]) for i in range(n)]\n                tri_set = set()\n                for (a, b) in self.edges:\n                    common = neighbor_sets[a].intersection(neighbor_sets[b])\n                    for c in common:\n                        t = tuple(sorted((a, b, c)))\n                        tri_set.add(t)\n                self.triangles = sorted(list(tri_set))\n\n            # edge weights favor edges with good endpoints and shorter length\n            if len(self.edges) == 0:\n                self.edge_weights = np.array([])\n                self.edge_scores = np.zeros(0, dtype=float)\n                return\n            fmin, fmax = np.min(pop_f), np.max(pop_f)\n            rng_scale = max(1e-9, fmax - fmin)\n            w = []\n            for (i, j) in self.edges:\n                edge_len = np.linalg.norm(pop_X[i] - pop_X[j])\n                score_end = (fmin - min(pop_f[i], pop_f[j])) / (rng_scale + 1e-12)\n                # combine endpoint quality and shortness preference\n                wv = np.exp(1.2 * score_end) * (1.0 / (1.0 + edge_len / (0.5 * np.mean(width))))\n                w.append(wv + 1e-6)\n            w = np.array(w, dtype=float)\n            # normalize\n            w = w / (np.sum(w) + 1e-12)\n            self.edge_weights = w\n            # initialize edge_scores (counts of improvements) to zero\n            self.edge_scores = np.zeros(len(self.edges), dtype=float)\n\n        refresh_graph()\n\n        iter_since_improve = 0\n\n        # helper: choose an edge index biased by weights and edge_scores\n        def choose_edge_idx():\n            if len(self.edges) == 0:\n                return None\n            # combine base weight with historical edge score (gives momentum)\n            base = self.edge_weights.copy()\n            hist = np.exp(0.6 * (self.edge_scores - np.mean(self.edge_scores))) if self.edge_scores.size > 0 else np.ones_like(base)\n            combo = base * (1.0 + 0.8 * (hist - np.min(hist)) / (np.ptp(hist) + 1e-12))\n            combo = np.clip(combo, 1e-12, None)\n            combo = combo / combo.sum()\n            return rng.choice(len(self.edges), p=combo)\n\n        # helper: choose triangle index (favor triangles with good vertices)\n        def choose_triangle_idx():\n            if len(self.triangles) == 0:\n                return None\n            # score triangle by best vertex fitness\n            tri_scores = []\n            fmin = np.min(pop_f)\n            fmax = np.max(pop_f)\n            scale = max(1e-9, fmax - fmin)\n            for (i, j, k) in self.triangles:\n                tri_scores.append(np.exp((fmin - min(pop_f[i], pop_f[j], pop_f[k])) / scale))\n            ts = np.array(tri_scores, dtype=float)\n            ts = ts / (ts.sum() + 1e-12)\n            return rng.choice(len(self.triangles), p=ts)\n\n        # helper: local PCA for a node/neighborhood (return orthonormal basis)\n        def local_basis_for_edge(i, j):\n            # gather neighborhood points: endpoints plus their knn neighbors\n            n = pop_X.shape[0]\n            pts = [pop_X[i], pop_X[j]]\n            if n >= 3:\n                # compute distances to midpoint\n                mid = 0.5 * (pop_X[i] + pop_X[j])\n                dists = np.linalg.norm(pop_X - mid, axis=1)\n                k = min(max(4, self.knn), n)\n                neigh_idx = np.argsort(dists)[:k]\n                for idx in neigh_idx:\n                    pts.append(pop_X[idx])\n            P = np.vstack(pts)\n            # center and SVD\n            P_centered = P - P.mean(axis=0)\n            # if too few unique rows, return standard basis\n            if P_centered.shape[0] <= 1 or np.linalg.matrix_rank(P_centered) <= 1:\n                return np.eye(dim)\n            U, S, Vt = np.linalg.svd(P_centered, full_matrices=False)\n            # Vt rows are principal directions ordered by variance\n            return Vt  # shape (dim, dim) or less; we will use first few rows\n\n        # MAIN LOOP: single eval per iteration (except special extra-eval branches handled carefully)\n        while evals < budget:\n            # dynamic probabilities\n            frac = evals / max(1.0, budget)\n            p_mesh = 0.55 * (1 - frac) + 0.25      # mesh sampling (edges/triangles)\n            p_tri = 0.28 * (1 - frac) + 0.12      # triangle sampling component\n            p_edge_walk = 0.18 + 0.18 * frac      # chance to continue/start edge-walk\n            p_local = 0.14 + 0.36 * frac          # local refinement grows over time\n            p_global = max(0.01, 1.0 - (p_mesh + p_local))  # global fallback\n            # normalize safety\n            ps = np.array([p_mesh, p_tri, p_edge_walk, p_local, p_global])\n            ps = np.clip(ps, 1e-8, 1.0)\n            ps = ps / ps.sum()\n            p_mesh, p_tri, p_edge_walk, p_local, p_global = ps\n\n            # prioritized: if there is an active walk_state, continue it with higher priority\n            if self.walk_state is not None and self.walk_state.get(\"steps_left\", 0) > 0 and rng.random() < 0.92:\n                # continue edge-walk\n                eidx = self.walk_state[\"edge_idx\"]\n                if eidx is None or eidx >= len(self.edges):\n                    self.walk_state = None\n                else:\n                    i, j = self.edges[eidx]\n                    xi, xj = pop_X[i], pop_X[j]\n                    u = xj - xi\n                    un = np.linalg.norm(u) + 1e-12\n                    udir = u / un\n                    # move along edge in the chosen sign direction, step-size proportional to edge length\n                    sign = self.walk_state.get(\"last_dir_sign\", 1)\n                    step_along = sign * 0.12 * un * (0.6 + 0.8 * rng.random())\n                    # orthogonal noise shrinks each step\n                    step_orth_scale = self.sigma * (0.6 * (self.walk_state[\"steps_left\"] / (self.walk_state[\"initial_steps\"] + 1)))\n                    # build orth basis via local PCA\n                    Vt = local_basis_for_edge(i, j)\n                    # pick 1-2 principal orth directions orthogonal to edge\n                    # ensure udir is not aligned with first principal component\n                    # get a vector orth to udir from principal components\n                    orth_dim = min(2, Vt.shape[0])\n                    orth_vec = np.zeros(dim)\n                    for r in range(orth_dim):\n                        v = Vt[r]\n                        # project out udir\n                        v = v - np.dot(v, udir) * udir\n                        if np.linalg.norm(v) > 1e-12:\n                            orth_vec += v / (np.linalg.norm(v) + 1e-12)\n                    if np.linalg.norm(orth_vec) < 1e-12:\n                        orth_vec = rng.normal(size=dim)\n                    orth_vec = orth_vec / (np.linalg.norm(orth_vec) + 1e-12)\n                    x_candidate = 0.5 * (xi + xj) + step_along * udir + step_orth_scale * rng.normal() * orth_vec\n\n                    # occasionally add small barycentric bias toward a better endpoint\n                    if rng.random() < 0.35:\n                        better = i if pop_f[i] <= pop_f[j] else j\n                        bias = 0.15 * (pop_X[better] - 0.5 * (xi + xj))\n                        x_candidate += bias\n\n                    # update walk_state\n                    self.walk_state[\"steps_left\"] -= 1\n                    if self.walk_state[\"steps_left\"] <= 0:\n                        self.walk_state = None\n\n            else:\n                choice = rng.random()\n                if choice < p_mesh:\n                    # Mesh sampling: either edge interpolation or triangle barycentric depending on sub-prob\n                    if rng.random() < 0.64 and len(self.edges) > 0:\n                        # Edge interpolation + manifold-aware orthogonal perturbation\n                        eidx = choose_edge_idx()\n                        if eidx is None:\n                            x_candidate = rng.uniform(lb, ub)\n                        else:\n                            i, j = self.edges[eidx]\n                            xi = pop_X[i]\n                            xj = pop_X[j]\n                            u = xj - xi\n                            un = np.linalg.norm(u) + 1e-12\n                            udir = u / un\n                            # interpolation alpha biased toward endpoints mid but allow extrapolation\n                            alpha = rng.beta(2.0, 2.0)\n                            alpha = np.clip(alpha + rng.uniform(-0.18, 0.18), -0.3, 1.3)\n                            base = alpha * xi + (1 - alpha) * xj\n                            # get local manifold basis\n                            Vt = local_basis_for_edge(i, j)\n                            # construct orth component from top principal directions orthogonal to udir\n                            orth_comp = np.zeros(dim)\n                            used = 0\n                            for row in Vt[:min(3, Vt.shape[0])]:\n                                v = row - np.dot(row, udir) * udir\n                                nrm = np.linalg.norm(v)\n                                if nrm > 1e-12:\n                                    orth_comp += (v / nrm) * rng.normal()\n                                    used += 1\n                            if used == 0:\n                                orth_comp = rng.normal(size=dim)\n                            orth_comp = orth_comp / (np.linalg.norm(orth_comp) + 1e-12)\n                            orth_scale = self.sigma * (0.4 + 1.2 * rng.random()) * min(1.2, un / (0.3 * np.mean(width) + 1e-12))\n                            # along-edge small jitter\n                            along_jitter = (alpha - 0.5) * un * rng.normal(loc=0.8, scale=0.5)\n                            x_candidate = base + along_jitter * udir + orth_scale * orth_comp\n                            # record which edge was used for possible update after evaluation\n                            used_edge_idx = eidx\n\n                    elif len(self.triangles) > 0 and rng.random() < p_tri / (p_mesh + 1e-12):\n                        # Triangle barycentric sampling\n                        tidx = choose_triangle_idx()\n                        if tidx is None:\n                            x_candidate = rng.uniform(lb, ub)\n                        else:\n                            i, j, k = self.triangles[tidx]\n                            Xi, Xj, Xk = pop_X[i], pop_X[j], pop_X[k]\n                            # Dirichlet(\u03b1) biasing toward vertices (\u03b1 < 1 => corners more likely), use >1 to favor interior sometimes\n                            alpha0 = 1.1 + 0.9 * rng.random()\n                            bary = rng.random(3)\n                            bary = bary / bary.sum()\n                            # skew barycentric to favor better vertices lightly\n                            fitness = np.array([pop_f[i], pop_f[j], pop_f[k]])\n                            invf = (np.max(fitness) - fitness) + 1e-9\n                            invw = invf / (invf.sum() + 1e-12)\n                            bary = 0.6 * bary + 0.4 * invw\n                            bary = bary / bary.sum()\n                            base = bary[0] * Xi + bary[1] * Xj + bary[2] * Xk\n                            # orth noise in plane normal or in local PCA orthsubspace\n                            P = np.vstack([Xi, Xj, Xk])\n                            P_centered = P - P.mean(axis=0)\n                            # get normal via SVD (smallest singular vector)\n                            if P_centered.shape[0] >= 3 and np.linalg.matrix_rank(P_centered) >= 2:\n                                U, S, Vt = np.linalg.svd(P_centered, full_matrices=False)\n                                normal = Vt[-1]\n                                normal = normal / (np.linalg.norm(normal) + 1e-12)\n                            else:\n                                normal = rng.normal(size=dim)\n                                normal = normal / (np.linalg.norm(normal) + 1e-12)\n                            orth_scale = self.sigma * (0.6 + 1.1 * rng.random()) * 0.6\n                            x_candidate = base + orth_scale * rng.normal() * normal\n                            used_triangle_idx = tidx\n                    else:\n                        # fallback uniform\n                        x_candidate = rng.uniform(lb, ub)\n\n                elif choice < p_mesh + p_edge_walk and len(self.edges) > 0:\n                    # Start a short edge-walk from a sampled edge (intensification)\n                    eidx = choose_edge_idx()\n                    if eidx is None:\n                        x_candidate = rng.uniform(lb, ub)\n                    else:\n                        i, j = self.edges[eidx]\n                        xi, xj = pop_X[i], pop_X[j]\n                        u = xj - xi\n                        un = np.linalg.norm(u) + 1e-12\n                        udir = u / un\n                        # initial step biased toward better endpoint\n                        better = i if pop_f[i] <= pop_f[j] else j\n                        bias = 0.2 * (pop_X[better] - 0.5 * (xi + xj))\n                        step_along = rng.uniform(-0.08, 0.28) * un\n                        orth_scale = self.sigma * (0.9 + rng.random() * 0.8)\n                        # compute orth basis via PCA\n                        Vt = local_basis_for_edge(i, j)\n                        orth_vec = Vt[0] - np.dot(Vt[0], udir) * udir\n                        if np.linalg.norm(orth_vec) < 1e-12:\n                            orth_vec = rng.normal(size=dim)\n                        orth_vec = orth_vec / (np.linalg.norm(orth_vec) + 1e-12)\n                        x_candidate = 0.5 * (xi + xj) + step_along * udir + orth_scale * rng.normal() * orth_vec + bias\n                        # initialize walk_state for several steps (1-6 steps)\n                        steps = max(1, int(2 + rng.integers(0, 5)))\n                        self.walk_state = {\"edge_idx\": eidx, \"steps_left\": steps - 1, \"initial_steps\": steps, \"last_dir_sign\": rng.choice([-1, 1])}\n                        used_edge_idx = eidx\n\n                elif choice < p_mesh + p_edge_walk + p_local:\n                    # Local anisotropic refinement around current best or a random elite\n                    if pop_X.shape[0] >= min(4, dim):\n                        if rng.random() < 0.72:\n                            center = x_best\n                        else:\n                            center = pop_X[rng.integers(0, pop_X.shape[0])]\n                        # anisotropy from population std\n                        if pop_X.shape[0] >= 3:\n                            stds = np.std(pop_X, axis=0)\n                            stds = np.maximum(stds, 1e-3 * np.mean(width))\n                            anis = stds / (np.mean(stds) + 1e-12)\n                            local_sigma = self.sigma * (0.35 + 0.85 * rng.random())\n                            x_candidate = center + local_sigma * (rng.normal(size=dim) * anis)\n                        else:\n                            x_candidate = center + self.sigma * rng.normal(size=dim)\n                    else:\n                        x_candidate = x_best + self.sigma * rng.normal(size=dim)\n\n                else:\n                    # global exploration: opposite sampling of random elite or uniform\n                    if rng.random() < 0.7 and pop_X.shape[0] > 0:\n                        idx = rng.integers(0, pop_X.shape[0])\n                        x0 = pop_X[idx]\n                        opp = lb + ub - x0\n                        x_candidate = opp + 0.45 * self.sigma * rng.normal(size=dim)\n                    else:\n                        x_candidate = rng.uniform(lb, ub)\n\n            # clip to domain\n            x_candidate = np.minimum(np.maximum(x_candidate, lb), ub)\n\n            # Evaluate candidate\n            f_candidate = float(func(x_candidate))\n            evals += 1\n\n            # Update archive\n            X_archive = np.vstack([X_archive, x_candidate])\n            f_archive = np.concatenate([f_archive, [f_candidate]])\n\n            improved = False\n            if f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = x_candidate.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # Update sigma adaptively\n            if improved:\n                self.sigma = max(self.sigma_min, self.sigma * self.success_shrink)\n            else:\n                self.sigma = min(self.sigma_max, self.sigma * self.failure_expand)\n\n            # Update compact population: keep best pop_size\n            if pop_X.shape[0] < self.pop_size:\n                pop_X = np.vstack([pop_X, x_candidate])\n                pop_f = np.concatenate([pop_f, [f_candidate]])\n            else:\n                # if candidate is better than worst, replace\n                worst_idx = int(np.argmax(pop_f))\n                if f_candidate < pop_f[worst_idx]:\n                    pop_X[worst_idx] = x_candidate\n                    pop_f[worst_idx] = f_candidate\n                else:\n                    # occasional random replacement to maintain diversity\n                    if rng.random() < 0.02:\n                        ridx = rng.integers(0, pop_X.shape[0])\n                        pop_X[ridx] = x_candidate\n                        pop_f[ridx] = f_candidate\n\n            # If last candidate was generated from an edge (used_edge_idx variable), update edge_scores\n            # Note: use try/except to avoid referencing undefined variable\n            try:\n                if 'used_edge_idx' in locals():\n                    eidx = used_edge_idx\n                    if 0 <= eidx < len(self.edges):\n                        if improved:\n                            # reward edge\n                            self.edge_scores[eidx] += 1.0 + 0.6 * (1.0 / (1.0 + abs(f_candidate - f_best)))\n                        else:\n                            # slight decay to avoid runaway\n                            self.edge_scores[eidx] *= 0.985\n                    del used_edge_idx\n            except Exception:\n                pass\n\n            # similarly, triangle update (not tracked persistently, but can nudge sigma)\n            try:\n                if 'used_triangle_idx' in locals():\n                    if improved:\n                        # reduce sigma a bit to refine interior\n                        self.sigma = max(self.sigma_min, self.sigma * 0.9)\n                    del used_triangle_idx\n            except Exception:\n                pass\n\n            # bound archive size\n            if X_archive.shape[0] > self.max_archive:\n                keep_best = int(self.max_archive * 0.6)\n                best_inds = np.argsort(f_archive)[:keep_best]\n                other_pool = np.setdiff1d(np.arange(X_archive.shape[0]), best_inds, assume_unique=False)\n                n_other = self.max_archive - keep_best\n                if other_pool.size > 0:\n                    other_inds = rng.choice(other_pool, size=n_other, replace=False)\n                else:\n                    other_inds = np.array([], dtype=int)\n                keep = np.concatenate([best_inds, other_inds])\n                X_archive = X_archive[keep]\n                f_archive = f_archive[keep]\n\n            # periodically refresh population and graph\n            if (evals % self.edge_refresh) == 0 or pop_X.shape[0] < 2:\n                pop_X, pop_f, pop_inds = build_population(X_archive, f_archive)\n                # if best moved, update x_best reference\n                best_idx = int(np.argmin(pop_f))\n                x_best = pop_X[best_idx].copy()\n                f_best = float(pop_f[best_idx])\n                refresh_graph()\n                # if edges exist and some have historical scores, normalize scores\n                if self.edge_scores is not None and self.edge_scores.size > 0:\n                    # smooth historical counts to probabilities scale\n                    self.edge_scores = self.edge_scores / (np.max(self.edge_scores) + 1e-12)\n\n            # emergency diversification: long stagnation -> inject a heavy global candidate (but check budget)\n            if iter_since_improve > max(80, 6 * dim) and rng.random() < 0.16 and evals < budget:\n                # temporary boost sigma\n                old_sigma = self.sigma\n                self.sigma = min(self.sigma_max, self.sigma * (1.8 + rng.random()))\n                xg = rng.uniform(lb, ub)\n                fg = float(func(xg))\n                evals += 1\n                X_archive = np.vstack([X_archive, xg])\n                f_archive = np.concatenate([f_archive, [fg]])\n                if fg < f_best:\n                    f_best = float(fg)\n                    x_best = xg.copy()\n                    iter_since_improve = 0\n                # possibly replace a worst population member\n                if pop_X.shape[0] >= 1 and fg < pop_f.max():\n                    worst_idx = int(np.argmax(pop_f))\n                    pop_X[worst_idx] = xg\n                    pop_f[worst_idx] = fg\n                self.sigma = old_sigma  # restore\n                # refresh graph if we used more than one eval in this branch\n                if (evals % self.edge_refresh) == 0:\n                    pop_X, pop_f, pop_inds = build_population(X_archive, f_archive)\n                    refresh_graph()\n\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 5, "feedback": "The algorithm EdgeMeshAdaptiveManifoldWalks scored 0.214 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["701bfc7e-c1fd-40fc-8533-beb7ddd599a9"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.20596794207684443}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.24733053665703364}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.2227637010774599}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02197989868056338}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.056095286766847496}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.04505387831926577}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0069501397015343525}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.009834482297760738}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.023659571677269065}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.8382849384050339}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.789482422610246}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.7447505805555921}], "aucs": [0.20596794207684443, 0.24733053665703364, 0.2227637010774599, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.02197989868056338, 0.056095286766847496, 0.04505387831926577, 0.0069501397015343525, 0.009834482297760738, 0.023659571677269065, 0.8382849384050339, 0.789482422610246, 0.7447505805555921]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4805.0, "Edges": 4804.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9995837669094694, "Degree Variance": 2.078251647769785, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 9.395201448619284, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.327011447838678, "Depth Entropy": 2.3637830147366716, "Assortativity": 0.0, "Average Eccentricity": 20.06493236212279, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.0002081165452653486, "Average Shortest Path": 11.884194146223967, "mean_complexity": 11.375, "total_complexity": 91.0, "mean_token_count": 530.75, "total_token_count": 4246.0, "mean_parameter_count": 2.0, "total_parameter_count": 16.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "45bacdb1-c496-4bcb-8a7b-2ccd06451f70", "fitness": 0.37554076889135085, "name": "DenseEdgeSurrogateDensification", "description": "Dense Edge Surrogate Densification (DESD) \u2014 aggressively densify the elite edge graph by generating a large pool of edge/triangle/multi-edge mixes, score them cheaply with a local linear surrogate in a PCA subspace, then evaluate only the most promising candidates to exploit very high \"edge density\" while keeping evaluations limited.", "code": "import numpy as np\n\nclass DenseEdgeSurrogateDensification:\n    \"\"\"\n    Dense Edge Surrogate Densification (DESD)\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Optional args:\n      pop_size: initial population size (clipped to budget)\n      elite_frac: fraction of population considered elite\n      k_neighbors: how many nearest neighbors to connect per elite\n      proposals_per_cycle: number of actual function evaluations attempted per cycle\n      pool_multiplier: how many virtual candidates to generate per actual evaluation (controls edge density)\n      tri_prob: probability to prefer triangle/multi-node barycentric proposals\n      mix_prob: probability to produce multi-edge mix (combining >2 nodes)\n      levy_prob: probability for heavy-tailed Cauchy jump\n      trust_init: initial trust radius fraction of search range\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.20,\n                 k_neighbors=8, proposals_per_cycle=40, pool_multiplier=8,\n                 tri_prob=0.5, mix_prob=0.25, levy_prob=0.06,\n                 trust_init=0.14, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.pool_multiplier = int(pool_multiplier)\n        self.tri_prob = float(tri_prob)\n        self.mix_prob = float(mix_prob)\n        self.levy_prob = float(levy_prob)\n        self.trust_init = float(trust_init)\n\n        # population sizing default heuristic\n        if pop_size is None:\n            base = int(np.clip(np.sqrt(self.budget) * 0.95, 8, 160))\n            self.pop_size = max(base, self.dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation constants\n        self.trust_expand = 1.22\n        self.trust_shrink = 0.72\n        self.edge_boost = 1.5\n        self.edge_decay = 0.985\n        self.min_trust = 1e-6\n        self.max_trust = 10.0\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        trust = float(self.trust_init) * range_norm\n\n        # ensure pop_size reasonable\n        pop_size = min(self.pop_size, max(4, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population and evaluate\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i])); evals += 1\n\n        # edge weight matrix (symmetric)\n        edge_w = np.ones((self.pop_size, self.pop_size), dtype=float)\n        np.fill_diagonal(edge_w, 0.0)\n\n        # bookkeeping\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        cycle_no_improve = 0\n        cycle = 0\n\n        # main search\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            # determine how many actual evaluations to do this cycle\n            act_proposals = min(self.proposals_per_cycle, remaining)\n            # create a candidate pool much larger than act_proposals to densify edges\n            pool_size = max(act_proposals * self.pool_multiplier, act_proposals + 8)\n            pool_size = int(pool_size)\n\n            improved_this_cycle = False\n\n            # decay edges slightly\n            edge_w *= self.edge_decay\n            edge_w[edge_w < 1e-12] = 1e-12\n            np.fill_diagonal(edge_w, 0.0)\n\n            # compute elites and distance matrix\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            order = np.argsort(pop_f)\n            elites = order[:elite_count]\n\n            dif = pop[:, None, :] - pop[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n\n            # densify edges: clique among elites + k neighbors per elite + many near-elite cross-links\n            edges_set = set()\n            for i in range(len(elites)):\n                for j in range(i+1, len(elites)):\n                    a = int(elites[i]); b = int(elites[j])\n                    edges_set.add((min(a,b), max(a,b)))\n            # k nearest for each elite\n            for a in elites:\n                neigh = np.argsort(dist2[a])\n                cnt = 0\n                for nb in neigh:\n                    if nb == a: continue\n                    edges_set.add((min(a,int(nb)), max(a,int(nb))))\n                    cnt += 1\n                    if cnt >= self.k_neighbors: break\n            # additional near-elite cross links to increase density\n            for a in elites:\n                # connect to close nodes beyond k_neighbors as well\n                neigh = np.argsort(dist2[a])\n                for nb in neigh[self.k_neighbors:self.k_neighbors+ (self.k_neighbors//2 + 1)]:\n                    if nb == a: continue\n                    edges_set.add((min(a,int(nb)), max(a,int(nb))))\n\n            # some random cross edges for exploration connectivity\n            rand_pairs = max(3, self.pop_size // 2)\n            for _ in range(rand_pairs):\n                a = int(rng.integers(0, self.pop_size)); b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges_set.add((min(a,b), max(a,b)))\n\n            edges = list(edges_set)\n            if len(edges) == 0:\n                edges = [(0, 1)]\n\n            # Build lightweight PCA + linear surrogate from elite set (cheap, uses only evaluated points)\n            # center using elites; if too few elites fall back to whole population\n            try:\n                basis_count = max(1, min(dim, max(1, elite_count - 1)))\n                X_base = pop[elites].astype(float)\n                Xc = X_base - X_base.mean(axis=0, keepdims=True)\n                # SVD\n                U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                # choose m components\n                m = min(basis_count, Vt.shape[0])\n                V = Vt[:m].copy()\n                # project elites into PCA subspace\n                Z = (Xc @ V.T)\n                # target values (use elites' fitness)\n                y = pop_f[elites].astype(float)\n                # solve ridge regression in subspace with intercept\n                Z_aug = np.hstack([np.ones((Z.shape[0], 1)), Z])\n                alpha = 1e-6 * (1.0 + np.var(y))\n                # (ZtZ + alpha I) w = Zt y\n                A = Z_aug.T @ Z_aug\n                A[np.diag_indices_from(A)] += alpha\n                w = np.linalg.solve(A, Z_aug.T @ y)\n                use_pca = True\n            except Exception:\n                # fallback: no PCA\n                V = None\n                Z = None\n                Z_aug = None\n                w = None\n                use_pca = False\n\n            # generate a large pool of candidate points (virtual edge density)\n            pool = np.empty((pool_size, dim), dtype=float)\n            pool_meta = []  # keep metadata like which nodes contributed for weight update\n            for i in range(pool_size):\n                r = rng.random()\n                meta = {}\n                if use_pca and r < self.tri_prob and len(elites) >= 3:\n                    # triangle barycentric among elites or mixed\n                    if rng.random() < 0.85:\n                        ids = rng.choice(elites, size=3, replace=False)\n                    else:\n                        ids = rng.choice(self.pop_size, size=3, replace=False)\n                    a, b, c = [int(x) for x in ids]\n                    Xa, Xb, Xc = pop[a], pop[b], pop[c]\n                    # Dirichlet-like weights\n                    wdir = rng.gamma(0.9, 1.0, size=3)\n                    wdir = wdir / (np.sum(wdir) + 1e-12)\n                    x = wdir[0]*Xa + wdir[1]*Xb + wdir[2]*Xc\n                    # modest extrapolation and orthogonal noise\n                    if rng.random() < 0.25:\n                        centroid = (Xa + Xb + Xc) / 3.0\n                        factor = rng.uniform(-0.6, 1.3)\n                        x = centroid + factor * (x - centroid)\n                    tri_spread = (np.linalg.norm(Xa-Xb)+np.linalg.norm(Xa-Xc)+np.linalg.norm(Xb-Xc)) / (3.0 + 1e-12)\n                    noise_scale = 0.6 * max(trust, 0.4 * tri_spread)\n                    x = x + noise_scale * rng.standard_normal(dim) * 0.8\n                    meta['type'] = 'tri'; meta['ids'] = (a,b,c)\n                elif r < self.tri_prob + self.mix_prob and self.pop_size >= 4:\n                    # multi-edge mix: combine 3-5 nodes (dense interpolation)\n                    k = rng.integers(3, min(6, self.pop_size) + 1)\n                    ids = rng.choice(self.pop_size, size=k, replace=False)\n                    coeff = rng.random(k)\n                    coeff /= coeff.sum()\n                    x = np.sum(pop[ids] * coeff[:, None], axis=0)\n                    # add small trust-based perturbation\n                    x = x + 0.45 * trust * rng.standard_normal(dim)\n                    meta['type'] = 'mix'; meta['ids'] = tuple(int(xi) for xi in ids)\n                else:\n                    # edge interpolation/extrapolation\n                    e_idx = rng.integers(0, len(edges))\n                    a, b = edges[e_idx]\n                    Xa, Xb = pop[a], pop[b]\n                    # bias towards better endpoint\n                    if pop_f[a] < pop_f[b]:\n                        t = rng.beta(2.2, 1.1)\n                    else:\n                        t = 1.0 - rng.beta(2.2, 1.1)\n                    if rng.random() < 0.12:\n                        t = rng.uniform(-0.8, 1.8)\n                    x = (1-t)*Xa + t*Xb\n                    # orthogonal perturbation plus along-edge jitter\n                    edge_dir = Xb - Xa\n                    nedge = np.linalg.norm(edge_dir)\n                    if nedge > 1e-12:\n                        unit = edge_dir / nedge\n                        x = x + 0.05 * trust * rng.standard_normal() * unit\n                    x = x + 0.6 * trust * rng.standard_normal(dim)\n                    meta['type'] = 'edge'; meta['ids'] = (int(a), int(b))\n                # occasional Levy-like jump for pool diversity (not too large)\n                if rng.random() < self.levy_prob * 0.8:\n                    try:\n                        cauch = rng.standard_cauchy(dim)\n                    except AttributeError:\n                        u = rng.random(dim); cauch = np.tan(np.pi * (u - 0.5))\n                    x = x + 0.08 * range_norm * cauch\n                # clip and store\n                pool[i] = np.clip(x, lb, ub)\n                pool_meta.append(meta)\n\n            # cheap scoring using surrogate (if available) to pick the most promising ones\n            if use_pca and w is not None:\n                # project pool into PCA subspace\n                Xcent = pool - X_base.mean(axis=0, keepdims=True)\n                Zpool = Xcent @ V.T\n                Zpool_aug = np.hstack([np.ones((Zpool.shape[0],1)), Zpool])\n                preds = Zpool_aug @ w  # predicted fitness (lower is better)\n            else:\n                # fallback: estimate by distance to best + weighted averaging with pop_f of nearest neighbors\n                # this is cheap and correlates with quality\n                d2best = np.sum((pool - x_best)**2, axis=1)\n                # compute nearest neighbor fitness among pop (approx)\n                # use a small random subset of pop to speed\n                idx_sub = rng.choice(self.pop_size, size=min(self.pop_size, 12), replace=False)\n                d2sub = np.sum((pool[:, None, :] - pop[idx_sub][None, :, :])**2, axis=2)\n                min_idx = np.argmin(d2sub, axis=1)\n                nn_f = pop_f[idx_sub][min_idx]\n                preds = 0.6 * (nn_f) + 0.4 * (f_best + 0.5 * np.sqrt(d2best + 1e-12))\n\n            # select top-K predicted candidates to evaluate (K = act_proposals), but guard by remaining\n            K = min(act_proposals, self.budget - evals)\n            if K <= 0:\n                break\n            order_pool = np.argsort(preds)\n            selected_idxs = order_pool[:K]\n\n            # evaluate selected candidates (actual expensive function calls)\n            for idx in selected_idxs:\n                if evals >= self.budget:\n                    break\n                x_prop = pool[int(idx)].copy()\n                f_prop = float(func(x_prop)); evals += 1\n\n                # replacement strategy: replace a nearby worse individual, otherwise replace global worst if better\n                d2 = np.sum((pop - x_prop)**2, axis=1)\n                nearest = np.argsort(d2)[:min(6, self.pop_size)]\n                # prefer replacing worse nearest\n                replaced_idx = -1\n                # sort nearest by fitness descending to attempt replace worst among them first\n                nearest_sorted = nearest[np.argsort(pop_f[nearest])[::-1]]\n                for cand in nearest_sorted:\n                    if f_prop < pop_f[cand]:\n                        replaced_idx = int(cand); break\n                if replaced_idx >= 0:\n                    pop[replaced_idx] = x_prop.copy()\n                    pop_f[replaced_idx] = f_prop\n                    # boost edges between replaced node and nodes used to create candidate\n                    meta = pool_meta[int(idx)]\n                    if 'ids' in meta and len(meta['ids']) >= 2:\n                        ids = tuple(int(i) for i in meta['ids'])\n                        # boost pairwise among ids and replaced node\n                        for a in ids:\n                            if a == replaced_idx: continue\n                            i1, i2 = min(a, replaced_idx), max(a, replaced_idx)\n                            edge_w[i1, i2] = edge_w[i1, i2] * self.edge_boost + 1e-12\n                            edge_w[i2, i1] = edge_w[i1, i2]\n                    # also boost edges among the contributing ids for this candidate (dense)\n                    if 'ids' in meta and len(meta['ids']) >= 2:\n                        ids = [int(x) for x in meta['ids']]\n                        for ii in range(len(ids)):\n                            for jj in range(ii+1, len(ids)):\n                                a, b = ids[ii], ids[jj]\n                                if a == b: continue\n                                i1, i2 = min(a,b), max(a,b)\n                                edge_w[i1, i2] = edge_w[i1, i2] * (1.0 + 0.5*(1.0/(1.0+pop_f[a]-pop_f[b] if abs(pop_f[a]-pop_f[b])>1e-12 else 1.0))) + 1e-12\n                                edge_w[i2, i1] = edge_w[i1, i2]\n                    # adapt trust upward\n                    trust = min(self.max_trust, trust * self.trust_expand)\n                    improved_this_cycle = True\n                else:\n                    # maybe replace global worst if improved\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        pop[idx_worst] = x_prop.copy()\n                        pop_f[idx_worst] = f_prop\n                        trust = min(self.max_trust, trust * self.trust_expand)\n                        improved_this_cycle = True\n                    else:\n                        # unsuccessful -> shrink trust a bit\n                        trust = max(self.min_trust, trust * self.trust_shrink)\n\n                # update best\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n\n                # keep edge matrix symmetric and non-zero diagonal zero\n                np.fill_diagonal(edge_w, 0.0)\n                edge_w[edge_w < 1e-12] = 1e-12\n\n                # if budget low, break early\n                if evals >= self.budget:\n                    break\n\n            # cycle end: stagnation handling and occasional global injections\n            if improved_this_cycle:\n                cycle_no_improve = 0\n            else:\n                cycle_no_improve += 1\n                # when stagnant, perform edge burst: evaluate a few midpoints on the currently strongest edges\n                if cycle_no_improve >= 3 and evals < self.budget:\n                    # find top edges by weight\n                    tri = np.triu_indices(self.pop_size, k=1)\n                    weights = edge_w[tri]\n                    if weights.size > 0:\n                        topk = max(2, min(8, weights.size))\n                        top_idx = np.argsort(weights)[-topk:]\n                        pairs = [(int(tri[0][i]), int(tri[1][i])) for i in top_idx]\n                        # evaluate midpoints of these edges (but limited by remaining evals)\n                        for (a,b) in pairs:\n                            if evals >= self.budget: break\n                            x_mid = 0.5*(pop[a] + pop[b]) + 0.35 * trust * rng.standard_normal(dim)\n                            x_mid = np.clip(x_mid, lb, ub)\n                            f_mid = float(func(x_mid)); evals += 1\n                            # replace worst if better\n                            idx_w = int(np.argmax(pop_f))\n                            if f_mid < pop_f[idx_w]:\n                                pop[idx_w] = x_mid.copy(); pop_f[idx_w] = f_mid\n                                # boost that edge strongly\n                                i1, i2 = min(a,b), max(a,b)\n                                edge_w[i1,i2] = edge_w[i1,i2] * (1.0 + self.edge_boost)\n                                edge_w[i2,i1] = edge_w[i1,i2]\n                                if f_mid < f_best:\n                                    f_best = f_mid; x_best = x_mid.copy()\n                        # after burst, shrink trust slightly to focus\n                        trust = max(self.min_trust, trust * 0.86)\n                    cycle_no_improve = 0\n\n            # if only few evaluations remain, break to final refinement\n            if self.budget - evals <= max(8, dim*2):\n                break\n\n        # final local refinement: coordinate search with decreasing step and occasional random restarts around elites\n        step = 0.25 * range_norm\n        step_min = max(1e-8, 1e-6 * range_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            # iterate through coordinates but randomize order to avoid bias\n            order_dims = list(range(dim))\n            rng.shuffle(order_dims)\n            for d in order_dims:\n                if evals >= self.budget: break\n                # positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                # negative\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n            else:\n                # if improved, slightly enlarge step to accelerate downhill\n                step = min(0.8 * range_norm, step * 1.1)\n\n        # record best\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DenseEdgeSurrogateDensification scored 0.376 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dad14baa-7d46-4686-8ce1-6aaea4db1ad0"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8131753127529074}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8063109226306261}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7970303272697449}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08366433476454271}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.060111332845149246}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.06636865929581237}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.04436342292911344}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0560991829921339}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.03942068196622406}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9497194235374943}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9478160086861575}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.968881923700358}], "aucs": [0.8131753127529074, 0.8063109226306261, 0.7970303272697449, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.08366433476454271, 0.060111332845149246, 0.06636865929581237, 0.04436342292911344, 0.0560991829921339, 0.03942068196622406, 0.9497194235374943, 0.9478160086861575, 0.968881923700358]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3906.0, "Edges": 3905.0, "Max Degree": 36.0, "Min Degree": 1.0, "Mean Degree": 1.9994879672299026, "Degree Variance": 2.268816942123517, "Transitivity": 0.0, "Max Depth": 21.0, "Min Depth": 2.0, "Mean Depth": 8.973830734966592, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3231519732510106, "Depth Entropy": 2.277157131221162, "Assortativity": 0.0, "Average Eccentricity": 23.239119303635434, "Diameter": 30.0, "Radius": 15.0, "Edge Density": 0.0002560163850486431, "Average Shortest Path": 11.326478519209097, "mean_complexity": 27.0, "total_complexity": 81.0, "mean_token_count": 1131.3333333333333, "total_token_count": 3394.0, "mean_parameter_count": 5.666666666666667, "total_parameter_count": 17.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "03ee2626-c735-4b50-a8f0-b9bb3e0f54ad", "fitness": 0.26937039276836316, "name": "EdgeTriangulationDenseAdaptivePrune", "description": "Densely-connected Edge-Triangle Adaptive Prune (DE-TAP) \u2014 aggressively densify an elite edge+triangle graph, reinforce productive edges, and generate many barycentric, edge, PCA-guided and opposition proposals inside an adaptive trust radius to intensify exploitation while preserving rare heavy-tailed escapes for exploration.", "code": "import numpy as np\n\nclass EdgeTriangulationDenseAdaptivePrune:\n    \"\"\"\n    Edge-Triangle Dense Adaptive Prune (DE-TAP)\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Optional args:\n      pop_size: initial population size (clipped to budget)\n      elite_frac: fraction of population considered elite\n      k_neighbors: nearest neighbors per elite for edge building\n      proposals_per_cycle: base number of proposals per cycle\n      tri_prob: base probability to use triangle barycentric proposals\n      pca_prob: probability to use PCA-guided subspace proposals\n      opp_prob: probability to try an opposition-based candidate\n      levy_prob: probability for heavy-tailed Cauchy jump on a proposal\n      trust_init: initial trust radius fraction of search range\n      densify_factor: multiplier to increase edge density around elites\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.25,\n                 k_neighbors=6, proposals_per_cycle=60, tri_prob=0.5,\n                 pca_prob=0.12, opp_prob=0.06, levy_prob=0.06, trust_init=0.16,\n                 densify_factor=2.0, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.tri_prob = float(tri_prob)\n        self.pca_prob = float(pca_prob)\n        self.opp_prob = float(opp_prob)\n        self.levy_prob = float(levy_prob)\n        self.trust_init = float(trust_init)\n        self.densify_factor = float(densify_factor)\n\n        # population sizing\n        if pop_size is None:\n            base = int(np.clip(np.sqrt(self.budget) * 1.2, 10, 160))\n            self.pop_size = max(base, self.dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation constants\n        self.trust_expand = 1.22\n        self.trust_shrink = 0.72\n        self.edge_reward_base = 1.3\n        self.edge_decay = 0.985\n        self.min_trust = 1e-6\n        self.max_trust = 5.0\n\n    def _get_bounds(self, func):\n        # try to read bounds if provided; else default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        # scale range and initial trust radius (absolute scale)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        trust = float(self.trust_init) * range_norm\n\n        # ensure pop_size <= budget and at least 3\n        pop_size = min(self.pop_size, max(3, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population uniformly in bounds and evaluate\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # symmetric edge weights matrix\n        edge_w = np.ones((self.pop_size, self.pop_size), dtype=float)\n        np.fill_diagonal(edge_w, 0.0)\n\n        # bookkeeping\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        # archive of top-k distinct solutions (for diversity/opposition sampling)\n        archive_size = max(6, int(np.ceil(0.08 * self.pop_size)))\n        archive_idx = list(np.argsort(pop_f)[:archive_size])\n\n        cycles_no_improve = 0\n        cycle = 0\n\n        # main loop\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            # scale proposals to remaining budget (avoid overshooting)\n            proposals = min(self.proposals_per_cycle, max(1, remaining // 3))\n            # densify proposals near elites: multiply attempts if many elites\n            proposals = int(proposals * min(2.5, 1.0 + self.densify_factor * (self.elite_frac)))\n            proposals = max(1, proposals)\n\n            improved_this_cycle = False\n\n            # slight decay of edge weights to forget old signals\n            edge_w *= self.edge_decay\n            edge_w[edge_w < 1e-12] = 1e-12\n            np.fill_diagonal(edge_w, 0.0)\n\n            # ranking and neighborhoods\n            elite_count = max(3, int(np.ceil(self.elite_frac * self.pop_size)))\n            order = np.argsort(pop_f)\n            elites = order[:elite_count]\n\n            # pairwise squared distances\n            dif = pop[:, None, :] - pop[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            dist = np.sqrt(dist2 + 1e-12)\n\n            # Build a very dense edge set around elites\n            edges_set = set()\n            # clique among elites\n            for i in range(len(elites)):\n                for j in range(i+1, len(elites)):\n                    a = int(elites[i]); b = int(elites[j])\n                    edges_set.add((min(a,b), max(a,b)))\n            # neighbors and neighbors-of-neighbors\n            for a in elites:\n                neigh = np.argsort(dist[a])  # closest first (includes itself)\n                cnt = 0\n                for nb in neigh:\n                    if nb == a: continue\n                    edges_set.add((min(int(a), int(nb)), max(int(a), int(nb))))\n                    cnt += 1\n                    if cnt >= self.k_neighbors * int(self.densify_factor):\n                        break\n                # connect neighbors pairwise to densify local subgraph\n                klist = neigh[1:1 + max(6, self.k_neighbors)]\n                for i in range(len(klist)):\n                    for j in range(i+1, len(klist)):\n                        a2 = int(klist[i]); b2 = int(klist[j])\n                        if a2 != b2:\n                            edges_set.add((min(a2,b2), max(a2,b2)))\n\n            # add edges between near pairs globally up to a threshold to capture geometry\n            # threshold = small quantile of all pair distances to limit too many edges\n            flat_dists = dist[np.triu_indices(self.pop_size, k=1)]\n            if flat_dists.size > 0:\n                thr = float(np.quantile(flat_dists, min(0.35, 0.35 * (1.0 + self.densify_factor/2.0))))\n                # add all pairs closer than thr\n                close_pairs = np.argwhere((dist <= thr) & (np.triu(np.ones_like(dist), k=1) == 1))\n                for pair in close_pairs:\n                    a,b = int(pair[0]), int(pair[1])\n                    edges_set.add((min(a,b), max(a,b)))\n\n            # a small number of random edges to keep exploration connected\n            rand_pairs = max(4, self.pop_size // 2)\n            for _ in range(rand_pairs):\n                a = int(rng.integers(0, self.pop_size)); b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges_set.add((min(a,b), max(a,b)))\n\n            edges = list(edges_set)\n            if len(edges) == 0:\n                edges = [(0,1)]\n\n            # precompute edge probabilities biased by edge weight and endpoint quality\n            edge_weights = np.array([edge_w[a,b] for (a,b) in edges], dtype=float)\n            # boost edges that connect to elites more strongly\n            elite_mask = np.array([(1 if (a in elites or b in elites) else 0) for (a,b) in edges], dtype=float)\n            edge_weights *= (1.0 + 1.3 * elite_mask)\n            # normalize into probabilities\n            if edge_weights.sum() <= 0:\n                edge_probs = np.full(len(edges), 1.0/len(edges))\n            else:\n                edge_probs = edge_weights / edge_weights.sum()\n\n            # population fitness scale useful for reward shaping\n            pop_f_std = float(np.std(pop_f)) + 1e-9\n            pop_f_mean = float(np.mean(pop_f))\n\n            # proposals loop\n            for _ in range(proposals):\n                if evals >= self.budget:\n                    break\n\n                mode_rand = rng.random()\n                # opposition sampling: reflect a chosen archive member across midpoint of bounds\n                if mode_rand < self.opp_prob and len(archive_idx) > 0:\n                    idx = int(rng.choice(archive_idx))\n                    x0 = pop[idx]\n                    x_opp = lb + ub - x0  # simple opposition\n                    # small jitter proportional to trust\n                    x_prop = x_opp + 0.6 * trust * rng.standard_normal(dim)\n                else:\n                    # choose between triangle, PCA-guided, or edge\n                    if mode_rand < (self.opp_prob + self.pca_prob):\n                        # PCA-guided: pick an elite, compute local PCA on its neighborhood and propose along top components\n                        e = int(rng.choice(elites))\n                        # neighborhood indices including e\n                        neigh = np.argsort(dist[e])[:min(1 + 6 + self.k_neighbors, self.pop_size)]\n                        X = pop[neigh] - pop[e]  # center at elite\n                        # compute covariance and principal directions (robust small-n handling)\n                        if X.shape[0] > 1:\n                            C = (X.T @ X) / (X.shape[0])\n                            try:\n                                eigvals, eigvecs = np.linalg.eigh(C)\n                                # take top 2 principal directions (if dim >=2)\n                                topk = min(2, dim)\n                                dirs = eigvecs[:, -topk:]  # shape dim x topk\n                                coeffs = rng.standard_normal(topk) * (0.8 * trust)\n                                move = dirs @ coeffs\n                            except Exception:\n                                move = 0.8 * trust * rng.standard_normal(dim)\n                        else:\n                            move = 0.8 * trust * rng.standard_normal(dim)\n                        # small bias toward elite position\n                        x_prop = pop[e] + move\n                    else:\n                        # choose triangle barycentric with higher probability or edge interpolation\n                        if rng.random() < self.tri_prob and self.pop_size >= 3 and len(edges) >= 2:\n                            # sample triangle preferably among elites or high-degree nodes\n                            if rng.random() < 0.85:\n                                # pick one elite and two neighbors (if possible)\n                                main = int(rng.choice(elites))\n                                neigh = np.argsort(dist[main])\n                                cand = [main]\n                                i_idx = 1\n                                while len(cand) < 3 and i_idx < len(neigh):\n                                    if neigh[i_idx] != main: cand.append(int(neigh[i_idx]))\n                                    i_idx += 1\n                                if len(cand) < 3:\n                                    # fallback to random sample\n                                    cand = list(rng.choice(self.pop_size, size=3, replace=False))\n                                idxs = cand[:3]\n                            else:\n                                idxs = list(rng.choice(self.pop_size, size=3, replace=False))\n                            a,b,c = int(idxs[0]), int(idxs[1]), int(idxs[2])\n                            Xa, Xb, Xc = pop[a], pop[b], pop[c]\n\n                            # Dirichlet-like biased to corners but with occasional centroid pull\n                            w = rng.gamma(0.9, 1.0, size=3)\n                            w = w / (np.sum(w) + 1e-12)\n                            x_prop = w[0]*Xa + w[1]*Xb + w[2]*Xc\n\n                            # stronger densification: also try midpoint spur proposals (no extra evals)\n                            if rng.random() < 0.28:\n                                # small extrapolation toward centroid\n                                centroid = (Xa + Xb + Xc) / 3.0\n                                alpha = rng.uniform(-0.6, 1.1)\n                                x_prop = centroid + alpha * (centroid - x_prop)\n\n                            # noise scaled by triangle spread and trust\n                            tri_spread = (np.linalg.norm(Xa-Xb)+np.linalg.norm(Xa-Xc)+np.linalg.norm(Xb-Xc)) / (3.0 + 1e-12)\n                            noise_scale = max(0.45*trust, 0.35*tri_spread)\n                            x_prop = x_prop + noise_scale * rng.standard_normal(dim)\n                        else:\n                            # edge interpolation/extrapolation\n                            e_idx = int(rng.choice(len(edges), p=edge_probs))\n                            a,b = edges[e_idx]\n                            Xa, Xb = pop[a], pop[b]\n                            # bias t toward better endpoint\n                            if pop_f[a] < pop_f[b]:\n                                t = rng.beta(2.0, 1.4)\n                            else:\n                                t = 1.0 - rng.beta(2.0, 1.4)\n                            if rng.random() < 0.12:\n                                t = rng.uniform(-0.7, 1.7)\n                            x_prop = (1-t)*Xa + t*Xb\n                            # perturb along orthogonal subspace and small along edge\n                            edge_dir = Xb - Xa\n                            norm_edge = np.linalg.norm(edge_dir)\n                            if norm_edge > 1e-12:\n                                unit = edge_dir / norm_edge\n                                x_prop += 0.06 * trust * rng.standard_normal() * unit\n                                # orthogonal component\n                                orth = rng.standard_normal(dim)\n                                orth -= orth.dot(unit)*unit\n                                x_prop += 0.6 * trust * orth / (np.linalg.norm(orth) + 1e-12) * rng.standard_normal()\n                            else:\n                                x_prop += 0.7 * trust * rng.standard_normal(dim)\n\n                # occasional heavy-tailed jump to escape basins\n                if rng.random() < self.levy_prob:\n                    try:\n                        cauch = rng.standard_cauchy(dim)\n                    except AttributeError:\n                        u = rng.random(dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                    x_prop = x_prop + 0.12 * range_norm * cauch\n\n                # clip and evaluate\n                x_prop = np.clip(x_prop, lb, ub)\n                f_prop = float(func(x_prop))\n                evals += 1\n\n                # choose replacement: prefer worse near neighbors or global worst\n                d2 = np.sum((pop - x_prop)**2, axis=1)\n                nearest = np.argsort(d2)[:min(6, self.pop_size)]\n                # pick a candidate among nearest that is worse\n                replaced_idx = -1\n                # check worst among nearest first\n                nearest_sorted_by_bad = nearest[np.argsort(pop_f[nearest])[::-1]]\n                for idx in nearest_sorted_by_bad:\n                    if f_prop < pop_f[int(idx)]:\n                        replaced_idx = int(idx)\n                        break\n                if replaced_idx >= 0:\n                    old_val = pop_f[replaced_idx]\n                    pop[replaced_idx] = x_prop.copy()\n                    pop_f[replaced_idx] = f_prop\n                    improved_this_cycle = True\n                    # reward edges connecting replaced node with its neighbors (both ways)\n                    # reward proportional to improvement magnitude normalized by pop std\n                    reward = self.edge_reward_base * max(0.01, (old_val - f_prop) / (pop_f_std + 1e-12))\n                    # neighbors to consider\n                    neigh = nearest\n                    for nb in neigh:\n                        if nb == replaced_idx: continue\n                        a,b = min(replaced_idx, int(nb)), max(replaced_idx, int(nb))\n                        edge_w[a,b] += reward\n                        edge_w[b,a] = edge_w[a,b]\n                    # slightly expand trust (successful exploitation)\n                    trust = min(self.max_trust, trust * self.trust_expand)\n                else:\n                    # maybe replace global worst\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        old_val = pop_f[idx_worst]\n                        pop[idx_worst] = x_prop.copy()\n                        pop_f[idx_worst] = f_prop\n                        improved_this_cycle = True\n                        reward = self.edge_reward_base * max(0.01, (old_val - f_prop) / (pop_f_std + 1e-12))\n                        # reward edges connecting worst to its neighbors\n                        neigh = np.argsort(dist[idx_worst])[:min(6, self.pop_size)]\n                        for nb in neigh:\n                            if nb == idx_worst: continue\n                            a,b = min(idx_worst, int(nb)), max(idx_worst, int(nb))\n                            edge_w[a,b] += reward\n                            edge_w[b,a] = edge_w[a,b]\n                        trust = min(self.max_trust, trust * self.trust_expand)\n                    else:\n                        # unsuccessful: shrink trust mildly\n                        trust = max(self.min_trust, trust * self.trust_shrink)\n\n                # update best and archive\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n                    improved_this_cycle = True\n\n                # keep edge matrix reasonable\n                np.fill_diagonal(edge_w, 0.0)\n                edge_w[edge_w < 1e-12] = 1e-12\n\n                # update archive (maintain a sorted list of best indices)\n                combined = np.argsort(pop_f)\n                archive_idx = list(combined[:archive_size])\n\n                if evals >= self.budget:\n                    break\n\n            # end proposals loop\n\n            # stagnation handling: inject multiple opposite and elite-perturb candidates if no improvement\n            if improved_this_cycle:\n                cycles_no_improve = 0\n            else:\n                cycles_no_improve += 1\n                if cycles_no_improve >= 3 and evals < self.budget:\n                    n_inject = min(4, self.budget - evals)\n                    for _ in range(n_inject):\n                        # pick an elite and sample around its opposition + gaussian\n                        e = int(rng.choice(elites))\n                        x_e = pop[e]\n                        x_opp = lb + ub - x_e\n                        cand = x_opp + 0.8 * trust * rng.standard_normal(dim)\n                        cand = np.clip(cand, lb, ub)\n                        f_c = float(func(cand)); evals += 1\n                        idx_w = int(np.argmax(pop_f))\n                        if f_c < pop_f[idx_w]:\n                            pop[idx_w] = cand.copy()\n                            pop_f[idx_w] = f_c\n                            if f_c < f_best:\n                                f_best = f_c; x_best = cand.copy()\n                    # focus more locally afterwards\n                    trust = max(self.min_trust, trust * 0.82)\n                    cycles_no_improve = 0\n\n            # early break to do final refinement if nearly out\n            if self.budget - evals <= max(8, dim * 2):\n                break\n\n        # final local refinement: coordinate & randomized pattern search\n        step = 0.18 * range_norm\n        step_min = max(1e-8, 1e-6 * range_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            # randomize coordinate order for robustness\n            order_coords = list(range(dim))\n            rng.shuffle(order_coords)\n            for d in order_coords:\n                if evals >= self.budget: break\n                # try positive and negative with small random scale too\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n\n        # finalize\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EdgeTriangulationDenseAdaptivePrune scored 0.269 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dad14baa-7d46-4686-8ce1-6aaea4db1ad0"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.6617296977169443}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.638130016695392}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.6612124711416121}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.05792490920080451}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08148124722053274}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08970401065133426}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.01545179788121076}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.033347407985368616}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.03748930773670034}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.6478355156872306}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.3887394362624246}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.7273600733458927}], "aucs": [0.6617296977169443, 0.638130016695392, 0.6612124711416121, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.05792490920080451, 0.08148124722053274, 0.08970401065133426, 0.01545179788121076, 0.033347407985368616, 0.03748930773670034, 0.6478355156872306, 0.3887394362624246, 0.7273600733458927]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3827.0, "Edges": 3826.0, "Max Degree": 35.0, "Min Degree": 1.0, "Mean Degree": 1.9994773974392475, "Degree Variance": 2.2795920968891767, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 9.168487636572744, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3217476362810743, "Depth Entropy": 2.3233207768687594, "Assortativity": 1.2609323703221129e-08, "Average Eccentricity": 19.291873530180297, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0002613012803762738, "Average Shortest Path": 11.368436854216696, "mean_complexity": 25.666666666666668, "total_complexity": 77.0, "mean_token_count": 1108.0, "total_token_count": 3324.0, "mean_parameter_count": 6.0, "total_parameter_count": 18.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "748f3e16-4a79-4143-885d-53f11030d51e", "fitness": 0.1938714063299901, "name": "ACLS_EdgeDense", "description": "ACLS-EdgeDense \u2014 Adaptive Covariance-Step with L\u00e9vy Escapes and high-density elite edge recombination to steer proposals along many interpolations/extrapolations of top solutions while retaining covariance-driven local search and rare heavy-tailed escapes.", "code": "import numpy as np\n\nclass ACLS_EdgeDense:\n    \"\"\"\n    ACLS_EdgeDense: Adaptive Covariance-Step with L\u00e9vy Escapes and Dense Edge Recombination.\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional tunables:\n      - rng: numpy Generator\n      - init_fraction: fraction of budget for initial sampling (default 0.12)\n      - jump_prob: base probability of heavy-tailed jump (default 0.06)\n      - target_success: target success rate for sigma adaptation (default 0.2)\n      - cov_update_rate: covariance learning rate (auto if None)\n      - sigma_initial: initial step-size (if None derived from domain)\n      - stagnation_restart: evals without improvement to trigger diversification\n      - elite_size: number of elites to maintain (default min(12, budget//10))\n      - k_edges: number of neighbors per elite to form dense edge graph (default 4)\n      - edge_proposals: nominal proposals per edge-batch (default 8)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_fraction=0.12, jump_prob=0.06, target_success=0.2,\n                 cov_update_rate=None, sigma_initial=None, stagnation_restart=500,\n                 elite_size=None, k_edges=4, edge_proposals=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # tunables\n        self.init_fraction = float(init_fraction)\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n        if cov_update_rate is None:\n            # scale learning rate inversely with dimension\n            self.c_cov = 0.25 / max(1.0, (self.dim + 2.0))\n        else:\n            self.c_cov = float(cov_update_rate)\n        self.sigma_initial = sigma_initial\n        self.stagnation_restart = int(stagnation_restart)\n\n        # edge/elite settings\n        self.elite_size = int(elite_size) if elite_size is not None else max(4, min(12, int(self.budget // 10)))\n        self.k_edges = int(max(1, k_edges))\n        self.edge_proposals = int(max(1, edge_proposals))\n\n    def _levy_vector(self, size):\n        # Cauchy-like direction, robustified\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        norm = np.linalg.norm(z)\n        if norm == 0:\n            return self.rng.standard_normal(size=size)\n        return z / norm\n\n    def _ensure_bounds_reflect(self, x, lb, ub):\n        # reflect out-of-bounds components and clip for safety\n        below = x < lb\n        above = x > ub\n        if np.any(below) or np.any(above):\n            x = np.where(below, lb + (lb - x), x)\n            x = np.where(above, ub - (x - ub), x)\n            x = np.clip(x, lb, ub)\n        return x\n\n    def __call__(self, func):\n        # get bounds from func\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial sampling\n        n_init = max(1, int(min(max(12, 2 * dim), max(1, int(budget * self.init_fraction)))))\n        n_init = min(n_init, budget)\n        archive_X = []\n        archive_F = []\n\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            archive_X.append(x.copy())\n            archive_F.append(float(f))\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n\n        archive_X = np.array(archive_X, dtype=float)\n        archive_F = np.array(archive_F, dtype=float)\n\n        # domain scale and initial sigma\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma_initial is None:\n            sigma = max(1e-8, 0.22 * range_scale)\n        else:\n            sigma = float(self.sigma_initial)\n\n        # initial covariance (diagonal)\n        C = np.eye(dim) * (((range_scale / 4.0) ** 2) + 1e-9)\n\n        p_succ = 0.0\n        ema_alpha = 0.12\n\n        since_improvement = 0\n\n        # elite archive maintenance\n        def update_elites(Xa, Fa, max_elite):\n            # keep sorted by fitness (ascending)\n            order = np.argsort(Fa)\n            keep = order[:min(len(order), max_elite)]\n            return Xa[keep], Fa[keep]\n\n        # initial elites\n        if archive_X.size == 0:\n            archive_X = np.zeros((0, dim))\n            archive_F = np.zeros((0,))\n        elites_X, elites_F = update_elites(archive_X, archive_F, self.elite_size)\n\n        # edge scoring (for weighted sampling), stored as dict keyed by tuple(i,j)\n        edge_scores = {}  # map (i,j)->score (positive)\n        node_scores = np.ones(max(1, len(elites_X)))  # fallback\n\n        # main loop: mix covariance steps and dense-edge proposals\n        while evals < budget:\n            remaining = budget - evals\n\n            # update elites from archive\n            elites_X, elites_F = update_elites(np.vstack([archive_X]) if archive_X.size else archive_X,\n                                              np.hstack([archive_F]) if archive_F.size else archive_F,\n                                              self.elite_size)\n            n_elite = len(elites_X)\n            if n_elite == 0:\n                # sample random if no archive (edge-case)\n                x = rng.uniform(lb, ub)\n                f = func(x)\n                evals += 1\n                archive_X = np.vstack([archive_X, x]) if archive_X.size else np.array([x])\n                archive_F = np.hstack([archive_F, f]) if archive_F.size else np.array([f])\n                if f < f_opt:\n                    f_opt = float(f); x_opt = x.copy(); since_improvement = 0\n                continue\n\n            # rebuild node scores\n            node_scores = np.ones(n_elite)\n            # build k-nearest neighbor edges among elites\n            if n_elite >= 2:\n                # pairwise distances\n                diff = elites_X[:, None, :] - elites_X[None, :, :]\n                dists = np.linalg.norm(diff, axis=2)\n                # set self-dist to large\n                np.fill_diagonal(dists, np.inf)\n                edges = set()\n                for i in range(n_elite):\n                    k = min(self.k_edges, n_elite - 1)\n                    neigh = np.argpartition(dists[i], k)[:k]\n                    for j in neigh:\n                        a, b = (i, j) if i < j else (j, i)\n                        edges.add((a, b))\n                edges = sorted(edges)\n            else:\n                edges = []\n\n            # initialize missing edge scores\n            for e in edges:\n                if e not in edge_scores:\n                    edge_scores[e] = 1.0  # small uniform prior\n\n            # compute probabilities for choosing edge vs covariance step\n            # bias towards edge sampling when more elites available\n            edge_mode_prob = min(0.85, 0.25 + 0.6 * (n_elite / max(1, self.elite_size)))\n            # reduce if sigma is very large: prefer edges to exploit structure\n            if sigma > 2.5 * range_scale:\n                edge_mode_prob = min(0.95, edge_mode_prob + 0.15)\n\n            # choose number of proposals this iteration (amortize cholesky)\n            batch = min(max(4, int(6 + dim // 2)), remaining)\n\n            # precompute cholesky\n            reg = 1e-9 * np.eye(dim)\n            try:\n                L = np.linalg.cholesky(C + reg)\n            except np.linalg.LinAlgError:\n                L = np.linalg.cholesky(C + (1e-6 * np.eye(dim)))\n\n            # produce a mix of edge proposals and covariance proposals\n            for _ in range(batch):\n                if evals >= budget:\n                    break\n\n                use_edge = (rng.random() < edge_mode_prob) and (len(edges) > 0)\n                if use_edge and (rng.random() < 0.9):\n                    # select an edge with probability proportional to its score\n                    e_keys = list(edges)\n                    scores = np.array([edge_scores[e] for e in e_keys], dtype=float)\n                    probs = scores / (np.sum(scores) + 1e-12)\n                    idx = rng.choice(len(e_keys), p=probs)\n                    i, j = e_keys[idx]\n                    xi = elites_X[i]\n                    xj = elites_X[j]\n                    # generate several edge-based trial samples (but count them one-by-one to respect budget)\n                    # choose mode: interpolation, extrapolation, barycentric (if triangle possible)\n                    mode_r = rng.random()\n                    if mode_r < 0.5:\n                        # interpolation/extrapolation along edge: sample weight in [-0.6, 1.6]\n                        w = rng.uniform(-0.6, 1.6)\n                        base = xi * (1.0 - w) + xj * w\n                    else:\n                        # barycentric with third node if available\n                        if n_elite > 2 and rng.random() < 0.7:\n                            # pick k != i,j\n                            candidates = [k for k in range(n_elite) if k not in (i, j)]\n                            k = rng.choice(candidates)\n                            xk = elites_X[k]\n                            # random barycentric weights (Dirichlet)\n                            weights = rng.gamma(1.0, 1.0, size=3)\n                            weights = weights / np.sum(weights)\n                            base = weights[0] * xi + weights[1] * xj + weights[2] * xk\n                        else:\n                            # midpoint\n                            base = 0.5 * (xi + xj)\n                    # add orthogonal perturbation to edge direction\n                    evec = xj - xi\n                    e_norm = np.linalg.norm(evec)\n                    if e_norm > 1e-12:\n                        # generate gaussian perturbation and remove its projection on evec\n                        z = rng.normal(size=dim)\n                        proj = (z @ evec) / (e_norm**2) * evec\n                        z_orth = z - proj\n                        norm_orth = np.linalg.norm(z_orth)\n                        if norm_orth < 1e-12:\n                            z_orth = rng.normal(size=dim)\n                            norm_orth = np.linalg.norm(z_orth)\n                        z_orth = z_orth / norm_orth\n                        orth_scale = rng.uniform(0.02, 0.6)  # fraction of sigma to push orthogonally\n                        perturb = z_orth * (orth_scale * sigma * (0.8 + rng.random() * 1.2))\n                    else:\n                        # fall back to isotropic\n                        perturb = (L @ rng.normal(size=dim)) * (0.5 * rng.random())\n\n                    # sometimes amplify along edge (extrapolation)\n                    edge_amp = 1.0 + 0.8 * (rng.random() - 0.5)\n                    x_trial = base + edge_amp * perturb\n\n                    # occasional small covariance-shaped perturbation around the trial\n                    if rng.random() < 0.35:\n                        x_trial = x_trial + sigma * (L @ rng.normal(size=dim)) * 0.3\n\n                else:\n                    # covariance-driven or heavy-tailed jump around current best\n                    if rng.random() < self.jump_prob:\n                        z = self._levy_vector(dim)\n                        dx = L.dot(z) * 3.0\n                    else:\n                        z = rng.normal(size=dim)\n                        dx = L.dot(z)\n                    x_trial = x_opt + sigma * dx\n\n                # ensure bounds (reflect)\n                x_trial = self._ensure_bounds_reflect(x_trial, lb, ub)\n\n                # evaluate\n                f_trial = func(x_trial)\n                evals += 1\n\n                # bookkeeping add to archive\n                if archive_X.size == 0:\n                    archive_X = np.array([x_trial.copy()])\n                    archive_F = np.array([float(f_trial)])\n                else:\n                    archive_X = np.vstack([archive_X, x_trial.copy()])\n                    archive_F = np.hstack([archive_F, float(f_trial)])\n\n                improved = False\n                if f_trial < f_opt - 1e-15:\n                    improved = True\n                    f_opt = float(f_trial)\n                    x_opt = x_trial.copy()\n                    since_improvement = 0\n                else:\n                    since_improvement += 1\n\n                # update covariance using displacement relative to current best/mean depending on mode\n                # compute y = (x_trial - x_opt)/sigma as normalized displacement (avoid zero)\n                y = (x_trial - x_opt) if x_opt is not None else (x_trial - np.mean(archive_X, axis=0))\n                y_norm = np.linalg.norm(y)\n                if y_norm > 1e-12:\n                    y_vec = (y / y_norm).reshape(-1, 1) * (y_norm)  # preserve magnitude\n                    C = (1.0 - self.c_cov) * C + self.c_cov * (y_vec @ y_vec.T)\n                    # slight shrink to keep conditioning\n                    C += 1e-12 * np.eye(dim)\n\n                # if used an edge, reward that edge for improvement (increase score) or slightly decay otherwise\n                if use_edge and len(edges) > 0:\n                    # map chosen edge if available\n                    try:\n                        key = e_keys[idx]\n                        if improved:\n                            edge_scores[key] = edge_scores.get(key, 1.0) * 1.2 + 0.5\n                        else:\n                            edge_scores[key] = edge_scores.get(key, 1.0) * 0.995\n                        # keep scores bounded\n                        edge_scores[key] = float(np.clip(edge_scores[key], 0.05, 1e6))\n                    except Exception:\n                        pass\n\n                # update EMA of success\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n\n                # adapt sigma multiplicatively toward target success rate\n                adjust = np.exp(0.7 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                sigma *= float(np.clip(adjust, 0.65, 1.6))\n                sigma = np.clip(sigma, 1e-9 * range_scale, 6.0 * range_scale)\n\n                # occasional global injection when stagnating\n                if since_improvement > max(50, int(0.08 * budget)) and rng.random() < 0.02:\n                    # sample random elite-disturbance to maintain diversity\n                    center = elites_X[rng.integers(0, n_elite)]\n                    x_inj = center + 1.5 * sigma * rng.normal(size=dim)\n                    x_inj = np.clip(x_inj, lb, ub)\n                    if evals < budget:\n                        f_inj = func(x_inj)\n                        evals += 1\n                        archive_X = np.vstack([archive_X, x_inj.copy()])\n                        archive_F = np.hstack([archive_F, float(f_inj)])\n                        if f_inj < f_opt:\n                            f_opt = float(f_inj); x_opt = x_inj.copy(); since_improvement = 0\n\n                # break if budget exhausted\n                if evals >= budget:\n                    break\n\n            # stagnation handling: when no improvement for long, do heavy diversification\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # try a series of directed restarts biased by edges and elites\n                tries = min(12, budget - evals)\n                for t in range(tries):\n                    if rng.random() < 0.6 and n_elite >= 1:\n                        center = elites_X[rng.integers(0, n_elite)]\n                    else:\n                        center = rng.uniform(lb, ub)\n                    # large jump plus local covariance\n                    if rng.random() < 0.4:\n                        x_try = center + 3.0 * sigma * rng.normal(size=dim)\n                    else:\n                        # a directed jump along weighted difference of two elites\n                        if n_elite >= 2:\n                            a, b = rng.choice(n_elite, size=2, replace=False)\n                            dirv = elites_X[a] - elites_X[b]\n                            if np.linalg.norm(dirv) < 1e-12:\n                                dirv = rng.normal(size=dim)\n                            dirv = dirv / (np.linalg.norm(dirv) + 1e-12)\n                            x_try = center + 2.5 * sigma * dirv * (0.6 + rng.random() * 1.4)\n                        else:\n                            x_try = center + 2.0 * sigma * rng.normal(size=dim)\n                    x_try = np.clip(x_try, lb, ub)\n                    f_try = func(x_try)\n                    evals += 1\n                    archive_X = np.vstack([archive_X, x_try.copy()])\n                    archive_F = np.hstack([archive_F, float(f_try)])\n                    if f_try < f_opt:\n                        f_opt = float(f_try)\n                        x_opt = x_try.copy()\n                        # encourage local refinement after finding improvement\n                        sigma = max(0.7 * sigma, 1e-9 * range_scale)\n                        # mix covariance with identity to re-broaden search\n                        C = 0.6 * C + 0.4 * np.eye(dim) * ((range_scale / 6.0) ** 2)\n                        since_improvement = 0\n                        break\n                    if evals >= budget:\n                        break\n                # if still stagnating, bump jump probability and enlarge sigma\n                if since_improvement >= self.stagnation_restart:\n                    self.jump_prob = min(0.6, self.jump_prob * 1.4)\n                    sigma = min(sigma * 2.0, 12.0 * range_scale)\n\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 5, "feedback": "The algorithm ACLS_EdgeDense scored 0.194 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9180234676337075}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9045675241951296}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9375361805135776}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.00141428995481796}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.037533454785487086}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.028118057236075966}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.01109084354191514}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.03005255372050486}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.015093157732346985}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.024391565636288415}], "aucs": [0.9180234676337075, 0.9045675241951296, 0.9375361805135776, 0.00141428995481796, 4.999999999999449e-05, 4.999999999999449e-05, 0.037533454785487086, 0.028118057236075966, 4.999999999999449e-05, 0.01109084354191514, 4.999999999999449e-05, 4.999999999999449e-05, 0.03005255372050486, 0.015093157732346985, 0.024391565636288415]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3094.0, "Edges": 3093.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.9993535875888817, "Degree Variance": 2.1176466409745243, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.643416370106761, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3290908713401701, "Depth Entropy": 2.2090068556482514, "Assortativity": 0.0, "Average Eccentricity": 18.637685843568196, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003232062055591467, "Average Shortest Path": 10.795316320962467, "mean_complexity": 14.4, "total_complexity": 72.0, "mean_token_count": 562.0, "total_token_count": 2810.0, "mean_parameter_count": 4.8, "total_parameter_count": 24.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "e688e627-30c3-48ce-b342-2a61ad48be96", "fitness": "-inf", "name": "EdgeDenseTriangulatedAdaptivePrune", "description": "Edge-Dense Triangulated Adaptive Prune (EDTAP) \u2014 aggressively densify an elite-focused edge+triangle graph, reinforce productive edges, generate many edge/triangle/edge-walk recombinations (with PCA-aligned perturbations and orthogonal tweaks), and adapt trust and graph density to concentrate search while retaining rare heavy-tailed escapes.", "code": "import numpy as np\n\nclass EdgeDenseTriangulatedAdaptivePrune:\n    \"\"\"\n    Edge-Dense Triangulated Adaptive Prune (EDTAP)\n\n    One-line: Aggressively densify an elite-centered edge+triangle graph, reinforce\n    productive connections, and sample many edge/triangle/edge-walk recombinations\n    (with PCA-aligned and orthogonal perturbations) to accelerate basin linking\n    and exploitation.\n\n    Required initializer signature: __init__(self, budget, dim, ...)\n    Required call signature: def __call__(self, func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.2,\n                 k_neighbors=7, proposals_per_cycle=120, tri_prob=0.5,\n                 edge_walk_prob=0.12, levy_prob=0.03, trust_init=0.14,\n                 reinforce=1.4, decay=0.985, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.tri_prob = float(tri_prob)\n        self.edge_walk_prob = float(edge_walk_prob)\n        self.levy_prob = float(levy_prob)\n        self.trust_init = float(trust_init)\n        self.reinforce = float(reinforce)\n        self.decay = float(decay)\n\n        # population sizing\n        if pop_size is None:\n            # bias to larger population for richer graph if budget permits\n            base = int(np.clip(np.sqrt(self.budget) * 1.25, 10, 180))\n            self.pop_size = max(base, self.dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        # trust control\n        self.trust_expand = 1.15\n        self.trust_shrink = 0.78\n        self.min_trust = 1e-6\n        self.max_trust = 5.0\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        trust = float(self.trust_init) * range_norm\n\n        # ensure pop_size reasonable relative to budget\n        pop_size = min(self.pop_size, max(3, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population and evaluate\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # initialize dense symmetric edge-weight matrix (small positive floor)\n        edge_w = np.full((self.pop_size, self.pop_size), 1e-3, dtype=float)\n        np.fill_diagonal(edge_w, 0.0)\n\n        # bookkeeping best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        cycles_no_improve = 0\n        cycle = 0\n\n        # helper: compute pairwise squared distances\n        def pairwise_dist2(X):\n            dif = X[:, None, :] - X[None, :, :]\n            return np.sum(dif * dif, axis=2)\n\n        # helper: ensure symmetric and floor\n        def sym_floor(W):\n            W = (W + W.T) * 0.5\n            np.fill_diagonal(W, 0.0)\n            W[W < 1e-12] = 1e-12\n            return W\n\n        # main loop\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            proposals = min(self.proposals_per_cycle, remaining)\n            improved_this_cycle = False\n\n            # decay edge weights lightly to forget old signals\n            edge_w *= self.decay\n            edge_w = sym_floor(edge_w)\n\n            # identify elites and build dense graph\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            order = np.argsort(pop_f)\n            elites = order[:elite_count]\n\n            # distances\n            dist2 = pairwise_dist2(pop)\n\n            # Build a dense set of edges prioritizing elites + neighbors\n            edges_set = set()\n            # full clique among elites (densify strongly)\n            for i in range(len(elites)):\n                for j in range(i+1, len(elites)):\n                    a = int(elites[i]); b = int(elites[j])\n                    edges_set.add((min(a, b), max(a, b)))\n            # for each elite: connect to k nearest (including near-elites)\n            for a in elites:\n                neigh = np.argsort(dist2[a])\n                cnt = 0\n                for nb in neigh:\n                    if nb == a: continue\n                    edges_set.add((min(a, int(nb)), max(a, int(nb))))\n                    cnt += 1\n                    if cnt >= (self.k_neighbors + 2):\n                        break\n            # also connect neighbors among themselves to create triangles\n            for a in elites:\n                neigh = [int(nb) for nb in np.argsort(dist2[a]) if nb != a][:self.k_neighbors]\n                for i in range(len(neigh)):\n                    for j in range(i+1, len(neigh)):\n                        edges_set.add((min(neigh[i], neigh[j]), max(neigh[i], neigh[j])))\n            # random elite-biased edges\n            rand_pairs = max(3, int(self.pop_size / 2))\n            for _ in range(rand_pairs):\n                a = int(rng.choice(elites) if rng.random() < 0.7 else rng.integers(0, self.pop_size))\n                b = int(rng.choice(elites) if rng.random() < 0.7 else rng.integers(0, self.pop_size))\n                if a != b:\n                    edges_set.add((min(a, b), max(a, b)))\n\n            edges = list(edges_set)\n            if len(edges) == 0:\n                edges = [(0, 1)]\n\n            # Precompute triangle pool: triangles formed by connected triples (dense)\n            triangles = []\n            # quick adjacency list\n            adj = {i: set() for i in range(self.pop_size)}\n            for (a, b) in edges:\n                adj[a].add(b); adj[b].add(a)\n            # find triangles by intersecting neighbor sets (only among elites or near-elites to limit count)\n            elite_nodes = set(elites.tolist())\n            candidate_nodes = list(elite_nodes.union(set([n for a in elites for n in adj[a]])))\n            for i_idx in range(len(candidate_nodes)):\n                i = candidate_nodes[i_idx]\n                for j_idx in range(i_idx+1, len(candidate_nodes)):\n                    j = candidate_nodes[j_idx]\n                    if j not in adj[i]: continue\n                    # triangles: nodes connected to both i and j\n                    common = adj[i].intersection(adj[j])\n                    for k in common:\n                        if k <= j:  # avoid duplicates; ensure order i<j<k\n                            continue\n                        triangles.append((int(i), int(j), int(k)))\n            # cap triangle count\n            if len(triangles) > 500:\n                triangles = rng.choice(triangles, size=500, replace=False).tolist()\n\n            # compute selection weights for edges (exponentiate to emphasize)\n            edge_list = edges\n            edge_weights = np.array([edge_w[a, b] for (a, b) in edge_list], dtype=float)\n            # small floor\n            edge_weights = np.maximum(edge_weights, 1e-12)\n            # sharpen for sampling\n            sharp = 1.2\n            edge_probs = edge_weights ** sharp\n            edge_probs = edge_probs / np.sum(edge_probs)\n\n            # proposals\n            for _ in range(proposals):\n                if evals >= self.budget:\n                    break\n\n                # pick type: triangle, edge-walk, or edge interpolation\n                r = rng.random()\n                if r < self.tri_prob and len(triangles) > 0:\n                    # triangle barycentric with PCA-aligned perturbation\n                    a, b, c = triangles[rng.integers(0, len(triangles))]\n                    Xa, Xb, Xc = pop[a], pop[b], pop[c]\n                    # barycentric weights biased to corners\n                    w = rng.gamma(0.9, 1.0, size=3)\n                    w /= (np.sum(w) + 1e-12)\n                    x_prop = w[0]*Xa + w[1]*Xb + w[2]*Xc\n                    # compute small local PCA (2D) to align perturbation to triangle plane\n                    M = np.vstack([Xa, Xb, Xc])\n                    Mc = M - np.mean(M, axis=0)\n                    try:\n                        # small SVD\n                        U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                        # principal direction\n                        pdir = Vt[0]\n                        sec = Vt[1] if Vt.shape[0] > 1 else None\n                    except Exception:\n                        pdir = rng.standard_normal(dim)\n                        sec = None\n                    # perturb along principal and orthogonal directions scaled by trust and triangle size\n                    tri_scale = (np.linalg.norm(Xa-Xb) + np.linalg.norm(Xa-Xc) + np.linalg.norm(Xb-Xc)) / 3.0 + 1e-12\n                    mag = trust * 0.6 + 0.35 * tri_scale\n                    x_prop = x_prop + mag * (0.9 * rng.standard_normal() * pdir\n                                             + 0.3 * rng.standard_normal(dim) * (0. if sec is None else sec))\n                    # slight centroid extrapolation occasionally\n                    if rng.random() < 0.12:\n                        centroid = (Xa + Xb + Xc)/3.0\n                        x_prop = centroid + rng.uniform(-0.7, 1.1) * (centroid - x_prop)\n\n                elif r < self.tri_prob + self.edge_walk_prob and len(edge_list) > 0:\n                    # edge-walk: follow a short path along strong edges to build extrapolated direction\n                    # start on a heavy edge\n                    e0 = rng.choice(len(edge_list), p=edge_probs)\n                    a, b = edge_list[e0]\n                    path = [a, b]\n                    # extend walk probabilistically using adjacency & edge weights\n                    max_walk = 3\n                    for _walk in range(max_walk - 1):\n                        cur = path[-1]\n                        neighs = list(adj[cur])\n                        if len(neighs) == 0: break\n                        # choose neighbor by edge_w weight\n                        neigh_w = np.array([edge_w[min(cur,n), max(cur,n)] for n in neighs], dtype=float)\n                        if np.sum(neigh_w) <= 0:\n                            nxt = rng.choice(neighs)\n                        else:\n                            probs_n = neigh_w / np.sum(neigh_w)\n                            nxt = rng.choice(neighs, p=probs_n)\n                        if nxt == path[-2] if len(path) >= 2 else False:\n                            break\n                        path.append(int(nxt))\n                    # build direction from start to end of walk\n                    Xs = pop[path[0]]\n                    Xe = pop[path[-1]]\n                    dir_vec = Xe - Xs\n                    if np.linalg.norm(dir_vec) < 1e-12:\n                        dir_vec = rng.standard_normal(dim)\n                    # propose along direction with orthogonal noise\n                    t = rng.uniform(-0.6, 1.6)\n                    x_prop = Xs + t * dir_vec + 0.5 * trust * rng.standard_normal(dim)\n\n                else:\n                    # edge interpolation/extrapolation\n                    e_idx = int(rng.choice(len(edge_list), p=edge_probs))\n                    a, b = edge_list[e_idx]\n                    Xa, Xb = pop[a], pop[b]\n                    # biased toward better endpoint\n                    if pop_f[a] < pop_f[b]:\n                        t = rng.beta(2.2, 1.1)\n                    else:\n                        t = 1.0 - rng.beta(2.2, 1.1)\n                    if rng.random() < 0.1:\n                        t = rng.uniform(-0.8, 1.8)\n                    x_prop = (1 - t) * Xa + t * Xb\n                    # orthogonal perturbation: remove along-edge component, add orthogonal gaussian\n                    edge_dir = Xb - Xa\n                    norm_edge = np.linalg.norm(edge_dir)\n                    if norm_edge > 1e-12:\n                        unit = edge_dir / norm_edge\n                        proj = np.dot(x_prop - Xa, unit) * unit\n                        orth = (x_prop - Xa) - proj\n                        x_prop = x_prop + 0.12 * trust * rng.standard_normal() * unit\n                        x_prop = x_prop + 0.8 * trust * rng.standard_normal(dim) - 0.8 * rng.standard_normal() * orth\n\n                # occasional Levy-like heavy jump to escape basins\n                if rng.random() < self.levy_prob:\n                    try:\n                        cauch = rng.standard_cauchy(dim)\n                    except Exception:\n                        u = rng.random(dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                    x_prop = x_prop + 0.12 * range_norm * cauch\n\n                # clip and evaluate\n                x_prop = np.clip(x_prop, lb, ub)\n                f_prop = float(func(x_prop))\n                evals += 1\n\n                # determine replacement: try to replace the worst among k-nearest that are worse\n                d2 = np.sum((pop - x_prop)**2, axis=1)\n                nearest = np.argsort(d2)[:min(6, self.pop_size)]\n                replaced_idx = -1\n                # prefer replacing worse ones with descent order\n                worst_order = nearest[np.argsort(pop_f[nearest])[::-1]]\n                for idx in worst_order:\n                    if f_prop < pop_f[idx]:\n                        replaced_idx = int(idx)\n                        break\n\n                if replaced_idx >= 0:\n                    # replace and reinforce edges to neighbors of replaced\n                    pop[replaced_idx] = x_prop.copy()\n                    old_val = pop_f[replaced_idx]\n                    pop_f[replaced_idx] = f_prop\n                    # strengthen edges between this node and its k nearest neighbors\n                    neighs = np.argsort(np.sum((pop - pop[replaced_idx])**2, axis=1))[1:1 + max(4, self.k_neighbors)]\n                    for nb in neighs:\n                        a_i, b_i = min(replaced_idx, int(nb)), max(replaced_idx, int(nb))\n                        edge_w[a_i, b_i] *= self.reinforce\n                    # also strengthen the specific edge if chosen from edge_list\n                    # expand trust lightly\n                    trust = min(self.max_trust, trust * self.trust_expand)\n                    improved_this_cycle = True\n                    # if the replacement improved global best, update\n                    if f_prop < f_best:\n                        f_best = f_prop; x_best = x_prop.copy()\n                else:\n                    # maybe replace global worst\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        pop[idx_worst] = x_prop.copy()\n                        pop_f[idx_worst] = f_prop\n                        # reinforce edges for that node\n                        neighs = np.argsort(np.sum((pop - pop[idx_worst])**2, axis=1))[1:1 + max(4, self.k_neighbors)]\n                        for nb in neighs:\n                            a_i, b_i = min(idx_worst, int(nb)), max(idx_worst, int(nb))\n                            edge_w[a_i, b_i] *= self.reinforce\n                        trust = min(self.max_trust, trust * self.trust_expand)\n                        improved_this_cycle = True\n                        if f_prop < f_best:\n                            f_best = f_prop; x_best = x_prop.copy()\n                    else:\n                        # unsuccessful: mild shrink and slight decay of involved edges to discourage unproductive connections\n                        trust = max(self.min_trust, trust * self.trust_shrink)\n                        # penalize the chosen edge a bit if it was used\n                        if 'e_idx' in locals():\n                            a_e, b_e = edge_list[e_idx]\n                            edge_w[a_e, b_e] *= 0.95\n\n                # maintain symmetric edge_w\n                edge_w = sym_floor(edge_w)\n\n                # occasionally densify graph around new good point: add edges to nearest elites\n                if improved_this_cycle and rng.random() < 0.18:\n                    idx_new = np.argmin(pop_f)\n                    near = np.argsort(np.sum((pop - pop[idx_new])**2, axis=1))[1:1 + max(6, self.k_neighbors)]\n                    for nb in near:\n                        a_i, b_i = min(int(idx_new), int(nb)), max(int(idx_new), int(nb))\n                        edge_w[a_i, b_i] = max(edge_w[a_i, b_i], 1.0)\n\n                # global opposite injection occasionally for diversity\n                if rng.random() < 0.015 and evals < self.budget:\n                    opp = lb + ub - x_best + 0.02 * range_norm * rng.standard_normal(dim)\n                    opp = np.clip(opp, lb, ub)\n                    f_opp = float(func(opp)); evals += 1\n                    if f_opp < np.max(pop_f):\n                        idx_replace = int(np.argmax(pop_f))\n                        pop[idx_replace] = opp; pop_f[idx_replace] = f_opp\n                        if f_opp < f_best:\n                            f_best = f_opp; x_best = opp.copy()\n\n                # update best if proposal improved independently of replacement\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n                    improved_this_cycle = True\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # end proposals loop\n\n            # stagnation handling: focused injection around best using current trust or shrink trust\n            if improved_this_cycle:\n                cycles_no_improve = 0\n            else:\n                cycles_no_improve += 1\n                if cycles_no_improve >= 4 and evals < self.budget:\n                    n_inject = min(4, self.budget - evals)\n                    for _ in range(n_inject):\n                        cand = x_best + trust * rng.standard_normal(dim) * 0.9\n                        cand = np.clip(cand, lb, ub)\n                        f_c = float(func(cand)); evals += 1\n                        # replace a worse individual if better\n                        idx_w = int(np.argmax(pop_f))\n                        if f_c < pop_f[idx_w]:\n                            pop[idx_w] = cand; pop_f[idx_w] = f_c\n                            # strengthen edges to elites\n                            for e in elites[:min(3, len(elites))]:\n                                a_i, b_i = min(idx_w, int(e)), max(idx_w, int(e))\n                                edge_w[a_i, b_i] = max(edge_w[a_i, b_i], 1.0)\n                            if f_c < f_best:\n                                f_best = f_c; x_best = cand.copy()\n                    trust = max(self.min_trust, trust * 0.80)\n                    cycles_no_improve = 0\n\n            # if remaining small, break to final refine\n            if self.budget - evals <= max(6, dim*2):\n                break\n\n        # final local refinement: coordinate pattern with diminishing step + small directional healing\n        step = 0.14 * range_norm\n        step_min = max(1e-8, 1e-6 * range_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 346, in __call__, the following error occurred:\nIndexError: list index out of range\nOn line: a_e, b_e = edge_list[e_idx]", "error": "In the code, line 346, in __call__, the following error occurred:\nIndexError: list index out of range\nOn line: a_e, b_e = edge_list[e_idx]", "parent_ids": ["dad14baa-7d46-4686-8ce1-6aaea4db1ad0"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4030.0, "Edges": 4029.0, "Max Degree": 36.0, "Min Degree": 1.0, "Mean Degree": 1.9995037220843672, "Degree Variance": 2.2312652623930944, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 9.048404840484048, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3245600720069228, "Depth Entropy": 2.251081858105048, "Assortativity": 0.0, "Average Eccentricity": 18.028535980148884, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00024813895781637717, "Average Shortest Path": 11.187921440523944, "mean_complexity": 19.4, "total_complexity": 97.0, "mean_token_count": 698.2, "total_token_count": 3491.0, "mean_parameter_count": 4.0, "total_parameter_count": 20.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "de97b1f1-b1c5-4fa6-b7cc-b99e9a2314d6", "fitness": 0.18194551215355273, "name": "EdgeFlowCurrents", "description": "Edge-Flow Currents \u2014 build an ultra-dense directed edge flow network among a compact population and drive recombination by \"currents\" (weighted summed edge vectors), barycentric edge-triangles and flow-updated edges; adapt flows from successes and use dual-population elite-periphery reseeding and occasional L\u00e9vy escapes to increase edge density and connectivity.", "code": "import numpy as np\n\nclass EdgeFlowCurrents:\n    \"\"\"\n    Edge-Flow Currents (EFC)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional args:\n        pop_size, elite_frac, k_edges, proposals_per_iter, levy_prob, seed\n    Main idea: maintain a dense directed edge flow matrix that records which\n    transitions produce improvements. Use flows to create \"currents\" (weighted\n    sums of neighboring edge vectors) that push individuals, plus rich\n    barycentric triangle recombination and directed extrapolations. Flows are\n    reinforced on success and slowly decay elsewhere to concentrate edge density.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.25,\n                 k_edges=None, proposals_per_iter=60, levy_prob=0.06, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.proposals_per_iter = int(proposals_per_iter)\n        self.levy_prob = float(levy_prob)\n\n        if pop_size is None:\n            base = int(np.clip(np.sqrt(self.budget) * 1.15, 18, 140))\n            self.pop_size = max(base, dim + 8)\n        else:\n            self.pop_size = int(pop_size)\n\n        if k_edges is None:\n            self.k_edges = max(3, min(self.pop_size - 1, int(np.clip(self.dim * 2, 3, self.pop_size-1))))\n        else:\n            self.k_edges = int(k_edges)\n\n        # adaptive coefficients\n        self.sigma_frac = 0.10     # initial local noise fraction of range\n        self.step_frac = 0.18      # base step fraction of range for currents/extrapolation\n        self.flow_lr = 0.18        # learning rate for flows on success\n        self.flow_decay = 0.985    # multiplicative decay each major iteration\n        self.max_flow = 50.0\n        self.min_flow = 1e-6\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        sigma_scale = self.sigma_frac * range_norm\n        step_scale = self.step_frac * range_norm\n\n        evals = 0\n        P = self.pop_size\n\n        # Initialize population uniformly in bounds\n        pop = rng.uniform(lb, ub, size=(P, dim))\n        pop_f = np.full(P, np.inf)\n        for i in range(P):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # edge flows: directed flows[i,j] means moving from i toward j historically improved\n        flows = np.full((P, P), 0.02, dtype=float)  # small baseline for dense connectivity\n        np.fill_diagonal(flows, 0.0)\n\n        # per-individual local scales\n        sigma = np.full(P, sigma_scale)\n        step = np.full(P, step_scale)\n\n        # bookkeeping\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        no_improve = 0\n        max_no_improve = max(20, P * 3)\n\n        # helper: build candidate neighbor lists (kNN biased to elites)\n        def build_knn_and_candidates():\n            # pairwise squared distances\n            X = pop\n            dif = X[:, None, :] - X[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            # produce candidate directed edge list with weights (from flows + elite bias)\n            elite_count = max(2, int(np.ceil(self.elite_frac * P)))\n            elites = np.argsort(pop_f)[:elite_count]\n            candidates = []\n            weights = []\n            # connect every elite to its k nearest\n            for i in elites:\n                neigh = np.argsort(dist2[i])  # includes self\n                added = 0\n                for j in neigh:\n                    if j == i: continue\n                    candidates.append((int(i), int(j)))\n                    # bias weight by eliteness (better f -> higher)\n                    rank_bonus = (elite_count - np.where(elites == i)[0][0]) / (elite_count + 1)\n                    weights.append(flows[i, j] + 0.5 * rank_bonus)\n                    added += 1\n                    if added >= self.k_edges: break\n            # densify: add top flow edges and some random edges\n            # top flows\n            flat_idx = np.argpartition(-flows.ravel(), min(200, P*P-1))[:min(200, P*P-1)]\n            for idx in flat_idx:\n                a = idx // P; b = idx % P\n                if a == b: continue\n                candidates.append((int(a), int(b)))\n                weights.append(flows[a,b])\n            # random densification\n            extra = max(0, P // 2)\n            for _ in range(extra):\n                a = int(rng.integers(0, P))\n                b = int(rng.integers(0, P))\n                if a == b: continue\n                candidates.append((a,b))\n                weights.append(flows[a,b] + 0.01)\n            weights = np.maximum(np.asarray(weights, dtype=float), 1e-12)\n            return candidates, weights\n\n        # main iterative loop\n        iter_count = 0\n        while evals < self.budget:\n            rem = self.budget - evals\n            improved_in_loop = False\n            iter_count += 1\n\n            # rebuild candidates and weights based on current pop and flows\n            candidates, weights = build_knn_and_candidates()\n            W = weights / np.sum(weights)\n\n            # Number of proposals this major iteration: limited to proposals_per_iter or remaining budget\n            n_props = min(self.proposals_per_iter, rem)\n            for _ in range(n_props):\n                if evals >= self.budget: break\n\n                # sample a candidate directed edge (i->j) proportional to combined weight\n                idx_choice = rng.choice(len(candidates), p=W)\n                i, j = candidates[idx_choice]\n                xi, xj = pop[i].copy(), pop[j].copy()\n                fi, fj = pop_f[i], pop_f[j]\n\n                # Choose recombination mode\n                mode_r = rng.random()\n                if mode_r < 0.35:\n                    # Current-push: weighted sum of outgoing flows from i (a \"current\" vector)\n                    neigh_weights = flows[i].copy()\n                    neigh_weights[i] = 0.0\n                    if np.sum(neigh_weights) <= 1e-12:\n                        # fallback to direction to j\n                        current = (xj - xi)\n                    else:\n                        # weighted average of neighbor displacement vectors\n                        vecs = (pop - xi)  # shape (P, dim)\n                        wsum = (neigh_weights[:, None] * vecs).sum(axis=0)\n                        current = wsum / (np.sum(neigh_weights) + 1e-12)\n                    # scale and noise\n                    s = step[i] * rng.uniform(0.5, 1.5)\n                    noise = sigma[i] * rng.standard_normal(dim) * 0.6\n                    x_prop = xi + s * (current / (np.linalg.norm(current) + 1e-12)) + noise\n\n                elif mode_r < 0.75:\n                    # Barycentric triangle recombination among i,j, and a connected k\n                    # choose k among neighbors of i or j\n                    potential = np.where((flows[i] > 0) | (flows[j] > 0))[0]\n                    if potential.size == 0:\n                        k = int(rng.integers(0, P))\n                    else:\n                        k = int(rng.choice(potential))\n                    xk = pop[k].copy()\n                    # sample barycentric weights biased toward better endpoints\n                    base = rng.random(3)\n                    # bias toward better among i,j\n                    if fi < fj:\n                        base[0] += 0.6\n                    else:\n                        base[1] += 0.6\n                    w = base / base.sum()\n                    x_prop = w[0]*xi + w[1]*xj + w[2]*xk\n                    # local perturbation scaled by local sigmas\n                    local_sig = (sigma[i] + sigma[j] + sigma[k]) / 3.0\n                    x_prop += local_sig * rng.standard_normal(dim) * 0.9\n\n                else:\n                    # Extrapolate along directed edge i->j, maybe beyond\n                    vec = (xj - xi)\n                    stretch = rng.uniform(0.6, 1.6)\n                    x_prop = xi + stretch * vec + sigma[i] * rng.standard_normal(dim)\n\n                # occasional spiral perturbation to increase edge crossing\n                if dim > 1 and rng.random() < 0.25:\n                    a, b = rng.choice(dim, size=2, replace=False)\n                    theta = rng.uniform(-np.pi/3, np.pi/3)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    da, db = x_prop[a], x_prop[b]\n                    x_prop[a] = ca*da - sb*db\n                    x_prop[b] = sb*da + ca*db\n\n                # occasional Levy jump\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    x_prop += 0.18 * range_norm * cauchy\n\n                # ensure bounds\n                x_prop = np.clip(x_prop, lb, ub)\n\n                # evaluate\n                f_prop = float(func(x_prop))\n                evals += 1\n\n                # Replacement & flow update rules\n                # try to insert into population: replace the worse of i or j if improved\n                replaced = None\n                if f_prop < fi or f_prop < fj:\n                    # decide which endpoint is worse (higher f)\n                    if fi >= fj:\n                        replace_idx = i\n                    else:\n                        replace_idx = j\n                    # greedy replace\n                    if f_prop < pop_f[replace_idx]:\n                        old_val = pop_f[replace_idx]\n                        pop[replace_idx] = x_prop\n                        pop_f[replace_idx] = f_prop\n                        replaced = replace_idx\n                        # strengthen the directed flow from the other endpoint toward this one\n                        # if we replaced i, flow from j->i should get boosted, vice versa.\n                        source = j if replace_idx == i else i\n                        target = replace_idx\n                        # reinforcement proportional to improvement magnitude\n                        improvement = max(0.0, (old_val - f_prop))\n                        flows[source, target] += self.flow_lr * (1.0 + improvement / (1.0 + abs(f_prop)))\n                        flows[source, target] = min(self.max_flow, flows[source, target])\n                        # a bit of local sigma/step success increase\n                        sigma[replace_idx] = min(2.0 * range_norm, sigma[replace_idx] * 1.18)\n                        step[replace_idx] = min(2.0 * range_norm, step[replace_idx] * 1.08)\n                        # also nudge reverse flow slightly smaller to encourage directedness\n                        flows[target, source] = max(self.min_flow, flows[target, source] * 0.96)\n                else:\n                    # weak uphill acceptance occasionally to diversify\n                    if rng.random() < 0.006:\n                        idx = int(rng.integers(0, P))\n                        if f_prop < pop_f[idx]:\n                            pop[idx] = x_prop\n                            pop_f[idx] = f_prop\n                            replaced = idx\n                            sigma[idx] = sigma[idx] * 1.06\n\n                # if replaced, maybe create triangle closure edges for densification\n                if replaced is not None:\n                    # connect replaced to k nearest neighbors (densify edges)\n                    dists = np.sum((pop - pop[replaced])**2, axis=1)\n                    nearest = np.argsort(dists)[1:self.k_edges+1]\n                    for nb in nearest:\n                        flows[replaced, nb] = max(flows[replaced, nb], 0.02)\n                        flows[nb, replaced] = max(flows[nb, replaced], 0.02)\n\n                # weaken flows globally slightly each proposal to maintain plasticity\n                flows *= 0.9998\n                np.fill_diagonal(flows, 0.0)\n\n                # track best\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n                    improved_in_loop = True\n                    no_improve = 0\n\n            # After a block of proposals, perform a small \"flow current sweep\": move a subset of population along their incident flow currents\n            sweep_count = max(1, P // 6)\n            indices = rng.choice(P, size=sweep_count, replace=False)\n            for i in indices:\n                if evals >= self.budget: break\n                # compute incoming flows to i: nodes k with flows[k,i]; use these to create a pull direction\n                incoming = flows[:, i].copy()\n                incoming[i] = 0.0\n                if np.sum(incoming) > 1e-12:\n                    pull = ((incoming[:, None]) * (pop - pop[i])).sum(axis=0) / (np.sum(incoming) + 1e-12)\n                else:\n                    # fallback to direction toward best\n                    pull = x_best - pop[i]\n                # scale and add noise\n                s = step[i] * rng.uniform(0.4, 1.2)\n                x_new = pop[i] + s * (pull / (np.linalg.norm(pull) + 1e-12)) + sigma[i] * rng.standard_normal(dim) * 0.6\n                # levy occasional\n                if rng.random() < self.levy_prob * 0.6:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    x_new += 0.14 * range_norm * cauchy\n                x_new = np.clip(x_new, lb, ub)\n                f_new = float(func(x_new))\n                evals += 1\n                # accept if better or small probability\n                if f_new < pop_f[i] or rng.random() < 0.005:\n                    # update flow: from i toward the node closest to x_new (if any)\n                    # find nearest existing node to x_new\n                    dists_to_new = np.sum((pop - x_new)**2, axis=1)\n                    nearest_idx = int(np.argmin(dists_to_new))\n                    # update population\n                    pop[i] = x_new\n                    pop_f[i] = f_new\n                    # update flows: reinforcement from i -> nearest_idx\n                    if nearest_idx != i:\n                        flows[i, nearest_idx] += self.flow_lr * (1.0 + max(0.0, pop_f[nearest_idx] - f_new) / (1.0 + abs(f_new)))\n                        flows[i, nearest_idx] = min(self.max_flow, flows[i, nearest_idx])\n                    # adapt sigma/step on success\n                    sigma[i] = min(2.0 * range_norm, sigma[i] * 1.12)\n                    step[i] = min(2.0 * range_norm, step[i] * 1.06)\n                    if f_new < f_best:\n                        f_best = f_new; x_best = x_new.copy(); improved_in_loop = True\n\n                else:\n                    # failure shrinkage\n                    sigma[i] *= 0.93\n                    step[i] *= 0.96\n\n            # decay flows gradually to keep plasticity and avoid runaway singular edges\n            flows *= self.flow_decay\n            flows = np.clip(flows, self.min_flow, self.max_flow)\n            np.fill_diagonal(flows, 0.0)\n\n            # global maintenance & stagnation handling\n            if improved_in_loop:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # occasional random injection of high-diversity individuals around elites to increase edges\n            if rng.random() < 0.05 and evals < self.budget:\n                # create around top elites and densify their outgoing flows\n                elite_idx = np.argsort(pop_f)[:max(2, int(np.ceil(self.elite_frac * P)))]\n                for e in elite_idx:\n                    if evals >= self.budget: break\n                    newp = pop[e] + 0.08 * range_vec * rng.standard_normal(dim)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    # replace worst if better\n                    idx_w = int(np.argmax(pop_f))\n                    if f_new < pop_f[idx_w]:\n                        pop[idx_w] = newp; pop_f[idx_w] = f_new\n                        # connect from elite to this new node\n                        flows[e, idx_w] = max(flows[e, idx_w], 0.03)\n                        flows[idx_w, e] = max(flows[idx_w, e], 0.01)\n                        if f_new < f_best:\n                            f_best = f_new; x_best = newp.copy()\n\n            # more aggressive reseeding when stagnation occurs\n            if no_improve >= max_no_improve and evals < self.budget:\n                no_improve = 0\n                n_reset = max(1, P // 3)\n                for _ in range(n_reset):\n                    if evals >= self.budget: break\n                    idx = int(rng.integers(0, P))\n                    # 70% create near best, 30% global random\n                    if rng.random() < 0.7:\n                        newp = x_best + 0.05 * range_vec * rng.standard_normal(dim)\n                    else:\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    pop[idx] = newp; pop_f[idx] = f_new\n                    sigma[idx] = sigma_scale\n                    step[idx] = step_scale\n                    # densify edges to/from idx\n                    dists = np.sum((pop - newp)**2, axis=1)\n                    nearest = np.argsort(dists)[1:self.k_edges+1]\n                    for nb in nearest:\n                        flows[idx, nb] = max(flows[idx, nb], 0.02)\n                        flows[nb, idx] = max(flows[nb, idx], 0.02)\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # if remaining budget small, break to local refinement\n            if self.budget - evals < max(12, dim * 3):\n                break\n\n        # Final local mesh-adaptive coordinate search around best\n        mesh = 0.12 * range_norm\n        mesh_min = 1e-7 * range_norm\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                # negative\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # store results\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        # keep some diagnostics\n        self._pop = pop\n        self._pop_f = pop_f\n        self._flows = flows\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EdgeFlowCurrents scored 0.182 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5713cbd-1130-4e8d-abdc-3c3ca8966b71"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.19113422335731733}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.16007275695168832}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.17574093342651576}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.026408124046007786}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.026122354874865583}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.01980447874684721}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.012380414952707586}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.013351752862227118}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.012571671409004948}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.7435857760684147}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.6672811911460718}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.6805790044616229}], "aucs": [0.19113422335731733, 0.16007275695168832, 0.17574093342651576, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.026408124046007786, 0.026122354874865583, 0.01980447874684721, 0.012380414952707586, 0.013351752862227118, 0.012571671409004948, 0.7435857760684147, 0.6672811911460718, 0.6805790044616229]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3796.0, "Edges": 3795.0, "Max Degree": 35.0, "Min Degree": 1.0, "Mean Degree": 1.9994731296101158, "Degree Variance": 2.1206530416910483, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.681662870159453, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3195521342510073, "Depth Entropy": 2.147204182164599, "Assortativity": 0.0, "Average Eccentricity": 18.906217070600633, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00026343519494204424, "Average Shortest Path": 11.097939027420862, "mean_complexity": 18.25, "total_complexity": 73.0, "mean_token_count": 803.5, "total_token_count": 3214.0, "mean_parameter_count": 3.25, "total_parameter_count": 13.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "605fdbf1-cefd-44d2-a10b-aa0a45b6b29a", "fitness": 0.0924379811762089, "name": "SimplexEdgeDensifier", "description": "Simplex Edge Densifier (SED) \u2014 build many dense samples inside small elite simplices (hyperedges) and along elite edges, guided by local PCA orthogonal perturbations and adaptive \"edge temperature\" to rapidly densify promising connections and escape basins.", "code": "import numpy as np\n\nclass SimplexEdgeDensifier:\n    \"\"\"\n    Simplex Edge Densifier (SED)\n\n    Main ideas:\n    - Maintain a compact population and an elite set.\n    - Create dense samples inside small simplices (hyperedges) formed by nearby elites via\n      barycentric (Dirichlet) sampling, producing many interior/extrapolated points (increases edge/hyperedge density).\n    - Complement simplex sampling with explicit edge (pair) probes and PCA-guided orthogonal jitter\n      around simplices to flood the local manifold with candidates.\n    - Adapt a temperature-like scale for sampling extent per simplex based on success; perform budget-aware local\n      line probes along promising edges.\n    - Final pattern-search coordinate refinement.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_size=None,\n                 simplex_order=3, densify_rate=0.25, edge_probe_prob=0.12,\n                 seed=None):\n        \"\"\"\n        Args:\n            budget (int): maximum number of function evaluations.\n            dim (int): problem dimension.\n            pop_size (int or None): population size (auto-scaled if None).\n            elite_size (int or None): elite archive size.\n            simplex_order (int): number of vertices per simplex (2=edge,3=triangle,...).\n            densify_rate (float): fraction of budget used per generation for simplex densification attempts.\n            edge_probe_prob (float): probability of performing targeted edge probes for a given elite pair.\n            seed: RNG seed.\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.simplex_order = max(2, int(simplex_order))\n        self.densify_rate = float(np.clip(densify_rate, 0.01, 0.8))\n        self.edge_probe_prob = float(np.clip(edge_probe_prob, 0.0, 1.0))\n\n        # population and elite sizes chosen sensibly relative to dim & budget\n        if pop_size is None:\n            self.pop_size = int(np.clip(12 + 3 * np.sqrt(self.budget / max(1, self.dim)), 8, 100))\n        else:\n            self.pop_size = int(pop_size)\n        if elite_size is None:\n            self.elite_size = max(3, min(int(self.pop_size // 4), 20))\n        else:\n            self.elite_size = int(elite_size)\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        range_norm = np.linalg.norm(ub - lb)\n        evals = 0\n\n        # initialize population (uniform + symmetric diversity)\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, dim))\n        # inject opposites for first few\n        nopp = min(4, self.pop_size)\n        pop[:nopp] = 0.5 * (pop[:nopp] + (lb + ub - pop[:nopp]))\n\n        pop_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # best so far\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        # per-simplex temperature (sampling scale), per-individual success counters\n        temp_elite = np.full(self.elite_size, 0.12 * range_norm)  # later resized when elites known\n        success_elite = None\n\n        gen = 0\n        no_improve = 0\n        max_no_improve = max(10, self.elite_size * 3)\n\n        # small helper to safely evaluate a candidate\n        def eval_candidate(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            evals += 1\n            return float(func(x))\n\n        while evals < self.budget:\n            gen += 1\n\n            # sort population and create elites\n            order = np.argsort(pop_f)\n            elites_idx = order[:self.elite_size]\n            elites = pop[elites_idx].copy()\n            elites_f = pop_f[elites_idx].copy()\n\n            # ensure temp_elite sized\n            if temp_elite.size != self.elite_size:\n                temp_elite = np.full(self.elite_size, 0.12 * range_norm)\n            if success_elite is None or success_elite.size != self.elite_size:\n                success_elite = np.zeros(self.elite_size, dtype=int)\n\n            improved_in_gen = False\n\n            # compute pairwise elite distances (for neighbor selection)\n            if self.elite_size > 1:\n                e_dists = np.linalg.norm(elites[:, None, :] - elites[None, :, :], axis=2)\n                np.fill_diagonal(e_dists, np.inf)\n            else:\n                e_dists = np.full((self.elite_size, self.elite_size), np.inf)\n\n            # budget allocation for this generation\n            remaining = self.budget - evals\n            # allocate a chunk for densification (but never exhaust)\n            densify_budget = max(1, min(int(remaining * self.densify_rate), remaining - 1))\n            # we'll also leave room for local improvement passes\n            remaining_for_pop = remaining - densify_budget\n\n            # 1) Local population moves: one candidate per individual using PCA-guided jitter + spiral toward best\n            for i in range(self.pop_size):\n                if evals >= self.budget: break\n\n                xi = pop[i].copy()\n                fi = pop_f[i]\n                # spiral-like attraction to best with random curvature\n                d = x_best - xi\n                nd = np.linalg.norm(d) + 1e-12\n                dir_unit = d / nd\n                theta = self.rng.uniform(-np.pi/6, np.pi/6) if dim > 1 else 0.0\n                if dim > 1:\n                    a, b = self.rng.choice(dim, size=2, replace=False)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    d_rot = d.copy()\n                    d_rot[a] = ca * d[a] - sb * d[b]\n                    d_rot[b] = sb * d[a] + ca * d[b]\n                    dir_unit = d_rot / (np.linalg.norm(d_rot) + 1e-12)\n                step = (0.2 + 0.8 * self.rng.random()) * 0.5 * range_norm * np.tanh(nd / (range_norm + 1e-12))\n                cand = np.clip(xi + step * dir_unit + self.rng.normal(scale=0.08 * range_norm, size=dim), lb, ub)\n                f_c = eval_candidate(cand)\n                if f_c < fi:\n                    pop[i] = cand\n                    pop_f[i] = f_c\n                    if f_c < f_best:\n                        f_best = f_c; x_best = cand.copy(); improved_in_gen = True\n\n            # 2) Simplex densification: form small simplices (hyperedges) from elites and sample barycentric points\n            # We'll iterate elites and pick nearest (simplex_order-1) neighbors to form a simplex\n            if densify_budget > 0 and self.elite_size >= 2:\n                used = 0\n                # plan simplexes per elite (prefer edges first if simplex_order==2)\n                simplexes = []\n                k = min(self.simplex_order, self.elite_size)\n                for a in range(self.elite_size):\n                    # choose (k-1) nearest neighbors\n                    if k - 1 > 0:\n                        neighbors = np.argsort(e_dists[a])[:k-1]\n                        simplex = tuple(sorted(np.concatenate(([a], neighbors))))\n                    else:\n                        simplex = (a,)\n                    simplexes.append(simplex)\n                # add also all pairs (edges) to increase edge density\n                if self.simplex_order > 2:\n                    for a in range(self.elite_size):\n                        for b in range(a+1, self.elite_size):\n                            simplexes.append((a, b))\n                # shuffle to randomize sampling order\n                self.rng.shuffle(simplexes)\n\n                # estimate samples per simplex based on densify_budget\n                total_simplex_slots = max(1, len(simplexes))\n                samples_per_simplex = max(1, densify_budget // total_simplex_slots)\n                # cap to avoid excessive per-simplex\n                samples_per_simplex = min(samples_per_simplex, 120)\n\n                for simplex in simplexes:\n                    if evals >= self.budget: break\n                    # gather vertices\n                    verts = elites[list(simplex)]\n                    # compute local scale: average pairwise distance of simplex\n                    if verts.shape[0] >= 2:\n                        paird = np.linalg.norm(verts[:, None, :] - verts[None, :, :], axis=2)\n                        local_scale = np.mean(paird[np.isfinite(paird)])\n                        if not np.isfinite(local_scale) or local_scale <= 0:\n                            local_scale = 0.1 * range_norm\n                    else:\n                        local_scale = 0.12 * range_norm\n\n                    # build PCA of verts to identify principal direction(s)\n                    try:\n                        cov = np.cov(verts.T)\n                        # if cov not full rank, fallback to isotropic\n                        eigvals, eigvecs = np.linalg.eigh(cov + 1e-12 * np.eye(dim))\n                    except Exception:\n                        eigvecs = np.eye(dim)\n                        eigvals = np.ones(dim)\n\n                    # main direction = eigenvector of largest eigenvalue\n                    main_dir = eigvecs[:, np.argmax(eigvals)]\n                    # orthonormal basis: generate random combination orthogonal to main_dir\n                    # sample inside simplex using Dirichlet weights so barycentric interior samples are produced\n                    for s in range(samples_per_simplex):\n                        if evals >= self.budget: break\n                        # barycentric weights with small chance to extrapolate (allow t slightly negative/greater than 1)\n                        w = self.rng.dirichlet(np.ones(len(simplex)))\n                        if self.rng.random() < 0.20:\n                            # allow slight extrapolation by stretching weights\n                            stretch = self.rng.uniform(0.9, 1.6)\n                            w = w * stretch\n                        # form barycentric point\n                        p = np.sum((w[:, None] * verts), axis=0)\n                        # orthogonal jitter: subtract projection on main_dir to get perpendicular component\n                        noise = self.rng.normal(size=dim)\n                        proj = np.dot(noise, main_dir) * main_dir\n                        orth = noise - proj\n                        if np.linalg.norm(orth) > 1e-12:\n                            orth = orth / np.linalg.norm(orth)\n                        # scale jitter by local_scale * temp (temp from the first vertex of simplex if available)\n                        temp_idx = simplex[0] if len(simplex) > 0 else 0\n                        t_scale = temp_elite[temp_idx] if temp_idx < temp_elite.size else 0.12 * range_norm\n                        # jitter magnitude mixes local_scale and temp\n                        jitter = orth * (t_scale * (0.5 + self.rng.random()))\n                        candidate = np.clip(p + jitter, lb, ub)\n                        f_c = eval_candidate(candidate)\n                        used += 1\n                        # if candidate is promising, replace worst population member\n                        if f_c < np.max(pop_f):\n                            idx_w = int(np.argmax(pop_f))\n                            pop[idx_w] = candidate\n                            pop_f[idx_w] = f_c\n                            if f_c < f_best:\n                                f_best = f_c; x_best = candidate.copy(); improved_in_gen = True\n                                # reward simplex temperature\n                                temp_elite[temp_idx] = min(temp_elite[temp_idx] * 1.10, range_norm * 0.8)\n                                success_elite[temp_idx] += 1\n                        else:\n                            # slight cooling on failure\n                            if temp_idx < temp_elite.size:\n                                temp_elite[temp_idx] = max(temp_elite[temp_idx] * 0.995, 1e-8 * range_norm)\n\n                # small global adjustment: if many successes, slightly increase temps\n                if np.sum(success_elite) > 3:\n                    temp_elite = np.minimum(temp_elite * 1.04, 0.9 * range_norm)\n                    success_elite[:] = 0  # reset window\n\n            # 3) targeted edge probes: for top pairs, perform short 1D scans along edges with limited samples\n            if self.edge_probe_prob > 0 and evals < self.budget and self.elite_size >= 2:\n                # pick several top edges (pairs of elites) by combined fitness\n                top_pairs = []\n                # create list of pairs sorted by sum of objective (low sum -> promising)\n                for a in range(self.elite_size):\n                    for b in range(a+1, self.elite_size):\n                        score = elites_f[a] + elites_f[b]\n                        top_pairs.append((score, a, b))\n                top_pairs.sort()\n                max_pairs = min(12, len(top_pairs))\n                for rank in range(max_pairs):\n                    if evals >= self.budget: break\n                    if self.rng.random() > self.edge_probe_prob:\n                        continue\n                    _, a, b = top_pairs[rank]\n                    pa = elites[a]; pb = elites[b]\n                    # small 1D bracket along segment [pa,pb], sample npoints (budget-aware)\n                    npoints = min(7, max(3, (self.budget - evals) // 20))\n                    if npoints <= 0: break\n                    ts = np.linspace(0.0, 1.0, npoints)\n                    best_local_val = np.inf\n                    best_local_x = None\n                    for t in ts:\n                        if evals >= self.budget: break\n                        cand = np.clip((1 - t) * pa + t * pb + self.rng.normal(scale=0.02 * range_norm, size=dim), lb, ub)\n                        f_c = eval_candidate(cand)\n                        if f_c < best_local_val:\n                            best_local_val = f_c\n                            best_local_x = cand.copy()\n                    if best_local_x is not None and best_local_val < np.max(pop_f):\n                        idx_w = int(np.argmax(pop_f))\n                        pop[idx_w] = best_local_x\n                        pop_f[idx_w] = best_local_val\n                        if best_local_val < f_best:\n                            f_best = best_local_val; x_best = best_local_x.copy(); improved_in_gen = True\n\n            # 4) occasional reseeding for diversity: sample around random elite or global uniform\n            if self.rng.random() < 0.05 and evals < self.budget:\n                n_replace = max(1, self.pop_size // 12)\n                for _ in range(n_replace):\n                    if evals >= self.budget: break\n                    if self.rng.random() < 0.7:\n                        eidx = int(self.rng.integers(0, self.elite_size))\n                        newp = elites[eidx] + self.rng.normal(scale=0.06 * range_norm, size=dim)\n                    else:\n                        newp = self.rng.uniform(lb, ub, size=dim)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = eval_candidate(newp)\n                    idx_w = int(np.argmax(pop_f))\n                    if f_new < pop_f[idx_w]:\n                        pop[idx_w] = newp\n                        pop_f[idx_w] = f_new\n                        if f_new < f_best:\n                            f_best = f_new; x_best = newp.copy(); improved_in_gen = True\n\n            # Update no_improve counter\n            if improved_in_gen:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # If stuck for many generations, perform a larger global densification burst (sample many simplices + edges)\n            if no_improve >= max_no_improve and evals < self.budget:\n                no_improve = 0\n                burst_budget = max(1, min(self.budget - evals - 1, 200 + 10 * dim))\n                # sample random simplexes from elites and across population hull\n                for _ in range(burst_budget):\n                    if evals >= self.budget: break\n                    # choose k vertices either from elites or mixture of elites and random pop\n                    use_elite = self.rng.random() < 0.85\n                    k = min(self.simplex_order, self.elite_size if use_elite else self.pop_size)\n                    if k < 2:\n                        # fallback random sample\n                        cand = self.rng.uniform(lb, ub, size=dim)\n                        f_c = eval_candidate(cand)\n                        if f_c < np.max(pop_f):\n                            idx_w = int(np.argmax(pop_f))\n                            pop[idx_w] = cand; pop_f[idx_w] = f_c\n                            if f_c < f_best:\n                                f_best = f_c; x_best = cand.copy()\n                        continue\n                    if use_elite:\n                        verts_idx = self.rng.choice(self.elite_size, size=k, replace=False)\n                        verts = elites[verts_idx]\n                    else:\n                        verts_idx = self.rng.choice(self.pop_size, size=k, replace=False)\n                        verts = pop[verts_idx]\n                    # barycentric sample interior or slight extrapolate\n                    w = self.rng.dirichlet(np.ones(k))\n                    if self.rng.random() < 0.25:\n                        w *= self.rng.uniform(0.9, 1.5)\n                    p = np.sum((w[:, None] * verts), axis=0)\n                    # apply PCA orthogonal jitter similar to above\n                    noise = self.rng.normal(size=dim)\n                    # compute main direction from verts mean to best\n                    main_dir = (x_best - p)\n                    mdnorm = np.linalg.norm(main_dir)\n                    if mdnorm > 1e-12:\n                        main_dir = main_dir / mdnorm\n                        proj = np.dot(noise, main_dir) * main_dir\n                        orth = noise - proj\n                        if np.linalg.norm(orth) > 1e-12:\n                            orth = orth / np.linalg.norm(orth)\n                        jitter_scale = 0.3 * range_norm * (0.5 + self.rng.random())\n                        candidate = np.clip(p + orth * jitter_scale, lb, ub)\n                    else:\n                        candidate = np.clip(p + noise * 0.03 * range_norm, lb, ub)\n                    f_c = eval_candidate(candidate)\n                    if f_c < np.max(pop_f):\n                        idx_w = int(np.argmax(pop_f))\n                        pop[idx_w] = candidate; pop_f[idx_w] = f_c\n                        if f_c < f_best:\n                            f_best = f_c; x_best = candidate.copy()\n\n            # if remaining budget low, break for local refinement\n            if self.budget - evals < max(12, dim * 3):\n                break\n\n        # Final local coordinate-wise pattern search refinement\n        mesh = 0.12 * range_norm\n        mesh_min = 1e-6 * range_norm\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # plus direction\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = eval_candidate(x_try)\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                # minus direction\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = eval_candidate(x_try)\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SimplexEdgeDensifier scored 0.092 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e513ab61-961c-43f3-98a2-7cbf094d03bc"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.24855981069544741}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.2031368932845704}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.18284938741186174}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.031653474416770244}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03805714274393557}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0242305284096741}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.03394145043284169}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.02309647948354543}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.00936730702099231}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.18303393489710196}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.27929157908618396}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.12920172976020872}], "aucs": [0.24855981069544741, 0.2031368932845704, 0.18284938741186174, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.031653474416770244, 0.03805714274393557, 0.0242305284096741, 0.03394145043284169, 0.02309647948354543, 0.00936730702099231, 0.18303393489710196, 0.27929157908618396, 0.12920172976020872]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3544.0, "Edges": 3543.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9994356659142212, "Degree Variance": 1.906320223287762, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.908921933085502, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3363412028592385, "Depth Entropy": 2.1492492916868184, "Assortativity": 1.8582363763899645e-08, "Average Eccentricity": 20.299661399548533, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.0002821670428893905, "Average Shortest Path": 11.248107577399622, "mean_complexity": 23.5, "total_complexity": 94.0, "mean_token_count": 764.5, "total_token_count": 3058.0, "mean_parameter_count": 3.5, "total_parameter_count": 14.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "9b2bac7e-59ab-4326-bce7-477ed6ca7f2e", "fitness": 0.2954206750988764, "name": "EdgeEnrichedLowRankSurrogates", "description": "Edge-Enriched Low-Rank Surrogate Graph Walks \u2014 build a dense elite edge-graph, repeatedly sample/interpolate/extrapolate along edges and small triangles, and combine these edge recombinations with low-rank quadratic surrogate solves in subspaces spanned by edges + local principal directions to rapidly densify productive connections and exploit them.", "code": "import numpy as np\n\nclass EdgeEnrichedLowRankSurrogates:\n    \"\"\"\n    Edge-Enriched Low-Rank Surrogate Graph Walks (EELRS)\n\n    Key ideas:\n      - Maintain an archive of evaluated points and per-node success weights.\n      - Build a dense local \"edge graph\" among elites (each elite connected to several nearest\n        elites). Edge proposals (midpoints, barycentric points, extrapolations) plus small\n        orthogonal perturbations produce dense sampling along promising connections.\n      - For stronger proposals, fit low-rank quadratic surrogates in subspaces spanned by the\n        edge vector and local principal directions, solve surrogate minima in that subspace,\n        and map back to ambient space with trust control.\n      - Reinforce node weights when proposals from them succeed; prefer edges touching\n        higher-weight nodes to increase observed \"edge density\" around productive solutions.\n    \"\"\"\n\n    def __init__(self, budget, dim, init_samples=None, elite_k=8, connect_k=5,\n                 init_radius=1.0, min_radius=1e-6, max_radius=5.0, max_subdim=4,\n                 global_inject_prob=0.06, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total function evaluations allowed\n          dim: problem dimensionality\n          init_samples: number of initial latinized/random seeds (default ~ min(40,budget//10))\n          elite_k: how many top points to consider elites\n          connect_k: how many neighbors per elite to connect (edge density control)\n          init_radius/min_radius/max_radius: trust radii bounds\n          max_subdim: max subspace dim for surrogate fits (<= dim)\n          global_inject_prob: probability per iteration to inject a global random sample\n          rng: numpy.random.Generator or None\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if init_samples is None:\n            self.init_samples = int(min(40, max(8, self.budget // 10)))\n        else:\n            self.init_samples = int(init_samples)\n        self.elite_k = int(elite_k)\n        self.connect_k = int(connect_k)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_subdim = int(min(max_subdim, dim))\n        self.global_inject_prob = float(global_inject_prob)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def _orthonormalize(self, A):\n        # columns of A become orthonormal basis (Gram-Schmidt via QR)\n        if A.shape[1] == 0:\n            return np.zeros((A.shape[0], 0))\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :A.shape[1]]\n\n    def _build_design(self, S):\n        # same as earlier: design with constant, linear, and symmetric quadratic terms\n        m, k = S.shape\n        cols = [np.ones((m, 1)), S]\n        for i in range(k):\n            cols.append(0.5 * (S[:, i:i+1] ** 2))\n        for i in range(k):\n            for j in range(i + 1, k):\n                cols.append((S[:, i] * S[:, j])[:, None])\n        return np.hstack(cols)\n\n    def _theta_to_quad(self, theta, k):\n        a = float(theta[0])\n        b = np.array(theta[1:1 + k], dtype=float)\n        H = np.zeros((k, k), dtype=float)\n        idx = 1 + k\n        for i in range(k):\n            H[i, i] = float(theta[idx]); idx += 1\n        for i in range(k):\n            for j in range(i + 1, k):\n                H[i, j] = float(theta[idx]); H[j, i] = H[i, j]; idx += 1\n        return a, b, H\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []       # list of points\n        F = []       # objectives\n        radii = []   # trust radii per node\n        node_w = []  # node weights (reinforcement per-node)\n\n        # initial seeding (random uniform)\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n            radii.append(self.init_radius)\n            node_w.append(1.0)\n\n        # ensure at least one\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x)); evals += 1\n            X.append(np.array(x, dtype=float)); F.append(f)\n            radii.append(self.init_radius); node_w.append(1.0)\n\n        X_arr = np.vstack(X)\n        best_idx = int(np.argmin(F))\n        best_x = X_arr[best_idx].copy()\n        best_f = float(F[best_idx])\n\n        stagn = 0\n        iter_no = 0\n\n        # helper: rebuild arrays\n        def sync_arrays():\n            nonlocal X_arr\n            X_arr = np.vstack(X)\n\n        sync_arrays()\n\n        # main loop\n        while evals < self.budget:\n            iter_no += 1\n            n = len(X)\n            sync_arrays()\n            idx_sorted = np.argsort(F)\n            top_m = min(max(3, self.elite_k * 2), n)  # consider top_m points to create graph\n            elite_idx = idx_sorted[:top_m].tolist()\n\n            # compute pairwise edges via k-nearest among elites\n            E = []  # list of (i,j) index pairs (global indices)\n            if len(elite_idx) >= 2:\n                coords = X_arr[elite_idx]\n                # distances between elites\n                dmat = np.linalg.norm(coords[:, None, :] - coords[None, :, :], axis=2)\n                for local_i, i in enumerate(elite_idx):\n                    # get neighbors excluding itself\n                    order = np.argsort(dmat[local_i])\n                    k_nei = min(self.connect_k + 1, len(order))  # +1 because order includes itself\n                    for rank in order[1:k_nei]:\n                        j = elite_idx[int(rank)]\n                        if i < j:\n                            E.append((i, j))\n                        else:\n                            E.append((j, i))\n                # deduplicate\n                E = list(dict.fromkeys(E))\n\n            # compute edge selection probabilities based on node weights and fitness\n            if len(E) > 0:\n                node_arr = np.array(node_w)\n                F_arr = np.array(F)\n                minF = F_arr.min()\n                # compute per-edge score\n                e_scores = []\n                for (i, j) in E:\n                    score_nodes = node_arr[i] + node_arr[j]\n                    # prefer edges connecting good fitness (lower F)\n                    fit_term = np.exp(-0.5 * ((F_arr[i] + F_arr[j]) / 2.0 - minF))\n                    e_scores.append(max(1e-8, score_nodes * fit_term))\n                e_scores = np.array(e_scores, dtype=float)\n                e_probs = e_scores / e_scores.sum()\n            else:\n                e_probs = None\n\n            # Choose mode: edge_recomb / surrogate_local / global / triangle\n            mode_r = self.rng.random()\n            candidate_x = None\n            candidate_desc = None\n            # Edge recombination heavy path\n            if len(E) > 0 and mode_r < 0.54:\n                # pick an edge\n                ei = self.rng.choice(len(E), p=e_probs)\n                i, j = E[ei]\n                x1 = X[i]; x2 = X[j]\n                f1, f2 = F[i], F[j]\n                # sample barycentric coefficient slightly biased towards better end\n                w = 0.5 + (0.25 * (f2 - f1) / (abs(f1) + abs(f2) + 1e-8))  # shift towards better\n                t = np.clip(self.rng.beta(2.0, 2.0) * w + (1 - w) * (1 - self.rng.beta(2.0, 2.0)), 0.0, 1.0)\n                # midpoint/extrapolate\n                if self.rng.random() < 0.15:\n                    # extrapolate slightly beyond better endpoint\n                    if f1 < f2:\n                        base = x1; other = x2\n                    else:\n                        base = x2; other = x1\n                    dirv = base - other\n                    t_ext = 1.0 + self.rng.random() * 0.6  # up to 60% extrapolation\n                    candidate_x = base + dirv * (t_ext - 1.0)\n                    candidate_desc = f\"edge_extrap_{i}_{j}\"\n                else:\n                    # barycentric interpolation near the middle with small orth perturb\n                    candidate_x = t * x1 + (1.0 - t) * x2\n                    candidate_desc = f\"edge_bary_{i}_{j}\"\n\n                # orthogonal small perturbation to explore around the edge\n                edge_vec = x2 - x1\n                norm_ev = np.linalg.norm(edge_vec)\n                if norm_ev < 1e-12:\n                    # jitter\n                    orth_dir = self.rng.normal(size=self.dim)\n                else:\n                    # build a random orthonormal basis that includes normalized edge_vec as first vector\n                    v = edge_vec / (norm_ev + 1e-12)\n                    # random matrix then orthonormalize with v as first column\n                    R = self.rng.normal(size=(self.dim, min(self.dim, 3)))\n                    R[:, 0] = v\n                    Q, _ = np.linalg.qr(R)\n                    orth_dir = Q[:, 1] if Q.shape[1] > 1 else self.rng.normal(size=self.dim)\n                # perturb size proportional to average of radii and distance between endpoints\n                avg_r = max(1e-8, 0.5 * (radii[i] + radii[j]))\n                perturb_scale = (self.rng.random() ** 2) * 0.8 * avg_r + 0.02 * norm_ev / (1.0 + norm_ev)\n                candidate_x = candidate_x + orth_dir / (np.linalg.norm(orth_dir) + 1e-12) * perturb_scale\n                candidate_x = np.minimum(np.maximum(candidate_x, lb), ub)\n\n            # Triangle / barycentric among triples\n            elif len(E) > 0 and mode_r < 0.74:\n                # pick an edge then third node among neighbors of endpoints to form triangle\n                ei = self.rng.choice(len(E), p=e_probs)\n                i, j = E[ei]\n                # pick a third index close to either i or j\n                distances = np.linalg.norm(X_arr - (X[i] + X[j]) * 0.5, axis=1)\n                k = int(np.argmin(np.where(np.arange(n) != i, distances + 1e3*(np.arange(n)==i), distances)))\n                if k == i or k == j:\n                    # fallback: nearest other point\n                    order = np.argsort(np.linalg.norm(X_arr - X[i], axis=1))\n                    for cand in order:\n                        if cand != i and cand != j:\n                            k = int(cand); break\n                pts = np.vstack([X[i], X[j], X[k]])\n                # sample barycentric coords biased to center\n                r0, r1 = self.rng.random(), self.rng.random()\n                a = r0 ** 0.8; b = r1 ** 0.8\n                s = a / (a + b + 1e-12)\n                t = b / (a + b + 1e-12)\n                # normalized to sum<1 then third gets remainder\n                if s + t > 0.95:\n                    s *= 0.95 / (s + t); t *= 0.95 / (s + t)\n                candidate_x = s * X[i] + t * X[j] + (1 - s - t) * X[k]\n                # small orthogonal perturb\n                candidate_x = candidate_x + self.rng.normal(scale=0.03 * self.init_radius, size=self.dim)\n                candidate_x = np.minimum(np.maximum(candidate_x, lb), ub)\n                candidate_desc = f\"triangle_{i}_{j}_{k}\"\n\n            # Surrogate-local path: build low-rank surrogate in subspace spanned by edge and local PCA\n            elif mode_r < 0.94 and len(E) > 0:\n                # pick an edge weighted by e_probs\n                ei = self.rng.choice(len(E), p=e_probs)\n                i, j = E[ei]\n                center = X[i] if F[i] < F[j] else X[j]\n                center_f = F[i] if F[i] < F[j] else F[j]\n                center_r = radii[i] if F[i] < F[j] else radii[j]\n                # choose subdim\n                k_sub = min(self.max_subdim, self.dim, 1 + int(self.rng.integers(0, self.max_subdim)))\n                # basis: normalized edge vector + top local PCA vectors orthonormalized\n                edge_vec = X[j] - X[i]\n                if np.linalg.norm(edge_vec) < 1e-12:\n                    edge_vec = self.rng.normal(size=self.dim)\n                # gather local neighbors within radius\n                dists = np.linalg.norm(X_arr - center, axis=1)\n                neigh_mask = dists <= max(1.6 * center_r, np.percentile(dists, 40) if len(dists) > 4 else 1.6 * center_r)\n                neigh_idx = np.nonzero(neigh_mask)[0]\n                if len(neigh_idx) < (1 + k_sub + k_sub * (k_sub + 1) // 2):\n                    # add nearest until enough\n                    order = np.argsort(dists)\n                    for idx in order:\n                        if idx not in neigh_idx:\n                            neigh_idx = np.append(neigh_idx, idx)\n                        if len(neigh_idx) >= (1 + k_sub + k_sub * (k_sub + 1) // 2):\n                            break\n                local_pts = X_arr[neigh_idx] - center\n                # compute PCA directions\n                if local_pts.shape[0] >= 2:\n                    C = (local_pts.T @ local_pts) / max(1.0, local_pts.shape[0])\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        order = np.argsort(-eigvals)\n                        pca_dirs = eigvecs[:, order[:max(0, k_sub - 1)]]\n                    except Exception:\n                        pca_dirs = np.zeros((self.dim, max(0, k_sub - 1)))\n                else:\n                    pca_dirs = np.zeros((self.dim, max(0, k_sub - 1)))\n\n                # build basis with edge_vec as first, then PCA dirs\n                B = np.zeros((self.dim, k_sub))\n                ev = edge_vec / (np.linalg.norm(edge_vec) + 1e-12)\n                B[:, 0] = ev\n                for t in range(1, k_sub):\n                    if t - 1 < pca_dirs.shape[1]:\n                        B[:, t] = pca_dirs[:, t - 1]\n                    else:\n                        B[:, t] = self.rng.normal(size=self.dim)\n                # orthonormalize columns\n                Q, _ = np.linalg.qr(B)\n                basis = Q[:, :k_sub]\n\n                # project local points\n                S = (X_arr[neigh_idx] - center) @ basis  # (m, k_sub)\n                Phi = self._build_design(S)\n                y = np.array([F[idx] for idx in neigh_idx], dtype=float)\n                p = Phi.shape[1]\n                reg = 1e-8 * max(1.0, np.var(y))\n                try:\n                    A = Phi.T @ Phi\n                    A[np.diag_indices_from(A)] += reg + 1e-12\n                    rhs = Phi.T @ y\n                    theta = np.linalg.solve(A, rhs)\n                except np.linalg.LinAlgError:\n                    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n                if theta is None or np.any(~np.isfinite(theta)):\n                    # fallback small step along negative linear term\n                    candidate_x = center + basis @ (-0.6 * np.ones(k_sub) * center_r)\n                    candidate_desc = \"surrogate_fallback\"\n                else:\n                    a_hat, b_hat, H_hat = self._theta_to_quad(theta, k_sub)\n                    # solve for s* = -H^{-1} b with small damping\n                    lam = 1e-8\n                    s_star = None\n                    for _ in range(6):\n                        try:\n                            s_star = -np.linalg.solve(H_hat + lam * np.eye(k_sub), b_hat)\n                            break\n                        except np.linalg.LinAlgError:\n                            lam = max(1e-6, lam * 10.0)\n                    if s_star is None or not np.all(np.isfinite(s_star)):\n                        # use negative gradient direction\n                        if np.linalg.norm(b_hat) < 1e-12:\n                            s_dir = self.rng.normal(size=k_sub)\n                        else:\n                            s_dir = -b_hat\n                        s_star = s_dir / (np.linalg.norm(s_dir) + 1e-12) * (0.8 * center_r)\n                    # limit step\n                    max_step = 1.6 * center_r\n                    x_prop = center + basis @ s_star\n                    disp_norm = np.linalg.norm(x_prop - center)\n                    if disp_norm > max_step:\n                        x_prop = center + (x_prop - center) * (max_step / (disp_norm + 1e-12))\n                    candidate_x = np.minimum(np.maximum(x_prop, lb), ub)\n                    candidate_desc = \"surrogate_opt\"\n\n            # Global injection\n            else:\n                # global uniform random sample or Gaussian around best\n                if self.rng.random() < 0.6:\n                    candidate_x = lb + self.rng.random(self.dim) * (ub - lb)\n                    candidate_desc = \"global_uniform\"\n                else:\n                    # gaussian around best_x\n                    scale = 0.6 * (ub - lb)\n                    candidate_x = best_x + self.rng.normal(scale=1.0, size=self.dim) * scale\n                    candidate_x = np.minimum(np.maximum(candidate_x, lb), ub)\n                    candidate_desc = \"global_gauss\"\n\n            # small chance to additionally try a local interpolation between best and candidate before evaluating\n            if candidate_x is None:\n                continue\n            if evals >= self.budget:\n                break\n\n            # evaluate candidate\n            f_cand = float(func(candidate_x))\n            evals += 1\n\n            # add to archive\n            X.append(candidate_x.copy()); F.append(f_cand)\n            radii.append(self.init_radius * 0.9)\n            node_w.append(1.0)  # baseline weight for new node\n\n            # update node (reinforce endpoints if improvement)\n            improved = False\n            if f_cand < best_f:\n                best_f = f_cand; best_x = candidate_x.copy()\n                improved = True\n                stagn = 0\n            else:\n                stagn += 1\n\n            # reward nearby nodes: if candidate improved relative to endpoints or local center, increase their node_w\n            # find k nearest existing nodes (excluding the just-added)\n            if len(X) > 1:\n                arr = np.vstack(X[:-1])\n                dists = np.linalg.norm(arr - candidate_x, axis=1)\n                k_nei = min(4, len(dists))\n                nearest = np.argsort(dists)[:k_nei]\n                for idx in nearest:\n                    # if candidate is better than neighbor, boost neighbor weight slightly (edge into good node becomes denser)\n                    if f_cand < F[idx]:\n                        node_w[idx] = node_w[idx] * 1.05 + 0.05\n                    else:\n                        node_w[idx] = max(0.5, node_w[idx] * 0.995)\n\n            # small reinforcement to the candidate itself if it's good\n            if f_cand < np.percentile(F, max(1, min(80, int(100.0 * self.elite_k / max(1, n))))):\n                node_w[-1] *= 1.2\n\n            # adapt radii: if candidate is much better than nearby, increase nearby radii; else shrink source radii\n            # find nearest node (excluding itself)\n            if len(X) > 1:\n                arr = np.vstack(X[:-1])\n                dists = np.linalg.norm(arr - candidate_x, axis=1)\n                nearest_idx = int(np.argmin(dists))\n                if f_cand + 1e-12 < F[nearest_idx]:\n                    # success: expand nearest radius slightly\n                    radii[nearest_idx] = min(self.max_radius, radii[nearest_idx] * 1.2)\n                    radii[-1] = min(self.max_radius, radii[-1] * 1.1)\n                else:\n                    radii[nearest_idx] = max(self.min_radius, radii[nearest_idx] * 0.85)\n\n            # occasional extra local probes along the best-connected edge of the new node\n            if self.rng.random() < 0.10 and evals < self.budget:\n                # probe midpoint between new node and a high-weight neighbor\n                arr_w = np.array(node_w[:-1])\n                if arr_w.sum() > 0:\n                    probs = arr_w / arr_w.sum()\n                    nei = int(self.rng.choice(len(arr_w), p=probs))\n                    probe_x = 0.5 * (X[nei] + candidate_x) + self.rng.normal(scale=0.02, size=self.dim)\n                    probe_x = np.minimum(np.maximum(probe_x, lb), ub)\n                    f_probe = float(func(probe_x)); evals += 1\n                    X.append(probe_x.copy()); F.append(f_probe); radii.append(self.init_radius * 0.7); node_w.append(1.0)\n                    if f_probe < best_f:\n                        best_f = f_probe; best_x = probe_x.copy(); stagn = 0\n\n            # archive trimming to keep memory and calculations reasonable\n            max_archive = max(250, 6 * self.dim + 80)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = int(max(40, 3 * self.dim))\n                keep = set(idx_sorted[:keep_best].tolist())\n                # keep additional nodes probabilistically favoring better F and higher node_w\n                Fs = np.array(F)\n                Ws = np.array(node_w)\n                score = np.exp(-(Fs - Fs.min()) / (1e-8 + max(1.0, Fs.std()))) * (1.0 + 0.3 * Ws)\n                score /= score.sum()\n                extra_count = max_archive - len(keep)\n                if extra_count > 0:\n                    extra_idx = self.rng.choice(len(X), size=min(extra_count, len(X) - keep_best),\n                                                replace=False, p=score)\n                    for ei in extra_idx:\n                        keep.add(int(ei))\n                keep_list = sorted(list(keep))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                radii = [radii[i] for i in keep_list]\n                node_w = [node_w[i] for i in keep_list]\n                sync_arrays()\n\n            # occasional global injection if stagnating\n            if stagn >= 10 and evals < self.budget:\n                n_inj = min(3, max(1, self.dim // 3))\n                for _ in range(n_inj):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg)); evals += 1\n                    X.append(xg.copy()); F.append(fg); radii.append(self.init_radius); node_w.append(1.0)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagn = 0\n                # if still stagnating, do a focused gaussian around best\n                if stagn >= 10 and evals < self.budget:\n                    scale = 0.5 * (ub - lb)\n                    xj = best_x + self.rng.normal(scale=1.0, size=self.dim) * scale\n                    xj = np.minimum(np.maximum(xj, lb), ub)\n                    fj = float(func(xj)); evals += 1\n                    X.append(xj.copy()); F.append(fj); radii.append(self.init_radius); node_w.append(1.0)\n                    if fj < best_f:\n                        best_f = fj; best_x = xj.copy(); stagn = 0\n                    else:\n                        # minor reset of node weights to diversify\n                        node_w = [max(0.8, w * 0.7) for w in node_w]\n                        stagn = 0\n\n        # finalize: return best found\n        X_arr = np.vstack(X)\n        best_i = int(np.argmin(F))\n        return float(F[best_i]), np.array(X_arr[best_i], dtype=float)", "configspace": "", "generation": 5, "feedback": "The algorithm EdgeEnrichedLowRankSurrogates scored 0.295 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0c98b60a-b401-4b27-b5de-4c183326b288"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5780131528613769}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.544141893826698}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.5769724248278374}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.015057948766423412}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.006621335001970374}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.005556455097784241}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9632873493067878}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.908027439499192}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.8333321272950763}], "aucs": [0.5780131528613769, 0.544141893826698, 0.5769724248278374, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.015057948766423412, 0.006621335001970374, 0.005556455097784241, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.9632873493067878, 0.908027439499192, 0.8333321272950763]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4993.0, "Edges": 4992.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9995994392149008, "Degree Variance": 2.0344480670695835, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 9.248791208791209, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.319892503461803, "Depth Entropy": 2.2887855162954573, "Assortativity": 0.0, "Average Eccentricity": 20.787502503504907, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.0002002803925495694, "Average Shortest Path": 11.500272496880248, "mean_complexity": 14.571428571428571, "total_complexity": 102.0, "mean_token_count": 620.1428571428571, "total_token_count": 4341.0, "mean_parameter_count": 3.2857142857142856, "total_parameter_count": 23.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "420e652e-261a-4009-ab94-5766a6186339", "fitness": "-inf", "name": "HyperEdgeDensificationAdaptiveEmbedding", "description": "HyperEdge Densification with Adaptive Local Embeddings (HEDA) \u2014 aggressively build and explore a hypergraph of multi-node \"edges\" (hyperedges), run many short edge-walks and hyperedge barycentric mixes, score candidates with lightweight local PCA+ridge surrogates and uncertainty estimates, and adaptively densify and reinforce the most productive hyperedges to rapidly concentrate search along high-density productive manifolds.", "code": "import numpy as np\n\nclass HyperEdgeDensificationAdaptiveEmbedding:\n    \"\"\"\n    HyperEdge Densification with Adaptive Local Embeddings (HEDA)\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Optional args:\n      pop_size: initial population size (clipped to budget)\n      elite_frac: fraction of population considered elite\n      k_neighbors: number of nearest neighbors for local hyperedges\n      proposals_per_cycle: number of actual evaluations attempted per cycle\n      pool_multiplier: virtual candidates generated per actual evaluation\n      hyperedge_max: maximum hyperedge size (>=2)\n      walk_length: max short-walk length along edges (for random walks)\n      levy_prob: chance of heavy-tailed perturb in pool\n      trust_init: initial trust radius fraction of search range\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.18,\n                 k_neighbors=6, proposals_per_cycle=36, pool_multiplier=10,\n                 hyperedge_max=5, walk_length=3, levy_prob=0.06,\n                 trust_init=0.12, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.pool_multiplier = int(pool_multiplier)\n        self.hyperedge_max = max(2, int(hyperedge_max))\n        self.walk_length = max(1, int(walk_length))\n        self.levy_prob = float(levy_prob)\n        self.trust_init = float(trust_init)\n\n        # population sizing default heuristic\n        if pop_size is None:\n            base = int(np.clip(np.sqrt(self.budget) * 0.95, 8, 160))\n            self.pop_size = max(base, self.dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation constants\n        self.trust_expand = 1.25\n        self.trust_shrink = 0.70\n        self.hyperedge_boost = 1.6\n        self.hyperedge_decay = 0.988\n        self.min_trust = 1e-8\n        self.max_trust = 1e2\n\n    def _get_bounds(self, func):\n        # prefer func.bounds if provided, else [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        trust = float(self.trust_init) * range_norm\n\n        # ensure pop_size reasonable\n        pop_size = min(self.pop_size, max(4, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population and evaluate\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i])); evals += 1\n\n        # adjacency (edge) weights and hyperedge dictionary\n        edge_w = np.ones((self.pop_size, self.pop_size), dtype=float)\n        np.fill_diagonal(edge_w, 0.0)\n        # hyperedge weights: key = frozenset(ids), value = weight\n        hyper_w = {}\n\n        # bookkeeping\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        cycle_no_improve = 0\n        cycle = 0\n\n        # main search loop\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            act_proposals = min(self.proposals_per_cycle, remaining)\n            pool_size = max(int(act_proposals * self.pool_multiplier), act_proposals + 6)\n            improved_this_cycle = False\n\n            # decay hyperedges slightly\n            edge_w *= self.hyperedge_decay\n            edge_w[edge_w < 1e-12] = 1e-12\n            np.fill_diagonal(edge_w, 0.0)\n            # decay hyperedge weights\n            if hyper_w:\n                for k in list(hyper_w.keys()):\n                    hyper_w[k] = hyper_w[k] * self.hyperedge_decay\n                    if hyper_w[k] < 1e-12:\n                        del hyper_w[k]\n\n            # compute elites and distances\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            order = np.argsort(pop_f)\n            elites = order[:elite_count]\n\n            dif = pop[:, None, :] - pop[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n\n            # build edges: nearest neighbors for each node and clique among elites\n            edges_set = set()\n            for i in range(self.pop_size):\n                neigh = np.argsort(dist2[i])\n                cnt = 0\n                for nb in neigh:\n                    if nb == i: continue\n                    edges_set.add((min(i,int(nb)), max(i,int(nb))))\n                    cnt += 1\n                    if cnt >= self.k_neighbors: break\n            # clique among elites for strong connectivity\n            for i in range(len(elites)):\n                for j in range(i+1, len(elites)):\n                    a = int(elites[i]); b = int(elites[j])\n                    edges_set.add((min(a,b), max(a,b)))\n            edges = list(edges_set)\n            if len(edges) == 0:\n                edges = [(0, 1)]\n\n            # create hyperedges (multi-node groups) by combining neighbor sets of elites\n            # prefer compact neighbor groups to capture local manifolds\n            for e_center in elites:\n                neigh = np.argsort(dist2[e_center])[: (self.k_neighbors + 3)]\n                # create hyperedges sizes 2..hyperedge_max\n                for s in range(2, min(self.hyperedge_max, len(neigh)) + 1):\n                    # sample a few combinations probabilistically to limit count\n                    tries = 2\n                    for _ in range(tries):\n                        ids = tuple(sorted(rng.choice(neigh, size=s, replace=False).tolist()))\n                        key = frozenset(ids)\n                        weight = 1.0 + 0.5 * (1.0 / (1.0 + pop_f[list(ids)].mean()))\n                        hyper_w[key] = hyper_w.get(key, 0.0) + weight\n\n            # strengthen hyperedges from strong edges\n            # also create intersection hyperedges from overlapping hyperedges\n            keys = list(hyper_w.keys())\n            for i in range(len(keys)):\n                for j in range(i+1, len(keys)):\n                    a = keys[i]; b = keys[j]\n                    inter = a.intersection(b)\n                    if len(inter) >= 2:\n                        key = frozenset(inter)\n                        hyper_w[key] = hyper_w.get(key, 0.0) + 0.5*(hyper_w.get(a,1.0)+hyper_w.get(b,1.0))\n\n            # Build lightweight local PCA + ridge surrogates for many hyperedges (cache)\n            surrogate_cache = {}  # key -> (mean, V, w_linear, y_mean, used_ids)\n            # For each hyperedge produce a surrogate if enough distinct nodes\n            for he, hw in list(hyper_w.items()):\n                ids = sorted(list(he))\n                if len(ids) < 2:\n                    continue\n                # pick up to  min(8, 2*dim) nearest neighbors to hyperedge centroid to enrich surrogate\n                centroid = pop[ids].mean(axis=0)\n                # find nearest points to centroid\n                d2c = np.sum((pop - centroid)**2, axis=1)\n                idxs = np.argsort(d2c)[: min(self.pop_size, max(len(ids), 2*dim, 8))]\n                X_base = pop[idxs].astype(float)\n                y_base = pop_f[idxs].astype(float)\n                try:\n                    Xc = X_base - X_base.mean(axis=0, keepdims=True)\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    m = max(1, min(dim, Vt.shape[0], 1 + len(ids)//1))\n                    V = Vt[:m].copy()\n                    Z = Xc @ V.T\n                    Z_aug = np.hstack([np.ones((Z.shape[0],1)), Z])\n                    alpha = 1e-6*(1.0 + np.var(y_base))\n                    A = Z_aug.T @ Z_aug\n                    A[np.diag_indices_from(A)] += alpha\n                    w_lin = np.linalg.solve(A, Z_aug.T @ y_base)\n                    surrogate_cache[he] = (X_base.mean(axis=0), V, w_lin, y_base.mean(), idxs)\n                except Exception:\n                    continue\n\n            # generate a large pool of candidate points (virtual nodes) using:\n            # - hyperedge barycentric mixes\n            # - short random walks along strong edges\n            # - intersection-extrapolations between hyperedges\n            pool = np.empty((pool_size, dim), dtype=float)\n            pool_meta = []\n            he_keys = list(hyper_w.keys())\n            he_weights = np.array([hyper_w[k] for k in he_keys], dtype=float) if he_keys else np.array([])\n            # prepare cumulative for weighted sampling\n            if he_weights.size > 0:\n                he_probs = he_weights / (np.sum(he_weights) + 1e-12)\n                he_cum = np.cumsum(he_probs)\n            else:\n                he_cum = None\n\n            for i in range(pool_size):\n                r = rng.random()\n                meta = {}\n                # choose strategy\n                if he_cum is not None and r < 0.46 and len(he_keys) > 0:\n                    # hyperedge barycentric mix (prefer larger and high-weight hyperedges)\n                    # select hyperedge by weight\n                    u = rng.random()\n                    idx_he = int(np.searchsorted(he_cum, u))\n                    he = he_keys[max(0, min(len(he_keys)-1, idx_he))]\n                    ids = sorted(list(he))\n                    coeff = rng.random(len(ids))\n                    coeff /= coeff.sum()\n                    x = np.sum(pop[ids] * coeff[:, None], axis=0)\n                    # small PCA-based local perturbation if surrogate exists\n                    if he in surrogate_cache and rng.random() < 0.7:\n                        mu, V, w_lin, ym, idxs = surrogate_cache[he]\n                        z = ((x - mu) @ V.T)\n                        # perturb in subspace\n                        z = z + 0.55 * trust * rng.standard_normal(z.shape)\n                        x = mu + (z @ V)\n                    else:\n                        x = x + 0.5 * trust * rng.standard_normal(dim)\n                    meta['type'] = 'hyper_mix'; meta['ids'] = tuple(int(j) for j in ids)\n                elif r < 0.82:\n                    # short random walk along strong edges: pick a strong edge and walk\n                    # compute strong edges by combining adjacency weights and hyperedge contributions\n                    # sample an edge proportional to its weight\n                    tri = np.triu_indices(self.pop_size, k=1)\n                    ew = edge_w[tri]\n                    if ew.size == 0 or np.all(np.isclose(ew, 0)):\n                        a,b = rng.integers(0,self.pop_size), rng.integers(0,self.pop_size)\n                        while b==a: b = rng.integers(0,self.pop_size)\n                    else:\n                        ew_norm = ew / (np.sum(ew) + 1e-12)\n                        idx = int(np.searchsorted(np.cumsum(ew_norm), rng.random()))\n                        a = int(tri[0][idx]); b = int(tri[1][idx])\n                    # start from better endpoint\n                    if pop_f[a] < pop_f[b]:\n                        cur = pop[a].copy()\n                    else:\n                        cur = pop[b].copy()\n                    path = [a,b]\n                    steps = rng.integers(1, self.walk_length+1)\n                    for _s in range(steps):\n                        # move toward neighbor of last node (prefer strong edges)\n                        last = path[-1]\n                        neigh = np.argsort(-edge_w[last])  # descending weight\n                        # pick among top few neighbors\n                        n_choice = neigh[: min(6, len(neigh))]\n                        if n_choice.size == 0:\n                            break\n                        nxt = int(rng.choice(n_choice))\n                        dirv = pop[nxt] - cur\n                        cur = cur + rng.uniform(0.18, 0.9) * dirv + 0.35 * trust * rng.standard_normal(dim)\n                        path.append(nxt)\n                    x = cur\n                    meta['type'] = 'walk'; meta['ids'] = tuple(int(j) for j in path)\n                else:\n                    # intersection-extrapolation: pick two hyperedges and extrapolate between centroids\n                    if len(he_keys) >= 2:\n                        a_idx, b_idx = rng.choice(len(he_keys), size=2, replace=False)\n                        he_a, he_b = he_keys[a_idx], he_keys[b_idx]\n                        cent_a = pop[sorted(list(he_a))].mean(axis=0)\n                        cent_b = pop[sorted(list(he_b))].mean(axis=0)\n                        # extrapolate from intersection direction\n                        inter = he_a.intersection(he_b)\n                        base = None\n                        if len(inter) >= 1:\n                            base = pop[sorted(list(inter))].mean(axis=0)\n                        else:\n                            # fallback to better centroid\n                            base = 0.5*(cent_a + cent_b)\n                        factor = rng.uniform(-0.9, 1.4)\n                        x = base + factor * (cent_b - cent_a) + 0.45 * trust * rng.standard_normal(dim)\n                        meta['type'] = 'inter'; meta['ids'] = tuple(sorted(list(inter))) if len(inter)>0 else tuple()\n                    else:\n                        # random perturbation around best or elite\n                        if rng.random() < 0.6:\n                            center = pop[rng.choice(elites)]\n                        else:\n                            center = x_best\n                        x = center + 0.9 * trust * rng.standard_normal(dim)\n                        meta['type'] = 'rand'; meta['ids'] = tuple()\n                # occasional Levy-like jump for diversity\n                if rng.random() < self.levy_prob:\n                    try:\n                        cauch = rng.standard_cauchy(dim)\n                    except Exception:\n                        u = rng.random(dim); cauch = np.tan(np.pi * (u - 0.5))\n                    x = x + 0.08 * range_norm * cauch\n                # clip and store\n                pool[i] = np.clip(x, lb, ub)\n                pool_meta.append(meta)\n\n            # cheap scoring using surrogate if available, else distance-weighted heuristic\n            preds = np.full(pool_size, np.inf)\n            uncert = np.zeros(pool_size, dtype=float)\n            for i in range(pool_size):\n                meta = pool_meta[i]\n                x = pool[i]\n                score = None\n                # try to identify a surrogate: if hyperedge used and available\n                used_he = None\n                if 'ids' in meta and len(meta['ids']) >= 2:\n                    he = frozenset(meta['ids'])\n                    if he in surrogate_cache:\n                        used_he = he\n                # fallback: find nearest surrogate among hyperedges by centroid proximity\n                if used_he is None and surrogate_cache:\n                    # choose surrogate with centroid nearest to x\n                    best_k = None; best_d = np.inf\n                    for he_key, (mu, V, w_lin, ym, idxs) in surrogate_cache.items():\n                        d = np.sum((x - mu)**2)\n                        if d < best_d:\n                            best_d = d; best_k = he_key\n                    used_he = best_k\n                if used_he is not None and used_he in surrogate_cache:\n                    mu, V, w_lin, ym, idxs = surrogate_cache[used_he]\n                    Z = (x - mu) @ V.T\n                    Zaug = np.concatenate(([1.0], Z))\n                    pred = float(Zaug @ w_lin)\n                    # uncertainty proxy: distance to surrogate training centroid + distance to nearest training point\n                    dist_to_cent = np.linalg.norm(x - mu)\n                    d2 = np.sum((pop[idxs] - x)**2, axis=1)\n                    min_d = float(np.sqrt(np.min(d2)+1e-12))\n                    uncert_val = 0.6 * dist_to_cent + 0.4 * min_d\n                    score = pred\n                    uncert[i] = uncert_val\n                else:\n                    # no surrogate: use mixture of neighbor fitness and distance to best heuristic\n                    d2best = np.sum((pool[i] - x_best)**2)\n                    idx_sub = rng.choice(self.pop_size, size=min(self.pop_size, 12), replace=False)\n                    d2sub = np.sum((pool[i][None,:] - pop[idx_sub])**2, axis=1)\n                    min_idx = np.argmin(d2sub)\n                    nn_f = pop_f[idx_sub][min_idx]\n                    score = 0.6*nn_f + 0.4*(f_best + 0.5*np.sqrt(d2best + 1e-12))\n                    uncert[i] = np.sqrt(np.min(d2sub)+1e-12)\n                # combine predicted score and uncertainty (lower better). Penalize more uncertain points slightly\n                preds[i] = score + 0.12 * uncert[i]  # lower predicted fitness is better\n\n            # select top-K predicted candidates to evaluate\n            K = min(act_proposals, self.budget - evals)\n            if K <= 0:\n                break\n            order_pool = np.argsort(preds)\n            selected_idxs = order_pool[:K]\n\n            # evaluate selected candidates\n            for s_idx in selected_idxs:\n                if evals >= self.budget:\n                    break\n                x_prop = pool[int(s_idx)].copy()\n                f_prop = float(func(x_prop)); evals += 1\n\n                # Replacement policy: try replace a worse among k-nearest, else replace global worst\n                d2 = np.sum((pop - x_prop)**2, axis=1)\n                nearest = np.argsort(d2)[: min(8, self.pop_size)]\n                replaced_idx = -1\n                # prefer replacing the worst among nearest\n                nn_sorted = nearest[np.argsort(pop_f[nearest])[::-1]]\n                for cand in nn_sorted:\n                    if f_prop < pop_f[cand]:\n                        replaced_idx = int(cand); break\n                if replaced_idx >= 0:\n                    # replace\n                    old_ids = tuple()\n                    pop[replaced_idx] = x_prop.copy()\n                    pop_f[replaced_idx] = f_prop\n                    # boost edges between replaced node and contributing ids (from meta)\n                    meta = pool_meta[int(s_idx)]\n                    if 'ids' in meta and len(meta['ids']) >= 1:\n                        ids = [int(ii) for ii in meta['ids']]\n                        for a in ids:\n                            if a == replaced_idx: continue\n                            i1, i2 = min(a, replaced_idx), max(a, replaced_idx)\n                            edge_w[i1, i2] = edge_w[i1, i2] * self.hyperedge_boost + 1e-12\n                            edge_w[i2, i1] = edge_w[i1, i2]\n                        # also create/update hyperedge among ids and the replaced node\n                        new_he = frozenset(tuple(sorted(set(ids + [replaced_idx]))))\n                        hyper_w[new_he] = hyper_w.get(new_he, 0.0) + 1.0\n                    # update adjacency to keep symmetric non-zero\n                    np.fill_diagonal(edge_w, 0.0)\n                    edge_w[edge_w < 1e-12] = 1e-12\n                    # expand trust slightly on success\n                    trust = min(self.max_trust, trust * self.trust_expand)\n                    improved_this_cycle = True\n                else:\n                    # maybe replace global worst if better\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        pop[idx_worst] = x_prop.copy()\n                        pop_f[idx_worst] = f_prop\n                        # boost edges linking contributing ids to the worst index\n                        meta = pool_meta[int(s_idx)]\n                        if 'ids' in meta and len(meta['ids']) >= 1:\n                            ids = [int(ii) for ii in meta['ids']]\n                            for a in ids:\n                                if a == idx_worst: continue\n                                i1, i2 = min(a, idx_worst), max(a, idx_worst)\n                                edge_w[i1, i2] = edge_w[i1, i2] * self.hyperedge_boost + 1e-12\n                                edge_w[i2, i1] = edge_w[i1, i2]\n                        trust = min(self.max_trust, trust * self.trust_expand)\n                        improved_this_cycle = True\n                    else:\n                        # unsuccessful -> shrink trust a bit\n                        trust = max(self.min_trust, trust * self.trust_shrink)\n\n                # update best\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n\n                # keep edge matrix symmetric and non-zero diagonal zero\n                np.fill_diagonal(edge_w, 0.0)\n                edge_w[edge_w < 1e-12] = 1e-12\n\n                # update hyperedge weights: reward hyperedges used to propose this candidate\n                meta = pool_meta[int(s_idx)]\n                if 'ids' in meta and len(meta['ids']) >= 2:\n                    ids = tuple(sorted([int(ii) for ii in meta['ids']]))\n                    he = frozenset(ids)\n                    hyper_w[he] = hyper_w.get(he, 0.0) + self.hyperedge_boost\n                    # also boost pairwise edges among contributors\n                    for ii in range(len(ids)):\n                        for jj in range(ii+1, len(ids)):\n                            a, b = ids[ii], ids[jj]\n                            if a == b: continue\n                            i1, i2 = min(a,b), max(a,b)\n                            edge_w[i1, i2] = edge_w[i1, i2] * (1.0 + 0.4/(1.0+abs(pop_f[a]-pop_f[b]))) + 1e-12\n                            edge_w[i2, i1] = edge_w[i1, i2]\n\n                # if budget low, break early\n                if evals >= self.budget:\n                    break\n\n            # cycle end: stagnation handling and hyperedge burst\n            if improved_this_cycle:\n                cycle_no_improve = 0\n            else:\n                cycle_no_improve += 1\n                if cycle_no_improve >= 3 and evals < self.budget:\n                    # evaluate some centroids and intersection probes of top hyperedges\n                    if hyper_w:\n                        # select top hyperedges\n                        he_items = sorted(hyper_w.items(), key=lambda x: x[1], reverse=True)\n                        topk = min(6, max(1, len(he_items)))\n                        selected_hes = [he_items[i][0] for i in range(topk)]\n                        for he in selected_hes:\n                            if evals >= self.budget: break\n                            ids = sorted(list(he))\n                            centroid = pop[ids].mean(axis=0)\n                            x_mid = centroid + 0.28 * trust * rng.standard_normal(dim)\n                            x_mid = np.clip(x_mid, lb, ub)\n                            f_mid = float(func(x_mid)); evals += 1\n                            idx_w = int(np.argmax(pop_f))\n                            if f_mid < pop_f[idx_w]:\n                                pop[idx_w] = x_mid.copy(); pop_f[idx_w] = f_mid\n                                hyper_w[he] = hyper_w.get(he, 0.0) + self.hyperedge_boost\n                                # boost edges among centroid contributors\n                                for ii in range(len(ids)):\n                                    for jj in range(ii+1, len(ids)):\n                                        a, b = ids[ii], ids[jj]\n                                        i1,i2 = min(a,b), max(a,b)\n                                        edge_w[i1,i2] = edge_w[i1,i2] * (1.0 + self.hyperedge_boost) + 1e-12\n                                        edge_w[i2,i1] = edge_w[i1,i2]\n                                if f_mid < f_best:\n                                    f_best = f_mid; x_best = x_mid.copy()\n                        trust = max(self.min_trust, trust * 0.82)\n                    cycle_no_improve = 0\n\n            # early break to allow final refinement when low budget remains\n            if self.budget - evals <= max(6, dim*2):\n                break\n\n        # final local refinement: adaptive coordinate descent with diminishing step and occasional small random restarts around elites\n        step = 0.22 * range_norm\n        step_min = max(1e-8, 1e-6 * range_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            # coordinate order randomization\n            order_dims = list(range(dim))\n            rng.shuffle(order_dims)\n            for d in order_dims:\n                if evals >= self.budget: break\n                # positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                # negative\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                # occasional small random restart around elites to escape tiny plateaus\n                if rng.random() < 0.12 and evals < self.budget:\n                    elite_choice = rng.choice(max(1, min(self.pop_size, int(max(2, self.pop_size*0.1)))))\n                    center = pop[elite_choice]\n                    x_rs = np.clip(center + 0.9 * step * rng.standard_normal(dim), lb, ub)\n                    f_rs = float(func(x_rs)); evals += 1\n                    if f_rs < f_best:\n                        f_best = f_rs; x_best = x_rs.copy(); improved_local = True\n                    # else reduce step\n                    step *= 0.5\n                else:\n                    step *= 0.5\n            else:\n                # if improved, slightly enlarge step\n                step = min(0.8 * range_norm, step * 1.12)\n\n        # final record\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["45bacdb1-c496-4bcb-8a7b-2ccd06451f70"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 5192.0, "Edges": 5191.0, "Max Degree": 42.0, "Min Degree": 1.0, "Mean Degree": 1.9996147919876734, "Degree Variance": 2.179891993371336, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 9.356444066354742, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3233117871918967, "Depth Entropy": 2.300405376528346, "Assortativity": 9.241847017918483e-09, "Average Eccentricity": 21.042180277349768, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.0001926040061633282, "Average Shortest Path": 11.91051048706737, "mean_complexity": 37.666666666666664, "total_complexity": 113.0, "mean_token_count": 1503.0, "total_token_count": 4509.0, "mean_parameter_count": 5.666666666666667, "total_parameter_count": 17.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "19291376-7d6f-4b08-9892-a85df9af9001", "fitness": "-inf", "name": "SimplicialEdgeDensification", "description": "Simplicial Edge-Densification with PCA/Local-Surrogate Scoring \u2014 generate very large pools of edge- and simplex-derived candidates (edges, triangle barycenters, extrapolations, orthogonal perturbations), cheaply score them with local PCA-weighted linear surrogates and inverse-distance ensembles, then selectively evaluate only the most promising ones to dramatically increase \"edge density\" while preserving evaluation budget.", "code": "import numpy as np\n\nclass SimplicialEdgeDensification:\n    \"\"\"\n    Simplicial Edge-Densification with PCA/Local-Surrogate Scoring (SEDS)\n\n    Main ideas:\n      - Keep a compact elite archive and explicit k-NN edges/triangles among elites.\n      - Densify edges by generating a large pool of candidates along edges and inside small simplices (triangles/tetrahedra).\n      - Score candidates cheaply with local PCA-weighted linear surrogates and inverse-distance weighted ensembles (no function calls).\n      - Evaluate only a small selection of top-ranked candidates per cycle, increasing true edge density while saving budget.\n      - Adapt edge weights by success history and inject occasional global/opposite/Levy proposals to avoid stagnation.\n    Works on box [-5,5]^dim, enforces exact evaluation budget.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, rng=None,\n                 pop_size=40, knn=6,\n                 proposals_per_edge=6, proposals_per_triangle=4,\n                 candidate_pool_cap=800, evals_per_cycle=3):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        self.pop_size = max(6, int(pop_size))\n        self.knn = max(1, int(knn))\n\n        # densification parameters\n        self.proposals_per_edge = max(1, int(proposals_per_edge))\n        self.proposals_per_triangle = max(0, int(proposals_per_triangle))\n        self.candidate_pool_cap = int(candidate_pool_cap)\n        self.evals_per_cycle = max(1, int(evals_per_cycle))\n\n        # step size\n        self.sigma0 = 0.5 * (self.ub - self.lb)\n        self.sigma = float(self.sigma0)\n        self.sigma_min = 1e-5\n        self.sigma_max = 5.0\n        self.success_shrink = 0.88\n        self.failure_expand = 1.035\n\n        # bookkeeping\n        self.edge_refresh = 10\n        self.max_archive = max(500, 40 * self.dim)\n\n        # surrogate params\n        self.local_k = min(28, max(6, 6 + self.dim))  # neighbors for local surrogate\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb)\n        ub = np.full(dim, self.ub)\n        width = ub - lb\n\n        evals = 0\n\n        # INITIALIZATION: space-filling mini-sample\n        n_init = int(min(budget, max(8, int(0.12 * budget), 6 * dim)))\n        X_archive = []\n        f_archive = []\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            X_archive.append(np.array(x, copy=True))\n            f_archive.append(float(f))\n            evals += 1\n            if evals >= budget:\n                break\n\n        # ensure at least one evaluation\n        if len(X_archive) == 0:\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_archive.append(x0.copy())\n            f_archive.append(float(f0))\n            evals = 1\n\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # helper: population build\n        def build_population(Xa, fa):\n            order = np.argsort(fa)\n            k = min(self.pop_size, len(order))\n            inds = order[:k]\n            return Xa[inds].copy(), fa[inds].copy()\n\n        pop_X, pop_f = build_population(X_archive, f_archive)\n        best_idx = int(np.argmin(pop_f))\n        x_best = pop_X[best_idx].copy()\n        f_best = float(pop_f[best_idx])\n\n        # graph structures\n        edges = []           # list of (i,j)\n        triangles = []       # list of (i,j,k)\n        edge_weights = np.array([])\n        edge_success = {}\n\n        def refresh_graph():\n            nonlocal edges, edge_weights, triangles\n            n = pop_X.shape[0]\n            edges = []\n            triangles = []\n            if n < 2:\n                edge_weights = np.array([])\n                return\n            # distances\n            D = np.linalg.norm(pop_X[:, None, :] - pop_X[None, :, :], axis=2)\n            kn = min(self.knn, n - 1)\n            pairs = set()\n            neighbors = []\n            for i in range(n):\n                neigh = np.argsort(D[i])[1:1 + kn]\n                neighbors.append(list(neigh))\n                for j in neigh:\n                    a, b = (i, j) if i < j else (j, i)\n                    pairs.add((a, b))\n            edges = sorted(list(pairs))\n\n            # triangles: for each node, form triangles from neighbor pairs\n            tri_set = set()\n            for i in range(n):\n                neigh = neighbors[i]\n                ln = len(neigh)\n                for a_idx in range(ln):\n                    for b_idx in range(a_idx + 1, ln):\n                        a = neigh[a_idx]\n                        b = neigh[b_idx]\n                        tri = tuple(sorted((i, a, b)))\n                        if len(set(tri)) == 3:\n                            tri_set.add(tri)\n            triangles = sorted(list(tri_set))\n\n            if len(edges) == 0:\n                edge_weights = np.array([])\n                return\n\n            # edge weights prefer edges involving good vertices and edges with past success\n            fmin, fmax = np.min(pop_f), np.max(pop_f)\n            rng_scale = max(fmax - fmin, 1e-9)\n            w = []\n            for e in edges:\n                i, j = e\n                # base preference: at least one endpoint is good\n                base = -min(pop_f[i], pop_f[j])\n                # include geometry: shorter edges get somewhat preferentially sampled (higher \"density\")\n                length = np.linalg.norm(pop_X[i] - pop_X[j])\n                length_score = np.exp(-length / (0.5 * np.mean(width) + 1e-12))\n                succ = edge_success.get(e, 0.0)\n                w.append(np.exp( (base - fmin) / (rng_scale + 1e-12) ) * (0.6 + 0.5 * length_score) * (1.0 + 0.6 * succ))\n            edge_weights = np.array(w, dtype=float)\n            if edge_weights.sum() <= 0 or not np.isfinite(edge_weights.sum()):\n                edge_weights = np.ones(len(edges), dtype=float)\n            edge_weights = edge_weights / np.sum(edge_weights)\n\n        refresh_graph()\n\n        iter_since_improve = 0\n        total_iters = 0\n\n        # helper: nearest neighbors indices in archive (for surrogate)\n        def nearest_indices(Xq, Xref, k):\n            # Xq: (m,dim) or (dim,) ; returns indices to Xref\n            if Xq.ndim == 1:\n                dists = np.linalg.norm(Xref - Xq, axis=1)\n                return np.argsort(dists)[:k]\n            else:\n                d = np.linalg.norm(Xref[None, :, :] - Xq[:, None, :], axis=2)\n                return np.argsort(d, axis=1)[:, :k]\n\n        # local surrogate: attempt weighted linear regression (affine) on nearest points\n        def local_surrogate_predict(xcandidates, Xref, fref, k_neighbors=None):\n            # xcandidates: (m,dim)\n            m = xcandidates.shape[0]\n            if k_neighbors is None:\n                k_neighbors = min(self.local_k, Xref.shape[0])\n            preds = np.zeros(m, dtype=float)\n            uncertainties = np.ones(m, dtype=float) * 1e6\n            # for each candidate, build local fit if enough neighbors\n            idxs = nearest_indices(xcandidates, Xref, min(k_neighbors, Xref.shape[0]))\n            for idx_c in range(m):\n                inds = idxs[idx_c]\n                Xn = Xref[inds]\n                fn = fref[inds]\n                # distances and weights\n                d = np.linalg.norm(Xn - xcandidates[idx_c], axis=1)\n                # avoid zeros\n                d = np.maximum(d, 1e-10)\n                w = 1.0 / (d + 1e-8)\n                w = w / w.sum()\n                # if there are enough neighbors relative to dim, do weighted linear regression\n                if Xn.shape[0] >= min( max(6, dim//2 + 2), dim + 2):\n                    # center data\n                    x0 = np.average(Xn, axis=0, weights=w)\n                    A = Xn - x0  # (k,dim)\n                    y = fn - np.dot(w, fn)  # centered\n                    # weighted least squares ridge\n                    W = np.sqrt(w)[:, None]\n                    A_w = A * W\n                    y_w = y * np.sqrt(w)\n                    # solve for coefficients b in y ~ A b (no intercept because centered)\n                    # use regularization dependent on variance\n                    reg = 1e-6 * np.eye(dim)\n                    try:\n                        b, *_ = np.linalg.lstsq(A_w.T @ A_w + reg, A_w.T @ y_w, rcond=None)\n                        pred = (np.dot(xcandidates[idx_c] - x0, b) + np.dot(w, fn))\n                        # compute residuals to estimate uncertainty\n                        y_pred_loc = A @ b + np.dot(w, fn)\n                        resid = fn - y_pred_loc\n                        uncert = np.sqrt(np.maximum(1e-12, np.dot(w, resid**2)))\n                    except Exception:\n                        pred = np.dot(w, fn)\n                        uncert = np.std(fn) + 1e-6\n                else:\n                    # fallback: inverse-distance weighted average\n                    pred = np.dot(w, fn)\n                    uncert = np.sqrt(np.maximum(1e-12, np.dot(w, (fn - pred)**2)))\n                preds[idx_c] = pred\n                uncertainties[idx_c] = uncert\n            return preds, uncertainties\n\n        # candidate generation: many cheap candidates around edges/triangles\n        def generate_candidate_pool():\n            pool = []\n            meta = []  # meta info: ('edge', edge_idx) or ('tri', tri_idx) or ('local','i') or ('global',None)\n            # edge-based proposals\n            if len(edges) > 0:\n                # sample edges proportionally to edge_weights, but cap total proposals\n                tot_edge_proposals = int(len(edges) * self.proposals_per_edge)\n                # compute how many proposals per edge (at least 1 for top edges)\n                probs = edge_weights.copy()\n                probs = probs / probs.sum()\n                # sample edges with replacement to create a list\n                chosen_edges_idx = self.rng.choice(len(edges), size=tot_edge_proposals, p=probs)\n                for eidx in chosen_edges_idx:\n                    i, j = edges[eidx]\n                    xi = pop_X[i]\n                    xj = pop_X[j]\n                    dir_vec = xj - xi\n                    dir_norm = np.linalg.norm(dir_vec) + 1e-12\n                    u = dir_vec / dir_norm\n                    # choose alpha near midpoint more often\n                    alpha = self.rng.beta(2.0, 2.0)\n                    alpha = np.clip(alpha + self.rng.uniform(-0.25, 0.25), -0.35, 1.35)\n                    base = alpha * xi + (1 - alpha) * xj\n                    # orthogonal perturbation: choose random vector and remove projection on u\n                    noise = self.rng.normal(size=dim)\n                    proj = np.dot(noise, u) * u\n                    orth = noise - proj\n                    onorm = np.linalg.norm(orth)\n                    if onorm > 1e-12:\n                        orth = orth / onorm\n                    else:\n                        orth = np.eye(dim)[self.rng.integers(0, dim)]\n                    orth_mag = self.sigma * (0.3 + 1.5 * self.rng.random()) * (min(dir_norm / (np.mean(width) + 1e-12), 1.2))\n                    along_noise = self.rng.normal(scale=0.12 * dir_norm)\n                    xcand = base + along_noise * u + orth_mag * orth\n                    # slight anisotropic jitter\n                    xcand = xcand + 0.03 * self.sigma * self.rng.normal(size=dim)\n                    # clip\n                    xcand = np.minimum(np.maximum(xcand, lb), ub)\n                    pool.append(xcand)\n                    meta.append(('edge', eidx))\n\n            # triangle-based proposals (barycentric samples)\n            if len(triangles) > 0 and self.proposals_per_triangle > 0:\n                total_tri = int(len(triangles) * self.proposals_per_triangle)\n                # pick triangles uniformly\n                tri_idx_choices = self.rng.integers(0, len(triangles), size=total_tri)\n                for tidx in tri_idx_choices:\n                    a, b, c = triangles[tidx]\n                    xa, xb, xc = pop_X[a], pop_X[b], pop_X[c]\n                    # sample barycentric coords via Dirichlet\n                    w = self.rng.gamma(1.2, 1.0, size=3)\n                    w = w / w.sum()\n                    base = w[0] * xa + w[1] * xb + w[2] * xc\n                    # local PCA to get main plane (triangle usually 2D) and perturb orthogonally small\n                    M = np.vstack([xa, xb, xc])\n                    # center\n                    Mc = M - M.mean(axis=0)\n                    # small SVD\n                    try:\n                        U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                        # normal is last row of Vt if rank>=2\n                        normal = Vt[-1] if Vt.shape[0] >= 2 else Vt[-1]\n                        normal = normal / (np.linalg.norm(normal) + 1e-12)\n                    except Exception:\n                        normal = self.rng.normal(size=dim)\n                        normal = normal / (np.linalg.norm(normal) + 1e-12)\n                    orth_mag = 0.6 * self.sigma * (0.2 + self.rng.random())\n                    xcand = base + orth_mag * normal + 0.02 * self.sigma * self.rng.normal(size=dim)\n                    xcand = np.minimum(np.maximum(xcand, lb), ub)\n                    pool.append(xcand)\n                    meta.append(('tri', tidx))\n\n            # local refinement candidates around elites\n            for i in range(pop_X.shape[0]):\n                if len(pool) >= self.candidate_pool_cap:\n                    break\n                xi = pop_X[i]\n                # sample couple of local jits\n                n_local = 2\n                for _ in range(n_local):\n                    scale = self.sigma * (0.2 + 0.9 * self.rng.random())\n                    xcand = xi + scale * self.rng.normal(size=dim)\n                    xcand = np.minimum(np.maximum(xcand, lb), ub)\n                    pool.append(xcand)\n                    meta.append(('local', i))\n                    if len(pool) >= self.candidate_pool_cap:\n                        break\n                if len(pool) >= self.candidate_pool_cap:\n                    break\n\n            # global/opposite/levy to ensure exploration\n            n_global = max(3, int(0.02 * self.candidate_pool_cap))\n            for _ in range(n_global):\n                if len(pool) >= self.candidate_pool_cap:\n                    break\n                if self.rng.random() < 0.6:\n                    # opposite of random elite\n                    idx = self.rng.integers(0, pop_X.shape[0])\n                    opp = lb + ub - pop_X[idx]\n                    xcand = opp + 0.15 * self.sigma * self.rng.normal(size=dim)\n                else:\n                    # Levy-like step\n                    step = np.tan(np.pi * (self.rng.random(dim) - 0.5)) * 0.02 * (self.ub - self.lb)\n                    xcand = x_best + self.sigma * step\n                xcand = np.minimum(np.maximum(xcand, lb), ub)\n                pool.append(xcand)\n                meta.append(('global', None))\n                if len(pool) >= self.candidate_pool_cap:\n                    break\n\n            # cap pool\n            if len(pool) > self.candidate_pool_cap:\n                pool = pool[:self.candidate_pool_cap]\n                meta = meta[:self.candidate_pool_cap]\n\n            pool = np.array(pool)\n            return pool, meta\n\n        # main loop\n        while evals < budget:\n            total_iters += 1\n            # adapt probabilities a bit\n            frac = evals / max(1.0, budget)\n            # generate a big candidate pool and score them with surrogate\n            pool, meta = generate_candidate_pool()\n            if pool.shape[0] == 0:\n                # fallback to global sample\n                x_candidate = self.rng.uniform(lb, ub)\n                f_candidate = float(func(x_candidate))\n                evals += 1\n                X_archive = np.vstack([X_archive, x_candidate])\n                f_archive = np.concatenate([f_archive, [f_candidate]])\n                if f_candidate < f_best:\n                    f_best = f_candidate\n                    x_best = x_candidate.copy()\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n                # update population occasionally\n                if (evals % self.edge_refresh) == 0:\n                    pop_X, pop_f = build_population(X_archive, f_archive)\n                    refresh_graph()\n                continue\n\n            # compute surrogate predictions and uncertainties for the pool\n            # clips/normalization might be useful\n            preds, uncerts = local_surrogate_predict(pool, X_archive, f_archive, k_neighbors=min(self.local_k, X_archive.shape[0]))\n            # scoring: predicted value minus exploration_bonus * uncertainty - distance_penalty * dist_to_best\n            d2best = np.linalg.norm(pool - x_best[None, :], axis=1)\n            # weight factors adapt over time: early bias toward exploration, later exploitation\n            exploration_coeff = 1.5 * (1.0 - frac) + 0.2\n            distance_coeff = 0.05 + 0.25 * (1.0 - frac)\n            scores = preds - exploration_coeff * uncerts + (- distance_coeff * (d2best / (np.linalg.norm(width) + 1e-12)))\n            # lower predicted objective is better -> sort ascending\n            order = np.argsort(scores)\n\n            # choose how many to evaluate this cycle based on remaining budget\n            remaining = budget - evals\n            to_eval = min(remaining, self.evals_per_cycle)\n            # but if pool suggests many good candidates, we might evaluate more early on (cap)\n            to_eval = max(1, to_eval)\n\n            eval_indices = []\n            # pick top candidates but ensure some diversity (not all from same meta)\n            picked_meta_types = set()\n            pick_ptr = 0\n            while len(eval_indices) < to_eval and pick_ptr < pool.shape[0]:\n                idx = order[pick_ptr]\n                # prefer to pick a mix of types; allow repeats after some threshold\n                typ, info = meta[idx]\n                if typ not in picked_meta_types or len(picked_meta_types) >= 3 or self.rng.random() < 0.35:\n                    eval_indices.append(idx)\n                    picked_meta_types.add(typ)\n                pick_ptr += 1\n            # if none picked fallback to top ones\n            if len(eval_indices) == 0:\n                eval_indices = list(order[:to_eval])\n\n            # evaluate selected candidates sequentially\n            for idx in eval_indices:\n                if evals >= budget:\n                    break\n                x_candidate = pool[idx]\n                f_candidate = float(func(x_candidate))\n                evals += 1\n\n                # update archive\n                X_archive = np.vstack([X_archive, x_candidate])\n                f_archive = np.concatenate([f_archive, [f_candidate]])\n\n                improved = False\n                if f_candidate < f_best:\n                    f_best = float(f_candidate)\n                    x_best = x_candidate.copy()\n                    improved = True\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n\n                # adapt sigma\n                if improved:\n                    self.sigma = max(self.sigma_min, self.sigma * self.success_shrink)\n                else:\n                    self.sigma = min(self.sigma_max, self.sigma * self.failure_expand)\n\n                # update population compact set\n                if pop_X.shape[0] < self.pop_size:\n                    pop_X = np.vstack([pop_X, x_candidate])\n                    pop_f = np.concatenate([pop_f, [f_candidate]])\n                else:\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_candidate < pop_f[worst_idx]:\n                        pop_X[worst_idx] = x_candidate\n                        pop_f[worst_idx] = f_candidate\n                    else:\n                        # occasional replacement for diversity\n                        if self.rng.random() < 0.015:\n                            ridx = self.rng.integers(0, pop_X.shape[0])\n                            pop_X[ridx] = x_candidate\n                            pop_f[ridx] = f_candidate\n\n                # update edge success counters if this candidate was generated from an edge/tri\n                typ, info = meta[idx]\n                if typ == 'edge':\n                    eidx = info\n                    if eidx >= 0 and eidx < len(edges):\n                        e = edges[eidx]\n                        # success measure: how much better relative to endpoints\n                        i, j = e\n                        baseline = min(pop_f[i], pop_f[j])\n                        if f_candidate < baseline - 1e-12:\n                            edge_success[e] = edge_success.get(e, 0.0) + 1.0\n                        else:\n                            # slight decay/penalty for failures\n                            edge_success[e] = edge_success.get(e, 0.0) * 0.985\n                elif typ == 'tri':\n                    tidx = info\n                    if tidx >= 0 and tidx < len(triangles):\n                        tri = triangles[tidx]\n                        baseline = min(pop_f[list(tri)])\n                        if f_candidate < baseline - 1e-12:\n                            # boost all edges in that triangle a bit\n                            a, b, c = tri\n                            tri_edges = [tuple(sorted((a, b))), tuple(sorted((a, c))), tuple(sorted((b, c)))]\n                            for e in tri_edges:\n                                edge_success[e] = edge_success.get(e, 0.0) + 0.6\n                        else:\n                            # slight decay\n                            for e in [tuple(sorted((tri[0], tri[1]))), tuple(sorted((tri[0], tri[2]))), tuple(sorted((tri[1], tri[2])))]:\n                                edge_success[e] = edge_success.get(e, 0.0) * 0.992\n                # optionally update best immediate\n\n                # bound archive size\n                if X_archive.shape[0] > self.max_archive:\n                    keep_best = int(self.max_archive * 0.6)\n                    best_inds = np.argsort(f_archive)[:keep_best]\n                    # random keep for remainder\n                    others = np.setdiff1d(np.arange(X_archive.shape[0]), best_inds)\n                    if others.size > 0:\n                        pick_other = self.rng.choice(others, size=self.max_archive - keep_best, replace=False)\n                        keep = np.unique(np.concatenate([best_inds, pick_other]))\n                    else:\n                        keep = best_inds\n                    X_archive = X_archive[keep]\n                    f_archive = f_archive[keep]\n\n            # periodically rebuild population & graph using updated archive\n            if (evals % self.edge_refresh) == 0 or iter_since_improve > max(80, 6 * dim):\n                pop_X, pop_f = build_population(X_archive, f_archive)\n                # sync edge_success keys to real edge tuples (some edges may map to new indices)\n                # easiest approach: reset successes if population changed a lot\n                edge_success = {k: v for k, v in edge_success.items() if isinstance(k, tuple)}\n                refresh_graph()\n\n            # emergency diversification if stagnation\n            if iter_since_improve > max(120, 8 * dim):\n                # inject a stronger global probe but ensure budget\n                if evals < budget:\n                    xg = self.rng.uniform(lb, ub)\n                    fg = float(func(xg))\n                    evals += 1\n                    X_archive = np.vstack([X_archive, xg])\n                    f_archive = np.concatenate([f_archive, [fg]])\n                    if fg < f_best:\n                        f_best = fg\n                        x_best = xg.copy()\n                        iter_since_improve = 0\n                    # update pop\n                    if pop_X.shape[0] < self.pop_size:\n                        pop_X = np.vstack([pop_X, xg])\n                        pop_f = np.concatenate([pop_f, [fg]])\n                    else:\n                        worst_idx = int(np.argmax(pop_f))\n                        if fg < pop_f[worst_idx]:\n                            pop_X[worst_idx] = xg\n                            pop_f[worst_idx] = fg\n                    refresh_graph()\n\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["701bfc7e-c1fd-40fc-8533-beb7ddd599a9"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4609.0, "Edges": 4608.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.999566066391842, "Degree Variance": 2.0733345914803176, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.75442795595979, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3283240707180917, "Depth Entropy": 2.1727471176240845, "Assortativity": 7.555756618465758e-09, "Average Eccentricity": 20.121067476676068, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.00021696680407897592, "Average Shortest Path": 11.611448991707046, "mean_complexity": 12.0, "total_complexity": 84.0, "mean_token_count": 572.1428571428571, "total_token_count": 4005.0, "mean_parameter_count": 3.0, "total_parameter_count": 21.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "efb33a12-13f2-4a5d-9b4e-785879562267", "fitness": 0.32305330578892294, "name": "AdaptiveSpiralLevyDenseEdgeFusion", "description": "Edge-Densified Spiral-Levy Graph Fusion \u2014 massively densify elite edges with virtual nodes and surrogate-ranked candidate pools, then evaluate only the most promising edge/triangle recombinations combined with spiral+L\u00e9vy local moves to exploit high edge density under a tight budget.", "code": "import numpy as np\n\nclass AdaptiveSpiralLevyDenseEdgeFusion:\n    \"\"\"\n    Edge-Densified Spiral-Levy Graph Fusion (improved from ASL-EdgeGraph)\n    - budget: max number of function evaluations\n    - dim: problem dimensionality\n    Optional args: pop_size, elite_frac, k_edges, proposals_per_edge, levy_prob, seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.2,\n                 k_edges=None, proposals_per_edge=3, levy_prob=0.08, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.levy_prob = float(levy_prob)\n        self.elite_frac = float(elite_frac)\n        self.proposals_per_edge = int(proposals_per_edge)\n\n        if pop_size is None:\n            base = int(max(12, min(72, np.clip(np.sqrt(self.budget) * 1.3, 12, 72))))\n            self.pop_size = max(base, dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        if k_edges is None:\n            self.k_edges = max(2, min(self.pop_size - 1, int(np.clip(self.dim, 2, 14))))\n        else:\n            self.k_edges = int(k_edges)\n\n        # adaptation rates\n        self.sigma_init = 0.10\n        self.alpha_init = 0.4\n        self.success_increase = 1.22\n        self.failure_decrease = 0.82\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        sigma_scale = self.sigma_init * range_norm\n        alpha_scale = self.alpha_init * range_norm\n\n        evals = 0\n\n        # initialize population\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        sigma = np.full(self.pop_size, sigma_scale)\n        alpha = np.full(self.pop_size, alpha_scale)\n        success_counts = np.zeros(self.pop_size, dtype=int)\n\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        no_improve = 0\n        max_no_improve = max(25, self.pop_size * 3)\n\n        # Helper: densify edges by adding \"virtual\" nodes around elites (no evals) and create many candidate recombinations.\n        def build_dense_edges():\n            # choose elites\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count]\n            # create virtual nodes around each elite (used only for graph connectivity)\n            V = 3  # number of virtual nodes per elite\n            virtuals = []\n            virtual_map = []  # maps virtual to elite index\n            for e in elites:\n                base = pop[e]\n                # small jitter scaled by current sigma of elite if available\n                local_sigma = sigma[e] if e < len(sigma) else sigma_scale\n                for _ in range(V):\n                    jitter = 0.35 * local_sigma * rng.standard_normal(dim)\n                    virtuals.append(base + jitter)\n                    virtual_map.append(int(e))\n            # build augmented set (real + virtual for neighbor computation)\n            X_aug = np.vstack([pop, np.asarray(virtuals) if len(virtuals) > 0 else np.empty((0, dim))])\n            N_aug = X_aug.shape[0]\n            # compute squared distances (vectorized)\n            dif = X_aug[:, None, :] - X_aug[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            edges = set()\n            # For each virtual node, connect to k nearest real population indices (excluding virtual-self)\n            n_virtuals = len(virtuals)\n            for v_idx in range(self.pop_size, self.pop_size + n_virtuals):\n                neigh = np.argsort(dist2[v_idx])\n                taken = 0\n                for j in neigh:\n                    if j == v_idx: \n                        continue\n                    # map indices > pop_size-1 as virtual -> their mapped real elite\n                    if j < self.pop_size:\n                        edges.add(tuple(sorted((virtual_map[v_idx - self.pop_size], j))))\n                    else:\n                        # connect virtual to virtual's mapped real as well\n                        edges.add(tuple(sorted((virtual_map[v_idx - self.pop_size], virtual_map[j - self.pop_size]))))\n                    taken += 1\n                    if taken >= self.k_edges:\n                        break\n            # Also ensure dense inter-elite connections: connect every elite to its k nearest elites\n            elite_positions = pop[elites]\n            edif = elite_positions[:, None, :] - elite_positions[None, :, :]\n            ed2 = np.sum(edif * edif, axis=2)\n            for idx_i, i in enumerate(elites):\n                neigh_idx = np.argsort(ed2[idx_i])\n                added = 0\n                for ni in neigh_idx:\n                    if ni == idx_i: continue\n                    j = elites[ni]\n                    edges.add(tuple(sorted((int(i), int(j)))))\n                    added += 1\n                    if added >= self.k_edges:\n                        break\n            # Add random extra edges to boost graph rich connectivity\n            extra = max(0, self.pop_size // 2)\n            for _ in range(extra):\n                a = int(rng.integers(0, self.pop_size))\n                b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges.add(tuple(sorted((a, b))))\n            return list(edges)\n\n        # Candidate generator for an edge, creates a pool then surrogate-ranks them\n        def edge_candidate_pool(i, j, pool_size=16):\n            xi, xj = pop[i], pop[j]\n            fi, fj = pop_f[i], pop_f[j]\n            edge_vec = xj - xi\n            edge_len = np.linalg.norm(edge_vec) + 1e-12\n            dir_unit = edge_vec / edge_len\n            candidates = []\n            # sample weights that emphasize near-endpoints but also extrapolate\n            for _ in range(pool_size):\n                # sample from a mixture to get dense sampling near endpoints and some extrapolation\n                r = rng.random()\n                if r < 0.65:\n                    # Beta distribution biased to endpoints (0 or 1)\n                    if rng.random() < 0.5:\n                        w = np.clip(rng.beta(2.0, 5.0), 0.0, 1.0)\n                    else:\n                        w = np.clip(1.0 - rng.beta(2.0, 5.0), 0.0, 1.0)\n                elif r < 0.9:\n                    w = rng.uniform(0.0, 1.0)\n                else:\n                    # extrapolate\n                    w = rng.uniform(-0.7, 1.7)\n                base = (1.0 - w) * xi + w * xj\n                # orthogonal perturbation: create a perpendicular component\n                noise_along = 0.5 * (sigma[i] + sigma[j]) * rng.standard_normal() * dir_unit\n                # generate perpendicular direction by random vector minus its projection\n                rv = rng.standard_normal(dim)\n                rv = rv - np.dot(rv, dir_unit) * dir_unit\n                if np.linalg.norm(rv) > 0:\n                    rv = rv / np.linalg.norm(rv)\n                perp_scale = (sigma[i] + sigma[j]) * 0.6 * rng.random()\n                perp = perp_scale * rv\n                jitter = 0.35 * (sigma[i] + sigma[j]) * rng.standard_normal(dim)\n                x_cand = base + noise_along + perp + jitter\n                x_cand = np.clip(x_cand, lb, ub)\n                # surrogate prediction: linear interpolation + penalty for perpendicular distance\n                t_proj = np.dot(x_cand - xi, edge_vec) / (edge_len * edge_len)\n                lin_pred = (1.0 - t_proj) * fi + t_proj * fj\n                perp_dist = np.linalg.norm(x_cand - (xi + np.clip(t_proj, 0.0, 1.0) * edge_vec))\n                grad_est = (fj - fi) / (edge_len + 1e-12)\n                surrogate = lin_pred + 0.5 * abs(grad_est) * perp_dist + 0.02 * perp_dist * range_norm\n                candidates.append((surrogate, x_cand))\n            # sort by surrogate ascending and return top-K recommendation without evaluating most\n            candidates.sort(key=lambda x: x[0])\n            return [c[1] for c in candidates]\n\n        # Triangle barycentric candidate pooling (dense local mixing)\n        def triangle_candidates(indices, pool_size=10):\n            a, b, c = indices\n            xa, xb, xc = pop[a], pop[b], pop[c]\n            fa, fb, fc = pop_f[a], pop_f[b], pop_f[c]\n            candidates = []\n            for _ in range(pool_size):\n                # sample barycentric coordinates skewed toward vertices\n                r1 = rng.random()\n                r2 = rng.random()\n                sqrt_r1 = np.sqrt(r1)\n                u = 1 - sqrt_r1\n                v = sqrt_r1 * (1 - r2)\n                w = sqrt_r1 * r2\n                base = u * xa + v * xb + w * xc\n                # small perturbation\n                jitter = 0.4 * (sigma[a] + sigma[b] + sigma[c]) / 3.0 * rng.standard_normal(dim)\n                x_cand = np.clip(base + jitter, lb, ub)\n                # surrogate: barycentric weighted f\n                sur = u * fa + v * fb + w * fc + 0.03 * np.linalg.norm(jitter) * range_norm\n                candidates.append((sur, x_cand))\n            candidates.sort(key=lambda x: x[0])\n            return [c[1] for c in candidates]\n\n        # Main optimization loop\n        while evals < self.budget:\n            rem = self.budget - evals\n            improved_in_loop = False\n\n            edges = build_dense_edges()\n            # shuffle to diversify order\n            rng.shuffle(edges)\n\n            # For efficiency: tune per-edge evaluation budget based on remaining budget\n            # We will generate a large candidate pool per edge but evaluate only top K_per_edge items by surrogate\n            K_eval_per_edge = max(1, min(3, rem // max(1, len(edges))))\n            # but ensure at least a little per-edge in early phase\n            if evals < self.budget * 0.6:\n                K_eval_per_edge = min(K_eval_per_edge + 1, 4)\n\n            for (i, j) in edges:\n                if evals >= self.budget:\n                    break\n                # Avoid degenerate equal indices\n                if i == j: \n                    continue\n                # Build a candidate pool and evaluate only top predicted\n                pool = edge_candidate_pool(i, j, pool_size=12 + 2 * self.proposals_per_edge)\n                # optionally include a couple of targeted spiral probes along the best-to-worst direction\n                # select up to K_eval_per_edge candidates to evaluate\n                to_eval = pool[:K_eval_per_edge]\n                for x_prop in to_eval:\n                    if evals >= self.budget: break\n                    # occasional Levy injection into selected candidate to expand sampling\n                    if rng.random() < 0.06:\n                        u = rng.random(dim)\n                        cauchy = np.tan(np.pi * (u - 0.5))\n                        x_prop = np.clip(x_prop + 0.12 * range_norm * cauchy, lb, ub)\n                    f_prop = float(func(x_prop))\n                    evals += 1\n                    # Place candidate: prefer replacing worse endpoint or global worst\n                    fi, fj = pop_f[i], pop_f[j]\n                    replaced = False\n                    if f_prop < fi or f_prop < fj:\n                        # replace the worse endpoint\n                        if fi >= fj:\n                            ridx = i\n                        else:\n                            ridx = j\n                        pop[ridx] = x_prop\n                        pop_f[ridx] = f_prop\n                        sigma[ridx] = min(2.0 * range_norm, sigma[ridx] * self.success_increase)\n                        alpha[ridx] = min(1.5 * range_norm, alpha[ridx] * 1.03)\n                        success_counts[ridx] += 1\n                        replaced = True\n                        if f_prop < f_best:\n                            f_best = f_prop; x_best = x_prop.copy(); improved_in_loop = True\n                    else:\n                        # small chance to replace global worst with a promising candidate\n                        if rng.random() < 0.01:\n                            idx_worst = int(np.argmax(pop_f))\n                            if f_prop < pop_f[idx_worst]:\n                                pop[idx_worst] = x_prop\n                                pop_f[idx_worst] = f_prop\n                                sigma[idx_worst] = sigma_scale\n                                replaced = True\n                                if f_prop < f_best:\n                                    f_best = f_prop; x_best = x_prop.copy(); improved_in_loop = True\n                    # failure adjustment: if neither endpoint improved, slightly shrink their sigmas\n                    if not replaced:\n                        sigma[i] *= self.failure_decrease\n                        sigma[j] *= self.failure_decrease\n\n                # occasional triangle mixing among neighbors of i and j (dense local mixing)\n                if rng.random() < 0.12 and evals + 2 < self.budget:\n                    # pick neighbor k different from i and j\n                    k = int(rng.integers(0, self.pop_size))\n                    if k != i and k != j:\n                        tris = triangle_candidates((i, j, k), pool_size=6)\n                        # evaluate top 1-2 triangle candidates\n                        for x_tri in tris[:2]:\n                            if evals >= self.budget: break\n                            f_tri = float(func(x_tri))\n                            evals += 1\n                            idx_worst = int(np.argmax(pop_f))\n                            if f_tri < pop_f[idx_worst]:\n                                pop[idx_worst] = x_tri\n                                pop_f[idx_worst] = f_tri\n                                sigma[idx_worst] = sigma_scale\n                                if f_tri < f_best:\n                                    f_best = f_tri; x_best = x_tri.copy(); improved_in_loop = True\n\n            # Spiral-directed per-individual moves (as in original) but less frequent and more targeted\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n                # skip some individuals probabilistically to save budget unless they are elite or high-sigma\n                if rng.random() < 0.35 and pop_f[i] > np.median(pop_f):\n                    continue\n\n                xi = pop[i].copy()\n                fi = pop_f[i]\n                d = x_best - xi\n                nd = np.linalg.norm(d) + 1e-12\n                dir_vec = d / nd\n\n                # small randomized spiral rotation\n                if dim > 1 and rng.random() < 0.55:\n                    a, b = rng.choice(dim, size=2, replace=False)\n                    theta = rng.uniform(-np.pi / 3, np.pi / 3)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    da, db = d[a], d[b]\n                    ra = ca * da - sb * db\n                    rb = sb * da + ca * db\n                    d_rot = d.copy()\n                    d_rot[a], d_rot[b] = ra, rb\n                    norm_dr = np.linalg.norm(d_rot) + 1e-12\n                    dir_vec = d_rot / norm_dr\n\n                mov_scale = alpha[i] * (0.6 + rng.random() * 0.7)\n                move = mov_scale * dir_vec\n                move += sigma[i] * rng.standard_normal(dim)\n\n                # occasional moderate Levy\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    move += 0.16 * range_norm * cauchy\n\n                x_new = np.clip(xi + move, lb, ub)\n\n                # small opposition and centroid sampling with low prob\n                if rng.random() < 0.05 and evals < self.budget:\n                    x_opp = lb + ub - xi\n                    x_opp = np.clip(x_opp, lb, ub)\n                    f_opp = float(func(x_opp))\n                    evals += 1\n                    if f_opp < fi:\n                        pop[i] = x_opp; pop_f[i] = f_opp\n                        sigma[i] *= self.success_increase\n                        alpha[i] *= 1.02\n                        if f_opp < f_best:\n                            f_best = f_opp; x_best = x_opp.copy(); improved_in_loop = True\n                        continue\n\n                if evals >= self.budget:\n                    break\n                f_new = float(func(x_new))\n                evals += 1\n\n                if f_new < fi or rng.random() < 0.01:\n                    pop[i] = x_new\n                    pop_f[i] = f_new\n                    sigma[i] *= self.success_increase\n                    alpha[i] *= 1.02\n                    success_counts[i] += 1\n                    if f_new < f_best:\n                        f_best = f_new; x_best = x_new.copy(); improved_in_loop = True\n                else:\n                    sigma[i] *= self.failure_decrease\n                    alpha[i] *= 0.985\n\n            # clamp parameters\n            sigma = np.clip(sigma, 1e-8 * range_norm, 2.5 * range_norm)\n            alpha = np.clip(alpha, 1e-8 * range_norm, 1.6 * range_norm)\n\n            if improved_in_loop:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # stagnation: strong densification around best (resample many virtuals as new real candidates)\n            if no_improve >= max_no_improve and evals < self.budget:\n                no_improve = 0\n                n_reset = max(1, self.pop_size // 2)\n                for _ in range(n_reset):\n                    if evals >= self.budget:\n                        break\n                    if rng.random() < 0.8:\n                        newp = x_best + 0.05 * range_vec * rng.standard_normal(dim)\n                    else:\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    idx = int(rng.integers(0, self.pop_size))\n                    pop[idx] = newp; pop_f[idx] = f_new\n                    sigma[idx] = sigma_scale\n                    alpha[idx] = alpha_scale\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # random injection\n            if rng.random() < 0.045 and evals < self.budget:\n                r = rng.uniform(lb, ub)\n                fr = float(func(r))\n                evals += 1\n                idx_worst = int(np.argmax(pop_f))\n                if fr < pop_f[idx_worst]:\n                    pop[idx_worst] = r; pop_f[idx_worst] = fr\n                    sigma[idx_worst] = sigma_scale\n                    if fr < f_best:\n                        f_best = fr; x_best = r.copy()\n\n            # if low remaining budget, break to local refinement\n            if self.budget - evals < max(12, dim * 4):\n                break\n\n        # Final local mesh-adaptive coordinate refinement (pattern search)\n        mesh = 0.12 * range_norm\n        mesh_min = 1e-6 * range_norm\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveSpiralLevyDenseEdgeFusion scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5713cbd-1130-4e8d-abdc-3c3ca8966b71"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5748452844959415}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.6122963760561841}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.6328237672193571}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.049819151145595986}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.05630093649972989}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.06036141049936905}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.021699796310673092}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.03237163098436591}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.021556057735893974}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9236371707202945}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8988715471630035}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9610664580034354}], "aucs": [0.5748452844959415, 0.6122963760561841, 0.6328237672193571, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.049819151145595986, 0.05630093649972989, 0.06036141049936905, 0.021699796310673092, 0.03237163098436591, 0.021556057735893974, 0.9236371707202945, 0.8988715471630035, 0.9610664580034354]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4134.0, "Edges": 4133.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.999516207063377, "Degree Variance": 2.0595062971490394, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.413720316622692, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3331438715046091, "Depth Entropy": 2.0635497783833445, "Assortativity": 0.0, "Average Eccentricity": 19.756168359941945, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.00024189646831156264, "Average Shortest Path": 11.113130992468493, "mean_complexity": 16.166666666666668, "total_complexity": 97.0, "mean_token_count": 575.8333333333334, "total_token_count": 3455.0, "mean_parameter_count": 3.0, "total_parameter_count": 18.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "46fe4fc7-7522-40aa-93bb-166b21d33e89", "fitness": "-inf", "name": "EDCGS", "description": "Edge-Densified Covariance Graph Search (EDCGS) \u2014 build a compact elite graph, aggressively generate many edge-centered candidates (midpoints, extrapolations and orthogonal perturbations), rank them by a cheap edge-aware surrogate that rewards under-explored edges, evaluate only the most promising ones while adaptively updating a global covariance and step-size to densify productive edges and escape basins.", "code": "import numpy as np\nfrom itertools import combinations\n\nclass EDCGS:\n    \"\"\"\n    Edge-Densified Covariance Graph Search (EDCGS).\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional parameters:\n      - rng: numpy Generator or None\n      - init_samples: number of initial random samples (int or None to auto)\n      - elite_size: number of elites to maintain\n      - candidates_per_edge: how many candidates to propose per edge\n      - pool_factor: multiplier to create a larger candidate pool before ranking\n      - jump_prob: probability of a heavy-tailed jump among evaluated proposals\n      - c_cov: covariance learning rate\n      - sigma0: initial step scale (if None auto)\n      - stagnation_restart: number of evals without improvement to trigger diversification\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_samples=None, elite_size=None,\n                 candidates_per_edge=3, pool_factor=6,\n                 jump_prob=0.05, c_cov=None, sigma0=None,\n                 stagnation_restart=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        self.init_samples = init_samples\n        if self.init_samples is None:\n            # bias initial samples to dimension and budget\n            self.init_samples = max(12, min(120, int(0.06 * self.budget + 4 * self.dim)))\n\n        self.elite_size = (elite_size if elite_size is not None else\n                           max(4, min(24, int(2 + self.dim * 1.5))))\n\n        self.candidates_per_edge = int(candidates_per_edge)\n        self.pool_factor = max(2, int(pool_factor))\n        self.jump_prob = float(jump_prob)\n        self.sigma0 = sigma0\n        if c_cov is None:\n            # smaller for larger dim to stabilize covariance learning\n            self.c_cov = 0.25 / max(1.0, (self.dim + 3.0))\n        else:\n            self.c_cov = float(c_cov)\n        self.stagnation_restart = int(stagnation_restart)\n\n    def _levy_dir(self, size):\n        # stable heavy-tailed direction (normalized)\n        z = self.rng.standard_cauchy(size=size)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        n = np.linalg.norm(z)\n        if n == 0:\n            return self.rng.standard_normal(size=size)\n        return z / n\n\n    def __call__(self, func):\n        # bounds handling: prefer func.bounds if present, else use [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        evals = 0\n        archive_X = []  # list of evaluated x\n        archive_f = []  # corresponding function values\n\n        # initial random sampling\n        n_init = min(self.init_samples, budget)\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_X.append(x.copy())\n            archive_f.append(f)\n\n        # build elites\n        def get_elites():\n            if len(archive_f) == 0:\n                return np.zeros((0, dim)), np.array([])\n            idx = np.argsort(archive_f)[:self.elite_size]\n            X = np.array([archive_X[i] for i in idx])\n            fvals = np.array([archive_f[i] for i in idx])\n            return X, fvals\n\n        X_elite, f_elite = get_elites()\n        if X_elite.shape[0] > 0:\n            best_idx = np.argmin(f_elite)\n            x_best = X_elite[best_idx].copy()\n            f_best = float(f_elite[best_idx])\n        else:\n            x_best = rng.uniform(lb, ub)\n            f_best = float(func(x_best)); evals += 1\n            archive_X.append(x_best.copy()); archive_f.append(f_best)\n            X_elite, f_elite = get_elites()\n\n        # range scale\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma0 is None:\n            sigma = max(1e-8, 0.18 * range_scale)\n        else:\n            sigma = float(self.sigma0)\n\n        # global covariance (starts isotropic)\n        C = np.eye(dim) * ((range_scale / 6.0) ** 2 + 1e-9)\n\n        # edge evaluation counters to encourage densification of less-explored edges\n        edge_counts = {}  # key: (i,j) with i<j\n\n        # success tracking for step-size control\n        p_succ = 0.0\n        ema_alpha = 0.12\n        target_success = 0.2\n\n        since_improvement = 0\n\n        # main loop: until budget exhausted\n        while evals < budget:\n            # refresh elites\n            X_elite, f_elite = get_elites()\n            n_elite = X_elite.shape[0]\n            if n_elite == 0:\n                # sample random to populate\n                x = rng.uniform(lb, ub)\n                f = float(func(x)); evals += 1\n                archive_X.append(x.copy()); archive_f.append(f)\n                X_elite, f_elite = get_elites()\n                n_elite = X_elite.shape[0]\n\n            # build list of edges (pairs)\n            edges = list(combinations(range(n_elite), 2))\n            if len(edges) == 0:\n                # only one elite: do Gaussian exploration around it\n                edges = [(0,0)]\n\n            # candidate pool size (produce many candidates, then rank cheaply)\n            pool_target = max(8, int(self.pool_factor * len(edges) * self.candidates_per_edge))\n            candidates = []  # tuples (score_est, x_candidate, edge_key, meta)\n            # Precompute L for covariance sampling (regularize)\n            reg = 1e-10 * np.eye(dim)\n            try:\n                L = np.linalg.cholesky(C + reg)\n            except np.linalg.LinAlgError:\n                L = np.linalg.cholesky(C + 1e-6 * np.eye(dim))\n\n            # produce candidates from edges\n            for (i, j) in edges:\n                xi = X_elite[i]\n                xj = X_elite[j] if i != j else X_elite[i]\n                fi = float(f_elite[i])\n                fj = float(f_elite[j]) if i != j else fi\n\n                # ensure edge key ordering\n                edge_key = (min(i, j), max(i, j))\n\n                # basic direction and length\n                d = xj - xi\n                dnorm = np.linalg.norm(d)\n                if dnorm == 0:\n                    d_unit = rng.normal(size=dim)\n                    d_unit /= np.linalg.norm(d_unit) + 1e-12\n                else:\n                    d_unit = d / dnorm\n\n                # small set of lambda positions along edge (including extrapolations)\n                alphas = [0.5]\n                if n_elite > 1:\n                    alphas += [0.25, 0.75]\n                    # occasional extrapolation candidates\n                    alphas += [-0.3, 1.3]\n\n                # produce orthogonal perturbations and small covariance samples\n                for a in alphas[:self.candidates_per_edge]:\n                    x_edge = xi * (1.0 - a) + xj * a\n                    # orthogonal perturbation: random vector projected orthogonally to d_unit\n                    z = rng.normal(size=dim)\n                    if dnorm > 1e-12:\n                        z = z - np.dot(z, d_unit) * d_unit\n                    # normalize and scale by a fraction of edge length or sigma\n                    znorm = np.linalg.norm(z)\n                    if znorm == 0:\n                        z = rng.normal(size=dim)\n                        znorm = np.linalg.norm(z)\n                    z = z / (znorm + 1e-12)\n\n                    # sample magnitude: proportional to min(sigma, edge length)\n                    mag = sigma * (0.8 + 0.8 * rng.random())\n                    if dnorm > 0:\n                        mag = min(mag, 0.5 * dnorm + 0.5 * sigma)\n\n                    # candidate variants: midpoint-like, orth perturbed, covariance-sampled\n                    # 1) plain edge candidate\n                    x1 = np.clip(x_edge, lb, ub)\n                    # 2) orthogonally perturbed\n                    x2 = np.clip(x_edge + mag * z, lb, ub)\n                    # 3) small covariance exploration around edge\n                    zc = L.dot(rng.normal(size=dim))\n                    x3 = np.clip(x_edge + 0.6 * sigma * zc, lb, ub)\n\n                    # estimate f cheaply using linear interpolation between endpoints\n                    f_est_linear = fi * (1.0 - a) + fj * a\n\n                    # novelty bonus based on edge_counts (prefer edges with fewer evaluations)\n                    ec = edge_counts.get(edge_key, 0)\n                    novelty = 1.0 / (1.0 + ec)  # higher if ec small\n\n                    # scoring: lower is better. Encourage lower predicted f and novelty\n                    # we subtract a novelty bonus (so lower estimated -> more desirable)\n                    novelty_bonus = 0.08 * novelty * range_scale\n                    # create candidate entries\n                    candidates.append((f_est_linear - novelty_bonus, x1, edge_key, dict(alpha=a, variant='edge')))\n                    candidates.append((f_est_linear - novelty_bonus - 0.01 * range_scale, x2, edge_key, dict(alpha=a, variant='orth')))\n                    candidates.append((f_est_linear - novelty_bonus + 0.02 * range_scale, x3, edge_key, dict(alpha=a, variant='cov')))\n\n                    if len(candidates) >= pool_target:\n                        break\n                if len(candidates) >= pool_target:\n                    break\n\n            # add a few global random and covariance-guided proposals to avoid overfitting edges\n            n_global = max(2, int(0.06 * pool_target))\n            for _ in range(n_global):\n                if rng.random() < 0.5:\n                    xg = rng.uniform(lb, ub)\n                else:\n                    # local around best\n                    z = L.dot(rng.normal(size=dim))\n                    xg = np.clip(x_best + sigma * z, lb, ub)\n                # cheap surrogate estimate: distance-weighted interpolation to best\n                d_to_best = np.linalg.norm(xg - x_best)\n                f_est = f_best + 0.05 * d_to_best\n                candidates.append((f_est, xg, None, dict(variant='global')))\n\n            # rank candidate pool by score (lower better) and pick top K to actually evaluate\n            candidates.sort(key=lambda t: t[0])\n            # how many evaluations to spend this cycle\n            remaining = budget - evals\n            # prefer to evaluate in small batches to allow adaptive updates\n            batch_eval = min( max(2, int(0.06 * remaining)), remaining )\n            # but don't exceed candidate pool\n            batch_eval = min(batch_eval, len(candidates))\n            # ensure at least 1\n            batch_eval = max(1, batch_eval)\n\n            # Evaluate top batch_eval candidates (with some randomized selection among the top to keep variety)\n            top_k = candidates[:min(len(candidates), max(batch_eval, 6))]\n            # sample exactly batch_eval from top_k with bias to best (softmax on -score)\n            scores = np.array([c[0] for c in top_k], dtype=float)\n            # transform to positive weights\n            inv = np.max(scores) - scores + 1e-12\n            probs = inv / np.sum(inv)\n            chosen_indices = rng.choice(len(top_k), size=batch_eval, replace=False, p=probs)\n\n            improved_in_cycle = False\n            for idx in chosen_indices:\n                score_est, xcand, edge_key, meta = top_k[idx]\n                # with small probability do heavy-tailed jump from xcand to diversify\n                if rng.random() < self.jump_prob:\n                    dx = self._levy_dir(dim) * (2.5 * sigma)\n                    x_eval = np.clip(xcand + dx, lb, ub)\n                else:\n                    x_eval = xcand\n\n                f_eval = float(func(x_eval)); evals += 1\n                archive_X.append(x_eval.copy()); archive_f.append(f_eval)\n\n                # update edge counts if applicable (increase density)\n                if edge_key is not None:\n                    edge_counts[edge_key] = edge_counts.get(edge_key, 0) + 1\n\n                # update best if improved\n                if f_eval < f_best:\n                    improved_in_cycle = True\n                    f_best = f_eval\n                    x_best = x_eval.copy()\n                    since_improvement = 0\n                else:\n                    since_improvement += 1\n\n                # update covariance using displacement from (local center)\n                # choose center for update: if from edge, center=midpoint of edge endpoints, else best\n                if edge_key is not None and len(X_elite) > 1:\n                    i, j = edge_key\n                    center = 0.5 * (X_elite[i] + X_elite[j])\n                else:\n                    center = x_best\n\n                y = (x_eval - center).reshape(-1, 1)\n                C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                # small regularization\n                C += 1e-14 * np.eye(dim)\n\n                # update p_succ EMA (treat any improvement over center as success)\n                # success if f_eval better than center's predicted (approx)\n                center_f_pred = None\n                if edge_key is not None and len(X_elite) > 1:\n                    # approximate center cost by average of endpoints\n                    center_f_pred = 0.5 * (float(f_elite[edge_key[0]]) + float(f_elite[edge_key[1]]))\n                else:\n                    center_f_pred = f_best\n                succ = 1.0 if f_eval < center_f_pred else 0.0\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * succ\n\n                # adapt sigma multiplicatively (clip extremes)\n                adjust = np.exp(0.7 * (p_succ - target_success) / max(1e-6, np.sqrt(dim)))\n                sigma *= float(np.clip(adjust, 0.7, 1.6))\n                sigma = np.clip(sigma, 1e-9 * range_scale, 6.0 * range_scale)\n\n                # early exit if budget exhausted\n                if evals >= budget:\n                    break\n\n            # periodic mild covariance inflation on stagnation to explore more edges\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # encourage exploration: inflate covariance and broaden sigma\n                C = 0.6 * C + 0.4 * np.eye(dim) * ((range_scale / 3.0) ** 2)\n                sigma = min(3.0 * sigma, 8.0 * range_scale)\n                # also add a few random edge+dense samples\n                pushes = min(6, budget - evals)\n                for _ in range(pushes):\n                    # pick a random elite and sample around it with larger sigma\n                    if len(X_elite) > 0 and rng.random() < 0.7:\n                        center = X_elite[rng.integers(len(X_elite))]\n                    else:\n                        center = rng.uniform(lb, ub)\n                    z = rng.normal(size=dim)\n                    x_try = np.clip(center + 2.0 * sigma * z, lb, ub)\n                    f_try = float(func(x_try)); evals += 1\n                    archive_X.append(x_try.copy()); archive_f.append(f_try)\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                        since_improvement = 0\n                        break\n                    if evals >= budget:\n                        break\n                # reset stagnation counter growth modestly so repeated triggers are spaced\n                since_improvement = int(since_improvement * 0.6)\n\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 6, "feedback": "In the code, line 144, in __call__, the following error occurred:\nNameError: name 'combinations' is not defined\nOn line: edges = list(combinations(range(n_elite), 2))", "error": "In the code, line 144, in __call__, the following error occurred:\nNameError: name 'combinations' is not defined\nOn line: edges = list(combinations(range(n_elite), 2))", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2676.0, "Edges": 2675.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.999252615844544, "Degree Variance": 2.3258589331956987, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.080231596360628, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3208455778784314, "Depth Entropy": 2.020888300001386, "Assortativity": 8.817356815711164e-09, "Average Eccentricity": 17.47608370702541, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0003736920777279522, "Average Shortest Path": 9.972963413100876, "mean_complexity": 13.25, "total_complexity": 53.0, "mean_token_count": 598.5, "total_token_count": 2394.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "27093538-3d85-4ccf-a2c0-a71f1be3f438", "fitness": "-inf", "name": "HyperDenseEdgeMeshFlow", "description": "HyperDense Edge Mesh Flow (HEMF) \u2014 create an ultra-dense virtual mesh of edge/triangle candidates scored cheaply by inverse-distance surrogates, selectively evaluate the best, and concentrate sampling by diffusing reward across neighbouring edges to rapidly densify productive connections.", "code": "import numpy as np\n\nclass HyperDenseEdgeMeshFlow:\n    \"\"\"\n    HyperDense Edge Mesh Flow (HEMF)\n\n    Main idea:\n      - Maintain a compact population and an evolving dense edge graph focused on elites.\n      - Generate a large pool of virtual candidates by sampling along edges and triangles (a hyper-dense mesh).\n      - Score virtual candidates cheaply using inverse-distance weighted predictions from nearby population points.\n      - Evaluate only the top-ranked virtual candidates (plus a few random explorers) to stay within budget.\n      - When a real evaluation improves the population, diffuse a reward to nearby edges (flow) to increase local edge density.\n      - Adapt local step sizes (sigma) and momentum for replaced individuals and perform a final small pattern-refinement.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, pop_size=None, elite_frac=0.25,\n                 k_neighbors=8, proposals_per_cycle=40, virtual_mult=12,\n                 tri_prob=0.42, walk_prob=0.28, levy_prob=0.05, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.virtual_mult = int(virtual_mult)  # multiplier controlling virtual candidate pool size\n        self.tri_prob = float(tri_prob)\n        self.walk_prob = float(walk_prob)\n        self.levy_prob = float(levy_prob)\n\n        # sensible population sizing\n        if pop_size is None:\n            self.pop_size = max(12, min(120, int(np.clip(np.sqrt(self.budget) * 1.5, 12, 120))))\n            self.pop_size = max(self.pop_size, self.dim + 4)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation hyperparams\n        self.sigma_init_frac = 0.09\n        self.success_boost = 1.28\n        self.failure_shrink = 0.82\n        self.edge_gain = 1.5\n        self.edge_decay = 0.996\n        self.flow_diffuse = 0.18  # diffusion strength to neighboring edges on success\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_norm = np.linalg.norm(ub - lb)\n        sigma_scale = max(1e-12, self.sigma_init_frac * range_norm)\n\n        # ensure population <= budget (must be able to evaluate initial pop)\n        pop_size = min(self.pop_size, max(2, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population uniformly\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf, dtype=float)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # per-individual sigma and momentum\n        sigma = np.full(self.pop_size, sigma_scale, dtype=float)\n        momentum = np.zeros((self.pop_size, dim), dtype=float)\n\n        # edge weights and flow (for diffusion)\n        edge_weights = {}   # key: (i,j) sorted -> weight\n        edge_flow = {}      # same keys -> accumulated flow measure\n\n        def keypair(a, b):\n            a = int(a); b = int(b)\n            if a == b:\n                if a == 0:\n                    b = 1\n                else:\n                    a -= 1\n            return (min(a, b), max(a, b))\n\n        def get_edge_weight(a, b):\n            return edge_weights.get(keypair(a, b), 1.0)\n\n        def inc_edge(a, b, factor):\n            k = keypair(a, b)\n            edge_weights[k] = edge_weights.get(k, 1.0) * factor\n            edge_flow[k] = edge_flow.get(k, 0.0) + (factor - 1.0)\n\n        def diffuse_flow(a, b, reward):\n            # add small reward to adjacent edges of a and b\n            kcentral = keypair(a, b)\n            # for each existing edge connected to a or b\n            for (u, v) in list(edge_weights.keys()):\n                if u == a or v == a or u == b or v == b:\n                    # neighbor factor\n                    edge_weights[(u, v)] *= (1.0 + self.flow_diffuse * reward)\n                    edge_flow[(u, v)] = edge_flow.get((u, v), 0.0) + self.flow_diffuse * reward\n\n        # build dense graph emphasizing elites\n        def build_dense_graph():\n            # decay weights slightly\n            if edge_weights:\n                for k in list(edge_weights.keys()):\n                    edge_weights[k] *= self.edge_decay\n            X = pop\n            # pairwise squared distances\n            dif = X[:, None, :] - X[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count].tolist()\n\n            edges = set()\n            # near-complete core among elites\n            for i in range(len(elites)):\n                for j in range(i+1, len(elites)):\n                    a = elites[i]; b = elites[j]\n                    edges.add(keypair(a, b))\n                    edge_weights.setdefault(keypair(a,b), 1.0)\n\n            # for each elite connect many neighbors (k-nearest)\n            for i in elites:\n                neighs = np.argsort(dist2[i])\n                added = 0\n                for j in neighs:\n                    if j == i: continue\n                    edges.add(keypair(i, int(j)))\n                    added += 1\n                    if added >= max(self.k_neighbors, 4):\n                        break\n\n            # link top-ranked non-elites more frequently to increase density\n            topk = max(elite_count, min(self.pop_size, elite_count*3))\n            top_indices = np.argsort(pop_f)[:topk]\n            for i in top_indices:\n                # connect to a few random nodes\n                for _ in range(3):\n                    j = int(rng.integers(0, self.pop_size))\n                    if j != i:\n                        edges.add(keypair(i, j))\n\n            # triangulation edges: pick many triangles among elites + near elites\n            if len(elites) >= 3:\n                for _ in range(max(8, elite_count * 3)):\n                    tri = rng.choice(elites, size=3, replace=False)\n                    a, b, c = int(tri[0]), int(tri[1]), int(tri[2])\n                    edges.add(keypair(a, b)); edges.add(keypair(b, c)); edges.add(keypair(a, c))\n\n            # random edges to ensure exploration\n            for _ in range(max(6, self.pop_size)):\n                a = int(rng.integers(0, self.pop_size)); b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges.add(keypair(a, b))\n\n            # seed weights for new edges\n            for k in edges:\n                edge_weights.setdefault(k, 1.0)\n            return list(edges)\n\n        # helper: cheaply predict f for candidate x by inverse-distance weighted average of nearest pop points\n        def cheap_predict(x, k=6):\n            # compute squared distances\n            d2 = np.sum((pop - x)**2, axis=1)\n            idxs = np.argsort(d2)[:min(self.pop_size, k)]\n            d = np.sqrt(np.maximum(1e-12, d2[idxs]))\n            # inverse distance weights\n            inv = 1.0 / (d + 1e-12)\n            w = inv / (np.sum(inv) + 1e-12)\n            pred = np.dot(w, pop_f[idxs])\n            # penalty for being far from population: encourage near-hull candidates\n            avgd = np.mean(d)\n            pred += 0.02 * (avgd / (range_norm + 1e-12))\n            return float(pred), idxs\n\n        # initial best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        cycle = 0\n        stagn_count = 0\n        while evals < self.budget:\n            remaining = self.budget - evals\n            cycle += 1\n            improved = False\n\n            # build graph and adjacency\n            edges = build_dense_graph()\n            adjacency = [[] for _ in range(self.pop_size)]\n            for (a, b) in edges:\n                adjacency[a].append(b)\n                adjacency[b].append(a)\n\n            # compute virtual pool size (many virtual candidates, only score cheaply)\n            virtual_pool = int(min(3000, max(200, self.virtual_mult * self.pop_size)))\n            virtual_candidates = []  # list of tuples (pred_f, x_candidate, parent_info, nearest_idxs)\n\n            # generate many virtual candidates cheaply (no expensive evaluations)\n            for _v in range(virtual_pool):\n                r = rng.random()\n                parent_info = None\n                if r < self.tri_prob and len(edges) >= 2 and self.pop_size >= 3:\n                    # triangle barycentric sample among elites or near elites\n                    if rng.random() < 0.8:\n                        elite_count = max(3, int(np.ceil(self.elite_frac * self.pop_size)))\n                        candidates = np.argsort(pop_f)[:elite_count]\n                        tri = rng.choice(candidates, size=3, replace=False)\n                    else:\n                        tri = rng.choice(self.pop_size, size=3, replace=False)\n                    a, b, c = int(tri[0]), int(tri[1]), int(tri[2])\n                    Xa, Xb, Xc = pop[a], pop[b], pop[c]\n                    # sample barycentric coords (corner-favoring)\n                    g = rng.gamma(0.6, 1.0, size=3)\n                    w = g / np.sum(g)\n                    x = w[0]*Xa + w[1]*Xb + w[2]*Xc\n                    # small extrapolation sometimes\n                    if rng.random() < 0.12:\n                        center = (Xa + Xb + Xc) / 3.0\n                        x = center + rng.uniform(-0.4, 1.2) * (center - x)\n                    # small jitter\n                    x = x + 0.4 * sigma_scale * rng.standard_normal(dim)\n                    parent_info = ('tri', (a,b,c))\n\n                elif r < self.tri_prob + self.walk_prob and any(len(n)>0 for n in adjacency):\n                    # short weighted graph-walk producing a virtual candidate\n                    if rng.random() < 0.75:\n                        start_candidates = np.argsort(pop_f)[:max(2, int(self.elite_frac * self.pop_size))]\n                        cur = int(rng.choice(start_candidates))\n                    else:\n                        cur = int(rng.integers(0, self.pop_size))\n                    steps = int(rng.integers(1, 4))\n                    cur_pos = pop[cur].copy()\n                    cur_idx = cur\n                    for s in range(steps):\n                        neigh = adjacency[cur_idx]\n                        if not neigh: break\n                        weights = np.array([get_edge_weight(cur_idx, nb) for nb in neigh], dtype=float)\n                        qualities = np.array([np.exp(-(pop_f[nb] - f_best) / (1e-8 + np.std(pop_f) + 1.0)) for nb in neigh])\n                        probs = weights * qualities\n                        if np.sum(probs) <= 0:\n                            probs = np.ones_like(probs) / len(probs)\n                        else:\n                            probs = probs / np.sum(probs)\n                        nb_idx = int(rng.choice(neigh, p=probs))\n                        direction = pop[nb_idx] - pop[cur_idx]\n                        dir_norm = np.linalg.norm(direction)\n                        if dir_norm < 1e-12:\n                            stepvec = 0.5 * sigma[cur_idx] * rng.standard_normal(dim)\n                        else:\n                            stepvec = 0.6 * (sigma[cur_idx] + sigma[nb_idx]) * (direction / dir_norm)\n                        cur_pos = cur_pos + stepvec + 0.4 * momentum[cur_idx]\n                        cur_idx = nb_idx\n                    x = cur_pos + 0.02 * range_norm * rng.standard_normal(dim)\n                    parent_info = ('walk', cur_idx)\n\n                else:\n                    # edge midpoint/extrapolation sampling\n                    # select an edge biased by edge weight\n                    if edges and rng.random() < 0.95:\n                        keys = edges\n                        weights = np.array([get_edge_weight(a,b) for (a,b) in keys], dtype=float)\n                        if np.sum(weights) <= 0:\n                            eidx = int(rng.integers(0, len(keys)))\n                        else:\n                            probs = weights / np.sum(weights)\n                            eidx = int(rng.choice(len(keys), p=probs))\n                        a,b = keys[eidx]\n                    else:\n                        a = int(rng.integers(0, self.pop_size)); b = int(rng.integers(0, self.pop_size))\n                        if a == b:\n                            b = (a+1)%self.pop_size\n                    Xa, Xb = pop[a], pop[b]\n                    style = rng.random()\n                    if style < 0.5:\n                        t = rng.uniform(0.2, 0.8)\n                    elif style < 0.92:\n                        t = rng.uniform(-0.1, 1.1)\n                    else:\n                        t = rng.uniform(-0.9, 1.9)\n                    x = (1-t)*Xa + t*Xb\n                    # add correlated jitter along edge\n                    edge_dir = Xb - Xa\n                    ed_norm = np.linalg.norm(edge_dir)\n                    if ed_norm > 1e-12:\n                        edge_unit = edge_dir / ed_norm\n                        x += 0.12 * (sigma[a] + sigma[b]) * rng.standard_normal() * edge_unit\n                    x += 0.6 * sigma[a] * rng.standard_normal(dim)\n                    parent_info = ('edge', (a,b))\n\n                # occasionally heavy-tailed virtual displacement to create distant virtuals\n                if rng.random() < 0.02:\n                    u = rng.random(dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    x += 0.12 * range_norm * cauch\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                pred, nearest = cheap_predict(x, k=min(8, self.pop_size))\n                virtual_candidates.append((pred, x, parent_info, nearest))\n\n            # sort virtual candidates by predicted f (ascending better)\n            virtual_candidates.sort(key=lambda t: t[0])\n\n            # decide how many to actually evaluate: keep below remaining budget and around proposals_per_cycle\n            to_eval = min(remaining, max(1, int(self.proposals_per_cycle)))\n            # evaluate top fraction plus a small number of randoms for exploration\n            top_fraction = int(max(1,  int(0.88 * to_eval)))\n            random_fraction = to_eval - top_fraction\n\n            eval_list = virtual_candidates[:top_fraction]\n            if random_fraction > 0:\n                # sample random indexes from the remainder (avoid duplicates)\n                rem = virtual_candidates[top_fraction:]\n                if rem:\n                    rnd_idxs = rng.choice(len(rem), size=min(random_fraction, len(rem)), replace=False)\n                    for idx in rnd_idxs:\n                        eval_list.append(rem[idx])\n\n            # Evaluate chosen candidates (real expensive calls) and integrate\n            for pred_f, x_cand, parent_info, nearest_idxs in eval_list:\n                if evals >= self.budget: break\n\n                f_cand = float(func(x_cand))\n                evals += 1\n\n                # attempt to insert into population: find worst among nearest or global worst\n                d2 = np.sum((pop - x_cand)**2, axis=1)\n                nearest = np.argsort(d2)[:min(4, self.pop_size)]\n                replaced = False\n                # try to replace a worse neighbor\n                for idx in nearest:\n                    if f_cand < pop_f[idx]:\n                        old_pos = pop[idx].copy()\n                        pop[idx] = x_cand\n                        old_f = pop_f[idx]\n                        pop_f[idx] = f_cand\n                        # momentum update uses movement\n                        move = x_cand - old_pos\n                        momentum[idx] = 0.45 * momentum[idx] + 0.5 * move\n                        sigma[idx] = max(1e-12 * range_norm, sigma[idx] * self.success_boost)\n                        replaced = True\n                        # reward edges connected to this index\n                        for nb in nearest:\n                            if nb != idx:\n                                inc_edge(idx, nb, self.edge_gain)\n                                diffuse_flow(idx, nb, min(1.0, (old_f - f_cand) / (1.0 + abs(old_f))))\n                        break\n                if not replaced:\n                    # replace global worst with some probability if candidate better\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_cand < pop_f[idx_worst]:\n                        old_pos = pop[idx_worst].copy()\n                        old_f = pop_f[idx_worst]\n                        pop[idx_worst] = x_cand\n                        pop_f[idx_worst] = f_cand\n                        move = x_cand - old_pos\n                        momentum[idx_worst] = 0.35 * momentum[idx_worst] + 0.5 * move\n                        sigma[idx_worst] = max(1e-12 * range_norm, sigma[idx_worst] * self.success_boost)\n                        replaced = True\n                        for nb in nearest:\n                            if nb != idx_worst:\n                                inc_edge(idx_worst, nb, self.edge_gain)\n                                diffuse_flow(idx_worst, nb, min(1.0, (old_f - f_cand) / (1.0 + abs(old_f))))\n\n                # edge attribution from parent_info when candidate is created from an edge/triangle\n                if replaced and parent_info is not None:\n                    if parent_info[0] == 'edge':\n                        a, b = parent_info[1]\n                        inc_edge(a, b, 1.0 + 0.25)\n                        diffuse_flow(a, b, 0.2)\n                    elif parent_info[0] == 'tri':\n                        a,b,c = parent_info[1]\n                        inc_edge(a,b, 1.12); inc_edge(b,c,1.12); inc_edge(a,c,1.12)\n                        diffuse_flow(a, b, 0.15)\n                        diffuse_flow(b, c, 0.15)\n                    elif parent_info[0] == 'walk':\n                        idx_src = parent_info[1]\n                        # reward edges incident to source\n                        for nb in adjacency[idx_src]:\n                            inc_edge(idx_src, nb, 1.08)\n\n                # update global best\n                if f_cand < f_best:\n                    f_best = f_cand\n                    x_best = x_cand.copy()\n                    improved = True\n\n                # if not replaced, penalize nearby sigmas slightly\n                if not replaced:\n                    for idx in nearest:\n                        sigma[idx] *= self.failure_shrink\n\n                if evals >= self.budget:\n                    break\n\n            # slight decay to sigma ceiling and momentum damping each cycle\n            sigma = np.clip(sigma, 1e-12 * range_norm, 2.5 * range_norm)\n            momentum *= 0.92\n\n            # occasional stochastic injection: evaluate a pure random sample\n            if rng.random() < 0.06 and evals < self.budget:\n                x_rnd = rng.uniform(lb, ub)\n                f_rnd = float(func(x_rnd))\n                evals += 1\n                idx_w = int(np.argmax(pop_f))\n                if f_rnd < pop_f[idx_w]:\n                    pop[idx_w] = x_rnd\n                    pop_f[idx_w] = f_rnd\n                    sigma[idx_w] = sigma_scale\n                    momentum[idx_w] = 0.0\n                    if f_rnd < f_best:\n                        f_best = f_rnd; x_best = x_rnd.copy()\n                    improved = True\n\n            # stagnation handling: if no improvement, densify around best: reseed many around best and boost edges connected to best\n            if not improved:\n                stagn_count += 1\n                if stagn_count % 3 == 0:\n                    # reseed a small cluster around best using Gaussian cloud and small extrapolations\n                    n_reset = max(1, self.pop_size // 8)\n                    for _ in range(n_reset):\n                        if evals >= self.budget: break\n                        if rng.random() < 0.85:\n                            cand = x_best + 0.04 * (ub - lb) * rng.standard_normal(dim)\n                        else:\n                            cand = rng.uniform(lb, ub)\n                        cand = np.clip(cand, lb, ub)\n                        f_c = float(func(cand))\n                        evals += 1\n                        idx_replace = int(rng.integers(0, self.pop_size))\n                        if f_c < pop_f[idx_replace]:\n                            oldf = pop_f[idx_replace]\n                            pop[idx_replace] = cand\n                            pop_f[idx_replace] = f_c\n                            sigma[idx_replace] = sigma_scale\n                            momentum[idx_replace] = 0.0\n                            # reward edges touching this index\n                            for nb in range(self.pop_size):\n                                if nb != idx_replace:\n                                    inc_edge(idx_replace, nb, 1.06)\n                            if f_c < f_best:\n                                f_best = f_c; x_best = cand.copy()\n                                improved = True\n            else:\n                stagn_count = 0\n\n            # if budget becomes low, break to local refinement\n            if self.budget - evals < max(8, 2*dim):\n                break\n\n        # final local refinement: coordinate pattern search around best\n        step = 0.12 * range_norm\n        step_min = 1e-6 * range_norm\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # plus\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                # minus\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n\n        # store and return\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["2c409501-52e9-4505-8ad4-bcf4ade87ef5"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4429.0, "Edges": 4428.0, "Max Degree": 35.0, "Min Degree": 1.0, "Mean Degree": 1.9995484307970197, "Degree Variance": 2.088055790666424, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.965122072745391, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.333868104812352, "Depth Entropy": 2.2695351697030466, "Assortativity": 0.0, "Average Eccentricity": 20.997290584782117, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.00022578460149017836, "Average Shortest Path": 11.662024926864758, "mean_complexity": 12.555555555555555, "total_complexity": 113.0, "mean_token_count": 437.6666666666667, "total_token_count": 3939.0, "mean_parameter_count": 3.111111111111111, "total_parameter_count": 28.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "e9f48252-fc24-44aa-95f6-c2b91f72e4be", "fitness": 0.24303230853008115, "name": "ACLS_EdgeDensify", "description": "ACLS-EdgeDensify \u2014 Adaptive Covariance-Step search with elite edge densification: maintain an adaptive covariance and elite archive, build a dense pool of edge-derived candidates (interpolations, extrapolations and orthogonal perturbations), cheaply score them with a proxy, evaluate the most promising few, and update covariance from successful edge-net moves and individual successes to rapidly densify productive connections and escape basins.", "code": "import numpy as np\n\nclass ACLS_EdgeDensify:\n    \"\"\"\n    ACLS_EdgeDensify: Adaptive Covariance-Step with L\u00e9vy Escapes and Edge Densification.\n\n    - budget: max function evaluations\n    - dim: dimensionality\n    Optional tuning arguments may be provided.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_fraction=0.12,\n                 jump_prob=0.06,\n                 target_success=0.2,\n                 cov_update_rate=None,\n                 sigma_initial=None,\n                 stagnation_restart=500,\n                 elite_size=None,\n                 edge_pool_size=200,\n                 edge_eval_batch=8,\n                 edge_cycle=25):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # meta params\n        self.init_fraction = float(init_fraction)\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n        if cov_update_rate is None:\n            self.c_cov = 0.2 / max(1.0, (self.dim + 2.0))\n        else:\n            self.c_cov = float(cov_update_rate)\n        self.sigma_initial = sigma_initial\n        self.stagnation_restart = int(stagnation_restart)\n\n        # edge densification params\n        self.elite_size = int(elite_size) if elite_size is not None else max(6, min(14, self.budget // 200 + self.dim // 2))\n        self.edge_pool_size = int(edge_pool_size)\n        self.edge_eval_batch = int(edge_eval_batch)\n        self.edge_cycle = int(edge_cycle)  # perform densify every this many iterations\n\n    def _levy_vector(self, size):\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        norm = np.linalg.norm(z)\n        if norm == 0:\n            return self.rng.standard_normal(size=size)\n        return z / norm\n\n    def _get_bounds(self, func):\n        # prefer func.bounds if available, otherwise fallback to [-5,5] per spec\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_clip(self, x, lb, ub):\n        below = x < lb\n        above = x > ub\n        if np.any(below):\n            x = np.where(below, lb + (lb - x), x)\n        if np.any(above):\n            x = np.where(above, ub - (x - ub), x)\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = rng.uniform(lb, ub)\n\n        # initial sampling\n        n_init = max(1, int(min(max(10, 2 * dim), max(1, int(budget * self.init_fraction)))))\n        n_init = min(n_init, budget)\n        samples_x = []\n        samples_f = []\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            samples_x.append(x.copy())\n            samples_f.append(float(f))\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n\n        # ensure we have at least one sample\n        if len(samples_x) == 0:\n            samples_x.append(x_opt.copy())\n            samples_f.append(float(f_opt))\n\n        # Initialize covariance and sigma\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma_initial is None:\n            sigma = max(1e-8, 0.25 * range_scale)\n        else:\n            sigma = float(self.sigma_initial)\n        C = np.eye(dim) * (((range_scale / 4.0) ** 2) + 1e-9)\n\n        # Build elite archive from initial samples\n        X_archive = np.array(samples_x[:], dtype=float)\n        F_archive = np.array(samples_f[:], dtype=float)\n        # optionally supplement archive with a few random points\n        while X_archive.shape[0] < self.elite_size and evals < budget:\n            x = rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X_archive = np.vstack([X_archive, x.reshape(1, -1)])\n            F_archive = np.hstack([F_archive, float(f)])\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n\n        # Keep only top elites\n        def trim_elites(X, F):\n            idx = np.argsort(F)\n            k = min(self.elite_size, X.shape[0])\n            return X[idx[:k], :], F[idx[:k]]\n        X_archive, F_archive = trim_elites(X_archive, F_archive)\n\n        # success tracking\n        p_succ = 0.0\n        ema_alpha = 0.15\n        since_improvement = 0\n\n        iter_count = 0\n\n        # Main loop\n        while evals < budget:\n            remaining = budget - evals\n            # normal sampling batch size\n            batch = min(max(1, int(8 + dim // 2)), remaining)\n            # occasionally do an edge densification phase\n            do_edge = (iter_count % self.edge_cycle == 0) and (X_archive.shape[0] >= 2)\n\n            if do_edge and remaining > 0:\n                # Generate a pool of candidates from elite edges\n                pool = []\n                pool_sources = []  # store (i,j,alpha,perturb_scale)\n                m = X_archive.shape[0]\n                # consider up to all pairs but cap produced candidates\n                pairs = []\n                for i in range(m):\n                    for j in range(i + 1, m):\n                        pairs.append((i, j))\n                # Shuffle and maybe prioritize pairs involving best\n                best_idx = int(np.argmin(F_archive))\n                rng.shuffle(pairs)\n                # bias pairs that include best to front\n                pairs.sort(key=lambda p: 0 if (p[0] == best_idx or p[1] == best_idx) else 1)\n                # create candidates\n                max_pairs = max(10, min(len(pairs), self.edge_pool_size // 6))\n                pair_count = 0\n                for (i, j) in pairs:\n                    if pair_count >= max_pairs:\n                        break\n                    xi = X_archive[i]\n                    xj = X_archive[j]\n                    dij = xj - xi\n                    dist = np.linalg.norm(dij)\n                    if dist == 0:\n                        continue\n                    # produce several alphas (interpolation + extrapolation)\n                    alphas = [0.25, 0.5, 0.75, -0.25, 1.25]\n                    for a in alphas:\n                        x_c = xi + a * dij\n                        # orthogonal perturbations: project random vector orthogonal to dij\n                        z = rng.normal(size=dim)\n                        # remove component along dij\n                        proj = np.dot(z, dij) / (dist ** 2) * dij\n                        orth = z - proj\n                        if np.linalg.norm(orth) == 0:\n                            orth = rng.normal(size=dim)\n                        orth = orth / (np.linalg.norm(orth) + 1e-12)\n                        # create a few perturb scales\n                        for eps_scale in [0.0, 0.08, 0.18]:\n                            x_p = x_c + eps_scale * dist * orth\n                            x_p = self._reflect_clip(x_p, lb, ub)\n                            pool.append(x_p)\n                            pool_sources.append((i, j, a, eps_scale))\n                            if len(pool) >= self.edge_pool_size:\n                                break\n                        if len(pool) >= self.edge_pool_size:\n                            break\n                    pair_count += 1\n\n                # add some random candidates around elites to densify local neighborhoods\n                for ii in range(min(m, max(3, self.edge_pool_size // 20))):\n                    x_e = X_archive[ii]\n                    for _ in range(3):\n                        x_p = x_e + sigma * 0.6 * rng.normal(size=dim)\n                        pool.append(self._reflect_clip(x_p, lb, ub))\n                        pool_sources.append(('elite', ii, None, None))\n                        if len(pool) >= self.edge_pool_size:\n                            break\n                    if len(pool) >= self.edge_pool_size:\n                        break\n\n                # Remove duplicates (simple)\n                if len(pool) == 0:\n                    # fallback to simple sampling\n                    pool = [rng.uniform(lb, ub) for _ in range(min(self.edge_pool_size, remaining))]\n                    pool_sources = [('rand', None, None, None)] * len(pool)\n\n                P = np.array(pool, dtype=float)\n                # Proxy scoring for pool (cheap): combine endpoint means with distance-to-best penalty\n                # score = mean(f_i,f_j) + alpha * (dist_to_best / range_scale), lower is better\n                scores = np.full(P.shape[0], np.inf)\n                for idx_p, x_p in enumerate(P):\n                    src = pool_sources[idx_p]\n                    # distance to current best\n                    d_best = np.linalg.norm(x_p - x_opt) / (range_scale + 1e-12)\n                    # if source is pair, use mean of endpoints, else use elite's f or median archive f\n                    if isinstance(src[0], int):\n                        i, j = src[0], src[1]\n                        f_mean = 0.5 * (F_archive[i] + F_archive[j])\n                        # bias weight: if pair contains best, give smaller penalty\n                        w = 1.0\n                        if i == best_idx or j == best_idx:\n                            w *= 0.7\n                        # also prefer pairs with similar f (likely along basin)\n                        f_diff = abs(F_archive[i] - F_archive[j])\n                        w *= 1.0 + (f_diff / (abs(np.max(F_archive) - np.min(F_archive)) + 1e-12)) * 0.5\n                        score = f_mean + 0.22 * d_best * w\n                    elif src[0] == 'elite':\n                        _, ii, _, _ = src\n                        score = F_archive[ii] + 0.18 * d_best\n                    else:\n                        # random candidate: penalize a bit by distance to best\n                        score = np.median(F_archive) + 0.4 * d_best\n                    scores[idx_p] = score\n\n                # pick top-k to evaluate based on budget\n                order = np.argsort(scores)\n                k_eval = min(self.edge_eval_batch, remaining)\n                # we may want to evaluate more than edge_eval_batch if many remaining and pool promising\n                # but be conservative to preserve budget: evaluate up to edge_eval_batch or 10% of remaining\n                k_eval = min(k_eval, max(1, int(0.1 * remaining)))\n                k_eval = max(1, k_eval)\n                chosen_idx = order[:k_eval]\n\n                # Evaluate chosen candidates\n                for ci in chosen_idx:\n                    x_try = P[ci]\n                    f_try = func(x_try)\n                    evals += 1\n                    # update archives and best\n                    if f_try < f_opt:\n                        f_opt = float(f_try)\n                        x_opt = x_try.copy()\n                        since_improvement = 0\n                        # update covariance from this successful move\n                        y = (x_try - x_opt).reshape(-1, 1)  # note x_opt updated; this yields small y => safe\n                        # fallback: use displacement from one of endpoints if known\n                        # use outer product update with a small learning rate\n                        C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                    else:\n                        since_improvement += 1\n\n                    # Always incorporate this evaluated point into samples and elites\n                    X_archive = np.vstack([X_archive, x_try.reshape(1, -1)])\n                    F_archive = np.hstack([F_archive, float(f_try)])\n                    X_archive, F_archive = trim_elites(X_archive, F_archive)\n\n                    # Update sigma and p_succ based on improvement relative to the best of its endpoints if possible\n                    improved = (f_try < np.min(F_archive))  # very conservative\n                    p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if f_try < f_opt else 0.0)\n\n                    # update sigma modestly\n                    adjust = np.exp(0.6 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                    sigma *= float(np.clip(adjust, 0.7, 1.5))\n                    sigma = np.clip(sigma, 1e-10 * range_scale + 1e-12, 8.0 * range_scale + 1e-12)\n\n                    if evals >= budget:\n                        break\n\n                # small covariance boost from edges: use a few top pairs to enrich covariance (rank-k)\n                # choose top few pairs by edge promise (low mean f)\n                if X_archive.shape[0] >= 2:\n                    # compute pairwise mean f and add outer products of differences with small weight\n                    m = X_archive.shape[0]\n                    pair_scores = []\n                    for i in range(m):\n                        for j in range(i + 1, m):\n                            pair_scores.append((0.5 * (F_archive[i] + F_archive[j]), i, j))\n                    pair_scores.sort(key=lambda t: t[0])\n                    n_pairs_add = min(4, len(pair_scores))\n                    for pidx in range(n_pairs_add):\n                        _, i, j = pair_scores[pidx]\n                        d = (X_archive[i] - X_archive[j]).reshape(-1, 1)\n                        C = (1.0 - 0.02 * self.c_cov) * C + (0.02 * self.c_cov) * (d @ d.T)\n\n            else:\n                # Standard ACLS-style sampling\n                # Precompute L\n                reg = 1e-9 * np.eye(dim)\n                try:\n                    L = np.linalg.cholesky(C + reg)\n                except np.linalg.LinAlgError:\n                    L = np.linalg.cholesky(C + (1e-6 * np.eye(dim)))\n\n                for _ in range(batch):\n                    if evals >= budget:\n                        break\n                    # Choose between levy jump and normal step\n                    if rng.random() < self.jump_prob:\n                        z = self._levy_vector(dim)\n                        dx = L.dot(z) * 3.0\n                    else:\n                        z = rng.normal(size=dim)\n                        dx = L.dot(z)\n\n                    x_trial = x_opt + sigma * dx\n                    x_trial = self._reflect_clip(x_trial, lb, ub)\n                    f_trial = func(x_trial)\n                    evals += 1\n\n                    improved = False\n                    if f_trial < f_opt:\n                        improved = True\n                        # covariance update using displacement relative to old best\n                        y = (x_trial - x_opt).reshape(-1, 1)\n                        C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                        C += 1e-12 * np.eye(dim)\n                        f_opt = float(f_trial)\n                        x_opt = x_trial.copy()\n                        since_improvement = 0\n                    else:\n                        since_improvement += 1\n\n                    # update archives\n                    X_archive = np.vstack([X_archive, x_trial.reshape(1, -1)])\n                    F_archive = np.hstack([F_archive, float(f_trial)])\n                    X_archive, F_archive = trim_elites(X_archive, F_archive)\n\n                    # update success EMA and sigma\n                    p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n                    adjust = np.exp(0.6 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                    sigma *= float(np.clip(adjust, 0.6, 1.6))\n                    sigma = np.clip(sigma, 1e-10 * range_scale + 1e-12, 8.0 * range_scale + 1e-12)\n\n                # after batch, small covariance stabilization\n                C = 0.999 * C + 1e-12 * np.eye(dim)\n\n            # stagnation handling\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # try several targeted restarts and enlarge exploration temporarily\n                pushes = min(8, budget - evals)\n                for _ in range(pushes):\n                    if rng.random() < 0.6:\n                        center = x_opt\n                    else:\n                        # sample an elite center\n                        cidx = rng.integers(0, X_archive.shape[0])\n                        center = X_archive[cidx]\n                    x_try = center + 2.5 * sigma * rng.normal(size=dim)\n                    x_try = self._reflect_clip(x_try, lb, ub)\n                    f_try = func(x_try)\n                    evals += 1\n                    if f_try < f_opt:\n                        f_opt = float(f_try)\n                        x_opt = x_try.copy()\n                        sigma = max(sigma * 0.8, 1e-10 * range_scale)\n                        C = 0.5 * C + 0.5 * np.eye(dim) * ((range_scale / 6.0) ** 2)\n                        since_improvement = 0\n                        break\n                    if evals >= budget:\n                        break\n                # if still no improvement, temporarily increase jump probability & sigma\n                if since_improvement >= self.stagnation_restart:\n                    self.jump_prob = min(0.5, self.jump_prob * 1.6)\n                    sigma = min(sigma * 2.5, 12.0 * range_scale)\n\n            iter_count += 1\n\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 6, "feedback": "The algorithm ACLS_EdgeDensify scored 0.243 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9792383159417003}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9788350989906555}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9796623173637576}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.15789183163282594}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.10627390376269685}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.08590433494714311}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.021544605614985612}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.04417953762206017}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.07161505039571292}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0038371322387782403}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.016171853562943816}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.01637387448668437}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.06117096913686404}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.08357928940012804}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.03920651285428045}], "aucs": [0.9792383159417003, 0.9788350989906555, 0.9796623173637576, 0.15789183163282594, 0.10627390376269685, 0.08590433494714311, 0.021544605614985612, 0.04417953762206017, 0.07161505039571292, 0.0038371322387782403, 0.016171853562943816, 0.01637387448668437, 0.06117096913686404, 0.08357928940012804, 0.03920651285428045]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3263.0, "Edges": 3262.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9993870671161509, "Degree Variance": 2.2224942611505463, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.713240186294078, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3315283599449064, "Depth Entropy": 2.1993459238665176, "Assortativity": 1.3074200872099083e-08, "Average Eccentricity": 18.852283174992337, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00030646644192460924, "Average Shortest Path": 10.883309191193534, "mean_complexity": 11.666666666666666, "total_complexity": 70.0, "mean_token_count": 484.0, "total_token_count": 2904.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 26.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "d3d1c6cb-a75c-47a7-b2a6-5c2a5e890593", "fitness": "-inf", "name": "EdgeEnrichedAdaptiveGraphSearch", "description": "Edge-Dense Surrogate-Guided Graph Augmentation \u2014 aggressively densify elite edges and triangles by generating large candidate pools along edges/triangles, cheaply score them with linear/quadratic surrogates and novelty bonuses, and only evaluate the most promising candidates to increase edge density while conserving budget.", "code": "import numpy as np\n\nclass EdgeEnrichedAdaptiveGraphSearch:\n    \"\"\"\n    Edge-Enriched Adaptive Graph Search (refined)\n\n    Improvements over prior EEAGS:\n      - Aggressive edge/triangle densification: generate large candidate pools along edges,\n        extrapolations and triangle barycentric combinations to increase \"edge density\".\n      - Surrogate screening: cheaply score candidate pool using endpoint fitness and simple\n        geometric penalties/novelty bonus; evaluate only top candidates to save budget.\n      - Controlled densification rounds: periodically produce larger pools and evaluate more\n        promising candidates when budget allows.\n      - Adaptive edge/triangle weighting and temporary local PCA-based orthogonal exploration.\n      - Budget-safe: all func() calls are counted and will not exceed the specified budget.\n    Works on functions defined on [-5,5]^dim.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 pop_size=40, knn=8, init_frac=0.12,\n                 cand_pool=48, eval_per_round=2, densify_every=16):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # population/graph params\n        self.pop_size = max(6, int(pop_size))\n        self.knn = max(1, int(knn))\n        self.init_frac = float(init_frac)\n\n        # candidate pool & evaluation control\n        self.cand_pool = max(8, int(cand_pool))\n        self.eval_per_round = max(1, int(eval_per_round))\n        self.densify_every = max(4, int(densify_every))\n\n        # step-size / adaptation\n        self.sigma0 = 0.45 * (self.ub - self.lb)\n        self.sigma = float(self.sigma0)\n        self.sigma_min = 1e-6\n        self.sigma_max = 5.0\n        self.success_shrink = 0.92\n        self.failure_expand = 1.035\n\n        # graph refresh frequency\n        self.edge_refresh = 10\n\n        # other\n        self.max_archive = max(400, 30 * self.dim)\n\n    def __call__(self, func):\n        budget = max(1, int(self.budget))\n        dim = self.dim\n        lb = np.full(dim, self.lb)\n        ub = np.full(dim, self.ub)\n        width = ub - lb\n        mean_width = float(np.mean(width))\n\n        evals = 0\n\n        # initialize with uniform samples\n        n_init = int(min(budget, max(6, int(self.init_frac * budget), 4 * dim)))\n        X_archive = []\n        f_archive = []\n\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            X_archive.append(np.array(x, copy=True))\n            f_archive.append(float(f))\n            evals += 1\n            if evals >= budget:\n                break\n\n        if len(X_archive) == 0:\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_archive.append(x0.copy())\n            f_archive.append(float(f0))\n            evals = 1\n\n        X_archive = np.array(X_archive)\n        f_archive = np.array(f_archive)\n\n        # build initial population: best ones\n        def build_population(Xa, fa):\n            order = np.argsort(fa)\n            k = min(self.pop_size, len(order))\n            inds = order[:k]\n            return Xa[inds].copy(), fa[inds].copy()\n\n        pop_X, pop_f = build_population(X_archive, f_archive)\n        best_idx = int(np.argmin(pop_f))\n        x_best = pop_X[best_idx].copy()\n        f_best = float(pop_f[best_idx])\n\n        # graph structures\n        edges = []         # list of (i,j)\n        edge_weights = np.array([])\n        triangles = []     # list of (i,j,k)\n        tri_weights = np.array([])\n\n        def refresh_graph():\n            nonlocal edges, edge_weights, triangles, tri_weights, pop_X, pop_f\n            n = pop_X.shape[0]\n            edges = []\n            triangles = []\n            if n < 2:\n                edge_weights = np.array([])\n                tri_weights = np.array([])\n                return\n            D = np.linalg.norm(pop_X[:, None, :] - pop_X[None, :, :], axis=2)\n            kn = min(self.knn, n - 1)\n            pairs = set()\n            neighs = []\n            for i in range(n):\n                neigh = np.argsort(D[i])[1:1 + kn]\n                neighs.append(set(neigh.tolist()))\n                for j in neigh:\n                    a, b = (i, j) if i < j else (j, i)\n                    pairs.add((a, b))\n            edges = sorted(list(pairs))\n            # build triangles by intersecting neighbor sets\n            tri_set = set()\n            for i in range(n):\n                neigh_i = neighs[i]\n                for j in neigh_i:\n                    for k in neigh_i.intersection(neighs[j] if j < n else set()):\n                        if i < j < k:\n                            tri_set.add((i, j, k))\n            triangles = sorted(list(tri_set))\n\n            # compute weights favoring edges/triangles that include good nodes\n            if len(edges) > 0:\n                fmin, fmax = np.min(pop_f), np.max(pop_f)\n                rng_scale = max(fmax - fmin, 1e-9)\n                w = []\n                for (i, j) in edges:\n                    pair_score = min(pop_f[i], pop_f[j]) - fmin\n                    # lower pair_score -> better (closer to fmin)\n                    w.append(np.exp(-0.8 * pair_score / rng_scale) * (1.0 + 0.2 * (1.0 / (1.0 + np.linalg.norm(pop_X[i] - pop_X[j])))))\n                edge_weights = np.array(w, dtype=float)\n                edge_weights = edge_weights / np.sum(edge_weights)\n            else:\n                edge_weights = np.array([])\n\n            if len(triangles) > 0:\n                fmin, fmax = np.min(pop_f), np.max(pop_f)\n                rng_scale = max(fmax - fmin, 1e-9)\n                w = []\n                for (i, j, k) in triangles:\n                    tri_score = min(pop_f[i], pop_f[j], pop_f[k]) - fmin\n                    w.append(np.exp(-0.9 * tri_score / rng_scale) * 1.1)\n                tri_weights = np.array(w, dtype=float)\n                tri_weights = tri_weights / np.sum(tri_weights)\n            else:\n                tri_weights = np.array([])\n\n        refresh_graph()\n\n        iter_since_improve = 0\n        total_iters = 0\n        wins = 0\n        trials = 0\n\n        # helper to safely choose indices with weighting\n        def choose_index(weights_array):\n            if weights_array.size == 0:\n                return None\n            w = weights_array.copy()\n            w = np.clip(w, 1e-12, None)\n            w = w / np.sum(w)\n            return self.rng.choice(len(w), p=w)\n\n        # surrogate scorer for a candidate: lower predicted -> better\n        def surrogate_score(x, info=None):\n            # info can contain ('edge',i,j,alpha) or ('tri',i,j,k,weights) or ('local',i) etc.\n            # baseline: predicted as weighted sum of known endpoint f\n            if info is None:\n                # fallback: use distance to best as proxy\n                d = np.linalg.norm(x - x_best)\n                return f_best + 0.5 * (d / (mean_width + 1e-12))\n            typ = info[0]\n            if typ == 'edge':\n                _, i, j, alpha = info\n                fi = pop_f[i]; fj = pop_f[j]\n                # alpha refers to weight for i (convex)\n                pred = alpha * fi + (1 - alpha) * fj\n                # penalty for distance from the segment\n                pi = pop_X[i]; pj = pop_X[j]\n                seg_len = np.linalg.norm(pj - pi) + 1e-12\n                # distance from line\n                proj_len = np.dot((x - pi), (pj - pi)) / seg_len\n                closest = pi + np.clip(proj_len / seg_len, 0.0, 1.0) * (pj - pi)\n                dist_line = np.linalg.norm(x - closest)\n                # penalty scaled by edge length and typical width\n                pred += 1.5 * (dist_line / (seg_len + mean_width * 0.2 + 1e-12))**2\n                # novelty bonus (favor some exploration) - smaller predicted if novel\n                d_near = np.min(np.linalg.norm(pop_X - x[None, :], axis=1))\n                novelty = -0.04 * (d_near / (mean_width + 1e-12))\n                return pred + novelty\n            elif typ == 'tri':\n                _, i, j, k, wts = info\n                pred = wts[0] * pop_f[i] + wts[1] * pop_f[j] + wts[2] * pop_f[k]\n                # small penalty for barycenter outside triangle (shouldn't happen) and distance from triangle plane\n                pts = np.vstack([pop_X[i], pop_X[j], pop_X[k]])\n                centroid = pts.mean(axis=0)\n                d_cent = np.linalg.norm(x - centroid)\n                pred += 0.8 * (d_cent / (mean_width + 1e-12))**2\n                d_near = np.min(np.linalg.norm(pop_X - x[None, :], axis=1))\n                novelty = -0.05 * (d_near / (mean_width + 1e-12))\n                return pred + novelty\n            elif typ == 'local':\n                _, i = info\n                # predicted by nearby fitness with distance weighting\n                d = np.linalg.norm(x - pop_X[i])\n                pred = pop_f[i] + 0.5 * (d / (mean_width + 1e-12))\n                return pred\n            else:\n                return surrogate_score(x, None)\n\n        # candidate generators (do not call func)\n        def gen_edge_candidate(eidx):\n            i, j = edges[eidx]\n            xi = pop_X[i]; xj = pop_X[j]\n            dir_vec = xj - xi\n            dir_norm = np.linalg.norm(dir_vec) + 1e-12\n            u = dir_vec / dir_norm\n            # alpha near midpoint but allow extrapolation\n            alpha = float(self.rng.beta(2.0, 2.0))\n            extrap = self.rng.normal(loc=0.0, scale=0.12)\n            alpha = np.clip(alpha + extrap, -0.4, 1.4)\n            base = alpha * xi + (1.0 - alpha) * xj\n            # orthogonal subspace exploration via small PCA-like perturb\n            noise = self.rng.normal(size=dim)\n            proj = np.dot(noise, u) * u\n            orth = noise - proj\n            orth_norm = np.linalg.norm(orth)\n            if orth_norm > 1e-12:\n                orth = orth / orth_norm\n            else:\n                orth = self.rng.normal(size=dim)\n                orth = orth / (np.linalg.norm(orth) + 1e-12)\n            # magnitudes adapt to edge length and sigma\n            along_mag = (alpha - 0.5) * dir_norm * self.rng.normal(loc=1.0, scale=0.35)\n            orth_mag = self.sigma * (0.5 + 1.6 * self.rng.random()) * min(dir_norm / (mean_width + 1e-12), 1.2)\n            x = base + along_mag * u + orth_mag * orth\n            return np.minimum(np.maximum(x, lb), ub), ('edge', i, j, alpha)\n\n        def gen_tri_candidate(tidx):\n            i, j, k = triangles[tidx]\n            pts = np.vstack([pop_X[i], pop_X[j], pop_X[k]])\n            # barycentric weights from Dirichlet, bias towards center\n            raw = self.rng.gamma(2.0, 1.0, size=3)\n            wts = raw / (raw.sum() + 1e-12)\n            base = wts[0] * pts[0] + wts[1] * pts[1] + wts[2] * pts[2]\n            # small isotropic noise scaled by local spread\n            local_spread = np.mean(np.std(pts, axis=0)) + 1e-12\n            noise = self.rng.normal(scale=self.sigma * 0.6, size=dim) * (1.0 + local_spread / (mean_width + 1e-12))\n            x = base + noise\n            return np.minimum(np.maximum(x, lb), ub), ('tri', i, j, k, wts)\n\n        def gen_local_candidate():\n            # anisotropic refinement around best using population stdev\n            if pop_X.shape[0] >= max(3, dim // 2):\n                stds = np.std(pop_X, axis=0)\n                stds = np.maximum(stds, 0.02 * mean_width)\n                anis = stds / np.mean(stds)\n                local_sigma = self.sigma * (0.4 + 0.9 * self.rng.random())\n                x = x_best + local_sigma * (self.rng.normal(size=dim) * anis)\n            else:\n                local_sigma = self.sigma * (0.6 + 0.8 * self.rng.random())\n                x = x_best + local_sigma * self.rng.normal(size=dim)\n            return np.minimum(np.maximum(x, lb), ub), ('local', 0)\n\n        def gen_opposite_candidate():\n            idx = self.rng.integers(0, pop_X.shape[0])\n            x0 = pop_X[idx]\n            opp = lb + ub - x0\n            x = opp + 0.28 * self.sigma * self.rng.normal(size=dim)\n            return np.minimum(np.maximum(x, lb), ub), ('global',)\n\n        def gen_random_candidate():\n            x = self.rng.uniform(lb, ub)\n            return x, ('global',)\n\n        # main loop\n        while evals < budget:\n            total_iters += 1\n            # adjust intensities based on budget progress\n            frac = evals / max(1.0, budget)\n            # more local as time passes, but always densify edges periodically\n            p_edge = 0.45 * (1 - frac) + 0.35\n            p_tri = 0.12 + 0.10 * (1 - frac)\n            p_local = 0.18 + 0.50 * frac\n            p_global = 1.0 - (p_edge + p_tri + p_local)\n            ps = np.array([p_edge, p_tri, p_local, p_global])\n            ps = np.clip(ps, 1e-6, None)\n            ps = ps / ps.sum()\n            p_edge, p_tri, p_local, p_global = ps\n\n            # determine densification mode: every densify_every iterations create a larger pool\n            densify_mode = (total_iters % self.densify_every == 0)\n\n            # candidate pool size and evaluations this round\n            pool_size = self.cand_pool * (2 if densify_mode else 1)\n            pool_size = min(pool_size, 200)  # cap\n            evals_this_round = min(self.eval_per_round * (2 if densify_mode else 1), budget - evals)\n            evals_this_round = max(1, int(evals_this_round))\n\n            # generate a pool of candidates and surrogate scores\n            cand_list = []\n            info_list = []\n            scores = []\n\n            # strategy: sample many candidates biased by graph structures\n            for _ in range(pool_size):\n                r = self.rng.random()\n                if r < p_edge and len(edges) > 0:\n                    eidx = choose_index(edge_weights) if len(edge_weights) > 0 else self.rng.integers(0, len(edges))\n                    if eidx is None:\n                        x, info = gen_random_candidate()\n                    else:\n                        x, info = gen_edge_candidate(eidx)\n                elif r < p_edge + p_tri and len(triangles) > 0:\n                    tidx = choose_index(tri_weights) if len(tri_weights) > 0 else self.rng.integers(0, len(triangles))\n                    if tidx is None:\n                        x, info = gen_random_candidate()\n                    else:\n                        x, info = gen_tri_candidate(tidx)\n                elif r < p_edge + p_tri + p_local:\n                    x, info = gen_local_candidate()\n                else:\n                    if self.rng.random() < 0.5:\n                        x, info = gen_opposite_candidate()\n                    else:\n                        x, info = gen_random_candidate()\n\n                s = surrogate_score(x, info)\n                cand_list.append(x)\n                info_list.append(info)\n                scores.append(s)\n\n            scores = np.array(scores)\n            # prefer candidates with lowest surrogate predictions, but add small jitter to break ties\n            order = np.argsort(scores + 1e-9 * self.rng.random(len(scores)))\n\n            # pick top candidates to evaluate, but ensure not exceeding budget\n            evaluated_this_batch = 0\n            for idx in order:\n                if evaluated_this_batch >= evals_this_round:\n                    break\n                # evaluate this candidate\n                x_cand = cand_list[idx]\n                f_cand = float(func(x_cand))\n                evals += 1\n                evaluated_this_batch += 1\n\n                # update archive\n                X_archive = np.vstack([X_archive, x_cand])\n                f_archive = np.concatenate([f_archive, [f_cand]])\n\n                # update best\n                improved = False\n                if f_cand < f_best:\n                    f_best = float(f_cand)\n                    x_best = x_cand.copy()\n                    improved = True\n                    iter_since_improve = 0\n                    wins += 1\n                else:\n                    iter_since_improve += 1\n\n                trials += 1\n\n                # adapt sigma conservatively\n                if improved:\n                    self.sigma = max(self.sigma_min, self.sigma * self.success_shrink)\n                else:\n                    self.sigma = min(self.sigma_max, self.sigma * self.failure_expand)\n\n                # maintain/populate compact population\n                if pop_X.shape[0] < self.pop_size:\n                    pop_X = np.vstack([pop_X, x_cand])\n                    pop_f = np.concatenate([pop_f, [f_cand]])\n                else:\n                    # if candidate better than worst, replace it\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_cand < pop_f[worst_idx]:\n                        pop_X[worst_idx] = x_cand\n                        pop_f[worst_idx] = f_cand\n                    else:\n                        # occasional random replacement to increase diversity\n                        if self.rng.random() < 0.015:\n                            ridx = self.rng.integers(0, pop_X.shape[0])\n                            pop_X[ridx] = x_cand\n                            pop_f[ridx] = f_cand\n\n                # optionally densify: if candidate generated from edge/tri, attempt to add midpoint/extrapolation to pop archive\n                info = info_list[idx]\n                if info is not None and info[0] == 'edge':\n                    _, i, j, alpha = info\n                    # create an explicit midpoint sample (cheaply infer, but must evaluate if we want to add)\n                    midpoint = 0.5 * (pop_X[i] + pop_X[j])\n                    # only evaluate midpoint if it's sufficiently far from existing archive and budget permits and surrogate promising\n                    d_to_archive = np.min(np.linalg.norm(X_archive - midpoint[None, :], axis=1))\n                    if d_to_archive > 1e-6 and evals < budget and d_to_archive > 0.02 * mean_width:\n                        # quick surrogate check: if midpoint predicted good relative to f_best, evaluate\n                        pred_mid = surrogate_score(midpoint, ('edge', i, j, 0.5))\n                        if pred_mid <= f_best + 0.5 * abs(f_best) + 1.0:\n                            fm = float(func(midpoint))\n                            evals += 1\n                            X_archive = np.vstack([X_archive, midpoint])\n                            f_archive = np.concatenate([f_archive, [fm]])\n                            if fm < f_best:\n                                f_best = float(fm)\n                                x_best = midpoint.copy()\n                                iter_since_improve = 0\n                                self.sigma = max(self.sigma_min, self.sigma * self.success_shrink)\n                # enforce archive cap\n                if X_archive.shape[0] > self.max_archive:\n                    keep_best = int(self.max_archive * 0.6)\n                    best_inds = np.argsort(f_archive)[:keep_best]\n                    other_pool = np.setdiff1d(np.arange(X_archive.shape[0]), best_inds)\n                    keep_other = max(0, self.max_archive - keep_best)\n                    if other_pool.size > 0 and keep_other > 0:\n                        other_inds = self.rng.choice(other_pool, size=min(keep_other, other_pool.size), replace=False)\n                        keep = np.unique(np.concatenate([best_inds, other_inds]))\n                    else:\n                        keep = best_inds\n                    X_archive = X_archive[keep]\n                    f_archive = f_archive[keep]\n\n                # stop if budget exhausted\n                if evals >= budget:\n                    break\n\n            # periodic rebuild population and refresh graph weights\n            if (evals % self.edge_refresh) == 0 or evaluated_this_batch > 0:\n                pop_X, pop_f = build_population(X_archive, f_archive)\n                # ensure consistency (pop_f may be float dtype)\n                pop_f = pop_f.astype(float)\n                refresh_graph()\n\n            # emergency diversification if stagnating\n            if iter_since_improve > max(80, 6 * dim) and self.rng.random() < 0.22 and evals < budget:\n                # generate a strong global sample and evaluate it\n                xg = self.rng.uniform(lb, ub)\n                fg = float(func(xg))\n                evals += 1\n                X_archive = np.vstack([X_archive, xg])\n                f_archive = np.concatenate([f_archive, [fg]])\n                # might replace worst\n                if pop_X.shape[0] < self.pop_size:\n                    pop_X = np.vstack([pop_X, xg])\n                    pop_f = np.concatenate([pop_f, [fg]])\n                else:\n                    worst_idx = int(np.argmax(pop_f))\n                    if fg < pop_f[worst_idx]:\n                        pop_X[worst_idx] = xg\n                        pop_f[worst_idx] = fg\n                if fg < f_best:\n                    f_best = fg\n                    x_best = xg.copy()\n                    iter_since_improve = 0\n                # after diversification, refresh graph\n                pop_X, pop_f = build_population(X_archive, f_archive)\n                refresh_graph()\n\n        return float(f_best), np.array(x_best, copy=True)", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["701bfc7e-c1fd-40fc-8533-beb7ddd599a9"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4271.0, "Edges": 4270.0, "Max Degree": 40.0, "Min Degree": 1.0, "Mean Degree": 1.9995317255911964, "Degree Variance": 2.228985966624019, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.334695963208993, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.327862778062432, "Depth Entropy": 2.1063201648913576, "Assortativity": 0.0, "Average Eccentricity": 20.386092250058535, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.00023413720440177945, "Average Shortest Path": 10.917675713940266, "mean_complexity": 6.818181818181818, "total_complexity": 75.0, "mean_token_count": 340.1818181818182, "total_token_count": 3742.0, "mean_parameter_count": 1.7272727272727273, "total_parameter_count": 19.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "1f8a98c6-4a86-48df-93f5-b4a599603be3", "fitness": "-inf", "name": "EDGES", "description": "Edge-Dense Elite Graph Sampling (EDGES) \u2014 build and densify an elite edge/triangle graph, generate large pools of edge/ and triangle-based recombinations (interpolation, extrapolation, orthogonal perturbations), score candidates with cheap local weighted linear surrogates, evaluate only the most promising ones, and bias future sampling toward edges that produced improvements to rapidly increase \"edge density\" of high-quality solutions.", "code": "import numpy as np\n\nclass EDGES:\n    \"\"\"\n    Edge-Dense Elite Graph Sampling (EDGES)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional kwargs:\n      rng: numpy Generator or None\n      init_fraction: fraction of budget used for initial random sampling\n      elite_size: maximum number of elites to keep\n      pool_mul: multiplier controlling how many candidates to propose per evaluation\n      orth_scale: relative scale of orthogonal perturbations\n      tri_prob: probability of triangle barycentric proposals vs edges\n      restart_after: stagnation threshold (evaluations without improvement) to trigger diversification\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None, init_fraction=0.12,\n                 elite_size=None, pool_mul=8, orth_scale=0.15, tri_prob=0.12,\n                 restart_after=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # sampling / meta parameters\n        self.init_fraction = float(init_fraction)\n        self.pool_mul = float(pool_mul)\n        self.orth_scale = float(orth_scale)\n        self.tri_prob = float(tri_prob)\n        # elite size defaults adaptively to problem dimension\n        if elite_size is None:\n            self.elite_size = max(6, min(30, 4 * self.dim))\n        else:\n            self.elite_size = int(elite_size)\n\n        # stagnation handling\n        self.restart_after = int(restart_after)\n\n    def __call__(self, func):\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        # determine bounds if available, else use [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(dim, lb.item(), dtype=float)\n            if ub.size == 1:\n                ub = np.full(dim, ub.item(), dtype=float)\n        except Exception:\n            lb = np.full(dim, -5.0, dtype=float)\n            ub = np.full(dim, 5.0, dtype=float)\n\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        # bookkeeping arrays of evaluated points and values\n        X = np.empty((0, dim), dtype=float)\n        F = np.empty((0,), dtype=float)\n        evals = 0\n\n        # initial sampling\n        n_init = min(budget, max(10, int(budget * self.init_fraction), 2 * dim))\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            X = np.vstack([X, x.reshape(1, -1)])\n            F = np.append(F, f)\n            evals += 1\n\n        # initial best\n        best_idx = int(np.argmin(F))\n        x_best = X[best_idx].copy()\n        f_best = float(F[best_idx])\n\n        # step scale sigma for orthogonal perturbations and extrapolation magnitude\n        sigma = max(1e-8, 0.18 * range_scale)\n\n        # edge success counters (map sorted pair->(trials, successes))\n        edge_stats = {}  # key: (i,j), value: [trials, successes]\n\n        # stagnation\n        since_improve = 0\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # build elites (indices into X)\n            n_elite = min(self.elite_size, X.shape[0])\n            elite_idx = np.argsort(F)[:n_elite]\n            elites = X[elite_idx]               # shape (n_elite, dim)\n            f_elites = F[elite_idx]\n\n            # compute how many evaluations we will allocate this cycle\n            # propose many candidates (pool) and evaluate only top-k\n            # pool_size scaled by pool_mul * evaluations_to_spend_this_cycle\n            # choose to spend at most a small fraction of remaining each cycle\n            eval_per_cycle = max(1, min(remaining, int(max(1, remaining * 0.06))))\n            pool_size = int(min(2000, max(50, int(self.pool_mul * eval_per_cycle))))\n            pool_size = max(pool_size, eval_per_cycle * 4)\n\n            candidates = np.empty((0, dim), dtype=float)\n            cand_meta = []  # store (i,j,kind,t) metadata for edge-tracking; kind: 'edge'/'tri'/'rand'\n\n            # bias pair selection by edge success (edges that improved more get higher chance)\n            # precompute pair weights for elites\n            if n_elite >= 2:\n                # build list of pairs for random sampling with weights\n                pair_list = []\n                pair_weights = []\n                for i in range(n_elite):\n                    for j in range(i + 1, n_elite):\n                        key = (elite_idx[i], elite_idx[j])\n                        stats = edge_stats.get(key, (0.0, 0.0))\n                        trials, succ = stats\n                        # weight base by quality of endpoints (favor good elites)\n                        w = 1.0 / (1.0 + 0.5 * (f_elites[i] + f_elites[j] - 2 * f_best) / (abs(f_best) + 1.0))\n                        # add success bias (more successes -> increase probability)\n                        w *= (1.0 + (succ / (1.0 + trials)) * 4.0)\n                        # slight size-based bias to diversify far-apart elites\n                        w *= (1.0 + np.linalg.norm(elites[i] - elites[j]) / (range_scale + 1e-9) * 0.3)\n                        pair_list.append((i, j))\n                        pair_weights.append(max(1e-6, float(w)))\n                pair_weights = np.array(pair_weights, dtype=float)\n                pair_weights /= pair_weights.sum()\n            else:\n                pair_list = []\n                pair_weights = np.array([])\n\n            # generate pool of candidate points\n            # strategies: edge interpolation/extrapolation + orthogonal perturbation, triangle barycentric, occasional global sample\n            while candidates.shape[0] < pool_size:\n                r = rng.random()\n                if (r < self.tri_prob) and n_elite >= 3:\n                    # triangle barycentric combination\n                    ids = rng.choice(n_elite, size=3, replace=False)\n                    a, b, c = elites[ids]\n                    # barycentric weights (allow slight extrapolation)\n                    w = rng.dirichlet([1.0, 1.0, 1.0])  # positive weights summing to 1\n                    # with some chance allow negative weight (extrapolation)\n                    if rng.random() < 0.18:\n                        w = w * (1.0 + rng.normal(0, 0.35, size=3))\n                    w = w / (np.sum(w) + 1e-12)\n                    x = w[0] * a + w[1] * b + w[2] * c\n                    kind = 'tri'\n                    meta = (tuple(elite_idx[ids]), kind, tuple(w))\n                elif n_elite >= 2 and (pair_list):\n                    # edge-based: sample a pair according to pair_weights\n                    sel = rng.choice(len(pair_list), p=pair_weights)\n                    i, j = pair_list[sel]\n                    xi = elites[i]\n                    xj = elites[j]\n                    fi = f_elites[i]\n                    fj = f_elites[j]\n                    # sample interpolation/extrapolation parameter t (allow extrapolation)\n                    t = rng.beta(1.2, 1.2)  # center towards middle\n                    # allow extrapolation occasionally\n                    if rng.random() < 0.18:\n                        t = rng.uniform(-0.4, 1.4)\n                    x = (1.0 - t) * xi + t * xj\n\n                    # add orthogonal perturbation to increase edge-density exploration\n                    u = xj - xi\n                    u_norm = np.linalg.norm(u)\n                    if u_norm > 1e-12:\n                        # random vector then orthogonalize to u\n                        v = rng.normal(size=dim)\n                        v -= (np.dot(v, u) / (u_norm**2)) * u\n                        v_norm = np.linalg.norm(v)\n                        if v_norm > 1e-12:\n                            # orthogonal perturbation scaled relative to edge length\n                            orth_mag = self.orth_scale * (u_norm / (range_scale + 1e-12)) * sigma\n                            x = x + (v / v_norm) * (rng.normal() * orth_mag)\n                    # small isotropic jitter\n                    x = x + rng.normal(scale=0.06 * sigma, size=dim)\n                    kind = 'edge'\n                    meta = ((int(elite_idx[i]), int(elite_idx[j])), kind, float(t))\n                else:\n                    # fallback random global sample\n                    x = rng.uniform(lb, ub)\n                    kind = 'rand'\n                    meta = (None, kind, None)\n\n                # clip/reflect to bounds (clip for simplicity)\n                x = np.clip(x, lb, ub)\n\n                candidates = np.vstack([candidates, x.reshape(1, -1)])\n                cand_meta.append(meta)\n\n            # Now we have a candidate pool. Score them cheaply using local weighted linear surrogates.\n            # For each candidate, find k nearest neighbors from X (existing evaluated points)\n            k_neigh = min(8, max(3, X.shape[0]))\n            # compute distances between candidates and X\n            # (pool_size x N) maybe large but acceptable for moderate pool_size\n            dists = np.linalg.norm(candidates[:, None, :] - X[None, :, :], axis=2)  # shape (pool, N)\n            # find k nearest indices\n            neigh_idx = np.argpartition(dists, kth=k_neigh - 1, axis=1)[:, :k_neigh]  # shape (pool, k_neigh)\n\n            # Prepare scoring arrays\n            scores = np.empty(candidates.shape[0], dtype=float)\n            pred_vals = np.empty_like(scores)\n            uncert = np.empty_like(scores)\n\n            # For numerical stability add small ridge in regression\n            ridge = 1e-8\n\n            for ii in range(candidates.shape[0]):\n                ni = neigh_idx[ii]\n                Xn = X[ni]  # k x d\n                yn = F[ni]  # k\n                # distances for weighting\n                dn = dists[ii, ni]\n                # compute weights based on distance (closer -> higher weight)\n                scale = np.median(dn) + 1e-9\n                w = np.exp(-0.5 * (dn / (1.2 * max(scale, 1e-9)))**2)\n                w_sum = w.sum()\n                if w_sum < 1e-12:\n                    w = np.ones_like(w)\n                    w_sum = w.sum()\n                # center X for regression to reduce collinearity\n                Xc = Xn - np.average(Xn, axis=0, weights=w).reshape(1, -1)\n                # design matrix with intercept\n                A = np.hstack([np.ones((Xc.shape[0], 1)), Xc])  # k x (d+1)\n                # weighted least squares via sqrt-weights\n                sqrtw = np.sqrt(w)\n                Aw = A * sqrtw[:, None]\n                yw = yn * sqrtw\n                # solve for theta\n                try:\n                    theta, *_ = np.linalg.lstsq(Aw, yw, rcond=None)\n                except Exception:\n                    theta = np.zeros(A.shape[1])\n                # predict for candidate\n                xc = candidates[ii] - np.average(Xn, axis=0, weights=w)\n                A_test = np.concatenate(([1.0], xc))\n                y_hat = float(np.dot(A_test, theta))\n                # residuals and estimated variance\n                res = yn - (A @ theta)\n                # weighted residual variance (unbiased-ish)\n                denom = max(1.0, w_sum - (dim + 1))\n                var = float(np.sum(w * (res**2)) / denom)\n                std = np.sqrt(max(var, 1e-12))\n                # inflate uncertainty for candidates far from neighbors\n                far_factor = 1.0 + (np.min(dn) / (range_scale + 1e-9))\n                uncertainty = std * far_factor\n                # final surrogate score: predicted value minus a multiple of uncertainty (lower is better)\n                score = y_hat - 0.9 * uncertainty\n                # small bias: prefer candidates closer to mid-edge (for edge metabolic densification) if meta kind is 'edge'\n                meta = cand_meta[ii]\n                if meta[1] == 'edge':\n                    t = meta[2]\n                    # prefer interior but allow extrapolation slightly\n                    score += 0.06 * abs(0.5 - t) * (abs(y_hat) + 1.0) * 0.02\n                # encourage exploration for 'rand' or for points far from any evaluation\n                if meta[1] == 'rand':\n                    score += 0.06 * (np.min(dists[ii]) / (range_scale + 1e-9))\n                scores[ii] = score\n                pred_vals[ii] = y_hat\n                uncert[ii] = uncertainty\n\n            # choose top eval_per_cycle candidates to actually evaluate\n            order = np.argsort(scores)\n            to_eval_idx = order[:min(eval_per_cycle, candidates.shape[0], remaining)]\n\n            # Evaluate selected candidates in order\n            for idx in to_eval_idx:\n                x_try = candidates[idx]\n                f_try = float(func(x_try))\n                # append to archive\n                X = np.vstack([X, x_try.reshape(1, -1)])\n                F = np.append(F, f_try)\n                evals += 1\n\n                # update best and edge stats\n                improved = False\n                if f_try < f_best:\n                    f_best = f_try\n                    x_best = x_try.copy()\n                    improved = True\n                    since_improve = 0\n                    # shrink sigma slightly when improving\n                    sigma = max(1e-12, sigma * 0.92)\n                else:\n                    since_improve += 1\n                    # slowly expand sigma when not improving\n                    sigma = min(10.0 * range_scale, sigma * 1.0015)\n\n                # update edge stats if this candidate was from an edge meta\n                meta = cand_meta[idx]\n                if meta[1] == 'edge' and meta[0] is not None:\n                    key = tuple(sorted((int(meta[0][0]), int(meta[0][1]))))\n                    trials, succ = edge_stats.get(key, (0.0, 0.0))\n                    trials += 1.0\n                    if improved:\n                        succ += 1.0\n                    edge_stats[key] = (trials, succ)\n                elif meta[1] == 'tri' and meta[0] is not None:\n                    # count triangle as successes for constituent edges to propagate densification\n                    ids = list(meta[0])\n                    for a in range(len(ids)):\n                        for b in range(a + 1, len(ids)):\n                            key = tuple(sorted((int(ids[a]), int(ids[b]))))\n                            trials, succ = edge_stats.get(key, (0.0, 0.0))\n                            trials += 0.5\n                            if improved:\n                                succ += 0.25\n                            edge_stats[key] = (trials, succ)\n\n                # stop early if budget exhausted\n                if evals >= budget:\n                    break\n\n            # If too long without improvement, inject diversification & local intensification\n            if since_improve >= self.restart_after and evals < budget:\n                # Try several targeted pushes around best and around random elites\n                pushes = min(10, budget - evals)\n                for _ in range(pushes):\n                    if rng.random() < 0.6:\n                        center = x_best\n                    else:\n                        # pick a random elite center\n                        ii = rng.integers(0, X.shape[0])\n                        center = X[ii]\n                    # strong perturbation to escape\n                    xp = center + rng.normal(scale=1.8 * sigma, size=dim)\n                    xp = np.clip(xp, lb, ub)\n                    f_p = float(func(xp))\n                    X = np.vstack([X, xp.reshape(1, -1)])\n                    F = np.append(F, f_p)\n                    evals += 1\n                    if f_p < f_best:\n                        f_best = f_p\n                        x_best = xp.copy()\n                        # reduce sigma to refine locally\n                        sigma = max(1e-12, sigma * 0.7)\n                        since_improve = 0\n                        break\n                    if evals >= budget:\n                        break\n                # also slightly increase orth_scale to explore more orthogonal density\n                self.orth_scale = min(0.6, self.orth_scale * 1.08)\n                # reduce tri_prob to focus edges if stagnation persists\n                self.tri_prob = max(0.03, self.tri_prob * 0.85)\n\n            # small safeguard to avoid infinite loop if no candidates remain (shouldn't happen)\n            if candidates.shape[0] == 0:\n                break\n\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2777.0, "Edges": 2776.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.9992797983435362, "Degree Variance": 2.328411436657071, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.797517455391777, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3205274711221582, "Depth Entropy": 2.18852342382273, "Assortativity": 1.758622270900939e-08, "Average Eccentricity": 18.543752250630178, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00036010082823190496, "Average Shortest Path": 10.607099901517094, "mean_complexity": 22.0, "total_complexity": 44.0, "mean_token_count": 1261.5, "total_token_count": 2523.0, "mean_parameter_count": 6.0, "total_parameter_count": 12.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "50997a29-66b6-4fe5-805d-985968c5ec6b", "fitness": "-inf", "name": "EdgeDensifiedLowRankTriangulationSurrogates", "description": "Edge-Densified Low-Rank Triangulation Surrogates (EDLRTS) \u2014 aggressively densify an elite edge graph by proposing many edge/triangle barycentric and 1-D quadratic-minimizer candidates, cheaply score them with 1-D/2-D surrogates (along edges/triangles), evaluate only the most promising, and adaptively boost edges that produce improvements while occasionally solving low-rank quadratic surrogates in small PCA/edge subspaces.", "code": "import numpy as np\n\nclass EdgeDensifiedLowRankTriangulationSurrogates:\n    \"\"\"\n    Edge-Densified Low-Rank Triangulation Surrogates (EDLRTS)\n\n    Main idea:\n      - Maintain an archive of evaluated points.\n      - Build a compact elite set and form an edge graph (k-NN among elites).\n      - For each edge generate many virtual candidates (interpolations, extrapolations,\n        barycentric points for triples, small orthogonal perturbations).\n      - Fit very cheap 1-D quadratics along edges (and small 2-D surrogates for triangles)\n        using archive projections to predict a minimizer along the edge/triangle.\n      - Rank candidates by predicted improvement and a diversity penalty, evaluate top\n        candidates within remaining budget.\n      - Adapt edge weights / scores (edges that improve are densified further).\n      - Occasionally run a low-rank quadratic surrogate around best points for refinement.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_samples=None,\n                 elite_size=10, k_edges=3, densify=6, batch_eval=6,\n                 subspace_dim=3, global_inject_prob=0.06, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total function evaluations\n          dim: problem dimensionality\n          init_samples: number of initial random samples (default ~ min(40, budget//10 or 10))\n          elite_size: number of top archive points used to form graph\n          k_edges: k nearest neighbors per elite to create edges\n          densify: base number of virtual candidates per edge (scaled by edge score)\n          batch_eval: how many top candidates to evaluate each iteration\n          subspace_dim: dimension for occasional low-rank quadratic solves\n          global_inject_prob: probability to inject a global random sample each iteration\n          rng: numpy.random.Generator or None\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if init_samples is None:\n            self.init_samples = int(min(40, max(8, self.budget // 12)))\n        else:\n            self.init_samples = int(init_samples)\n        self.elite_size = int(elite_size)\n        self.k_edges = int(k_edges)\n        self.densify = float(densify)\n        self.batch_eval = int(max(1, batch_eval))\n        self.subspace_dim = int(min(subspace_dim, dim))\n        self.global_inject_prob = float(global_inject_prob)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _orthonormal_basis(self, k):\n        A = self.rng.normal(size=(self.dim, k))\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def _proj_param_on_line(self, xi, xj, X):\n        # projects rows of X onto line xi->xj, returns t (scalar) and perp distances\n        v = xj - xi\n        v2 = np.dot(v, v)\n        if v2 <= 1e-16:\n            # degenerate edge\n            t = np.zeros(X.shape[0])\n            perp = np.linalg.norm(X - xi[None, :], axis=1)\n            return t, perp\n        t = (X - xi[None, :]) @ v / v2\n        proj = xi[None, :] + np.outer(t, v)\n        perp = np.linalg.norm(X - proj, axis=1)\n        return t, perp\n\n    def _fit_quad_1d(self, t, y):\n        # Fit quadratic y = a t^2 + b t + c via ridge lsq; return a,b,c or None\n        if len(t) < 3:\n            # fallback to linear fit if insufficient points\n            A = np.vstack([np.ones_like(t), t]).T\n            reg = 1e-8\n            try:\n                params = np.linalg.lstsq(A.T @ A + reg * np.eye(2), A.T @ y, rcond=None)[0]\n                c, b = params[0], params[1]\n                return 0.0, float(b), float(c)\n            except Exception:\n                return None\n        T = np.vstack([t**2, t, np.ones_like(t)]).T\n        reg = 1e-8 * max(1.0, np.var(y))\n        try:\n            ATA = T.T @ T\n            ATA[np.diag_indices_from(ATA)] += reg + 1e-12\n            abc = np.linalg.solve(ATA, T.T @ y)\n            return float(abc[0]), float(abc[1]), float(abc[2])\n        except np.linalg.LinAlgError:\n            try:\n                abc, *_ = np.linalg.lstsq(T, y, rcond=None)\n                return float(abc[0]), float(abc[1]), float(abc[2])\n            except Exception:\n                return None\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []\n        F = []\n\n        # initial seeding\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(float(f))\n\n        # ensure at least one sample\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x)); evals += 1\n            X.append(np.array(x, dtype=float)); F.append(float(f))\n\n        best_idx = int(np.argmin(F))\n        best_f = float(F[best_idx]); best_x = X[best_idx].copy()\n\n        # edge scoring (dict keyed by tuple(i,j))\n        edge_scores = {}\n\n        iter_count = 0\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            n_archive = len(X)\n            X_arr = np.vstack(X)\n            F_arr = np.array(F, dtype=float)\n\n            # identify elites\n            e_size = min(self.elite_size, n_archive)\n            idx_sorted = np.argsort(F_arr)\n            elites = idx_sorted[:e_size].tolist()\n\n            # build edges: for each elite, connect to k nearest elites (in ambient)\n            edges = set()\n            for i in elites:\n                xi = X_arr[i]\n                # distances to other elites\n                others = [j for j in elites if j != i]\n                if not others:\n                    continue\n                dists = np.linalg.norm(X_arr[others] - xi[None, :], axis=1)\n                k = min(self.k_edges, len(others))\n                idx_nei = np.argsort(dists)[:k]\n                for idxn in idx_nei:\n                    j = others[idxn]\n                    a, b = (i, j) if i < j else (j, i)\n                    edges.add((a, b))\n            edges = sorted(list(edges))\n            if len(edges) == 0:\n                # fallback random perturb around best_x\n                if evals >= self.budget:\n                    break\n                xg = np.clip(best_x + self.rng.normal(0, 0.1, size=self.dim), lb, ub)\n                fg = float(func(xg)); evals += 1\n                X.append(xg.copy()); F.append(fg)\n                if fg < best_f:\n                    best_f = fg; best_x = xg.copy()\n                continue\n\n            # candidate pool (predicted)\n            candidates = []  # list of dicts: {'x':, 'pred':, 'edge':, 'score':, 'dist_pen':}\n            # global scale for orthogonal perturbations based on archive spread\n            spread = np.mean(np.std(X_arr, axis=0))\n            orth_scale_base = max(1e-6, 0.05 * spread + 1e-3)\n\n            # precompute neighbor selection radius and distance matrix maybe\n            for (i, j) in edges:\n                xi = X_arr[i]; xj = X_arr[j]\n                key = (i, j)\n                score = edge_scores.get(key, 1.0)\n                n_virt = int(max(1, round(self.densify * score)))\n                # project archive onto edge\n                t_all, perp_all = self._proj_param_on_line(xi, xj, X_arr)\n                # keep samples with small perp or endpoints themselves\n                active_mask = (perp_all <= max(1e-6, 0.5 * np.linalg.norm(xj - xi))) | (np.arange(n_archive) == i) | (np.arange(n_archive) == j)\n                t_sel = t_all[active_mask]\n                y_sel = F_arr[active_mask]\n                # fit quadratic along edge\n                quad = self._fit_quad_1d(t_sel, y_sel)\n                # fallback to linear or simple midpoint prediction if quad not ok\n                if quad is not None:\n                    a, b, c = quad\n                    if abs(a) > 1e-12:\n                        t_star = -b / (2.0 * a)\n                    else:\n                        # linear fallback: choose best of endpoints or direction of slope\n                        t_star = t_sel[np.argmin(y_sel)] if len(t_sel) else 0.5\n                else:\n                    t_star = 0.5  # midpoint\n                # allow multiple virtuals: around t_star, uniform interpolation/extrapolation and small orth perturbations\n                v = xj - xi\n                vnorm2 = np.dot(v, v)\n                for k_virt in range(n_virt):\n                    # jitter t\n                    jitter = self.rng.normal(scale=0.15)\n                    t_try = t_star + jitter\n                    # clamp to slightly beyond endpoints\n                    t_try = float(np.clip(t_try, -0.6, 1.6))\n                    base = xi + t_try * v\n                    # orthogonal perturbation\n                    if self.dim > 1:\n                        # create small orthonormal basis orthogonal to v\n                        if vnorm2 <= 1e-16:\n                            ort = self.rng.normal(size=self.dim)\n                            ort = ort / (np.linalg.norm(ort) + 1e-12)\n                            perp_vec = ort\n                        else:\n                            # random vector and remove projection on v\n                            r = self.rng.normal(size=self.dim)\n                            proj = v * (np.dot(r, v) / (vnorm2 + 1e-12))\n                            perp_vec = r - proj\n                            nrm = np.linalg.norm(perp_vec)\n                            if nrm < 1e-12:\n                                # fallback orth basis from QR\n                                perp_vec = self._orthonormal_basis(1)[:, 0]\n                            else:\n                                perp_vec = perp_vec / nrm\n                        orth_scale = orth_scale_base * (0.2 + 0.8 * self.rng.random()) * (1.0 / (1.0 + abs(t_try - 0.5)))\n                        base = base + perp_vec * orth_scale * (0.6 + 0.8 * self.rng.random())\n                    x_cand = np.minimum(np.maximum(base, lb), ub)\n                    # predicted value via quadratic if available\n                    if quad is not None:\n                        pred_val = float(a * (t_try**2) + b * t_try + c)\n                    else:\n                        # linear interpolation between endpoints\n                        pred_val = float((1 - t_try) * F_arr[i] + t_try * F_arr[j]) if 0.0 <= t_try <= 1.0 else min(F_arr[i], F_arr[j]) + 0.1\n                    # diversity penalty: distance to nearest archive point\n                    dists = np.linalg.norm(X_arr - x_cand[None, :], axis=1)\n                    min_dist = float(dists.min())\n                    diversity_pen = np.exp(-min_dist / (1e-6 + 0.1 * (np.mean(np.std(X_arr, axis=0)) + 1e-6)))\n                    # predicted improvement relative to best of endpoints\n                    baseline = min(F_arr[i], F_arr[j])\n                    pred_imp = max(0.0, baseline - pred_val)\n                    score_cand = pred_imp * (1.0 + 0.5 * score) - 0.2 * diversity_pen\n                    candidates.append({'x': x_cand, 'pred': pred_val, 'edge': key, 'score': score_cand, 'min_dist': min_dist, 'base': baseline})\n\n                # Also create a pure midpoint/extrapolation candidate (no quad) for safety\n                mid = np.minimum(np.maximum(0.5 * (xi + xj), lb), ub)\n                candidates.append({'x': mid, 'pred': float(0.5 * (F_arr[i] + F_arr[j])), 'edge': key, 'score': 0.5 * score, 'min_dist': float(np.min(np.linalg.norm(X_arr - mid[None, :], axis=1))), 'base': min(F_arr[i], F_arr[j])})\n\n            if not candidates:\n                # fallback random\n                if evals >= self.budget:\n                    break\n                xg = lb + self.rng.random(self.dim) * (ub - lb)\n                fg = float(func(xg)); evals += 1\n                X.append(xg.copy()); F.append(fg)\n                if fg < best_f:\n                    best_f = fg; best_x = xg.copy()\n                continue\n\n            # rank candidates\n            # convert to arrays for sorting\n            scores = np.array([c['score'] for c in candidates])\n            # add small bonus for further candidates (diversity)\n            min_dists = np.array([c['min_dist'] for c in candidates])\n            scores = scores + 0.01 * (min_dists / (1.0 + np.mean(min_dists)))\n            order = np.argsort(-scores)\n            # decide how many to evaluate this round\n            remain = self.budget - evals\n            to_eval = min(remain, self.batch_eval, len(candidates))\n            # Evaluate top to_eval candidates sequentially\n            evaluated_edges_improved = {}\n            eval_count = 0\n            for idx in order[:to_eval]:\n                if evals >= self.budget:\n                    break\n                cand = candidates[int(idx)]\n                x_try = cand['x']\n                # safety clip\n                x_try = np.minimum(np.maximum(np.array(x_try, dtype=float), lb), ub)\n                # avoid re-evaluating exact duplicates: if too close to an archived point, skip unless very promising\n                dists_to_archive = np.linalg.norm(X_arr - x_try[None, :], axis=1)\n                if dists_to_archive.min() < 1e-8:\n                    # duplicate; skip\n                    continue\n                f_try = float(func(x_try)); evals += 1; eval_count += 1\n                X.append(x_try.copy()); F.append(float(f_try))\n                # update best\n                if f_try < best_f:\n                    best_f = f_try; best_x = x_try.copy()\n                # update edge score feedback\n                key = cand['edge']\n                if f_try < cand['base'] - 1e-12:\n                    # improvement on edge baseline\n                    edge_scores[key] = edge_scores.get(key, 1.0) * 1.15 + 0.05\n                    evaluated_edges_improved.setdefault(key, []).append(True)\n                else:\n                    edge_scores[key] = edge_scores.get(key, 1.0) * 0.92\n                    evaluated_edges_improved.setdefault(key, []).append(False)\n\n            # occasionally perform a low-rank quadratic surrogate around best_x\n            if evals < self.budget and self.rng.random() < 0.18:\n                k_sub = min(self.subspace_dim, self.dim)\n                basis = self._orthonormal_basis(k_sub)\n                # collect nearby archive points around best_x\n                X_arr = np.vstack(X)\n                dists = np.linalg.norm(X_arr - best_x[None, :], axis=1)\n                # take nearest m points\n                m = min(max(1 + k_sub + k_sub*(k_sub+1)//2, 6), len(X))\n                nn_idx = np.argsort(dists)[:m]\n                S = (X_arr[nn_idx] - best_x[None, :]) @ basis\n                # fit quadratic in-subspace: y = a + b^T s + 0.5 s^T H s\n                # build Phi\n                def build_phi(Sm):\n                    mloc, kloc = Sm.shape\n                    cols = [np.ones((mloc, 1)), Sm]\n                    for ii in range(kloc):\n                        cols.append(0.5 * (Sm[:, ii:ii+1] ** 2))\n                    for ii in range(kloc):\n                        for jj in range(ii+1, kloc):\n                            cols.append((Sm[:, ii] * Sm[:, jj])[:, None])\n                    return np.hstack(cols)\n                Phi = build_phi(S)\n                y = np.array([F[int(idx)] for idx in nn_idx], dtype=float)\n                reg = 1e-8 * max(1.0, np.var(y))\n                try:\n                    A = Phi.T @ Phi\n                    A[np.diag_indices_from(A)] += reg + 1e-12\n                    rhs = Phi.T @ y\n                    theta = np.linalg.solve(A, rhs)\n                except Exception:\n                    theta = None\n                if theta is not None and not np.any(np.isnan(theta)):\n                    # extract and solve for s* (similar to ALRQS mapping routine)\n                    a = float(theta[0])\n                    b = np.array(theta[1:1+k_sub], dtype=float)\n                    H = np.zeros((k_sub, k_sub), dtype=float)\n                    idx = 1 + k_sub\n                    for ii in range(k_sub):\n                        H[ii, ii] = float(theta[idx]); idx += 1\n                    for ii in range(k_sub):\n                        for jj in range(ii+1, k_sub):\n                            H[ii, jj] = float(theta[idx]); H[jj, ii] = H[ii, jj]; idx += 1\n                    # solve (H) s = -b with regularization\n                    s_star = None\n                    lam = 1e-8\n                    for attempt in range(6):\n                        Hreg = H + lam * np.eye(k_sub)\n                        try:\n                            s_star = -np.linalg.solve(Hreg, b)\n                            break\n                        except np.linalg.LinAlgError:\n                            lam *= 10.0\n                    if s_star is None:\n                        s_star = -b\n                    # clamp step length to moderate fraction of domain\n                    max_step = 0.6 * np.linalg.norm(ub - lb)\n                    if np.linalg.norm(s_star) > max_step:\n                        s_star = s_star / (np.linalg.norm(s_star) + 1e-12) * max_step\n                    x_prop = best_x + basis @ s_star\n                    x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n                    # evaluate\n                    if evals < self.budget:\n                        # skip if nearly duplicate\n                        if np.min(np.linalg.norm(X_arr - x_prop[None, :], axis=1)) > 1e-8:\n                            f_prop = float(func(x_prop)); evals += 1\n                            X.append(x_prop.copy()); F.append(float(f_prop))\n                            if f_prop < best_f:\n                                best_f = f_prop; best_x = x_prop.copy()\n                            # modestly boost nearby edges if this improved\n                            # find closest elite edge endpoints\n                            # (we won't change edge_scores strongly here)\n                # else: ignore this surrogate attempt\n\n            # occasional global injection to maintain exploration\n            if evals < self.budget and self.rng.random() < self.global_inject_prob:\n                if evals >= self.budget:\n                    break\n                xg = lb + self.rng.random(self.dim) * (ub - lb)\n                fg = float(func(xg)); evals += 1\n                X.append(xg.copy()); F.append(fg)\n                if fg < best_f:\n                    best_f = fg; best_x = xg.copy()\n\n            # decay and clip edge scores to keep them stable\n            for k in list(edge_scores.keys()):\n                edge_scores[k] = float(np.clip(edge_scores[k] * 0.995, 0.3, 6.0))\n\n            # trim archive if too large to keep computations efficient\n            max_archive = max(300, 8 * self.dim + 80)\n            if len(X) > max_archive:\n                X_arr = np.vstack(X); F_arr = np.array(F, dtype=float)\n                idx_sorted = np.argsort(F_arr)\n                keep_best = int(max(40, 4 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                # add diversity-based survivors with bias\n                probs = np.exp(-(F_arr - F_arr.min()) / (1e-8 + max(1.0, np.std(F_arr))))\n                probs = probs / probs.sum()\n                n_extra = min(len(X) - keep_best, max_archive - keep_best)\n                extra = self.rng.choice(len(X), size=n_extra, replace=False, p=probs)\n                for e in extra:\n                    keep_set.add(int(e))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                # rebuild best info\n                best_idx = int(np.argmin(F))\n                best_f = float(F[best_idx]); best_x = X[best_idx].copy()\n\n        # finalize\n        F_arr = np.array(F, dtype=float)\n        best_i = int(np.argmin(F_arr))\n        return float(F_arr[best_i]), np.array(X[best_i], dtype=float)", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["0c98b60a-b401-4b27-b5de-4c183326b288"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4223.0, "Edges": 4222.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9995264030310207, "Degree Variance": 2.029836384751613, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.697127937336814, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3223959309386024, "Depth Entropy": 2.1882336856799216, "Assortativity": 1.6244322061629925e-08, "Average Eccentricity": 19.60738811271608, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.00023679848448969926, "Average Shortest Path": 11.246946494198998, "mean_complexity": 11.714285714285714, "total_complexity": 82.0, "mean_token_count": 532.8571428571429, "total_token_count": 3730.0, "mean_parameter_count": 3.5714285714285716, "total_parameter_count": 25.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "e2cff1d2-98e9-4b7b-9f18-b799c4fd3660", "fitness": "-inf", "name": "AdaptiveLowRankQuadraticSurrogatesEdgeDensified", "description": "Adaptive Low-Rank Quadratic Surrogates with Edge-Densification \u2014 densify promising edges between elites with many virtual recombinations screened by cheap local quadratics, then evaluate only top surrogate-ranked candidates while still using subspace surrogate minima and trust-region updates.", "code": "import numpy as np\n\nclass AdaptiveLowRankQuadraticSurrogatesEdgeDensified:\n    \"\"\"\n    Adaptive Low-Rank Quadratic Surrogates with Edge-Densification (ALRQS-ED)\n\n    Main idea:\n      - Maintain an archive of evaluated points and per-point trust radii.\n      - Build many candidate points by:\n          * solving low-rank quadratic surrogates in small random subspaces (as before),\n          * densifying edges between elite points (midpoints, barycentric, extrapolations,\n            virtual nodes and perpendicular perturbations).\n      - Use cheap local quadratic predictions (in the appropriate low-dim subspace)\n        to rank a pool of candidates (large), then evaluate only a small top subset.\n      - Update trust radii by comparing predicted vs actual reduction, keep archive\n        trimmed, inject global perturbations on stagnation.\n    \"\"\"\n\n    def __init__(self, budget, dim, init_samples=None, elite_k=8,\n                 init_radius=1.0, min_radius=1e-6, max_radius=5.0,\n                 max_subdim=4, pool_max=120, eval_batch=4, edge_k=4,\n                 rng=None):\n        \"\"\"\n        Arguments:\n          budget: total function evaluations allowed (int)\n          dim: problem dimensionality\n          init_samples: number of initial random samples (default ~ min(50, budget//10 or 10))\n          elite_k: number of elites used to build dense edges\n          init_radius: initial trust radius for centers\n          min_radius, max_radius: trust radius bounds\n          max_subdim: maximum low-subspace dimension to fit (<= dim)\n          pool_max: maximum number of candidate points to generate per generation (screened)\n          eval_batch: how many top-ranked candidates to actually evaluate per generation\n          edge_k: for each elite, how many nearby elites to connect to (k-NN)\n          rng: numpy.random.Generator or None\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if init_samples is None:\n            self.init_samples = int(min(50, max(10, self.budget // 10)))\n        else:\n            self.init_samples = int(init_samples)\n        self.elite_k = int(elite_k)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_subdim = int(min(max_subdim, dim))\n        self.pool_max = int(pool_max)\n        self.eval_batch = max(1, int(eval_batch))\n        self.edge_k = int(edge_k)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _orthonormal_basis(self, k):\n        A = self.rng.normal(size=(self.dim, k))\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def _build_phi_and_map(self, S):\n        m, k = S.shape\n        cols = [np.ones((m, 1))]\n        cols.append(S)\n        quad_cols = []\n        for i in range(k):\n            quad_cols.append(0.5 * (S[:, i:i+1] ** 2))\n        for i in range(k):\n            for j in range(i+1, k):\n                quad_cols.append((S[:, i] * S[:, j])[:, None]\n                                 )\n        cols.extend(quad_cols)\n        Phi = np.hstack(cols)\n        return Phi\n\n    def _theta_to_quad(self, theta, k):\n        a = float(theta[0])\n        b = np.array(theta[1:1 + k], dtype=float)\n        H = np.zeros((k, k), dtype=float)\n        idx = 1 + k\n        for i in range(k):\n            H[i, i] = float(theta[idx]); idx += 1\n        for i in range(k):\n            for j in range(i + 1, k):\n                H[i, j] = float(theta[idx]); H[j, i] = H[i, j]; idx += 1\n        return a, b, H\n\n    def _fit_local_quad(self, center_x, basis, X_arr, F_arr, neigh_idxs, k_sub):\n        # Fit quadratic in subspace coordinates S for neighbors; return theta, success flag and S_coords\n        X_neigh = X_arr[neigh_idxs]\n        S = (X_neigh - center_x[None, :]) @ basis  # (m, k_sub)\n        Phi = self._build_phi_and_map(S)\n        y = np.array([F_arr[i] for i in neigh_idxs], dtype=float)\n        p = Phi.shape[1]\n        reg = 1e-8 * max(1.0, np.var(y))\n        try:\n            A = Phi.T @ Phi\n            A[np.diag_indices_from(A)] += reg + 1e-12\n            rhs = Phi.T @ y\n            theta = np.linalg.solve(A, rhs)\n            return theta, True, S\n        except np.linalg.LinAlgError:\n            try:\n                theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n                return theta, True, S\n            except Exception:\n                return None, False, S\n\n    def _predict_from_theta(self, theta, s):\n        # given theta fitted in subspace produce prediction at s (1D)\n        k = s.size\n        a, b, H = self._theta_to_quad(theta, k)\n        return float(a + b.dot(s) + 0.5 * float(s.T @ (H @ s)))\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0])); ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []\n        F = []\n        radii = []\n\n        # initial seeding\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n            radii.append(self.init_radius)\n\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n            radii.append(self.init_radius)\n\n        X_arr = np.vstack(X)\n        best_idx = int(np.argmin(F)); best_f = F[best_idx]; best_x = X_arr[best_idx].copy()\n        stagnation = 0\n        iter_count = 0\n\n        # helper to update X_arr when new points appended\n        def refresh_arrays():\n            return np.vstack(X), np.array(F), np.array(radii)\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            X_arr, F_arr, R_arr = refresh_arrays()\n            n_archive = X_arr.shape[0]\n\n            # determine elites\n            top_k = min(self.elite_k, n_archive)\n            idx_sorted = np.argsort(F_arr)\n            elites = idx_sorted[:top_k].tolist()\n\n            # precompute pairwise distances among elites for edge building\n            elite_coords = X_arr[elites]\n            if elite_coords.shape[0] > 1:\n                d_elite = np.linalg.norm(elite_coords[:, None, :] - elite_coords[None, :, :], axis=2)\n            else:\n                d_elite = np.array([[0.0]])\n\n            # Candidate pool (will be screened)\n            pool = []  # each entry: dict with x (candidate), center_idx, basis, s_coords (if applicable), predicted_f (float or None)\n            # 1) Surrogate minima proposals (like original) - produce some candidates\n            n_surrogate_proposals = max(1, min(6, self.eval_batch))\n            for _ in range(n_surrogate_proposals):\n                if len(pool) >= self.pool_max:\n                    break\n                # choose center probabilistically from top_k\n                if top_k >= 1:\n                    top_indices = elites\n                    top_fs = np.array([F_arr[i] for i in top_indices])\n                    w = np.exp(-(top_fs - top_fs.min()) / (1e-8 + max(1.0, np.std(top_fs))))\n                    probs = w / w.sum()\n                    center_choice = int(self.rng.choice(top_indices, p=probs))\n                else:\n                    center_choice = int(self.rng.integers(0, n_archive))\n                center_x = X_arr[center_choice].copy()\n                center_r = float(R_arr[center_choice]) if center_choice < len(R_arr) else self.init_radius\n                k_sub = int(self.rng.integers(1, min(self.max_subdim, self.dim) + 1))\n                basis = self._orthonormal_basis(k_sub)\n                # neighborhood: nearest neighbors in ambient, need at least p_min\n                dists = np.linalg.norm(X_arr - center_x[None, :], axis=1)\n                p_min = 1 + k_sub + (k_sub * (k_sub + 1)) // 2\n                order = np.argsort(dists)\n                neigh_idxs = order[:max(p_min, min(30, n_archive))]\n                # fit local quad\n                theta, ok, S = self._fit_local_quad(center_x, basis, X_arr, F_arr, neigh_idxs, k_sub)\n                if not ok:\n                    # fallback small random candidate around center\n                    step = self.rng.normal(size=self.dim)\n                    step = step / (np.linalg.norm(step) + 1e-12) * center_r * self.rng.random()\n                    x_prop = np.clip(center_x + step, lb, ub)\n                    pool.append({\"x\": x_prop, \"center\": center_choice, \"basis\": None, \"s\": None, \"pred\": None})\n                else:\n                    # compute minimizer in subspace, regularize\n                    a_hat, b_hat, H_hat = self._theta_to_quad(theta, k_sub)\n                    s_star = None; lam = 1e-8\n                    for attempt in range(6):\n                        try:\n                            H_reg = H_hat + lam * np.eye(k_sub)\n                            s_star = -np.linalg.solve(H_reg, b_hat)\n                            break\n                        except np.linalg.LinAlgError:\n                            lam = max(1e-6, lam * 10.0)\n                    if s_star is None:\n                        # use -b direction scaled to center radius\n                        if np.linalg.norm(b_hat) < 1e-12:\n                            s_dir = self.rng.normal(size=k_sub)\n                        else:\n                            s_dir = -b_hat\n                        s_star = s_dir / (np.linalg.norm(s_dir) + 1e-12) * (0.75 * center_r)\n                    x_prop = center_x + basis @ s_star\n                    disp = x_prop - center_x\n                    disp_norm = np.linalg.norm(disp)\n                    max_step = max(1e-12, 1.5 * center_r)\n                    if disp_norm > max_step:\n                        x_prop = center_x + disp * (max_step / disp_norm)\n                    x_prop = np.clip(x_prop, lb, ub)\n                    pred_val = self._predict_from_theta(theta, s_star)\n                    pool.append({\"x\": x_prop, \"center\": center_choice, \"basis\": basis, \"s\": s_star, \"pred\": pred_val})\n\n            # 2) Edge densification between elites\n            # For each elite, connect to nearest 'edge_k' elites and create several virtual nodes along edges\n            if len(elites) >= 2:\n                for i_idx, i in enumerate(elites):\n                    if len(pool) >= self.pool_max:\n                        break\n                    # find nearest elites for this elite by distances among elites\n                    if d_elite.shape[0] == 1:\n                        neighbors = []\n                    else:\n                        order = np.argsort(d_elite[i_idx])\n                        # skip self (first element)\n                        neighbors = [elites[j] for j in order[1:1 + self.edge_k] if j != i_idx]\n                    for j in neighbors:\n                        if len(pool) >= self.pool_max:\n                            break\n                        xi = X_arr[i]; xj = X_arr[j]\n                        edge_vec = xj - xi\n                        edge_len = np.linalg.norm(edge_vec)\n                        if edge_len < 1e-12:\n                            continue\n                        # create t samples along edge including slight extrapolation\n                        t_points = [0.5, 0.25, 0.75, -0.15, 1.15]\n                        # shuffle and maybe restrict to avoid too many\n                        self.rng.shuffle(t_points)\n                        for t in t_points:\n                            if len(pool) >= self.pool_max:\n                                break\n                            x_v = xi + t * edge_vec\n                            # perpendicular small perturb in random orthonormal direction to the edge\n                            # build one random orth vector\n                            v = edge_vec / (edge_len + 1e-12)\n                            # random vector, orthonormalize\n                            r = self.rng.normal(size=self.dim)\n                            r = r - v * (v.dot(r))\n                            nr = np.linalg.norm(r)\n                            if nr > 1e-12:\n                                r = r / nr\n                                # perturb scale proportional to local radius of endpoints\n                                r_scale = 0.1 * max(self.init_radius, R_arr[i], R_arr[j])\n                                x_v = x_v + r * (self.rng.random() * r_scale)\n                            x_v = np.clip(x_v, lb, ub)\n                            pool.append({\"x\": x_v, \"center\": i, \"basis\": None, \"s\": None, \"pred\": None})\n                        # also add a midpoint virtual node with small orthogonal exploration\n                        if len(pool) < self.pool_max:\n                            mid = (xi + xj) * 0.5\n                            # small orthogonal perturb with random rotation in the span of xi-xj and random dir\n                            r = self.rng.normal(size=self.dim)\n                            r = r - edge_vec * (edge_vec.dot(r) / (edge_len**2 + 1e-12))\n                            nr = np.linalg.norm(r)\n                            if nr > 1e-12:\n                                r = r / nr\n                                scale = 0.2 * max(self.init_radius, R_arr[i], R_arr[j])\n                                mid = mid + r * (self.rng.uniform(-1.0, 1.0) * scale)\n                            mid = np.clip(mid, lb, ub)\n                            pool.append({\"x\": mid, \"center\": i, \"basis\": None, \"s\": None, \"pred\": None})\n\n            # 3) Cross-edge barycentric triangles among triples of elites (sparse)\n            if len(elites) >= 3 and len(pool) < self.pool_max:\n                # sample some triples\n                n_triples = min(8, max(1, len(elites)//2))\n                for _ in range(n_triples):\n                    if len(pool) >= self.pool_max:\n                        break\n                    triple = list(self.rng.choice(elites, size=3, replace=False))\n                    pts = X_arr[triple]\n                    # random barycentric weights\n                    w = self.rng.random(3)\n                    w = w / (np.sum(w) + 1e-12)\n                    x_b = w[0]*pts[0] + w[1]*pts[1] + w[2]*pts[2]\n                    # small jitter scaled by average radius\n                    avg_r = float(np.mean([R_arr[t] for t in triple]))\n                    x_b = x_b + self.rng.normal(scale=0.08*avg_r, size=self.dim)\n                    x_b = np.clip(x_b, lb, ub)\n                    pool.append({\"x\": x_b, \"center\": triple[0], \"basis\": None, \"s\": None, \"pred\": None})\n\n            # If pool too small, add some global randoms\n            while len(pool) < max(6, self.eval_batch*2) and len(pool) < self.pool_max:\n                xg = lb + self.rng.random(self.dim) * (ub - lb)\n                pool.append({\"x\": xg, \"center\": int(self.rng.integers(0, n_archive)), \"basis\": None, \"s\": None, \"pred\": None})\n\n            # Screen pool: for each candidate, compute a cheap predicted f using a local low-rank quadratic fit\n            # Limit neighbor size and subdim adaptively\n            # Precompute a kd-like nearest neighbor ordering (simple)\n            # We'll use for each candidate its nearest archived point as center for fitting\n            X_arr, F_arr, R_arr = refresh_arrays()\n            archive_tree_order = None  # using brute force distances is OK for small archives\n            for entry in pool:\n                if entry[\"pred\"] is not None:\n                    continue\n                x_c = entry[\"x\"]\n                # find nearest archive point index\n                dists = np.linalg.norm(X_arr - x_c[None, :], axis=1)\n                nearest = int(np.argmin(dists))\n                # choose a local subspace basis: use difference vectors to some neighbors of nearest\n                # subdimension chosen relative to archive size and eval budget\n                k_sub = min(self.max_subdim, max(1, min(4, n_archive // 10 + 1)))\n                # build a basis around nearest using random directions biased to differences to neighbors\n                # collect neighbor indices\n                order = np.argsort(np.linalg.norm(X_arr - X_arr[nearest], axis=1))\n                p_min = 1 + k_sub + (k_sub * (k_sub + 1)) // 2\n                neigh_idxs = order[:max(p_min, min(30, n_archive))]\n                # create basis: combine global random basis and principal directions of local neighbors\n                local_diffs = (X_arr[neigh_idxs] - X_arr[nearest])\n                # if possible, compute top k_sub PCA directions\n                try:\n                    if local_diffs.shape[0] > k_sub:\n                        U, svals, Vt = np.linalg.svd(local_diffs, full_matrices=False)\n                        basis = Vt.T[:, :k_sub]\n                    else:\n                        basis = self._orthonormal_basis(k_sub)\n                except Exception:\n                    basis = self._orthonormal_basis(k_sub)\n                # project candidate into local subspace coordinates relative to the nearest center\n                s_c = (x_c - X_arr[nearest]) @ basis\n                # fit local quadratic using neigh_idxs\n                theta, ok, S_coords = self._fit_local_quad(X_arr[nearest], basis, X_arr, F_arr, neigh_idxs, k_sub)\n                if ok:\n                    try:\n                        pred = self._predict_from_theta(theta, s_c)\n                    except Exception:\n                        pred = float(np.mean(F_arr[neigh_idxs]))\n                else:\n                    pred = float(np.mean(F_arr[neigh_idxs]))\n                entry[\"pred\"] = pred\n                entry[\"center\"] = nearest\n                entry[\"basis\"] = basis\n                entry[\"s\"] = s_c\n\n            # sort candidate pool by predicted f ascending\n            pool_sorted = sorted(pool, key=lambda e: e[\"pred\"] if e[\"pred\"] is not None else 1e99)\n            # evaluate top few candidates but respect budget and avoid duplicates (very close points)\n            to_eval = min(self.eval_batch, max(1, int(np.ceil(0.10 * (self.budget - evals)))))  # adaptive batch\n            to_eval = min(to_eval, len(pool_sorted))\n            evaluated_count = 0\n            chosen_flags = [False] * len(pool_sorted)\n            for idx_entry, entry in enumerate(pool_sorted):\n                if evaluated_count >= to_eval or evals >= self.budget:\n                    break\n                x_try = np.array(entry[\"x\"], dtype=float)\n                # avoid evaluating if candidate is extremely near an existing archive point\n                dmin = np.min(np.linalg.norm(X_arr - x_try[None, :], axis=1))\n                if dmin < 1e-8:\n                    continue\n                # evaluate\n                f_try = float(func(x_try))\n                evals += 1\n                evaluated_count += 1\n                X.append(x_try.copy()); F.append(f_try); radii.append(self.init_radius)\n                # update arrays for immediate neighbor computations\n                X_arr, F_arr, R_arr = refresh_arrays()\n                # compare predicted reduction if available: predicted reduction referenced to entry's center\n                center_idx = entry.get(\"center\", None)\n                if center_idx is None or center_idx >= X_arr.shape[0]:\n                    center_idx = int(np.argmin(F_arr))\n                center_f = float(F_arr[center_idx])\n                pred_val = entry.get(\"pred\", None)\n                predicted_reduction = center_f - pred_val if pred_val is not None else None\n                actual_reduction = center_f - f_try\n                # update trust radii using ratio rho similar to trust-region logic\n                if predicted_reduction is None or abs(predicted_reduction) < 1e-12:\n                    rho = 1.0 if actual_reduction > 0 else 0.0\n                else:\n                    rho = actual_reduction / predicted_reduction\n                # adjust center radius\n                old_center_r = radii[center_idx]\n                if rho > 0.75:\n                    radii[center_idx] = min(self.max_radius, old_center_r * 1.5)\n                    radii[-1] = min(self.max_radius, radii[-1] * 1.2)\n                elif rho > 0.25:\n                    radii[center_idx] = min(self.max_radius, old_center_r * 1.15)\n                else:\n                    radii[center_idx] = max(self.min_radius, old_center_r * 0.5)\n                # update best\n                if f_try < best_f:\n                    best_f = f_try; best_x = x_try.copy(); stagnation = 0\n                else:\n                    stagnation += 1\n\n            # occasional short greedy interpolation between best and random good elites\n            if evals < self.budget and self.rng.random() < 0.12:\n                pick = int(self.rng.integers(0, n_archive))\n                x_probe = np.clip(0.6 * best_x + 0.4 * X_arr[pick], lb, ub)\n                fp = float(func(x_probe)); evals += 1\n                X.append(x_probe.copy()); F.append(fp); radii.append(self.init_radius*0.8)\n                if fp < best_f:\n                    best_f = fp; best_x = x_probe.copy(); stagnation = 0\n\n            # stagnation handling: densified global injection and targeted dense-edge local exploration near best\n            if stagnation >= 8 and evals < self.budget:\n                # a few global samples\n                n_global = min(6, max(2, self.dim // 3))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg)); evals += 1\n                    X.append(xg.copy()); F.append(fg); radii.append(self.init_radius)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagnation = 0\n                # targeted dense edge sweep around best: connect best with some elites and evaluate a few virtual midpoints\n                if evals < self.budget:\n                    # find k nearest archived points to best\n                    X_arr, F_arr, R_arr = refresh_arrays()\n                    dists_to_best = np.linalg.norm(X_arr - best_x[None, :], axis=1)\n                    order = np.argsort(dists_to_best)\n                    neighbors = order[1:1 + min(8, n_archive-1)]\n                    for nb in neighbors[:min(4, len(neighbors))]:\n                        if evals >= self.budget:\n                            break\n                        xi = X_arr[nb]; xj = best_x\n                        for t in [0.3, 0.6, 0.5]:\n                            if evals >= self.budget:\n                                break\n                            xv = np.clip(xi * (1 - t) + xj * t + self.rng.normal(scale=0.03*self.init_radius, size=self.dim), lb, ub)\n                            fv = float(func(xv)); evals += 1\n                            X.append(xv.copy()); F.append(fv); radii.append(self.init_radius)\n                            if fv < best_f:\n                                best_f = fv; best_x = xv.copy(); stagnation = 0\n                # reset radii mild randomization to encourage new fits\n                radii = [max(self.min_radius, min(self.max_radius, r * self.rng.uniform(0.6, 1.2))) for r in radii]\n                stagnation = 0\n\n            # trim archive moderately to keep computation light\n            max_archive = max(200, 8 * self.dim + 50)\n            if len(X) > max_archive:\n                X_arr = np.vstack(X); F_arr = np.array(F)\n                idx_sorted = np.argsort(F_arr)\n                keep_best = int(max(30, 4 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                probs = np.exp(-(F_arr - np.min(F_arr)) / (1e-8 + max(1.0, np.std(F_arr))))\n                probs /= probs.sum()\n                extra = self.rng.choice(len(X), size=min(len(X) - keep_best, max_archive - keep_best), replace=False, p=probs)\n                for e in extra:\n                    keep_set.add(int(e))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                radii = [radii[i] for i in keep_list]\n\n            # update local view\n            X_arr = np.vstack(X)\n            best_idx = int(np.argmin(F)); best_f = float(F[best_idx]); best_x = X_arr[best_idx].copy()\n\n        # finalize\n        X_arr = np.vstack(X)\n        best_i = int(np.argmin(F))\n        return float(F[best_i]), np.array(X[best_i], dtype=float)", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["0c98b60a-b401-4b27-b5de-4c183326b288"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4897.0, "Edges": 4896.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.999591586685726, "Degree Variance": 2.151112759479962, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.692685102586976, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3299198321143475, "Depth Entropy": 2.199204719239363, "Assortativity": 3.621214561825998e-09, "Average Eccentricity": 19.93199918317337, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.00020420665713702266, "Average Shortest Path": 11.384436216117377, "mean_complexity": 11.555555555555555, "total_complexity": 104.0, "mean_token_count": 480.77777777777777, "total_token_count": 4327.0, "mean_parameter_count": 3.7777777777777777, "total_parameter_count": 34.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "3b25d21b-4a7d-4055-a90d-f4652fad8843", "fitness": "-inf", "name": "EdgeDensePatchworkMesh", "description": "Edge-Dense Patchwork Mesh (EDPM) \u2014 aggressively densify elite connections by creating many virtual edge/triangle mesh nodes, rank them with cheap local linear surrogates and residual-based uncertainty, then evaluate only the top candidates to concentrate edge density while adaptively refining and escaping with L\u00e9vy-like jumps.", "code": "import numpy as np\n\nclass EdgeDensePatchworkMesh:\n    \"\"\"\n    Edge-Dense Patchwork Mesh (EDPM)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Optional args:\n      pop_size: number of maintained real-evaluated solutions\n      elite_frac: fraction considered elite for edge densification\n      k_edges: neighbors per elite to create edges\n      samples_per_edge: virtual nodes created per edge (scored cheaply)\n      triangle_frac: fraction of triangles to consider per elite\n      levy_prob: probability of heavy tail jump on replacements\n      seed: RNG seed\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.25,\n                 k_edges=None, samples_per_edge=8, triangle_frac=0.15,\n                 levy_prob=0.06, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.samples_per_edge = int(samples_per_edge)\n        self.triangle_frac = float(triangle_frac)\n        self.levy_prob = float(levy_prob)\n\n        if pop_size is None:\n            # compact population scaled by dimension and budget\n            self.pop_size = int(max(12, min(120, np.clip(np.sqrt(self.budget) * 1.2, 12, 120))))\n            self.pop_size = max(self.pop_size, dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        if k_edges is None:\n            self.k_edges = max(2, min(self.pop_size - 1, int(np.clip(self.dim, 2, 14))))\n        else:\n            self.k_edges = int(k_edges)\n\n        # adaptation multipliers\n        self.success_inc = 1.18\n        self.failure_dec = 0.82\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        sigma_base = 0.08 * range_norm\n        alpha_base = 0.35 * range_norm\n\n        evals = 0\n\n        # initialize population uniformly (but not exceeding budget)\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # per-individual step sizes\n        sigma = np.full(self.pop_size, sigma_base)\n        alpha = np.full(self.pop_size, alpha_base)\n        success_counts = np.zeros(self.pop_size, dtype=int)\n\n        # best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        no_improve = 0\n        max_no_improve = max(25, self.pop_size * 3)\n\n        # helpers\n        def build_elite_edges():\n            # compute pairwise squared distances\n            X = pop\n            dif = X[:, None, :] - X[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            edges = set()\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count]\n            # fully connect elites among themselves for densification\n            for i in range(len(elites)):\n                for j in range(i + 1, len(elites)):\n                    edges.add((int(elites[i]), int(elites[j])))\n            # for each elite, connect to k nearest neighbors (excluding itself)\n            for i in elites:\n                neigh = np.argsort(dist2[i])\n                added = 0\n                for j in neigh:\n                    if j == i: continue\n                    edges.add((int(i), int(j)))\n                    added += 1\n                    if added >= self.k_edges:\n                        break\n            # add some random edges among whole pop\n            extra = max(0, self.pop_size // 4)\n            for _ in range(extra):\n                a = int(rng.integers(0, self.pop_size))\n                b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges.add((min(a, b), max(a, b)))\n            return list(edges)\n\n        def nearest_indices(x, k= max(6, dim + 2)):\n            # return indices of k nearest evaluated points in pop\n            dif = pop - x\n            d2 = np.sum(dif * dif, axis=1)\n            k = min(len(d2), k)\n            return np.argpartition(d2, k - 1)[:k]\n\n        def local_linear_surrogate(x_cand, neighbors_idx):\n            # fit f ~ a + b^T x on neighbors, return predicted f and residual std\n            Xn = pop[neighbors_idx]\n            yn = pop_f[neighbors_idx]\n            # design: add constant\n            A = np.concatenate([np.ones((Xn.shape[0], 1)), Xn], axis=1)\n            # regularize small ridge to stabilize\n            lam = 1e-8 + 1e-6 * np.mean(np.abs(yn) + 1.0)\n            try:\n                coeffs, *_ = np.linalg.lstsq(A, yn, rcond=None)\n                pred = float(np.dot(np.concatenate(([1.0], x_cand)), coeffs))\n                residuals = yn - (A @ coeffs)\n                if residuals.size > 1:\n                    res_std = float(np.sqrt(np.mean(residuals**2)))\n                else:\n                    res_std = float(np.abs(residuals).sum() + 1e-8)\n            except Exception:\n                pred = float(np.mean(yn))\n                res_std = float(np.std(yn) + 1e-8)\n            return pred, res_std\n\n        # main loop\n        while evals < self.budget:\n            rem = self.budget - evals\n            improved_in_cycle = False\n\n            # build many virtual candidates by densifying edges and triangles\n            edges = build_elite_edges()\n\n            candidates = []  # tuples (score, x_candidate, meta)\n            # For edges: generate several evenly/randomly spaced virtual nodes + small orth perturbations\n            for (i, j) in edges:\n                if len(candidates) > 3000:\n                    # cap candidate generation to avoid huge overhead (scored cheaply)\n                    break\n                xi, xj = pop[i], pop[j]\n                edge_vec = xj - xi\n                edge_len = np.linalg.norm(edge_vec) + 1e-12\n                # set t points: include interior and slight extrapolations\n                ts = rng.uniform(-0.25, 1.25, size=self.samples_per_edge)\n                for t in ts:\n                    xv = xi + t * edge_vec\n                    # add small orthogonal perturbation scaled by local sigmas and orthonormal basis\n                    local_sigma = (sigma[i] + sigma[j]) * 0.5\n                    if dim > 1 and rng.random() < 0.85:\n                        # create a small orthonormal perturbation orthogonal to edge_vec\n                        v = rng.standard_normal(dim)\n                        # remove projection onto edge_vec\n                        v = v - (np.dot(v, edge_vec) / (edge_len**2 + 1e-12)) * edge_vec\n                        v_norm = np.linalg.norm(v) + 1e-12\n                        v = v / v_norm\n                        orth_scale = rng.normal(0, 0.25) * local_sigma\n                        xv = xv + orth_scale * v\n                    # clamp\n                    xv = np.clip(xv, lb, ub)\n                    # score candidate cheaply using local surrogate from nearest evaluated points\n                    nbrs = nearest_indices(xv, k=max(8, dim + 3))\n                    pred, uncert = local_linear_surrogate(xv, nbrs)\n                    # scoring: predicted value minus uncertainty bonus to favor low-pred & low-uncert\n                    score = pred - 0.8 * uncert\n                    candidates.append((score, xv, (\"edge\", i, j)))\n            # Triangles: select triples among kNN of elites to produce barycentric candidates\n            elite_count = max(3, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count]\n            for e in elites:\n                # neighbors of e\n                dif = pop - pop[e]\n                d2 = np.sum(dif * dif, axis=1)\n                k = min(self.pop_size - 1, max(self.k_edges, 6))\n                neigh = np.argsort(d2)[1:k+1]  # exclude self\n                # sample some triangle combinations\n                tri_samples = max(1, int(np.ceil(self.triangle_frac * len(neigh))))\n                for _ in range(tri_samples):\n                    a, b = rng.choice(neigh, size=2, replace=False)\n                    pa, pb, pc = pop[a], pop[b], pop[e]\n                    # barycentric coords biased toward vertices with better f\n                    weights = rng.random(3)\n                    weights = weights / weights.sum()\n                    xv = weights[0] * pa + weights[1] * pb + weights[2] * pc\n                    # small gaussian noise along principal directions of the triangle neighborhood\n                    nbrs = nearest_indices(xv, k=max(8, dim + 3))\n                    M = pop[nbrs] - np.mean(pop[nbrs], axis=0)\n                    if M.shape[0] > 2:\n                        try:\n                            U, S, Vt = np.linalg.svd(M, full_matrices=False)\n                            # perturb more along leading PC, less on others\n                            pcs = Vt[:min(3, Vt.shape[0])]\n                            pert = np.zeros(dim)\n                            for idx_pc, pc in enumerate(pcs):\n                                pert += (rng.normal(0, 0.22 * (0.7 ** idx_pc)) * S[idx_pc]) * pc\n                            xv = xv + 0.5 * (sigma.mean() / (1e-8 + np.linalg.norm(range_vec))) * pert\n                        except Exception:\n                            xv = xv + 0.01 * sigma.mean() * rng.standard_normal(dim)\n                    else:\n                        xv = xv + 0.01 * sigma.mean() * rng.standard_normal(dim)\n                    xv = np.clip(xv, lb, ub)\n                    pred, uncert = local_linear_surrogate(xv, nbrs)\n                    score = pred - 0.9 * uncert\n                    candidates.append((score, xv, (\"tri\", int(e), int(a), int(b))))\n\n            # also add a small pool of random global candidates to keep exploration\n            for _ in range(min(20, max(2, dim * 2))):\n                xv = rng.uniform(lb, ub)\n                nbrs = nearest_indices(xv, k=max(6, dim + 2))\n                pred, uncert = local_linear_surrogate(xv, nbrs)\n                score = pred - 1.0 * uncert\n                candidates.append((score, xv, (\"rand\",)))\n\n            # sort candidates by score (lower = better)\n            if not candidates:\n                break\n            candidates.sort(key=lambda z: z[0])\n\n            # decide how many top candidates to evaluate this cycle\n            # be conservative: evaluate up to batch = min(remaining_budget, pop_size//2 + dim)\n            batch = min(rem, max(1, self.pop_size // 2 + dim))\n            # but also don't evaluate more candidates than available\n            batch = min(batch, len(candidates))\n\n            # evaluate top-ranked candidates, with budget protection\n            evaluated = 0\n            for idx_c in range(batch):\n                if evals >= self.budget:\n                    break\n                _, x_cand, meta = candidates[idx_c]\n                # small refinement: combine candidate with best using a convex mix sometimes\n                if rng.random() < 0.12:\n                    t = rng.uniform(0.2, 0.6)\n                    x_cand = np.clip(t * x_best + (1 - t) * x_cand + 0.02 * sigma.mean() * rng.standard_normal(dim), lb, ub)\n                # slight chance to apply Levy jump before evaluation to test far-off but promising virtual nodes\n                if rng.random() < self.levy_prob * 0.4:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    x_cand = np.clip(x_cand + 0.18 * range_norm * cauchy, lb, ub)\n                f_cand = float(func(x_cand))\n                evals += 1\n                evaluated += 1\n\n                # replacement strategy:\n                # - if candidate beats worst, place into worst slot\n                # - if candidate beats one endpoint (if edge), replace that endpoint\n                # - otherwise maybe replace a random high-cost individual with small probability\n                idx_worst = int(np.argmax(pop_f))\n                if f_cand < pop_f[idx_worst]:\n                    pop[idx_worst] = x_cand\n                    pop_f[idx_worst] = f_cand\n                    sigma[idx_worst] = sigma_base\n                    alpha[idx_worst] = alpha_base\n                    success_counts[idx_worst] = 0\n                    if f_cand < f_best:\n                        f_best = f_cand; x_best = x_cand.copy(); improved_in_cycle = True\n                    continue\n\n                # try endpoint replacement if meta indicates edge\n                replaced = False\n                if meta[0] == \"edge\":\n                    _, a_i, b_j = meta\n                    if f_cand < pop_f[a_i]:\n                        pop[a_i] = x_cand; pop_f[a_i] = f_cand\n                        sigma[a_i] *= self.success_inc\n                        success_counts[a_i] += 1\n                        replaced = True\n                    elif f_cand < pop_f[b_j]:\n                        pop[b_j] = x_cand; pop_f[b_j] = f_cand\n                        sigma[b_j] *= self.success_inc\n                        success_counts[b_j] += 1\n                        replaced = True\n                elif meta[0] == \"tri\":\n                    # attempt replace worst among the triangle vertices\n                    _, e, a, b = meta\n                    worst_local = int(np.argmax(pop_f[[e, a, b]]))\n                    local_idx = [e, a, b][worst_local]\n                    if f_cand < pop_f[local_idx]:\n                        pop[local_idx] = x_cand; pop_f[local_idx] = f_cand\n                        sigma[local_idx] *= self.success_inc\n                        success_counts[local_idx] += 1\n                        replaced = True\n\n                if replaced:\n                    if f_cand < f_best:\n                        f_best = f_cand; x_best = x_cand.copy(); improved_in_cycle = True\n                    continue\n\n                # otherwise, small probability to replace a moderately bad individual to maintain exploration\n                if rng.random() < 0.012:\n                    # pick one of upper-half worst indices (but not best)\n                    sorted_idx = np.argsort(pop_f)\n                    pick_pool = sorted_idx[-max(1, self.pop_size // 4):]\n                    pick = int(rng.choice(pick_pool))\n                    if f_cand < pop_f[pick]:\n                        pop[pick] = x_cand; pop_f[pick] = f_cand\n                        sigma[pick] = sigma_base\n                        if f_cand < f_best:\n                            f_best = f_cand; x_best = x_cand.copy(); improved_in_cycle = True\n                        continue\n\n                # otherwise, candidate discarded; penalize local sigma of its nearest evaluated neighbor pair to shrink\n                nbrs = nearest_indices(x_cand, k=2)\n                for n in nbrs:\n                    sigma[n] *= self.failure_dec\n\n            # after batch, apply local spiral-relocation moves for each individual (small budget cost)\n            # allocate a small fraction of remaining budget to population-directed moves\n            rem = self.budget - evals\n            if rem <= 0:\n                break\n            # plan to do up to pop_moves = min(pop_size, rem)\n            pop_moves = min(self.pop_size, rem)\n            # iterate random subset of indices to apply directed spiral moves\n            idxs = rng.choice(self.pop_size, size=pop_moves, replace=False)\n            for i in idxs:\n                if evals >= self.budget:\n                    break\n                xi = pop[i].copy()\n                fi = pop_f[i]\n                d = x_best - xi\n                nd = np.linalg.norm(d) + 1e-12\n                dir_vec = d / nd\n                # small randomized spiral rotation\n                if dim > 1 and rng.random() < 0.6:\n                    a, b = rng.choice(dim, size=2, replace=False)\n                    theta = rng.uniform(-np.pi/3, np.pi/3)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    da, db = d[a], d[b]\n                    ra = ca * da - sb * db\n                    rb = sb * da + ca * db\n                    d_rot = d.copy()\n                    d_rot[a], d_rot[b] = ra, rb\n                    dir_vec = d_rot / (np.linalg.norm(d_rot) + 1e-12)\n                mov_scale = alpha[i] * (0.5 + rng.random() * 0.8)\n                move = mov_scale * dir_vec + sigma[i] * rng.standard_normal(dim)\n                # occasional tiny levy\n                if rng.random() < self.levy_prob * 0.5:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    move += 0.12 * range_norm * cauchy\n                x_new = np.clip(xi + move, lb, ub)\n                f_new = float(func(x_new))\n                evals += 1\n                # greedy acceptance with small uphill chance\n                if f_new < fi or rng.random() < 0.006:\n                    pop[i] = x_new; pop_f[i] = f_new\n                    sigma[i] *= self.success_inc\n                    alpha[i] *= 1.01\n                    success_counts[i] += 1\n                    if f_new < f_best:\n                        f_best = f_new; x_best = x_new.copy(); improved_in_cycle = True\n                else:\n                    sigma[i] *= self.failure_dec\n                    alpha[i] *= 0.988\n\n            # maintenance: clamp sigma/alpha and count improvements\n            sigma = np.clip(sigma, 1e-9 * range_norm, 2.5 * range_norm)\n            alpha = np.clip(alpha, 1e-9 * range_norm, 1.8 * range_norm)\n\n            if improved_in_cycle:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # stagnation handling: reseed a portion around best + a few global randoms\n            if no_improve >= max_no_improve and evals < self.budget:\n                no_improve = 0\n                n_reset = max(1, self.pop_size // 2)\n                for _ in range(n_reset):\n                    if evals >= self.budget:\n                        break\n                    idx = int(rng.integers(0, self.pop_size))\n                    if rng.random() < 0.8:\n                        # near-best reseed\n                        newp = x_best + 0.06 * range_vec * rng.standard_normal(dim)\n                    else:\n                        # global\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    pop[idx] = newp; pop_f[idx] = f_new\n                    sigma[idx] = sigma_base; alpha[idx] = alpha_base\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # occasional injection of a strong Levy jump from best to find new basin\n            if rng.random() < 0.03 and evals < self.budget:\n                u = rng.random(dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                cand = np.clip(x_best + 0.8 * range_norm * cauchy, lb, ub)\n                f_cand = float(func(cand))\n                evals += 1\n                if f_cand < pop_f.max():\n                    idx_w = int(np.argmax(pop_f))\n                    pop[idx_w] = cand; pop_f[idx_w] = f_cand\n                    sigma[idx_w] = sigma_base\n                    if f_cand < f_best:\n                        f_best = f_cand; x_best = cand.copy()\n\n            # if very low remaining budget, break to local refinement stage\n            if self.budget - evals < max(8, dim * 2):\n                break\n\n        # final local mesh-adaptive coordinate search around best\n        mesh = 0.12 * range_norm\n        mesh_min = 1e-6 * range_norm\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                # negative\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # store result\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["f5713cbd-1130-4e8d-abdc-3c3ca8966b71"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4219.0, "Edges": 4218.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.9995259540175396, "Degree Variance": 2.042189867719373, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.538621047174702, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3376393096385184, "Depth Entropy": 2.0872609036473717, "Assortativity": 1.1467210528253906e-08, "Average Eccentricity": 20.593031524057835, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.00023702299123014932, "Average Shortest Path": 10.91435310761417, "mean_complexity": 14.0, "total_complexity": 84.0, "mean_token_count": 598.5, "total_token_count": 3591.0, "mean_parameter_count": 3.1666666666666665, "total_parameter_count": 19.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "01c6c4b9-24ff-44d4-9c47-036e635e9784", "fitness": 0.13002178861423586, "name": "UltraDenseEdgeMesh", "description": "Ultra-Dense Edge Mesh \u2014 aggressively generate a very high-density virtual mesh of edge/triangle barycenters among elites, cheaply score the mesh with local linear surrogates, and only evaluate the most promising nodes to densify the edge graph and expand along productive mesh corridors (surrogate-ranked dense-mesh sampling + adaptive local trust).", "code": "import numpy as np\n\nclass UltraDenseEdgeMesh:\n    \"\"\"\n    Ultra-Dense Edge Mesh Sampler (UDEM)\n    - budget: maximum number of function evaluations\n    - dim: dimensionality\n    Optional args:\n      pop_size: initial population size (clipped to budget)\n      elite_frac: fraction of population treated as elites (dense mesh core)\n      mesh_density: number of virtual points generated per elite-edge\n      tri_density: number of triangle barycenters per triple\n      candidate_pool: maximum number of virtual candidates to generate before surrogate filtering\n      proposals_per_cycle: how many actual evaluations to perform per main cycle (subject to budget)\n      levy_prob: probability of heavy-tailed global jump evaluation\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.18,\n                 mesh_density=8, tri_density=5, candidate_pool=1200,\n                 proposals_per_cycle=40, levy_prob=0.05, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.mesh_density = int(mesh_density)\n        self.tri_density = int(tri_density)\n        self.candidate_pool = int(candidate_pool)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.levy_prob = float(levy_prob)\n\n        # sensible default pop_size\n        if pop_size is None:\n            self.pop_size = int(min(max(12, int(np.sqrt(self.budget) * 1.5)), self.budget))\n            self.pop_size = max(self.pop_size, self.dim + 4)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation state\n        self.edge_weights = {}  # map (i,j) -> weight\n        self.global_decay = 0.996\n        self.edge_reward = 1.3\n        self.sigma0 = 0.08  # initial relative sigma (fraction of range)\n        # outputs\n        self.x_opt = None\n        self.f_opt = None\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_norm = np.linalg.norm(ub - lb)\n        sigma_scale = max(1e-12, self.sigma0 * range_norm)\n\n        # ensure pop_size <= budget\n        self.pop_size = int(min(self.pop_size, max(2, self.budget)))\n        pop_size = self.pop_size\n\n        # initialize population uniformly and evaluate\n        pop = rng.uniform(lb, ub, size=(pop_size, dim))\n        pop_f = np.full(pop_size, np.inf)\n        evals = 0\n        for i in range(pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # per-individual sigma, local covariance, momentum\n        sigma = np.full(pop_size, sigma_scale)\n        momentum = np.zeros((pop_size, dim), dtype=float)\n\n        # track best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        # helper edge weight functions\n        def ekey(a, b):\n            a, b = int(min(a, b)), int(max(a, b))\n            return (a, b)\n\n        def get_edge_weight(a, b):\n            return self.edge_weights.get(ekey(a, b), 1.0)\n\n        def inc_edge_weight(a, b, factor):\n            k = ekey(a, b)\n            self.edge_weights[k] = self.edge_weights.get(k, 1.0) * factor\n\n        # main loop\n        cycle = 0\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n\n            # decay edge weights slowly\n            if self.edge_weights:\n                for k in list(self.edge_weights.keys()):\n                    self.edge_weights[k] *= self.global_decay\n\n            # choose elites\n            elite_count = max(2, int(np.ceil(self.elite_frac * pop_size)))\n            elite_idx = np.argsort(pop_f)[:elite_count]\n            nonelite_idx = [i for i in range(pop_size) if i not in elite_idx]\n\n            # Build candidate virtual mesh (no function evals)\n            # We'll produce edge virtual points between many pairs among elites and some top non-elites\n            candidates = []\n            candidate_meta = []  # store (type, parents tuple)\n            # select nodes to densely connect: elites U top-k nonelites\n            topk = max(elite_count, min(pop_size, elite_count * 2))\n            connector = np.argsort(pop_f)[:topk]\n\n            # produce dense edges: combine each elite with many other connectors\n            # limit total candidates to candidate_pool\n            pool_limit = max(200, self.candidate_pool)\n            # plan counts\n            edges_per_elite = max(1, pool_limit // max(1, elite_count))\n            # create edges\n            for i in elite_idx:\n                partners = rng.choice(connector, size=min(len(connector), edges_per_elite),\n                                      replace=False)\n                for j in partners:\n                    if i == j: continue\n                    # produce mesh_density points along edge with slight perpendicular jitter\n                    Xa = pop[i]; Xb = pop[j]\n                    for k in range(self.mesh_density):\n                        t = (k + 1) / (self.mesh_density + 1.0)\n                        # bias t slightly toward better endpoint\n                        if pop_f[i] < pop_f[j]:\n                            t = t * 0.8\n                        else:\n                            t = 0.2 + 0.8 * t\n                        base = (1 - t) * Xa + t * Xb\n                        # orthogonal jitter: sample small gaussian in full space but scale down and project partially off edge\n                        dir_edge = Xb - Xa\n                        norm_dir = np.linalg.norm(dir_edge) + 1e-12\n                        if norm_dir > 1e-12:\n                            unit = dir_edge / norm_dir\n                            rand = rng.standard_normal(dim)\n                            # remove component along edge to make orth orthogonal-ish\n                            rand = rand - np.dot(rand, unit) * unit\n                        else:\n                            rand = rng.standard_normal(dim)\n                        jitter = 0.06 * (sigma[i] + sigma[j]) * rand\n                        cand = base + jitter\n                        candidates.append(np.clip(cand, lb, ub))\n                        candidate_meta.append(('edge', (int(i), int(j))))\n                        if len(candidates) >= pool_limit:\n                            break\n                    if len(candidates) >= pool_limit:\n                        break\n                if len(candidates) >= pool_limit:\n                    break\n\n            # triangle barycenters among random triples of top nodes\n            tri_samples = min(self.tri_density * max(1, elite_count), pool_limit - len(candidates))\n            if tri_samples > 0:\n                triples_pool = np.unique(rng.choice(connector, size=tri_samples * 3, replace=True))\n                triples_pool = triples_pool.tolist()\n                for _ in range(tri_samples):\n                    if len(candidates) >= pool_limit: break\n                    trip = rng.choice(connector, size=3, replace=False)\n                    Xa, Xb, Xc = pop[trip[0]], pop[trip[1]], pop[trip[2]]\n                    weights = rng.dirichlet(np.ones(3) * 0.9)\n                    base = weights[0] * Xa + weights[1] * Xb + weights[2] * Xc\n                    # small extrapolation chance\n                    if rng.random() < 0.18:\n                        expo = rng.uniform(-0.5, 1.0)\n                        centroid = (Xa + Xb + Xc) / 3.0\n                        base = centroid + expo * (base - centroid)\n                    jitter = 0.05 * np.mean(sigma[list(trip)]) * rng.standard_normal(dim)\n                    cand = np.clip(base + jitter, lb, ub)\n                    candidates.append(cand)\n                    candidate_meta.append(('tri', tuple(map(int, trip))))\n            # add some random local sampling around best to fill pool\n            add_local = min(pool_limit - len(candidates), 150)\n            for _ in range(add_local):\n                cand = x_best + 0.08 * range_norm * rng.standard_normal(dim)\n                candidates.append(np.clip(cand, lb, ub))\n                candidate_meta.append(('local_best', (idx_best,)))\n\n            # If too few candidates, add random global ones\n            while len(candidates) < min(200, pool_limit):\n                candidates.append(rng.uniform(lb, ub))\n                candidate_meta.append(('rand', ()))\n\n            candidates = np.asarray(candidates, dtype=float)\n            ncand = len(candidates)\n\n            # Build cheap local linear surrogates for elites (or for small clusters)\n            # For each elite, build linear model from its nearest neighbors (in evaluated pop)\n            # We'll predict candidate f by nearest-elites model and apply a novelty penalty\n            Xpop = pop\n            Ypop = pop_f\n            # precompute nearest neighbors for elites\n            # guard: if insufficient unique data points for regression, fallback to using mean\n            surrogate_preds = np.full(ncand, np.inf)\n            # compute pop_f statistics small for scaling\n            f_std = max(1e-6, np.std(Ypop))\n            for e in elite_idx:\n                # find k nearest evaluated points around e\n                dif = Xpop - pop[e]\n                dist2 = np.sum(dif * dif, axis=1)\n                order = np.argsort(dist2)\n                k = min(max(4, int(1.5 * dim + 1)), pop_size)\n                neighbors = order[:k]\n                Xloc = Xpop[neighbors]\n                Yloc = Ypop[neighbors]\n                # build linear model y = w0 + W x (ridge)\n                Xmat = np.hstack([np.ones((Xloc.shape[0], 1)), Xloc])\n                reg = 1e-6 * (np.linalg.norm(Yloc) + 1.0)\n                try:\n                    # solve ridge: (X^T X + reg*I)^{-1} X^T y\n                    A = Xmat.T.dot(Xmat)\n                    A[np.diag_indices_from(A)] += reg\n                    w = np.linalg.solve(A, Xmat.T.dot(Yloc))\n                    # predict for candidates that are near this elite (quick filter)\n                    # compute distance to elite to restrict predictions to reasonably close cand\n                    cand_d2 = np.sum((candidates - pop[e])**2, axis=1)\n                    close_mask = cand_d2 <= ( (2.5 * np.mean(sigma[neighbors]) + 1e-12) * range_norm )**2\n                    if not np.any(close_mask):\n                        # if none close, broaden mask for a small proportion\n                        idxs = np.argsort(cand_d2)[:max(1, ncand//10)]\n                        close_mask[:] = False\n                        close_mask[idxs] = True\n                    Xc = np.hstack([np.ones((ncand, 1)), candidates])\n                    preds = Xc.dot(w)\n                    # combine predictions: keep minimum predicted value per candidate from any surrogate (conservative)\n                    # apply mild novelty penalty for distance & for being far from generating parents\n                    dist_pen = np.exp(-0.5 * (cand_d2 / ( (0.8 * range_norm)**2 + 1e-12 )))\n                    # we only accept preds from this surrogate where close_mask true; elsewhere ignore\n                    preds_masked = np.where(close_mask, preds, np.inf)\n                    # Merge: keep smaller\n                    surrogate_preds = np.minimum(surrogate_preds, preds_masked)\n                except np.linalg.LinAlgError:\n                    # skip this surrogate if ill-conditioned\n                    continue\n\n            # Fallback for candidates that have inf prediction: use simple distance-to-best heuristic\n            inf_mask = ~np.isfinite(surrogate_preds)\n            if np.any(inf_mask):\n                # predict by distance to best (closer is promising) + global mean\n                cand_d2 = np.sum((candidates[inf_mask] - x_best)**2, axis=1)\n                surrogate_preds[inf_mask] = f_best + 0.5 * (cand_d2 / (range_norm**2 + 1e-12)) * (1.0 + f_std)\n\n            # Add small random jitter to surrogate predictions to diversify rank\n            surrogate_preds += 1e-6 * rng.standard_normal(ncand)\n\n            # pick top-k predicted candidates to actually evaluate (bounded by remaining and proposals_per_cycle)\n            to_eval = min(remaining, max(2, min(self.proposals_per_cycle, int(max(1, remaining * 0.12)))))\n            to_eval = min(to_eval, ncand)\n            # choose indices of best predicted candidates\n            best_pred_idx = np.argsort(surrogate_preds)[:to_eval]\n\n            # Occasionally include a heavy-tailed jump candidate\n            extra_levy_idx = None\n            if rng.random() < self.levy_prob and evals + 1 <= self.budget:\n                # create cauchy sample\n                u = rng.random(dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                lev = x_best + 0.18 * range_norm * cauchy\n                lev = np.clip(lev, lb, ub)\n                # evaluate immediately (use one of the to_eval slots if any)\n                f_lev = float(func(lev))\n                evals += 1\n                # incorporate\n                if f_lev < f_best:\n                    f_best = f_lev; x_best = lev.copy()\n                # try to replace global worst\n                idx_w = int(np.argmax(pop_f))\n                if f_lev < pop_f[idx_w]:\n                    pop[idx_w] = lev; pop_f[idx_w] = f_lev\n                    sigma[idx_w] = sigma_scale\n                # reward no specific edges (global jump), continue\n                # reduce slots to evaluate from candidate list by 1 if we used one budget\n                if to_eval > 0:\n                    to_eval = max(0, to_eval - 1)\n\n            # Evaluate selected candidate list (in order of predicted quality)\n            for ii in range(min(len(best_pred_idx), to_eval)):\n                if evals >= self.budget: break\n                ci = int(best_pred_idx[ii])\n                x_c = candidates[ci]\n                f_c = float(func(x_c))\n                evals += 1\n\n                # Replacement policy:\n                # find k nearest population members and attempt to replace the worst among them if improved\n                d2 = np.sum((pop - x_c)**2, axis=1)\n                k_near = min(pop_size, max(2, dim + 2))\n                near_idx = np.argsort(d2)[:k_near]\n                # find worst among nearest\n                worst_near = near_idx[np.argmax(pop_f[near_idx])]\n                replaced = False\n                if f_c < pop_f[worst_near]:\n                    # replace worst_near\n                    old = pop[worst_near].copy()\n                    pop[worst_near] = x_c.copy()\n                    pop_f[worst_near] = f_c\n                    # adapt sigma and momentum\n                    sigma[worst_near] = min(2.0 * range_norm, max(1e-12, sigma[worst_near] * 1.2))\n                    momentum[worst_near] = 0.6 * momentum[worst_near] + 0.4 * (x_c - old)\n                    replaced = True\n                    # reward edges between candidate's generating parents (if any)\n                    meta = candidate_meta[ci]\n                    if meta[0] == 'edge':\n                        a, b = meta[1]\n                        inc_edge_weight(a, b, self.edge_reward)\n                    elif meta[0] == 'tri':\n                        a,b,c = meta[1]\n                        inc_edge_weight(a,b, self.edge_reward)\n                        inc_edge_weight(b,c, self.edge_reward)\n                        inc_edge_weight(a,c, self.edge_reward)\n\n                else:\n                    # maybe replace global worst stochastically if candidate is decent\n                    idx_w = int(np.argmax(pop_f))\n                    if f_c < pop_f[idx_w] and rng.random() < 0.6:\n                        old = pop[idx_w].copy()\n                        pop[idx_w] = x_c.copy()\n                        pop_f[idx_w] = f_c\n                        sigma[idx_w] = sigma_scale\n                        momentum[idx_w] = 0.3 * (x_c - old)\n                        replaced = True\n\n                # update best\n                if f_c < f_best:\n                    f_best = f_c; x_best = x_c.copy()\n\n                # if not replaced, nudge nearest sigmas down slightly\n                if not replaced:\n                    sigma[near_idx] *= 0.94\n\n                # update local estimates: small local covariance increase around success\n                if replaced:\n                    # find neighbors of replaced and increase their sigma slightly to encourage local exploration\n                    nb = np.argsort(np.sum((pop - pop[worst_near])**2, axis=1))[:max(3, int(0.15 * pop_size))]\n                    for j in nb:\n                        sigma[j] = min(2.0 * range_norm, sigma[j] * 1.08)\n\n            # occasional small injection of promising surrogate-predicted candidates (diversify)\n            if rng.random() < 0.07 and evals < self.budget:\n                # evaluate a randomly chosen high-ranked surrogate candidate not yet evaluated\n                pool_candidates = list(set(range(ncand)) - set(best_pred_idx[:to_eval]))\n                if pool_candidates:\n                    ci = int(rng.choice(pool_candidates))\n                    x_c = candidates[ci]\n                    f_c = float(func(x_c))\n                    evals += 1\n                    # replace global worst if better\n                    idx_w = int(np.argmax(pop_f))\n                    if f_c < pop_f[idx_w]:\n                        pop[idx_w] = x_c.copy()\n                        pop_f[idx_w] = f_c\n                        sigma[idx_w] = sigma_scale\n                        momentum[idx_w] *= 0.4\n                        if f_c < f_best:\n                            f_best = f_c; x_best = x_c.copy()\n\n            # stagnation handling: if no improvement for a while, reseed part of population around elites & best\n            idx_best = int(np.argmin(pop_f))\n            if rng.random() < 0.12 and evals < self.budget:\n                nreset = max(1, pop_size // 8)\n                for _ in range(nreset):\n                    if evals >= self.budget: break\n                    i = int(rng.integers(0, pop_size))\n                    # 80% near best, 20% random near an elite\n                    if rng.random() < 0.8:\n                        cand = x_best + 0.05 * range_norm * rng.standard_normal(dim)\n                    else:\n                        e = int(rng.choice(elite_idx))\n                        cand = pop[e] + 0.06 * range_norm * rng.standard_normal(dim)\n                    cand = np.clip(cand, lb, ub)\n                    f_c = float(func(cand))\n                    evals += 1\n                    if f_c < pop_f[i]:\n                        pop[i] = cand; pop_f[i] = f_c; sigma[i] = sigma_scale\n                        if f_c < f_best:\n                            f_best = f_c; x_best = cand.copy()\n\n            # break early if very low budget remains to go to local refine\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final local refinement around current best (simple small-step coordinate/pattern search)\n        step = 0.12 * range_norm\n        step_min = 1e-6 * range_norm\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                # negative\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n\n        # store\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm UltraDenseEdgeMesh scored 0.130 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2c409501-52e9-4505-8ad4-bcf4ade87ef5"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.143853704083292}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.14773859701528147}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.13899419429528603}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.00972912774925494}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.006583966220607751}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.013122318430993984}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.005575976559400231}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.010001769651181158}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.003830419278284203}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.8601827282004684}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.45346802898960137}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.15709599873988622}], "aucs": [0.143853704083292, 0.14773859701528147, 0.13899419429528603, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.00972912774925494, 0.006583966220607751, 0.013122318430993984, 0.005575976559400231, 0.010001769651181158, 0.003830419278284203, 0.8601827282004684, 0.45346802898960137, 0.15709599873988622]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3680.0, "Edges": 3679.0, "Max Degree": 40.0, "Min Degree": 1.0, "Mean Degree": 1.9994565217391305, "Degree Variance": 2.2478257915879016, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.499399038461538, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3232037574963573, "Depth Entropy": 2.095179361591324, "Assortativity": 0.0, "Average Eccentricity": 18.964402173913044, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0002717391304347826, "Average Shortest Path": 10.619316301688785, "mean_complexity": 12.5, "total_complexity": 75.0, "mean_token_count": 531.6666666666666, "total_token_count": 3190.0, "mean_parameter_count": 3.6666666666666665, "total_parameter_count": 22.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "30fc870a-4958-46a1-8bef-6da6ff5b6d07", "fitness": 0.2116715082309703, "name": "ACLS_EdgeDensify", "description": "ACLS-EdgeDensify \u2014 Adaptive Covariance-Step with Edge-Densified Recombination: combine adaptive covariance/step control and occasional L\u00e9vy escapes with an elite edge graph that generates many edge/interpolation/extrapolation candidates, cheaply ranks them by linear surrogate along edges, and evaluates only the most promising to rapidly densify productive connections.", "code": "import numpy as np\n\nclass ACLS_EdgeDensify:\n    \"\"\"\n    ACLS_EdgeDensify \u2014 Adaptive Covariance-Step with Edge-Densified Recombination.\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Key ideas:\n    - Maintain an elite archive of best evaluated points.\n    - Adapt a multivariate covariance and step-size (sigma) by success rate.\n    - Periodically build a k-NN edge graph among elites and generate a large pool of\n      edge-based candidates (midpoints, weighted interpolations, extrapolations,\n      orthogonal perturbations). Use a cheap linear surrogate along each edge to\n      rank candidates and only evaluate the top ones (densification).\n    - Keep occasional heavy-tailed L\u00e9vy-like jumps to escape basins and restarts\n      on stagnation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_fraction=0.12, jump_prob=0.06, target_success=0.2,\n                 cov_update_rate=None, sigma_initial=None,\n                 stagnation_restart=400,\n                 elite_max=40, k_neighbors=4,\n                 edge_pool_cap=400, edge_eval_frac=0.06,\n                 edge_every=30):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # tuning\n        self.init_fraction = float(init_fraction)\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n        if cov_update_rate is None:\n            self.c_cov = 0.18 / max(1.0, (self.dim + 2.0))\n        else:\n            self.c_cov = float(cov_update_rate)\n        self.sigma_initial = sigma_initial\n        self.stagnation_restart = int(stagnation_restart)\n\n        # edge/elite parameters\n        self.elite_max = int(elite_max)\n        self.k_neighbors = int(max(1, k_neighbors))\n        self.edge_pool_cap = int(edge_pool_cap)\n        self.edge_eval_frac = float(edge_eval_frac)   # fraction of remaining budget to spend on edge-evals when edge-densify happens\n        self.edge_every = int(edge_every)             # perform edge densification every this many evaluated candidates (approx)\n\n    def _levy_vector(self, size):\n        # normalized Cauchy-based heavy-tailed direction\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        norm = np.linalg.norm(z)\n        if norm == 0:\n            return self.rng.standard_normal(size=size)\n        return z / norm\n\n    def _reflect_clip(self, x, lb, ub):\n        # simple reflecting boundaries with final clip\n        below = x < lb\n        above = x > ub\n        if np.any(below):\n            x = np.where(below, lb + (lb - x), x)\n        if np.any(above):\n            x = np.where(above, ub - (x - ub), x)\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        # bounds extraction (compatible with provided func)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial sampling\n        n_init = max(1, int(min(max(10, 2 * dim), max(1, int(budget * self.init_fraction)))))\n        n_init = min(n_init, budget)\n        elites_x = []\n        elites_f = []\n\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            elites_x.append(x.copy())\n            elites_f.append(f)\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n\n        # convert elites to arrays sorted\n        elites_x = np.array(elites_x)\n        elites_f = np.array(elites_f)\n        order = np.argsort(elites_f)\n        elites_x = elites_x[order][:self.elite_max]\n        elites_f = elites_f[order][:self.elite_max]\n\n        # sigma and covariance init\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma_initial is None:\n            sigma = max(1e-8, 0.25 * range_scale)\n        else:\n            sigma = float(self.sigma_initial)\n        C = np.eye(dim) * (((range_scale / 4.0) ** 2) + 1e-9)\n\n        # success EMA\n        p_succ = 0.0\n        ema_alpha = 0.15\n\n        since_improvement = 0\n        steps_since_edge = 0\n\n        # helper to update elites with a new evaluated point\n        def update_elites(x_new, f_new):\n            nonlocal elites_x, elites_f\n            if elites_x.size == 0:\n                elites_x = np.array([x_new.copy()])\n                elites_f = np.array([float(f_new)])\n            else:\n                # if better than worst or not full, include\n                if elites_x.shape[0] < self.elite_max or f_new < np.max(elites_f):\n                    elites_x = np.vstack([elites_x, x_new.copy()])\n                    elites_f = np.append(elites_f, float(f_new))\n                    order = np.argsort(elites_f)\n                    elites_x = elites_x[order][:self.elite_max]\n                    elites_f = elites_f[order][:self.elite_max]\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n\n            # choose whether to do edge-densification or covariance sampling\n            # prefer edge operations when archive has multiple elites and not too recently done\n            can_edge = (elites_x.shape[0] >= 2) and (remaining > 1)\n            edge_prob = 0.5 * (1.0 - np.exp(-max(0, elites_x.shape[0] - 1) / 6.0))\n            do_edge = can_edge and ( (steps_since_edge >= self.edge_every and rng.random() < 0.95) or rng.random() < edge_prob )\n\n            if do_edge:\n                # Build an edge candidate pool (virtual candidates) and evaluate top predicted ones\n                steps_since_edge = 0\n\n                # compute pairwise nearest neighbors indices\n                X = elites_x\n                n_el = X.shape[0]\n                # distances matrix (small n_el, safe)\n                dists = np.sum((X[:, None, :] - X[None, :, :]) ** 2, axis=2)\n                # for each node, pick up to k_neighbors nearest (excluding itself)\n                neighbors = []\n                for i in range(n_el):\n                    idx = np.argsort(dists[i])\n                    # skip self\n                    idx = idx[idx != i]\n                    neighbors.append(idx[:min(self.k_neighbors, idx.size)])\n\n                # generate candidate pool in memory (no func eval yet)\n                pool = []\n                pool_pred = []  # predicted surrogate f\n                pool_x = []\n\n                # heuristics for candidate generation counts\n                # try to generate many candidates but cap by edge_pool_cap\n                cap = self.edge_pool_cap\n                for i in range(n_el):\n                    xi, fi = X[i], float(elites_f[i])\n                    for j in neighbors[i]:\n                        xj, fj = X[j], float(elites_f[j])\n                        edge_vec = xj - xi\n                        edge_len = np.linalg.norm(edge_vec) + 1e-12\n\n                        # generate a small family of candidates per edge:\n                        # - weighted interior points (Beta distributed weights)\n                        # - midpoint\n                        # - slight extrapolations beyond endpoints\n                        # - orthogonal perturbations (projected gaussian)\n\n                        # midpoint\n                        w = 0.5\n                        x_m = (1.0 - w) * xi + w * xj\n                        pred_m = (1.0 - w) * fi + w * fj\n                        pool_x.append(x_m)\n                        pool_pred.append(pred_m)\n\n                        # weighted interior samples\n                        for _ in range(2):\n                            # favor center weights but allow bias towards ends\n                            w = self.rng.beta(2.0, 2.0)\n                            xw = (1.0 - w) * xi + w * xj\n                            pool_x.append(xw)\n                            pool_pred.append((1.0 - w) * fi + w * fj)\n\n                        # extrapolations (small)\n                        for _ in range(2):\n                            # sample extrapolation factor outside [0,1]\n                            if rng.random() < 0.5:\n                                t = rng.uniform(-0.35, -0.08)\n                                w = t\n                            else:\n                                t = rng.uniform(1.08, 1.35)\n                                w = t\n                            x_ext = (1.0 - w) * xi + w * xj\n                            # penalize surrogate for distance from edge center to prefer near-edge\n                            penalty = 0.12 * (abs(w - 0.5)) * (edge_len / (range_scale + 1e-12))\n                            pool_x.append(x_ext)\n                            pool_pred.append((1.0 - w) * fi + w * fj + penalty)\n\n                        # orthogonal perturbations\n                        for _ in range(2):\n                            z = rng.normal(size=dim)\n                            # remove projection on edge to get orthogonal component\n                            if edge_len > 1e-12:\n                                proj = (np.dot(z, edge_vec) / (edge_len ** 2)) * edge_vec\n                                z_orth = z - proj\n                                norm_orth = np.linalg.norm(z_orth)\n                                if norm_orth < 1e-12:\n                                    z_orth = rng.normal(size=dim)\n                                    norm_orth = np.linalg.norm(z_orth)\n                                z_orth = z_orth / norm_orth\n                            else:\n                                z_orth = z / (np.linalg.norm(z) + 1e-12)\n                            # scale orthogonal noise by fraction of sigma (small local exploration)\n                            perturb = z_orth * (0.6 * sigma)\n                            x_per = (xi + xj) / 2.0 + perturb\n                            x_per = np.clip(x_per, lb, ub)\n                            pool_x.append(x_per)\n                            pool_pred.append(0.5 * fi + 0.5 * fj + 0.03 * (np.linalg.norm(perturb) / (range_scale + 1e-12)))\n\n                        # stop if pool too big\n                        if len(pool_x) >= cap:\n                            break\n                    if len(pool_x) >= cap:\n                        break\n\n                # vectorize\n                pool_x = np.array(pool_x)\n                pool_pred = np.array(pool_pred)\n\n                # remove duplicates (by rounding)\n                if pool_x.shape[0] > 1:\n                    # hashing by rounding to reduce exact duplicates\n                    key = np.round(pool_x, decimals=8)\n                    uniq, idx = np.unique(key, axis=0, return_index=True)\n                    pool_x = pool_x[np.sort(idx)]\n                    pool_pred = pool_pred[np.sort(idx)]\n\n                # score candidates: combine surrogate pred + distance penalty from x_opt to favour near known good region\n                if x_opt is not None:\n                    dist_to_best = np.linalg.norm(pool_x - x_opt, axis=1)\n                    pool_pred = pool_pred + 0.02 * (dist_to_best / (range_scale + 1e-12))\n                # rank\n                order = np.argsort(pool_pred)\n                pool_x = pool_x[order]\n                pool_pred = pool_pred[order]\n\n                # decide how many to actually evaluate: fraction of remaining budget, but clipped\n                max_edge_evals = max(1, int(np.ceil(self.edge_eval_frac * remaining)))\n                max_edge_evals = min(max_edge_evals, remaining)\n                # but don't attempt to evaluate huge numbers; cap to 30 per densify\n                max_edge_evals = min(max_edge_evals, 30)\n                # evaluate top candidates one by one until budget or early stop if many failures\n                failures = 0\n                max_failures = max(8, dim)  # stop if many evaluated edges don't improve\n                for k in range(min(max_edge_evals, pool_x.shape[0])):\n                    x_c = pool_x[k]\n                    # ensure in bounds\n                    x_c = self._reflect_clip(x_c, lb, ub)\n                    f_c = float(func(x_c))\n                    evals += 1\n\n                    improved = False\n                    if f_c < f_opt:\n                        improved = True\n                        f_opt = f_c\n                        x_opt = x_c.copy()\n                        since_improvement = 0\n                        # adjust covariance towards the new displacement\n                        y = (x_c - x_opt).reshape(-1, 1) if False else (x_c - x_opt).reshape(-1, 1)\n                        # If x_opt equals x_c (rare), fallback to small gaussian update\n                        if np.linalg.norm(y) < 1e-12:\n                            y = (rng.normal(size=dim) * (0.1 * sigma)).reshape(-1, 1)\n                        C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                        C += 1e-12 * np.eye(dim)\n                        # small sigma shrink to refine\n                        sigma = max(sigma * 0.9, 1e-8 * range_scale)\n                        # incorporate to elites\n                        update_elites(x_c, f_c)\n                        failures = 0\n                    else:\n                        failures += 1\n                        since_improvement += 1\n                        # occasional mild covariance inflation to encourage exploration\n                        if failures > 3:\n                            C += 1e-14 * np.eye(dim)\n\n                    # update success EMA (consider any improvement on elite list as success)\n                    p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n                    # step-size control\n                    adjust = np.exp(0.6 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                    sigma *= float(np.clip(adjust, 0.65, 1.5))\n                    sigma = np.clip(sigma, 1e-9 * range_scale + 1e-12, 6.0 * range_scale + 1e-12)\n\n                    # if we improved significantly, evaluate a tight local sample around the winner\n                    if improved and evals < budget:\n                        # small local exploitation: sample a few candidates near winner shaped by covariance\n                        L = np.linalg.cholesky(C + 1e-10 * np.eye(dim))\n                        for _ in range(min(3, budget - evals)):\n                            if rng.random() < self.jump_prob * 0.2:\n                                z = self._levy_vector(dim)\n                                dx = L.dot(z) * 2.0\n                            else:\n                                z = rng.normal(size=dim)\n                                dx = L.dot(z)\n                            x_try = x_opt + sigma * dx * 0.6\n                            x_try = self._reflect_clip(x_try, lb, ub)\n                            f_try = float(func(x_try))\n                            evals += 1\n                            if f_try < f_opt:\n                                f_opt = f_try\n                                x_opt = x_try.copy()\n                                update_elites(x_try, f_try)\n                                since_improvement = 0\n                                # covariance update\n                                y = (dx).reshape(-1, 1)\n                                C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                                C += 1e-12 * np.eye(dim)\n                                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * 1.0\n                            else:\n                                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * 0.0\n                                since_improvement += 1\n                            if evals >= budget:\n                                break\n\n                    if failures >= max_failures and evals < budget:\n                        # stop early if densification doesn't produce improvements\n                        break\n                    if evals >= budget:\n                        break\n\n                # after edge densification, slightly increase jump_prob back down to baseline\n                self.jump_prob = max(0.02, self.jump_prob * 0.98)\n                continue  # proceed to next main loop iteration\n\n            # else: covariance-driven sampling (CMA-like rank-one updates + rare Levy)\n            steps_since_edge += 1\n            # sample batch size\n            batch = min(max(1, int(6 + dim // 2)), remaining)\n            # precompute chol\n            try:\n                L = np.linalg.cholesky(C + 1e-10 * np.eye(dim))\n            except np.linalg.LinAlgError:\n                L = np.linalg.cholesky(C + (1e-6 * np.eye(dim)))\n\n            for _ in range(batch):\n                if evals >= budget:\n                    break\n\n                # choose heavy tail or gaussian\n                if rng.random() < self.jump_prob:\n                    z = self._levy_vector(dim)\n                    dx = L.dot(z) * 3.0\n                else:\n                    z = rng.normal(size=dim)\n                    dx = L.dot(z)\n\n                # propose around current best if exists, else around a random elite or global random\n                if x_opt is None:\n                    center = rng.uniform(lb, ub)\n                else:\n                    # sometimes pick a random elite center to encourage multi-modal search\n                    if elites_x.shape[0] > 1 and rng.random() < 0.12:\n                        idx = rng.integers(low=0, high=elites_x.shape[0])\n                        center = elites_x[idx]\n                    else:\n                        center = x_opt\n\n                x_trial = center + sigma * dx\n                x_trial = self._reflect_clip(x_trial, lb, ub)\n                f_trial = float(func(x_trial))\n                evals += 1\n\n                improved = False\n                if f_trial < f_opt:\n                    improved = True\n                    # update covariance using displacement relative to center\n                    y = (sigma * dx).reshape(-1, 1)\n                    C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                    C += 1e-12 * np.eye(dim)\n\n                    f_opt = f_trial\n                    x_opt = x_trial.copy()\n                    since_improvement = 0\n                    # incorporate to elites\n                    update_elites(x_trial, f_trial)\n                else:\n                    since_improvement += 1\n\n                # EMA and sigma control\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n                adjust = np.exp(0.6 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                sigma *= float(np.clip(adjust, 0.65, 1.5))\n                sigma = np.clip(sigma, 1e-9 * range_scale + 1e-12, 6.0 * range_scale + 1e-12)\n\n                # occasional covariance shrink/expand heuristics\n                if since_improvement > max(20, dim * 4):\n                    # mild inflation to escape\n                    C += 1e-12 * np.eye(dim)\n                    sigma = min(sigma * 1.06, 8.0 * range_scale)\n\n                # if stagnation reached, do targeted restart pushes\n                if since_improvement >= self.stagnation_restart and evals < budget:\n                    # attempt a few targeted random pushes\n                    pushes = min(6, budget - evals)\n                    for _ in range(pushes):\n                        if rng.random() < 0.5:\n                            center = rng.uniform(lb, ub)\n                        else:\n                            center = x_opt if x_opt is not None else rng.uniform(lb, ub)\n                        x_try = center + 2.2 * sigma * rng.normal(size=dim)\n                        x_try = np.clip(x_try, lb, ub)\n                        f_try = float(func(x_try))\n                        evals += 1\n                        if f_try < f_opt:\n                            f_opt = f_try\n                            x_opt = x_try.copy()\n                            sigma = max(sigma * 0.8, 1e-9 * range_scale)\n                            C = 0.6 * C + 0.4 * np.eye(dim) * ((range_scale / 6.0) ** 2)\n                            since_improvement = 0\n                            update_elites(x_try, f_try)\n                            break\n                        if evals >= budget:\n                            break\n                    # escalate jump probability temporarily to encourage escapes\n                    if since_improvement >= self.stagnation_restart:\n                        self.jump_prob = min(0.5, self.jump_prob * 1.6)\n                        sigma = min(sigma * 2.5, 12.0 * range_scale)\n\n            # end covariance batch\n\n        # final return best found\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 7, "feedback": "The algorithm ACLS_EdgeDensify scored 0.212 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9671298258914479}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9712556956428758}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9720858781129368}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.03171925206800097}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.01614248200207291}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.011408850389342251}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0006399237645069444}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.011663280882536875}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.0693063715196236}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.05803760550931203}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.06548345768189878}], "aucs": [0.9671298258914479, 0.9712556956428758, 0.9720858781129368, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.03171925206800097, 0.01614248200207291, 0.011408850389342251, 0.0006399237645069444, 0.011663280882536875, 4.999999999999449e-05, 0.0693063715196236, 0.05803760550931203, 0.06548345768189878]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3483.0, "Edges": 3482.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.999425782371519, "Degree Variance": 2.3117998425394037, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 9.145251396648044, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.323526462100488, "Depth Entropy": 2.3011565861661474, "Assortativity": 0.0, "Average Eccentricity": 19.261843238587424, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0002871088142405972, "Average Shortest Path": 11.407288671998876, "mean_complexity": 14.0, "total_complexity": 70.0, "mean_token_count": 626.0, "total_token_count": 3130.0, "mean_parameter_count": 5.0, "total_parameter_count": 25.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "006dbbcd-4203-4b11-92d2-7adbf45abdfd", "fitness": "-inf", "name": "ACLS_EdgeDense", "description": "ACLS-EdgeDense \u2014 Adaptive Covariance-Step Search with Edge-Densified Candidate Pools: keep an evolving covariance-driven local search but periodically build a very dense pool of inexpensive edge/triangle recombinations from an elite graph, rank them with a cheap weighted linear surrogate, and evaluate only top candidates to exploit high \"edge density\" while keeping evaluations tight.", "code": "import numpy as np\n\nclass ACLS_EdgeDense:\n    \"\"\"\n    ACLS_EdgeDense - Adaptive Covariance-Step with Edge-Densified Candidate Pools.\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional tuning parameters can be passed as keyword arguments.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_fraction=0.12, jump_prob=0.06, target_success=0.2,\n                 cov_update_rate=None, sigma_initial=None, stagnation_restart=500,\n                 elite_size=None, edge_candidate_pool=600, edge_evals_per_round=8,\n                 edge_period=40, surrogate_reg=1e-6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # meta-parameters\n        self.init_fraction = float(init_fraction)\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n        if cov_update_rate is None:\n            self.c_cov = 0.2 / max(1.0, (self.dim + 2.0))\n        else:\n            self.c_cov = float(cov_update_rate)\n        self.sigma_initial = sigma_initial\n        self.stagnation_restart = int(stagnation_restart)\n\n        # edge-dense parameters\n        self.elite_size = int(elite_size) if elite_size is not None else max(8, min(40, 6 + self.dim))\n        self.edge_candidate_pool = int(edge_candidate_pool)  # number of virtual candidates to synthesize per edge phase\n        self.edge_evals_per_round = int(edge_evals_per_round)  # how many of those to actually evaluate\n        self.edge_period = int(edge_period)  # how often (in evaluations) to trigger edge densification\n        self.surrogate_reg = float(surrogate_reg)\n\n    def _levy_vector(self, size):\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        norm = np.linalg.norm(z)\n        if norm == 0:\n            return self.rng.standard_normal(size=size)\n        return z / norm\n\n    def _get_bounds(self, func):\n        # Many benchmark wrappers provide bounds; otherwise defaults to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _weighted_linear_surrogate(self, X, y, weights=None):\n        # Fit linear surrogate: y \u2248 a + b^T x, return intercept a and coeffs b\n        # Solve weighted ridge regression\n        n, d = X.shape\n        X_aug = np.hstack([np.ones((n, 1)), X])  # [1 x]\n        if weights is None:\n            W = np.eye(n)\n        else:\n            W = np.diag(weights)\n        # normal equations with ridge regularization\n        A = X_aug.T @ W @ X_aug\n        reg = self.surrogate_reg * np.eye(d + 1)\n        A += reg\n        b = X_aug.T @ W @ y\n        try:\n            params = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            params = np.linalg.lstsq(A, b, rcond=None)[0]\n        intercept = params[0]\n        coeffs = params[1:]\n        return intercept, coeffs\n\n    def _generate_edge_candidates(self, elites_X, elites_idx, pool_size):\n        # Generate pool_size virtual candidates using edges and triangles among elites\n        # elites_X: (m, dim)\n        m, dim = elites_X.shape\n        pool = []\n        rng = self.rng\n        if m < 2:\n            # fallback: random uniform\n            for _ in range(pool_size):\n                pool.append(rng.uniform(-5.0, 5.0, size=dim))\n            return np.array(pool)\n\n        # We'll use a mixture of strategies: interpolate/extrapolate along random edges,\n        # barycentric combinations of triples, and small orthogonal perturbations.\n        for _ in range(pool_size):\n            choice = rng.random()\n            if choice < 0.65:\n                # edge interpolation / extrapolation\n                i, j = rng.integers(0, m), rng.integers(0, m)\n                if i == j:\n                    j = (i + 1) % m\n                a = elites_X[i]\n                b = elites_X[j]\n                t = rng.uniform(-0.5, 1.5)  # allow extrapolation beyond endpoints\n                pt = (1.0 - t) * a + t * b\n                # add a perturbation orthogonal (small)\n                if rng.random() < 0.5:\n                    # small orthogonal-ish noise scaled to edge length\n                    d = b - a\n                    dnorm = np.linalg.norm(d)\n                    if dnorm > 1e-12:\n                        # sample random vector and remove projection on d\n                        v = rng.normal(size=dim)\n                        v -= (v @ d) / (dnorm**2) * d\n                        if np.linalg.norm(v) > 1e-12:\n                            v = v / np.linalg.norm(v) * rng.normal() * (0.15 * dnorm + 1e-6)\n                            pt = pt + v\n                pool.append(pt)\n            elif choice < 0.9 and m >= 3:\n                # barycentric among three elites (convex combos + slight extrapolation)\n                i, j, k = rng.choice(m, size=3, replace=False)\n                w = rng.dirichlet(alpha=np.ones(3))\n                # occasionally allow one negative weight to extrapolate\n                if rng.random() < 0.2:\n                    idx = rng.integers(0, 3)\n                    w[idx] = w[idx] - rng.uniform(0.0, 0.4)\n                pt = w[0]*elites_X[i] + w[1]*elites_X[j] + w[2]*elites_X[k]\n                # small isotropic perturbation\n                if rng.random() < 0.5:\n                    pt = pt + 0.05 * rng.normal(size=dim)\n                pool.append(pt)\n            else:\n                # local jitter around a single elite\n                i = rng.integers(0, m)\n                pt = elites_X[i] + 0.2 * rng.normal(size=dim)\n                pool.append(pt)\n        return np.array(pool)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure lengths\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = int(self.budget)\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial sampling\n        n_init = max(1, int(min(max(10, 2 * dim), max(1, int(budget * self.init_fraction)))))\n        n_init = min(n_init, budget)\n        X_hist = []\n        f_hist = []\n\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            X_hist.append(x.copy())\n            f_hist.append(f)\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n\n        # range scaling\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma_initial is None:\n            sigma = max(1e-8, 0.25 * range_scale)\n        else:\n            sigma = float(self.sigma_initial)\n\n        # covariance init\n        C = np.eye(dim) * (((range_scale / 4.0) ** 2) + 1e-9)\n\n        p_succ = 0.0\n        ema_alpha = 0.15\n        since_improvement = 0\n        last_edge_eval_at = 0\n\n        # maintain elites as arrays\n        def refresh_elites():\n            if len(f_hist) == 0:\n                return np.zeros((0, dim)), np.array([]), []\n            idx = np.argsort(f_hist)[:self.elite_size]\n            elites_X = np.array([X_hist[i] for i in idx])\n            elites_f = np.array([f_hist[i] for i in idx])\n            elites_idx = list(idx)\n            return elites_X, elites_f, elites_idx\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # small batch size\n            batch = min(max(1, int(6 + dim // 2)), remaining)\n\n            # prepare cholesky\n            reg = 1e-9 * np.eye(dim)\n            try:\n                L = np.linalg.cholesky(C + reg)\n            except np.linalg.LinAlgError:\n                L = np.linalg.cholesky(C + (1e-6 * np.eye(dim)))\n\n            for _ in range(batch):\n                # local or heavy jump\n                if rng.random() < self.jump_prob:\n                    z = self._levy_vector(dim)\n                    dx = L.dot(z) * 3.0\n                else:\n                    z = rng.normal(size=dim)\n                    dx = L.dot(z)\n\n                x_trial = x_opt + sigma * dx\n                # reflect/clip\n                below_lb = x_trial < lb\n                above_ub = x_trial > ub\n                if np.any(below_lb) or np.any(above_ub):\n                    x_trial = np.where(below_lb, lb + (lb - x_trial), x_trial)\n                    x_trial = np.where(above_ub, ub - (x_trial - ub), x_trial)\n                    x_trial = np.clip(x_trial, lb, ub)\n\n                f_trial = float(func(x_trial))\n                evals += 1\n                X_hist.append(x_trial.copy())\n                f_hist.append(f_trial)\n\n                improved = False\n                if f_trial < f_opt:\n                    improved = True\n                    # covariance rank-one update\n                    y = (dx).reshape(-1, 1)\n                    C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                    C += 1e-12 * np.eye(dim)\n                    f_opt = f_trial\n                    x_opt = x_trial.copy()\n                    since_improvement = 0\n                else:\n                    since_improvement += 1\n\n                # update EMA success\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n\n                # adjust sigma\n                adjust = np.exp(0.6 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                sigma *= float(np.clip(adjust, 0.6, 1.6))\n                sigma = np.clip(sigma, 1e-8 * range_scale + 1e-12, 6.0 * range_scale + 1e-12)\n\n                # periodic edge-densification phase (trigger by eval count or stagnation)\n                if (evals - last_edge_eval_at) >= self.edge_period or (since_improvement > max(20, self.stagnation_restart//4)):\n                    # prepare elites\n                    elites_X, elites_f, elites_idx = refresh_elites()\n                    m = elites_X.shape[0]\n                    if m >= 2 and evals < budget:\n                        # build many virtual candidates\n                        pool_size = min(self.edge_candidate_pool, max(80, 40 * m))\n                        pool = self._generate_edge_candidates(elites_X, elites_idx, pool_size)\n\n                        # clip to bounds\n                        pool = np.clip(pool, lb, ub)\n\n                        # prepare surrogate training (use elites and some nearby samples)\n                        # select up to 3*elite_size points weighted by closeness to x_opt\n                        if len(f_hist) > 0:\n                            X_arr = np.array(X_hist)\n                            f_arr = np.array(f_hist)\n                            # compute distances to x_opt and weights\n                            dists = np.linalg.norm(X_arr - x_opt, axis=1) + 1e-12\n                            # weights: prefer close points and better fitness\n                            w_fit = np.exp(-(f_arr - np.min(f_arr)) / (abs(np.median(f_arr) - np.min(f_arr)) + 1e-8))\n                            w_dist = np.exp(- (dists / (np.mean(dists) + 1e-12)) )\n                            weights = w_fit * w_dist\n                            # subsample if too many points\n                            sel_count = min(len(f_arr), max(3 * m, 30))\n                            idx_sel = np.argsort(-weights)[:sel_count]\n                            X_train = X_arr[idx_sel]\n                            y_train = f_arr[idx_sel]\n                            w_train = weights[idx_sel]\n                        else:\n                            X_train = elites_X\n                            y_train = elites_f\n                            w_train = np.ones(len(y_train))\n\n                        # fit weighted linear surrogate\n                        try:\n                            intercept, coeffs = self._weighted_linear_surrogate(X_train, y_train, weights=w_train)\n                            # predict for pool\n                            preds = intercept + pool.dot(coeffs)\n                        except Exception:\n                            # fallback: use distance-to-best heuristic (closer to best lower predicted f)\n                            preds = np.linalg.norm(pool - x_opt, axis=1)\n\n                        # select top-k candidates (lowest predicted)\n                        order = np.argsort(preds)\n                        k_eval = min(self.edge_evals_per_round, budget - evals)\n                        chosen_idx = order[:k_eval]\n\n                        # actually evaluate those chosen points (in prioritized order)\n                        for idxp in chosen_idx:\n                            x_cand = pool[idxp]\n                            f_cand = float(func(x_cand))\n                            evals += 1\n                            X_hist.append(x_cand.copy())\n                            f_hist.append(f_cand)\n                            if f_cand < f_opt:\n                                # incorporate into covariance (update towards successful displacement from x_opt)\n                                disp = (x_cand - x_opt) / max(1e-12, sigma)\n                                yv = disp.reshape(-1, 1)\n                                C = (1.0 - self.c_cov) * C + self.c_cov * (yv @ yv.T)\n                                C += 1e-12 * np.eye(dim)\n                                f_opt = f_cand\n                                x_opt = x_cand.copy()\n                                since_improvement = 0\n                            else:\n                                since_improvement += 1\n                            # break early if no budget\n                            if evals >= budget:\n                                break\n                        last_edge_eval_at = evals\n\n                # stagnation handling: if stuck, diversify more aggressively\n                if since_improvement >= self.stagnation_restart and evals < budget:\n                    # inject random elites replacement: evaluate a few random uniform points\n                    pushes = min(6, budget - evals)\n                    for _ in range(pushes):\n                        x_try = rng.uniform(lb, ub)\n                        f_try = float(func(x_try))\n                        evals += 1\n                        X_hist.append(x_try.copy())\n                        f_hist.append(f_try)\n                        if f_try < f_opt:\n                            f_opt = f_try\n                            x_opt = x_try.copy()\n                            # mix covariance and reduce sigma\n                            sigma = max(sigma * 0.7, 1e-8 * range_scale)\n                            C = 0.5 * C + 0.5 * np.eye(dim) * ((range_scale / 6.0) ** 2)\n                            since_improvement = 0\n                            break\n                        if evals >= budget:\n                            break\n                    # increase jump probability and inflate sigma to escape basins\n                    self.jump_prob = min(0.6, self.jump_prob * 1.5)\n                    sigma = min(sigma * 2.0, 10.0 * range_scale)\n\n                if evals >= budget:\n                    break\n\n            # occasionally refresh covariance from elite scatter to encourage exploration directions\n            if evals < budget and len(f_hist) >= self.elite_size:\n                elites_X, elites_f, _ = refresh_elites()\n                # compute empirical covariance of elites and blend\n                if elites_X.shape[0] >= 2:\n                    emp_cov = np.cov(elites_X.T)\n                    if emp_cov.shape == (dim, dim):\n                        C = 0.85 * C + 0.15 * (emp_cov + 1e-8 * np.eye(dim))\n\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3013.0, "Edges": 3012.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9993362097577165, "Degree Variance": 2.102887046936447, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.601316752011705, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3299419691586911, "Depth Entropy": 2.282008130806498, "Assortativity": 0.0, "Average Eccentricity": 22.273149684699636, "Diameter": 31.0, "Radius": 16.0, "Edge Density": 0.00033189512114171923, "Average Shortest Path": 11.896582714390805, "mean_complexity": 9.0, "total_complexity": 63.0, "mean_token_count": 382.0, "total_token_count": 2674.0, "mean_parameter_count": 4.142857142857143, "total_parameter_count": 29.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "a5e91017-1e3f-452f-afdd-b7a0622c6183", "fitness": "-inf", "name": "HyperMeshEdgeDensifier", "description": "Hyper-Meshed Edge Densification with Surrogate-Ranked Virtual Nodes \u2014 aggressively create many virtual nodes along elite edges and triangle faces, rank them cheaply with a distance-weighted surrogate, evaluate only the best, and amplify productive edge regions to form an ultra-dense local mesh.", "code": "import numpy as np\n\nclass HyperMeshEdgeDensifier:\n    \"\"\"\n    Hyper-Meshed Edge Densification (HMED) with surrogate-ranked virtual candidates.\n\n    Main ideas:\n      - Build an ultra-dense \"mesh\" by creating many virtual nodes along elite edges and\n        triangle barycenters. Use a very cheap distance-weighted surrogate (local average)\n        to predict and rank those virtual candidates.\n      - Evaluate only top-ranked virtuals (respecting budget). Successful virtuals increase\n        local edge density (amplify mesh) and spawn further local proposals.\n      - Keep an adaptive population with per-individual step sizes and momentum. Periodically\n        inject global samples and perform a short local refinement at the end.\n\n    Required:\n      - __init__(self, budget, dim, ...) and __call__(self, func)\n      - func(x) is a black-box evaluation; search bounds assumed [-5,5] unless func provides .bounds\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, pop_size=None, elite_frac=0.2,\n                 k_neighbors=6, base_virtual=4, tri_virtual=6, proposals_per_cycle=80,\n                 levy_prob=0.03, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.base_virtual = int(base_virtual)\n        self.tri_virtual = int(tri_virtual)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.levy_prob = float(levy_prob)\n\n        if pop_size is None:\n            # small sensible default, but ensure <= budget later\n            self.pop_size = max(8, min(140, int(np.clip(np.sqrt(self.budget) * 1.1, 8, 140))))\n            self.pop_size = max(self.pop_size, self.dim + 4)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation hyperparams\n        self.sigma_scale = 0.08  # relative to problem range\n        self.success_boost = 1.25\n        self.failure_shrink = 0.82\n        self.edge_amplify = 1.6  # multiply density when edge produces success\n        self.edge_decay = 0.94   # fade density slowly\n\n    def _get_bounds(self, func):\n        # try to infer bounds, otherwise default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_range = ub - lb\n        range_norm = np.linalg.norm(box_range)\n        sigma_init = max(1e-12, self.sigma_scale * range_norm)\n\n        # clamp pop_size to budget (need at least 2)\n        self.pop_size = int(min(self.pop_size, max(2, self.budget)))\n        pop_size = self.pop_size\n\n        # initial population (uniform)\n        pop = rng.uniform(lb, ub, size=(pop_size, dim))\n        pop_f = np.full(pop_size, np.inf, dtype=float)\n        evals = 0\n        for i in range(pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # per-individual sigma and momentum\n        sigma = np.full(pop_size, sigma_init, dtype=float)\n        momentum = np.zeros((pop_size, dim), dtype=float)\n\n        # edge density stores how many virtual nodes to spawn along that edge (key sorted tuple)\n        edge_density = {}  # key -> float density; default 1.0\n        def edge_key(a, b):\n            return (int(min(a,b)), int(max(a,b)))\n        def get_density(a,b):\n            return edge_density.get(edge_key(a,b), 1.0)\n        def set_density(a,b,val):\n            edge_density[edge_key(a,b)] = float(val)\n\n        # current best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        # helper to build graph edges emphasizing elites and kNN\n        def build_edges():\n            # decays densities gradually\n            for k in list(edge_density.keys()):\n                edge_density[k] *= self.edge_decay\n                if edge_density[k] < 1e-3:\n                    del edge_density[k]\n            # compute distances squared\n            X = pop\n            dif = X[:, None, :] - X[None, :, :]\n            dist2 = np.sum(dif*dif, axis=2)\n            elite_count = max(2, int(np.ceil(self.elite_frac * pop_size)))\n            elites = np.argsort(pop_f)[:elite_count].tolist()\n\n            edges = set()\n            # fully connect elites\n            for i in range(len(elites)):\n                for j in range(i+1, len(elites)):\n                    a = elites[i]; b = elites[j]\n                    edges.add(edge_key(a,b))\n                    if edge_key(a,b) not in edge_density:\n                        edge_density[edge_key(a,b)] = 1.0\n\n            # k-nearest neighbors for every point (but focus on elites first)\n            for i in elites:\n                neigh = np.argsort(dist2[i])\n                count = 0\n                for j in neigh:\n                    if j == i: continue\n                    edges.add(edge_key(i,int(j)))\n                    if edge_key(i,j) not in edge_density:\n                        edge_density[edge_key(i,j)] = 1.0\n                    count += 1\n                    if count >= self.k_neighbors: break\n\n            # some extra edges among top 2*elite_count to create cross-links\n            topk = min(pop_size, max(elite_count, elite_count*2))\n            top_indices = np.argsort(pop_f)[:topk]\n            for i in top_indices:\n                for _ in range(2):\n                    j = int(rng.integers(0, pop_size))\n                    if i == j: continue\n                    edges.add(edge_key(i,j))\n                    if edge_key(i,j) not in edge_density:\n                        edge_density[edge_key(i,j)] = 1.0\n\n            # random extra edges for connectivity\n            for _ in range(max(6, pop_size//2)):\n                a = int(rng.integers(0,pop_size)); b = int(rng.integers(0,pop_size))\n                if a == b: continue\n                edges.add(edge_key(a,b))\n                if edge_key(a,b) not in edge_density:\n                    edge_density[edge_key(a,b)] = 1.0\n\n            return list(edges)\n\n        # cheap distance-weighted surrogate predictor:\n        # given candidate matrix C (m x dim), predict f_pred and uncertainty (weighted var)\n        def surrogate_predict(C, k_nn=8, eps=1e-12):\n            # distances from candidates to population points\n            # shape m x pop_size\n            if C.ndim == 1:\n                C = C[None, :]\n            m = C.shape[0]\n            # vectorized euclidean distances\n            d2 = np.sum((C[:, None, :] - pop[None, :, :])**2, axis=2)  # (m, pop_size)\n            # for numerical stability, use sqrt\n            d = np.sqrt(d2 + eps)\n            # pick nearest k_nn neighbors indices\n            kn = min(max(2, k_nn), pop_size)\n            idxs = np.argpartition(d, kn, axis=1)[:, :kn]  # not sorted\n            f_preds = np.empty(m, dtype=float)\n            uncert = np.empty(m, dtype=float)\n            pop_f_local = pop_f  # closure\n            for i in range(m):\n                neigh = idxs[i]\n                di = d[i, neigh] + 1e-12\n                # weights inverse-dist with small kernel width scaled by median\n                med = np.median(di)\n                width = max(med, 1e-6 * range_norm)\n                w = np.exp(- (di / (width + 1e-12)))\n                w = w / (np.sum(w) + 1e-12)\n                vals = pop_f_local[neigh]\n                f_mean = np.sum(w * vals)\n                # weighted variance as uncertainty\n                var = np.sum(w * (vals - f_mean)**2)\n                f_preds[i] = f_mean\n                uncert[i] = var\n            return f_preds, uncert\n\n        cycle = 0\n        # main search loop\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            improved = False\n\n            # build edges\n            edges = build_edges()\n\n            # adjacency map for random walks if needed\n            adjacency = [[] for _ in range(pop_size)]\n            for a,b in edges:\n                adjacency[a].append(b)\n                adjacency[b].append(a)\n\n            # create a candidate pool (virtual nodes) far larger than we will evaluate;\n            # we'll score them cheaply and evaluate only top choices to respect budget.\n            candidate_list = []\n\n            # Create virtual nodes along edges: for each edge produce N virtuals scaled by edge_density\n            for (a,b) in edges:\n                dens = float(get_density(a,b))\n                # number of virtual nodes is base_virtual * dens (bounded)\n                n_virtual = int(min(24, max(1, int(round(self.base_virtual * (0.6 + dens))))))\n                Xa = pop[a]; Xb = pop[b]\n                for t in rng.random(n_virtual):\n                    # sample t biased toward endpoints by Beta\n                    t_bias = rng.beta(0.9, 0.9) * (1.0 if rng.random() < 0.9 else 1.8)  # sometimes allow extrap\n                    t_val = np.clip(t_bias, -0.8, 1.8)\n                    x = (1.0 - t_val) * Xa + t_val * Xb\n                    # add small along-edge jitter and orthogonal noise\n                    edge_vec = Xb - Xa\n                    if np.linalg.norm(edge_vec) > 1e-12:\n                        edge_unit = edge_vec / (np.linalg.norm(edge_vec) + 1e-12)\n                        x = x + 0.06 * sigma[a] * rng.standard_normal() * edge_unit\n                    x = x + 0.5 * (sigma[a] + sigma[b]) * rng.standard_normal(dim)\n                    # bias toward elites: move slightly closer to better endpoint\n                    if pop_f[a] < pop_f[b] and rng.random() < 0.45:\n                        x = x * 0.88 + Xa * 0.12\n                    elif pop_f[b] < pop_f[a] and rng.random() < 0.45:\n                        x = x * 0.88 + Xb * 0.12\n                    candidate_list.append((x, ('edge', a, b)))\n\n            # triangle barycentric virtuals among top elites\n            elite_count = max(3, int(np.ceil(self.elite_frac * pop_size)))\n            elites = np.argsort(pop_f)[:elite_count]\n            # generate combinations of 3 elites (sampled)\n            tri_samples = max(6, min(80, elite_count * 3))\n            for _ in range(tri_samples):\n                if elite_count >= 3 and rng.random() < 0.9:\n                    tri = rng.choice(elites, size=3, replace=False)\n                else:\n                    tri = rng.choice(pop_size, size=3, replace=False)\n                a,b,c = int(tri[0]), int(tri[1]), int(tri[2])\n                Xa, Xb, Xc = pop[a], pop[b], pop[c]\n                # barycentric weights from Dirichlet (concentrate small to explore corners)\n                alpha = 0.6 if rng.random() < 0.9 else 1.2\n                w = rng.gamma(alpha, 1.0, size=3)\n                w = w / (np.sum(w) + 1e-12)\n                x = w[0]*Xa + w[1]*Xb + w[2]*Xc\n                # occasionally extrapolate along centroid direction\n                if rng.random() < 0.18:\n                    centroid = (Xa + Xb + Xc) / 3.0\n                    expo = rng.uniform(-0.5, 1.2)\n                    x = centroid + expo * (centroid - x)\n                # small barycentric noise scaled by local sigmas\n                local_sigma = (sigma[a] + sigma[b] + sigma[c]) / 3.0\n                x = x + 0.6 * local_sigma * rng.standard_normal(dim)\n                candidate_list.append((x, ('tri', a, b, c)))\n\n            # Add a set of random global and gaussian-inflated around best\n            for _ in range(max(8, pop_size//3)):\n                if rng.random() < 0.55:\n                    # around best\n                    x = x_best + 0.08 * range_norm * rng.standard_normal(dim)\n                else:\n                    x = rng.uniform(lb, ub)\n                candidate_list.append((x, ('rand',)))\n\n            # deduplicate (clip candidates to bounds first)\n            if not candidate_list:\n                break\n            candidates = np.array([np.clip(c[0], lb, ub) for c in candidate_list])\n            origins = [c[1] for c in candidate_list]\n\n            # surrogate predict for candidates\n            f_pred, uncert = surrogate_predict(candidates, k_nn=min(12, pop_size))\n            # acquisition: lower predicted + small preference for high uncertainty to explore\n            # we compute score = f_pred - alpha * sqrt(uncert)\n            alpha = 0.8 + 0.6 * rng.random()\n            scores = f_pred - alpha * np.sqrt(uncert + 1e-12)\n\n            # rank candidates by score (lower is better)\n            order = np.argsort(scores)\n            # we'll evaluate up to a fraction of remaining budget, but not exceeding proposals_per_cycle\n            eval_quota = min(remaining, max(1, min(self.proposals_per_cycle, remaining//1)))\n            # but we also don't want to evaluate too many; cap to  max(remaining, 1)\n            to_eval = int(min(eval_quota, len(candidates)))\n            # choose top-to_eval candidates, but randomize a bit among the top 2*to_eval for diversification\n            pool_top = order[:min(len(order), max(4, to_eval*2))]\n            chosen_indices = []\n            if to_eval > 0:\n                # pick with slight softmax from top pool\n                pool_scores = scores[pool_top]\n                # invert so lower score -> higher weight\n                w = np.max(pool_scores) - pool_scores + 1e-12\n                w = w / np.sum(w)\n                # draw without replacement\n                chosen = rng.choice(len(pool_top), size=to_eval, replace=False, p=w)\n                chosen_indices = [int(pool_top[i]) for i in chosen]\n\n            # Evaluate chosen candidates sequentially (respect budget) and handle replacements\n            for idx in chosen_indices:\n                if evals >= self.budget: break\n                x_prop = np.clip(candidates[idx], lb, ub)\n                # occasional levy heavy tail\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi*(u - 0.5))\n                    x_prop = np.clip(x_prop + 0.08 * range_norm * cauchy, lb, ub)\n                f_prop = float(func(x_prop))\n                evals += 1\n\n                # find nearest population indices by distance\n                d2 = np.sum((pop - x_prop)**2, axis=1)\n                nearest = np.argsort(d2)[:max(1, min(4, pop_size))]\n                replaced = False\n\n                # replacement rule: try to replace a nearby worse member; prefer worst among nearest\n                worst_near = int(nearest[np.argmax(pop_f[nearest])])\n                if f_prop < pop_f[worst_near]:\n                    # replace\n                    old_vec = pop[worst_near].copy()\n                    pop[worst_near] = x_prop\n                    old_f = pop_f[worst_near]\n                    pop_f[worst_near] = f_prop\n                    sigma[worst_near] = max(1e-12 * range_norm, sigma[worst_near] * self.success_boost)\n                    # momentum update uses displacement from old vec\n                    momentum[worst_near] = 0.6 * momentum[worst_near] + 0.4 * (x_prop - old_vec)\n                    replaced = True\n                    # amplify densities for incident edges to encourage local densification\n                    # find edges connecting worst_near to neighbors among nearest (excluding itself)\n                    for nb in nearest:\n                        if nb == worst_near: continue\n                        k = edge_key(worst_near, int(nb))\n                        edge_density[k] = edge_density.get(k, 1.0) * self.edge_amplify\n\n                else:\n                    # as fallback, maybe replace global worst if better\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        old_vec = pop[idx_worst].copy()\n                        pop[idx_worst] = x_prop\n                        old_f = pop_f[idx_worst]\n                        pop_f[idx_worst] = f_prop\n                        sigma[idx_worst] = max(1e-12 * range_norm, sigma[idx_worst] * self.success_boost)\n                        momentum[idx_worst] = 0.4 * (x_prop - old_vec)\n                        replaced = True\n                        # amplify edges incident to idx_worst\n                        for nb in range(pop_size):\n                            if nb == idx_worst: continue\n                            k = edge_key(idx_worst, nb)\n                            edge_density[k] = edge_density.get(k, 1.0) * self.edge_amplify\n\n                # if replaced, update best and mark improvement\n                if replaced:\n                    improved = True\n                    if f_prop < f_best:\n                        f_best = f_prop\n                        x_best = x_prop.copy()\n                else:\n                    # no replacement -> slightly shrink nearest sigmas\n                    for nn in nearest:\n                        sigma[nn] *= self.failure_shrink\n\n                # small housekeeping: ensure pop_f remains consistent\n                idx_best = int(np.argmin(pop_f))\n                if pop_f[idx_best] < f_best:\n                    f_best = float(pop_f[idx_best]); x_best = pop[idx_best].copy()\n\n                if evals >= self.budget:\n                    break\n\n            # end evaluating chosen candidates\n\n            # occasional injection: sample around best or pure random\n            if rng.random() < 0.07 and evals < self.budget:\n                if rng.random() < 0.85:\n                    newx = x_best + 0.08 * range_norm * rng.standard_normal(dim)\n                else:\n                    newx = rng.uniform(lb, ub)\n                newx = np.clip(newx, lb, ub)\n                f_new = float(func(newx)); evals += 1\n                idx_w = int(np.argmax(pop_f))\n                if f_new < pop_f[idx_w]:\n                    pop[idx_w] = newx; pop_f[idx_w] = f_new\n                    sigma[idx_w] = sigma_init\n                    momentum[idx_w] = 0.0\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newx.copy()\n\n            # adapt sigmas and momentum decay\n            sigma = np.clip(sigma, 1e-12 * range_norm, 2.0 * range_norm)\n            momentum *= 0.92\n\n            # If no improvement in this cycle, perform focused reseed around best\n            if not improved and evals < self.budget:\n                if rng.random() < 0.22:\n                    n_reset = max(1, pop_size // 6)\n                    for _ in range(n_reset):\n                        if evals >= self.budget: break\n                        idx = int(rng.integers(0, pop_size))\n                        if rng.random() < 0.85:\n                            candidate = x_best + 0.06 * box_range * rng.standard_normal(dim)\n                        else:\n                            candidate = rng.uniform(lb, ub)\n                        candidate = np.clip(candidate, lb, ub)\n                        f_c = float(func(candidate)); evals += 1\n                        if f_c < pop_f[idx]:\n                            pop[idx] = candidate; pop_f[idx] = f_c\n                            sigma[idx] = sigma_init; momentum[idx] = 0.0\n                            if f_c < f_best:\n                                f_best = f_c; x_best = candidate.copy()\n\n            # break to final local refinement if budget low\n            if self.budget - evals < max(8, dim * 2):\n                break\n\n        # final local refinement: coordinate pattern-search with decreasing step (simple)\n        step = 0.12 * range_norm\n        step_min = 1e-6 * range_norm\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # forward\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                # backward\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n\n        # final bookkeeping\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "In the code, line 57, in _wrapfunc, the following error occurred:\nValueError: kth(=11) out of bounds (11)\nOn line: return lb, ub", "error": "In the code, line 57, in _wrapfunc, the following error occurred:\nValueError: kth(=11) out of bounds (11)\nOn line: return lb, ub", "parent_ids": ["2c409501-52e9-4505-8ad4-bcf4ade87ef5"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3940.0, "Edges": 3939.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.999492385786802, "Degree Variance": 2.183756087505476, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.285634432643935, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3299246328054595, "Depth Entropy": 2.0341089188212207, "Assortativity": 9.24452470392929e-09, "Average Eccentricity": 18.654314720812184, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0002538071065989848, "Average Shortest Path": 10.670036714721844, "mean_complexity": 11.75, "total_complexity": 94.0, "mean_token_count": 434.75, "total_token_count": 3478.0, "mean_parameter_count": 3.125, "total_parameter_count": 25.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "abe80f39-d227-4abb-840d-fde4f959b53c", "fitness": "-inf", "name": "DenseEdgeTriangulationGraphImproved", "description": "Ultra-dense Edge-Triangulation with Virtual Nodes and Surrogate Pre-screening \u2014 massively densify the elite edge/triangle space by creating many virtual midpoints/barycenters, rank the huge candidate pool with cheap local linear surrogates, then evaluate only the top promises and adapt edge weights/momentum to concentrate search while preserving heavy-tailed diversification.", "code": "import numpy as np\n\nclass DenseEdgeTriangulationGraphImproved:\n    \"\"\"\n    Improved Dense Edge Triangulation Graph Walks with Virtual Nodes and Surrogate Pre-screening\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional args:\n      pop_size: initial population size (will be clipped to budget)\n      elite_frac: fraction of population considered elite for dense core\n      k_neighbors: neighbors per elite to add edges\n      proposals_per_cycle: number of real evaluations allowed per cycle (max)\n      virtual_pool_factor: multiply proposals_per_cycle to create virtual candidates before surrogate filtering\n      tri_prob, walk_prob, levy_prob: strategy mix probabilities\n      surrogate_k: neighbors for local linear surrogate\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.25,\n                 k_neighbors=6, proposals_per_cycle=40, virtual_pool_factor=8,\n                 tri_prob=0.45, walk_prob=0.30, levy_prob=0.06, surrogate_k=10,\n                 seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.virtual_pool_factor = int(max(2, virtual_pool_factor))\n        self.tri_prob = float(tri_prob)\n        self.walk_prob = float(walk_prob)\n        self.levy_prob = float(levy_prob)\n        self.surrogate_k = int(surrogate_k)\n\n        # sensible default population\n        if pop_size is None:\n            base = int(max(12, min(120, np.clip(np.sqrt(self.budget) * 1.2, 12, 120))))\n            self.pop_size = max(base, self.dim + 6)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation constants\n        self.sigma_init = 0.09\n        self.success_boost = 1.20\n        self.failure_shrink = 0.82\n        self.edge_gain = 1.25\n        self.edge_decay = 0.993\n        self.min_sigma = 1e-12\n\n    def _get_bounds(self, func):\n        # try to get bounds from func object if provided, otherwise use [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_norm = np.linalg.norm(ub - lb)\n        sigma_scale = max(1e-8, self.sigma_init * range_norm)\n\n        # ensure population not larger than budget\n        pop_size = min(self.pop_size, max(2, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population uniformly and evaluate\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # per-individual sigma and momentum\n        sigma = np.full(self.pop_size, sigma_scale)\n        momentum = np.zeros((self.pop_size, dim), dtype=float)\n\n        # edge weights keyed by sorted pair\n        edge_weights = {}\n\n        def edge_key(a, b):\n            a, b = int(a), int(b)\n            return (min(a, b), max(a, b))\n\n        def get_edge_weight(a, b):\n            return edge_weights.get(edge_key(a, b), 1.0)\n\n        def inc_edge_weight(a, b, factor):\n            key = edge_key(a, b)\n            edge_weights[key] = edge_weights.get(key, 1.0) * factor\n\n        # initial best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        # helper: build dense graph with virtual densification seeds (create many real edges)\n        def build_dense_graph():\n            # decay\n            if edge_weights:\n                for k in list(edge_weights.keys()):\n                    edge_weights[k] *= self.edge_decay\n\n            X = pop\n            # pairwise distances (squared)\n            dif = X[:, None, :] - X[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count].tolist()\n\n            edges = set()\n\n            # near-complete elite core (dense)\n            for i_idx in range(len(elites)):\n                for j_idx in range(i_idx + 1, len(elites)):\n                    a = elites[i_idx]; b = elites[j_idx]\n                    edges.add(edge_key(a, b))\n                    edge_weights.setdefault(edge_key(a, b), 1.0)\n\n            # nearest-neighbor edges around elites\n            for i in elites:\n                neigh = np.argsort(dist2[i])\n                added = 0\n                for j in neigh:\n                    if j == i: continue\n                    edges.add(edge_key(i, int(j)))\n                    edge_weights.setdefault(edge_key(i, int(j)), 1.0)\n                    added += 1\n                    if added >= self.k_neighbors:\n                        break\n\n            # connect top-k performers among themselves and to randoms\n            topk = max(elite_count, min(self.pop_size, elite_count * 3))\n            top_indices = np.argsort(pop_f)[:topk]\n            for i in top_indices:\n                # connect to a few random nodes for diversity\n                for _ in range(3):\n                    j = int(rng.integers(0, self.pop_size))\n                    if j != i:\n                        edges.add(edge_key(i, j))\n                        edge_weights.setdefault(edge_key(i, j), 1.0)\n\n            # add many random edges to boost density (parameterized by pop_size)\n            for _ in range(max(8, self.pop_size)):\n                a = int(rng.integers(0, self.pop_size))\n                b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges.add(edge_key(a, b))\n                    edge_weights.setdefault(edge_key(a, b), 1.0)\n\n            return list(edges)\n\n        # local linear surrogate prediction (weighted)\n        def local_linear_predict(x_cand, X, y, k):\n            # pick k nearest neighbors\n            d2 = np.sum((X - x_cand) ** 2, axis=1)\n            idx = np.argsort(d2)[:max(2, min(k, X.shape[0]))]\n            Xi = X[idx]\n            yi = y[idx]\n            # weights: RBF-ish using distances\n            d = np.sqrt(np.maximum(1e-12, d2[idx]))\n            # scale based on median distance to avoid numerical issues\n            scale = np.median(d) + 1e-8\n            w = np.exp(- (d / (scale + 1e-12)) ** 2)\n            # fallback to simple weighted mean if degenerate\n            if Xi.shape[0] <= 2 or np.allclose(w, 0):\n                return float(np.sum(w * yi) / (np.sum(w) + 1e-12))\n            # center\n            w_s = np.sqrt(w)\n            Xi_c = Xi - np.average(Xi, axis=0, weights=w)\n            yi_c = yi - np.average(yi, weights=w)\n            # weighted least squares solve for linear coefficients\n            A = (w_s[:, None] * Xi_c)\n            b = w_s * yi_c\n            # regularization for stability\n            try:\n                beta, *_ = np.linalg.lstsq(A, b, rcond=None)\n                pred = np.average(yi, weights=w) + np.dot((x_cand - np.average(Xi, axis=0, weights=w)), beta)\n                return float(pred)\n            except Exception:\n                return float(np.sum(w * yi) / (np.sum(w) + 1e-12))\n\n        cycle = 0\n        stagnant_cycles = 0\n        while evals < self.budget:\n            remaining = self.budget - evals\n            cycle += 1\n            improved = False\n\n            # build real edges and adjacency\n            edges = build_dense_graph()\n            adjacency = [[] for _ in range(self.pop_size)]\n            for (a, b) in edges:\n                adjacency[a].append(b)\n                adjacency[b].append(a)\n\n            # create a large virtual candidate pool (combining edges, triangles, midpoints) then pre-screen\n            pool_scale = self.virtual_pool_factor\n            virtual_pool_size = min(2000, max(200, self.proposals_per_cycle * pool_scale))\n            virtual_candidates = []\n            virtual_parents = []  # store tuple of parent indices used (for credit assignment)\n\n            # sample mix: many midpoints, many barycenters from elite triplets, some random perturbations\n            # generate midpoints along edges (weighted sampling by edge weight)\n            if edges:\n                edge_list = edges\n                weights = np.array([get_edge_weight(a, b) for (a, b) in edge_list], dtype=float)\n                # avoid zeros\n                weights = weights + 1e-12\n                probs = weights / np.sum(weights)\n                n_mid = int(virtual_pool_size * 0.45)\n                chosen_idx = rng.choice(len(edge_list), size=n_mid, p=probs)\n                for ci in chosen_idx:\n                    a, b = edge_list[ci]\n                    Xa, Xb = pop[a], pop[b]\n                    # sample interpolation/extrapolation coefficient biased near center\n                    t = rng.normal(0.5, 0.18)\n                    t = float(np.clip(t, -0.6, 1.6))\n                    x = (1 - t) * Xa + t * Xb\n                    # small correlated noise along edge\n                    edge_dir = Xb - Xa\n                    nrm = np.linalg.norm(edge_dir)\n                    if nrm > 1e-12:\n                        edge_unit = edge_dir / nrm\n                        x += 0.04 * sigma_scale * rng.standard_normal() * edge_unit\n                    x += 0.4 * (sigma[a] + sigma[b]) * 0.5 * rng.standard_normal(dim)\n                    virtual_candidates.append(np.clip(x, lb, ub))\n                    virtual_parents.append((a, b))\n\n            # barycenters among elites and mixed elites\n            elite_count = max(3, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count]\n            n_tri = int(virtual_pool_size * 0.35)\n            for _ in range(n_tri):\n                # choose 3 elites with bias towards top\n                trip = rng.choice(elites, size=3, replace=False)\n                Xa, Xb, Xc = pop[trip[0]], pop[trip[1]], pop[trip[2]]\n                # sample barycentric weights (favor corners sometimes)\n                w = rng.gamma(0.8, 1.0, size=3)\n                w = w / np.sum(w)\n                x = w[0] * Xa + w[1] * Xb + w[2] * Xc\n                # occasional small extrapolation\n                if rng.random() < 0.22:\n                    expo = rng.uniform(-0.3, 1.1)\n                    centroid = (Xa + Xb + Xc) / 3.0\n                    x = centroid + expo * (centroid - x)\n                x += 0.5 * sigma_scale * rng.standard_normal(dim)\n                virtual_candidates.append(np.clip(x, lb, ub))\n                virtual_parents.append(tuple(int(p) for p in trip))\n\n            # graph-walk style virtuals: start from elites and take sequence hops producing interpolated positions\n            n_walks = int(virtual_pool_size * 0.15)\n            for _ in range(n_walks):\n                if rng.random() < 0.85:\n                    start = int(rng.choice(elites))\n                else:\n                    start = int(rng.integers(0, self.pop_size))\n                cur_idx = start\n                cur_pos = pop[cur_idx].copy()\n                steps = int(rng.integers(1, 5))\n                parents = [cur_idx]\n                for s in range(steps):\n                    neigh = adjacency[cur_idx]\n                    if not neigh: break\n                    nb = int(rng.choice(neigh))\n                    parents.append(nb)\n                    direction = pop[nb] - pop[cur_idx]\n                    normd = np.linalg.norm(direction) + 1e-12\n                    step_scale = 0.6 * (sigma[cur_idx] + sigma[nb]) * 0.5\n                    cur_pos = cur_pos + 0.9 * step_scale * (direction / normd)\n                    cur_idx = nb\n                cur_pos += 0.03 * range_norm * rng.standard_normal(dim)\n                virtual_candidates.append(np.clip(cur_pos, lb, ub))\n                virtual_parents.append(tuple(parents))\n\n            # a few random perturbations around best to keep density there\n            n_local = int(virtual_pool_size * 0.05)\n            for _ in range(n_local):\n                x = x_best + 0.06 * (ub - lb) * rng.standard_normal(dim)\n                virtual_candidates.append(np.clip(x, lb, ub))\n                virtual_parents.append((idx_best,))\n\n            # ensure at least one random global candidate\n            virtual_candidates.append(rng.uniform(lb, ub))\n            virtual_parents.append(tuple())\n\n            # convert to arrays\n            virtual_candidates = np.array(virtual_candidates, dtype=float)\n            # perform surrogate pre-scoring to pick the best subset to actually evaluate\n            n_candidates = virtual_candidates.shape[0]\n            preds = np.full(n_candidates, np.inf)\n            # precompute to speed surrogate calls: X and y\n            Xpop = pop.copy()\n            ypop = pop_f.copy()\n            k_sur = min(self.surrogate_k, max(2, self.pop_size))\n            for i in range(n_candidates):\n                preds[i] = local_linear_predict(virtual_candidates[i], Xpop, ypop, k_sur)\n\n            # choose top-k by predicted quality to evaluate; but maintain budget\n            max_real = min(self.proposals_per_cycle, remaining)\n            topk = max(1, min(max_real * 2, n_candidates // 4))  # evaluate up to some multiplier but capped later\n            # sort by predicted (lower is better)\n            order = np.argsort(preds)\n            # evaluate in batches but stop when budget used or we've done max_real real evaluations\n            real_evals = 0\n            j = 0\n            while evals < self.budget and real_evals < max_real and j < n_candidates:\n                idx_cand = order[j]\n                j += 1\n                x_prop = virtual_candidates[idx_cand]\n                # small random jitter proportional to local sigma to diversify ties\n                x_prop = np.clip(x_prop + 1e-6 * sigma_scale * rng.standard_normal(dim), lb, ub)\n                # evaluate\n                f_prop = float(func(x_prop))\n                evals += 1\n                real_evals += 1\n\n                # optionally apply heavy-tailed large move occasionally\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    x_prop_lev = np.clip(x_prop + 0.12 * range_norm * cauchy, lb, ub)\n                    if evals < self.budget:\n                        f_prop_lev = float(func(x_prop_lev))\n                        evals += 1\n                        # accept the better of the two\n                        if f_prop_lev < f_prop:\n                            x_prop = x_prop_lev\n                            f_prop = f_prop_lev\n\n                # integrate candidate into population: find nearest indices\n                d2 = np.sum((pop - x_prop) ** 2, axis=1)\n                nearest = np.argsort(d2)[:max(1, min(4, self.pop_size))]\n                replaced = False\n                # attempt to replace worst among nearest or a random weak individual if better\n                for idx in nearest:\n                    if f_prop < pop_f[idx]:\n                        old_pos = pop[idx].copy()\n                        pop[idx] = x_prop\n                        old_f = pop_f[idx]\n                        pop_f[idx] = f_prop\n                        # update sigma and momentum using previous location\n                        sigma[idx] = max(self.min_sigma * range_norm, sigma[idx] * self.success_boost)\n                        momentum[idx] = 0.6 * momentum[idx] + 0.4 * (x_prop - old_pos)\n                        replaced = True\n                        # reward edges connecting idx and its neighbors\n                        for nb in nearest:\n                            if nb != idx:\n                                inc_edge_weight(idx, nb, self.edge_gain)\n                        break\n                if not replaced:\n                    # maybe replace global worst\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        old_pos = pop[idx_worst].copy()\n                        pop[idx_worst] = x_prop\n                        pop_f[idx_worst] = f_prop\n                        sigma[idx_worst] = max(self.min_sigma * range_norm, sigma[idx_worst] * self.success_boost)\n                        momentum[idx_worst] = 0.3 * (x_prop - old_pos)\n                        replaced = True\n\n                if replaced:\n                    improved = True\n                    stagnant_cycles = 0\n                    # attribute credit to virtual parents if available (increment their connecting edges)\n                    parents = virtual_parents[idx_cand]\n                    # if parents are a tuple of indices, strengthen edges between them and with nearest replaced\n                    if isinstance(parents, (list, tuple)) and len(parents) >= 2:\n                        for i_p in range(len(parents)):\n                            for j_p in range(i_p + 1, len(parents)):\n                                a = int(parents[i_p]); b = int(parents[j_p])\n                                if a != b:\n                                    inc_edge_weight(a, b, self.edge_gain)\n                    # also reward edges between nearest real neighbors of x_prop\n                    if len(nearest) >= 2:\n                        inc_edge_weight(nearest[0], nearest[1], self.edge_gain)\n\n                else:\n                    # penalize sigma of nearest nodes slightly on failure\n                    for idx in nearest:\n                        sigma[idx] *= self.failure_shrink\n\n                # update global best\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n                    improved = True\n\n            # end evaluation loop for this cycle\n\n            # housekeeping\n            sigma = np.clip(sigma, self.min_sigma * range_norm, 2.5 * range_norm)\n            momentum *= 0.93\n\n            # occasional injection of a pure random individual (diversify)\n            if rng.random() < 0.06 and evals < self.budget:\n                x_r = rng.uniform(lb, ub)\n                f_r = float(func(x_r))\n                evals += 1\n                idx_w = int(np.argmax(pop_f))\n                if f_r < pop_f[idx_w]:\n                    pop[idx_w] = x_r\n                    pop_f[idx_w] = f_r\n                    sigma[idx_w] = sigma_scale\n                    momentum[idx_w] = 0.0\n                    if f_r < f_best:\n                        f_best = f_r; x_best = x_r.copy()\n                    improved = True\n\n            # stagnation handling: if no improvement, densify around best aggressively\n            if not improved:\n                stagnant_cycles += 1\n                if stagnant_cycles > 2:\n                    # reseed a fraction of population around best (local densification)\n                    n_reset = max(1, self.pop_size // 6)\n                    for _ in range(n_reset):\n                        if evals >= self.budget: break\n                        idx = int(rng.integers(0, self.pop_size))\n                        candidate = x_best + 0.04 * (ub - lb) * rng.standard_normal(dim)\n                        candidate = np.clip(candidate, lb, ub)\n                        f_c = float(func(candidate))\n                        evals += 1\n                        if f_c < pop_f[idx]:\n                            pop[idx] = candidate\n                            pop_f[idx] = f_c\n                            sigma[idx] = sigma_scale\n                            momentum[idx] = 0.0\n                            if f_c < f_best:\n                                f_best = f_c; x_best = candidate.copy()\n                    # connect best to many random nodes to increase edge density\n                    for _ in range(max(8, self.pop_size // 2)):\n                        j = int(rng.integers(0, self.pop_size))\n                        if j != idx_best:\n                            edge_weights.setdefault(edge_key(idx_best, j), 1.0)\n            else:\n                stagnant_cycles = 0\n\n            # break to final refinement if budget low\n            if self.budget - evals < max(6, dim * 2):\n                break\n\n        # final local pattern search around best (greedy coordinate search with decreasing step)\n        step = 0.10 * range_norm\n        step_min = 1e-6 * range_norm\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # try positive\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                # try negative\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["2c409501-52e9-4505-8ad4-bcf4ade87ef5"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4138.0, "Edges": 4137.0, "Max Degree": 41.0, "Min Degree": 1.0, "Mean Degree": 1.999516674722088, "Degree Variance": 2.3291442806547717, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.270430107526881, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3292643307160426, "Depth Entropy": 2.052582154083866, "Assortativity": 0.0, "Average Eccentricity": 18.648380860318994, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0002416626389560174, "Average Shortest Path": 10.650932950972452, "mean_complexity": 11.25, "total_complexity": 90.0, "mean_token_count": 450.5, "total_token_count": 3604.0, "mean_parameter_count": 3.5, "total_parameter_count": 28.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "c2a9a7df-fd64-4ab4-a5e3-0d1c9b1e0c09", "fitness": "-inf", "name": "EdgeDensifiedAdaptiveLowRankQuadraticSurrogates", "description": "Edge-Densified Adaptive Low-Rank Quadratic Surrogates (ED-ALRQS) \u2014 combine many cheap, virtual \"edge\" and triangle candidates densely sampled from an elite k-NN graph with low-rank quadratic surrogate solves in random subspaces; use fast surrogate scoring to pick the best few real-evaluation candidates, adapt trust radii from predicted vs actual reductions and inject dense edge-born proposals to dramatically increase edge density and connectivity of the search.", "code": "import numpy as np\n\nclass EdgeDensifiedAdaptiveLowRankQuadraticSurrogates:\n    \"\"\"\n    Edge-Densified Adaptive Low-Rank Quadratic Surrogates (ED-ALRQS)\n\n    Key ideas:\n      - Maintain an archive of evaluated points and per-point trust radii.\n      - Build an elite k-NN graph and generate a dense pool of virtual candidates\n        along edges and triangles (midpoints, extrapolations, orthogonal perturbations).\n      - Score virtual candidates cheaply with tiny local quadratic fits (or IDW fallback).\n      - Evaluate only the top-scoring candidates (respecting the evaluation budget).\n      - Also include classic subspace quadratic solves around promising centers.\n      - Adapt trust radii by comparing predicted vs actual reductions.\n      - Periodically inject global samples and larger jumps to escape stagnation.\n    \"\"\"\n    def __init__(self, budget, dim, rng=None,\n                 init_samples=None, elite_k=12, k_nn=4,\n                 init_radius=1.0, min_radius=1e-6, max_radius=5.0,\n                 max_subdim=4, max_virtual=300, eval_per_iter=3):\n        \"\"\"\n        Arguments:\n          budget: total function evaluations allowed (int)\n          dim: problem dimensionality (int)\n          rng: numpy.random.Generator or None\n          init_samples: initial random samples (defaults to min(50, max(10, budget//10)))\n          elite_k: how many top points considered elite (for graph)\n          k_nn: neighbors per elite for edge graph\n          init_radius/min_radius/max_radius: trust radius control\n          max_subdim: max subspace dim for surrogate fits\n          max_virtual: maximum number of virtual candidates to generate per iteration\n          eval_per_iter: max number of real evaluations per main iteration (keeps control)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        if init_samples is None:\n            self.init_samples = int(min(50, max(10, self.budget // 10)))\n        else:\n            self.init_samples = int(init_samples)\n        self.elite_k = max(3, int(elite_k))\n        self.k_nn = max(1, int(k_nn))\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_subdim = int(min(max_subdim, dim))\n        self.max_virtual = int(max_virtual)\n        self.eval_per_iter = max(1, int(eval_per_iter))\n\n    # -------------------- utilities --------------------\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _orthonormal_basis(self, k):\n        A = self.rng.normal(size=(self.dim, k))\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def _build_phi_and_map(self, S):\n        m, k = S.shape\n        cols = [np.ones((m, 1))]\n        cols.append(S)\n        quad_cols = []\n        for i in range(k):\n            quad_cols.append(0.5 * (S[:, i:i+1] ** 2))\n        for i in range(k):\n            for j in range(i+1, k):\n                quad_cols.append((S[:, i] * S[:, j])[:, None])\n        Phi = np.hstack(cols + quad_cols)\n        return Phi\n\n    def _theta_to_quad(self, theta, k):\n        a = float(theta[0])\n        b = np.array(theta[1:1 + k], dtype=float)\n        H = np.zeros((k, k), dtype=float)\n        idx = 1 + k\n        for i in range(k):\n            H[i, i] = float(theta[idx]); idx += 1\n        for i in range(k):\n            for j in range(i + 1, k):\n                H[i, j] = float(theta[idx]); H[j, i] = H[i, j]; idx += 1\n        return a, b, H\n\n    def _local_quadratic_predict(self, xq, X_arr, F_arr, k_sub=2, nneigh=20):\n        \"\"\"\n        Cheap local surrogate prediction for candidate xq:\n          - find nneigh nearest neighbors, build PCA basis of neighbor offsets (k_sub dims)\n          - fit small quadratic in that subspace and predict; if fails fallback to IDW average.\n        Returns (predicted_value, success_flag)\n        \"\"\"\n        n = X_arr.shape[0]\n        if n == 0:\n            return None, False\n        # distances\n        dists = np.linalg.norm(X_arr - xq[None, :], axis=1)\n        order = np.argsort(dists)\n        nneigh = min(nneigh, n)\n        idxs = order[:nneigh]\n        Xn = X_arr[idxs]\n        Fn = F_arr[idxs]\n        center = Xn.mean(axis=0)\n        diffs = Xn - center\n        # PCA basis via SVD on diffs; handle degenerate case\n        try:\n            U, svals, Vt = np.linalg.svd(diffs, full_matrices=False)\n            if k_sub > 0:\n                basis = Vt.T[:, :max(1, min(k_sub, Vt.shape[0]))]  # (dim, k_sub_eff)\n            else:\n                basis = np.zeros((self.dim, 0))\n        except Exception:\n            basis = self._orthonormal_basis(max(1, min(1, self.dim)))\n        k_eff = basis.shape[1]\n        # project neighbors into subspace coordinates relative to center\n        S = (Xn - center) @ basis  # (m, k_eff)\n        # ensure we have enough points for quadratic: p = 1 + k + k*(k+1)/2\n        p = 1 + k_eff + (k_eff * (k_eff + 1)) // 2\n        if S.shape[0] < p:\n            # reduce subdim if needed\n            while k_eff > 0 and S.shape[0] < (1 + k_eff + (k_eff * (k_eff + 1)) // 2):\n                k_eff -= 1\n                if k_eff == 0:\n                    break\n                basis = basis[:, :k_eff]\n                S = (Xn - center) @ basis\n            # if still not enough for any quadratic, fallback to IDW\n        if k_eff <= 0 or S.shape[0] < (1 + k_eff + (k_eff * (k_eff + 1)) // 2):\n            # IDW fallback\n            w = 1.0 / (dists[idxs] + 1e-12)\n            w = w / w.sum()\n            pred = float((w @ Fn))\n            return pred, True\n        # build Phi\n        Phi = self._build_phi_and_map(S)\n        y = Fn.astype(float)\n        reg = 1e-8 * max(1.0, np.var(y))\n        try:\n            A = Phi.T @ Phi\n            A[np.diag_indices_from(A)] += reg + 1e-12\n            rhs = Phi.T @ y\n            theta = np.linalg.solve(A, rhs)\n            a_hat, b_hat, H_hat = self._theta_to_quad(theta, k_eff)\n            # coordinate of xq in that basis\n            sq = (xq - center) @ basis\n            pred = float(a_hat + b_hat.dot(sq) + 0.5 * float(sq.T @ (H_hat @ sq)))\n            return pred, True\n        except Exception:\n            # fallback IDW\n            w = 1.0 / (dists[idxs] + 1e-12)\n            w = w / w.sum()\n            pred = float((w @ Fn))\n            return pred, True\n\n    # -------------------- main optimization --------------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []\n        F = []\n        radii = []\n\n        # initial seeding\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            fx = float(func(x0))\n            evals += 1\n            X.append(np.array(x0, dtype=float)); F.append(fx); radii.append(self.init_radius)\n\n        if len(X) == 0 and evals < self.budget:\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            fx = float(func(x0)); evals += 1\n            X.append(np.array(x0, dtype=float)); F.append(fx); radii.append(self.init_radius)\n\n        X_arr = np.vstack(X)\n        F_arr = np.array(F, dtype=float)\n        best_idx = int(np.argmin(F_arr))\n        best_x = X_arr[best_idx].copy()\n        best_f = float(F_arr[best_idx])\n\n        stagnation = 0\n        it = 0\n\n        # main loop\n        while evals < self.budget:\n            it += 1\n            n_archive = len(X)\n            X_arr = np.vstack(X)\n            F_arr = np.array(F, dtype=float)\n\n            # determine elites\n            elite_n = min(self.elite_k, n_archive)\n            idx_sorted = np.argsort(F_arr)\n            elites = idx_sorted[:elite_n].tolist()\n\n            # build k-NN edges among elites\n            edges = set()\n            if len(elites) >= 2:\n                elite_points = X_arr[elites]\n                # pairwise distances\n                from_idx = list(range(len(elites)))\n                dmat = np.linalg.norm(elite_points[:, None, :] - elite_points[None, :, :], axis=2)\n                for i in range(len(elites)):\n                    # ignore self\n                    neighbors = np.argsort(dmat[i])  # index in elite list\n                    count = min(self.k_nn + 1, len(neighbors))  # include self maybe\n                    for j in neighbors[1:count]:  # skip self at index 0\n                        a = elites[i]; b = elites[j]\n                        if a == b: continue\n                        edges.add(tuple(sorted((int(a), int(b)))))\n            edges = list(edges)\n\n            # Virtual candidate generation from edges and triangles\n            virtual_candidates = []\n            # helper: append candidate ensuring bounds\n            def add_virtual(xv, meta=None):\n                xv = np.minimum(np.maximum(xv, lb), ub)\n                virtual_candidates.append((xv, meta))\n\n            # edge-based samples\n            if len(edges) > 0:\n                # limit edges considered\n                e_sample = min(len(edges), max(1, int(self.max_virtual // 6)))\n                chosen_edges = self.rng.choice(len(edges), size=e_sample, replace=False)\n                for ei in chosen_edges:\n                    a_idx, b_idx = edges[ei]\n                    xa = X_arr[a_idx]; xb = X_arr[b_idx]\n                    ra = radii[a_idx]; rb = radii[b_idx]\n                    d = xb - xa\n                    norm_d = np.linalg.norm(d) + 1e-12\n                    # midpoints and extrapolations\n                    for alpha in (0.0, 0.25, 0.5, 0.75, 1.0, -0.3, 1.3):\n                        xv = xa + alpha * d\n                        add_virtual(xv, meta=(\"edge\", a_idx, b_idx, alpha))\n                    # orthogonal perturbations in local 1D-2D subspace\n                    # build a perpendicular vector\n                    v = self.rng.normal(size=self.dim)\n                    v = v - (v @ d) / (norm_d**2) * d  # remove projection\n                    vnorm = np.linalg.norm(v)\n                    if vnorm < 1e-8:\n                        v = self.rng.normal(size=self.dim); vnorm = np.linalg.norm(v)\n                    v = v / (vnorm + 1e-12)\n                    scale = 0.5 * (ra + rb)\n                    for beta in (0.2, -0.2, 0.5):\n                        add_virtual(0.5 * (xa + xb) + beta * scale * v, meta=(\"edge-ortho\", a_idx, b_idx, beta))\n                    # small random barycentric with a nearby elite third\n                    if elite_n > 2:\n                        # pick one extra elite not in edge\n                        c_choices = [e for e in elites if e not in (a_idx, b_idx)]\n                        if len(c_choices) > 0:\n                            c_idx = self.rng.choice(c_choices)\n                            xc = X_arr[c_idx]\n                            for weights in [(0.4,0.4,0.2),(0.2,0.3,0.5),(0.7,0.15,0.15)]:\n                                w1,w2,w3 = weights\n                                add_virtual(w1*xa + w2*xb + w3*xc, meta=(\"triangle\", a_idx, b_idx, c_idx, weights)))\n\n            # Add surrogate-minimizer proposals around a few top centers (like ALRQS)\n            centers_considered = min(4, elite_n)\n            center_choices = elites[:centers_considered]\n            for cidx in center_choices:\n                center_x = X_arr[cidx]\n                center_r = radii[cidx]\n                k_sub = min(self.max_subdim, max(1, min(3, self.dim)))\n                basis = self._orthonormal_basis(k_sub)\n                # collect neighbors\n                dists = np.linalg.norm(X_arr - center_x[None, :], axis=1)\n                neigh_radius = max(center_r * 1.8, np.percentile(dists, min(80, max(20, int(100 * (1 - 1.0 / max(1, n_archive)))))) if n_archive > 3 else center_r * 1.8)\n                within = np.where(dists <= neigh_radius)[0].tolist()\n                p_min = 1 + k_sub + (k_sub * (k_sub + 1)) // 2\n                if len(within) < p_min:\n                    order = np.argsort(dists)\n                    for idx in order:\n                        if idx not in within:\n                            within.append(int(idx))\n                        if len(within) >= p_min:\n                            break\n                Xn = X_arr[within]\n                S = (Xn - center_x[None, :]) @ basis\n                Phi = self._build_phi_and_map(S)\n                y = np.array([F[i] for i in within], dtype=float)\n                p = Phi.shape[1]\n                reg = 1e-8 * max(1.0, np.var(y))\n                theta = None\n                try:\n                    A = Phi.T @ Phi\n                    A[np.diag_indices_from(A)] += reg + 1e-12\n                    theta = np.linalg.solve(A, Phi.T @ y)\n                except Exception:\n                    try:\n                        theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n                    except Exception:\n                        theta = None\n                if theta is not None and not np.any(np.isnan(theta)):\n                    a_hat, b_hat, H_hat = self._theta_to_quad(theta, k_sub)\n                    # solve for s_star\n                    s_star = None; lam = 1e-8\n                    for attempt in range(6):\n                        try:\n                            s_star = -np.linalg.solve(H_hat + lam * np.eye(k_sub), b_hat)\n                            break\n                        except Exception:\n                            lam = max(1e-6, lam * 10.0)\n                    if s_star is None:\n                        if np.linalg.norm(b_hat) < 1e-12:\n                            s_star = self.rng.normal(size=k_sub) * (0.3 * center_r)\n                        else:\n                            s_star = -b_hat / (np.linalg.norm(b_hat) + 1e-12) * (0.7 * center_r)\n                    x_prop = center_x + basis @ s_star\n                    # limit step\n                    disp = x_prop - center_x\n                    max_step = max(1e-12, 1.5 * center_r)\n                    dn = np.linalg.norm(disp)\n                    if dn > max_step:\n                        x_prop = center_x + disp * (max_step / dn)\n                    add_virtual(np.minimum(np.maximum(x_prop, lb), ub), meta=(\"subsur\", cidx))\n\n            # global random virtuals to add diversity\n            n_global_virtual = max(2, self.max_virtual // 50)\n            for _ in range(n_global_virtual):\n                xv = lb + self.rng.random(self.dim) * (ub - lb)\n                add_virtual(xv, meta=(\"global\",))\n\n            # deduplicate virtual candidates (by hashing rounded coords)\n            uniq = {}\n            vc_list = []\n            for xv, meta in virtual_candidates:\n                key = tuple(np.round(xv, 8))  # coarse rounding\n                if key not in uniq:\n                    uniq[key] = (xv, meta)\n                    vc_list.append((xv, meta))\n            virtual_candidates = vc_list\n            # limit pool size\n            if len(virtual_candidates) > self.max_virtual:\n                virtual_candidates = self.rng.choice(virtual_candidates, size=self.max_virtual, replace=False).tolist()\n\n            # Score virtual candidates cheaply using local quadratic predictions\n            scored = []\n            for (xv, meta) in virtual_candidates:\n                pred, ok = self._local_quadratic_predict(xv, X_arr, F_arr, k_sub=min(3, self.max_subdim), nneigh=min(30, max(6, len(X))))\n                if not ok:\n                    pred = np.mean(F_arr)\n                # relative improvement vs best\n                rel = best_f - pred\n                scored.append((float(pred), float(rel), xv, meta))\n            # sort by predicted improvement descending\n            scored.sort(key=lambda t: (-t[1], t[0]))\n\n            # pick top candidates to actually evaluate (but don't exceed budget)\n            to_eval = min(len(scored), max(1, min(self.eval_per_iter, self.budget - evals)))\n            # but also ensure we can evaluate a few more if we have larger budget remaining\n            # We'll evaluate sequentially best ones until budget or until we've done to_eval\n            eval_count = 0\n            for sidx in range(len(scored)):\n                if eval_count >= to_eval or evals >= self.budget:\n                    break\n                pred, rel, x_cand, meta = scored[sidx]\n                # skip if too close to existing archives\n                dmin = np.min(np.linalg.norm(X_arr - x_cand[None, :], axis=1))\n                tol = 1e-8 + 1e-6 * np.linalg.norm(ub - lb)\n                if dmin < tol:\n                    continue\n                # evaluate\n                f_cand = float(func(x_cand))\n                evals += 1; eval_count += 1\n                X.append(x_cand.copy()); F.append(f_cand); radii.append(self.init_radius)\n                X_arr = np.vstack(X); F_arr = np.array(F)\n                # update best\n                if f_cand < best_f:\n                    best_f = f_cand; best_x = x_cand.copy(); stagnation = 0\n                else:\n                    stagnation += 1\n                # trust update heuristics: if meta refers to endpoints, update their radii\n                if meta and isinstance(meta, tuple):\n                    if meta[0] in (\"edge\", \"edge-ortho\", \"triangle\"):\n                        # meta like (\"edge\", a_idx, b_idx, ...)\n                        ids = [int(i) for i in meta[1:3]] if meta[0] in (\"edge\",\"edge-ortho\") else [int(meta[1]), int(meta[2])]\n                        for pid in ids:\n                            # predicted for candidate relative to that point (simple)\n                            if f_cand < F[pid]:\n                                radii[pid] = min(self.max_radius, radii[pid] * 1.25)\n                            else:\n                                radii[pid] = max(self.min_radius, radii[pid] * 0.6)\n                    elif meta[0] == \"subsur\":\n                        cid = int(meta[1])\n                        # if surrogate helped, increase center radius\n                        if f_cand < F[cid]:\n                            radii[cid] = min(self.max_radius, radii[cid] * 1.3)\n                        else:\n                            radii[cid] = max(self.min_radius, radii[cid] * 0.7)\n\n            # Occasionally perform a directed short probe between best and random elite\n            if evals < self.budget and self.rng.random() < 0.15 and len(elites) > 0:\n                other = int(self.rng.choice(elites))\n                x_probe = np.clip(0.3 * best_x + 0.7 * X_arr[other], lb, ub)\n                if np.min(np.linalg.norm(X_arr - x_probe[None,:], axis=1)) > 1e-8:\n                    f_probe = float(func(x_probe)); evals += 1\n                    X.append(x_probe.copy()); F.append(f_probe); radii.append(self.init_radius*0.7)\n                    if f_probe < best_f:\n                        best_f = f_probe; best_x = x_probe.copy(); stagnation = 0\n                    else:\n                        stagnation += 1\n\n            # If stagnation, inject some global or large jump evaluations\n            if stagnation >= 10 and evals < self.budget:\n                n_global = min(4, max(1, self.dim // 3))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg)); evals += 1\n                    X.append(xg.copy()); F.append(fg); radii.append(self.init_radius)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagnation = 0\n                # a single larger Gaussian perturbation around best\n                if stagnation >= 10 and evals < self.budget:\n                    jump_scale = 0.6 * (ub - lb)\n                    xj = best_x + self.rng.normal(0, 1.0, size=self.dim) * jump_scale\n                    xj = np.minimum(np.maximum(xj, lb), ub)\n                    fj = float(func(xj)); evals += 1\n                    X.append(xj.copy()); F.append(fj); radii.append(self.init_radius)\n                    if fj < best_f:\n                        best_f = fj; best_x = xj.copy(); stagnation = 0\n                    else:\n                        # gentle reset of radii to encourage diversity\n                        radii = [max(self.min_radius, 0.6 * self.init_radius) for _ in radii]\n                        stagnation = 0\n\n            # archive trimming\n            max_archive = max(300, 6 * self.dim + 80)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = int(max(40, 4 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                # sample extra with probability favoring better ones\n                probs = np.exp(-(np.array(F) - np.min(F)) / (1e-8 + max(1.0, np.std(F))))\n                probs /= probs.sum()\n                extra_n = min(len(X) - keep_best, max_archive - keep_best)\n                extra = self.rng.choice(len(X), size=extra_n, replace=False, p=probs)\n                for e in extra:\n                    keep_set.add(int(e))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                radii = [radii[i] for i in keep_list]\n\n        best_i = int(np.argmin(F))\n        return float(F[best_i]), np.array(X[best_i], dtype=float)", "configspace": "", "generation": 8, "feedback": "In the code, line 228, in evaluate, the following error occurred:\nSyntaxError: unmatched ')'\nOn line: a = elites[i]; b = elites[j]", "error": "In the code, line 228, in evaluate, the following error occurred:\nSyntaxError: unmatched ')'\nOn line: a = elites[i]; b = elites[j]", "parent_ids": ["0c98b60a-b401-4b27-b5de-4c183326b288"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "78e7e71f-3fd3-45d7-a2c5-290bde28ce6c", "fitness": 0.0508411040230239, "name": "ACLS_EdgeDense", "description": "Adaptive Covariance-Step with Edge-Dense Recombination \u2014 combine ACLS's covariance-driven local search with an elite edge-densification generator (many midpoint/extrapolation/triangle recombinations plus orthogonal perturbations) and a cheap linear surrogate to rank and evaluate only the most promising candidates.", "code": "import numpy as np\n\nclass ACLS_EdgeDense:\n    \"\"\"\n    Adaptive Covariance-Step with Edge-Dense Recombination (ACLS-EdgeDense)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Design:\n    - maintain an elite archive of best evaluated points\n    - build many edge/triangle recombinations among elites (midpoints, extrapolations)\n      and orthogonal perturbations to increase edge density\n    - generate a large candidate pool, rank candidates with a cheap linear surrogate,\n      then evaluate only top candidates within budget\n    - adapt covariance (C) and step-size (sigma) like ACLS; rare Levy-like jumps kept\n      to retain global escapes; occasional restarts if stagnation\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_fraction=0.12, jump_prob=0.04, target_success=0.2,\n                 cov_update_rate=None, sigma_initial=None,\n                 stagnation_restart=400, elite_size=12, max_candidates=200,\n                 evals_per_round=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # tunables\n        self.init_fraction = float(init_fraction)\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n        if cov_update_rate is None:\n            # scale covariance learning with dimension\n            self.c_cov = 0.18 / max(1.0, (self.dim + 3.0))\n        else:\n            self.c_cov = float(cov_update_rate)\n        self.sigma_initial = sigma_initial\n        self.stagnation_restart = int(stagnation_restart)\n        self.elite_size = int(max(3, elite_size))\n        self.max_candidates = int(max(20, max_candidates))\n        self.evals_per_round = int(max(1, evals_per_round))\n\n        # surrogate regularization\n        self._ridge = 1e-6\n\n    def _levy_vector(self, size):\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        n = np.linalg.norm(z)\n        if n == 0 or not np.isfinite(n):\n            return self.rng.standard_normal(size=size)\n        return z / n\n\n    def _reflect_clip(self, x, lb, ub):\n        # reflect once then clip to be safe\n        below = x < lb\n        above = x > ub\n        if np.any(below):\n            x = np.where(below, lb + (lb - x), x)\n        if np.any(above):\n            x = np.where(above, ub - (x - ub), x)\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        # extract bounds (support scalar or vector)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = rng.uniform(lb, ub)  # placeholder until eval\n        archive_X = []  # list of np arrays\n        archive_f = []\n\n        # initial sampling\n        n_init = max(2, int(min(max(10, 2 * dim), int(max(1, budget * self.init_fraction)))))\n        n_init = min(n_init, budget)\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_X.append(x.copy())\n            archive_f.append(f)\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n\n        # set initial sigma and covariance\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma_initial is None:\n            sigma = max(1e-8, 0.22 * range_scale)\n        else:\n            sigma = float(self.sigma_initial)\n\n        # initial covariance: diagonal proportional to range^2\n        C = np.eye(dim) * (((range_scale / 4.0) ** 2) + 1e-9)\n\n        # success EMA and other trackers\n        p_succ = 0.0\n        ema_alpha = 0.14\n        since_improvement = 0\n        rounds_without_improve = 0\n\n        # helper to update archive and keep elites\n        def add_to_archive(x, f):\n            nonlocal archive_X, archive_f\n            archive_X.append(x.copy())\n            archive_f.append(float(f))\n            # maintain archive sorted by f (ascending)\n            if len(archive_f) > 2 * self.elite_size:\n                # partial prune to keep memory small\n                idx = np.argpartition(archive_f, self.elite_size)[:2 * self.elite_size]\n                archive_X = [archive_X[i] for i in idx]\n                archive_f = [archive_f[i] for i in idx]\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # prepare elite list\n            if len(archive_f) == 0:\n                elites_idx = []\n            else:\n                k = min(self.elite_size, len(archive_f))\n                elites_idx = np.argsort(archive_f)[:k]\n            elites = [archive_X[i].copy() for i in elites_idx]\n            elites_f = [archive_f[i] for i in elites_idx]\n\n            # compute Cholesky of C for sampling\n            reg = 1e-8 * np.eye(dim)\n            try:\n                L = np.linalg.cholesky(C + reg)\n            except np.linalg.LinAlgError:\n                L = np.linalg.cholesky(C + (1e-6 * np.eye(dim)))\n\n            # Candidate generation: dense edge/triangle + covariance + diversifiers\n            # We will generate many candidates but only evaluate top by surrogate.\n            cand_list = []\n\n            # 1) Covariance-driven candidates around x_opt and some elites\n            n_cov = max(2, min(12, self.evals_per_round * 2))\n            centers = [x_opt] + elites[:min(3, len(elites))]\n            for c in centers:\n                for _ in range(n_cov // len(centers)):\n                    if rng.random() < self.jump_prob:\n                        z = self._levy_vector(dim)\n                        dx = L.dot(z) * (2.5 + rng.random() * 1.5)\n                    else:\n                        z = rng.normal(size=dim)\n                        dx = L.dot(z)\n                    x_cand = c + sigma * dx\n                    x_cand = self._reflect_clip(x_cand, lb, ub)\n                    cand_list.append(x_cand)\n\n            # 2) Edge densification: many pair recombinations (midpoints, extrapolations)\n            if len(elites) >= 2:\n                # enumerate pairs but limit count\n                pair_indices = []\n                k = len(elites)\n                # propose random pairs with bias to best elites\n                attempts = min(200, k * (k - 1) // 1 + 50)\n                for _ in range(attempts):\n                    i = min(k - 1, int(abs(rng.normal(0, 1.0) * (k / 2.2)) % k))\n                    j = rng.integers(0, k)\n                    if i == j:\n                        j = (j + 1) % k\n                    pair_indices.append((i, j))\n                # create candidates from pairs\n                for (i, j) in pair_indices[:max(10, int(self.max_candidates // 3))]:\n                    xi = elites[i]\n                    xj = elites[j]\n                    edge = xj - xi\n                    # random interpolation/extrapolation weight\n                    alpha = rng.uniform(-0.6, 1.6)\n                    base = xi + alpha * edge\n                    # orthogonal perturbation: project random vector to orth component\n                    v = rng.normal(size=dim)\n                    e_norm = np.linalg.norm(edge)\n                    if e_norm > 1e-12:\n                        e_hat = edge / e_norm\n                        v = v - (np.dot(v, e_hat) * e_hat)\n                        v_norm = np.linalg.norm(v)\n                        if v_norm > 1e-12:\n                            v = v / v_norm\n                            orth_mag = sigma * (0.3 + rng.random() * 1.2) * (0.6 + 0.8 * rng.random())\n                            cand = base + orth_mag * v\n                        else:\n                            cand = base + sigma * rng.normal(size=dim) * 0.5\n                    else:\n                        cand = base + sigma * rng.normal(size=dim) * 0.5\n                    cand = self._reflect_clip(cand, lb, ub)\n                    cand_list.append(cand)\n                    # also add midpoint with small jitter\n                    mid = xi + 0.5 * edge + 0.25 * sigma * rng.normal(size=dim)\n                    mid = self._reflect_clip(mid, lb, ub)\n                    cand_list.append(mid)\n\n            # 3) Triangle barycentric recombinations (increase edge density in higher order)\n            if len(elites) >= 3:\n                tri_count = max(4, min(20, self.max_candidates // 8))\n                for _ in range(tri_count):\n                    ids = rng.choice(len(elites), size=3, replace=False)\n                    w = rng.random(3)\n                    w = w / np.sum(w)\n                    tri = w[0] * elites[ids[0]] + w[1] * elites[ids[1]] + w[2] * elites[ids[2]]\n                    tri += 0.25 * sigma * rng.normal(size=dim)\n                    tri = self._reflect_clip(tri, lb, ub)\n                    cand_list.append(tri)\n\n            # 4) Diversifiers: global random samples and large Levy jumps from elites\n            divers = max(4, int(self.max_candidates * 0.05))\n            for _ in range(divers):\n                if rng.random() < 0.5:\n                    cand_list.append(rng.uniform(lb, ub))\n                else:\n                    if len(elites) > 0 and rng.random() < 0.6:\n                        center = elites[rng.integers(0, len(elites))]\n                    else:\n                        center = rng.uniform(lb, ub)\n                    cand = center + sigma * 2.0 * self._levy_vector(dim)\n                    cand = self._reflect_clip(cand, lb, ub)\n                    cand_list.append(cand)\n\n            # Remove duplicates (within small tolerance)\n            if len(cand_list) > 0:\n                Xcand = np.stack(cand_list)\n                # unique rows\n                # use rounding-based hashing to avoid expensive pairwise distance\n                key = np.round((Xcand - lb) / (ub - lb + 1e-12), 6)\n                uniq_idx = np.unique(key.view([('', key.dtype)] * key.shape[1]), return_index=True)[1]\n                Xcand = Xcand[sorted(uniq_idx)]\n            else:\n                Xcand = np.empty((0, dim))\n\n            # limit candidate pool size\n            if Xcand.shape[0] > self.max_candidates:\n                # randomly subsample to keep variety\n                ids = rng.choice(Xcand.shape[0], size=self.max_candidates, replace=False)\n                Xcand = Xcand[ids]\n\n            # Surrogate predictor: simple ridge linear model on recent archive\n            # Use up to last 100 points\n            X_train = np.array(archive_X[-200:]) if len(archive_X) > 0 else np.empty((0, dim))\n            y_train = np.array(archive_f[-200:]) if len(archive_f) > 0 else np.empty((0,))\n            if X_train.shape[0] >= max(5, dim // 2):\n                # center data to improve conditioning\n                Xc = X_train - X_train.mean(axis=0)\n                yc = y_train - y_train.mean()\n                # solve ridge (Xc^T Xc + lambda I) w = Xc^T yc\n                A = Xc.T.dot(Xc) + self._ridge * np.eye(dim)\n                b = Xc.T.dot(yc)\n                try:\n                    w = np.linalg.solve(A, b)\n                    intercept = y_train.mean() - X_train.mean(axis=0).dot(w)\n                    preds = Xcand.dot(w) + intercept\n                except np.linalg.LinAlgError:\n                    preds = np.repeat(np.median(y_train) if y_train.size else 0.0, Xcand.shape[0])\n            else:\n                # not enough data: use distance to best elite heuristic\n                if len(elites) > 0:\n                    best_e = elites[0]\n                    dists = np.linalg.norm(Xcand - best_e.reshape(1, -1), axis=1)\n                    preds = np.median(archive_f) + 0.5 * (dists / (np.linalg.norm(range_vec) + 1e-12))\n                else:\n                    preds = np.zeros(Xcand.shape[0])\n\n            # rank candidates (lower predicted f is better)\n            order = np.argsort(preds)\n            Xcand = Xcand[order]\n\n            # evaluate top candidates but do not exceed a small budget chunk\n            to_eval = min(remaining, max(1, min(self.evals_per_round, Xcand.shape[0])))\n            # If very constrained budget, reduce to 1 at a time\n            to_eval = max(1, to_eval)\n\n            improved_this_round = False\n            for i in range(to_eval):\n                if evals >= budget:\n                    break\n                x_trial = Xcand[i]\n                # ensure numeric type\n                x_trial = np.asarray(x_trial, dtype=float)\n                f_trial = float(func(x_trial))\n                evals += 1\n                add_to_archive(x_trial, f_trial)\n\n                improved = False\n                if f_trial < f_opt:\n                    improved = True\n                    f_opt = f_trial\n                    x_opt = x_trial.copy()\n                    since_improvement = 0\n                    rounds_without_improve = 0\n                else:\n                    since_improvement += 1\n\n                # covariance adaptation: use displacement scaled (approx)\n                # compute normalized displacement relative to current best\n                disp = (x_trial - x_opt)\n                # fall back displacement if identical\n                if np.linalg.norm(disp) < 1e-12:\n                    # small random displacement guided by L\n                    z = rng.normal(size=dim)\n                    disp = (L.dot(z)) * (0.5 * sigma)\n                y = disp.reshape(-1, 1)\n                # update covariance with rank-one if improvement, else small forget to add diversity\n                if improved:\n                    C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                    # slightly reduce sigma on success\n                    sigma *= 0.92\n                else:\n                    # mild drift towards isotropic to avoid collapse\n                    C = (1.0 - self.c_cov * 0.2) * C + (self.c_cov * 0.2) * np.eye(dim) * (((range_scale / 6.0) ** 2) + 1e-12)\n                    # mildly increase sigma occasionally\n                    sigma *= 1.0 + 0.01 * (rng.random() - 0.4)\n\n                # clamp C to be positive definite-ish\n                C += 1e-12 * np.eye(dim)\n                # update p_succ EMA and sigma multiplicative adjustment (like ACLS)\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n                adjust = np.exp(0.6 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                sigma *= float(np.clip(adjust, 0.7, 1.5))\n                sigma = np.clip(sigma, 1e-9 * range_scale + 1e-12, 6.0 * range_scale + 1e-12)\n\n                if improved:\n                    improved_this_round = True\n\n            if not improved_this_round:\n                rounds_without_improve += 1\n\n            # occasional Levy escape from best elite when stagnating\n            if rounds_without_improve > max(3, int(dim / 3)):\n                if rng.random() < min(0.6, 0.05 + 0.02 * rounds_without_improve):\n                    # big escape\n                    z = self._levy_vector(dim)\n                    trial = x_opt + sigma * 5.0 * z\n                    trial = self._reflect_clip(trial, lb, ub)\n                    if evals < budget:\n                        f_trial = float(func(trial))\n                        evals += 1\n                        add_to_archive(trial, f_trial)\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = trial.copy()\n                            since_improvement = 0\n                            rounds_without_improve = 0\n                            # increase covariance diversity\n                            C = 0.6 * C + 0.4 * np.eye(dim) * (((range_scale / 5.5) ** 2) + 1e-12)\n                            sigma = max(sigma * 0.6, 1e-8 * range_scale)\n\n            # stagnation stronger handling: if too long without improvement try hard restart\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # try a few guided restarts biased by elites and global points\n                pushes = min(10, budget - evals)\n                for _ in range(pushes):\n                    if rng.random() < 0.6 and len(elites) > 0:\n                        center = elites[rng.integers(0, len(elites))]\n                    else:\n                        center = rng.uniform(lb, ub)\n                    cand = center + 3.0 * sigma * rng.normal(size=dim)\n                    cand = self._reflect_clip(cand, lb, ub)\n                    f_try = float(func(cand))\n                    evals += 1\n                    add_to_archive(cand, f_try)\n                    if f_try < f_opt:\n                        f_opt = f_try\n                        x_opt = cand.copy()\n                        # reset some parameters to refine around new basin\n                        sigma = max(0.7 * sigma, 0.02 * range_scale)\n                        C = 0.5 * C + 0.5 * np.eye(dim) * (((range_scale / 6.0) ** 2) + 1e-12)\n                        since_improvement = 0\n                        break\n                    if evals >= budget:\n                        break\n                # if still no improvement, raise jump probability and inflate sigma moderately\n                if since_improvement >= self.stagnation_restart:\n                    self.jump_prob = min(0.6, self.jump_prob * 1.6)\n                    sigma = min(sigma * 2.5, 12.0 * range_scale)\n\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 8, "feedback": "The algorithm ACLS_EdgeDense scored 0.051 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.13898996144246334}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.16906199307125735}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.12157395678625194}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0006417602086202789}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.002329691016006885}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.005475812027938232}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.12531213007746922}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.08555162054168974}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.11337963517366156}], "aucs": [0.13898996144246334, 0.16906199307125735, 0.12157395678625194, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.0006417602086202789, 4.999999999999449e-05, 0.002329691016006885, 4.999999999999449e-05, 4.999999999999449e-05, 0.005475812027938232, 0.12531213007746922, 0.08555162054168974, 0.11337963517366156]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3343.0, "Edges": 3342.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.999401734968591, "Degree Variance": 2.1040977575441033, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.57994757536042, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3278456034386943, "Depth Entropy": 2.163891873104646, "Assortativity": 0.0, "Average Eccentricity": 18.448399641040982, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0002991325157044571, "Average Shortest Path": 10.802724522582894, "mean_complexity": 13.4, "total_complexity": 67.0, "mean_token_count": 602.6, "total_token_count": 3013.0, "mean_parameter_count": 4.6, "total_parameter_count": 23.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "65960b23-ec2b-4ba2-8da0-929a65064903", "fitness": "-inf", "name": "EdgeDensifiedSpiralLevyFusionV2", "description": "Edge-Densified Multi-Scale Spiral-Levy Fusion \u2014 massively densify elite edges with multi-scale virtual nodes, centrality-weighted edge-subdivision and surrogate-ranked candidate pools, then concentrate evaluations on high-centrality edge corridors with adaptive per-edge budgets and light spiral/Levy refinement.", "code": "import numpy as np\n\nclass EdgeDensifiedSpiralLevyFusionV2:\n    \"\"\"\n    Edge-Densified Multi-Scale Spiral-Levy Fusion (v2)\n    - budget: max number of function evaluations\n    - dim: problem dimensionality\n    Optional args: pop_size, elite_frac, k_edges, proposals_per_edge, levy_prob, seed\n    Main innovations vs prior:\n     - Multi-scale virtual node creation around elites (small/medium jitters) to dramatically increase edge density\n     - Graph centrality scoring to prioritize high-value edges for evaluation budget\n     - Multi-scale edge subdivision (midpoints, near-ends, extrapolations) and surrogate ranking using linear+curvature proxy\n     - Cache of evaluated points to avoid duplicates\n     - Adaptive per-edge allocation that gives more evaluations to edges that link elites or have high surrogate promise\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.2, k_edges=None,\n                 proposals_per_edge=4, levy_prob=0.08, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.levy_prob = float(levy_prob)\n        self.elite_frac = float(elite_frac)\n        self.proposals_per_edge = int(proposals_per_edge)\n\n        if pop_size is None:\n            base = int(max(14, min(96, np.clip(np.sqrt(self.budget) * 1.4, 14, 96))))\n            self.pop_size = max(base, dim + 8)\n        else:\n            self.pop_size = int(pop_size)\n\n        if k_edges is None:\n            self.k_edges = max(2, min(self.pop_size - 1, int(np.clip(self.dim, 2, 18))))\n        else:\n            self.k_edges = int(k_edges)\n\n        # step scaling\n        self.sigma_init = 0.08\n        self.alpha_init = 0.35\n        # adaptation multipliers\n        self.success_increase = 1.20\n        self.failure_decrease = 0.80\n\n    def _get_bounds(self, func):\n        # permit problems that expose bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        sigma_scale = self.sigma_init * range_norm\n        alpha_scale = self.alpha_init * range_norm\n\n        evals = 0\n        budget = self.budget\n\n        # cached evaluations to avoid duplicate costly calls\n        eval_cache = {}\n\n        def key_of(x):\n            # discretize position for caching - preserves practical uniqueness but avoids duplicates\n            return tuple(np.round(x, 9))\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return None\n            k = key_of(x)\n            if k in eval_cache:\n                return eval_cache[k]\n            # clamp before evaluating\n            x_cl = np.clip(x, lb, ub)\n            val = float(func(x_cl))\n            evals += 1\n            eval_cache[k] = val\n            return val\n\n        # initialize population\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            fval = safe_eval(pop[i])\n            # safe_eval returns None if exhausted\n            if fval is None:\n                break\n            pop_f[i] = fval\n\n        # If any uninitialized individuals remain (rare), fill with random and maybe leave them inf if no budget\n        for i in range(self.pop_size):\n            if not np.isfinite(pop_f[i]) and evals < budget:\n                fval = safe_eval(pop[i])\n                if fval is None:\n                    break\n                pop_f[i] = fval\n\n        # if budget exhausted early, return best known\n        if evals >= budget:\n            idx_best = int(np.argmin(pop_f))\n            self.f_opt = float(pop_f[idx_best])\n            self.x_opt = pop[idx_best].copy()\n            return self.f_opt, self.x_opt\n\n        sigma = np.full(self.pop_size, sigma_scale)\n        alpha = np.full(self.pop_size, alpha_scale)\n        success_counts = np.zeros(self.pop_size, dtype=int)\n\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        no_improve = 0\n        max_no_improve = max(25, self.pop_size * 3)\n\n        # Build a highly-dense edge graph using multi-scale virtuals\n        def build_dense_edge_graph():\n            # select elites\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            elites = np.argsort(pop_f)[:elite_count]\n            # multi-scale virtuals: small & medium jitters around elites\n            virtuals = []\n            virtual_map = []  # maps virtual index to elite index\n            # scales: very small, small, medium multiplicative relative to sigma[elite]\n            scales = [0.18, 0.5, 1.1]\n            V_per_scale = max(2, int(np.ceil(0.8 * self.k_edges / len(scales))))\n            for e in elites:\n                base = pop[e]\n                local_sig = max(1e-12, sigma[e] if e < len(sigma) else sigma_scale)\n                for s in scales:\n                    for _ in range(V_per_scale):\n                        jitter = s * local_sig * rng.standard_normal(dim)\n                        virtuals.append(base + jitter)\n                        virtual_map.append(int(e))\n                # also add a direct mini-grid of axis-aligned perturbations for richer connectivity\n                for d in range(min(dim, 3)):\n                    shift = np.zeros(dim)\n                    shift[d] = 0.25 * local_sig * (1.0 if rng.random() < 0.5 else -1.0)\n                    virtuals.append(base + shift)\n                    virtual_map.append(int(e))\n\n            X_real = pop\n            X_virtual = np.asarray(virtuals) if len(virtuals) > 0 else np.empty((0, dim))\n            X_aug = np.vstack([X_real, X_virtual]) if X_virtual.shape[0] > 0 else X_real.copy()\n            N_aug = X_aug.shape[0]\n            # vectorized distances\n            dif = X_aug[:, None, :] - X_aug[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            edges = set()\n            n_virtuals = X_virtual.shape[0]\n            # for each virtual, connect to many nearby reals and other virtuals mapped to different elites\n            for v_idx in range(self.pop_size, self.pop_size + n_virtuals):\n                order = np.argsort(dist2[v_idx])\n                taken = 0\n                for j in order:\n                    if j == v_idx:\n                        continue\n                    # map j to real index representative\n                    if j < self.pop_size:\n                        a = virtual_map[v_idx - self.pop_size]\n                        b = int(j)\n                    else:\n                        a = virtual_map[v_idx - self.pop_size]\n                        b = virtual_map[j - self.pop_size]\n                    if a != b:\n                        edges.add(tuple(sorted((int(a), int(b)))))\n                    taken += 1\n                    if taken >= max(4, self.k_edges):\n                        break\n\n            # real-to-real: k nearest among elites prioritized, plus random augmentation\n            # compute neighbor among all real points\n            dif_real = X_real[:, None, :] - X_real[None, :, :]\n            dreal2 = np.sum(dif_real * dif_real, axis=2)\n            for i in range(self.pop_size):\n                neigh = np.argsort(dreal2[i])\n                taken = 0\n                for j in neigh:\n                    if j == i: continue\n                    edges.add(tuple(sorted((int(i), int(j)))))\n                    taken += 1\n                    if taken >= self.k_edges:\n                        break\n            # densify inter-elite connections specifically\n            for ai in range(len(elites)):\n                i = int(elites[ai])\n                for aj in range(ai + 1, len(elites)):\n                    j = int(elites[aj])\n                    edges.add(tuple(sorted((i, j))))\n\n            # random extra edges\n            extra = max(0, self.pop_size // 2)\n            for _ in range(extra):\n                a = int(rng.integers(0, self.pop_size))\n                b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    edges.add(tuple(sorted((a, b))))\n            return list(edges), elites, virtuals, virtual_map\n\n        # Surrogate ranking for candidate x along edge (i, j)\n        def surrogate_for_candidate(x_cand, xi, xj, fi, fj):\n            # linear interpolation projection along edge and perpendicular penalty\n            edge = xj - xi\n            edge_len = np.linalg.norm(edge) + 1e-12\n            t = np.dot(x_cand - xi, edge) / (edge_len * edge_len)\n            lin = (1.0 - t) * fi + t * fj\n            # curvature proxy: use difference in endpoint fitness relative to range and candidate offset\n            perp = x_cand - (xi + np.clip(t, 0.0, 1.0) * edge)\n            perp_dist = np.linalg.norm(perp)\n            grad_est = (fj - fi) / (edge_len + 1e-12)\n            # curvature penalty scaled by perpendicular and by absolute gradient\n            surrogate = lin + 0.4 * abs(grad_est) * perp_dist + 0.035 * perp_dist * range_norm\n            # additional penalty if far from current best\n            dist_best = np.linalg.norm(x_cand - x_best)\n            surrogate += 0.02 * dist_best\n            return surrogate\n\n        # Generate a pool of candidates for an edge using multi-scale subdivision\n        def edge_candidate_pool(i, j, pool_total=28):\n            xi, xj = pop[i], pop[j]\n            fi, fj = pop_f[i], pop_f[j]\n            edge_vec = xj - xi\n            edge_len = np.linalg.norm(edge_vec) + 1e-12\n            dir_unit = edge_vec / edge_len\n            candidates = []\n            # Multi-scale strategies:\n            #  - dense near endpoints\n            #  - midpoints/subdivisions\n            #  - slight orthogonal perturbations\n            #  - extrapolations beyond endpoints\n            #  - PCA-based perturbation (if enough dims)\n            # sample counts for scales\n            n_near = int(0.45 * pool_total)\n            n_mid = int(0.20 * pool_total)\n            n_perp = int(0.20 * pool_total)\n            n_extrap = pool_total - (n_near + n_mid + n_perp)\n            # near endpoints (biased Beta)\n            for _ in range(n_near):\n                if rng.random() < 0.5:\n                    w = np.clip(rng.beta(2.2, 6.0), 0.0, 1.0)  # near xi\n                else:\n                    w = 1.0 - np.clip(rng.beta(2.2, 6.0), 0.0, 1.0)  # near xj\n                base = (1.0 - w) * xi + w * xj\n                # small along-edge jitter + small perpendicular\n                along_jitter = 0.08 * (sigma[i] + sigma[j]) * rng.standard_normal() * dir_unit\n                rv = rng.standard_normal(dim)\n                rv = rv - np.dot(rv, dir_unit) * dir_unit\n                if np.linalg.norm(rv) > 0:\n                    rv = rv / np.linalg.norm(rv)\n                perp_jitter = 0.35 * (sigma[i] + sigma[j]) * rng.random() * rv\n                jitter = 0.06 * range_norm * rng.standard_normal(dim)\n                x_c = base + along_jitter + perp_jitter + jitter\n                x_c = np.clip(x_c, lb, ub)\n                sur = surrogate_for_candidate(x_c, xi, xj, fi, fj)\n                candidates.append((sur, x_c))\n            # midpoints & subdivisions\n            for k in range(1, n_mid + 1):\n                t = (k / float(n_mid + 1))\n                base = (1.0 - t) * xi + t * xj\n                # smaller orthogonal perturbations\n                rv = rng.standard_normal(dim)\n                rv = rv - np.dot(rv, dir_unit) * dir_unit\n                if np.linalg.norm(rv) > 0:\n                    rv = rv / np.linalg.norm(rv)\n                perp = 0.45 * (sigma[i] + sigma[j]) * rng.random() * rv\n                jitter = 0.03 * range_norm * rng.standard_normal(dim)\n                x_c = base + perp + jitter\n                x_c = np.clip(x_c, lb, ub)\n                sur = surrogate_for_candidate(x_c, xi, xj, fi, fj)\n                candidates.append((sur, x_c))\n            # perpendicular explorations / PCA-based perturbations\n            for _ in range(n_perp):\n                # random perpendicular heavy or light\n                rv = rng.standard_normal(dim)\n                rv = rv - np.dot(rv, dir_unit) * dir_unit\n                if np.linalg.norm(rv) > 0:\n                    rv = rv / np.linalg.norm(rv)\n                t = rng.uniform(0.1, 0.9)\n                base = (1.0 - t) * xi + t * xj\n                scale = (sigma[i] + sigma[j]) * rng.uniform(0.3, 1.2)\n                x_c = base + scale * rv + 0.02 * range_norm * rng.standard_normal(dim)\n                x_c = np.clip(x_c, lb, ub)\n                sur = surrogate_for_candidate(x_c, xi, xj, fi, fj)\n                candidates.append((sur, x_c))\n            # extrapolations beyond endpoints\n            for _ in range(n_extrap):\n                w = rng.uniform(-0.6, 1.6)\n                base = (1.0 - w) * xi + w * xj\n                rv = rng.standard_normal(dim)\n                rv = rv - np.dot(rv, dir_unit) * dir_unit\n                if np.linalg.norm(rv) > 0:\n                    rv = rv / np.linalg.norm(rv)\n                x_c = base + 0.5 * (sigma[i] + sigma[j]) * rng.standard_normal() * dir_unit + 0.6 * (sigma[i] + sigma[j]) * rng.random() * rv\n                x_c = np.clip(x_c, lb, ub)\n                sur = surrogate_for_candidate(x_c, xi, xj, fi, fj)\n                candidates.append((sur, x_c))\n            # sort and de-duplicate (by rounded key) keeping top ones\n            candidates.sort(key=lambda x: x[0])\n            uniq = []\n            seen = set()\n            for s, x in candidates:\n                k = key_of(x)\n                if k not in seen:\n                    seen.add(k)\n                    uniq.append((s, x))\n                if len(uniq) >= pool_total:\n                    break\n            return [c[1] for c in uniq]\n\n        # Triangle-based pooling for triple recombination (similar but sparser)\n        def triangle_candidates(i, j, k, pool_size=8):\n            xa, xb, xc = pop[i], pop[j], pop[k]\n            fa, fb, fc = pop_f[i], pop_f[j], pop_f[k]\n            candidates = []\n            for _ in range(pool_size):\n                r1 = rng.random()\n                r2 = rng.random()\n                sqrt_r1 = np.sqrt(r1)\n                u = 1 - sqrt_r1\n                v = sqrt_r1 * (1 - r2)\n                w = sqrt_r1 * r2\n                base = u * xa + v * xb + w * xc\n                jitter = 0.25 * (sigma[i] + sigma[j] + sigma[k]) / 3.0 * rng.standard_normal(dim)\n                x_c = np.clip(base + jitter, lb, ub)\n                sur = u * fa + v * fb + w * fc + 0.03 * np.linalg.norm(jitter) * range_norm\n                candidates.append((sur, x_c))\n            candidates.sort(key=lambda x: x[0])\n            uniq = []\n            seen = set()\n            for s, x in candidates:\n                kx = key_of(x)\n                if kx not in seen:\n                    seen.add(kx)\n                    uniq.append(x)\n                if len(uniq) >= 4:\n                    break\n            return uniq\n\n        # Main loop\n        iteration = 0\n        while evals < budget:\n            iteration += 1\n            rem = budget - evals\n            improved_in_loop = False\n\n            edges, elites, virtuals, virtual_map = build_dense_edge_graph()\n            if len(edges) == 0:\n                break\n\n            # compute simple centrality: edges connecting elites get boost; edges that connect to current best get boost\n            edge_scores = []\n            elite_set = set(elites.tolist())\n            for (i, j) in edges:\n                score = 0.0\n                if i in elite_set: score += 0.7\n                if j in elite_set: score += 0.7\n                if i == idx_best or j == idx_best: score += 1.0\n                # smaller fitness average gives more promise\n                avgf = 0.5 * (pop_f[i] + pop_f[j])\n                score += max(0.0, 1.0 - (avgf - f_best) / (abs(f_best) + 1.0 + 1e-12))\n                # encourage short edges (local refinement) and moderate-length edges for exploration\n                l = np.linalg.norm(pop[i] - pop[j]) / (range_norm + 1e-12)\n                score += 0.5 * np.exp(-((l - 0.25) ** 2) / 0.08)  # preference to medium-short edges\n                edge_scores.append(((i, j), score))\n\n            # sort by score descending to allocate budget preferentially\n            edge_scores.sort(key=lambda x: -x[1])\n\n            # adaptive per-edge allocation based on score and remaining budget\n            total_score = sum(s for (_, s) in edge_scores) + 1e-12\n            # cap number of edges to consider to save overhead if budget low\n            max_edges_considered = min(len(edge_scores), max(6, int(rem / (4 + dim))))\n            edges_considered = edge_scores[:max_edges_considered]\n\n            # For each considered edge, produce pool and evaluate top-K by surrogate\n            for (i_j, score) in edges_considered:\n                if evals >= budget:\n                    break\n                i, j = i_j\n                if i == j: \n                    continue\n                # allocate K_evals proportional to score but bounded\n                frac = score / total_score\n                K_alloc = int(np.clip(np.ceil(frac * rem * 0.18), 1, min(6, rem)))\n                # higher priority edges get more proposals to choose from\n                pool_size = 18 + int(np.clip(score * 10, 0, 30))\n                pool = edge_candidate_pool(i, j, pool_total=pool_size)\n\n                # choose top surrogate candidates (pool already surrogate-sorted) - prefer candidates closer to best\n                # sort pool by surrogate + distance-to-best factor\n                scored_pool = []\n                for x in pool:\n                    sur = surrogate_for_candidate(x, pop[i], pop[j], pop_f[i], pop_f[j])\n                    # bias prefer near x_best slightly\n                    sur += 0.02 * np.linalg.norm(x - x_best)\n                    scored_pool.append((sur, x))\n                scored_pool.sort(key=lambda x: x[0])\n                to_eval = [x for (_, x) in scored_pool[:K_alloc]]\n\n                # evaluate them with careful budget checks and cache\n                for x_prop in to_eval:\n                    if evals >= budget:\n                        break\n                    # occasional heavy-tailed push for promising edge candidates\n                    if rng.random() < 0.05:\n                        u = rng.random(dim)\n                        cauchy = np.tan(np.pi * (u - 0.5))\n                        x_prop = np.clip(x_prop + 0.12 * range_norm * cauchy, lb, ub)\n                    f_prop = safe_eval(x_prop)\n                    if f_prop is None:\n                        break\n                    # insertion policy: replace worst if better, else try replace farther endpoint if worse than candidate\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        # replace worst\n                        pop[idx_worst] = x_prop\n                        pop_f[idx_worst] = f_prop\n                        sigma[idx_worst] = sigma_scale\n                        alpha[idx_worst] = alpha_scale\n                        if f_prop < f_best:\n                            f_best = f_prop; x_best = x_prop.copy(); improved_in_loop = True\n                    else:\n                        # if candidate improves either endpoint, replace the worse endpoint\n                        if f_prop < pop_f[i] or f_prop < pop_f[j]:\n                            if pop_f[i] >= pop_f[j]:\n                                ridx = i\n                            else:\n                                ridx = j\n                            pop[ridx] = x_prop\n                            pop_f[ridx] = f_prop\n                            sigma[ridx] = min(2.0 * range_norm, sigma[ridx] * self.success_increase)\n                            alpha[ridx] = min(1.5 * range_norm, alpha[ridx] * 1.02)\n                            success_counts[ridx] += 1\n                            if f_prop < f_best:\n                                f_best = f_prop; x_best = x_prop.copy(); improved_in_loop = True\n                        else:\n                            # small chance to inject promising candidate by replacing a random poor individual\n                            if rng.random() < 0.03:\n                                # choose a random low-performance index\n                                bottoms = np.argsort(pop_f)[-max(1, min(3, self.pop_size//6)):]\n                                idx_replace = int(rng.choice(bottoms))\n                                if f_prop < pop_f[idx_replace]:\n                                    pop[idx_replace] = x_prop\n                                    pop_f[idx_replace] = f_prop\n                                    sigma[idx_replace] = sigma_scale\n                                    if f_prop < f_best:\n                                        f_best = f_prop; x_best = x_prop.copy(); improved_in_loop = True\n                            else:\n                                # penalize endpoints a bit for non-success\n                                sigma[i] *= self.failure_decrease\n                                sigma[j] *= self.failure_decrease\n\n                # occasionally do a small triangle recombination around this edge\n                if rng.random() < 0.18 and evals + 2 < budget:\n                    # pick a third neighbor k that is not i or j (prioritize elites)\n                    candidate_k = None\n                    # try elites first\n                    for c in elites:\n                        if c != i and c != j:\n                            candidate_k = int(c)\n                            break\n                    if candidate_k is None:\n                        candidate_k = int(rng.integers(0, self.pop_size))\n                        attempts = 0\n                        while candidate_k == i or candidate_k == j and attempts < 6:\n                            candidate_k = int(rng.integers(0, self.pop_size)); attempts += 1\n                    tris = triangle_candidates(i, j, candidate_k, pool_size=6)\n                    for x_tri in tris[:2]:\n                        if evals >= budget: break\n                        f_tri = safe_eval(x_tri)\n                        if f_tri is None:\n                            break\n                        idx_worst = int(np.argmax(pop_f))\n                        if f_tri < pop_f[idx_worst]:\n                            pop[idx_worst] = x_tri\n                            pop_f[idx_worst] = f_tri\n                            sigma[idx_worst] = sigma_scale\n                            if f_tri < f_best:\n                                f_best = f_tri; x_best = x_tri.copy(); improved_in_loop = True\n\n            # Spiral-directed per-individual refinement (light-weight)\n            for i in range(self.pop_size):\n                if evals >= budget:\n                    break\n                # skip many individuals to save budget; focus on elites or high sigma\n                if rng.random() < 0.40 and pop_f[i] > np.median(pop_f):\n                    continue\n\n                xi = pop[i].copy()\n                fi = pop_f[i]\n                d = x_best - xi\n                nd = np.linalg.norm(d) + 1e-12\n                dir_vec = d / nd\n\n                # small randomized spiral rotation for variety\n                if dim > 1 and rng.random() < 0.5:\n                    a, b = rng.choice(dim, size=2, replace=False)\n                    theta = rng.uniform(-np.pi / 3, np.pi / 3)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    da, db = d[a], d[b]\n                    ra = ca * da - sb * db\n                    rb = sb * da + ca * db\n                    d_rot = d.copy()\n                    d_rot[a], d_rot[b] = ra, rb\n                    norm_dr = np.linalg.norm(d_rot) + 1e-12\n                    dir_vec = d_rot / norm_dr\n\n                mov_scale = alpha[i] * (0.6 + rng.random() * 0.8)\n                move = mov_scale * dir_vec\n                move += sigma[i] * rng.standard_normal(dim)\n\n                # Levy occasionally\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    move += 0.16 * range_norm * cauchy\n\n                x_new = np.clip(xi + move, lb, ub)\n\n                # opposition sample occasionally\n                if rng.random() < 0.05 and evals < budget:\n                    x_opp = lb + ub - xi\n                    x_opp = np.clip(x_opp, lb, ub)\n                    f_opp = safe_eval(x_opp)\n                    if f_opp is None:\n                        break\n                    if f_opp < fi:\n                        pop[i] = x_opp; pop_f[i] = f_opp\n                        sigma[i] *= self.success_increase\n                        alpha[i] *= 1.02\n                        if f_opp < f_best:\n                            f_best = f_opp; x_best = x_opp.copy(); improved_in_loop = True\n                        continue\n\n                if evals >= budget:\n                    break\n                f_new = safe_eval(x_new)\n                if f_new is None:\n                    break\n                if f_new < fi or rng.random() < 0.01:\n                    pop[i] = x_new\n                    pop_f[i] = f_new\n                    sigma[i] *= self.success_increase\n                    alpha[i] *= 1.02\n                    success_counts[i] += 1\n                    if f_new < f_best:\n                        f_best = f_new; x_best = x_new.copy(); improved_in_loop = True\n                else:\n                    sigma[i] *= self.failure_decrease\n                    alpha[i] *= 0.985\n\n            # clamp parameters\n            sigma = np.clip(sigma, 1e-9 * range_norm, 3.0 * range_norm)\n            alpha = np.clip(alpha, 1e-9 * range_norm, 1.8 * range_norm)\n\n            if improved_in_loop:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # If stagnating, aggressive edge-based densification around the best:\n            if no_improve >= max_no_improve and evals < budget:\n                no_improve = 0\n                n_reset = max(1, self.pop_size // 2)\n                for _ in range(n_reset):\n                    if evals >= budget:\n                        break\n                    if rng.random() < 0.85:\n                        # multi-scale perturbation about x_best\n                        scale = rng.choice([0.02, 0.06, 0.15], p=[0.5, 0.35, 0.15])\n                        newp = x_best + scale * range_vec * rng.standard_normal(dim)\n                    else:\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = safe_eval(newp)\n                    if f_new is None:\n                        break\n                    idx = int(rng.integers(0, self.pop_size))\n                    pop[idx] = newp; pop_f[idx] = f_new\n                    sigma[idx] = sigma_scale\n                    alpha[idx] = alpha_scale\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # occasional random injection\n            if rng.random() < 0.05 and evals < budget:\n                r = rng.uniform(lb, ub)\n                fr = safe_eval(r)\n                if fr is None:\n                    break\n                idx_worst = int(np.argmax(pop_f))\n                if fr < pop_f[idx_worst]:\n                    pop[idx_worst] = r; pop_f[idx_worst] = fr\n                    sigma[idx_worst] = sigma_scale\n                    if fr < f_best:\n                        f_best = fr; x_best = r.copy()\n\n            # if budget low, break to final local refinement\n            if budget - evals < max(12, dim * 4):\n                break\n\n        # Final local pattern search refinement around best (coordinate mesh)\n        mesh = 0.10 * range_norm\n        mesh_min = 1e-6 * range_norm\n        while evals < budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                if evals >= budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["efb33a12-13f2-4a5d-9b4e-785879562267"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 5913.0, "Edges": 5912.0, "Max Degree": 39.0, "Min Degree": 1.0, "Mean Degree": 1.9996617622188397, "Degree Variance": 2.1217654868128593, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.449537892791128, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3361293861308872, "Depth Entropy": 2.061201032431086, "Assortativity": 0.0, "Average Eccentricity": 20.929815660409268, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.00016911889058007779, "Average Shortest Path": 11.145518452381362, "mean_complexity": 16.22222222222222, "total_complexity": 146.0, "mean_token_count": 554.2222222222222, "total_token_count": 4988.0, "mean_parameter_count": 3.0, "total_parameter_count": 27.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "2793a51c-f4ab-4552-b28c-2e065366ccf2", "fitness": "-inf", "name": "ACLS_EdgeDense", "description": "Adaptive Covariance + Edge-Densification (ACLS-EdgeDense) \u2014 combine adaptive multivariate covariance steps and rare L\u00e9vy escapes with aggressive, surrogate-ranked dense recombinations along an elite edge graph to concentrate evaluations on promising inter-elite \"edges\" while keeping global exploration.", "code": "import numpy as np\nimport math\nimport itertools\n\nclass ACLS_EdgeDense:\n    \"\"\"\n    Adaptive Covariance-Step with Edge-Dense Recombination and L\u00e9vy Escapes.\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional tuning parameters can be passed as keyword arguments.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_fraction=0.12, jump_prob=0.05, target_success=0.2,\n                 cov_update_rate=None, sigma_initial=None,\n                 stagnation_restart=400, elite_size=None,\n                 edge_candidate_mult=6, candidate_score_penalty=0.02):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # meta-parameters\n        self.init_fraction = float(init_fraction)\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n        if cov_update_rate is None:\n            # smaller for larger dims\n            self.c_cov = 0.18 / max(1.0, (self.dim + 2.0))\n        else:\n            self.c_cov = float(cov_update_rate)\n\n        self.sigma_initial = sigma_initial\n        self.stagnation_restart = int(stagnation_restart)\n\n        # elite and edge settings\n        self.elite_size = elite_size if elite_size is not None else max(6, min(30, int(6 + dim)))\n        # how many raw candidates per elite pair (multiplicative)\n        self.edge_candidate_mult = int(edge_candidate_mult)\n        # penalty weight for surrogate scoring (relative to function scale)\n        self.candidate_score_penalty = float(candidate_score_penalty)\n\n        # internal rng seeds ok\n\n    def _levy_vector(self, size):\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        norm = np.linalg.norm(z)\n        if norm == 0:\n            return self.rng.standard_normal(size=size)\n        return z / norm\n\n    def _get_bounds(self, func):\n        # Try to read bounds if provided; otherwise default to [-5,5]\n        lb = None\n        ub = None\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            lb = getattr(b, \"lb\", None)\n            ub = getattr(b, \"ub\", None)\n        if lb is None or ub is None:\n            # fallback to common default\n            lb = -5.0\n            ub = 5.0\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # determine initial sample count\n        n_init = max(1, int(min(max(12, 3 * dim), max(1, int(budget * self.init_fraction)))))\n        n_init = min(n_init, budget)\n\n        # store archive of evaluated points to seed elite graph\n        # we'll keep only elites but maintain counts\n        elites_x = []\n        elites_f = []\n\n        # initial uniform sampling\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n            elites_x.append(x.copy())\n            elites_f.append(f)\n\n        elites_x = np.array(elites_x, dtype=float)\n        elites_f = np.array(elites_f, dtype=float)\n\n        # limit elites to elite_size best\n        if elites_x.shape[0] > 0:\n            order = np.argsort(elites_f)\n            sel = order[:min(self.elite_size, elites_x.shape[0])]\n            elites_x = elites_x[sel].copy()\n            elites_f = elites_f[sel].copy()\n        else:\n            elites_x = np.empty((0, dim), dtype=float)\n            elites_f = np.array([], dtype=float)\n\n        # scales and initial sigma\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n        if self.sigma_initial is None:\n            sigma = max(1e-8, 0.22 * range_scale)\n        else:\n            sigma = float(self.sigma_initial)\n\n        # initial covariance\n        C = np.eye(dim) * (((range_scale / 4.0) ** 2) + 1e-9)\n\n        p_succ = 0.0\n        ema_alpha = 0.14\n        since_improvement = 0\n\n        # helper: update elites with new point\n        def update_elites(x_new, f_new):\n            nonlocal elites_x, elites_f\n            # append then trim\n            elites_x = np.vstack([elites_x, x_new.copy()])\n            elites_f = np.concatenate([elites_f, np.array([float(f_new)])])\n            # keep best elite_size\n            order = np.argsort(elites_f)\n            keep = order[:min(self.elite_size, elites_x.shape[0])]\n            elites_x = elites_x[keep].copy()\n            elites_f = elites_f[keep].copy()\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n\n            # precompute decomposition for covariance sampling\n            reg = 1e-9 * np.eye(dim)\n            try:\n                L = np.linalg.cholesky(C + reg)\n            except np.linalg.LinAlgError:\n                L = np.linalg.cholesky(C + (1e-6 * np.eye(dim)))\n\n            # decide strategy probabilities adaptively:\n            # use edge-densification more when we have multiple elites\n            prob_edge = 0.55 if elites_x.shape[0] >= 3 else 0.25\n            prob_cov = 0.35\n            prob_levy = 0.10\n\n            r = rng.random()\n            if r < prob_edge and elites_x.shape[0] >= 2:\n                # Edge-dense recombination step\n                # enumerate candidate edges (cap pair count)\n                k = elites_x.shape[0]\n                all_pairs = list(itertools.combinations(range(k), 2))\n                # weight pairs by improvement potential (lower f is better)\n                pair_scores = []\n                for (i, j) in all_pairs:\n                    # prefer connecting good elites and moderately distant ones\n                    fi, fj = elites_f[i], elites_f[j]\n                    dist = np.linalg.norm(elites_x[i] - elites_x[j])\n                    pair_score = min(fi, fj) - 0.02 * (dist / (range_scale + 1e-12))\n                    pair_scores.append(pair_score)\n                # sort pairs by predicted promise (ascending)\n                idx_pairs = np.argsort(pair_scores)\n                max_pairs = min(len(all_pairs), max(10, int(2.5 * self.elite_size)))\n                chosen_pairs = [all_pairs[i] for i in idx_pairs[:max_pairs]]\n\n                # generate many candidates along each chosen edge (but cheaply score them)\n                candidates = []\n                candidate_info = []  # store endpoints indices and a surrogate baseline\n                for (i, j) in chosen_pairs:\n                    xi = elites_x[i]\n                    xj = elites_x[j]\n                    fi = elites_f[i]\n                    fj = elites_f[j]\n                    edge_vec = xj - xi\n                    d = np.linalg.norm(edge_vec)\n                    if d < 1e-12:\n                        # skip degenerate\n                        continue\n                    # normalized edge direction\n                    e_dir = edge_vec / d\n\n                    # propose types per edge\n                    n_per_edge = max(3, self.edge_candidate_mult)\n                    for _m in range(n_per_edge):\n                        ty = rng.random()\n                        if ty < 0.35:\n                            # midpoint biased toward better endpoint\n                            if fi < fj:\n                                weigh = rng.uniform(0.0, 0.6)\n                                x_c = xi * (1.0 - weigh) + xj * weigh\n                            else:\n                                weigh = rng.uniform(0.0, 0.6)\n                                x_c = xj * (1.0 - weigh) + xi * weigh\n                        elif ty < 0.6:\n                            # pure midpoint with slight offset\n                            t = 0.5 + rng.normal(scale=0.06)\n                            x_c = xi * (1.0 - t) + xj * t\n                        elif ty < 0.85:\n                            # extrapolation beyond the better endpoint\n                            if fi < fj:\n                                base = xi\n                                dir_v = (xi - xj)\n                            else:\n                                base = xj\n                                dir_v = (xj - xi)\n                            alpha = rng.uniform(0.12, 0.6)\n                            x_c = base + alpha * dir_v\n                        else:\n                            # orthogonal perturbation to edge midpoint\n                            mid = 0.5 * (xi + xj)\n                            # random normal then remove projection onto e_dir\n                            v = rng.normal(size=dim)\n                            v -= e_dir * (v @ e_dir)\n                            vn = v / (np.linalg.norm(v) + 1e-12)\n                            scale = rng.normal(scale=0.5) * sigma * (0.5 + rng.random())\n                            x_c = mid + scale * vn\n\n                        # clip and slightly reflect to maintain bounds\n                        x_c = np.clip(x_c, lb, ub)\n                        # compute cheap surrogate predicted f: inverse-distance weighted endpoint mix\n                        d1 = np.linalg.norm(x_c - xi) + 1e-12\n                        d2 = np.linalg.norm(x_c - xj) + 1e-12\n                        w1 = 1.0 / d1\n                        w2 = 1.0 / d2\n                        f_pred = (w1 * fi + w2 * fj) / (w1 + w2)\n                        # penalty: Mahalanobis wrt C and distance to mean\n                        mean_ij = 0.5 * (xi + xj)\n                        dx_m = x_c - mean_ij\n                        # approximate Mahalanobis using diagonal of C for speed\n                        diagC = np.diag(C)\n                        mahal = np.sum((dx_m ** 2) / (diagC + 1e-12))\n                        # normalize\n                        penalty = self.candidate_score_penalty * (mahal / max(1.0, dim))\n                        score = f_pred + penalty\n                        candidates.append(x_c)\n                        candidate_info.append((i, j, score, f_pred))\n                        # cap candidate pool to avoid explosion\n                        if len(candidates) >= min(1200, max(80, self.edge_candidate_mult * len(chosen_pairs))):\n                            break\n                    if len(candidates) >= min(1200, max(80, self.edge_candidate_mult * len(chosen_pairs))):\n                        break\n\n                if len(candidates) == 0:\n                    # fallback to covariance step\n                    strategy = 'cov'\n                else:\n                    # select top candidates by surrogate score to evaluate (but not too many)\n                    cand_scores = np.array([ci[2] for ci in candidate_info], dtype=float)\n                    # choose evaluate_count proportional to remaining budget and candidates\n                    eval_count = min(remaining, max(1, int(min(30, max(1, round(0.06 * len(candidates)))))))\n                    top_idx = np.argsort(cand_scores)[:eval_count]\n                    improved_any = False\n                    for idx in top_idx:\n                        x_try = candidates[idx].copy()\n                        # small adaptive jitter to candidates to help escape flats\n                        jitter = 0.6 * sigma * rng.normal(size=dim) * rng.random()\n                        x_try += jitter\n                        x_try = np.clip(x_try, lb, ub)\n                        f_try = float(func(x_try))\n                        evals += 1\n\n                        if f_try < f_opt:\n                            improved_any = True\n                            # update covariance using displacement relative to current best\n                            y = (x_try - x_opt) / (sigma + 1e-12)\n                            y = y.reshape(-1, 1)\n                            C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                            C += 1e-12 * np.eye(dim)\n                            f_opt = f_try\n                            x_opt = x_try.copy()\n                            since_improvement = 0\n                            # update elite set\n                            update_elites(x_try, f_try)\n                        else:\n                            since_improvement += 1\n                            # still add to elites archive if reasonably good\n                            update_elites(x_try, f_try)\n\n                        # update EMA success\n                        p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if f_try < f_opt else 0.0)\n                        # adjust sigma\n                        adjust = math.exp(0.6 * (p_succ - self.target_success) / max(1e-6, math.sqrt(dim)))\n                        sigma *= float(np.clip(adjust, 0.6, 1.6))\n                        sigma = np.clip(sigma, 1e-8 * range_scale + 1e-12, 6.0 * range_scale + 1e-12)\n\n                        if evals >= budget:\n                            break\n\n                    # if no improvement from the dense-edge batch, slightly encourage heavy tails\n                    if not improved_any:\n                        if rng.random() < 0.5:\n                            # small temporary bump to jump_prob\n                            self.jump_prob = min(0.45, self.jump_prob * 1.25)\n                            sigma = min(sigma * 1.2, 8.0 * range_scale)\n                    continue  # go to next main loop iteration\n\n            # else do covariance-based sampling or levy\n            # choose number of tries in this mini-batch\n            batch = min(remaining, max(1, int(6 + dim // 2)))\n\n            for _b in range(batch):\n                if rng.random() < self.jump_prob:\n                    # L\u00e9vy-like heavy jump (normalized cauchy scaled)\n                    z = self._levy_vector(dim)\n                    dx = L.dot(z) * 3.5\n                else:\n                    z = rng.normal(size=dim)\n                    dx = L.dot(z)\n\n                if x_opt is None:\n                    center = rng.uniform(lb, ub)\n                else:\n                    center = x_opt\n\n                x_trial = center + sigma * dx\n                # reflect bounds\n                below_lb = x_trial < lb\n                above_ub = x_trial > ub\n                if np.any(below_lb) or np.any(above_ub):\n                    x_trial = np.where(below_lb, lb + (lb - x_trial), x_trial)\n                    x_trial = np.where(above_ub, ub - (x_trial - ub), x_trial)\n                    x_trial = np.clip(x_trial, lb, ub)\n\n                f_trial = float(func(x_trial))\n                evals += 1\n\n                improved = False\n                if f_trial < f_opt:\n                    improved = True\n                    # covariance rank-one update using normalized displacement\n                    y = (x_trial - center) / (sigma + 1e-12)\n                    y = y.reshape(-1, 1)\n                    C = (1.0 - self.c_cov) * C + self.c_cov * (y @ y.T)\n                    C += 1e-12 * np.eye(dim)\n                    f_opt = f_trial\n                    x_opt = x_trial.copy()\n                    since_improvement = 0\n                    update_elites(x_trial, f_trial)\n                else:\n                    since_improvement += 1\n                    update_elites(x_trial, f_trial)\n\n                # update EMA and sigma\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n                adjust = math.exp(0.6 * (p_succ - self.target_success) / max(1e-6, math.sqrt(dim)))\n                sigma *= float(np.clip(adjust, 0.6, 1.6))\n                sigma = np.clip(sigma, 1e-8 * range_scale + 1e-12, 6.0 * range_scale + 1e-12)\n\n                if evals >= budget:\n                    break\n\n            # stagnation handling\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # perform a concentrated densification around elites and a few random restarts\n                pushes = min(12, budget - evals)\n                improved_flag = False\n                for _p in range(pushes):\n                    if rng.random() < 0.6 and elites_x.shape[0] > 0:\n                        # pick a random elite center\n                        idx = rng.integers(0, elites_x.shape[0])\n                        center = elites_x[idx]\n                    else:\n                        center = rng.uniform(lb, ub)\n                    x_try = center + 2.0 * sigma * rng.normal(size=dim)\n                    x_try = np.clip(x_try, lb, ub)\n                    f_try = float(func(x_try))\n                    evals += 1\n                    if f_try < f_opt:\n                        improved_flag = True\n                        f_opt = f_try\n                        x_opt = x_try.copy()\n                        sigma = max(sigma * 0.75, 1e-8 * range_scale)\n                        C = 0.6 * C + 0.4 * np.eye(dim) * ((range_scale / 6.0) ** 2)\n                        since_improvement = 0\n                        update_elites(x_try, f_try)\n                        break\n                    else:\n                        since_improvement += 1\n                        update_elites(x_try, f_try)\n                    if evals >= budget:\n                        break\n                if not improved_flag:\n                    # escalate escapes\n                    self.jump_prob = min(0.6, self.jump_prob * 1.5)\n                    sigma = min(sigma * 2.2, 12.0 * range_scale)\n\n        # return best found\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 8, "feedback": "In the code, line 165, in __call__, the following error occurred:\nNameError: name 'itertools' is not defined\nOn line: all_pairs = list(itertools.combinations(range(k), 2))", "error": "In the code, line 165, in __call__, the following error occurred:\nNameError: name 'itertools' is not defined\nOn line: all_pairs = list(itertools.combinations(range(k), 2))", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3107.0, "Edges": 3106.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9993562922433215, "Degree Variance": 2.170582141160118, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.859747545582048, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.324062075560085, "Depth Entropy": 2.295433652606258, "Assortativity": 1.578261469208301e-08, "Average Eccentricity": 19.028644995172193, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.000321853878339234, "Average Shortest Path": 11.183479715019427, "mean_complexity": 11.8, "total_complexity": 59.0, "mean_token_count": 560.0, "total_token_count": 2800.0, "mean_parameter_count": 4.2, "total_parameter_count": 21.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "66ee387c-87c8-46a4-813c-e9705785d549", "fitness": "-inf", "name": "DenseEdgeSurrogateDensification", "description": "Dense-Edge Cascaded Surrogate Densification \u2014 aggressively create massive virtual node clouds along high-weight edges (multi-scale midpoints, cascades and barycentric mixes), score them with a lightweight PCA-linear + nearest-neighbor uncertainty ensemble acquisition (favoring low prediction and high uncertainty), and iteratively densify successful edge-hubs while adaptively expanding the pool multiplier on stagnation to build extremely dense, informative edge neighborhoods under a tight evaluation budget.", "code": "import numpy as np\n\nclass DenseEdgeSurrogateDensification:\n    \"\"\"\n    Dense Edge Surrogate Densification (refined)\n\n    One-line: Dense-Edge Cascaded Surrogate Densification.\n\n    - budget: total function evaluations\n    - dim: dimensionality\n    Optional tunables provided in __init__.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.18,\n                 k_neighbors=8, proposals_per_cycle=36, pool_multiplier=10,\n                 tri_prob=0.5, mix_prob=0.22, levy_prob=0.06,\n                 trust_init=0.12, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.pool_multiplier = int(pool_multiplier)\n        self.tri_prob = float(tri_prob)\n        self.mix_prob = float(mix_prob)\n        self.levy_prob = float(levy_prob)\n        self.trust_init = float(trust_init)\n\n        if pop_size is None:\n            base = int(np.clip(np.sqrt(self.budget) * 0.95, 10, 220))\n            self.pop_size = max(base, self.dim + 8)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptive constants\n        self.trust_expand = 1.25\n        self.trust_shrink = 0.70\n        self.edge_boost = 1.6\n        self.edge_decay = 0.986\n        self.min_trust = 1e-8\n        self.max_trust = 1e3\n\n        # exploration weight for acquisition: how much uncertainty to prefer\n        self.explore_weight_init = 0.28\n        self.explore_weight = float(self.explore_weight_init)\n        # stagnation multiplier increases the pool density when trapped\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 3\n        self.max_pool_multiplier = 60\n\n        # safety caps\n        self.max_pop_size = max(self.pop_size, 4)\n\n    def _get_bounds(self, func):\n        # prefer func-provided bounds if available, otherwise -5..5\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = float(np.linalg.norm(range_vec))\n        trust = float(self.trust_init) * max(1e-12, range_norm)\n\n        # adjust initial population to budget\n        pop_size = min(self.pop_size, max(4, int(self.budget)))\n        pop_size = int(pop_size)\n        self.pop_size = pop_size\n\n        # initialize population uniformly and evaluate (until budget)\n        pop = rng.uniform(lb, ub, size=(pop_size, dim))\n        pop_f = np.full(pop_size, np.inf)\n        evals = 0\n        for i in range(pop_size):\n            if evals >= self.budget: break\n            pop_f[i] = float(func(pop[i])); evals += 1\n\n        # edge weight matrix symmetric\n        edge_w = np.ones((pop_size, pop_size), dtype=float)\n        np.fill_diagonal(edge_w, 0.0)\n\n        # best so far\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        cycle = 0\n        no_improve_cycles = 0\n\n        # main loop: generate huge virtual pool along edges & tri-mixes, rank with surrogate+uncertainty, evaluate top-K\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            act_proposals = min(self.proposals_per_cycle, remaining)\n            if act_proposals <= 0:\n                break\n\n            # dynamic pool multiplier: increase when stagnating to densify further\n            pool_mult = int(min(self.pool_multiplier * (1 + 0.6 * max(0, no_improve_cycles - 1)), self.max_pool_multiplier))\n            pool_size = max(act_proposals * pool_mult, act_proposals + 6)\n            pool_size = int(pool_size)\n\n            improved = False\n\n            # slight decay of edges for forgetfulness\n            edge_w *= self.edge_decay\n            edge_w[edge_w < 1e-12] = 1e-12\n            np.fill_diagonal(edge_w, 0.0)\n\n            # elites\n            elite_count = max(2, int(np.ceil(self.elite_frac * pop_size)))\n            order = np.argsort(pop_f)\n            elites = order[:elite_count]\n\n            # pairwise squared distances\n            dif = pop[:, None, :] - pop[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n\n            # build dense edge set: clique among elites, k-NN per elite, cross-links, and weighted repetition by edge weight\n            edges = set()\n            # clique among elites\n            for i in range(len(elites)):\n                for j in range(i + 1, len(elites)):\n                    a = int(elites[i]); b = int(elites[j])\n                    edges.add((min(a, b), max(a, b)))\n            # k nearest for each elite\n            for a in elites:\n                neigh = np.argsort(dist2[a])\n                cnt = 0\n                for nb in neigh:\n                    if nb == a: continue\n                    edges.add((min(int(a), int(nb)), max(int(a), int(nb))))\n                    cnt += 1\n                    if cnt >= self.k_neighbors: break\n            # extra near-elite cross-links\n            for a in elites:\n                neigh = np.argsort(dist2[a])\n                for nb in neigh[self.k_neighbors:self.k_neighbors + max(2, self.k_neighbors // 2)]:\n                    if nb == a: continue\n                    edges.add((min(int(a), int(nb)), max(int(a), int(nb))))\n\n            # random ties\n            rand_pairs = max(4, pop_size // 3)\n            for _ in range(rand_pairs):\n                a = int(rng.integers(0, pop_size)); b = int(rng.integers(0, pop_size))\n                if a != b:\n                    edges.add((min(a, b), max(a, b)))\n\n            # if extremely sparse, ensure at least one edge\n            if len(edges) == 0:\n                edges = {(0, 1)}\n            edges = list(edges)\n\n            # Build lightweight PCA + ridge linear surrogate from elites (cheap)\n            use_pca = False\n            V = None; X_base_mean = None; w_ridge = None; Z_base = None\n            try:\n                basis_count = max(1, min(dim, elite_count - 1))\n                X_base = pop[elites].astype(float)\n                X_base_mean = X_base.mean(axis=0, keepdims=True)\n                Xc = X_base - X_base_mean\n                U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                m = min(basis_count, Vt.shape[0])\n                V = Vt[:m].copy()\n                Z_base = (Xc @ V.T)\n                y = pop_f[elites].astype(float)\n                Z_aug = np.hstack([np.ones((Z_base.shape[0], 1)), Z_base])\n                alpha = 1e-6 * (1.0 + np.var(y))\n                A = Z_aug.T @ Z_aug\n                A[np.diag_indices_from(A)] += alpha\n                w_ridge = np.linalg.solve(A, Z_aug.T @ y)\n                use_pca = True\n            except Exception:\n                use_pca = False\n\n            # pool generation: heavily densify high-weight edges via cascaded subdivisions + barycentric mixes\n            pool = np.empty((pool_size, dim), dtype=float)\n            pool_meta = [None] * pool_size\n            # precompute cumulative edge weights to sample edges with probability proportional to weight\n            tri_u = np.triu_indices(pop_size, k=1)\n            # construct an array mapping edge tuple -> weight for quick sampling\n            # create a list of edges repeated proportional to their weight (coarse) to bias sampling\n            edge_weights_list = np.array([edge_w[a, b] for (a, b) in edges], dtype=float)\n            ew_sum = edge_weights_list.sum()\n            if ew_sum <= 0:\n                probs = np.full(len(edges), 1.0 / len(edges))\n            else:\n                probs = edge_weights_list / ew_sum\n\n            for i in range(pool_size):\n                r = rng.random()\n                meta = {}\n                # triangular barycentric mixes among elites (dense hubs)\n                if use_pca and r < self.tri_prob and len(elites) >= 3:\n                    if rng.random() < 0.82:\n                        ids = rng.choice(elites, size=3, replace=False)\n                    else:\n                        ids = rng.choice(pop_size, size=3, replace=False)\n                    a, b, c = [int(x) for x in ids]\n                    Xa, Xb, Xc = pop[a], pop[b], pop[c]\n                    wdir = rng.gamma(shape=0.95, scale=1.0, size=3)\n                    wdir /= (np.sum(wdir) + 1e-12)\n                    x = wdir[0] * Xa + wdir[1] * Xb + wdir[2] * Xc\n                    # cascade extrapolation occasionally (+/-)\n                    if rng.random() < 0.28:\n                        centroid = (Xa + Xb + Xc) / 3.0\n                        factor = rng.normal(0.15, 0.45)\n                        x = centroid + factor * (x - centroid)\n                    tri_spread = (np.linalg.norm(Xa - Xb) + np.linalg.norm(Xa - Xc) + np.linalg.norm(Xb - Xc)) / (3.0 + 1e-12)\n                    noise_scale = 0.45 * max(trust, 0.35 * tri_spread)\n                    x = x + noise_scale * rng.standard_normal(dim)\n                    meta['type'] = 'tri'; meta['ids'] = (a, b, c)\n                # multi-node mix\n                elif r < self.tri_prob + self.mix_prob and pop_size >= 4:\n                    k = rng.integers(3, min(6, pop_size) + 1)\n                    ids = rng.choice(pop_size, size=k, replace=False)\n                    coeff = rng.random(k)\n                    coeff /= coeff.sum()\n                    x = np.sum(pop[ids] * coeff[:, None], axis=0)\n                    x = x + 0.42 * trust * rng.standard_normal(dim)\n                    meta['type'] = 'mix'; meta['ids'] = tuple(int(xi) for xi in ids)\n                else:\n                    # pick an edge weighted by its current weight\n                    e_idx = int(rng.choice(len(edges), p=probs))\n                    a, b = edges[e_idx]\n                    Xa, Xb = pop[a], pop[b]\n                    # multi-scale interpolation: choose stage t in [0,1] possibly outside for extrapolation\n                    if rng.random() < 0.10:\n                        t = rng.uniform(-0.6, 1.6)\n                    else:\n                        # bias towards better endpoint\n                        if pop_f[a] < pop_f[b]:\n                            t = rng.beta(2.3, 1.05)\n                        else:\n                            t = 1.0 - rng.beta(2.3, 1.05)\n                    x = (1 - t) * Xa + t * Xb\n\n                    # edge cascade: if edge weight is high, insert small-scale subdivisions along the edge\n                    ew = edge_w[a, b]\n                    subdiv_noise = 0.6 * trust * rng.standard_normal(dim)\n                    # orthogonal jitter scaled by edge length to escape strictly 1D\n                    edge_dir = Xb - Xa\n                    nedge = np.linalg.norm(edge_dir)\n                    if nedge > 1e-12:\n                        unit = edge_dir / nedge\n                        orth = rng.standard_normal(dim)\n                        orth -= orth.dot(unit) * unit\n                        if np.linalg.norm(orth) > 1e-12:\n                            orth = orth / np.linalg.norm(orth)\n                            x = x + 0.06 * min(trust, 0.4 * nedge) * orth * rng.random()\n                    x = x + subdiv_noise * (0.6 + 0.6 * min(1.0, ew / (1.0 + ew)))\n                    meta['type'] = 'edge'; meta['ids'] = (int(a), int(b))\n\n                    # occasionally produce cascade multi-points: if a heavy edge, generate additional virtual neighbors\n                    if rng.random() < min(0.18, 0.18 * (ew / (1.0 + ew))):\n                        # produce a small cluster around x (kept as virtual only)\n                        x = x + 0.18 * trust * rng.standard_normal(dim)\n                # occasional Levy-like jump\n                if rng.random() < self.levy_prob * 0.85:\n                    try:\n                        cauch = rng.standard_cauchy(dim)\n                    except AttributeError:\n                        u = rng.random(dim); cauch = np.tan(np.pi * (u - 0.5))\n                    x = x + 0.06 * range_norm * cauch\n                # clip to bounds and store\n                pool[i] = np.clip(x, lb, ub)\n                pool_meta[i] = meta\n\n            # Surrogate scoring: PCA linear prediction + nearest-neighbor local mean; also estimate uncertainty = min distance to pop\n            if use_pca and w_ridge is not None:\n                Xcent = pool - X_base_mean\n                Zpool = Xcent @ V.T\n                Zpool_aug = np.hstack([np.ones((Zpool.shape[0], 1)), Zpool])\n                preds_lin = Zpool_aug @ w_ridge\n            else:\n                preds_lin = None\n\n            # nearest neighbor prediction and uncertainty\n            # use a small subset of population for speed but ensure includes best and some elites\n            subset_size = min(pop_size, max(12, elite_count + 6))\n            idx_sub = rng.choice(pop_size, size=subset_size, replace=False)\n            pop_sub = pop[idx_sub]\n            pop_f_sub = pop_f[idx_sub]\n\n            # compute distances (pool x subset)\n            d2sub = np.sum((pool[:, None, :] - pop_sub[None, :, :]) ** 2, axis=2)\n            idx_nn = np.argmin(d2sub, axis=1)\n            dist_nn = np.sqrt(d2sub[np.arange(pool.shape[0]), idx_nn])  # Euclidean to nearest evaluated point\n            nn_f = pop_f_sub[idx_nn]\n\n            # blended prediction\n            if preds_lin is not None:\n                # blend linear surrogate with local NN (robust)\n                preds = 0.55 * preds_lin + 0.45 * nn_f\n            else:\n                preds = nn_f.copy()\n\n            # acquisition: lower is better; encourage high uncertainty (large dist_nn)\n            # normalize uncertainty to [0,1] using range_norm\n            unc = dist_nn / (range_norm + 1e-12)\n            # dynamic explore weight: increase exploration when stagnating\n            explore = self.explore_weight * (1.0 + 0.5 * max(0, no_improve_cycles - 1))\n            acquisition = preds - explore * unc  # prefer low predicted and high uncertainty\n\n            # apply slight bias to prefer candidates near strong edges (proportional to average edge weight among contributing ids)\n            bias = np.zeros(pool_size)\n            for i in range(pool_size):\n                meta = pool_meta[i]\n                if meta is None: continue\n                if 'ids' in meta:\n                    ids = meta['ids']\n                    if isinstance(ids, (tuple, list, np.ndarray)):\n                        # average pairwise weight\n                        if len(ids) >= 2:\n                            wsum = 0.0; cnt = 0\n                            for ii in range(len(ids)):\n                                for jj in range(ii+1, len(ids)):\n                                    a = int(ids[ii]); b = int(ids[jj])\n                                    wsum += edge_w[min(a, b), max(a, b)]\n                                    cnt += 1\n                            if cnt > 0:\n                                bias[i] = -0.08 * (wsum / (cnt + 1e-12))\n            acquisition = acquisition + bias\n\n            # select top-K by acquisition\n            K = min(act_proposals, self.budget - evals)\n            if K <= 0:\n                break\n            selected = np.argsort(acquisition)[:K]\n\n            # Evaluate selected candidates (actual func calls) and integrate results\n            for s in selected:\n                if evals >= self.budget: break\n                x_prop = pool[int(s)].copy()\n                f_prop = float(func(x_prop)); evals += 1\n\n                # replacement: try to replace a worse neighbor (within small nearest set) else replace global worst if beneficial\n                d2 = np.sum((pop - x_prop) ** 2, axis=1)\n                nearest = np.argsort(d2)[:min(8, pop_size)]\n                replaced_idx = -1\n                # attempt to replace the worst among nearest\n                worst_order = nearest[np.argsort(pop_f[nearest])[::-1]]\n                for cand in worst_order:\n                    if f_prop < pop_f[cand]:\n                        replaced_idx = int(cand); break\n\n                if replaced_idx >= 0:\n                    pop[replaced_idx] = x_prop.copy()\n                    pop_f[replaced_idx] = f_prop\n                    # boost edges linking replaced node with contributors metadata\n                    meta = pool_meta[int(s)]\n                    if meta is not None and 'ids' in meta:\n                        ids = tuple(int(i) for i in meta['ids'])\n                        for a in ids:\n                            if a == replaced_idx: continue\n                            i1, i2 = min(a, replaced_idx), max(a, replaced_idx)\n                            edge_w[i1, i2] = edge_w[i1, i2] * self.edge_boost + 1e-12\n                            edge_w[i2, i1] = edge_w[i1, i2]\n                        # also densify the clique among contributors\n                        if len(ids) >= 2:\n                            for ii in range(len(ids)):\n                                for jj in range(ii + 1, len(ids)):\n                                    a, b = int(ids[ii]), int(ids[jj])\n                                    i1, i2 = min(a, b), max(a, b)\n                                    edge_w[i1, i2] = edge_w[i1, i2] + 0.2\n                                    edge_w[i2, i1] = edge_w[i1, i2]\n                    # grow trust on success\n                    trust = min(self.max_trust, trust * self.trust_expand)\n                    improved = True\n                else:\n                    idx_worst = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_worst]:\n                        pop[idx_worst] = x_prop.copy()\n                        pop_f[idx_worst] = f_prop\n                        trust = min(self.max_trust, trust * self.trust_expand)\n                        improved = True\n                    else:\n                        trust = max(self.min_trust, trust * self.trust_shrink)\n\n                # update best\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n\n                # ensure edge_w stays symmetric, non-zero diagonal zero\n                np.fill_diagonal(edge_w, 0.0)\n                edge_w[edge_w < 1e-12] = 1e-12\n\n                # If a heavy edge produced success, cascade: generate additional virtual nodes on that edge immediately (add more weight)\n                meta = pool_meta[int(s)]\n                if improved and meta is not None and 'ids' in meta and len(meta['ids']) >= 2:\n                    ids = tuple(int(i) for i in meta['ids'])\n                    # boost pairwise\n                    for ii in range(len(ids)):\n                        for jj in range(ii + 1, len(ids)):\n                            a, b = ids[ii], ids[jj]\n                            i1, i2 = min(a, b), max(a, b)\n                            edge_w[i1, i2] = edge_w[i1, i2] * (1.0 + 0.9) + 1e-12\n                            edge_w[i2, i1] = edge_w[i1, i2]\n\n                # stop if budget used\n                if evals >= self.budget:\n                    break\n\n            # update stagnation counters and exploration weight\n            if improved:\n                no_improve_cycles = 0\n                self.stagnation_counter = 0\n                # slightly reduce exploration weight to intensify exploitation after progress\n                self.explore_weight = max(0.06, self.explore_weight * 0.94)\n            else:\n                no_improve_cycles += 1\n                self.stagnation_counter += 1\n                # increase exploration weight slowly\n                self.explore_weight = min(1.2, self.explore_weight * 1.06)\n                # when multiple cycles without improvement, bump pool_multiplier (more densification)\n                if no_improve_cycles >= self.stagnation_threshold:\n                    # modestly increase pool multiplier but cap\n                    self.pool_multiplier = int(min(self.max_pool_multiplier, max(self.pool_multiplier, int(self.pool_multiplier * 1.4 + 2))))\n                    no_improve_cycles = 0  # reset after increasing density\n\n            # if only few evals remain, break to final stage\n            if self.budget - evals <= max(8, dim * 2):\n                break\n\n        # final refinement: focused dense edge local search around x_best and elite hubs\n        # sample along edges connecting x_best to top elites and do small line searches (limited by budget)\n        # collect top hubs: nodes with small fitness and high sum(edge_w)\n        node_strength = edge_w.sum(axis=0)\n        top_nodes = np.argsort(pop_f + 0.01 * (node_strength.max() - node_strength))[:min(pop_size, max(6, int(0.06 * pop_size)))]\n        # create candidates by interpolation between x_best and these nodes and small random steps\n        final_candidates = []\n        for a in top_nodes:\n            Xa = pop[int(a)]\n            for t in (0.15, 0.35, 0.5, 0.7):\n                final_candidates.append(0.5 * (x_best + Xa) + (t - 0.5) * 0.18 * trust * rng.standard_normal(dim))\n            # short directed line towards node\n            vec = Xa - x_best\n            final_candidates.append(x_best + 0.18 * vec + 0.06 * trust * rng.standard_normal(dim))\n            final_candidates.append(0.5 * (x_best + Xa) + 0.06 * trust * rng.standard_normal(dim))\n        # evaluate until budget\n        for x in final_candidates:\n            if evals >= self.budget:\n                break\n            x_try = np.clip(x, lb, ub)\n            f_try = float(func(x_try)); evals += 1\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy()\n\n        # small coordinate hill-climb if budget left\n        step = 0.18 * range_norm\n        step_min = max(1e-8, 1e-6 * range_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            dims = list(range(dim))\n            rng.shuffle(dims)\n            for d in dims:\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.46\n            else:\n                step = min(0.9 * range_norm, step * 1.08)\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "In the code, line 890, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities contain NaN", "error": "In the code, line 890, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities contain NaN", "parent_ids": ["45bacdb1-c496-4bcb-8a7b-2ccd06451f70"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4591.0, "Edges": 4590.0, "Max Degree": 61.0, "Min Degree": 1.0, "Mean Degree": 1.999564365062078, "Degree Variance": 2.761054046771972, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.748932130991932, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3180880250500955, "Depth Entropy": 2.258318486820256, "Assortativity": 1.0916785833616717e-08, "Average Eccentricity": 19.34894358527554, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00021781746896101068, "Average Shortest Path": 11.042898652236616, "mean_complexity": 32.0, "total_complexity": 96.0, "mean_token_count": 1338.0, "total_token_count": 4014.0, "mean_parameter_count": 5.666666666666667, "total_parameter_count": 17.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "a66bcb4a-d4ad-4cd2-bcfd-3ff1a37e3197", "fitness": 0.43559722809356016, "name": "EdgeDensifiedAdaptiveSubspaceExploratorySearch", "description": "Edge-Densified Adaptive Subspace Exploratory Search (ED-ASES) \u2014 combine adaptive low-dimensional subspace exploration with a dense elite-edge recombination engine that generates many edge- and triangle-based recombinations, ranks them via cheap linear interpolation predictions, and evaluates only the most promising candidates to boost \"edge density\" and exploit productive connections under strict budget.", "code": "import numpy as np\n\nclass EdgeDensifiedAdaptiveSubspaceExploratorySearch:\n    \"\"\"\n    Edge-Densified Adaptive Subspace Exploratory Search (ED-ASES)\n\n    Combines:\n      - Adaptive randomized low-dimensional subspace explorations (trust-region style).\n      - An elite archive of best points.\n      - Dense edge- and triangle-based recombinations among elites:\n          * many candidates along edges (midpoints, extrapolations, barycentric mixes)\n          * small orthogonal perturbations to escape manifolds\n      - Cheap ranking of a large candidate pool via linear-interpolation prediction\n        (no extra function evaluations), then evaluating only top predicted candidates.\n      - Adaptive trust radius expansion/contraction by success/failure.\n      - Periodic global diversification and opposite checks.\n\n    The func is assumed noiseless and evaluated at most `budget` times.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, init_radius=2.5, min_radius=1e-6,\n                 max_radius=2.5, stagnation_patience=6, elite_size=None,\n                 edge_phase_freq=10, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: dimensionality\n          init_radius, min_radius, max_radius: trust-region radius bounds\n          stagnation_patience: iterations w/o improvement before forced diversifications\n          elite_size: number of elite points to keep (default: min(12, max(4, dim)))\n          edge_phase_freq: run an edge-densification phase every this many major iterations\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.stagnation_patience = int(stagnation_patience)\n        self.edge_phase_freq = int(edge_phase_freq)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        if elite_size is None:\n            self.elite_size = min(12, max(4, self.dim))\n        else:\n            self.elite_size = int(elite_size)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        evals = 0\n\n        # Initial seed: a small diverse pool to create an initial elite graph\n        seed_pool = min(max(6, self.elite_size * 2), max(6, self.budget // 20))\n        seed_pool = max(2, seed_pool)\n        pool = []\n        for _ in range(seed_pool):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            pool.append((f, x.copy()))\n            evals += 1\n        # ensure at least one evaluated (budget may be 0 handled by user)\n        if len(pool) == 0:\n            return None, None\n\n        # sort pool and create elite archive\n        pool.sort(key=lambda t: t[0])\n        archive = pool[:self.elite_size]\n        f_best, x_best = archive[0][0], archive[0][1].copy()\n\n        # state variables\n        radius = float(self.init_radius)\n        stagnation = 0\n        major_iter = 0\n\n        # helper: update archive with a new candidate (f,x)\n        def update_archive(fx, xx):\n            nonlocal archive, f_best, x_best\n            # Add and maintain sorted archive\n            archive.append((fx, xx.copy()))\n            archive.sort(key=lambda t: t[0])\n            if len(archive) > self.elite_size:\n                archive = archive[:self.elite_size]\n            if fx < f_best:\n                f_best = fx\n                x_best = xx.copy()\n\n        # main loop: alternate subspace batches with occasional dense-edge phases\n        while evals < self.budget:\n            major_iter += 1\n            remaining = self.budget - evals\n\n            # SUBSPACE EXPLOITATION PHASE\n            # choose subspace dimension k biased to small dims but sometimes larger\n            raw = self.rng.beta(1.6, 6.0)\n            k = int(np.clip(np.ceil(raw * self.dim), 1, self.dim))\n\n            # decide batch size: scale with radius and problem size but never exceed budget\n            approx_batch = int(np.clip(6 * (1.0 + (radius / max(1e-12, self.max_radius))) * (1.0 + k / max(1, self.dim)), 1, 60))\n            batch = max(1, min(remaining, approx_batch))\n\n            improved_in_batch = False\n            sigma = max(radius / np.sqrt(k), 1e-12)\n\n            for _ in range(batch):\n                if evals >= self.budget:\n                    break\n\n                # choose coordinates\n                if k == self.dim:\n                    inds = np.arange(self.dim)\n                else:\n                    inds = self.rng.choice(self.dim, size=k, replace=False)\n\n                step = self.rng.normal(0.0, sigma, size=k)\n                x_cand = x_best.copy()\n                x_cand[inds] += step\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n\n                f_cand = float(func(x_cand))\n                evals += 1\n                update_archive(f_cand, x_cand)\n\n                if f_cand < f_best:\n                    improved_in_batch = True\n                    stagnation = 0\n                    # expand radius modestly\n                    radius = min(self.max_radius, radius * 1.18 + 1e-12)\n                    # short greedy probe along the same direction\n                    if evals < self.budget:\n                        probe = x_cand.copy()\n                        probe[inds] += 0.4 * step\n                        probe = np.minimum(np.maximum(probe, lb), ub)\n                        f_probe = float(func(probe))\n                        evals += 1\n                        update_archive(f_probe, probe)\n                        if f_probe < f_best:\n                            radius = min(self.max_radius, radius * 1.25)\n                # else, continue\n\n            if not improved_in_batch:\n                stagnation += 1\n                radius *= 0.62\n            else:\n                stagnation = 0\n\n            # occasional opposite check\n            if evals < self.budget and stagnation >= 2 and self.rng.random() < 0.22:\n                x_opp = lb + ub - x_best\n                x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                f_opp = float(func(x_opp))\n                evals += 1\n                update_archive(f_opp, x_opp)\n                if f_opp < f_best:\n                    radius = min(self.max_radius, radius * 1.18)\n                    stagnation = 0\n\n            # EDGE-DENSIFICATION PHASE (periodic or triggered by stagnation)\n            do_edge_phase = (major_iter % self.edge_phase_freq == 0) or (stagnation >= max(2, self.stagnation_patience // 2))\n            if do_edge_phase and evals < self.budget and len(archive) >= 2:\n                # build dense candidate pool from edges and a few triangles\n                elites = [np.array(x) for (_, x) in archive]\n                elites_f = [float(f) for (f, _) in archive]\n                E = len(elites)\n\n                candidate_pool = []\n                # create candidates along every edge with several t values biased toward the better endpoint\n                t_values = [0.0, 0.25, 0.5, 0.75, 1.0, 1.25]  # includes extrapolation\n                for i in range(E):\n                    xi = elites[i]\n                    fi = elites_f[i]\n                    for j in range(i + 1, E):\n                        xj = elites[j]\n                        fj = elites_f[j]\n                        # determine better endpoint and orient t values to prefer that side\n                        for t in t_values:\n                            # candidate x = (1-t)*xi + t*xj  (t outside [0,1] extrapolates)\n                            x_c = xi * (1.0 - t) + xj * t\n                            # orthogonal small perturbation: make a few variants\n                            # create a single orth perturbation vector (random and orth to edge)\n                            v = xj - xi\n                            v_norm = np.linalg.norm(v)\n                            if v_norm > 1e-12:\n                                # random orth component\n                                z = self.rng.normal(size=self.dim)\n                                z -= np.dot(z, v) / (v_norm ** 2) * v\n                                z_norm = np.linalg.norm(z)\n                                if z_norm > 1e-12:\n                                    z = z / z_norm\n                                    # magnitude scaled by edge length and current radius to encourage small escapes\n                                    orth_scale = 0.12 * (v_norm / max(1e-12, (ub - lb).mean())) * radius\n                                    x_orth = np.minimum(np.maximum(x_c + orth_scale * z, lb), ub)\n                                    # rank both base candidate and orth-perturbed candidate\n                                    # predicted score: linear interpolation of f-values (cheap)\n                                    f_pred = fi * (1.0 - t) + fj * t\n                                    candidate_pool.append((f_pred, x_c.copy()))\n                                    candidate_pool.append((f_pred - 0.01 * abs(fj - fi), x_orth.copy()))\n                                else:\n                                    f_pred = fi * (1.0 - t) + fj * t\n                                    candidate_pool.append((f_pred, x_c.copy()))\n                            else:\n                                # identical elites (rare) -> just add midpoint\n                                f_pred = fi * (1.0 - t) + fj * t\n                                candidate_pool.append((f_pred, x_c.copy()))\n\n                # add a limited number of triangle barycentric mixes to increase connectivity\n                num_triangles = min(6, max(0, E * (E - 1) * (E - 2) // 6))\n                num_triangles = min(num_triangles, 12)\n                if E >= 3 and num_triangles > 0:\n                    # pick random triples and generate barycentric points biased toward better vertex\n                    triples = []\n                    for _ in range(num_triangles):\n                        a, b, c = self.rng.choice(E, size=3, replace=False)\n                        triples.append((a, b, c))\n                    for (a, b, c) in triples:\n                        xa, xb, xc = elites[a], elites[b], elites[c]\n                        fa, fb, fc = elites_f[a], elites_f[b], elites_f[c]\n                        # weights biased by inverse f (better gets more weight)\n                        inv = np.array([1.0 / (1e-12 + fa), 1.0 / (1e-12 + fb), 1.0 / (1e-12 + fc)])\n                        w = inv / inv.sum()\n                        x_tri = xa * w[0] + xb * w[1] + xc * w[2]\n                        f_pred_tri = fa * w[0] + fb * w[1] + fc * w[2]\n                        candidate_pool.append((f_pred_tri, x_tri.copy()))\n                        # small perturb variants\n                        perturb = self.rng.normal(0, 0.05 * (ub - lb).mean(), size=self.dim)\n                        x_tri2 = np.minimum(np.maximum(x_tri + perturb, lb), ub)\n                        candidate_pool.append((f_pred_tri - 0.005 * np.std([fa, fb, fc]), x_tri2.copy()))\n\n                # prune and prioritize candidate_pool\n                if len(candidate_pool) > 0:\n                    # keep unique candidates (by hashing rounded coordinates) to avoid duplicates\n                    # rounding tolerance to group near-identical candidates\n                    uniq = {}\n                    for fpred, xc in candidate_pool:\n                        key = tuple(np.round(xc, decimals=7))\n                        if key in uniq:\n                            # keep minimal predicted f\n                            if fpred < uniq[key][0]:\n                                uniq[key] = (fpred, xc)\n                        else:\n                            uniq[key] = (fpred, xc)\n                    candidate_pool = list(uniq.values())\n\n                    # sort by predicted value (ascending) and distance to current best (prefer near-best)\n                    # compute a composite score: predicted + 0.12 * normalized_dist\n                    xs = np.array([x for (_, x) in candidate_pool])\n                    fps = np.array([fp for (fp, _) in candidate_pool])\n                    if xs.shape[0] > 0:\n                        dists = np.linalg.norm(xs - x_best[None, :], axis=1)\n                        normalized_dist = dists / max(1e-12, np.linalg.norm(ub - lb))\n                        composite = fps + 0.12 * normalized_dist * abs(f_best + 1.0)\n                        order = np.argsort(composite)\n                        candidate_pool_sorted = [candidate_pool[i][1] for i in order]\n                    else:\n                        candidate_pool_sorted = []\n\n                    # decide how many to evaluate: scale with elite count and remaining budget\n                    max_eval_edge_phase = min(max(5, self.elite_size * 3), max(1, remaining // 6))\n                    max_eval_edge_phase = min(max_eval_edge_phase, len(candidate_pool_sorted))\n                    # evaluate top candidates until budget exhausted or allowed count used\n                    eval_count = 0\n                    for idx in range(max_eval_edge_phase):\n                        if evals >= self.budget:\n                            break\n                        x_try = candidate_pool_sorted[idx]\n                        # clip bounds safety\n                        x_try = np.minimum(np.maximum(x_try, lb), ub)\n                        f_try = float(func(x_try))\n                        evals += 1\n                        update_archive(f_try, x_try)\n                        eval_count += 1\n                        if f_try < f_best:\n                            # reward: expand radius and reset stagnation\n                            radius = min(self.max_radius, radius * 1.25)\n                            stagnation = 0\n                    if eval_count == 0:\n                        # penalize radius if no edge candidate was evaluated or none improved\n                        radius *= 0.85\n                        stagnation += 1\n\n            # forced diversification if stagnated too long\n            if stagnation >= self.stagnation_patience and evals < self.budget:\n                num_global = min(max(2, self.dim // 2), max(1, (self.budget - evals) // 10))\n                improved = False\n                for _ in range(num_global):\n                    if evals >= self.budget:\n                        break\n                    x_rand = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_rand = float(func(x_rand))\n                    evals += 1\n                    update_archive(f_rand, x_rand)\n                    if f_rand < f_best:\n                        improved = True\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n                        break\n                if not improved and evals < self.budget:\n                    # near-random jump around a random elite to diversify\n                    anchor_idx = self.rng.integers(0, len(archive))\n                    anchor = archive[anchor_idx][1]\n                    jump_scale = 0.6 * (ub - lb)\n                    x_jump = anchor + self.rng.normal(0, 1.0, size=self.dim) * jump_scale\n                    x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                    f_jump = float(func(x_jump))\n                    evals += 1\n                    update_archive(f_jump, x_jump)\n                    if f_jump < f_best:\n                        radius = min(self.max_radius, self.init_radius)\n                    else:\n                        radius = max(self.min_radius, self.init_radius * 0.55)\n                    stagnation = 0  # restart cycles after diversification\n\n            # ensure radius remains in sensible bounds\n            radius = float(np.clip(radius, self.min_radius, self.max_radius))\n\n            # if too many evals consumed (safety), break\n            if evals >= self.budget:\n                break\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 8, "feedback": "The algorithm EdgeDensifiedAdaptiveSubspaceExploratorySearch scored 0.436 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5573973561518653}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.56272208800102}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.5566993349520802}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.33218960021341293}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.3260970648675019}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.2937281357185376}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.2201826015321665}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.19116937669349887}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.1634502291462988}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1345910916089721}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.2040763089369142}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.11421669028272607}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9661078417683663}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9476552740249284}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9636754275051135}], "aucs": [0.5573973561518653, 0.56272208800102, 0.5566993349520802, 0.33218960021341293, 0.3260970648675019, 0.2937281357185376, 0.2201826015321665, 0.19116937669349887, 0.1634502291462988, 0.1345910916089721, 0.2040763089369142, 0.11421669028272607, 0.9661078417683663, 0.9476552740249284, 0.9636754275051135]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2841.0, "Edges": 2840.0, "Max Degree": 20.0, "Min Degree": 1.0, "Mean Degree": 1.9992960225272791, "Degree Variance": 1.8697636719623567, "Transitivity": 0.0, "Max Depth": 19.0, "Min Depth": 2.0, "Mean Depth": 9.136085626911315, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3357197815069413, "Depth Entropy": 2.333096533067618, "Assortativity": 0.0, "Average Eccentricity": 20.867300246392116, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.00035198873636043646, "Average Shortest Path": 11.400481877537665, "mean_complexity": 16.25, "total_complexity": 65.0, "mean_token_count": 617.5, "total_token_count": 2470.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "7516fc34-d43c-4696-bca7-3524b4b1cf90", "fitness": 0.4269980030551103, "name": "EdgeDensifiedAdaptiveSubspaceSearch", "description": "Edge-Densified Adaptive Subspace Search (ED-ASES) \u2014 maintain an elite archive, build a dense k-NN \"edge graph\" among elites and generate many virtual nodes along/around edges (midpoints, extrapolations, orthogonal perturbations), cheaply rank candidates by linear interpolation of endpoint values, evaluate only the most promising while continuing adaptive low-dimensional subspace explorations around the current best; adapt trust radius by successes to concentrate search along dense productive edges and rapidly densify promising basins.", "code": "import numpy as np\n\nclass EdgeDensifiedAdaptiveSubspaceSearch:\n    \"\"\"\n    Edge-Densified Adaptive Subspace Search (ED-ASES)\n\n    Key ideas:\n      - Keep an elite archive (small set of best found solutions).\n      - Build a k-NN graph among elites (\"edges\") and create many virtual nodes\n        along/around those edges (interpolations, extrapolations, orthogonal perturbations).\n      - Use cheap linear interpolation of endpoint function values to rank a large pool\n        of edge-derived candidates, then evaluate only the top-ranked ones given remaining budget.\n      - Combine this with adaptive low-dimensional Gaussian subspace probes around the current best.\n      - Adapt trust radius (global) by success/failure and bias edge sampling towards edges connecting good elites.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, init_radius=2.5, min_radius=1e-6,\n                 max_radius=2.5, stagnation_patience=6, rng=None,\n                 max_archive=40, k_nn=4, edge_candidates_pool=200, max_edge_evals_per_cycle=12):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          init_radius: starting trust-region radius\n          min_radius: minimum radius before forced diversification\n          max_radius: maximum radius allowed\n          stagnation_patience: iterations without improvement before diversification\n          rng: optional numpy.random.Generator\n          max_archive: max number of elites to keep\n          k_nn: number of nearest neighbors per elite to form edges\n          edge_candidates_pool: how many virtual edge candidates to generate/score (cheaply) per densification\n          max_edge_evals_per_cycle: maximum true function evaluations taken from edge candidates per densification cycle\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.max_archive = int(max_archive)\n        self.k_nn = int(k_nn)\n        self.edge_candidates_pool = int(edge_candidates_pool)\n        self.max_edge_evals_per_cycle = int(max_edge_evals_per_cycle)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shapes\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        evals = 0\n        # initial sample: few random starts to seed archive\n        n_init = min(5, max(1, self.dim // 2))\n        archive_x = []\n        archive_f = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n        # ensure at least one sample\n        if len(archive_x) == 0:\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n\n        # initialize best\n        idx_best = int(np.argmin(archive_f))\n        x_best = archive_x[idx_best].copy()\n        f_best = float(archive_f[idx_best])\n\n        # sort archive\n        def prune_archive():\n            # keep unique-ish elites by distance and best f values\n            if len(archive_x) <= self.max_archive:\n                return\n            # sort by f\n            order = np.argsort(archive_f)\n            kept = []\n            kept_f = []\n            for i in order:\n                xi = archive_x[i]\n                fi = archive_f[i]\n                too_close = False\n                for xk in kept:\n                    if np.linalg.norm(xi - xk) < 1e-6:\n                        too_close = True\n                        break\n                if not too_close:\n                    kept.append(xi.copy()); kept_f.append(fi)\n                if len(kept) >= self.max_archive:\n                    break\n            archive_x[:] = kept\n            archive_f[:] = kept_f\n\n        radius = float(self.init_radius)\n        stagnation = 0\n        # edge success weights (persist across cycles) keyed by frozenset((i,j)) indices of archive entries.\n        # We'll instead weight edges by endpoint quality at runtime to avoid stale indices.\n\n        # Main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # 1) Adaptive low-dimensional subspace exploration around x_best (exploit)\n            # Subspace dimension selection skewed to small dims\n            raw = self.rng.beta(1.6, 5.0)\n            k = int(np.clip(np.ceil(raw * self.dim), 1, self.dim))\n\n            approx_batch = int(np.clip(6 * (1.0 + (radius / max(self.max_radius, 1e-12))), 1, 50))\n            batch = max(1, min(remaining, approx_batch))\n            sigma = max(radius / np.sqrt(max(1, k)), 1e-12)\n            improved_in_batch = False\n\n            for _ in range(batch):\n                if evals >= self.budget:\n                    break\n                # choose coords\n                if k == self.dim:\n                    indices = np.arange(self.dim)\n                else:\n                    indices = self.rng.choice(self.dim, size=k, replace=False)\n                step = self.rng.normal(0.0, sigma, size=k)\n                x_cand = x_best.copy()\n                x_cand[indices] += step\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand.copy()\n                    archive_x.append(x_best.copy()); archive_f.append(f_best)\n                    improved_in_batch = True\n                    stagnation = 0\n                    radius = min(self.max_radius, radius * 1.18 + 1e-12)\n                    # small greedy probe\n                    if evals < self.budget:\n                        probe = x_cand.copy()\n                        probe[indices] += 0.5 * step\n                        probe = np.minimum(np.maximum(probe, lb), ub)\n                        f_probe = float(func(probe)); evals += 1\n                        if f_probe < f_best:\n                            f_best = f_probe; x_best = probe.copy()\n                            archive_x.append(x_best.copy()); archive_f.append(f_best)\n                            radius = min(self.max_radius, radius * 1.25)\n                # else nothing\n            if not improved_in_batch:\n                stagnation += 1\n                radius *= 0.62  # shrink on failures\n\n            # 2) Archive pruning and ensure top representative elites exist\n            prune_archive()\n            # ensure x_best is in archive and archive values correct\n            # If best not present, add it\n            present = any(np.allclose(x_best, xa, atol=1e-12) for xa in archive_x)\n            if not present:\n                archive_x.append(x_best.copy()); archive_f.append(f_best)\n                prune_archive()\n\n            # Build K-NN edges among elites (compute distances)\n            n_elite = len(archive_x)\n            if n_elite >= 2 and evals < self.budget:\n                X = np.vstack(archive_x)\n                F = np.array(archive_f)\n                # compute pairwise distances\n                # For small archives, simple loops ok\n                dists = np.linalg.norm(X[:, None, :] - X[None, :, :], axis=2)\n                # for each node, consider k nearest neighbors (excluding self)\n                k_use = min(self.k_nn, n_elite - 1)\n                edges = set()\n                # weight edges by endpoint qualities: lower F => higher weight\n                f_ranks = np.argsort(F)\n                rank_of = np.empty_like(f_ranks)\n                rank_of[f_ranks] = np.arange(n_elite)\n                for i in range(n_elite):\n                    nei_idx = np.argsort(dists[i])[1:k_use+1]\n                    for j in nei_idx:\n                        if i == j: continue\n                        edges.add(tuple(sorted((i, int(j)))))\n                edges = list(edges)\n                # generate many virtual candidates along edges\n                # Candidate generation parameters\n                lambdas = np.array([0.25, 0.5, 0.75, -0.25, 1.25])  # include small extrapolations\n                # If too many edges, prefer edges among top-ranked elites\n                # Score edge importance by sum of ranks (smaller better)\n                edge_scores = []\n                for (i, j) in edges:\n                    score = rank_of[i] + rank_of[j]\n                    edge_scores.append((score, (i, j)))\n                edge_scores.sort()\n                # Keep only a subset of edges to densify based on archive size and pool size\n                max_edges_to_use = max(1, min(len(edge_scores), self.edge_candidates_pool // len(lambdas)))\n                selected_edges = [e for _, e in edge_scores[:max_edges_to_use]]\n\n                # build candidate pool (cheaply score using linear interpolation of endpoint F)\n                pool = []\n                for (i, j) in selected_edges:\n                    xi = X[i]; xj = X[j]\n                    fi = float(F[i]); fj = float(F[j])\n                    d = xj - xi\n                    dnorm2 = np.dot(d, d)\n                    # ensure we can form orthonormal direction later\n                    for lam in lambdas:\n                        x_virt = (1.0 - lam) * xi + lam * xj\n                        # predicted f via linear interpolation\n                        f_pred = (1.0 - lam) * fi + lam * fj\n                        # encourage points nearer to better endpoint: subtract small bonus based on endpoint qualities\n                        # (we only use this as ranking heuristic)\n                        endpoint_bonus = -0.01 * (min(fi, fj) - f_best)  # prefer edges with good endpoints\n                        # compute distance to center (prefer moderate distances)\n                        dist_to_best = np.linalg.norm(x_virt - x_best)\n                        pool.append((f_pred + endpoint_bonus + 1e-8 * dist_to_best, i, j, lam, x_virt))\n                if len(pool) > 0:\n                    # sort pool predicting low f first\n                    pool.sort(key=lambda e: e[0])\n                    # select top candidates to actually evaluate (but limit per cycle)\n                    max_edge_eval = min(self.max_edge_evals_per_cycle, remaining, max(1, len(pool)))\n                    # allow some extra diversity: take top and a few from near-top (e.g., top 2/3)\n                    pick_count = max_edge_eval\n                    # Evaluate sequentially with orthogonal perturbations added to candidates\n                    picks = pool[:min(len(pool), max(len(pool)//2, pick_count*2))]  # shortlist but still bounded\n                    picks = picks[:min(len(picks), self.edge_candidates_pool)]\n                    # Now re-rank the shortlist strictly by predicted score (already sorted)\n                    shortlist = picks\n                    # Final choose top pick_count from shortlist\n                    shortlist = shortlist[:pick_count]\n                    for (_, i, j, lam, x_virt) in shortlist:\n                        if evals >= self.budget:\n                            break\n                        # add an orthogonal perturbation to explore around the edge\n                        xi = X[i]; xj = X[j]\n                        d = xj - xi\n                        dnorm2 = np.dot(d, d)\n                        if dnorm2 > 1e-14:\n                            # build a random vector, project out component along d\n                            v = self.rng.normal(size=self.dim)\n                            proj = (np.dot(v, d) / dnorm2) * d\n                            v_ort = v - proj\n                            norm_ort = np.linalg.norm(v_ort)\n                            if norm_ort > 1e-12:\n                                v_ort = v_ort / norm_ort\n                                orth_scale = max(1e-12, 0.18 * radius)  # small orth moves relative to radius\n                                x_cand = x_virt + v_ort * self.rng.normal(scale=orth_scale)\n                            else:\n                                # fallback small isotropic\n                                x_cand = x_virt + self.rng.normal(scale=0.08*radius, size=self.dim)\n                        else:\n                            # nearly identical points, just jitter\n                            x_cand = x_virt + self.rng.normal(scale=0.08*radius, size=self.dim)\n\n                        # clip to bounds\n                        x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                        # skip duplicates extremely close to known elites\n                        too_close = False\n                        for xa in archive_x:\n                            if np.linalg.norm(x_cand - xa) < 1e-8:\n                                too_close = True; break\n                        if too_close:\n                            continue\n                        f_cand = float(func(x_cand)); evals += 1\n                        archive_x.append(x_cand.copy()); archive_f.append(f_cand)\n                        if f_cand < f_best:\n                            f_best = f_cand; x_best = x_cand.copy()\n                            radius = min(self.max_radius, radius * 1.22)\n                            stagnation = 0\n                        else:\n                            # slight shrink if no improvement\n                            radius *= 0.95\n\n            # 3) Occasional opposite-point check and global diversification on stagnation\n            if evals < self.budget and (stagnation >= 2 and self.rng.random() < 0.28):\n                x_opp = lb + ub - x_best\n                x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                # avoid re-evaluating if it's equal to known elite\n                if not any(np.allclose(x_opp, xa, atol=1e-12) for xa in archive_x):\n                    f_opp = float(func(x_opp)); evals += 1\n                    archive_x.append(x_opp.copy()); archive_f.append(f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy()\n                        radius = min(self.max_radius, radius * 1.2)\n                        stagnation = 0\n\n            if stagnation >= self.stagnation_patience:\n                # global random samples\n                num_global = min(6, max(1, self.dim // 2))\n                improved = False\n                for _ in range(num_global):\n                    if evals >= self.budget:\n                        break\n                    x_rand = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_rand = float(func(x_rand)); evals += 1\n                    archive_x.append(x_rand.copy()); archive_f.append(f_rand)\n                    if f_rand < f_best:\n                        f_best = f_rand; x_best = x_rand.copy()\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n                        improved = True\n                        break\n                if not improved and evals < self.budget:\n                    # near-random jump centered on random elite to attempt escape\n                    idx = int(self.rng.integers(0, len(archive_x)))\n                    base = archive_x[idx]\n                    jump_scale = 0.6 * (ub - lb)\n                    x_jump = base + self.rng.normal(scale=1.0, size=self.dim) * jump_scale\n                    x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                    f_jump = float(func(x_jump)); evals += 1\n                    archive_x.append(x_jump.copy()); archive_f.append(f_jump)\n                    if f_jump < f_best:\n                        f_best = f_jump; x_best = x_jump.copy()\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n                    else:\n                        radius = max(self.min_radius, self.init_radius * 0.6)\n                        stagnation = 0\n\n            # ensure radius not too small\n            if radius < self.min_radius:\n                radius = max(self.min_radius, self.init_radius * 0.5)\n                if evals < self.budget:\n                    x_try = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_try = float(func(x_try)); evals += 1\n                    archive_x.append(x_try.copy()); archive_f.append(f_try)\n                    if f_try < f_best:\n                        f_best = f_try; x_best = x_try.copy()\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n\n            # housekeeping prune archive to keep memory small\n            prune_archive()\n\n            # safety small break if no budget remains\n            if evals >= self.budget:\n                break\n\n            # increase stagnation if no improvements seen recently (we only reset on improvement)\n            # Already tracked as stagnation variable.\n\n        # Final return\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 8, "feedback": "The algorithm EdgeDensifiedAdaptiveSubspaceSearch scored 0.427 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9693777491924473}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9706526618042726}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9679095083051384}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.08927142883993788}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.021882215518585713}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.11660847317367151}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.07501302495125184}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06411078862352737}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08059872936646117}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.08163305054112557}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07691189606825721}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07968696697857736}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9647512138866935}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9209283175348543}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.925634021041852}], "aucs": [0.9693777491924473, 0.9706526618042726, 0.9679095083051384, 0.08927142883993788, 0.021882215518585713, 0.11660847317367151, 0.07501302495125184, 0.06411078862352737, 0.08059872936646117, 0.08163305054112557, 0.07691189606825721, 0.07968696697857736, 0.9647512138866935, 0.9209283175348543, 0.925634021041852]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2915.0, "Edges": 2914.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.9993138936535162, "Degree Variance": 2.0082328054158856, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.667441860465116, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.328735072969763, "Depth Entropy": 2.1675779387160077, "Assortativity": 1.652224801802016e-08, "Average Eccentricity": 18.00377358490566, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00034305317324185246, "Average Shortest Path": 10.72261973014877, "mean_complexity": 16.25, "total_complexity": 65.0, "mean_token_count": 634.0, "total_token_count": 2536.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "879df5f6-b6ec-4c70-9f15-3a53bc971269", "fitness": 0.29494535249173065, "name": "EdgeClusteredMultiScale", "description": "Edge-Clustered Multi-Scale Graph Search (ECMGS) \u2014 massively densify edges using multi-scale virtual nodes and cluster-bridging centroids, rank candidates with cheap local surrogates, and adaptively sample edges and principal directions to exploit dense connectivity.", "code": "import numpy as np\n\nclass EdgeClusteredMultiScale:\n    \"\"\"\n    Edge-Clustered Multi-Scale Graph Search (ECMGS)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional args: pop_size, elite_frac, virtual_per_elite, k_edges, pool_size, seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.25,\n                 virtual_per_elite=6, k_edges=None, pool_size=18, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.virtual_per_elite = int(virtual_per_elite)\n        self.pool_size = int(pool_size)\n\n        if pop_size is None:\n            # heuristic initial population size (bounded)\n            base = int(np.clip(np.sqrt(self.budget) * 1.1, 8, 64))\n            self.pop_size = max(base, dim + 4)\n        else:\n            self.pop_size = int(pop_size)\n\n        if k_edges is None:\n            self.k_edges = max(2, min(self.pop_size - 1, int(np.clip(self.dim, 2, 18))))\n        else:\n            self.k_edges = int(k_edges)\n\n        # adaptation controls\n        self.succ_inc = 1.25\n        self.fail_dec = 0.78\n        self.levy_prob = 0.07\n\n    def _get_bounds(self, func):\n        # detect bounds attribute if present, else default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n\n        # choose a practical initial population size so we don't exhaust budget immediately\n        pop_size_use = min(self.pop_size, max(4, int(max(4, self.budget // 6))))\n        pop = rng.uniform(lb, ub, size=(pop_size_use, dim))\n        pop_f = np.full(pop_size_use, np.inf)\n        evals = 0\n        # initial evaluation: try to evaluate all initial pop until budget\n        for i in range(pop_size_use):\n            if evals >= self.budget:\n                break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # ensure at least one evaluated individual\n        if np.all(np.isinf(pop_f)):\n            # budget was zero or something odd; return default\n            self.x_opt = np.clip(pop[0], lb, ub).copy()\n            self.f_opt = float(pop_f[0]) if np.isfinite(pop_f[0]) else np.inf\n            return self.f_opt, self.x_opt\n\n        sigma = np.full(pop_size_use, 0.08 * range_norm)\n        alpha = np.full(pop_size_use, 0.25 * range_norm)\n        success_counts = np.zeros(pop_size_use, dtype=int)\n\n        idx_best = int(np.nanargmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        no_improve = 0\n        max_no_improve = max(20, pop_size_use * 3)\n\n        # small helper: k-means on elites (few iterations)\n        def kmeans_vectors(X, k, iters=6):\n            if X.shape[0] <= k:\n                return X.copy()\n            # init centroids randomly from X\n            ids = rng.choice(X.shape[0], size=k, replace=False)\n            cent = X[ids].copy()\n            for _ in range(iters):\n                # assign\n                dif = X[:, None, :] - cent[None, :, :]\n                d2 = np.sum(dif * dif, axis=2)\n                labels = np.argmin(d2, axis=1)\n                changed = False\n                for j in range(k):\n                    sel = X[labels == j]\n                    if sel.shape[0] > 0:\n                        newc = sel.mean(axis=0)\n                        if not np.allclose(newc, cent[j]):\n                            cent[j] = newc\n                            changed = True\n                if not changed:\n                    break\n            return cent\n\n        # build highly-densified edges using multi-scale virtual nodes and cluster centroids\n        def build_dense_edge_list():\n            # Only consider evaluated (finite) individuals as real nodes\n            real_idx = np.where(np.isfinite(pop_f))[0]\n            if real_idx.size < 2:\n                return []\n            # elites: top fraction among evaluated\n            sorted_real = real_idx[np.argsort(pop_f[real_idx])]\n            elite_count = max(2, int(np.ceil(self.elite_frac * sorted_real.size)))\n            elites = sorted_real[:elite_count]\n\n            # create multi-scale virtual nodes around elites\n            virtuals = []\n            virtual_map = []  # maps virtual index -> one real index (elite parent)\n            scales = [0.06, 0.18, 0.45]  # small, medium, large jitter scales relative to range_norm\n            for e in elites:\n                base = pop[e]\n                local_sigma = max(sigma[e], 1e-8 * range_norm)\n                # variable number per scale to increase density near elite\n                for s in scales:\n                    count = max(1, int(round(self.virtual_per_elite * (0.6 if s == scales[0] else (0.3 if s == scales[1] else 0.1)))))\n                    for _ in range(count):\n                        jitter = s * local_sigma * rng.standard_normal(dim)\n                        virtuals.append(base + jitter)\n                        virtual_map.append(int(e))\n\n            # cluster centroids as bridging virtuals (connect clusters strongly)\n            cluster_count = max(2, min(elite_count // 2 + 1, 6))\n            if elites.size >= 2:\n                centroids = kmeans_vectors(pop[elites], cluster_count, iters=8)\n            else:\n                centroids = np.empty((0, dim))\n            # augment list\n            virtuals_arr = np.asarray(virtuals) if len(virtuals) > 0 else np.empty((0, dim))\n            X_aug = np.vstack([pop[real_idx], virtuals_arr, centroids])  # only evaluated reals in first block\n            n_real = real_idx.size\n            n_virtuals = virtuals_arr.shape[0]\n            n_cent = centroids.shape[0]\n            N_aug = X_aug.shape[0]\n\n            # compute pairwise squared distances\n            dif = X_aug[:, None, :] - X_aug[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n\n            edges = set()\n            # connect each virtual to many nearest real nodes (first block indices 0..n_real-1)\n            for v_idx in range(n_real, n_real + n_virtuals + n_cent):\n                # find nearest real nodes\n                neigh_order = np.argsort(dist2[v_idx, :n_real])\n                taken = 0\n                for j in neigh_order:\n                    if taken >= self.k_edges + 2:\n                        break\n                    # map augmented index to real index id\n                    i_real = real_idx[j]\n                    # parent real of virtual (if virtual mapping exists)\n                    if v_idx < n_real + n_virtuals:\n                        parent = virtual_map[v_idx - n_real]\n                        edges.add(tuple(sorted((int(parent), int(i_real)))))\n                    else:\n                        # centroid connecting to real j\n                        # connect centroid's cluster neighborhood densified\n                        edges.add(tuple(sorted((int(real_idx[j]), int(real_idx[0])))))  # ensure some bridging\n                        edges.add(tuple(sorted((int(real_idx[j]), int(real_idx[-1])))))\n                    taken += 1\n\n            # inter-elite dense connections: connect each elite to its nearest elites\n            if elites.size >= 2:\n                epos = pop[elites]\n                edif = epos[:, None, :] - epos[None, :, :]\n                ed2 = np.sum(edif * edif, axis=2)\n                for ii, i in enumerate(elites):\n                    neighs = np.argsort(ed2[ii])\n                    taken = 0\n                    for ni in neighs:\n                        if ni == ii:\n                            continue\n                        j = elites[ni]\n                        edges.add(tuple(sorted((int(i), int(j)))))\n                        taken += 1\n                        if taken >= self.k_edges:\n                            break\n\n            # Add extra random edges to boost density\n            extras = max(0, int(n_real * 2))\n            for _ in range(extras):\n                a = int(rng.choice(real_idx))\n                b = int(rng.choice(real_idx))\n                if a != b:\n                    edges.add(tuple(sorted((a, b))))\n            return list(edges)\n\n        # candidate generation for an edge using multi-scale proposals and cheap surrogate ranking\n        def edge_pool(i, j, pool_size=None):\n            if pool_size is None:\n                pool_size = self.pool_size\n            xi, xj = pop[i], pop[j]\n            fi, fj = pop_f[i], pop_f[j]\n            edge_vec = xj - xi\n            edge_len = np.linalg.norm(edge_vec) + 1e-12\n            dir_unit = edge_vec / edge_len\n            candidates = []\n            avg_sigma = max(1e-12, 0.5 * (sigma[i] + sigma[j]))\n            for _ in range(pool_size):\n                r = rng.random()\n                # mixture: biased-to-ends, uniform, extrapolate\n                if r < 0.6:\n                    # emphasize endpoints (beta near 0 or 1)\n                    if rng.random() < 0.5:\n                        w = np.clip(rng.beta(2.5, 6.0), 0.0, 1.0)\n                    else:\n                        w = 1.0 - np.clip(rng.beta(2.5, 6.0), 0.0, 1.0)\n                elif r < 0.9:\n                    w = rng.uniform(0.0, 1.0)\n                else:\n                    w = rng.uniform(-0.8, 1.8)  # extrapolate\n                base = (1.0 - w) * xi + w * xj\n                # multi-scale perpendicular + along-edge jitter\n                scale_mult = rng.choice([0.35, 0.9, 1.8], p=[0.6, 0.3, 0.1])\n                noise_along = 0.4 * avg_sigma * rng.standard_normal() * dir_unit\n                rv = rng.standard_normal(dim)\n                rv = rv - np.dot(rv, dir_unit) * dir_unit\n                rvn = rv / (np.linalg.norm(rv) + 1e-12)\n                perp = (avg_sigma * 0.7 * scale_mult * rng.random()) * rvn\n                jitter = (avg_sigma * 0.45 * rng.standard_normal(dim)) * scale_mult\n                x_c = base + noise_along + perp + jitter\n                x_c = np.clip(x_c, lb, ub)\n                # surrogate prediction: linear interpolation + perpendicular penalty + curvature bias\n                t_proj = np.dot(x_c - xi, edge_vec) / (edge_len * edge_len)\n                lin_pred = (1.0 - t_proj) * fi + t_proj * fj\n                perp_dist = np.linalg.norm(x_c - (xi + np.clip(t_proj, 0.0, 1.0) * edge_vec))\n                grad_est = (fj - fi) / (edge_len + 1e-12)\n                surrogate = lin_pred + 0.45 * abs(grad_est) * perp_dist + 0.012 * perp_dist * range_norm\n                # penalize strong extrapolations\n                if (t_proj < -0.1) or (t_proj > 1.1):\n                    surrogate += 0.2 * range_norm\n                candidates.append((surrogate, x_c))\n            candidates.sort(key=lambda x: x[0])\n            return [c[1] for c in candidates]\n\n        # cluster-local PCA sampling to exploit dense neighborhoods\n        def cluster_pca_candidates(indices, n_samples=8):\n            pts = pop[indices]\n            if pts.shape[0] <= 1:\n                return []\n            # center and PCA (SVD)\n            C = pts.mean(axis=0)\n            M = pts - C\n            U, S, Vt = np.linalg.svd(M, full_matrices=False)\n            directions = Vt[:min(3, Vt.shape[0])]\n            candidates = []\n            for _ in range(n_samples):\n                # combine a few principal directions with small coefficients\n                coeffs = (rng.normal(0, 0.35, size=directions.shape[0]) * (S[:directions.shape[0]] / (np.max(S) + 1e-12)))\n                move = coeffs @ directions\n                # scale by local sigma\n                scale = 0.6 + rng.random() * 1.6\n                x_c = np.clip(C + scale * move * range_norm * 0.02, lb, ub)\n                # surrogate: predict by weighted average of cluster f\n                sur = np.mean(pop_f[indices]) - 0.05 * np.linalg.norm(move) * range_norm\n                candidates.append((sur, x_c))\n            candidates.sort(key=lambda x: x[0])\n            return [c[1] for c in candidates]\n\n        # Main loop\n        while evals < self.budget:\n            rem = self.budget - evals\n            improved = False\n\n            edges = build_dense_edge_list()\n            if len(edges) == 0:\n                # fallback: random sampling\n                x = rng.uniform(lb, ub)\n                if evals < self.budget:\n                    f = float(func(x)); evals += 1\n                    if f < f_best:\n                        f_best = f; x_best = x.copy(); improved = True\n                break\n\n            rng.shuffle(edges)\n\n            # budget-aware per-edge allocation\n            K_eval = max(1, min(4, rem // max(1, len(edges))))\n            if evals < self.budget * 0.5:\n                K_eval = min(4, K_eval + 1)\n\n            for (i, j) in edges:\n                if evals >= self.budget:\n                    break\n                if i == j:\n                    continue\n                # both endpoints must be evaluated; skip edges with inf\n                if not (np.isfinite(pop_f[i]) and np.isfinite(pop_f[j])):\n                    continue\n\n                pool = edge_pool(i, j, pool_size=min(self.pool_size, 26))\n                # choose top-K by surrogate\n                for x_c in pool[:K_eval]:\n                    if evals >= self.budget:\n                        break\n                    # occasional Levy injection\n                    if rng.random() < 0.06:\n                        u = rng.random(dim)\n                        cauchy = np.tan(np.pi * (u - 0.5))\n                        x_c = np.clip(x_c + 0.12 * range_norm * cauchy, lb, ub)\n                    f_c = float(func(x_c)); evals += 1\n                    # replace logic: replace worse endpoint if improved\n                    if f_c < pop_f[i] or f_c < pop_f[j]:\n                        # replace worse of the two\n                        ridx = i if pop_f[i] >= pop_f[j] else j\n                        pop[ridx] = x_c\n                        pop_f[ridx] = f_c\n                        sigma[ridx] = min(2.5 * range_norm, sigma[ridx] * self.succ_inc)\n                        alpha[ridx] = min(1.6 * range_norm, alpha[ridx] * 1.02)\n                        success_counts[ridx] += 1\n                        if f_c < f_best:\n                            f_best = f_c; x_best = x_c.copy(); improved = True\n                    else:\n                        # possible replacement of worst overall\n                        if rng.random() < 0.015:\n                            idx_w = int(np.nanargmax(pop_f))\n                            if f_c < pop_f[idx_w]:\n                                pop[idx_w] = x_c; pop_f[idx_w] = f_c\n                                sigma[idx_w] = 0.08 * range_norm\n                                if f_c < f_best:\n                                    f_best = f_c; x_best = x_c.copy(); improved = True\n                        # else shrink endpoints\n                        sigma[i] *= self.fail_dec\n                        sigma[j] *= self.fail_dec\n\n                # occasional cluster-PCA refinement\n                if rng.random() < 0.14 and evals + 1 < self.budget:\n                    # pick a small local neighborhood among nearest reals to i\n                    real_idx = np.where(np.isfinite(pop_f))[0]\n                    # compute distances to i\n                    dists = np.linalg.norm(pop[real_idx] - pop[i], axis=1)\n                    near = real_idx[np.argsort(dists)[:max(2, min(8, real_idx.size))]]\n                    cand_list = cluster_pca_candidates(near, n_samples=6)\n                    for x_c in cand_list[:2]:\n                        if evals >= self.budget: break\n                        f_c = float(func(x_c)); evals += 1\n                        idx_w = int(np.nanargmax(pop_f))\n                        if f_c < pop_f[idx_w]:\n                            pop[idx_w] = x_c; pop_f[idx_w] = f_c\n                            sigma[idx_w] = 0.08 * range_norm\n                            if f_c < f_best:\n                                f_best = f_c; x_best = x_c.copy(); improved = True\n\n            # targeted spiral/centroid moves toward best for a subset\n            real_idx = np.where(np.isfinite(pop_f))[0]\n            # sort by quality and operate more on mid/worst to densify edges around good areas\n            order = real_idx[np.argsort(pop_f[real_idx])[::-1]]  # worst-first\n            for i in order[:min(len(order), max(3, pop.shape[0] // 3))]:\n                if evals >= self.budget:\n                    break\n                xi = pop[i].copy()\n                fi = pop_f[i]\n                d = x_best - xi\n                nd = np.linalg.norm(d) + 1e-12\n                dirv = d / nd\n                # small randomized rotation in a random 2D subspace\n                if dim > 1 and rng.random() < 0.6:\n                    a, b = rng.choice(dim, size=2, replace=False)\n                    theta = rng.uniform(-np.pi/4, np.pi/4)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    dr = d.copy()\n                    da, db = dr[a], dr[b]\n                    dr[a] = ca * da - sb * db\n                    dr[b] = sb * da + ca * db\n                    dirv = dr / (np.linalg.norm(dr) + 1e-12)\n                scale = alpha[i] * (0.4 + rng.random() * 0.9)\n                move = scale * dirv + sigma[i] * rng.standard_normal(dim)\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    move += 0.12 * range_norm * cauchy\n                x_new = np.clip(xi + move, lb, ub)\n\n                # opposition sampling occasionally\n                if rng.random() < 0.05 and evals < self.budget:\n                    x_opp = lb + ub - xi\n                    x_opp = np.clip(x_opp, lb, ub)\n                    f_opp = float(func(x_opp)); evals += 1\n                    if f_opp < fi:\n                        pop[i] = x_opp; pop_f[i] = f_opp\n                        sigma[i] *= self.succ_inc\n                        alpha[i] *= 1.02\n                        if f_opp < f_best:\n                            f_best = f_opp; x_best = x_opp.copy(); improved = True\n                        continue\n\n                if evals >= self.budget:\n                    break\n                f_new = float(func(x_new)); evals += 1\n                if f_new < fi or rng.random() < 0.012:\n                    pop[i] = x_new; pop_f[i] = f_new\n                    sigma[i] *= self.succ_inc\n                    alpha[i] *= 1.015\n                    success_counts[i] += 1\n                    if f_new < f_best:\n                        f_best = f_new; x_best = x_new.copy(); improved = True\n                else:\n                    sigma[i] *= self.fail_dec\n                    alpha[i] *= 0.988\n\n            # parameter clamps\n            sigma = np.clip(sigma, 1e-9 * range_norm, 3.0 * range_norm)\n            alpha = np.clip(alpha, 1e-9 * range_norm, 2.0 * range_norm)\n\n            if improved:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # stagnation intensification: sample many virtual candidates around best and bridges between clusters\n            if no_improve >= max_no_improve and evals < self.budget:\n                no_improve = 0\n                intens = min(24, max(6, pop.shape[0]))\n                for _ in range(intens):\n                    if evals >= self.budget:\n                        break\n                    if rng.random() < 0.85:\n                        # multi-scale jitter about x_best\n                        scale = rng.choice([0.02, 0.06, 0.18], p=[0.6, 0.3, 0.1])\n                        cand = np.clip(x_best + scale * range_vec * rng.standard_normal(dim), lb, ub)\n                    else:\n                        cand = rng.uniform(lb, ub)\n                    f_c = float(func(cand)); evals += 1\n                    idx_w = int(np.nanargmax(pop_f))\n                    if f_c < pop_f[idx_w]:\n                        pop[idx_w] = cand; pop_f[idx_w] = f_c\n                        sigma[idx_w] = 0.08 * range_norm\n                        alpha[idx_w] = 0.25 * range_norm\n                        if f_c < f_best:\n                            f_best = f_c; x_best = cand.copy()\n\n            # small random injection occasionally\n            if rng.random() < 0.05 and evals < self.budget:\n                r = rng.uniform(lb, ub)\n                fr = float(func(r)); evals += 1\n                idx_w = int(np.nanargmax(pop_f))\n                if fr < pop_f[idx_w]:\n                    pop[idx_w] = r; pop_f[idx_w] = fr\n                    sigma[idx_w] = 0.08 * range_norm\n                    if fr < f_best:\n                        f_best = fr; x_best = r.copy()\n\n            # if remaining budget small, break to local refine\n            if self.budget - evals < max(12, dim * 3):\n                break\n\n        # Final local coordinate mesh/pattern search around best\n        mesh = 0.16 * range_norm\n        mesh_min = 1e-6 * range_norm\n        while evals < self.budget and mesh > mesh_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget:\n                    break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                mesh *= 0.5\n\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EdgeClusteredMultiScale scored 0.295 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["efb33a12-13f2-4a5d-9b4e-785879562267"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.6510291397302354}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.62974901181848}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.5391822836748436}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.044305462355408554}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.05577054699258899}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.053049541176776294}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.039115559167773206}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.027371532939682974}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.02576579889556374}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.8176776279411304}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.70933049873201}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.8316832839514671}], "aucs": [0.6510291397302354, 0.62974901181848, 0.5391822836748436, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.044305462355408554, 0.05577054699258899, 0.053049541176776294, 0.039115559167773206, 0.027371532939682974, 0.02576579889556374, 0.8176776279411304, 0.70933049873201, 0.8316832839514671]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4772.0, "Edges": 4771.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9995808885163453, "Degree Variance": 2.0167642836917503, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.49203459262631, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3380371729226999, "Depth Entropy": 2.066342995678484, "Assortativity": 1.5806957007190923e-08, "Average Eccentricity": 19.597443419949705, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00020955574182732607, "Average Shortest Path": 11.272948747523412, "mean_complexity": 16.285714285714285, "total_complexity": 114.0, "mean_token_count": 593.4285714285714, "total_token_count": 4154.0, "mean_parameter_count": 3.0, "total_parameter_count": 21.0}, "archive_direction": "increase", "archive_feature": "edge density"}
{"id": "8383e266-ed89-444c-a936-0717ad207fdf", "fitness": "-inf", "name": "SparseEdgeLowRankAnnealedSearch", "description": "Sparse Edge Low-Rank Annealed Search (SELARS) \u2014 build a sparse, low-degree k-NN edge graph, fit tiny low-rank quadratic-linear surrogates in local PCA subspaces around elites to propose analytic minima and sparse recombinations; cap node degree to keep the edge graph simple, anneal trust radius by success, and use prioritized surrogate scoring to spend evaluations only on promising candidates.", "code": "import numpy as np\n\nclass SparseEdgeLowRankAnnealedSearch:\n    \"\"\"\n    Sparse Edge Low-Rank Annealed Search (SELARS)\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Optional args:\n      pop_size: initial population size (clipped to budget)\n      elite_frac: fraction of population considered elite\n      k_neighbors: nominal neighbors to consider when building sparse graph\n      max_degree: hard cap on node degree in the edge graph (keeps graph sparse / low-degree)\n      proposals_per_cycle: number of actual evaluations attempted per cycle\n      local_m: max PCA dims for local surrogate\n      trust_init: initial trust radius fraction of search range\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, pop_size=None, elite_frac=0.2,\n                 k_neighbors=6, max_degree=3, proposals_per_cycle=20,\n                 local_m=2, trust_init=0.12, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.max_degree = int(max(1, max_degree))\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.local_m = int(max(1, local_m))\n        self.trust_init = float(trust_init)\n\n        # population sizing default heuristic (kept small to focus evaluations)\n        if pop_size is None:\n            base = int(np.clip(np.sqrt(self.budget)*0.9, 8, 200))\n            self.pop_size = max(base, self.dim + 4)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation constants\n        self.expand = 1.28\n        self.shrink = 0.74\n        self.min_trust = 1e-8\n        self.max_trust = 1e3\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        span = ub - lb\n        span_norm = np.linalg.norm(span)\n        trust = float(self.trust_init) * span_norm\n\n        # ensure sensible population size\n        pop_size = min(self.pop_size, max(4, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            pop_f[i] = float(func(pop[i])); evals += 1\n\n        # adjacency (sparse) graph and weights: we keep a symmetric weight matrix but enforce degree caps\n        edge_w = np.zeros((self.pop_size, self.pop_size), dtype=float)\n        degrees = np.zeros(self.pop_size, dtype=int)\n\n        # helper to add edge if degrees permit\n        def add_edge(i, j, weight=1.0):\n            if i == j: return False\n            if degrees[i] >= self.max_degree or degrees[j] >= self.max_degree:\n                # try to replace a weaker connection if possible\n                # find smallest weight neighbor for i and j and replace if weaker than new\n                replaced = False\n                if degrees[i] >= self.max_degree:\n                    nei = np.nonzero(edge_w[i])[0]\n                    if nei.size > 0:\n                        min_idx = nei[np.argmin(edge_w[i, nei])]\n                        if edge_w[i, min_idx] < weight:\n                            # remove symmetric\n                            edge_w[i, min_idx] = 0.0\n                            edge_w[min_idx, i] = 0.0\n                            degrees[i] -= 1\n                            degrees[min_idx] -= 1\n                            replaced = True\n                if degrees[j] >= self.max_degree and not replaced:\n                    nei = np.nonzero(edge_w[j])[0]\n                    if nei.size > 0:\n                        min_idx = nei[np.argmin(edge_w[j, nei])]\n                        if edge_w[j, min_idx] < weight:\n                            edge_w[j, min_idx] = 0.0\n                            edge_w[min_idx, j] = 0.0\n                            degrees[j] -= 1\n                            degrees[min_idx] -= 1\n                            replaced = True\n                if not replaced:\n                    return False\n            if edge_w[i, j] == 0.0:\n                degrees[i] += 1\n                degrees[j] += 1\n            edge_w[i, j] = max(edge_w[i, j], weight)\n            edge_w[j, i] = edge_w[i, j]\n            return True\n\n        # initial sparse graph: k-NN but cap degrees\n        def build_sparse_graph():\n            nonlocal edge_w, degrees\n            edge_w[:, :] = 0.0\n            degrees[:] = 0\n            # distance matrix\n            dif = pop[:, None, :] - pop[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            for i in range(self.pop_size):\n                neigh = np.argsort(dist2[i])\n                added = 0\n                for nb in neigh[1: 1 + self.k_neighbors*3]:\n                    if nb == i: continue\n                    if add_edge(i, int(nb), weight=1.0/(1.0 + dist2[i, int(nb)])):\n                        added += 1\n                    if added >= self.k_neighbors:\n                        break\n            # ensure connectivity with a few random edges\n            for _ in range(max(2, self.pop_size // 6)):\n                a = int(rng.integers(0, self.pop_size)); b = int(rng.integers(0, self.pop_size))\n                if a != b:\n                    add_edge(a,b, weight=0.5)\n            # keep small positive floor\n            edge_w[edge_w < 1e-12] = 0.0\n\n        build_sparse_graph()\n\n        # bookkeeping best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        cycle_no_improve = 0\n        cycle = 0\n\n        # main search loop\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            act_K = min(self.proposals_per_cycle, remaining)\n            if act_K <= 0: break\n\n            # compute elites\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            order = np.argsort(pop_f)\n            elites = order[:elite_count]\n\n            # prepare candidate pool (surrogate-driven)\n            candidates = []\n            cand_meta = []\n\n            # per-elite small local surrogate (linear + isotropic quadratic in PCA coords)\n            for e in elites:\n                # choose neighborhood: include e and its neighbors from sparse graph and some nearest by distance\n                neigh_idx = np.nonzero(edge_w[e])[0].tolist()\n                # include a few nearest by raw distance as backup\n                if len(neigh_idx) < self.k_neighbors:\n                    dif = pop - pop[e]\n                    d2 = np.sum(dif*dif, axis=1)\n                    near = np.argsort(d2)\n                    for nb in near:\n                        if nb == e: continue\n                        if nb not in neigh_idx:\n                            neigh_idx.append(int(nb))\n                        if len(neigh_idx) >= self.k_neighbors:\n                            break\n                # ensure we have at least 4 points to fit small model if possible\n                sample_ids = [int(e)] + [int(x) for x in neigh_idx[:max(1, self.k_neighbors)]]\n                sample_ids = list(dict.fromkeys(sample_ids))  # unique preserving order\n                Xs = pop[sample_ids].astype(float)\n                ys = pop_f[sample_ids].astype(float)\n                Xmean = Xs.mean(axis=0, keepdims=True)\n                Xc = Xs - Xmean\n                # PCA (tiny)\n                try:\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    m = min(self.local_m, Vt.shape[0])\n                    V = Vt[:m].copy()\n                    Z = Xc @ V.T  # (n_samples, m)\n                    # design: [1, Z (m dims), r2], where r2 = ||Z||^2 (isotropic quadratic)\n                    r2 = np.sum(Z*Z, axis=1, keepdims=True)\n                    Zaug = np.hstack([np.ones((Z.shape[0],1)), Z, r2])\n                    # ridge to stabilize\n                    alpha = 1e-6 * (1.0 + np.var(ys))\n                    A = Zaug.T @ Zaug\n                    A[np.diag_indices_from(A)] += alpha\n                    coeff = np.linalg.solve(A, Zaug.T @ ys)  # shape (1+m+1,)\n                    intercept = coeff[0]\n                    b = coeff[1:1+m]  # linear in Z\n                    c = float(coeff[-1])  # radial quadratic coefficient\n                    # predict surrogate minimum in Z: minimize intercept + b^T z + c ||z||^2\n                    # if c <= small_positive, push it to small positive to get bounded min\n                    c_pos = c if c > 1e-8 else 1e-3 + abs(c)*1e-3\n                    z_star = -b / (2.0 * c_pos)\n                    # limit step length in original space\n                    x_star = (Xmean + (z_star @ V)).ravel()\n                    # move toward x_star from mean but cap to trust region\n                    step_vec = x_star - Xmean.ravel()\n                    step_norm = np.linalg.norm(step_vec)\n                    if step_norm > 1.6 * trust and step_norm > 0:\n                        step_vec = step_vec * (1.6 * trust / step_norm)\n                        x_star = Xmean.ravel() + step_vec\n                    # add small randomized relaxations around surrogate minimizer\n                    for jitter in (0, 1):\n                        x_cand = x_star.copy()\n                        if jitter:\n                            x_cand = x_cand + 0.35 * trust * rng.standard_normal(dim)\n                        x_cand = np.clip(x_cand, lb, ub)\n                        candidates.append(x_cand)\n                        cand_meta.append({'type':'surrogate','elite':int(e),'ids':list(sample_ids)})\n                except Exception:\n                    # fallback simple candidate: small gaussian around elite\n                    x_cand = pop[int(e)].copy() + 0.6 * trust * rng.standard_normal(dim)\n                    x_cand = np.clip(x_cand, lb, ub)\n                    candidates.append(x_cand)\n                    cand_meta.append({'type':'fallback','elite':int(e),'ids':[int(e)]})\n\n            # add sparse edge recombinations: limited-degree edges only\n            # gather existing edges\n            tri = np.triu_indices(self.pop_size, k=1)\n            edge_list = [(int(tri[0][i]), int(tri[1][i])) for i in range(tri[0].size) if edge_w[tri[0][i], tri[1][i]] > 0]\n            rng.shuffle(edge_list)\n            # generate a few midpoints/extrapolations from edges, capped by proposals we plan\n            for a,b in edge_list[:max(6, act_K*2)]:\n                if len(candidates) >= act_K * 6:\n                    break\n                Xa, Xb = pop[a], pop[b]\n                # biased interpolation toward better endpoint\n                if pop_f[a] < pop_f[b]:\n                    t = rng.beta(2.4, 1.0)\n                else:\n                    t = 1.0 - rng.beta(2.4, 1.0)\n                # small chance to extrapolate\n                if rng.random() < 0.1:\n                    t = rng.uniform(-0.5, 1.7)\n                x_e = (1-t)*Xa + t*Xb\n                # orthogonal perturbation smaller than trust\n                dir_v = Xb - Xa\n                nrm = np.linalg.norm(dir_v)\n                if nrm > 1e-12:\n                    unit = dir_v / nrm\n                    # add small jitter orthogonal to edge\n                    perp = rng.standard_normal(dim)\n                    perp = perp - perp.dot(unit)*unit\n                    if np.linalg.norm(perp) > 1e-12:\n                        perp = perp / np.linalg.norm(perp)\n                        x_e = x_e + 0.06 * trust * perp\n                x_e = x_e + 0.45 * trust * rng.standard_normal(dim)\n                x_e = np.clip(x_e, lb, ub)\n                candidates.append(x_e)\n                cand_meta.append({'type':'edge','ids':(a,b)})\n\n            # add some global quasi-uniform samples (space-filling micro-proposals)\n            micro = max(2, act_K // 3)\n            for _ in range(micro):\n                # stratified sampling in each cycle for spread: sample each dim from jittered grid\n                vec = rng.random(dim)\n                xg = lb + vec * span\n                xg = xg + 0.12 * trust * rng.standard_normal(dim)\n                xg = np.clip(xg, lb, ub)\n                candidates.append(xg)\n                cand_meta.append({'type':'global'})\n\n            # now score candidates cheaply using a fast heuristic:\n            #  - prefer predicted improvement from nearest elite linear model if available\n            #  - prefer closeness to best and to weighted average of nearby elites\n            cand_array = np.asarray(candidates)\n            nc = cand_array.shape[0]\n            if nc == 0:\n                break\n\n            # compute distance to best and to nearest elite\n            d2_best = np.sum((cand_array - x_best[None,:])**2, axis=1)\n            # estimate NN fitness from small subset of population\n            sub_n = min(self.pop_size, 12)\n            idx_sub = rng.choice(self.pop_size, size=sub_n, replace=False)\n            d2_sub = np.sum((cand_array[:,None,:] - pop[idx_sub][None,:,:])**2, axis=2)\n            nn_idx = np.argmin(d2_sub, axis=1)\n            nn_f = pop_f[idx_sub][nn_idx]\n\n            # combine features to score (lower better)\n            score = 0.6 * nn_f + 0.25 * (f_best + 0.5 * np.sqrt(d2_best + 1e-12))\n            # add small bias for candidates that are surrogate-proposed (favor them)\n            for i,meta in enumerate(cand_meta):\n                if meta.get('type','') == 'surrogate':\n                    score[i] -= 0.08 * abs(f_best) + 0.01\n                if meta.get('type','') == 'edge':\n                    a,b = meta.get('ids', (None,None))\n                    if a is not None:\n                        score[i] -= 0.02 * (edge_w[a,b] if edge_w[a,b]>0 else 0.0)\n\n            # choose top-K by score to actually evaluate\n            order_c = np.argsort(score)\n            to_eval = order_c[:min(act_K, nc, remaining)]\n\n            improved_this_cycle = False\n            for idx in to_eval:\n                if evals >= self.budget: break\n                x_prop = cand_array[int(idx)].copy()\n                f_prop = float(func(x_prop)); evals += 1\n\n                # replacement: try to replace a worse nearby point among small neighbor set\n                d2 = np.sum((pop - x_prop)**2, axis=1)\n                near = np.argsort(d2)[:min(8, self.pop_size)]\n                replaced = False\n                # attempt to replace the worst among near if candidate better\n                worst_near = near[np.argmax(pop_f[near])]\n                if f_prop < pop_f[worst_near]:\n                    pop[worst_near] = x_prop.copy()\n                    pop_f[worst_near] = f_prop\n                    replaced = True\n                    # update sparse graph: add edges between replaced node and contributing ids if any\n                    meta = cand_meta[int(idx)]\n                    if 'ids' in meta and isinstance(meta['ids'], (list, tuple)):\n                        for a in meta['ids'][:self.k_neighbors]:\n                            if a == worst_near: continue\n                            add_edge(int(a), int(worst_near), weight=1.0 + 0.5/(1.0+abs(pop_f[int(a)]-f_prop)))\n                else:\n                    # optional replacement of global worst\n                    worst_global = int(np.argmax(pop_f))\n                    if f_prop < pop_f[worst_global]:\n                        pop[worst_global] = x_prop.copy()\n                        pop_f[worst_global] = f_prop\n                        replaced = True\n                # adapt trust\n                if replaced:\n                    trust = min(self.max_trust, trust * self.expand)\n                    improved_this_cycle = True\n                    # rebuild sparse graph occasionally to reflect new geometry\n                    if rng.random() < 0.18:\n                        build_sparse_graph()\n                else:\n                    trust = max(self.min_trust, trust * self.shrink)\n\n                # update best\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n\n                # if budget exhausted break\n                if evals >= self.budget:\n                    break\n\n            if improved_this_cycle:\n                cycle_no_improve = 0\n            else:\n                cycle_no_improve += 1\n                # stagnation action: small number of focused midpoints on strongest sparse edges\n                if cycle_no_improve >= 4 and evals < self.budget:\n                    # find edges with highest weight\n                    tri_idx = np.triu_indices(self.pop_size, k=1)\n                    ww = edge_w[tri_idx]\n                    if ww.size > 0:\n                        topk = min(6, ww.size)\n                        top = np.argsort(ww)[-topk:]\n                        for i in top:\n                            a = int(tri_idx[0][i]); b = int(tri_idx[1][i])\n                            x_mid = 0.5*(pop[a] + pop[b]) + 0.28 * trust * rng.standard_normal(dim)\n                            x_mid = np.clip(x_mid, lb, ub)\n                            f_mid = float(func(x_mid)); evals += 1\n                            worst_global = int(np.argmax(pop_f))\n                            if f_mid < pop_f[worst_global]:\n                                pop[worst_global] = x_mid.copy()\n                                pop_f[worst_global] = f_mid\n                                add_edge(a, b, weight=1.0)\n                                if f_mid < f_best:\n                                    f_best = f_mid; x_best = x_mid.copy()\n                            if evals >= self.budget:\n                                break\n                    cycle_no_improve = 0\n                    # tighten trust after burst\n                    trust = max(self.min_trust, trust * 0.78)\n\n            # mid-loop rebuild to prevent stale graph if many replacements\n            if rng.random() < 0.06:\n                build_sparse_graph()\n\n            # if very few evaluations left, exit main loop to do local refinement\n            if self.budget - evals <= max(6, dim):\n                break\n\n        # final local refinement: simple adaptive coordinate/pattern search around best\n        step = 0.18 * span_norm\n        step_min = max(1e-8, 1e-6 * span_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            order_dims = list(range(dim))\n            rng.shuffle(order_dims)\n            for d in order_dims:\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                f_try = float(func(x_try)); evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n            else:\n                step = min(0.6 * span_norm, step * 1.12)\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["45bacdb1-c496-4bcb-8a7b-2ccd06451f70"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3866.0, "Edges": 3865.0, "Max Degree": 36.0, "Min Degree": 1.0, "Mean Degree": 1.999482669425763, "Degree Variance": 2.2240038710136707, "Transitivity": 0.0, "Max Depth": 19.0, "Min Depth": 2.0, "Mean Depth": 8.564705882352941, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3299740898806491, "Depth Entropy": 2.1563669984732448, "Assortativity": 0.0, "Average Eccentricity": 21.66037247801345, "Diameter": 30.0, "Radius": 15.0, "Edge Density": 0.0002586652871184687, "Average Shortest Path": 11.098100600384551, "mean_complexity": 18.4, "total_complexity": 92.0, "mean_token_count": 670.8, "total_token_count": 3354.0, "mean_parameter_count": 3.6, "total_parameter_count": 18.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "6dea71e8-8e44-4201-b13c-905627831165", "fitness": 0.0956213379485234, "name": "AdaptiveDiagonalLowRankQuadraticSurrogates", "description": "Adaptive Diagonal Low-Rank Quadratic Surrogates (ADLQS) \u2014 fit many small, diagonal-quadratic surrogates in randomized/principal low-dimensional subspaces (no cross terms) to reduce model degree and ill-conditioning; solve elementwise quadratic minima, line-search a few short probes, and adapt trust radii with cheap global diversification.", "code": "import numpy as np\n\nclass AdaptiveDiagonalLowRankQuadraticSurrogates:\n    \"\"\"\n    Adaptive Diagonal Low-Rank Quadratic Surrogates (ADLQS)\n\n    Key ideas:\n      - Use many very-small (1..k) subspace quadratic surrogates but only keep diagonal\n        quadratic terms (no cross terms). This greatly reduces surrogate degree and\n        parameter count, improving numerical stability on small data.\n      - Orient subspaces either by local PCA (when neighborhood available) or random orthonormal\n        directions. Fit simple ridge regression to [1, s_i, 0.5*s_i^2] terms per subspace.\n      - Solve surrogate minimum elementwise: s* = -b / diag(H) (with robust regularization).\n      - Do short line-search-like probes along s* and -b directions (few cheap extra evals),\n        update trust radii via predicted vs actual reduction (rho rule).\n      - Occasionally diversify with global/opposite samples. Trim archive to keep operations cheap.\n    \"\"\"\n\n    def __init__(self, budget, dim, init_samples=None, max_subdim=4, elite_k=8,\n                 init_radius=1.0, min_radius=1e-6, max_radius=5.0, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.max_subdim = int(min(max_subdim, dim))\n        self.elite_k = int(elite_k)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        if init_samples is None:\n            # modest seeding: enough to build small local fits but not wasteful\n            self.init_samples = int(min(40, max(6, self.budget // 12)))\n        else:\n            self.init_samples = int(init_samples)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _random_orthonormal(self, k):\n        A = self.rng.normal(size=(self.dim, k))\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        X = []\n        F = []\n        radii = []\n        evals = 0\n\n        # initial seeding: spread plus one near-opposite sample of center\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            fx = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(fx)\n            radii.append(self.init_radius)\n\n        # ensure at least one sample\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            fx = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(fx)\n            radii.append(self.init_radius)\n\n        best_idx = int(np.argmin(F))\n        best_f = F[best_idx]\n        best_x = X[best_idx].copy()\n\n        stagnation = 0\n\n        # main loop\n        while evals < self.budget:\n            n_archive = len(X)\n            # Select center among top-k (soft preference to better)\n            top_k = min(self.elite_k, n_archive)\n            idx_sorted = np.argsort(F)\n            top_inds = idx_sorted[:top_k]\n            top_fs = np.array([F[i] for i in top_inds], dtype=float)\n            # stable softmax-like weights (lower f -> higher weight)\n            s = top_fs - np.min(top_fs)\n            denom = 1e-8 + np.std(s)\n            weights = np.exp(-s / (denom if denom > 0 else 1.0))\n            weights = weights / weights.sum()\n            center_idx = int(self.rng.choice(top_inds, p=weights))\n            center_x = X[center_idx].copy()\n            center_f = F[center_idx]\n            center_r = radii[center_idx]\n\n            # choose subspace dim\n            k_sub = int(self.rng.integers(1, min(self.max_subdim, self.dim) + 1))\n\n            # choose neighborhood points for fitting\n            X_arr = np.vstack(X)\n            dists = np.linalg.norm(X_arr - center_x[None, :], axis=1)\n            # neighborhood radius: tied to center_r and data distribution\n            neigh_r = max(center_r * 1.6, np.percentile(dists, min(85, max(50, int(100 - 100.0 / max(1, n_archive)))) ) if n_archive > 3 else center_r * 1.6)\n            within = np.where(dists <= neigh_r)[0].tolist()\n            # need at least p_min samples: p = 1 + k + k (linear + diag quad)\n            p_min = 1 + 2 * k_sub\n            if len(within) < p_min:\n                order = np.argsort(dists)\n                for idx in order:\n                    if idx not in within:\n                        within.append(int(idx))\n                    if len(within) >= p_min:\n                        break\n\n            X_neigh = X_arr[within]\n            # If enough neighbors, do PCA to orient subspace; else random orthonormal\n            if len(X_neigh) >= k_sub + 2:\n                # compute local covariance\n                M = X_neigh - X_neigh.mean(axis=0, keepdims=True)\n                try:\n                    U, Svals, _ = np.linalg.svd(M, full_matrices=False)\n                    basis = U[:, :k_sub]\n                    # But U from SVD of sample x dim is shape (m, dim), not (dim, dim). Use covariance approach:\n                    if basis.shape[0] != self.dim:\n                        # fallback to covariance eigenvectors\n                        C = (M.T @ M) / max(1, M.shape[0] - 1)\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        idxs = np.argsort(-eigvals)[:k_sub]\n                        basis = eigvecs[:, idxs]\n                except Exception:\n                    basis = self._random_orthonormal(k_sub)\n            else:\n                basis = self._random_orthonormal(k_sub)\n\n            # Project neighbors into subspace coordinates s\n            S = (X_neigh - center_x[None, :]) @ basis  # (m, k_sub)\n\n            # Build design matrix for diagonal quadratic: columns = [1, s1..sk, 0.5*s1^2..0.5*sk^2]\n            m = S.shape[0]\n            ones = np.ones((m, 1))\n            lin = S\n            quad = 0.5 * (S ** 2)\n            Phi = np.hstack([ones, lin, quad])  # shape (m, 1 + k + k)\n            y = np.array([F[i] for i in within], dtype=float)\n\n            # Regularized ridge solve\n            p = Phi.shape[1]\n            reg = 1e-8 + 1e-6 * max(1.0, np.var(y))\n            try:\n                A = Phi.T @ Phi\n                A[np.diag_indices_from(A)] += reg\n                rhs = Phi.T @ y\n                theta = np.linalg.solve(A, rhs)\n            except np.linalg.LinAlgError:\n                try:\n                    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n                except Exception:\n                    theta = None\n\n            if theta is None or np.any(~np.isfinite(theta)):\n                # fallback: small random local trial\n                if evals >= self.budget:\n                    break\n                step = self.rng.normal(size=self.dim)\n                step = step / (np.linalg.norm(step) + 1e-12) * center_r * self.rng.random()\n                x_cand = np.clip(center_x + step, lb, ub)\n                f_cand = float(func(x_cand))\n                evals += 1\n                X.append(x_cand.copy()); F.append(f_cand); radii.append(self.init_radius)\n                if f_cand < best_f:\n                    best_f = f_cand; best_x = x_cand.copy(); stagnation = 0\n                else:\n                    stagnation += 1\n                continue\n\n            # unpack theta: a, b (k_sub), diagH (k_sub)\n            a_hat = float(theta[0])\n            b_hat = np.array(theta[1:1 + k_sub], dtype=float)\n            diagH = np.array(theta[1 + k_sub:1 + 2 * k_sub], dtype=float)\n            # enforce reasonable curvature: if diagH too small or negative, push to small positive\n            h_eps = 1e-6 * (1.0 + np.std(y))\n            diagH_safe = np.where(diagH <= h_eps, h_eps + np.abs(diagH) * 0.0 + 1e-9, diagH)\n\n            # Compute subspace minimizer elementwise: s* = -b / diagH_safe\n            s_star = -b_hat / (diagH_safe + 1e-16)\n\n            # scale step to respect trust radius: ||basis @ s_star|| <= 1.5*center_r\n            disp = basis @ s_star\n            disp_norm = np.linalg.norm(disp)\n            max_step = max(1e-12, 1.5 * center_r)\n            if disp_norm > max_step:\n                s_star = s_star * (max_step / (disp_norm + 1e-12))\n\n            x_prop = center_x + basis @ s_star\n            x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n\n            # predicted value using diagonal surrogate\n            pred_val = a_hat + b_hat.dot(s_star) + 0.5 * (diagH * (s_star ** 2)).sum()\n            predicted_reduction = center_f - pred_val\n\n            # If predicted reduction is tiny or negative, attempt small descent along -b direction\n            tried_probes = []\n            if predicted_reduction <= 1e-12:\n                # direction in subspace is -b_hat\n                if np.linalg.norm(b_hat) < 1e-12:\n                    sub_dir = self.rng.normal(size=k_sub)\n                else:\n                    sub_dir = -b_hat\n                sub_dir = sub_dir / (np.linalg.norm(sub_dir) + 1e-12)\n                frac = 0.6 * center_r\n                s_try = sub_dir * frac\n                x_prop = center_x + basis @ s_try\n                x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n\n            # Evaluate main candidate if budget\n            if evals >= self.budget:\n                break\n            f_prop = float(func(x_prop))\n            evals += 1\n            X.append(x_prop.copy()); F.append(f_prop); radii.append(max(self.min_radius, min(self.max_radius, center_r * 0.9)))\n\n            # compute rho safely\n            actual_reduction = center_f - f_prop\n            denom = max(1e-12, predicted_reduction)\n            rho = actual_reduction / denom if denom > 0 else (1.0 if actual_reduction > 0 else 0.0)\n\n            # trust update: conservative rules\n            if rho > 0.8:\n                radii[center_idx] = min(self.max_radius, center_r * 1.5)\n                radii[-1] = min(self.max_radius, radii[-1] * 1.2)\n            elif rho > 0.3:\n                radii[center_idx] = min(self.max_radius, center_r * 1.1)\n            else:\n                radii[center_idx] = max(self.min_radius, center_r * 0.6)\n\n            # update best and stagnation\n            if f_prop < best_f:\n                best_f = f_prop\n                best_x = x_prop.copy()\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # small greedy line probes (cheap local exploitation) \u2014 up to two extra evals\n            if evals < self.budget and self.rng.random() < 0.25:\n                # try scaled reductions of s_star: 0.5 and 0.25\n                scales = [0.5, 0.25]\n                for sc in scales:\n                    if evals >= self.budget:\n                        break\n                    s_probe = s_star * sc\n                    x_probe = np.clip(center_x + basis @ s_probe, lb, ub)\n                    f_probe = float(func(x_probe))\n                    evals += 1\n                    X.append(x_probe.copy()); F.append(f_probe); radii.append(max(self.min_radius, center_r * 0.8))\n                    if f_probe < best_f:\n                        best_f = f_probe; best_x = x_probe.copy(); stagnation = 0\n\n            # occasional cheap interpolation between center and best\n            if evals < self.budget and self.rng.random() < 0.12:\n                alpha = self.rng.random() * 0.5\n                x_probe = np.clip(alpha * center_x + (1 - alpha) * best_x, lb, ub)\n                f_probe = float(func(x_probe))\n                evals += 1\n                X.append(x_probe.copy()); F.append(f_probe); radii.append(self.init_radius * 0.8)\n                if f_probe < best_f:\n                    best_f = f_probe; best_x = x_probe.copy(); stagnation = 0\n\n            # diversification if stagnation high\n            if stagnation >= 10 and evals < self.budget:\n                # generate 2..dim//3 global samples\n                n_global = min(6, max(2, self.dim // 3))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg))\n                    evals += 1\n                    X.append(xg.copy()); F.append(fg); radii.append(self.init_radius)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagnation = 0\n                # one opposite-of-best probe to try exploit symmetry\n                if evals < self.budget:\n                    xopp = np.clip(best_x * -0.8 + (self.rng.random(self.dim) - 0.5) * 0.05 * (ub - lb), lb, ub)\n                    fopp = float(func(xopp))\n                    evals += 1\n                    X.append(xopp.copy()); F.append(fopp); radii.append(self.init_radius)\n                    if fopp < best_f:\n                        best_f = fopp; best_x = xopp.copy(); stagnation = 0\n                    else:\n                        # mild decay reset to encourage new local fits\n                        radii = [max(self.min_radius, 0.7 * r) for r in radii]\n                        stagnation = 0\n\n            # trim archive to keep cost manageable\n            max_archive = max(150, 6 * self.dim + 40)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = int(max(20, 3 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                # sample additional survivors by weighted probabilities favoring good ones\n                probs = np.exp(-(np.array(F) - np.min(F)) / (1e-8 + max(1.0, np.std(F))))\n                probs /= probs.sum()\n                extras = min(len(X) - keep_best, max_archive - keep_best)\n                if extras > 0:\n                    sel = self.rng.choice(len(X), size=extras, replace=False, p=probs)\n                    for s in sel:\n                        keep_set.add(int(s))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                radii = [radii[i] for i in keep_list]\n\n        # final best\n        best_i = int(np.argmin(F))\n        return float(F[best_i]), np.array(X[best_i], dtype=float)", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDiagonalLowRankQuadraticSurrogates scored 0.096 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0c98b60a-b401-4b27-b5de-4c183326b288"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.11487315655842611}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.09339040143012711}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.09673234960252541}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.025625198413631867}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0009480924200399032}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0013612942834606567}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.048194039589250504}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9753812549737486}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.0775142819566409}], "aucs": [0.11487315655842611, 0.09339040143012711, 0.09673234960252541, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.025625198413631867, 4.999999999999449e-05, 4.999999999999449e-05, 0.0009480924200399032, 4.999999999999449e-05, 0.0013612942834606567, 0.048194039589250504, 0.9753812549737486, 0.0775142819566409]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3111.0, "Edges": 3110.0, "Max Degree": 67.0, "Min Degree": 1.0, "Mean Degree": 1.9993571198971392, "Degree Variance": 2.8678877255672752, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.063263838964774, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3180163516891228, "Depth Entropy": 2.001247772638803, "Assortativity": 7.828803515382554e-09, "Average Eccentricity": 17.919639987142396, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003214400514304082, "Average Shortest Path": 9.780340064970167, "mean_complexity": 16.0, "total_complexity": 64.0, "mean_token_count": 676.0, "total_token_count": 2704.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "baa79ccd-085d-46f2-9189-881253aa493b", "fitness": 0.1827234967404749, "name": "SparseDegreeLimitedEdgeSearch", "description": "Sparse Degree-Limited Edge + 1D Line-Search Heuristic \u2014 build a sparse, degree-capped elite edge graph, generate few high-quality edge-derived candidates, and combine with focused 1D/low-dim line probes around the best to keep solutions simple (low \"degree\") and efficient under tight budgets.", "code": "import numpy as np\n\nclass SparseDegreeLimitedEdgeSearch:\n    \"\"\"\n    Sparse Degree-Limited Edge Search (SDLES)\n\n    Main ideas:\n      - Maintain a small elite archive of best-found points.\n      - Build a sparse, degree-capped edge graph among elites (limit per-elite degree).\n      - Generate a modest pool of virtual candidates along those sparse edges (interpolations & small extrapolations),\n        rank cheaply by linear interpolation + distance bias, and evaluate only the best few.\n      - Combine the edge proposals with focused 1D (coordinate) probes (cheap line-search-like) around current best,\n        and occasional sparse recombination that only changes a few coordinates (encourages low-degree/sparse moves).\n      - Adapt a single trust radius by success/failure to control step sizes.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, rng=None,\n                 init_radius=2.0, min_radius=1e-6, max_radius=4.5,\n                 max_archive=40, max_degree=2,\n                 edge_candidates_pool=120, max_edge_evals_per_cycle=8,\n                 coord_probes_per_cycle=4):\n        \"\"\"\n        Arguments:\n          budget: total function evaluations allowed\n          dim: dimensionality of problem\n          rng: optional numpy.random.Generator\n          init_radius: starting trust radius\n          min_radius, max_radius: radius bounds\n          max_archive: maximum number of elites to keep\n          max_degree: maximum degree per elite in the sparse edge graph\n          edge_candidates_pool: how many virtual candidates to assemble (cheap scoring)\n          max_edge_evals_per_cycle: how many true evaluations to spend on edge candidates per cycle\n          coord_probes_per_cycle: number of 1D coordinate probes per cycle\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_radius = float(init_radius)\n        self.radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_archive = int(max_archive)\n        self.max_degree = int(max_degree)\n        self.edge_candidates_pool = int(edge_candidates_pool)\n        self.max_edge_evals_per_cycle = int(max_edge_evals_per_cycle)\n        self.coord_probes_per_cycle = int(coord_probes_per_cycle)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        # try to read if func provides bounds as an attribute\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shapes consistent\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n        if ub.shape[0] != self.dim:\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        evals = 0\n        archive_x = []\n        archive_f = []\n\n        # Seed archive: a few random samples evenly spread if possible\n        n_init = min(12, max(2, int(self.budget * 0.06)))  # small initial sampling fraction\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n\n        # ensure at least one sample\n        if len(archive_x) == 0:\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n\n        # current best\n        idx_best = int(np.argmin(archive_f))\n        x_best = archive_x[idx_best].copy()\n        f_best = float(archive_f[idx_best])\n\n        def prune_archive():\n            # keep at most max_archive by best f and uniqueness tolerance\n            if len(archive_x) <= self.max_archive:\n                return\n            order = np.argsort(archive_f)\n            kept_x = []\n            kept_f = []\n            for i in order:\n                xi = archive_x[i]\n                fi = archive_f[i]\n                too_close = False\n                for xk in kept_x:\n                    if np.linalg.norm(xi - xk) < 1e-8:\n                        too_close = True; break\n                if not too_close:\n                    kept_x.append(xi.copy()); kept_f.append(fi)\n                if len(kept_x) >= self.max_archive:\n                    break\n            archive_x[:] = kept_x\n            archive_f[:] = kept_f\n\n        stagnation = 0\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n\n            # 1) Focused coordinate (1D) probes around x_best (cheap line-search-like)\n            # We pick coordinates with largest estimated local variability first (via archive differences)\n            # Limit probes by remaining budget\n            num_probes = min(self.coord_probes_per_cycle, remaining)\n            # compute variability per coord from archive (std dev)\n            if len(archive_x) >= 2:\n                X = np.vstack(archive_x)\n                coord_std = np.std(X, axis=0)\n                coord_order = np.argsort(-coord_std)  # descending\n            else:\n                coord_order = np.arange(self.dim)\n            improved = False\n            for k_i in range(num_probes):\n                if evals >= self.budget:\n                    break\n                i = int(coord_order[k_i % self.dim])\n                h = max(1e-8, 0.9 * self.radius)  # step size along coordinate\n                # create two probes: +h and -h\n                x_p = x_best.copy(); x_m = x_best.copy()\n                x_p[i] = np.clip(x_p[i] + h, lb[i], ub[i])\n                x_m[i] = np.clip(x_m[i] - h, lb[i], ub[i])\n                # avoid re-eval duplicates\n                do_p = not any(np.allclose(x_p, xa, atol=1e-12) for xa in archive_x)\n                do_m = not any(np.allclose(x_m, xa, atol=1e-12) for xa in archive_x)\n                # evaluate whichever are allowed and budget permits\n                if do_p and evals < self.budget:\n                    f_p = float(func(x_p)); evals += 1\n                    archive_x.append(x_p.copy()); archive_f.append(f_p)\n                    if f_p < f_best:\n                        f_best = f_p; x_best = x_p.copy(); improved = True\n                        self.radius = min(self.max_radius, self.radius * 1.18)\n                if do_m and evals < self.budget:\n                    f_m = float(func(x_m)); evals += 1\n                    archive_x.append(x_m.copy()); archive_f.append(f_m)\n                    if f_m < f_best:\n                        f_best = f_m; x_best = x_m.copy(); improved = True\n                        self.radius = min(self.max_radius, self.radius * 1.18)\n                # cheap parabolic estimate: if both f_p and f_m known and better than center, fit 3-pt parabola\n                # We need f_center = f_best (center) but ensure it's the value at x_best\n                if improved:\n                    stagnation = 0\n                else:\n                    stagnation += 0.5  # fractional stagnation increment for coord probes\n\n            # 2) Sparse, degree-limited edge graph and candidate generation\n            prune_archive()\n            n_elite = len(archive_x)\n            if n_elite >= 2 and evals < self.budget:\n                X = np.vstack(archive_x)\n                F = np.array(archive_f)\n                # compute ranks (lower is better)\n                ranks = np.argsort(np.argsort(F))\n                # all pairwise edges sorted by preferability: small rank sum and short distance\n                dists = np.linalg.norm(X[:, None, :] - X[None, :, :], axis=2)\n                edges = []\n                for i in range(n_elite):\n                    for j in range(i+1, n_elite):\n                        # prefer edges connecting good (low rank) nodes and shorter distances\n                        score = (ranks[i] + ranks[j]) + 0.01 * dists[i, j]\n                        edges.append((score, i, j, dists[i, j]))\n                edges.sort(key=lambda e: e[0])\n                # greedy add edges while respecting max_degree per node (sparse graph)\n                degree = np.zeros(n_elite, dtype=int)\n                sparse_edges = []\n                for score, i, j, dij in edges:\n                    if degree[i] < self.max_degree and degree[j] < self.max_degree:\n                        sparse_edges.append((i, j))\n                        degree[i] += 1; degree[j] += 1\n                    # stop if we have enough edges to form candidate pool budget\n                    if len(sparse_edges) >= max(1, self.edge_candidates_pool // 4):\n                        break\n\n                # build candidate pool using a limited set of lambdas\n                lambdas = np.array([0.5, 0.25, 0.75, -0.15, 1.15])\n                pool = []\n                for (i, j) in sparse_edges:\n                    xi = X[i]; xj = X[j]\n                    fi = float(F[i]); fj = float(F[j])\n                    # displacement\n                    d = xj - xi\n                    for lam in lambdas:\n                        x_virt = (1.0 - lam) * xi + lam * xj\n                        # cheap prediction by linear interpolation\n                        f_pred = (1.0 - lam) * fi + lam * fj\n                        # bias: prefer candidates closer to current best and from better endpoints\n                        dist_best = np.linalg.norm(x_virt - x_best)\n                        endpoint_bonus = -0.02 * min(fi, fj)  # smaller better\n                        # combine into a score (lower better)\n                        score = f_pred + 0.01 * dist_best + 1e-8 * (endpoint_bonus)\n                        pool.append((score, i, j, lam, x_virt))\n                        if len(pool) >= self.edge_candidates_pool:\n                            break\n                    if len(pool) >= self.edge_candidates_pool:\n                        break\n\n                if len(pool) > 0:\n                    pool.sort(key=lambda e: e[0])\n                    # decide how many to evaluate this cycle\n                    max_edge_eval = min(self.max_edge_evals_per_cycle, remaining, max(1, len(pool)))\n                    # pick top candidates but slight diversity: take some from top and some from near-top\n                    top_fraction = max(1, int(0.7 * len(pool)))\n                    picks = []\n                    # take most of them from top\n                    picks.extend(pool[:min(top_fraction, len(pool))])\n                    # and some randomly sampled from the top region\n                    remaining_pool = pool[:min(len(pool), max(1, int(0.9 * len(pool))))]\n                    # randomly pick a few more unique ones\n                    n_rand = max(0, max_edge_eval - len(picks))\n                    if n_rand > 0 and len(remaining_pool) > len(picks):\n                        idxs = self.rng.choice(len(remaining_pool), size=min(n_rand, len(remaining_pool)), replace=False)\n                        for ii in idxs:\n                            picks.append(remaining_pool[ii])\n                    # finally limit to max_edge_eval\n                    picks = picks[:max_edge_eval]\n\n                    # Evaluate picks, around each add a small sparse perturbation (only change few coords)\n                    for (_, i, j, lam, x_virt) in picks:\n                        if evals >= self.budget:\n                            break\n                        # generate sparse mask: change only top-k coordinates of |xj-xi|\n                        d = X[j] - X[i]\n                        absd = np.abs(d)\n                        if np.any(absd > 1e-12):\n                            # pick at most  max(1, dim//6) coords with largest differences to enforce sparse moves\n                            k_sparse = max(1, min(self.dim, int(max(1, self.dim // 6))))\n                            idxs = np.argsort(-absd)[:k_sparse]\n                            # build small perturbation only on these coordinates\n                            v = np.zeros(self.dim)\n                            v[idxs] = self.rng.normal(scale=0.12 * self.radius, size=k_sparse)\n                            x_cand = x_virt.copy()\n                            x_cand[idxs] += v[idxs]\n                        else:\n                            # full-jitter fallback\n                            x_cand = x_virt + self.rng.normal(scale=0.08 * self.radius, size=self.dim)\n\n                        # clip to bounds\n                        x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                        # skip if extremely close to existing elites\n                        if any(np.linalg.norm(x_cand - xa) < 1e-9 for xa in archive_x):\n                            continue\n                        f_cand = float(func(x_cand)); evals += 1\n                        archive_x.append(x_cand.copy()); archive_f.append(f_cand)\n                        if f_cand < f_best:\n                            f_best = f_cand; x_best = x_cand.copy()\n                            # reward success by expanding radius a bit\n                            self.radius = min(self.max_radius, self.radius * 1.22)\n                            stagnation = 0\n                        else:\n                            # mild shrink on non-improvement\n                            self.radius = max(self.min_radius, self.radius * 0.96)\n                            stagnation += 0.5\n\n            # 3) Occasional sparse recombination across two elites (few coords changed)\n            if evals < self.budget and self.rng.random() < 0.18:\n                # pick two elites: prefer one is best and another random good one\n                idx_a = np.argmin(archive_f)\n                idx_b = int(self.rng.integers(0, len(archive_x)))\n                if idx_b == idx_a and len(archive_x) > 1:\n                    idx_b = (idx_b + 1) % len(archive_x)\n                xa = archive_x[idx_a]; xb = archive_x[idx_b]\n                # choose small number of coordinates to swap/perturb\n                k_swap = max(1, min(self.dim, int(self.dim // 8)))\n                # choose indices where endpoints differ most\n                diffs = np.abs(xa - xb)\n                swap_idx = np.argsort(-diffs)[:k_swap]\n                x_new = x_best.copy()\n                # move x_best towards xb on those coords by random fraction\n                frac = self.rng.uniform(0.2, 0.9)\n                x_new[swap_idx] = (1 - frac) * x_best[swap_idx] + frac * xb[swap_idx]\n                x_new = np.minimum(np.maximum(x_new, lb), ub)\n                if not any(np.allclose(x_new, xa2, atol=1e-12) for xa2 in archive_x) and evals < self.budget:\n                    f_new = float(func(x_new)); evals += 1\n                    archive_x.append(x_new.copy()); archive_f.append(f_new)\n                    if f_new < f_best:\n                        f_best = f_new; x_best = x_new.copy()\n                        self.radius = min(self.max_radius, self.radius * 1.2)\n                        stagnation = 0\n                    else:\n                        stagnation += 0.5\n                        self.radius = max(self.min_radius, self.radius * 0.98)\n\n            # 4) Stagnation-driven diversification\n            if stagnation > 6 and evals < self.budget:\n                # small number of global random samples\n                n_global = min(6, self.budget - evals)\n                improved_flag = False\n                for _ in range(n_global):\n                    if evals >= self.budget: break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg)); evals += 1\n                    archive_x.append(xg.copy()); archive_f.append(fg)\n                    if fg < f_best:\n                        f_best = fg; x_best = xg.copy()\n                        self.radius = self.init_radius\n                        stagnation = 0\n                        improved_flag = True\n                        break\n                if not improved_flag:\n                    # try a larger sparse jump around a random elite\n                    idx = int(self.rng.integers(0, len(archive_x)))\n                    base = archive_x[idx]\n                    jump_scale = 0.9 * (ub - lb)\n                    xjump = base + self.rng.normal(scale=1.0, size=self.dim) * jump_scale\n                    xjump = np.minimum(np.maximum(xjump, lb), ub)\n                    if evals < self.budget:\n                        fj = float(func(xjump)); evals += 1\n                        archive_x.append(xjump.copy()); archive_f.append(fj)\n                        if fj < f_best:\n                            f_best = fj; x_best = xjump.copy()\n                            self.radius = self.init_radius\n                            stagnation = 0\n                        else:\n                            stagnation = 0\n                            self.radius = max(self.min_radius, self.radius * 0.5)\n\n            # minor housekeeping: ensure radius in bounds, prune archive, update best if needed\n            self.radius = np.clip(self.radius, self.min_radius, self.max_radius)\n            prune_archive()\n            # ensure x_best present in archive\n            if not any(np.allclose(x_best, xa, atol=1e-12) for xa in archive_x):\n                archive_x.append(x_best.copy()); archive_f.append(f_best)\n                prune_archive()\n\n            # tighten stagnation increment a bit if iterations continue with no real improvements\n            # (we already modified stagnation inside steps)\n\n            # safety break if no budget remains\n            if evals >= self.budget:\n                break\n\n        # final return\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 9, "feedback": "The algorithm SparseDegreeLimitedEdgeSearch scored 0.183 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7516fc34-d43c-4696-bca7-3524b4b1cf90"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.20748095240350717}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.10276670565997714}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.17983226383121453}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.04189895161592794}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0042667485187113385}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03957789334952899}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.004379817842454781}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.04236355144490733}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.4737656211019261}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.6579447780651797}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9863751672737888}], "aucs": [0.20748095240350717, 0.10276670565997714, 0.17983226383121453, 4.999999999999449e-05, 0.04189895161592794, 4.999999999999449e-05, 0.0042667485187113385, 0.03957789334952899, 0.004379817842454781, 0.04236355144490733, 4.999999999999449e-05, 4.999999999999449e-05, 0.4737656211019261, 0.6579447780651797, 0.9863751672737888]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3112.0, "Edges": 3111.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.999357326478149, "Degree Variance": 1.906811926302364, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.762662807525325, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.332243017062305, "Depth Entropy": 2.157614084594161, "Assortativity": 0.0, "Average Eccentricity": 18.047879177377894, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00032133676092544985, "Average Shortest Path": 10.830766564285119, "mean_complexity": 18.5, "total_complexity": 74.0, "mean_token_count": 677.5, "total_token_count": 2710.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "8ee78a7c-fcff-4c26-baac-f5bca6f872d3", "fitness": "-inf", "name": "LRADS", "description": "Low-Rank Adaptive Directional Search with Sparse L\u00e9vy Escapes (LRADS) \u2014 maintain a small set of recent successful direction vectors (low-rank directional memory), propose candidates by mixing isotropic Gaussian noise with a sparse linear combination of those directions, adapt step-size by success rate, and use rare normalized Cauchy (L\u00e9vy-like) jumps plus light restarts to escape basins. This keeps per-evaluation complexity low (reduced \"degree\") while focusing search along promising low-dimensional manifolds.", "code": "import numpy as np\nfrom collections import deque\n\nclass LRADS:\n    \"\"\"\n    Low-Rank Adaptive Directional Search with Sparse L\u00e9vy Escapes (LRADS).\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional tuning (kept small and simple to reduce algorithmic degree):\n      - rng: numpy Generator or None\n      - init_fraction: fraction of budget used for uniform initial sampling\n      - k_dirs: number of stored direction vectors (low-rank memory)\n      - jump_prob: base probability for heavy-tailed L\u00e9vy jump\n      - target_success: target success rate for gentle sigma control\n      - sigma0: initial step-size (if None derived from search range)\n      - stagnation: iterations without improvement before escalation\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_fraction=0.08, k_dirs=3, jump_prob=0.04,\n                 target_success=0.2, sigma0=None, stagnation=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # Tunable but intentionally small/simple to reduce algorithmic degree\n        self.init_fraction = float(init_fraction)\n        self.k_dirs = max(1, int(k_dirs))\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n        self.sigma0 = None if sigma0 is None else float(sigma0)\n        self.stagnation = max(10, int(stagnation))\n\n    def _safe_bounds(self, func):\n        # Attempt to read bounds from func; otherwise fall back to [-5,5] per spec\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            except Exception:\n                lb, ub = None, None\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        return lb, ub\n\n    def _levy_dir(self, size):\n        # normalized Cauchy ~ heavy-tailed direction (L\u00e9vy-like behavior)\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        # guard\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        n = np.linalg.norm(z)\n        if n == 0:\n            return self.rng.standard_normal(size=size)\n        return z / n\n\n    def _reflect(self, x, lb, ub):\n        # reflection at boundaries (keeps points inside without hard clipping bias)\n        # reflect repeatedly if necessary\n        x = np.asarray(x, dtype=float)\n        over = x > ub\n        under = x < lb\n        if np.any(over) or np.any(under):\n            x = np.where(over, ub - (x - ub), x)\n            x = np.where(under, lb + (lb - x), x)\n            # final safety clip\n            x = np.clip(x, lb, ub)\n        return x\n\n    def __call__(self, func):\n        lb, ub = self._safe_bounds(func)\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        evals = 0\n        f_best = float(\"inf\")\n        x_best = rng.uniform(lb, ub)\n\n        # initial sampling (small uniform exploration)\n        n_init = max(4, int(min(budget, max(10, int(budget * self.init_fraction)))))\n        n_init = min(n_init, budget)\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n\n        # set initial sigma relative to mean range\n        range_scale = float(np.mean(ub - lb))\n        if self.sigma0 is None:\n            sigma = max(1e-8, 0.18 * range_scale)\n        else:\n            sigma = float(self.sigma0)\n        # keep sigma bounds sane\n        sigma_min = 1e-8 * range_scale + 1e-12\n        sigma_max = 8.0 * range_scale + 1e-12\n        sigma = float(np.clip(sigma, sigma_min, sigma_max))\n\n        # low-rank directional memory (store normalized direction vectors)\n        dirs = deque(maxlen=self.k_dirs)\n\n        # EMA of success and simple multiplicative control (kept simple)\n        p_succ = 0.0\n        ema_alpha = 0.14\n        since_improve = 0\n\n        # small batch to amortize cost but keep reactive\n        while evals < budget:\n            remaining = budget - evals\n            batch = min(6, remaining)  # small batch\n            for _ in range(batch):\n                # decide jump or local mixed proposal\n                if rng.random() < self.jump_prob:\n                    # larger heavy-tailed jump\n                    jump = self._levy_dir(dim) * (2.0 * range_scale)\n                    x_trial = x_best + jump\n                    # after a heavy jump, clear or shrink directional memory (encourage new directions)\n                    # but do not completely forget: shrink existing directions\n                    if dirs:\n                        # decay directions by removing smallest probability item occasionally\n                        if rng.random() < 0.4:\n                            dirs.pop()\n                else:\n                    # Gaussian isotropic base\n                    z = rng.normal(size=dim)\n                    # combine with a sparse linear combination of stored directions\n                    if dirs:\n                        # draw small coefficients for each stored direction\n                        coeffs = rng.normal(loc=0.0, scale=0.8, size=len(dirs))\n                        mix = np.zeros(dim)\n                        for c, d in zip(coeffs, dirs):\n                            mix += c * d\n                        dx = z + mix * 0.8  # mix weight constant keeps low-degree behavior\n                    else:\n                        dx = z\n                    x_trial = x_best + sigma * dx\n\n                # reflect to stay within bounds\n                x_trial = self._reflect(x_trial, lb, ub)\n\n                # evaluate\n                f_trial = float(func(x_trial))\n                evals += 1\n\n                improved = False\n                if f_trial < f_best:\n                    improved = True\n                    # capture normalized direction (scaled by step size) relative to best\n                    step = x_trial - x_best\n                    nrm = np.linalg.norm(step)\n                    if nrm > 1e-12:\n                        dvec = step / nrm\n                        # orthogonalize against existing directions to maintain diversity\n                        if dirs:\n                            # simple Gram-Schmidt against small set\n                            d = dvec.copy()\n                            for od in list(dirs):\n                                proj = np.dot(d, od)\n                                d = d - proj * od\n                            dn = np.linalg.norm(d)\n                            if dn > 1e-12:\n                                dvec = d / dn\n                            else:\n                                # if nearly dependent, keep original normalized step\n                                dvec = dvec\n                        dirs.appendleft(dvec)  # newest first\n                    # update best\n                    x_best = x_trial.copy()\n                    f_best = f_trial\n                    since_improve = 0\n                else:\n                    since_improve += 1\n\n                # update success EMA and sigma (gentle multiplicative changes)\n                p_succ = (1.0 - ema_alpha) * p_succ + ema_alpha * (1.0 if improved else 0.0)\n                # multiplicative factor derived from success deviation (bounded)\n                factor = np.exp(0.9 * (p_succ - self.target_success) / max(1e-6, np.sqrt(dim)))\n                factor = float(np.clip(factor, 0.7, 1.4))\n                sigma *= factor\n                sigma = float(np.clip(sigma, sigma_min, sigma_max))\n\n                # minor adaptive reductions of jump_prob when things are improving, increase when stuck\n                if improved:\n                    self.jump_prob = max(0.005, self.jump_prob * 0.95)\n                else:\n                    # slightly increase chance to escape local optima slowly\n                    self.jump_prob = min(0.5, self.jump_prob * 1.001)\n\n                if evals >= budget:\n                    break\n\n            # stagnation escalation: small, cheap maneuvers to escape plateaus\n            if since_improve >= self.stagnation and evals < budget:\n                # try a handful of broad random probes with larger sigma\n                probes = min(6, budget - evals)\n                for _ in range(probes):\n                    if rng.random() < 0.5:\n                        center = rng.uniform(lb, ub)\n                    else:\n                        center = x_best\n                    x_try = center + 1.8 * sigma * rng.normal(size=dim)\n                    x_try = self._reflect(x_try, lb, ub)\n                    f_try = float(func(x_try))\n                    evals += 1\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                        # encourage local search by injecting the successful direction\n                        step = x_try - center\n                        nrm = np.linalg.norm(step)\n                        if nrm > 1e-12:\n                            dvec = step / nrm\n                            dirs.appendleft(dvec)\n                        # slightly reduce sigma to refine\n                        sigma = max(sigma * 0.7, sigma_min)\n                        since_improve = 0\n                        break\n                    if evals >= budget:\n                        break\n                # if still stuck, temporarily increase jump probability and sigma moderately\n                if since_improve >= self.stagnation:\n                    self.jump_prob = min(0.5, self.jump_prob * 1.6)\n                    sigma = min(sigma * 1.8, sigma_max)\n                    # also occasionally clear oldest direction to avoid cycling\n                    if dirs and rng.random() < 0.6:\n                        dirs.pop()\n\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 9, "feedback": "In the code, line 109, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dirs = deque(maxlen=self.k_dirs)", "error": "In the code, line 109, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dirs = deque(maxlen=self.k_dirs)", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1581.0, "Edges": 1580.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.9987349778621126, "Degree Variance": 1.9721679126854672, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.726884779516358, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3385786031766442, "Depth Entropy": 2.119814369872012, "Assortativity": 0.0, "Average Eccentricity": 16.21821631878558, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.0006325110689437065, "Average Shortest Path": 10.282695618059392, "mean_complexity": 8.8, "total_complexity": 44.0, "mean_token_count": 283.8, "total_token_count": 1419.0, "mean_parameter_count": 4.0, "total_parameter_count": 20.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "859f20dd-2220-495e-9580-9351c1888f4d", "fitness": "-inf", "name": "SparseLinearEnsembleDescent", "description": "Sparse Linear Ensemble Descent (SLED) \u2014 build many low-degree (affine) directional models from a small elite set (principal directions + sparse axis cues), perform adaptive 1D line searches along those linear directions, and use cheap linear recombination/midpoints to propose and greedily accept improvements; strictly use linear (degree-1) predictions for ranking to keep model degree low and computations sparse.", "code": "import numpy as np\n\nclass SparseLinearEnsembleDescent:\n    \"\"\"\n    Sparse Linear Ensemble Descent (SLED)\n\n    Key ideas:\n      - Maintain a small elite archive of best evaluated points.\n      - Build a sparse low-degree ensemble of search directions:\n          * principal directions from differences among elites (SVD)\n          * a few sparse axis-aligned cues coming from largest coordinate differences\n      - Perform short, adaptive 1D line-search probes along each direction (affine moves only).\n      - Generate a few cheap linear-recombination candidates (midpoints/extrapolations),\n        rank them by linear interpolation predictions (degree-1), and evaluate top predicted ones.\n      - Keep step sizes adaptive (expand on success, shrink on failure).\n      - Periodic sparse, coordinate-wise perturbation diversification.\n      - Works only with affine (linear) surrogate/prediction for ranking (keeps 'max degree' low).\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_step=1.0, min_step=1e-6, max_step=2.5,\n                 elite_size=None, stagnation_patience=8, diversify_freq=18, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.stagnation_patience = int(stagnation_patience)\n        self.diversify_freq = int(diversify_freq)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        if elite_size is None:\n            # keep archive small and adaptive to dimension\n            self.elite_size = min(8 + int(self.dim // 6), max(4, self.dim // 2))\n        else:\n            self.elite_size = int(elite_size)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        # try to read bounds from func if available (best-effort)\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.size != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n        if ub.size != self.dim:\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        rng = self.rng\n\n        # initial sampling: a small diverse seed\n        n_init = min(max(6, 4 * self.dim), max(2, self.budget // 12))\n        n_init = max(1, n_init)\n        archive = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            archive.append((f, x.copy()))\n        if len(archive) == 0:\n            return None, None\n\n        archive.sort(key=lambda t: t[0])\n        archive = archive[:self.elite_size]\n        f_best, x_best = archive[0][0], archive[0][1].copy()\n\n        step = float(self.init_step)\n        stagnation = 0\n        iter_count = 0\n\n        # helper to update archive while keeping best\n        def update_archive(fx, xx):\n            nonlocal archive, f_best, x_best\n            archive.append((fx, xx.copy()))\n            archive.sort(key=lambda t: t[0])\n            if len(archive) > self.elite_size:\n                archive = archive[:self.elite_size]\n            if fx < f_best:\n                f_best = float(fx)\n                x_best = xx.copy()\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            remaining = self.budget - evals\n\n            # quick refresh of elites\n            archive.sort(key=lambda t: t[0])\n            elites = [x for (_, x) in archive]\n            elites_f = [f for (f, _) in archive]\n            E = len(elites)\n\n            # construct sparse low-degree direction ensemble (affine only)\n            directions = []\n\n            # 1) principal directions from SVD of elite diffs (if available)\n            if E >= 2:\n                X = np.vstack([e - x_best for e in elites if not np.allclose(e, x_best)])\n                if X.shape[0] >= 1:\n                    # compute up to 3 principal components (safely)\n                    try:\n                        U, S, Vt = np.linalg.svd(X, full_matrices=False)\n                        pcs = Vt[:min(3, Vt.shape[0])]\n                        for v in pcs:\n                            nrm = np.linalg.norm(v)\n                            if nrm > 1e-12:\n                                directions.append(v / nrm)\n                    except Exception:\n                        pass\n\n            # 2) a few sparse axis cues: coordinates with largest absolute spread among elites\n            if E >= 2:\n                coords_spread = np.ptp(np.vstack(elites), axis=0)  # peak-to-peak\n                top_k = min(3, max(1, int(np.ceil(self.dim * 0.12))))\n                top_idx = np.argsort(-coords_spread)[:top_k]\n                for i in top_idx:\n                    e = np.zeros(self.dim, dtype=float)\n                    e[i] = 1.0\n                    directions.append(e)\n\n            # 3) fallback: random sparse directions (very few)\n            while len(directions) < 3 and len(directions) < self.dim:\n                z = rng.normal(size=self.dim)\n                # sparsify: keep only a few largest coords\n                keep = max(1, int(np.ceil(self.dim * 0.12)))\n                idxs = np.argsort(-np.abs(z))[:keep]\n                s = np.zeros(self.dim)\n                s[idxs] = z[idxs]\n                nrm = np.linalg.norm(s)\n                if nrm > 1e-12:\n                    directions.append(s / nrm)\n\n            # ensure unique normalized directions\n            if len(directions) > 1:\n                uniq = []\n                keys = set()\n                for d in directions:\n                    key = tuple(np.round(d / (np.linalg.norm(d) + 1e-16), 8))\n                    if key not in keys:\n                        keys.add(key)\n                        uniq.append(d)\n                directions = uniq\n\n            # limit number of directions per iteration (controls degree of branching)\n            max_dirs = min(6, max(1, len(directions)))\n            directions = directions[:max_dirs]\n\n            improved_this_iter = False\n\n            # Directional 1D adaptive probes (affine-only moves)\n            for d in directions:\n                if evals >= self.budget:\n                    break\n\n                # ensure unit direction\n                nd = d.copy()\n                nrm = np.linalg.norm(nd)\n                if nrm < 1e-12:\n                    continue\n                nd /= nrm\n\n                # propose alphas relative to current step (affine moves)\n                alphas = [1.0, 0.5, -0.5, -1.0, 2.0, -2.0]\n                # order alphas to try more likely beneficial moves first (small positive)\n                alphas = sorted(alphas, key=lambda a: (abs(a - 0.5), a))\n                tried_any = False\n                improved_dir = False\n                for a in alphas:\n                    if evals >= self.budget:\n                        break\n                    alpha = a * step\n                    x_try = x_best + alpha * nd\n                    x_try = np.minimum(np.maximum(x_try, lb), ub)\n                    # don't re-evaluate identical to a known elite point (cheap hash)\n                    # but we are conservative: evaluate if not exactly equal to best\n                    if np.allclose(x_try, x_best, atol=1e-12):\n                        continue\n                    f_try = float(func(x_try))\n                    evals += 1\n                    tried_any = True\n                    update_archive(f_try, x_try)\n                    if f_try < f_best - 1e-15:\n                        # success: accept move as new base for further probes in direction\n                        improved_this_iter = True\n                        improved_dir = True\n                        stagnation = 0\n                        # modestly expand step\n                        step = min(self.max_step, step * 1.25)\n                        # continue probing positive side a bit (attempt small refinement)\n                        # but avoid too many additional evaluations: try one refinement if budget\n                        if evals < self.budget:\n                            x_ref = x_try + 0.4 * alpha * nd\n                            x_ref = np.minimum(np.maximum(x_ref, lb), ub)\n                            if not np.allclose(x_ref, x_try):\n                                f_ref = float(func(x_ref))\n                                evals += 1\n                                update_archive(f_ref, x_ref)\n                                if f_ref < f_best:\n                                    step = min(self.max_step, step * 1.15)\n                        break\n                if not tried_any:\n                    # nothing tried, skip\n                    continue\n                if not improved_dir:\n                    # penalize unsuccessful direction by shrinking step modestly\n                    step = max(self.min_step, step * 0.72)\n                    stagnation += 1\n\n            # cheap linear recombination / midpoint phase (degree-1 ranking only)\n            # take a small subset of best elites and form midpoints/extrapolations\n            if evals < self.budget and E >= 2:\n                top_k = min(6, E)\n                top_idxs = list(range(top_k))\n                lin_candidates = []\n                for i in range(top_k):\n                    xi = elites[i]\n                    fi = elites_f[i]\n                    # mix with best (affine)\n                    mix_points = [0.25, 0.5, 0.75, 1.25]\n                    for t in mix_points:\n                        x_c = (1 - t) * x_best + t * xi\n                        x_c = np.minimum(np.maximum(x_c, lb), ub)\n                        # predicted f: linear interpolation between endpoints (degree-1)\n                        f_pred = (1 - t) * f_best + t * fi\n                        lin_candidates.append((f_pred, x_c))\n                # unique and rank by predicted only (no surrogate higher degree)\n                uniq = {}\n                for fp, xc in lin_candidates:\n                    key = tuple(np.round(xc, 8))\n                    if key not in uniq or fp < uniq[key][0]:\n                        uniq[key] = (fp, xc)\n                lin_candidates = list(uniq.values())\n                lin_candidates.sort(key=lambda t: t[0])\n                # evaluate only top few according to predicted values\n                max_eval_lin = min(len(lin_candidates), max(1, remaining // 8, 2))\n                for idx in range(max_eval_lin):\n                    if evals >= self.budget:\n                        break\n                    x_try = lin_candidates[idx][1]\n                    # skip if essentially identical to known best\n                    if np.allclose(x_try, x_best, atol=1e-12):\n                        continue\n                    f_try = float(func(x_try))\n                    evals += 1\n                    update_archive(f_try, x_try)\n                    if f_try < f_best:\n                        improved_this_iter = True\n                        stagnation = 0\n                        step = min(self.max_step, step * 1.2)\n\n            # occasional sparse coordinate perturbation for diversification\n            if (iter_count % self.diversify_freq == 0 and evals < self.budget) or (stagnation >= self.stagnation_patience // 2 and evals < self.budget):\n                # perturb only a small subset of coordinates to keep moves low-degree/sparse\n                k = max(1, int(np.ceil(self.dim * 0.12)))\n                coords = rng.choice(self.dim, size=k, replace=False)\n                perturb_scale = 0.28 * step * (ub - lb).mean()\n                x_pert = x_best.copy()\n                x_pert[coords] += rng.normal(0, 1.0, size=k) * perturb_scale\n                x_pert = np.minimum(np.maximum(x_pert, lb), ub)\n                if not np.allclose(x_pert, x_best):\n                    f_pert = float(func(x_pert))\n                    evals += 1\n                    update_archive(f_pert, x_pert)\n                    if f_pert < f_best:\n                        improved_this_iter = True\n                        stagnation = 0\n                        step = min(self.max_step, self.init_step)\n                    else:\n                        stagnation += 1\n\n            # forced global diversification on prolonged stagnation\n            if stagnation >= self.stagnation_patience and evals < self.budget:\n                # try a few global randoms, but then quickly return\n                tries = min(4, max(1, (self.budget - evals) // 12))\n                found = False\n                for _ in range(tries):\n                    if evals >= self.budget:\n                        break\n                    x_r = lb + rng.random(self.dim) * (ub - lb)\n                    f_r = float(func(x_r))\n                    evals += 1\n                    update_archive(f_r, x_r)\n                    if f_r < f_best:\n                        found = True\n                        stagnation = 0\n                        step = min(self.max_step, self.init_step)\n                        break\n                if not found and evals < self.budget:\n                    # gentle restart: sample around a random elite but sparse\n                    anchor_idx = rng.integers(0, max(1, len(archive)))\n                    anchor = archive[anchor_idx][1]\n                    kcoord = max(1, int(np.ceil(self.dim * 0.18)))\n                    coords = rng.choice(self.dim, size=kcoord, replace=False)\n                    jump = np.zeros(self.dim)\n                    jump[coords] = rng.normal(0, 1.0, size=kcoord) * (0.9 * (ub - lb).mean())\n                    x_jump = np.minimum(np.maximum(anchor + jump, lb), ub)\n                    if not np.allclose(x_jump, anchor):\n                        f_jump = float(func(x_jump))\n                        evals += 1\n                        update_archive(f_jump, x_jump)\n                        if f_jump < f_best:\n                            stagnation = 0\n                            step = min(self.max_step, self.init_step)\n                        else:\n                            # reduce step to encourage finer local search\n                            step = max(self.min_step, step * 0.45)\n                            stagnation = 0  # restart stagnation cycles after diversification\n\n            # update stagnation if no improvement in this outer iter\n            if not improved_this_iter:\n                stagnation += 1\n                # very gently shrink step\n                step = max(self.min_step, step * 0.94)\n            else:\n                stagnation = 0\n\n            # safety clamp step and sanity on evals\n            step = float(np.clip(step, self.min_step, self.max_step))\n            if evals >= self.budget:\n                break\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 9, "feedback": "In the code, line 292, in vstack, the following error occurred:\nValueError: need at least one array to concatenate\nOn line: if evals >= self.budget:", "error": "In the code, line 292, in vstack, the following error occurred:\nValueError: need at least one array to concatenate\nOn line: if evals >= self.budget:", "parent_ids": ["a66bcb4a-d4ad-4cd2-bcfd-3ff1a37e3197"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2636.0, "Edges": 2635.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.9992412746585735, "Degree Variance": 1.9104698340475408, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.391925988225399, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.338636454665657, "Depth Entropy": 2.103981563473035, "Assortativity": 1.5998742132390138e-08, "Average Eccentricity": 17.44802731411229, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00037936267071320183, "Average Shortest Path": 10.47898949878057, "mean_complexity": 18.5, "total_complexity": 74.0, "mean_token_count": 573.5, "total_token_count": 2294.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "e4846193-25cf-45f4-ba2c-eb3bcfa534c0", "fitness": "-inf", "name": "SparseAdaptiveSubspaceTriangulation", "description": "Sparse Adaptive Subspace Triangulation (SAST) \u2014 lean, low-degree heuristic that focuses search in randomly chosen small subspaces (1\u20133 dims), fits cheap local quadratic surrogates from nearby archive points, solves the subspace minimizer inside a trust region, and supplements with sparse barycentric/line proposals and coordinate polishing.", "code": "import numpy as np\n\nclass SparseAdaptiveSubspaceTriangulation:\n    \"\"\"\n    Sparse Adaptive Subspace Triangulation (SAST)\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Optional args:\n      init_points: number of initial random samples (kept small)\n      seed: RNG seed\n      trust_init: initial trust fraction of global range norm\n      proposals_per_cycle: number of actual function evaluations attempted per cycle\n      max_subspace: maximum subspace dimensionality (typically 1..3)\n    \"\"\"\n    def __init__(self, budget, dim, init_points=None, seed=None,\n                 trust_init=0.20, proposals_per_cycle=2, max_subspace=3):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.trust_init = float(trust_init)\n        self.proposals_per_cycle = max(1, int(proposals_per_cycle))\n        self.max_subspace = max(1, min(int(max_subspace), self.dim))\n        # keep initialization lightweight to lower algorithmic \"degree\"\n        if init_points is None:\n            self.init_points = min(max(6, 2*self.dim), max(6, int(np.sqrt(self.budget)*1.2)))\n        else:\n            self.init_points = int(init_points)\n\n    def _get_bounds(self, func):\n        # default bound [-5,5]; respect func.bounds if provided\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        trust = max(1e-12, self.trust_init * range_norm)\n\n        # archive of evaluated points (kept as lists for easy append)\n        X = []\n        F = []\n        evals = 0\n\n        # initial sampling (small)\n        n_init = min(self.init_points, self.budget)\n        for _ in range(n_init):\n            if evals >= self.budget: break\n            x = rng.uniform(lb, ub)\n            f = float(func(x)); evals += 1\n            X.append(x.copy()); F.append(f)\n\n        # ensure we have at least one point\n        if len(X) == 0 and evals < self.budget:\n            x = rng.uniform(lb, ub)\n            f = float(func(x)); evals += 1\n            X.append(x.copy()); F.append(f)\n\n        X = np.asarray(X)\n        F = np.asarray(F)\n        # current best\n        idx_best = int(np.argmin(F))\n        x_best = X[idx_best].copy()\n        f_best = float(F[idx_best])\n\n        stagnation = 0\n        cycle = 0\n\n        # helper: evaluate candidate safely respecting budget\n        def eval_candidate(xcand):\n            nonlocal evals, X, F, x_best, f_best\n            if evals >= self.budget:\n                return None\n            xc = np.clip(xcand, lb, ub)\n            fc = float(func(xc)); evals += 1\n            X = np.vstack([X, xc[None, :]]) if X.size else np.array([xc])\n            F = np.hstack([F, fc]) if F.size else np.array([fc])\n            if fc < f_best:\n                f_best = fc\n                x_best = xc.copy()\n            return fc\n\n        # main loop: propose small number of candidates per cycle using sparse subspace quadratics\n        while evals < self.budget:\n            cycle += 1\n            improved = False\n            # determine how many actual evaluations to use this cycle\n            K = min(self.proposals_per_cycle, self.budget - evals)\n\n            proposals = []\n            predicted = []\n\n            # build kd-like nearest selection using current archive\n            if X.shape[0] >= 5:\n                dists = np.sum((X - x_best[None, :])**2, axis=1)\n                nearest_idx = np.argsort(dists)\n            else:\n                nearest_idx = np.arange(X.shape[0])\n\n            for _ in range(max(2, K*2)):\n                # choose subspace dimensionality (prefer 1..min(3,dim))\n                sdim = int(rng.integers(1, min(self.max_subspace, dim) + 1))\n                dims = rng.choice(dim, size=sdim, replace=False)\n\n                # gather local samples (projected) from nearest archive points\n                nb = min(X.shape[0], max(8, 4*sdim))\n                idxs = nearest_idx[:nb]\n                Z = (X[idxs][:, dims] - x_best[dims][None, :])  # center at best in subspace\n\n                ys = F[idxs]\n\n                # design (quadratic) requires enough samples; if not enough, create a randomized proposal\n                basis_size = 1 + sdim + sdim*(sdim+1)//2\n                if Z.shape[0] < max(basis_size, 6):\n                    # fallback: sparse barycentric between best and a random near neighbor (in chosen dims)\n                    j = rng.choice(idxs)\n                    coeff = rng.random(2)\n                    coeff = coeff / coeff.sum()\n                    xj = X[j]\n                    xprop = x_best.copy()\n                    xprop[dims] = coeff[0]*x_best[dims] + coeff[1]*xj[dims]\n                    # add small gaussian in subspace scaled by trust\n                    xprop[dims] = xprop[dims] + 0.6 * trust * rng.standard_normal(sdim)\n                    proposals.append(xprop)\n                    # cheap proxy predicted using distance in subspace to best + neighbor fitness\n                    pred = 0.5*(F[j] + f_best) + 0.5*np.sqrt(np.sum((xprop[dims]-x_best[dims])**2)+1e-12)\n                    predicted.append(pred)\n                    continue\n\n                # build quadratic basis phi: [1, z1..zk, z1^2, z1*z2, ...]\n                Z_rel = Z  # shape (n_samples, sdim)\n                n_samp = Z_rel.shape[0]\n                phi_cols = 1 + sdim + sdim*(sdim+1)//2\n                Phi = np.empty((n_samp, phi_cols), dtype=float)\n                Phi[:, 0] = 1.0\n                Phi[:, 1:1+sdim] = Z_rel\n                col = 1 + sdim\n                for i in range(sdim):\n                    for j in range(i, sdim):\n                        Phi[:, col] = Z_rel[:, i] * Z_rel[:, j]\n                        col += 1\n\n                # ridge regression to fit quadratic surrogate locally (predict value relative to f_best to help scaling)\n                y = ys\n                # small ridge based on variance\n                alpha = 1e-6 * (1.0 + np.var(y))\n                A = Phi.T @ Phi\n                A[np.diag_indices_from(A)] += alpha\n                try:\n                    w = np.linalg.solve(A, Phi.T @ y)\n                except np.linalg.LinAlgError:\n                    # fallback to least-squares\n                    w, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n\n                # extract linear term b and quadratic matrix H (such that 0.5 z^T H z + b^T z + a)\n                a = float(w[0])\n                b = w[1:1+sdim].copy()\n                # build symmetric H from phi quadratic coeffs: coefficients c_ij correspond to z_i*z_j terms,\n                # relationship: model = a + b^T z + sum_{i<=j} c_{ij} z_i z_j\n                c = w[1+sdim:].copy()\n                H = np.zeros((sdim, sdim), dtype=float)\n                idxc = 0\n                for i in range(sdim):\n                    for j in range(i, sdim):\n                        cij = float(c[idxc]); idxc += 1\n                        if i == j:\n                            H[i, j] = 2.0 * cij  # because c_ii z_i^2 corresponds to 0.5 * H_ii * z_i^2 => H_ii = 2*c_ii\n                        else:\n                            H[i, j] = cij\n                            H[j, i] = cij\n\n                # regularize H to be solvable / positive definite for minimizer computation\n                # try to compute minimizer: H z = -b  => z* = -H^{-1} b\n                # compute small regularization based on diag\n                reg = 1e-8 + 1e-6 * (np.mean(np.abs(np.diag(H))) + np.std(y))\n                # ensure symmetric\n                H = 0.5*(H + H.T)\n                # eigen check to ensure SPD\n                try:\n                    eigs = np.linalg.eigvalsh(H)\n                    min_eig = eigs[0]\n                except Exception:\n                    min_eig = None\n                if min_eig is None or min_eig <= 1e-9:\n                    reg_add = (abs(min_eig) + 1e-6) if min_eig is not None else 1e-6\n                    reg = max(reg, reg_add)\n                H_reg = H + reg * np.eye(sdim)\n\n                # attempt to solve for z*\n                try:\n                    z_star = -np.linalg.solve(H_reg, b)\n                except Exception:\n                    # fallback to scaled steepest\n                    denom = 1e-12 + np.sum(b*b)\n                    z_star = - b * (min(trust, 0.5*range_norm) / (np.sqrt(denom)+1e-12))\n\n                # clip z_star to trust radius in subspace\n                z_norm = np.linalg.norm(z_star)\n                if z_norm > trust:\n                    z_star = z_star * (trust / (z_norm + 1e-12))\n\n                # build candidate in full space\n                x_prop = x_best.copy()\n                x_prop[dims] = x_best[dims] + z_star\n\n                # also add slight orthogonal jitter in full space for exploration\n                if rng.random() < 0.35:\n                    jitter = 0.18 * trust * rng.standard_normal(dim)\n                    # zero out dims' jitter to preserve subspace structure less strictly\n                    jitter[dims] = jitter[dims] * 0.3\n                    x_prop = x_prop + jitter\n\n                proposals.append(np.clip(x_prop, lb, ub))\n                # predicted value from surrogate\n                # compute phi(z_star)\n                z = z_star\n                phi_z = np.empty(phi_cols, dtype=float)\n                phi_z[0] = 1.0\n                phi_z[1:1+sdim] = z\n                col = 1 + sdim\n                for i in range(sdim):\n                    for j in range(i, sdim):\n                        phi_z[col] = z[i]*z[j]; col += 1\n                pred = float(phi_z @ w)\n                predicted.append(pred)\n\n                # occasionally also propose a simple barycentric (mix) across 2-3 nearest in subspace\n                if rng.random() < 0.25 and len(idxs) >= 2:\n                    k = min(3, len(idxs))\n                    picks = rng.choice(idxs, size=k, replace=False)\n                    coeffs = rng.random(k); coeffs /= coeffs.sum()\n                    mix = np.zeros(dim, dtype=float)\n                    for ii, pj in enumerate(picks):\n                        mix += coeffs[ii] * X[pj]\n                    # small trust-based perturbation\n                    mix = mix + 0.5 * trust * rng.standard_normal(dim)\n                    proposals.append(np.clip(mix, lb, ub))\n                    predicted.append(0.5*(np.mean(F[picks]) + pred))\n\n                if len(proposals) >= max(8, 3*K):\n                    break\n\n            # sort proposals by predicted (lower better)\n            if len(proposals) == 0:\n                break\n            order = np.argsort(predicted)\n            selected = []\n            for idx in order:\n                if len(selected) >= K or evals >= self.budget:\n                    break\n                # avoid duplicates: check distance to already proposed evaluated ones\n                xcan = proposals[int(idx)]\n                too_close = False\n                # compare to last few archive points for duplication avoidance\n                recent = X[-min(12, X.shape[0]):] if X.size else np.empty((0,dim))\n                if recent.size:\n                    d2 = np.sum((recent - xcan[None,:])**2, axis=1)\n                    if np.min(d2) < 1e-12:\n                        too_close = True\n                if not too_close:\n                    selected.append(xcan)\n\n            # evaluate selected candidates\n            for xcand in selected:\n                if evals >= self.budget: break\n                fc = eval_candidate(xcand)\n                if fc is None: break\n                if fc < f_best - 1e-12:\n                    improved = True\n\n            # adjust trust based on performance\n            if improved:\n                trust = min(range_norm*2.0, trust * 1.18)\n                stagnation = 0\n            else:\n                trust = max(1e-10, trust * 0.75)\n                stagnation += 1\n\n            # stagnation handling: occasional global injection when stuck\n            if stagnation >= 6 and evals < self.budget:\n                stagnation = 0\n                # inject a few global samples around random archive points biased toward best\n                injections = min(4, self.budget - evals)\n                for _ in range(injections):\n                    j = rng.integers(0, X.shape[0])\n                    xinj = 0.6 * x_best + 0.4 * X[j] + 0.6 * range_norm * (rng.standard_normal(dim) * 0.12)\n                    xinj = np.clip(xinj, lb, ub)\n                    fc = eval_candidate(xinj)\n                    if fc is None: break\n                    if fc < f_best:\n                        improved = True\n\n            # if near budget end, break to polishing\n            if self.budget - evals <= max(8, dim):\n                break\n\n        # final simple coordinate polishing (low-degree, deterministic refinement)\n        step = 0.14 * range_norm\n        step_min = max(1e-8, 1e-6 * range_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            order_dims = list(range(dim))\n            rng.shuffle(order_dims)\n            for d in order_dims:\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step, lb[d], ub[d])\n                fc = eval_candidate(x_try)\n                if fc is None: break\n                if fc < f_best:\n                    f_best = fc; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step, lb[d], ub[d])\n                fc = eval_candidate(x_try)\n                if fc is None: break\n                if fc < f_best:\n                    f_best = fc; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n            else:\n                step = min(0.8*range_norm, step * 1.08)\n\n        # final record\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["45bacdb1-c496-4bcb-8a7b-2ccd06451f70"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2905.0, "Edges": 2904.0, "Max Degree": 52.0, "Min Degree": 1.0, "Mean Degree": 1.9993115318416523, "Degree Variance": 2.6141131232577224, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.275318829707427, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3165050429826977, "Depth Entropy": 2.0479000366396214, "Assortativity": 0.0, "Average Eccentricity": 18.45370051635112, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00034423407917383823, "Average Shortest Path": 10.08441273950584, "mean_complexity": 17.5, "total_complexity": 70.0, "mean_token_count": 625.0, "total_token_count": 2500.0, "mean_parameter_count": 3.25, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "5641e1fd-1d15-4afd-91af-17537bcec510", "fitness": 0.008693274357254591, "name": "SparseSubspaceLineAndEdgeSearch", "description": "Sparse Subspace Line-and-Edge Search (SSLES) \u2014 combine very-low-degree sparse directional 1\u2011D line probes around the best point with occasional lightweight edge midpoints/extrapolations among a small elite; keep the algorithm simple, low-degree and budget-frugal while adapting a single trust radius.", "code": "import numpy as np\n\nclass SparseSubspaceLineAndEdgeSearch:\n    \"\"\"\n    Sparse Subspace Line-and-Edge Search (SSLES)\n\n    Main idea:\n      - Maintain a compact elite archive.\n      - Most work: very cheap sparse directional 1-D probes (few coordinates nonzero)\n        from the current best: evaluate a tiny line of +/- steps (budget 1-5 per probe).\n      - Occasionally (low probability) perform a tiny edge-phase: pick two elites,\n        form midpoint and mild extrapolations, predict by linear interpolation and\n        evaluate only the top 1-2 candidates.\n      - Adapt a single radius parameter (step length) by simple success/failure rules.\n      - Keep code and hyper-structure minimal to reduce algorithmic \"degree\".\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, rng=None,\n                 init_radius=2.0, min_radius=1e-6, max_radius=5.0,\n                 elite_size=None, edge_prob=0.12, stagnation_patience=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.edge_prob = float(edge_prob)\n        self.stagnation_patience = int(stagnation_patience)\n        if elite_size is None:\n            # keep a small elite to reduce complexity but allow scaling with dim\n            self.elite_size = max(4, min(8, int(np.sqrt(max(1, self.dim)) * 2)))\n        else:\n            self.elite_size = int(elite_size)\n\n    def __call__(self, func):\n        # fixed bounds for the benchmark\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n\n        if self.budget <= 0:\n            return None, None\n\n        evals = 0\n\n        # initial sampling: small diverse pool\n        n_init = min(self.budget, max(4, self.elite_size * 2))\n        archive = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            fx = float(func(x))\n            evals += 1\n            archive.append((fx, x.copy()))\n        if len(archive) == 0:\n            return None, None\n        archive.sort(key=lambda t: t[0])\n        archive = archive[:self.elite_size]\n        f_best, x_best = archive[0][0], archive[0][1].copy()\n\n        radius = float(self.init_radius)\n        stagnation = 0\n\n        def add_to_archive(fx, x):\n            nonlocal archive, f_best, x_best\n            archive.append((fx, x.copy()))\n            archive.sort(key=lambda t: t[0])\n            if len(archive) > self.elite_size:\n                archive = archive[:self.elite_size]\n            if fx < f_best:\n                f_best = fx\n                x_best = x.copy()\n\n        # small helper: choose sparse direction (few coords nonzero)\n        def sparse_direction(k_nonzero=2):\n            d = np.zeros(self.dim)\n            k = max(1, min(self.dim, int(k_nonzero)))\n            inds = self.rng.choice(self.dim, size=k, replace=False)\n            # use sign +/-1 with small random scaling for diversity\n            vals = self.rng.normal(loc=0.0, scale=1.0, size=k)\n            # normalize to unit on the selected coords (so step=radius scales uniformly)\n            vals = vals / (np.linalg.norm(vals) + 1e-12)\n            d[inds] = vals\n            return d\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n\n            # choose phase: edge-phase rarely, otherwise sparse line probes\n            if self.rng.random() < self.edge_prob and len(archive) >= 2 and remaining >= 1:\n                # Lightweight edge-phase: pick two elites biased to better ones\n                # compute simple selection probabilities by rank (better -> higher p)\n                ranks = np.arange(1, len(archive) + 1)  # 1 best\n                # give best more weight\n                probs = (len(archive) + 1 - ranks).astype(float)\n                probs = probs / probs.sum()\n                i = self.rng.choice(len(archive), p=probs)\n                j = self.rng.choice(len(archive), p=probs)\n                if j == i:\n                    j = (i + 1) % len(archive)\n                xi = archive[i][1]\n                xj = archive[j][1]\n                fi = float(archive[i][0])\n                fj = float(archive[j][0])\n\n                # candidate ts: midpoint and mild extrapolations\n                ts = [0.5, 0.25, 0.75, 1.05, -0.05]\n                cand_list = []\n                for t in ts:\n                    x_c = xi * (1.0 - t) + xj * t\n                    # small orth perturb occasionally\n                    if self.rng.random() < 0.2:\n                        z = self.rng.normal(size=self.dim)\n                        # bias orthogonality to the edge direction for variety but keep tiny\n                        v = xj - xi\n                        if np.linalg.norm(v) > 1e-12:\n                            z -= (z @ v) / (np.dot(v, v) + 1e-12) * v\n                        z = z / (np.linalg.norm(z) + 1e-12)\n                        x_c = x_c + 0.03 * radius * z\n                    # clip\n                    x_c = np.minimum(np.maximum(x_c, lb), ub)\n                    # predicted f by linear interpolation\n                    fpred = fi * (1.0 - t) + fj * t\n                    cand_list.append((fpred, x_c))\n                # pick top predicted (usually 1 or 2) to evaluate (keep it tiny)\n                cand_list.sort(key=lambda q: q[0])\n                num_eval = min(2, remaining)\n                eval_count = 0\n                for idx in range(min(len(cand_list), num_eval)):\n                    if evals >= self.budget:\n                        break\n                    x_try = cand_list[idx][1]\n                    f_try = float(func(x_try))\n                    evals += 1\n                    eval_count += 1\n                    add_to_archive(f_try, x_try)\n                    if f_try < f_best:\n                        radius = min(self.max_radius, radius * 1.25)\n                        stagnation = 0\n                if eval_count == 0:\n                    stagnation += 1\n                else:\n                    # if none improved, slightly contract\n                    if all(cand_list[i][1] is None for i in range(eval_count)):  # never true, safe place\n                        radius *= 0.9\n\n            else:\n                # SPARSE 1-D LINE PROBE around x_best\n                # pick number of nonzero coords small, biased to 1-3\n                k = int(np.clip(round(1 + self.rng.exponential(scale=1.0)), 1, min(3, self.dim)))\n                d = sparse_direction(k_nonzero=k)\n                # normalize direction vector so magnitude 1 on selected coords\n                d = d / (np.linalg.norm(d) + 1e-12)\n\n                # propose a tiny sequence along +/- multiples of radius (budget-frugal)\n                # order intimately: prefer small steps that are likelier to improve\n                multipliers = [0.0, 1.0, -1.0, 2.0, -2.0]  # 0 is reserved (x_best) but we'll skip evaluating 0\n                # scale base by radius and a small random factor for diversification\n                base = radius * (0.8 + 0.4 * self.rng.random())\n                improved = False\n                used = 0\n                # evaluate until we either exhaust a tiny local budget or hit an improving point\n                for m in multipliers[1:]:\n                    if evals >= self.budget:\n                        break\n                    if used >= 5:\n                        break\n                    x_try = x_best + d * (base * m)\n                    x_try = np.minimum(np.maximum(x_try, lb), ub)\n                    # skip identical to archive best if numeric rounding makes no change\n                    if np.allclose(x_try, x_best, atol=1e-12, rtol=0.0):\n                        continue\n                    f_try = float(func(x_try))\n                    evals += 1\n                    used += 1\n                    add_to_archive(f_try, x_try)\n                    if f_try < f_best:\n                        improved = True\n                        # take a short greedy local probe continuing the direction (one more eval if budget)\n                        if evals < self.budget:\n                            x_more = x_try + d * (0.6 * base * np.sign(m))\n                            x_more = np.minimum(np.maximum(x_more, lb), ub)\n                            if not np.allclose(x_more, x_try, atol=1e-12, rtol=0.0):\n                                f_more = float(func(x_more))\n                                evals += 1\n                                add_to_archive(f_more, x_more)\n                                if f_more < f_best:\n                                    x_try = x_more\n                        break\n\n                # adaptation of radius based on success\n                if improved:\n                    radius = min(self.max_radius, max(self.min_radius, radius * 1.18))\n                    stagnation = 0\n                else:\n                    stagnation += 1\n                    radius = max(self.min_radius, radius * 0.72)\n\n            # occasional opposition or random probe to avoid simple traps\n            if stagnation >= max(3, self.stagnation_patience // 3) and evals < self.budget:\n                # do a single opposite or random anchored probe\n                if self.rng.random() < 0.5:\n                    x_opp = lb + ub - x_best\n                    x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                    f_opp = float(func(x_opp))\n                    evals += 1\n                    add_to_archive(f_opp, x_opp)\n                    if f_opp < f_best:\n                        radius = min(self.max_radius, radius * 1.25)\n                        stagnation = 0\n                    else:\n                        stagnation += 1\n                else:\n                    # random small restart near a random elite\n                    anchor = archive[self.rng.integers(0, len(archive))][1]\n                    jitter = self.rng.normal(0, 0.6 * radius, size=self.dim)\n                    x_rand = np.minimum(np.maximum(anchor + jitter, lb), ub)\n                    f_rand = float(func(x_rand))\n                    evals += 1\n                    add_to_archive(f_rand, x_rand)\n                    if f_rand < f_best:\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n                    else:\n                        stagnation += 1\n\n            # keep radius within bounds\n            radius = float(np.clip(radius, self.min_radius, self.max_radius))\n\n            # safety: truncate archive size\n            if len(archive) > self.elite_size:\n                archive = archive[:self.elite_size]\n\n            # break if out of budget\n            if evals >= self.budget:\n                break\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 9, "feedback": "The algorithm SparseSubspaceLineAndEdgeSearch scored 0.009 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a66bcb4a-d4ad-4cd2-bcfd-3ff1a37e3197"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.06452701760980994}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.017085894490155074}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.006938987334647706}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.0008605672516043272}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.029009620095359012}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.01152702857724286}], "aucs": [0.06452701760980994, 0.017085894490155074, 0.006938987334647706, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.0008605672516043272, 0.029009620095359012, 0.01152702857724286]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1845.0, "Edges": 1844.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9989159891598915, "Degree Variance": 2.1907847327795773, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.41440377804014, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3272213354540336, "Depth Entropy": 2.14952480403376, "Assortativity": 0.0, "Average Eccentricity": 18.920325203252034, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0005420054200542005, "Average Shortest Path": 10.182261961448248, "mean_complexity": 10.25, "total_complexity": 41.0, "mean_token_count": 418.75, "total_token_count": 1675.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "2a3b4a95-14a2-4906-a7fc-cf5f4ed2d5c6", "fitness": 0.5308244845925668, "name": "RotatedCoordinateExtrapolation", "description": "Rotated Coordinate Descent with Directional Extrapolation (RCD-DE) \u2014 combine cheap 1-D coordinate probes, small 2-D rotated-plane scans built from recent successful directions, and lightweight elite recombination; amplify steps on success and shrink on failure to get rapid, low-degree exploitation while keeping simple diversification.", "code": "import numpy as np\n\nclass RotatedCoordinateExtrapolation:\n    \"\"\"\n    Rotated Coordinate Descent with Directional Extrapolation (RCD-DE)\n\n    Key ideas:\n      - Use cheap 1-D coordinate probes (\u00b1 step) to get directional signal and perform\n        controlled extrapolation along improving coordinates.\n      - Use 2-D rotated-plane scans combining a coordinate axis and a recent successful\n        direction to discover curved descent directions with only a few evaluations.\n      - Maintain a tiny elite archive and a short memory of successful direction vectors\n        used to bias rotated scans and simple elite recombination for diversification.\n      - Adaptive step scaling: grow on success, shrink on failure; occasional mirror/elite\n        recombination when stagnating.\n      - All operations respect the evaluation budget strictly.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, init_step=2.5, min_step=1e-6,\n                 max_step=2.5, elite_size=3, dir_memory=6, stagn_patience=8, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations (int)\n          dim: problem dimensionality (int)\n          init_step: initial step magnitude (float)\n          min_step, max_step: clamping for step scale\n          elite_size: number of elite points to keep (small)\n          dir_memory: how many past successful directions to remember\n          stagn_patience: iterations without improvement before stronger diversification\n          rng: numpy.random.Generator (optional)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.elite_size = int(elite_size)\n        self.dir_memory = int(dir_memory)\n        self.stagn_patience = int(stagn_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shapes match dimension\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        # initial sampling: small pool to seed elites\n        n_init = min(4, max(1, self.budget // 50 + 1))  # a few initial points but bounded\n        xs = []\n        fs = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            xs.append(x)\n            fs.append(f)\n\n        if len(fs) == 0:\n            # budget is zero? return a default point\n            x0 = lb + 0.5 * (ub - lb)\n            return float(func(x0)), x0\n\n        # pick best among initial\n        idx_best = int(np.argmin(fs))\n        x_best = xs[idx_best].copy()\n        f_best = float(fs[idx_best])\n\n        # elite archive: keep tuples (f, x)\n        elite = sorted(zip(fs, xs), key=lambda t: t[0])[: self.elite_size]\n        # canonicalize elites as np arrays\n        elite = [(float(t[0]), t[1].copy()) for t in elite]\n\n        # memory of recent successful directions (unit vectors)\n        dir_mem = []\n\n        step = float(self.init_step)\n        stagn = 0\n\n        # helper functions\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def push_dir(v):\n            # store normalized direction if significant\n            norm = np.linalg.norm(v)\n            if norm > 1e-12:\n                u = v / norm\n                dir_mem.insert(0, u)\n                if len(dir_mem) > self.dir_memory:\n                    dir_mem.pop()\n\n        def update_elite(xcand, fcand):\n            nonlocal elite\n            # add if better than worst in archive or if space available\n            xs_list = [np.asarray(e[1]) for e in elite]\n            # check uniqueness (farther than small tol)\n            unique_enough = True\n            tol = 1e-6 + 1e-6 * np.linalg.norm(ub - lb)\n            for _, xe in elite:\n                if np.linalg.norm(xcand - xe) < tol:\n                    unique_enough = False\n                    break\n            if unique_enough:\n                elite.append((float(fcand), xcand.copy()))\n                elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n\n        # main optimization loop\n        # We will alternate between coordinate probes, rotated 2D scans, and light recombination\n        while evals < self.budget:\n            remaining = self.budget - evals\n            improved = False\n\n            r = self.rng.random()\n            # Most iterations use coordinate probe; many use rotated scans; occasional recombination/random\n            if r < 0.55:\n                # coordinate probe + possible extrapolation\n                # pick coordinate (prefer coordinates that had larger recent variance in elites)\n                if len(elite) >= 2:\n                    # compute per-dim spread among elites\n                    coords = np.vstack([e[1] for e in elite])\n                    spreads = np.std(coords, axis=0)\n                    # avoid all zeros\n                    if spreads.sum() > 0:\n                        probs = spreads + 1e-12\n                        probs = probs / probs.sum()\n                        idx = int(self.rng.choice(self.dim, p=probs))\n                    else:\n                        idx = int(self.rng.integers(self.dim))\n                else:\n                    idx = int(self.rng.integers(self.dim))\n\n                delta = np.zeros(self.dim, dtype=float)\n                delta[idx] = step\n\n                # decide to evaluate both sides if budget allows\n                to_eval = []\n                if remaining >= 2:\n                    x_p = clip(x_best + delta)\n                    x_m = clip(x_best - delta)\n                    to_eval = [x_p, x_m]\n                else:\n                    # only one evaluation left\n                    x_p = clip(x_best + delta)\n                    to_eval = [x_p]\n\n                best_local_x = x_best\n                best_local_f = f_best\n                for xc in to_eval:\n                    if evals >= self.budget:\n                        break\n                    fc = float(func(xc))\n                    evals += 1\n                    if fc < best_local_f:\n                        best_local_f = fc\n                        best_local_x = xc.copy()\n\n                if best_local_f < f_best:\n                    # improve and attempt controlled extrapolation along same sign\n                    v = best_local_x - x_best\n                    push_dir(v)\n                    # extrapolate gradually while improving and budget allows\n                    growth = 1.6\n                    cur_x = best_local_x.copy()\n                    cur_f = best_local_f\n                    while evals < self.budget:\n                        next_x = clip(cur_x + growth * (cur_x - x_best))\n                        # if no real movement, break\n                        if np.linalg.norm(next_x - cur_x) < 1e-12:\n                            break\n                        fnext = float(func(next_x))\n                        evals += 1\n                        if fnext < cur_f:\n                            x_best_old = x_best\n                            x_best = next_x.copy()\n                            f_best = fnext\n                            cur_x = next_x\n                            cur_f = fnext\n                            improved = True\n                            # store larger direction\n                            push_dir(x_best - x_best_old)\n                            # allow continue extrapolation\n                            continue\n                        else:\n                            break\n                    # if we didn't extrapolate, still accept the first improvement\n                    if not improved:\n                        x_best = best_local_x.copy()\n                        f_best = best_local_f\n                        improved = True\n                # else: no improvement\n            elif r < 0.9:\n                # 2-D rotated-plane scan combining a coordinate axis and a remembered direction (or a random direction)\n                # choose coordinate axis\n                idx = int(self.rng.integers(self.dim))\n                u = np.zeros(self.dim, dtype=float)\n                u[idx] = 1.0\n                if dir_mem and self.rng.random() < 0.75:\n                    d = dir_mem[self.rng.integers(len(dir_mem))]\n                else:\n                    d = self.rng.normal(size=self.dim)\n                    nrm = np.linalg.norm(d)\n                    if nrm < 1e-12:\n                        d = np.ones(self.dim) / np.sqrt(self.dim)\n                    else:\n                        d = d / nrm\n                # make v orthogonal to u\n                proj = u.dot(d) * u\n                v = d - proj\n                vn = np.linalg.norm(v)\n                if vn < 1e-12:\n                    # fall back to a random orthogonal vector\n                    v = self.rng.normal(size=self.dim)\n                    v[idx] = 0.0\n                    vn = np.linalg.norm(v)\n                    if vn < 1e-12:\n                        v = np.zeros(self.dim); v[(idx+1) % self.dim] = 1.0\n                        vn = 1.0\n                v = v / vn\n\n                # sample a few angles on the small circle in the plane\n                angles = [0.0, np.pi/4.0, np.pi/2.0, 3*np.pi/4.0, np.pi]\n                cand_xs = []\n                for th in angles:\n                    direction = np.cos(th) * u + np.sin(th) * v\n                    cand = clip(x_best + step * direction)\n                    cand_xs.append(cand)\n\n                # evaluate candidates (but do not exceed budget)\n                best_local_x = x_best\n                best_local_f = f_best\n                for xc in cand_xs:\n                    if evals >= self.budget:\n                        break\n                    fc = float(func(xc))\n                    evals += 1\n                    if fc < best_local_f:\n                        best_local_f = fc\n                        best_local_x = xc.copy()\n\n                if best_local_f < f_best:\n                    # accept and try small radial extrapolation along chosen direction\n                    vstep = best_local_x - x_best\n                    push_dir(vstep)\n                    # try one or two radial enlargements if budget allows\n                    radial_mult = 1.4\n                    cur_x = best_local_x.copy()\n                    cur_f = best_local_f\n                    for _ in range(2):\n                        if evals >= self.budget:\n                            break\n                        next_x = clip(x_best + radial_mult * (cur_x - x_best))\n                        if np.linalg.norm(next_x - cur_x) < 1e-12:\n                            break\n                        fnext = float(func(next_x))\n                        evals += 1\n                        if fnext < cur_f:\n                            x_best_old = x_best\n                            x_best = next_x.copy()\n                            f_best = fnext\n                            cur_x = next_x\n                            cur_f = fnext\n                            improved = True\n                            push_dir(x_best - x_best_old)\n                        else:\n                            break\n                    if not improved:\n                        x_best = best_local_x.copy()\n                        f_best = best_local_f\n                        improved = True\n                # else no improvement\n            else:\n                # recombination / random diversification\n                if elite and len(elite) >= 2 and self.rng.random() < 0.8:\n                    # combine two elites and add a small perturbation\n                    ids = self.rng.choice(len(elite), size=2, replace=False)\n                    x_a = elite[ids[0]][1]\n                    x_b = elite[ids[1]][1]\n                    weight = self.rng.random()\n                    x_new = weight * x_a + (1.0 - weight) * x_b\n                    # small Gaussian jitter proportional to step\n                    jitter = self.rng.normal(scale=0.08 * max(step, 1e-12), size=self.dim)\n                    x_new = clip(x_new + jitter)\n                    if evals < self.budget:\n                        fnew = float(func(x_new))\n                        evals += 1\n                        if fnew < f_best:\n                            push_dir(x_new - x_best)\n                            x_best = x_new.copy()\n                            f_best = fnew\n                            improved = True\n                        update_elite(x_new, fnew)\n                else:\n                    # pure random sample\n                    x_new = clip(lb + self.rng.random(self.dim) * (ub - lb))\n                    if evals < self.budget:\n                        fnew = float(func(x_new))\n                        evals += 1\n                        if fnew < f_best:\n                            push_dir(x_new - x_best)\n                            x_best = x_new.copy()\n                            f_best = fnew\n                            improved = True\n                        update_elite(x_new, fnew)\n\n            # update elites if x_best changed or occasionally\n            update_elite(x_best, f_best)\n\n            # adapt step size\n            if improved:\n                step = min(self.max_step, step * 1.18)\n                stagn = 0\n            else:\n                step = max(self.min_step, step * 0.65)\n                stagn += 1\n\n            # occasional mirror check to escape symmetric traps\n            if evals < self.budget and stagn >= 3 and self.rng.random() < 0.35:\n                x_mirror = clip(lb + ub - x_best)\n                f_mirror = float(func(x_mirror))\n                evals += 1\n                if f_mirror < f_best:\n                    push_dir(x_mirror - x_best)\n                    x_best = x_mirror.copy()\n                    f_best = f_mirror\n                    step = min(self.max_step, self.init_step)\n                    stagn = 0\n                    update_elite(x_best, f_best)\n\n            # stronger diversification when stagnation persists\n            if stagn >= self.stagn_patience:\n                # try convex combinations of elites and a near-random jump\n                if len(elite) >= 2:\n                    # convex mix of best elites\n                    weights = self.rng.random(len(elite))\n                    weights = weights / weights.sum()\n                    mix = np.zeros(self.dim)\n                    for w, e in zip(weights, elite):\n                        mix += w * e[1]\n                    x_mix = clip(mix + self.rng.normal(scale=0.12 * max(step, 1e-12), size=self.dim))\n                    if evals < self.budget:\n                        fmix = float(func(x_mix))\n                        evals += 1\n                        if fmix < f_best:\n                            push_dir(x_mix - x_best)\n                            x_best = x_mix.copy()\n                            f_best = fmix\n                            step = min(self.max_step, self.init_step)\n                            stagn = 0\n                            update_elite(x_best, f_best)\n                # near-random jump centered at best\n                if stagn >= self.stagn_patience and evals < self.budget:\n                    jump_scale = 0.6 * (ub - lb)\n                    x_jump = clip(x_best + self.rng.normal(scale=1.0, size=self.dim) * jump_scale)\n                    fjump = float(func(x_jump))\n                    evals += 1\n                    if fjump < f_best:\n                        push_dir(x_jump - x_best)\n                        x_best = x_jump.copy()\n                        f_best = fjump\n                        step = min(self.max_step, self.init_step)\n                        stagn = 0\n                        update_elite(x_best, f_best)\n                    else:\n                        # reduce step but reset stagnation so we resume with changed scale\n                        step = max(self.min_step, 0.5 * self.init_step)\n                        stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 9, "feedback": "The algorithm RotatedCoordinateExtrapolation scored 0.531 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9625438707627173}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9521941770202678}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7730246054336114}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9259089281379419}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.5392962384882233}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.28753696025705155}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08316447638250013}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10819813511416454}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.10323636734338193}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10157735627409237}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.09944127353025267}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.11481790043395868}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.977499509340104}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9653190399729754}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9686084303972597}], "aucs": [0.9625438707627173, 0.9521941770202678, 0.7730246054336114, 0.9259089281379419, 0.5392962384882233, 0.28753696025705155, 0.08316447638250013, 0.10819813511416454, 0.10323636734338193, 0.10157735627409237, 0.09944127353025267, 0.11481790043395868, 0.977499509340104, 0.9653190399729754, 0.9686084303972597]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2748.0, "Edges": 2747.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.9992721979621544, "Degree Variance": 1.834060605675373, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.894302229562346, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3332127380766432, "Depth Entropy": 2.180945717289678, "Assortativity": 0.0, "Average Eccentricity": 17.483260553129547, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.000363901018922853, "Average Shortest Path": 10.880816123875245, "mean_complexity": 12.166666666666666, "total_complexity": 73.0, "mean_token_count": 390.0, "total_token_count": 2340.0, "mean_parameter_count": 3.0, "total_parameter_count": 18.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "2c9b5cc2-2d12-48ca-bed2-c10833794d25", "fitness": 0.30206801273415174, "name": "DegreeLimitedAdaptiveSpiralLevyEdgeGraph", "description": "Degree-Limited Adaptive Spiral\u2013L\u00e9vy Edge Graph (DL-ASSL) \u2014 aggressively limit per-node degree and candidate evaluations by building a sparse, elite-biased k-NN edge graph, cheaply scoring many virtual edge-derived proposals with a linear interpolation predictor, and only evaluating the top-ranked few; combine those low-degree edge recombinations with lightweight spiral moves and rare L\u00e9vy escapes for robust exploration/exploitation under tight budgets.", "code": "import numpy as np\n\nclass DegreeLimitedAdaptiveSpiralLevyEdgeGraph:\n    \"\"\"\n    Degree-Limited Adaptive Spiral-Levy Edge Graph (DL-ASSL)\n\n    - budget: maximum number of function evaluations (int)\n    - dim: problem dimensionality (int)\n\n    Optional parameters:\n    - pop_size: number of individuals (default small, scales with dim)\n    - elite_frac: fraction of population considered elite for edge building\n    - k_edges: max neighbors per elite (degree-limited, small)\n    - proposals_per_edge: proposals generated per edge (kept small)\n    - levy_prob: probability of a L\u00e9vy-like jump for moves\n    - seed: RNG seed\n    \"\"\"\n\n    def __init__(self, budget, dim, pop_size=None, elite_frac=0.25,\n                 k_edges=2, proposals_per_edge=1, levy_prob=0.06, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_edges = max(1, int(k_edges))\n        self.proposals_per_edge = max(1, int(proposals_per_edge))\n        self.levy_prob = float(levy_prob)\n\n        # compact population: prefer small populations to reduce graph degree explosion\n        if pop_size is None:\n            # baseline: scale with dimension but keep compact\n            self.pop_size = max(10, min(40, int(3 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptive multipliers\n        self.sigma_init_frac = 0.10  # fraction of full range\n        self.alpha_init_frac = 0.35\n\n    def _get_bounds(self, func):\n        # The problem statement: default bounds [-5,5] unless func provides bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        sigma_scale = max(1e-12, self.sigma_init_frac * range_norm)\n        alpha_scale = max(1e-12, self.alpha_init_frac * range_norm)\n\n        evals = 0\n        budget = self.budget\n\n        # initialize population\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= budget: break\n            pop_f[i] = float(func(pop[i]))\n            evals += 1\n\n        # per-individual parameters\n        sigma = np.full(self.pop_size, sigma_scale)\n        alpha = np.full(self.pop_size, alpha_scale)\n\n        success_inc = 1.18\n        failure_dec = 0.82\n\n        # best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        no_improve = 0\n        max_no_improve = max(12, self.pop_size * 2)\n\n        # helper: build a sparse, degree-limited, elite-biased edge set\n        def build_sparse_edges():\n            X = pop\n            n = self.pop_size\n            # pairwise distances efficiently\n            # compute squared distances\n            dif = X[:, None, :] - X[None, :, :]\n            dist2 = np.sum(dif * dif, axis=2)\n            elite_count = max(2, int(np.ceil(self.elite_frac * n)))\n            elites = np.argsort(pop_f)[:elite_count]\n            edges_set = set()\n            # For each elite, connect to up to k_edges nearest neighbors (exclude itself)\n            for i in elites:\n                neigh = np.argsort(dist2[i])\n                taken = 0\n                for j in neigh:\n                    if j == i: continue\n                    a, b = (int(i), int(j))\n                    if a > b:\n                        a, b = b, a\n                    if a == b:\n                        continue\n                    if (a, b) in edges_set:\n                        continue\n                    edges_set.add((a, b))\n                    taken += 1\n                    if taken >= self.k_edges:\n                        break\n            # add a few random low-degree edges to keep topology variety, but limited\n            extra = max(0, n // 6)\n            tries = 0\n            while len(edges_set) < (elite_count * self.k_edges + extra) and tries < n * 4:\n                a = int(rng.integers(0, n))\n                b = int(rng.integers(0, n))\n                if a == b:\n                    tries += 1\n                    continue\n                x, y = (a, b) if a < b else (b, a)\n                edges_set.add((x, y))\n                tries += 1\n            # convert to list\n            edges = list(edges_set)\n            # enforce global cap to keep max degree low (limit total edges)\n            max_edges = max(1, min(len(edges), n * 2))\n            if len(edges) > max_edges:\n                edges = list(rng.choice(edges, size=max_edges, replace=False))\n            return edges\n\n        # main loop: focus on evaluating a small number of high-quality edge proposals each iteration\n        while evals < budget:\n            rem = budget - evals\n            improved = False\n\n            # Build sparse edges (degree-limited)\n            edges = build_sparse_edges()\n\n            # generate many virtual candidates but cheaply score them with linear interpolation predictor\n            candidates = []  # tuples (pred_score, x_prop, i, j, w)\n            for (a, b) in edges:\n                if rem <= 0:\n                    break\n                xa, xb = pop[a], pop[b]\n                fa, fb = pop_f[a], pop_f[b]\n                edge_vec = xb - xa\n                edge_len = np.linalg.norm(edge_vec) + 1e-12\n\n                for _ in range(self.proposals_per_edge):\n                    # choose style: keep simple, small extrapolation allowed\n                    r = rng.random()\n                    if r < 0.45:\n                        w = 0.5  # midpoint\n                    elif r < 0.85:\n                        # bias toward better endpoint\n                        if fa < fb:\n                            w = rng.uniform(0.1, 0.55)\n                        else:\n                            w = rng.uniform(0.45, 0.9)\n                    else:\n                        # small extrapolation\n                        if fa < fb:\n                            w = rng.uniform(-0.3, 0.2)\n                        else:\n                            w = rng.uniform(0.8, 1.3)\n\n                    x_prop = (1.0 - w) * xa + w * xb\n                    # small noise proportional to average sigma and edge length\n                    local_scale = 0.5 * (sigma[a] + sigma[b])\n                    noise = local_scale * rng.standard_normal(dim)\n                    x_prop = x_prop + noise\n\n                    # occasionally a small rotational perturbation (dimension-limited)\n                    if dim > 1 and rng.random() < 0.18:\n                        idxs = rng.choice(dim, size=min(2, dim), replace=False)\n                        theta = rng.uniform(-np.pi/6, np.pi/6)\n                        ca, sb = np.cos(theta), np.sin(theta)\n                        u, v = idxs[0], idxs[-1]\n                        da, db = edge_vec[u], edge_vec[v]\n                        ra = ca * da - sb * db\n                        rb = sb * da + ca * db\n                        rot = np.zeros(dim)\n                        rot[u], rot[v] = (ra - da), (rb - db)\n                        x_prop += 0.06 * rot\n\n                    # compute cheap predicted f (linear interpolation + small penalty for extrapolation)\n                    pred = (1.0 - w) * fa + w * fb\n                    extrap_penalty = 0.0\n                    if w < 0.0 or w > 1.0:\n                        # penalize extrapolation modestly proportional to endpoints gap\n                        extrap_penalty = 0.08 * abs(w - 0.5) * abs(fa - fb + 1e-12)\n                    diversity_bonus = 0.001 * rng.random()  # small jitter to diversify equal scores\n                    pred_score = pred + extrap_penalty + diversity_bonus\n\n                    # clip to bounds\n                    x_prop = np.clip(x_prop, lb, ub)\n\n                    candidates.append((pred_score, x_prop, a, b, w))\n\n            if len(candidates) == 0:\n                # nothing to evaluate via edges (unlikely), fallback to a random sample\n                if evals < budget:\n                    r = rng.uniform(lb, ub)\n                    fr = float(func(r))\n                    evals += 1\n                    if fr < f_best:\n                        f_best = fr; x_best = r.copy()\n                    # replace worst if better\n                    idx_w = int(np.argmax(pop_f))\n                    if fr < pop_f[idx_w]:\n                        pop[idx_w] = r; pop_f[idx_w] = fr; sigma[idx_w] = sigma_scale\n                continue\n\n            # sort candidates and evaluate only top-K (to reduce max per-node evaluation degree)\n            candidates.sort(key=lambda t: t[0])\n            # choose number to evaluate: fraction of remaining budget but limited\n            max_eval_candidates = max(1, int(min(len(candidates), max(1, int((budget - evals) * 0.40)))))\n            # but also cap to a modest absolute number to reduce degree spikes\n            max_eval_candidates = min(max_eval_candidates, 12 + dim)\n            to_eval = candidates[:max_eval_candidates]\n\n            # evaluate selected candidates\n            for (pred_score, x_prop, a, b, w) in to_eval:\n                if evals >= budget:\n                    break\n                f_prop = float(func(x_prop))\n                evals += 1\n                # if improvement against either endpoint, replace worse one\n                fa, fb = pop_f[a], pop_f[b]\n                if f_prop < fa or f_prop < fb:\n                    # replace the worse endpoint\n                    if fa >= fb:\n                        idx_rep = a\n                    else:\n                        idx_rep = b\n                    pop[idx_rep] = x_prop\n                    pop_f[idx_rep] = f_prop\n                    # mild success adaptation\n                    sigma[idx_rep] = min(sigma[idx_rep] * success_inc, 2.0 * range_norm)\n                    alpha[idx_rep] = min(alpha[idx_rep] * 1.03, 1.5 * range_norm)\n                    improved = True\n                    if f_prop < f_best:\n                        f_best = f_prop; x_best = x_prop.copy()\n                else:\n                    # failure shrink local sigmas of endpoints slightly\n                    sigma[a] *= failure_dec\n                    sigma[b] *= failure_dec\n                    # occasional small replacement of global worst if candidate is competitive\n                    if rng.random() < 0.012:\n                        idx_w = int(np.argmax(pop_f))\n                        if f_prop < pop_f[idx_w]:\n                            pop[idx_w] = x_prop\n                            pop_f[idx_w] = f_prop\n                            sigma[idx_w] = sigma_scale\n                            if f_prop < f_best:\n                                f_best = f_prop; x_best = x_prop.copy()\n\n            # Spiral directional moves: but only evaluate a subset or with early exit to control evaluations\n            # Choose a random subset fraction to keep per-iteration evals small\n            indiv_idxs = rng.permutation(self.pop_size)\n            max_spiral_evals = max(1, int(min(self.pop_size, (budget - evals) // max(1, dim))))\n            assessed = 0\n            for idx in indiv_idxs:\n                if evals >= budget or assessed >= max_spiral_evals:\n                    break\n                xi = pop[idx].copy()\n                fi = pop_f[idx]\n                # direction to best\n                d = x_best - xi\n                nd = np.linalg.norm(d) + 1e-12\n                dir_vec = d / nd\n                # occasional small random rotation for spiral\n                if dim > 1 and rng.random() < 0.5:\n                    a, b = rng.choice(dim, size=2, replace=False)\n                    theta = rng.uniform(-np.pi/4, np.pi/4)\n                    ca, sb = np.cos(theta), np.sin(theta)\n                    da, db = d[a], d[b]\n                    ra = ca * da - sb * db\n                    rb = sb * da + ca * db\n                    d_rot = d.copy()\n                    d_rot[a], d_rot[b] = ra, rb\n                    dir_vec = d_rot / (np.linalg.norm(d_rot) + 1e-12)\n\n                mov_scale = alpha[idx] * (0.5 + 0.6 * rng.random())\n                move = mov_scale * dir_vec\n                move += sigma[idx] * rng.standard_normal(dim)\n\n                # rare L\u00e9vy\n                if rng.random() < self.levy_prob:\n                    u = rng.random(dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    move += 0.12 * range_norm * cauchy\n\n                x_new = np.clip(xi + move, lb, ub)\n\n                # small chance to try opposition (cheap acceptance)\n                if rng.random() < 0.05 and evals < budget:\n                    x_opp = lb + ub - xi\n                    x_opp = np.clip(x_opp, lb, ub)\n                    f_opp = float(func(x_opp))\n                    evals += 1\n                    assessed += 1\n                    if f_opp < fi:\n                        pop[idx] = x_opp; pop_f[idx] = f_opp\n                        sigma[idx] *= success_inc\n                        alpha[idx] *= 1.02\n                        improved = True\n                        if f_opp < f_best:\n                            f_best = f_opp; x_best = x_opp.copy()\n                        continue\n\n                # evaluate the spiral proposal\n                if evals >= budget:\n                    break\n                f_new = float(func(x_new))\n                evals += 1\n                assessed += 1\n\n                # greedy replacement with small uphill acceptance to preserve diversity\n                if f_new < fi or rng.random() < 0.01:\n                    pop[idx] = x_new\n                    pop_f[idx] = f_new\n                    sigma[idx] *= success_inc\n                    alpha[idx] *= 1.02\n                    improved = True\n                    if f_new < f_best:\n                        f_best = f_new; x_best = x_new.copy()\n                else:\n                    sigma[idx] *= failure_dec\n                    alpha[idx] *= 0.987\n\n            # maintenance: clamp sigma/alpha\n            sigma = np.clip(sigma, 1e-10 * range_norm, 2.0 * range_norm)\n            alpha = np.clip(alpha, 1e-10 * range_norm, 1.5 * range_norm)\n\n            # stagnation detection & light reseeding (but less disruptive)\n            if improved:\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            if no_improve >= max_no_improve and evals < budget:\n                no_improve = 0\n                n_reset = max(1, self.pop_size // 3)\n                for _ in range(n_reset):\n                    if evals >= budget: break\n                    idx = int(rng.integers(0, self.pop_size))\n                    if rng.random() < 0.7:\n                        # reseed near-best with small Gaussian\n                        newp = x_best + 0.05 * range_vec * rng.standard_normal(dim)\n                    else:\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    pop[idx] = newp\n                    pop_f[idx] = f_new\n                    sigma[idx] = sigma_scale\n                    alpha[idx] = alpha_scale\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # occasional purely random injection (very low rate)\n            if rng.random() < 0.03 and evals < budget:\n                r = rng.uniform(lb, ub)\n                fr = float(func(r))\n                evals += 1\n                idx_w = int(np.argmax(pop_f))\n                if fr < pop_f[idx_w]:\n                    pop[idx_w] = r; pop_f[idx_w] = fr\n                    sigma[idx_w] = sigma_scale\n                    if fr < f_best:\n                        f_best = fr; x_best = r.copy()\n\n            # if remaining budget is small, break and go to final local refinement\n            if budget - evals < max(8, dim * 2):\n                break\n\n        # Final localized refinement: coordinate pattern search (mesh)\n        mesh = 0.12 * range_norm\n        mesh_min = 1e-6 * range_norm\n        while evals < budget and mesh > mesh_min:\n            improved_local = False\n            for d in range(dim):\n                if evals >= budget:\n                    break\n                # positive direction\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n                    continue\n                if evals >= budget:\n                    break\n                # negative direction\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved_local = True\n            if not improved_local:\n                mesh *= 0.5\n\n        # store results\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm DegreeLimitedAdaptiveSpiralLevyEdgeGraph scored 0.302 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5713cbd-1130-4e8d-abdc-3c3ca8966b71"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.4832785323460337}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.45278338686495456}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.4276556161118946}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 5.156977439835231e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.025979857633741443}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06323471933238156}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.07230456137473806}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.05639705810346474}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.024049695328233978}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.016154136225303595}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9752567599757017}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9675250054892901}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9662492924521398}], "aucs": [0.4832785323460337, 0.45278338686495456, 0.4276556161118946, 4.999999999999449e-05, 5.156977439835231e-05, 4.999999999999449e-05, 0.025979857633741443, 0.06323471933238156, 0.07230456137473806, 0.05639705810346474, 0.024049695328233978, 0.016154136225303595, 0.9752567599757017, 0.9675250054892901, 0.9662492924521398]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3381.0, "Edges": 3380.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.9994084590357881, "Degree Variance": 2.147293350168019, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.26179702650291, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3224348531721284, "Depth Entropy": 1.9836044687601666, "Assortativity": 0.0, "Average Eccentricity": 16.75184856551316, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0002957704821058858, "Average Shortest Path": 10.382640547857939, "mean_complexity": 20.75, "total_complexity": 83.0, "mean_token_count": 697.75, "total_token_count": 2791.0, "mean_parameter_count": 3.25, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "52858a4a-25dd-4a00-a79d-aaffdc317198", "fitness": 0.4427348454836928, "name": "SeparableAdaptiveBlockParabolaSearch", "description": "Separable Adaptive Block Parabolic Search (SABPS) \u2014 use many cheap separable (per-dimension) parabolic interpolations in small adaptive coordinate blocks plus occasional directional parabolas; adapt per-dimension step sizes by success/failure to keep the model low-degree and robust.", "code": "import numpy as np\n\nclass SeparableAdaptiveBlockParabolaSearch:\n    \"\"\"\n    Separable Adaptive Block Parabola Search (SABPS)\n\n    Key ideas:\n      - Keep an archive of evaluated points.\n      - Maintain a per-dimension trust radius (step size).\n      - Repeatedly pick a promising center (elite-biased) and a small block\n        of coordinates. For each coordinate in the block, probe plus/minus\n        along the axis (two cheap evaluations) and fit a 1-D parabola using\n        (t=0, t=+h, t=-h). Compute the scalar optimal shift for each coordinate\n        independently (separable quadratic assumption, i.e. diagonal Hessian).\n      - Combine the per-coordinate shifts into a single candidate and evaluate.\n      - Update per-dimension radii by success/failure scaling. Occasionally\n        do a random directional parabola probe to capture a small coupled move.\n      - Low polynomial degree (only 1-D parabolas / separable quadratics)\n        keeps the model simple and robust.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_samples=None,\n                 init_radius=1.0, min_radius=1e-6, max_radius=5.0,\n                 max_block=3, elite_k=6, stagnation_restart=8, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total function evaluations allowed\n          dim: problem dimensionality\n          init_samples: initial random samples (default min(40, max(8, budget//8)))\n          init_radius: initial per-dimension trust radius\n          min_radius, max_radius: bounds on per-dimension radii\n          max_block: maximum number of coordinates to adjust jointly (small)\n          elite_k: how many top points to consider as centers\n          stagnation_restart: number of unsuccessful iterations before aggressive diversification\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if init_samples is None:\n            self.init_samples = int(min(40, max(8, self.budget // 8)))\n        else:\n            self.init_samples = int(init_samples)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_block = int(max(1, min(max_block, dim)))\n        self.elite_k = int(elite_k)\n        self.stagnation_restart = int(stagnation_restart)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n        if ub.shape[0] != self.dim:\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []\n        F = []\n        # per-dimension radii\n        radii_dim = np.full(self.dim, self.init_radius, dtype=float)\n\n        # helper to evaluate safely\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise RuntimeError(\"Budget exhausted\")\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # initial seeding (diverse)\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = safe_eval(x)\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n\n        # ensure at least one sample\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = safe_eval(x)\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n\n        best_idx = int(np.argmin(F))\n        best_x = X[best_idx].copy()\n        best_f = F[best_idx]\n\n        stagnation = 0\n        iter_count = 0\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            n_archive = len(X)\n            # pick a center among elites with bias to better ones\n            top_k = min(self.elite_k, n_archive)\n            idx_sorted = np.argsort(F)\n            top_indices = idx_sorted[:top_k]\n            top_fs = np.array([F[i] for i in top_indices], dtype=float)\n            # compute softmax-like weights on negative fitness (better -> larger)\n            if top_fs.std() < 1e-12:\n                probs = np.ones(len(top_fs)) / len(top_fs)\n            else:\n                scores = -(top_fs - top_fs.min()) / (top_fs.std() + 1e-12)\n                exp_s = np.exp(scores - np.max(scores))\n                probs = exp_s / exp_s.sum()\n            center_choice = int(self.rng.choice(top_indices, p=probs))\n            center_x = X[center_choice].copy()\n            center_f = F[center_choice]\n\n            # choose block size adaptively: often 1, sometimes up to max_block\n            # bias toward small blocks (to keep low-degree separable updates)\n            if self.rng.random() < 0.7:\n                block_size = 1\n            else:\n                block_size = int(self.rng.integers(1, self.max_block + 1))\n            # pick block coordinates: prefer dims with larger radii or larger variance\n            # compute per-dim archive variance as importance signal\n            X_arr = np.vstack(X)\n            per_dim_var = np.var(X_arr, axis=0) + 1e-12\n            # compute score combining radii and variance\n            dim_scores = radii_dim * (per_dim_var + 1e-6)\n            # choose dims without replacement, weighting by dim_scores\n            dim_probs = dim_scores / dim_scores.sum()\n            block_dims = list(self.rng.choice(self.dim, size=block_size, replace=False, p=dim_probs))\n\n            # Estimate cost: for each dim we prefer to take two probes (+h and -h).\n            # If budget is low, reduce block size accordingly.\n            # Reserve 1 eval for resulting candidate.\n            remaining = self.budget - evals\n            max_possible_dims = max(1, (remaining - 1) // 2)  # each dim costs 2, plus 1 candidate\n            if max_possible_dims < len(block_dims):\n                block_dims = block_dims[:max_possible_dims]\n                if len(block_dims) == 0:\n                    # fallback: do a single small random probe around best_x/best\n                    if evals >= self.budget:\n                        break\n                    step = (self.rng.random(self.dim) * 2 - 1) * (0.5 * self.init_radius)\n                    x_probe = np.clip(best_x + step, lb, ub)\n                    f_probe = safe_eval(x_probe)\n                    X.append(x_probe.copy()); F.append(f_probe)\n                    if f_probe < best_f:\n                        best_f = f_probe; best_x = x_probe.copy(); stagnation = 0\n                    else:\n                        stagnation += 1\n                    continue\n\n            # For each selected dimension, probe +h and -h and fit parabola\n            s_shifts = np.zeros(self.dim, dtype=float)\n            used_dims = []\n            for d in block_dims:\n                if evals + 2 > self.budget:\n                    break  # not enough budget for symmetric probe\n                h = radii_dim[d] * (0.5 + 0.5 * self.rng.random())  # between 0.5..1.0 of radius\n                if h < 1e-12:\n                    # effectively no step possible\n                    continue\n                x_plus = center_x.copy()\n                x_plus[d] = np.minimum(ub[d], center_x[d] + h)\n                # If plus is identical due to bounds, reduce h\n                if x_plus[d] == center_x[d]:\n                    x_plus[d] = center_x[d] + np.sign(h) * 1e-6\n                f_plus = safe_eval(x_plus)\n                X.append(x_plus.copy()); F.append(f_plus)\n\n                x_minus = center_x.copy()\n                x_minus[d] = np.maximum(lb[d], center_x[d] - h)\n                if x_minus[d] == center_x[d]:\n                    x_minus[d] = center_x[d] - np.sign(h) * 1e-6\n                f_minus = safe_eval(x_minus)\n                X.append(x_minus.copy()); F.append(f_minus)\n\n                # Fit quadratic y(t) = a t^2 + b t + c with t in {0, +h, -h}\n                y0 = center_f\n                y1 = f_plus\n                y2 = f_minus\n                # compute b = (y1 - y2) / (2h), a = ((y1 - y0) - b*h) / (h^2)\n                denom = 2.0 * h\n                b = (y1 - y2) / (denom + 1e-16)\n                a = ((y1 - y0) - b * h) / (h * h + 1e-16)\n                # predicted minimizer t* = -b / (2a)\n                if abs(a) < 1e-12:\n                    # nearly linear; take small move opposite gradient sign (i.e., -b)\n                    t_star = -np.sign(b) * min(h * 0.9, 1.0 * h)\n                else:\n                    t_star = -b / (2.0 * a)\n                    # sanity clamp to a reasonable range relative to h\n                    t_star = float(np.clip(t_star, -2.5 * h, 2.5 * h))\n                # Record predicted shift\n                s_shifts[d] = t_star\n                used_dims.append(d)\n\n            # Build candidate by applying all shifts (separable assumption)\n            x_cand = center_x.copy()\n            if len(used_dims) > 0:\n                x_cand[used_dims] += s_shifts[used_dims]\n            # Clip and enforce max step per center (sum norm bounded by 2*mean radius)\n            disp = x_cand - center_x\n            disp_norm = np.linalg.norm(disp)\n            max_allowed = max(1e-12, 2.0 * np.mean(radii_dim))\n            if disp_norm > max_allowed:\n                x_cand = center_x + disp * (max_allowed / disp_norm)\n            x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n\n            # If we have budget, evaluate candidate\n            if evals >= self.budget:\n                break\n            f_cand = safe_eval(x_cand)\n            X.append(x_cand.copy()); F.append(f_cand)\n\n            # Update per-dimension radii depending on improvement compared to center\n            improvement = center_f - f_cand\n            if improvement > 1e-12:\n                # success: increase radii for used dims moderately\n                for d in used_dims:\n                    radii_dim[d] = min(self.max_radius, radii_dim[d] * (1.25 + 0.2 * self.rng.random()))\n                stagnation = 0\n            else:\n                # failure: reduce radii for used dims\n                for d in used_dims:\n                    radii_dim[d] = max(self.min_radius, radii_dim[d] * (0.4 + 0.4 * self.rng.random()))\n                stagnation += 1\n\n            # update best\n            if f_cand < best_f:\n                best_f = f_cand\n                best_x = x_cand.copy()\n                stagnation = 0\n\n            # Occasionally perform a small directional parabola along a random direction\n            if evals + 2 < self.budget and self.rng.random() < 0.15:\n                # direction from differences among recent improvements or random\n                if len(X) >= 3 and self.rng.random() < 0.6:\n                    # combine a couple of recent successful displacements\n                    recent = np.array(X[-min(6, len(X)):])\n                    diffs = recent - center_x\n                    # pick a direction as PCA-1 of diffs\n                    try:\n                        cov = (diffs).T @ (diffs)\n                        w, v = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                        direc = v[:, np.argmax(w)]\n                    except Exception:\n                        direc = self.rng.normal(size=self.dim)\n                else:\n                    direc = self.rng.normal(size=self.dim)\n                direc = direc / (np.linalg.norm(direc) + 1e-12)\n                # choose scalar step size h_dir (based on mean radii)\n                h_dir = np.mean(radii_dim) * (0.7 + 0.6 * self.rng.random())\n                x_plus = np.clip(center_x + direc * h_dir, lb, ub)\n                f_plus = safe_eval(x_plus); X.append(x_plus.copy()); F.append(f_plus)\n                x_minus = np.clip(center_x - direc * h_dir, lb, ub)\n                f_minus = safe_eval(x_minus); X.append(x_minus.copy()); F.append(f_minus)\n                # build parabola in scalar t (0, +h_dir, -h_dir)\n                y0 = center_f; y1 = f_plus; y2 = f_minus\n                b = (y1 - y2) / (2.0 * h_dir + 1e-16)\n                a = ((y1 - y0) - b * h_dir) / (h_dir * h_dir + 1e-16)\n                if abs(a) < 1e-12:\n                    t_star = -np.sign(b) * min(h_dir * 0.9, h_dir)\n                else:\n                    t_star = -b / (2.0 * a)\n                    t_star = float(np.clip(t_star, -2.0 * h_dir, 2.0 * h_dir))\n                x_dir_cand = np.clip(center_x + direc * t_star, lb, ub)\n                if evals < self.budget:\n                    f_dir_cand = safe_eval(x_dir_cand); X.append(x_dir_cand.copy()); F.append(f_dir_cand)\n                    if f_dir_cand < best_f:\n                        best_f = f_dir_cand; best_x = x_dir_cand.copy(); stagnation = 0\n                    else:\n                        stagnation += 1\n                # adapt radii along direction component (projected)\n                proj = np.abs(direc)\n                if f_plus < center_f or f_minus < center_f:\n                    radii_dim = np.minimum(self.max_radius, radii_dim * (1.0 + 0.15 * proj))\n                else:\n                    radii_dim = np.maximum(self.min_radius, radii_dim * (0.9 - 0.25 * proj))\n\n            # If stagnation persists, perform diversification\n            if stagnation >= self.stagnation_restart and evals < self.budget:\n                # sample a few global points\n                n_global = min(4, max(1, self.dim // 3))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = safe_eval(xg); X.append(xg.copy()); F.append(fg)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagnation = 0\n                # one opposite-point probe relative to best_x\n                if evals < self.budget:\n                    x_opp = np.clip(0.5 * (lb + ub) - (best_x - 0.5 * (lb + ub)), lb, ub)\n                    f_opp = safe_eval(x_opp); X.append(x_opp.copy()); F.append(f_opp)\n                    if f_opp < best_f:\n                        best_f = f_opp; best_x = x_opp.copy(); stagnation = 0\n                # mildly reset radii to diversify\n                radii_dim = np.maximum(self.min_radius, np.minimum(self.max_radius, radii_dim * (0.6 + 0.6 * self.rng.random())))\n                stagnation = 0\n\n            # Archive trimming to limit memory and keep computations cheap\n            max_archive = max(200, 6 * self.dim + 50)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = int(max(30, 3 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                # add probabilistic survivors\n                all_f = np.array(F)\n                probs = np.exp(-(all_f - all_f.min()) / (1e-8 + max(1.0, all_f.std())))\n                probs = probs / probs.sum()\n                extras_needed = min(len(X) - keep_best, max_archive - keep_best)\n                extra_idx = self.rng.choice(len(X), size=extras_needed, replace=False, p=probs)\n                for e in extra_idx:\n                    keep_set.add(int(e))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                # radii_dim stays global, not per-sample in this design\n\n        # finalize and return best\n        best_idx = int(np.argmin(F))\n        return float(F[best_idx]), np.array(X[best_idx], dtype=float)", "configspace": "", "generation": 10, "feedback": "The algorithm SeparableAdaptiveBlockParabolaSearch scored 0.443 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0c98b60a-b401-4b27-b5de-4c183326b288"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9617112358889929}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9759682933106879}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9806687015654716}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.15810048772948682}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.5139872549043032}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.002453235661328157}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.011987739499979888}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.040772215055586813}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0015599569527817714}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.05343885766292966}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9711371842197963}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9847887181112509}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9843488016927976}], "aucs": [0.9617112358889929, 0.9759682933106879, 0.9806687015654716, 4.999999999999449e-05, 0.15810048772948682, 0.5139872549043032, 0.002453235661328157, 0.011987739499979888, 0.040772215055586813, 0.0015599569527817714, 0.05343885766292966, 4.999999999999449e-05, 0.9711371842197963, 0.9847887181112509, 0.9843488016927976]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3032.0, "Edges": 3031.0, "Max Degree": 42.0, "Min Degree": 1.0, "Mean Degree": 1.9993403693931397, "Degree Variance": 2.3918201453623964, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.219096965210955, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3190032726418046, "Depth Entropy": 2.0330700425657344, "Assortativity": 1.0947176519772926e-08, "Average Eccentricity": 17.16325857519789, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00032981530343007914, "Average Shortest Path": 9.977515323190707, "mean_complexity": 14.75, "total_complexity": 59.0, "mean_token_count": 665.5, "total_token_count": 2662.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "f9522a70-e49a-453b-8024-cb690f4e3a68", "fitness": "-inf", "name": "SparseEdgeDensifiedAdaptiveSubspaceSearch", "description": "Sparse Edge-Densified Adaptive Subspace Search (Sparse-ED-ASES) \u2014 same edge-densify + adaptive subspace idea but build a sparse, low-degree edge graph (MST + capped k-NN), use a cheap local linear+ridge surrogate to better rank edge-derived candidates, and dynamically budget edge evaluations to focus on the most promising densifications while keeping node degrees low.", "code": "import numpy as np\n\nclass SparseEdgeDensifiedAdaptiveSubspaceSearch:\n    \"\"\"\n    Sparse Edge-Densified Adaptive Subspace Search (Sparse-ED-ASES)\n\n    Main refinements vs. classic ED-ASES:\n      - Build a sparse, low-degree edge graph: always include an MST to preserve connectivity,\n        then add a few k-NN edges but enforce a maximum degree per node (reduces max degree).\n      - Generate virtual candidates along/around edges, but re-rank them using a blended predictor:\n        linear interpolation between endpoints + a tiny ridge linear surrogate fitted on a small local neighborhood.\n      - Dynamically adapt how many edge candidates to truly evaluate based on remaining budget & recent success.\n      - Keep lightweight, budget-aware adaptive subspace exploitation around the current best.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10,\n                 init_radius=2.0, min_radius=1e-6, max_radius=2.5,\n                 stagnation_patience=6, rng=None,\n                 max_archive=40, k_nn=3, degree_max=3,\n                 edge_candidates_pool=120, max_edge_evals_per_cycle=8,\n                 surrogate_lambda=1e-3):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.max_archive = int(max_archive)\n        self.k_nn = int(k_nn)\n        self.degree_max = int(degree_max)\n        self.edge_candidates_pool = int(edge_candidates_pool)\n        self.max_edge_evals_per_cycle = int(max_edge_evals_per_cycle)\n        self.surrogate_lambda = float(surrogate_lambda)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        evals = 0\n        # seed archive with a few diverse points\n        n_init = min(6, max(1, self.dim // 2))\n        archive_x = []\n        archive_f = []\n        for _ in range(n_init):\n            if evals >= self.budget: break\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n        if len(archive_x) == 0:\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n\n        # bookkeeping\n        def prune_archive():\n            if len(archive_x) <= self.max_archive: return\n            order = np.argsort(archive_f)\n            kept_x = []\n            kept_f = []\n            for i in order:\n                xi = archive_x[i]; fi = archive_f[i]\n                too_close = False\n                for xk in kept_x:\n                    if np.linalg.norm(xi - xk) < 1e-9:\n                        too_close = True; break\n                if not too_close:\n                    kept_x.append(xi.copy()); kept_f.append(fi)\n                if len(kept_x) >= self.max_archive: break\n            archive_x[:] = kept_x; archive_f[:] = kept_f\n\n        idx_best = int(np.argmin(archive_f))\n        x_best = archive_x[idx_best].copy()\n        f_best = float(archive_f[idx_best])\n\n        prune_archive()\n        radius = float(self.init_radius)\n        stagnation = 0\n        cycles_since_improve = 0\n\n        # helper: MST (Prim) to ensure minimal connectivity (keeps degrees low)\n        def build_sparse_edge_set(X):\n            n = X.shape[0]\n            if n < 2: return []\n            dists = np.linalg.norm(X[:, None, :] - X[None, :, :], axis=2)\n            in_mst = np.zeros(n, dtype=bool)\n            in_mst[0] = True\n            nearest = np.argmin(dists[0, :])\n            edges = set()\n            # Prim-like\n            for _ in range(n - 1):\n                min_d = np.inf; u = -1; v = -1\n                for i in range(n):\n                    if not in_mst[i]:\n                        # find nearest in current tree\n                        # find j in in_mst minimizing dists[i,j]\n                        js = np.where(in_mst)[0]\n                        if js.size == 0: continue\n                        j = js[np.argmin(dists[i, js])]\n                        if dists[i, j] < min_d:\n                            min_d = dists[i, j]; u = int(i); v = int(j)\n                if u == -1: break\n                edges.add(tuple(sorted((u, v))))\n                in_mst[u] = True\n            # Now add limited k-NN edges but enforce degree cap\n            deg = [0] * n\n            for (a, b) in list(edges):\n                deg[a] += 1; deg[b] += 1\n            # candidate k-NN edges sorted by endpoint quality (prefer better endpoints)\n            for i in range(n):\n                nei = np.argsort(dists[i])[1:1 + self.k_nn]\n                for j in nei:\n                    if i == j: continue\n                    a, b = sorted((int(i), int(j)))\n                    if (a, b) in edges: continue\n                    if deg[a] >= self.degree_max or deg[b] >= self.degree_max:\n                        continue\n                    edges.add((a, b))\n                    deg[a] += 1; deg[b] += 1\n            return list(edges)\n\n        # small routine: local ridge linear surrogate (fit to small neighborhood)\n        def local_linear_predict(x_query, X_local, F_local):\n            # X_local: m x d, F_local: m\n            m, d = X_local.shape\n            if m < 2:\n                return None\n            # Build design with bias\n            A = np.concatenate([X_local, np.ones((m, 1))], axis=1)\n            # ridge: solve (A^T A + lambda I) w = A^T y\n            lam = self.surrogate_lambda\n            ATA = A.T @ A\n            # regularize only on weights, not bias: add lam to diagonal except last element\n            reg = lam * np.eye(d + 1)\n            reg[-1, -1] = 0.0\n            try:\n                w = np.linalg.solve(ATA + reg, A.T @ F_local)\n            except np.linalg.LinAlgError:\n                # fallback to lstsq\n                w, *_ = np.linalg.lstsq(A, F_local, rcond=None)\n            xq_aug = np.concatenate([x_query, np.array([1.0])])\n            return float(np.dot(w, xq_aug))\n\n        # Main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n\n            # 1) Adaptive small-subspace exploitation around x_best\n            raw = self.rng.beta(1.8, 5.5)\n            k = int(np.clip(np.ceil(raw * self.dim), 1, self.dim))\n            approx_batch = int(np.clip(5 * (1.0 + (radius / max(self.max_radius, 1e-12))), 1, 40))\n            batch = max(1, min(remaining, approx_batch))\n            sigma = max(radius / np.sqrt(max(1, k)), 1e-12)\n            improved_in_batch = False\n            for _ in range(batch):\n                if evals >= self.budget: break\n                if k == self.dim:\n                    indices = np.arange(self.dim)\n                else:\n                    indices = self.rng.choice(self.dim, size=k, replace=False)\n                step = self.rng.normal(0.0, sigma, size=k)\n                x_cand = x_best.copy()\n                x_cand[indices] += step\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best - 1e-12:\n                    f_best = f_cand; x_best = x_cand.copy()\n                    archive_x.append(x_best.copy()); archive_f.append(f_best)\n                    improved_in_batch = True\n                    stagnation = 0; cycles_since_improve = 0\n                    radius = min(self.max_radius, radius * 1.2 + 1e-12)\n                    # small exploitation extrapolation along step direction\n                    if evals < self.budget:\n                        probe = x_cand.copy()\n                        probe[indices] += 0.6 * step\n                        probe = np.minimum(np.maximum(probe, lb), ub)\n                        f_probe = float(func(probe)); evals += 1\n                        if f_probe < f_best:\n                            f_best = f_probe; x_best = probe.copy()\n                            archive_x.append(x_best.copy()); archive_f.append(f_best)\n                            radius = min(self.max_radius, radius * 1.3)\n                else:\n                    # slight contraction if many failures\n                    pass\n            if not improved_in_batch:\n                stagnation += 1\n                cycles_since_improve += 1\n                radius *= 0.70  # shrink on average failures\n\n            # 2) Ensure archive correctness & pruning\n            prune_archive()\n            if not any(np.allclose(x_best, xa, atol=1e-12) for xa in archive_x):\n                archive_x.append(x_best.copy()); archive_f.append(f_best)\n                prune_archive()\n\n            # 3) Build sparse edge set and generate candidates (densification)\n            n_elite = len(archive_x)\n            if n_elite >= 2 and evals < self.budget:\n                X = np.vstack(archive_x)\n                F = np.array(archive_f)\n                # compute edges with low max degree\n                edges = build_sparse_edge_set(X)\n                # ordering edges by quality (sum of ranks), prefer edges involving strong elites\n                rank_of = np.argsort(np.argsort(F))  # lower f -> lower rank\n                edge_scores = []\n                for (i, j) in edges:\n                    score = rank_of[i] + rank_of[j]\n                    edge_scores.append((score, (i, j)))\n                edge_scores.sort()\n\n                # generate a candidate pool (cheap predicted scores using linear interpolation + tiny surrogate)\n                lambdas = np.array([0.25, 0.5, 0.75, -0.2, 1.15])\n                pool = []\n                # limit number of edges considered based on pool budget\n                max_edges_to_consider = max(1, min(len(edge_scores), self.edge_candidates_pool // max(1, len(lambdas))))\n                selected_edges = [e for _, e in edge_scores[:max_edges_to_consider]]\n                # create all virtuals (but only store their predicted score cheaply)\n                for (i, j) in selected_edges:\n                    xi = X[i]; xj = X[j]\n                    fi = float(F[i]); fj = float(F[j])\n                    for lam in lambdas:\n                        x_virt = (1.0 - lam) * xi + lam * xj\n                        # linear interpolation baseline prediction\n                        f_lin = (1.0 - lam) * fi + lam * fj\n                        # build a small local neighborhood to fit surrogate: endpoints + nearest up to 4 elites\n                        # nearest indices by distance to x_virt\n                        dists_to_virt = np.linalg.norm(X - x_virt, axis=1)\n                        idxs = np.argsort(dists_to_virt)[:min(6, n_elite)]\n                        X_local = X[idxs]\n                        F_local = F[idxs]\n                        f_surr = local_linear_predict(x_virt, X_local, F_local)\n                        if f_surr is None:\n                            f_pred = f_lin\n                        else:\n                            # blend: favor interpolation for pure edge but use surrogate to adjust\n                            f_pred = 0.65 * f_lin + 0.35 * f_surr\n                        # small bonus for proximity to best and to better endpoints\n                        endpoint_bonus = -0.02 * (min(fi, fj) - f_best)\n                        dist_best = np.linalg.norm(x_virt - x_best)\n                        pool.append((f_pred + endpoint_bonus + 1e-8 * dist_best, i, j, lam, x_virt))\n                if len(pool) > 0:\n                    pool.sort(key=lambda e: e[0])\n                    # Decide how many to truly evaluate this cycle based on remaining budget and recent improvement rate\n                    max_edge_eval = int(min(self.max_edge_evals_per_cycle, max(1, remaining // 10)))\n                    # if many recent improvements, allow slightly more edge evals, else be conservative\n                    if cycles_since_improve <= 1:\n                        max_edge_eval = min(len(pool), max_edge_eval * 2)\n                    max_edge_eval = min(len(pool), max(1, max_edge_eval))\n                    # shortlist: top 2*max_edge_eval for diversity then pick scattered ones\n                    shortlist = pool[:min(len(pool), max(2 * max_edge_eval, 6))]\n                    # pick a mixture: mostly top items plus a couple of near-top randomizations\n                    picks = []\n                    ntop = max(1, int(np.ceil(0.7 * max_edge_eval)))\n                    picks.extend(shortlist[:ntop])\n                    # add some from the next chunk for exploration\n                    if len(shortlist) > ntop:\n                        extras_pool = shortlist[ntop:]\n                        kextras = max_edge_eval - ntop\n                        # sample without replacement if possible, bias toward better predicted\n                        if len(extras_pool) <= kextras:\n                            picks.extend(extras_pool)\n                        else:\n                            probs = np.exp(-np.linspace(0, 2.5, len(extras_pool)))  # bias decreasing\n                            probs = probs / probs.sum()\n                            idxs = self.rng.choice(len(extras_pool), size=kextras, replace=False, p=probs)\n                            for ii in idxs: picks.append(extras_pool[ii])\n                    # evaluate picks sequentially with small orthogonal jitter\n                    for (_, i, j, lam, x_virt) in picks[:max_edge_eval]:\n                        if evals >= self.budget: break\n                        xi = X[i]; xj = X[j]\n                        d = xj - xi\n                        dnorm2 = np.dot(d, d)\n                        if dnorm2 > 1e-14:\n                            v = self.rng.normal(size=self.dim)\n                            proj = (np.dot(v, d) / dnorm2) * d\n                            v_ort = v - proj\n                            norm_ort = np.linalg.norm(v_ort)\n                            if norm_ort > 1e-12:\n                                v_ort = v_ort / norm_ort\n                                orth_scale = max(1e-12, 0.16 * radius)\n                                x_cand = x_virt + v_ort * self.rng.normal(scale=orth_scale)\n                            else:\n                                x_cand = x_virt + self.rng.normal(scale=0.06 * radius, size=self.dim)\n                        else:\n                            x_cand = x_virt + self.rng.normal(scale=0.06 * radius, size=self.dim)\n                        x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                        # skip if duplicate to high precision\n                        too_close = False\n                        for xa in archive_x:\n                            if np.linalg.norm(x_cand - xa) < 1e-9:\n                                too_close = True; break\n                        if too_close:\n                            continue\n                        f_cand = float(func(x_cand)); evals += 1\n                        archive_x.append(x_cand.copy()); archive_f.append(f_cand)\n                        if f_cand < f_best - 1e-12:\n                            f_best = f_cand; x_best = x_cand.copy()\n                            radius = min(self.max_radius, radius * 1.25)\n                            stagnation = 0; cycles_since_improve = 0\n                        else:\n                            radius *= 0.94\n                            cycles_since_improve += 1\n\n            # 4) Occasional opposite sample and targeted global probes on stagnation\n            if evals < self.budget and (stagnation >= 2 and self.rng.random() < 0.28):\n                x_opp = lb + ub - x_best\n                x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                if not any(np.allclose(x_opp, xa, atol=1e-12) for xa in archive_x):\n                    f_opp = float(func(x_opp)); evals += 1\n                    archive_x.append(x_opp.copy()); archive_f.append(f_opp)\n                    if f_opp < f_best - 1e-12:\n                        f_best = f_opp; x_best = x_opp.copy()\n                        radius = min(self.max_radius, radius * 1.2)\n                        stagnation = 0; cycles_since_improve = 0\n\n            if stagnation >= self.stagnation_patience:\n                # controlled global diversification: a few random + one larger jump from a random elite\n                num_global = min(6, max(1, self.dim // 2))\n                improved = False\n                for _ in range(num_global):\n                    if evals >= self.budget: break\n                    x_rand = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_rand = float(func(x_rand)); evals += 1\n                    archive_x.append(x_rand.copy()); archive_f.append(f_rand)\n                    if f_rand < f_best - 1e-12:\n                        f_best = f_rand; x_best = x_rand.copy()\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0; cycles_since_improve = 0\n                        improved = True\n                        break\n                if not improved and evals < self.budget:\n                    idx = int(self.rng.integers(0, len(archive_x)))\n                    base = archive_x[idx]\n                    jump_scale = 0.5 * (ub - lb)\n                    x_jump = base + self.rng.normal(scale=1.0, size=self.dim) * jump_scale\n                    x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                    f_jump = float(func(x_jump)); evals += 1\n                    archive_x.append(x_jump.copy()); archive_f.append(f_jump)\n                    if f_jump < f_best - 1e-12:\n                        f_best = f_jump; x_best = x_jump.copy()\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0; cycles_since_improve = 0\n                    else:\n                        radius = max(self.min_radius, self.init_radius * 0.6)\n                        stagnation = 0\n\n            # safety radius floor handling\n            if radius < self.min_radius:\n                radius = max(self.min_radius, self.init_radius * 0.5)\n                if evals < self.budget:\n                    x_try = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_try = float(func(x_try)); evals += 1\n                    archive_x.append(x_try.copy()); archive_f.append(f_try)\n                    if f_try < f_best - 1e-12:\n                        f_best = f_try; x_best = x_try.copy()\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0; cycles_since_improve = 0\n\n            prune_archive()\n            if evals >= self.budget:\n                break\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 10, "feedback": "Evaluation timed out after 1800 seconds.", "error": "Evaluation timed out after 1800 seconds.", "parent_ids": ["7516fc34-d43c-4696-bca7-3524b4b1cf90"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3723.0, "Edges": 3722.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9994627988181575, "Degree Variance": 2.0214877586885938, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.68884892086331, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3299405984262025, "Depth Entropy": 2.151173742851753, "Assortativity": 0.0, "Average Eccentricity": 18.630942788074133, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00026860059092130003, "Average Shortest Path": 11.121929513489421, "mean_complexity": 14.166666666666666, "total_complexity": 85.0, "mean_token_count": 542.5, "total_token_count": 3255.0, "mean_parameter_count": 3.6666666666666665, "total_parameter_count": 22.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "860477b6-7839-43ad-ad6e-633b0121ab33", "fitness": 0.3031856329939492, "name": "AdaptiveLowRankLinearQuadraticEnsemble", "description": "Adaptive Low-Rank Linear-Quadratic Ensemble (ALR-LQE) \u2014 prefer simple linear surrogates in very low random subspaces with robust weighted LOOCV checks and escalate to small quadratic corrections only when justified; combine with edge extrapolations and controlled L\u00e9vy-style jumps to reduce model degree while keeping strong local exploitation.", "code": "import numpy as np\n\nclass AdaptiveLowRankLinearQuadraticEnsemble:\n    \"\"\"\n    Adaptive Low-Rank Linear-Quadratic Ensemble (ALR-LQE)\n\n    Strategy summary:\n      - Maintain an archive of evaluated points with local trust radii.\n      - For each iteration sample a promising center (biased toward best archived points).\n      - Pick a small random orthonormal subspace (1..max_subdim).\n      - Fit a weighted linear surrogate first (ridge) using nearby archive points projected\n        into that subspace; compute a cheap LOOCV error (via hat-matrix diag) to assess\n        predictive power.\n      - Only escalate to a quadratic surrogate when linear LOOCV is poor *and* a quadratic\n        fit yields substantially better LOOCV (thresholded). This reduces the \"max degree\"\n        the algorithm relies on, guarding against overfitting and poorly conditioned high-degree fits.\n      - Propose candidates by minimizing the accepted surrogate inside a trust radius:\n          - linear => move opposite to linear slope scaled to trust radius (boundary move).\n          - quadratic => solve regularized Hessian system ensuring positive definiteness.\n      - Occasionally evaluate midpoint/extrapolated points between elites (edge extrapolations),\n        and perform rare L\u00e9vy-style jumps when stagnating.\n      - Update per-center trust radii using a predicted vs actual reduction ratio that uses\n        the surrogate's LOOCV as a penalty to predicted decrease (trust weaker surrogates less).\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, init_samples=None,\n                 elite_k=6, init_radius=1.0, min_radius=1e-6, max_radius=5.0,\n                 max_subdim=3, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if init_samples is None:\n            self.init_samples = int(min(50, max(10, self.budget // 12)))\n        else:\n            self.init_samples = int(init_samples)\n        self.elite_k = int(elite_k)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_subdim = int(min(max_subdim, dim))\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _orthonormal_basis(self, k):\n        A = self.rng.normal(size=(self.dim, k))\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def _build_phi_linear(self, S):\n        # S: (m,k), returns Phi_lin: (m, 1+k)\n        m, k = S.shape\n        return np.hstack([np.ones((m, 1)), S])\n\n    def _build_phi_quad(self, S):\n        # combine linear + unique quadratic terms\n        m, k = S.shape\n        cols = [np.ones((m, 1)), S]\n        for i in range(k):\n            cols.append(0.5 * (S[:, i:i+1] ** 2))\n        for i in range(k):\n            for j in range(i+1, k):\n                cols.append((S[:, i] * S[:, j])[:, None])\n        return np.hstack(cols)\n\n    def _weighted_ridge_fit_and_loo(self, Phi, y, w=None, reg=1e-8):\n        \"\"\"\n        Weighted ridge fit: minimize || sqrt(W)(Phi theta - y) ||^2 + reg ||theta||^2\n        Returns theta, y_pred, loo_mse, hat_diag\n        \"\"\"\n        m, p = Phi.shape\n        if w is None:\n            w = np.ones(m)\n        sqrt_w = np.sqrt(w)\n        Phi_s = Phi * sqrt_w[:, None]\n        y_s = y * sqrt_w\n        A = Phi_s.T @ Phi_s\n        A[np.diag_indices_from(A)] += reg + 1e-12\n        rhs = Phi_s.T @ y_s\n        try:\n            invA = np.linalg.inv(A)\n            theta = invA @ rhs\n            y_pred = Phi @ theta\n            # hat diag for LOOCV (on scaled system)\n            H_diag = np.sum((Phi_s @ invA) * Phi_s, axis=1)\n            # prevent division by near 1\n            denom = 1.0 - H_diag\n            denom = np.where(np.abs(denom) < 1e-8, 1e-8 * np.sign(denom) + denom, denom)\n            loo_resid = (y - y_pred) / denom\n            loo_mse = float(np.mean(loo_resid ** 2))\n            return theta, y_pred, loo_mse, H_diag\n        except np.linalg.LinAlgError:\n            # fallback to least squares\n            try:\n                theta, *_ = np.linalg.lstsq(Phi_s, y_s, rcond=None)\n                # map theta back (theta solved for scaled system) -> same theta is okay\n                y_pred = Phi @ theta\n                # approximate H_diag via small regularization solve if possible\n                try:\n                    invA = np.linalg.pinv(A + reg * np.eye(p))\n                    H_diag = np.sum((Phi_s @ invA) * Phi_s, axis=1)\n                    denom = 1.0 - H_diag\n                    denom = np.where(np.abs(denom) < 1e-8, 1e-8 * np.sign(denom) + denom, denom)\n                    loo_resid = (y - y_pred) / denom\n                    loo_mse = float(np.mean(loo_resid ** 2))\n                except Exception:\n                    loo_mse = float(np.mean((y - y_pred) ** 2))\n                    H_diag = np.zeros(m)\n                return theta, y_pred, loo_mse, H_diag\n            except Exception:\n                return None, None, np.inf, np.zeros(m)\n\n    def _theta_to_quad(self, theta, k):\n        a = float(theta[0])\n        b = np.array(theta[1:1 + k], dtype=float)\n        H = np.zeros((k, k), dtype=float)\n        idx = 1 + k\n        for i in range(k):\n            H[i, i] = float(theta[idx]); idx += 1\n        for i in range(k):\n            for j in range(i + 1, k):\n                H[i, j] = float(theta[idx]); H[j, i] = H[i, j]; idx += 1\n        return a, b, H\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []\n        F = []\n        radii = []\n\n        # initial diverse sampling\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n            radii.append(self.init_radius)\n\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x)); evals += 1\n            X.append(np.array(x, dtype=float)); F.append(f); radii.append(self.init_radius)\n\n        best_idx = int(np.argmin(F))\n        best_f = F[best_idx]; best_x = X[best_idx].copy()\n\n        stagnation = 0\n        iter_count = 0\n\n        while evals < self.budget:\n            iter_count += 1\n            n_archive = len(X)\n            # select center among top elites with soft weights\n            top_k = min(self.elite_k, n_archive)\n            idx_sorted = np.argsort(F)\n            top_indices = idx_sorted[:top_k]\n            top_fs = np.array([F[i] for i in top_indices])\n            scale = max(1e-6, np.std(top_fs))\n            w = np.exp(-(top_fs - top_fs.min()) / (1e-8 + scale))\n            probs = w / w.sum()\n            center_choice = int(self.rng.choice(top_indices, p=probs))\n            center_x = X[center_choice].copy()\n            center_f = F[center_choice]\n            center_r = radii[center_choice]\n\n            # subspace dimension\n            k_sub = int(self.rng.integers(1, min(self.max_subdim, self.dim) + 1))\n            basis = self._orthonormal_basis(k_sub)\n\n            # neighbor selection: use ambient distance with local kernel\n            X_arr = np.vstack(X)\n            dists = np.linalg.norm(X_arr - center_x[None, :], axis=1)\n            # neighborhood radius: prefer up to 2*center_r but ensure some coverage\n            neigh_radius = max(center_r * 1.6, np.percentile(dists, min(80, max(30, int(100 * (1 - 1.0 / max(1, n_archive)))))) if n_archive > 3 else center_r * 1.6)\n            within = np.where(dists <= neigh_radius)[0].tolist()\n            # ensure we have at least minimal points\n            p_lin = 1 + k_sub\n            p_quad = 1 + k_sub + (k_sub * (k_sub + 1)) // 2\n            p_need = min(max(p_lin + 1, p_quad + 1), max(p_quad + 1, 6))\n            if len(within) < p_need:\n                order = np.argsort(dists)\n                for idx in order:\n                    if idx not in within:\n                        within.append(int(idx))\n                    if len(within) >= p_need:\n                        break\n\n            X_neigh = X_arr[within]\n            S = (X_neigh - center_x[None, :]) @ basis  # projected coords\n            y = np.array([F[i] for i in within], dtype=float)\n\n            # weighting by distance (locality): gaussian kernel\n            scale_dist = max(1e-8, np.median(np.linalg.norm(X_neigh - center_x[None, :], axis=1)))\n            if scale_dist < 1e-8:\n                scale_dist = center_r if center_r > 0 else 1.0\n            w_kernel = np.exp(-0.5 * (np.linalg.norm(X_neigh - center_x[None, :], axis=1) / (scale_dist + 1e-12)) ** 2)\n            # add small floor\n            w_kernel = np.maximum(w_kernel, 1e-3)\n\n            # try linear fit first\n            Phi_lin = self._build_phi_linear(S)\n            reg_lin = 1e-6 * max(1.0, np.var(y))\n            theta_lin, ypred_lin, loo_lin, hat_lin = self._weighted_ridge_fit_and_loo(Phi_lin, y, w=w_kernel, reg=reg_lin)\n\n            # baseline variance\n            var_y = float(np.var(y)) + 1e-12\n            # if linear LOOCV is small relative to var_y -> accept linear model\n            accept_linear = (loo_lin <= 0.4 * var_y) or (len(within) < p_quad + 3)\n\n            theta_quad = None; ypred_quad = None; loo_quad = np.inf; hat_quad = None\n            if not accept_linear and len(within) >= p_quad + 1:\n                # attempt quadratic fit only if it appears likely to help\n                Phi_quad = self._build_phi_quad(S)\n                reg_quad = 1e-6 * max(1.0, np.var(y))\n                theta_quad, ypred_quad, loo_quad, hat_quad = self._weighted_ridge_fit_and_loo(Phi_quad, y, w=w_kernel, reg=reg_quad)\n                # accept quadratic if LOOCV improves sufficiently (e.g., 30% reduction) compared to linear\n                if not (loo_quad < 0.7 * loo_lin):\n                    # discard quadratic\n                    theta_quad = None\n                    loo_quad = np.inf\n\n            # Based on accepted model, propose candidate\n            if theta_lin is None and theta_quad is None:\n                # fallback: small random step around center\n                if evals >= self.budget:\n                    break\n                step = self.rng.normal(size=self.dim)\n                step = step / (np.linalg.norm(step) + 1e-12) * (center_r * self.rng.random())\n                x_cand = np.clip(center_x + step, lb, ub)\n                f_cand = float(func(x_cand)); evals += 1\n                X.append(x_cand.copy()); F.append(f_cand); radii.append(self.init_radius)\n                if f_cand < best_f:\n                    best_f = f_cand; best_x = x_cand.copy(); stagnation = 0\n                else:\n                    stagnation += 1\n                continue\n\n            model_type = 'linear' if (theta_quad is None) else 'quadratic'\n\n            if model_type == 'linear':\n                # extract intercept and slope\n                a = float(theta_lin[0])\n                b = np.array(theta_lin[1:1 + k_sub], dtype=float)\n                # if slope is tiny -> tiny random probe\n                if np.linalg.norm(b) < 1e-12:\n                    s_dir = self.rng.normal(size=k_sub)\n                else:\n                    s_dir = -b  # move opposite slope to decrease\n                norm_dir = np.linalg.norm(s_dir)\n                step_len = min(center_r * 0.95, max(1e-8, norm_dir))\n                s_prop = s_dir / (norm_dir + 1e-12) * step_len\n                x_prop = center_x + basis @ s_prop\n                # predicted surrogate reduction (linear: directional)\n                pred_val = float(a + b.dot(s_prop))\n                predicted_reduction = center_f - pred_val\n            else:\n                # quadratic path\n                a_hat, b_hat, H_hat = self._theta_to_quad(theta_quad, k_sub)\n                # ensure Hessian is positive definite for minimization; regularize\n                lam = 1e-8\n                s_star = None\n                for attempt in range(8):\n                    H_reg = H_hat + lam * np.eye(k_sub)\n                    try:\n                        s_star = -np.linalg.solve(H_reg, b_hat)\n                        break\n                    except np.linalg.LinAlgError:\n                        lam = max(1e-6, lam * 10.0)\n                if s_star is None or not np.all(np.isfinite(s_star)):\n                    # fallback to linear-like step\n                    if np.linalg.norm(b_hat) < 1e-12:\n                        s_dir = self.rng.normal(size=k_sub)\n                    else:\n                        s_dir = -b_hat\n                    s_star = s_dir / (np.linalg.norm(s_dir) + 1e-12) * (0.75 * center_r)\n                # clip to trust radius\n                if np.linalg.norm(s_star) > 1.5 * center_r:\n                    s_star = s_star / np.linalg.norm(s_star) * (1.5 * center_r)\n                x_prop = center_x + basis @ s_star\n                pred_val = float(a_hat + b_hat.dot(s_star) + 0.5 * float(s_star.T @ (H_hat @ s_star)))\n                predicted_reduction = center_f - pred_val\n\n            # safety: enforce bounds and trust radius\n            x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n            disp = x_prop - center_x\n            disp_norm = np.linalg.norm(disp)\n            max_step = max(1e-12, 1.5 * center_r)\n            if disp_norm > max_step:\n                x_prop = center_x + disp * (max_step / disp_norm)\n            x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n\n            # if predicted reduction is negative or tiny, convert to short step along -slope\n            if predicted_reduction <= 1e-12:\n                # use linear slope if available, else random small\n                if theta_lin is not None:\n                    b_lin = np.array(theta_lin[1:1 + k_sub], dtype=float)\n                    dir_s = -b_lin if np.linalg.norm(b_lin) > 1e-12 else self.rng.normal(size=k_sub)\n                else:\n                    dir_s = - (b_hat if 'b_hat' in locals() else self.rng.normal(size=k_sub))\n                dir_norm = np.linalg.norm(dir_s)\n                s_try = dir_s / (dir_norm + 1e-12) * (0.6 * center_r)\n                x_prop = center_x + basis @ s_try\n                x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n\n            # Evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            f_prop = float(func(x_prop)); evals += 1\n            X.append(x_prop.copy()); F.append(f_prop)\n            new_r = max(self.min_radius, min(self.max_radius, center_r * 0.95))\n            radii.append(new_r)\n\n            # compute actual reduction and adjusted predicted reduction (penalize weak surrogate)\n            actual_reduction = center_f - f_prop\n            # penalize predicted reduction by surrogate LOOCV normalized by var_y: weaker surrogate -> less trusted\n            penalty = 1.0\n            if model_type == 'linear':\n                penalty += min(5.0, (loo_lin / (var_y + 1e-12)))\n            else:\n                penalty += min(5.0, (loo_quad / (var_y + 1e-12)))\n            denom = max(1e-12, predicted_reduction) * penalty\n            rho = actual_reduction / denom if denom != 0 else (1.0 if actual_reduction > 0 else 0.0)\n\n            # trust update\n            if rho > 0.7:\n                radii[center_choice] = min(self.max_radius, center_r * 1.5)\n                radii[-1] = min(self.max_radius, radii[-1] * 1.2)\n            elif rho > 0.2:\n                radii[center_choice] = min(self.max_radius, center_r * 1.12)\n            else:\n                radii[center_choice] = max(self.min_radius, center_r * 0.55)\n\n            # update best and stagnation\n            if f_prop < best_f:\n                best_f = f_prop; best_x = x_prop.copy(); stagnation = 0\n            else:\n                stagnation += 1\n\n            # small quick edge extrapolation occasionally (low-degree heuristic)\n            if evals < self.budget and self.rng.random() < 0.10 and n_archive >= 2:\n                # pick two elites and suggest midpoint and mild extrapolation\n                elites = idx_sorted[:min(6, n_archive)]\n                i1, i2 = int(self.rng.choice(elites)), int(self.rng.choice(elites))\n                if i1 != i2:\n                    x1 = X[i1]; x2 = X[i2]\n                    mid = 0.5 * (x1 + x2)\n                    extrap = np.clip(mid + 0.2 * (mid - center_x), lb, ub)\n                    for cand in (mid, extrap):\n                        if evals >= self.budget:\n                            break\n                        fc = float(func(cand)); evals += 1\n                        X.append(cand.copy()); F.append(fc); radii.append(self.init_radius * 0.9)\n                        if fc < best_f:\n                            best_f = fc; best_x = cand.copy(); stagnation = 0\n\n            # if stagnation high -> diversify with a few random and 1 L\u00e9vy-style jump\n            if stagnation >= 8 and evals < self.budget:\n                n_global = min(3, max(1, self.dim // 4))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg)); evals += 1\n                    X.append(xg.copy()); F.append(fg); radii.append(self.init_radius)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagnation = 0\n                if stagnation >= 8 and evals < self.budget:\n                    # L\u00e9vy-style heavy but clipped jump around best_x\n                    sigma = 0.8 * (ub - lb)\n                    jump = self.rng.standard_cauchy(size=self.dim)\n                    jump = jump / (np.nanmean(np.abs(jump)) + 1e-12)\n                    xj = np.clip(best_x + jump * sigma * 0.3, lb, ub)\n                    fj = float(func(xj)); evals += 1\n                    X.append(xj.copy()); F.append(fj); radii.append(self.init_radius)\n                    if fj < best_f:\n                        best_f = fj; best_x = xj.copy(); stagnation = 0\n                    else:\n                        # gently reset radii to encourage fresh local fits\n                        radii = [max(self.min_radius, 0.6 * self.init_radius) for _ in radii]\n                        stagnation = 0\n\n            # prune archive for cost control (keep best + diversity)\n            max_archive = max(200, 6 * self.dim + 50)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = int(max(40, 4 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                # probabilistic selection favoring better ones\n                Fs = np.array(F)\n                probs = np.exp(-(Fs - Fs.min()) / (1e-8 + max(1.0, np.std(Fs))))\n                probs /= probs.sum()\n                extra_count = min(len(X) - keep_best, max_archive - keep_best)\n                extra = self.rng.choice(len(X), size=extra_count, replace=False, p=probs)\n                for e in extra:\n                    keep_set.add(int(e))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                radii = [radii[i] for i in keep_list]\n\n        best_i = int(np.argmin(F))\n        return float(F[best_i]), np.array(X[best_i], dtype=float)", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveLowRankLinearQuadraticEnsemble scored 0.303 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0c98b60a-b401-4b27-b5de-4c183326b288"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.6712840690160741}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.6682419828412749}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.657371150042338}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.005600341958596267}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.004151833965745366}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.8823536803613142}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8470882219238057}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.81134321480009}], "aucs": [0.6712840690160741, 0.6682419828412749, 0.657371150042338, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.005600341958596267, 0.004151833965745366, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.8823536803613142, 0.8470882219238057, 0.81134321480009]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4286.0, "Edges": 4285.0, "Max Degree": 70.0, "Min Degree": 1.0, "Mean Degree": 1.9995333644423705, "Degree Variance": 2.695753398676828, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.105615662029882, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.322698761642593, "Depth Entropy": 2.0683015842621386, "Assortativity": 4.83023127499173e-09, "Average Eccentricity": 19.911339244050396, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.0002333177788147457, "Average Shortest Path": 10.723461314169876, "mean_complexity": 11.25, "total_complexity": 90.0, "mean_token_count": 469.5, "total_token_count": 3756.0, "mean_parameter_count": 3.5, "total_parameter_count": 28.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "70ca4c92-ebc6-4b42-8e81-d8f095e6b15e", "fitness": 0.24698573159894188, "name": "SparseLowDegreeEdgeSurrogate", "description": "Sparse Low-Degree Edge Surrogate (SLoDES) \u2014 aggressively densify pairwise/triangle edges but strictly cap mixing degree and node degree; use a compact PCA + low-rank quadratic surrogate to score low-degree recombinations and favor diverse, local, trust-region-consistent proposals.", "code": "import numpy as np\n\nclass SparseLowDegreeEdgeSurrogate:\n    \"\"\"\n    Sparse Low-Degree Edge Surrogate (SLoDES)\n\n    One-line: Cap combination degree and node connectivity while using a compact PCA + low-rank\n    quadratic surrogate to score low-degree edge/triangle recombinations within an adaptive trust region.\n\n    Interface:\n      __init__(self, budget, dim, ...) and __call__(self, func)\n\n    Parameters (common defaults tuned for moderate budgets):\n      budget: total function evaluations allowed\n      dim: problem dimensionality\n      pop_size: initial population size (will be clipped against budget)\n      elite_frac: fraction of population considered elite for surrogate fitting\n      k_neighbors: neighbor count considered when building sparse edge graph (candidate edges)\n      proposals_per_cycle: how many actual evaluations to attempt per cycle\n      pool_multiplier: how many virtual candidates to generate per actual evaluation (controls density)\n      max_mix_degree: maximum number of parents allowed in a mix (<=3 recommended)\n      cap_node_degree: maximum allowed degree per node in the edge graph (keeps graph sparse)\n      trust_init: initial trust radius as fraction of search range\n      tri_prob: probability to generate 3-parent barycentric candidate (<=0.5)\n      levy_prob: small probability to add Cauchy-like jump for diversity\n      seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, elite_frac=0.18,\n                 k_neighbors=6, proposals_per_cycle=36, pool_multiplier=8,\n                 max_mix_degree=3, cap_node_degree=10,\n                 trust_init=0.12, tri_prob=0.35, levy_prob=0.05, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_frac = float(elite_frac)\n        self.k_neighbors = int(k_neighbors)\n        self.proposals_per_cycle = int(proposals_per_cycle)\n        self.pool_multiplier = int(pool_multiplier)\n        self.max_mix_degree = int(max(2, min(3, max_mix_degree)))  # enforce 2 or 3\n        self.cap_node_degree = int(max(2, cap_node_degree))\n        self.trust_init = float(trust_init)\n        self.tri_prob = float(tri_prob)\n        self.levy_prob = float(levy_prob)\n\n        # population sizing default heuristic\n        if pop_size is None:\n            base = int(np.clip(np.sqrt(self.budget) * 0.9, 8, 160))\n            self.pop_size = max(base, self.dim + 4)\n        else:\n            self.pop_size = int(pop_size)\n\n        # adaptation constants\n        self.trust_expand = 1.25\n        self.trust_shrink = 0.70\n        self.min_trust = 1e-8\n        self.max_trust = 10.0\n\n    def _get_bounds(self, func):\n        # func may optionally carry bounds, otherwise default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        trust = float(self.trust_init) * range_norm\n\n        # ensure pop_size reasonable\n        pop_size = min(self.pop_size, max(4, self.budget))\n        self.pop_size = int(pop_size)\n\n        # initialize population uniformly and evaluate\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            pop_f[i] = float(func(pop[i])); evals += 1\n\n        # adjacency/edge weights and degrees (sparse)\n        edge_w = np.zeros((self.pop_size, self.pop_size), dtype=float)\n        np.fill_diagonal(edge_w, 0.0)\n        deg = np.zeros(self.pop_size, dtype=int)\n\n        # bookkeeping best\n        idx_best = int(np.argmin(pop_f))\n        x_best = pop[idx_best].copy()\n        f_best = float(pop_f[idx_best])\n\n        cycle = 0\n        stagn = 0\n\n        # helper: add sparse edge but cap node degrees\n        def add_edge(a, b, weight=1.0):\n            if a == b: return\n            if deg[a] >= self.cap_node_degree and a != idx_best:\n                return\n            if deg[b] >= self.cap_node_degree and b != idx_best:\n                return\n            if edge_w[a, b] == 0.0:\n                deg[a] += 1; deg[b] += 1\n                edge_w[a, b] = weight\n                edge_w[b, a] = weight\n            else:\n                edge_w[a, b] = edge_w[a, b] + 0.6 * weight\n                edge_w[b, a] = edge_w[a, b]\n\n        # seed some edges: k-nearest among initial population but capped\n        dif = pop[:, None, :] - pop[None, :, :]\n        dist2 = np.sum(dif * dif, axis=2)\n        for i in range(self.pop_size):\n            neigh = np.argsort(dist2[i])\n            added = 0\n            for nb in neigh:\n                if nb == i: continue\n                add_edge(i, int(nb), weight=1.0)\n                added += 1\n                if added >= self.k_neighbors: break\n\n        # main loop\n        while evals < self.budget:\n            cycle += 1\n            remaining = self.budget - evals\n            act_proposals = min(self.proposals_per_cycle, remaining)\n            if act_proposals <= 0:\n                break\n            pool_size = int(max(act_proposals * self.pool_multiplier, act_proposals + 6))\n\n            improved = False\n\n            # small decay to edge weights to avoid stale dominance\n            edge_w *= 0.98\n            edge_w[edge_w < 1e-12] = 0.0\n            deg = (edge_w > 0).sum(axis=1)\n\n            # compute elites\n            elite_count = max(2, int(np.ceil(self.elite_frac * self.pop_size)))\n            order = np.argsort(pop_f)\n            elites = order[:elite_count]\n\n            # Build compact PCA subspace using elites, choose m small (1..3)\n            use_surrogate = False\n            V = None; Xmean = None; quad = None\n            try:\n                X_base = pop[elites].astype(float)\n                Xmean = X_base.mean(axis=0)\n                Xc = X_base - Xmean\n                U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                m = min(max(1, min(3, Xc.shape[0]-1)), dim)  # small m<=3\n                V = Vt[:m].copy()\n                Z = Xc @ V.T  # elites projected\n                y = pop_f[elites].astype(float)\n                # fit quadratic in subspace: y ~ 1 + z + z^2 + cross (small m keeps feature count small)\n                # build design matrix\n                cols = [np.ones((Z.shape[0], 1))]\n                cols.append(Z)  # linear terms\n                # quadratic and cross\n                quad_cols = []\n                for i in range(m):\n                    for j in range(i, m):\n                        quad_cols.append((Z[:, i] * Z[:, j])[:, None])\n                cols += quad_cols\n                A = np.hstack(cols)\n                # ridge\n                alpha = 1e-6 * (1.0 + np.var(y))\n                AtA = A.T @ A\n                AtA[np.diag_indices_from(AtA)] += alpha\n                w = np.linalg.solve(AtA, A.T @ y)\n                # parse weights\n                offset = 0\n                w0 = w[0]\n                offset += 1\n                w_lin = w[offset:offset+m]; offset += m\n                w_quad = w[offset:].reshape((-1,))  # flatten\n                # build Q matrix (symmetric) for quadratic form in z: 0.5 z^T (Q2) z + w_lin^T z + w0\n                Q = np.zeros((m, m))\n                idx_q = 0\n                for i in range(m):\n                    for j in range(i, m):\n                        coeff = w_quad[idx_q]\n                        if i == j:\n                            Q[i, j] = coeff\n                        else:\n                            Q[i, j] = coeff * 0.5\n                            Q[j, i] = coeff * 0.5\n                        idx_q += 1\n                quad = (w0, w_lin, Q)\n                use_surrogate = True\n            except Exception:\n                use_surrogate = False\n                V = None; quad = None\n\n            # extract sparse candidate edges list based on edge_w and nearest neighbors but enforce degree cap\n            pairs = []\n            triads = []\n            # pairs from highest weights\n            tri_idx = np.triu_indices(self.pop_size, k=1)\n            wflat = edge_w[tri_idx]\n            if wflat.size > 0:\n                topn = min(len(wflat), max(8, self.pop_size))\n                top_idx = np.argsort(wflat)[-topn:]\n                for k in top_idx:\n                    a, b = int(tri_idx[0][k]), int(tri_idx[1][k])\n                    pairs.append((a, b))\n            # ensure some local nearest pairs not present in edge_w\n            for a in elites:\n                neigh = np.argsort(dist2[a])[:(self.k_neighbors*2)]\n                for nb in neigh[:min(3, len(neigh))]:\n                    if a == nb: continue\n                    pairs.append((int(a), int(nb)))\n\n            # unique pairs\n            unique_pairs = []\n            seen = set()\n            for (a,b) in pairs:\n                key = (min(a,b), max(a,b))\n                if key not in seen:\n                    seen.add(key); unique_pairs.append(key)\n            pairs = unique_pairs\n\n            # triads constructed only from Elite triples but limit number (low-degree mixes only)\n            if self.max_mix_degree >= 3 and len(elites) >= 3:\n                ecount = len(elites)\n                samples = min(12, max(3, ecount))\n                for _ in range(samples):\n                    ids = tuple(sorted(map(int, rng.choice(elites, size=3, replace=False))))\n                    if len(set(ids)) == 3:\n                        if ids not in triads:\n                            triads.append(ids)\n\n            # generate candidate pool using only low-degree mixes (pairs and occasional triads)\n            pool = np.empty((pool_size, dim), dtype=float)\n            pool_meta = []\n            for i in range(pool_size):\n                r = rng.random()\n                meta = {}\n                if r < self.tri_prob and len(triads) > 0:\n                    # triangle barycentric limited extrapolation and small noise\n                    ids = triads[rng.integers(0, len(triads))]\n                    a, b, c = ids\n                    Xa, Xb, Xc = pop[a], pop[b], pop[c]\n                    # sample barycentric weights biased toward corners\n                    wdir = rng.gamma(1.0, 1.0, size=3)\n                    wdir /= (wdir.sum() + 1e-12)\n                    x = wdir[0]*Xa + wdir[1]*Xb + wdir[2]*Xc\n                    # mild extrapolation but limited to keep degree low effect\n                    if rng.random() < 0.2:\n                        centroid = (Xa + Xb + Xc) / 3.0\n                        factor = rng.uniform(-0.25, 0.7)\n                        x = centroid + factor * (x - centroid)\n                    x = x + 0.4 * trust * rng.standard_normal(dim)\n                    meta['type'] = 'tri'; meta['ids'] = ids\n                else:\n                    # pairwise interpolation/extrapolation\n                    if len(pairs) == 0:\n                        a, b = 0, min(1, self.pop_size-1)\n                    else:\n                        a, b = pairs[rng.integers(0, len(pairs))]\n                    Xa, Xb = pop[a], pop[b]\n                    # bias toward better endpoint deterministically\n                    if pop_f[a] < pop_f[b]:\n                        t = rng.beta(2.0, 1.2)\n                    else:\n                        t = 1.0 - rng.beta(2.0, 1.2)\n                    # small extrapolation probability\n                    if rng.random() < 0.08:\n                        t = rng.uniform(-0.5, 1.5)\n                    x = (1-t) * Xa + t * Xb\n                    # add orthogonal small perturbation and trust-based jitter\n                    edge_dir = Xb - Xa\n                    norm_ed = np.linalg.norm(edge_dir)\n                    if norm_ed > 1e-12:\n                        unit = edge_dir / norm_ed\n                        # small along-edge jitter\n                        x = x + 0.04 * trust * rng.standard_normal() * unit\n                    x = x + 0.45 * trust * rng.standard_normal(dim)\n                    meta['type'] = 'pair'; meta['ids'] = (int(a), int(b))\n\n                # occasional Levy-like diversity\n                if rng.random() < self.levy_prob:\n                    try:\n                        cauch = rng.standard_cauchy(dim)\n                    except Exception:\n                        u = rng.random(dim); cauch = np.tan(np.pi * (u - 0.5))\n                    x = x + 0.06 * range_norm * cauch\n\n                pool[i] = np.clip(x, lb, ub)\n                pool_meta.append(meta)\n\n            # score pool via surrogate if available, otherwise by distance-to-best + neighbor fitness\n            if use_surrogate and quad is not None and V is not None:\n                # project pool\n                Zpool = (pool - Xmean) @ V.T\n                w0, w_lin, Q = quad\n                # compute predicted y = w0 + w_lin^T z + z^T Q z\n                preds = w0 + Zpool.dot(w_lin) + np.sum(Zpool.dot(Q) * Zpool, axis=1)\n                # penalize too-far-out-of-subspace points\n                # measure reconstruction error: distance between x and back-projection\n                recon = (Zpool @ V) + Xmean\n                recon_err = np.sum((pool - recon)**2, axis=1)\n                preds += 0.02 * (recon_err / (range_norm**2 + 1e-12))\n            else:\n                # cheap heuristic: distance to best plus nearest-pop fitness\n                d2best = np.sum((pool - x_best)**2, axis=1)\n                idx_sub = rng.choice(self.pop_size, size=min(self.pop_size, 12), replace=False)\n                d2sub = np.sum((pool[:, None, :] - pop[idx_sub][None, :, :])**2, axis=2)\n                min_idx = np.argmin(d2sub, axis=1)\n                nn_f = pop_f[idx_sub][min_idx]\n                preds = 0.55 * nn_f + 0.45 * (f_best + 0.6 * np.sqrt(d2best + 1e-12))\n\n            # add small diversity bonus for points that are close to strong edges (prefer candidates near strong edges)\n            # compute a quick edge-influence by sampling a few top-weight edges\n            tri = np.triu_indices(self.pop_size, k=1)\n            if tri[0].size > 0:\n                wflat = edge_w[tri]\n                if wflat.size > 0:\n                    topk = min(10, wflat.size)\n                    top_idx = np.argsort(wflat)[-topk:]\n                    pairs_top = [(int(tri[0][k]), int(tri[1][k])) for k in top_idx]\n                    # for each pool point compute min squared distance to top edges midlines\n                    edge_infl = np.full(pool_size, np.inf)\n                    for (a,b) in pairs_top:\n                        A = pop[a]; B = pop[b]\n                        AB = B - A\n                        denom = np.dot(AB, AB) + 1e-12\n                        t_proj = ((pool - A) @ AB) / denom\n                        t_proj = np.clip(t_proj, 0.0, 1.0)\n                        proj = A[None, :] + t_proj[:, None] * AB[None, :]\n                        d2 = np.sum((pool - proj)**2, axis=1)\n                        edge_infl = np.minimum(edge_infl, d2)\n                    # smaller distance -> better, scale and subtract\n                    preds -= 0.02 * (1.0 / (1.0 + edge_infl / (range_norm**2 + 1e-12)))\n\n            # pick top-K to evaluate\n            K = min(act_proposals, self.budget - evals)\n            order_pool = np.argsort(preds)\n            selected = order_pool[:K]\n\n            # evaluate selected candidates\n            for idx in selected:\n                if evals >= self.budget:\n                    break\n                x_prop = pool[int(idx)].copy()\n                f_prop = float(func(x_prop)); evals += 1\n\n                # decide replacement: try to replace a worse neighbor among nearest few; otherwise replace worst global if improved\n                d2 = np.sum((pop - x_prop)**2, axis=1)\n                nearest = np.argsort(d2)[:min(6, self.pop_size)]\n                replaced = -1\n                # prefer replace the worst among nearest if candidate better\n                nn_order = nearest[np.argsort(pop_f[nearest])[::-1]]\n                for cand in nn_order:\n                    if f_prop < pop_f[cand]:\n                        replaced = int(cand); break\n                if replaced >= 0:\n                    pop[replaced] = x_prop.copy()\n                    pop_f[replaced] = f_prop\n                    # update edges: connect replaced node to parents used in generation (low-degree links)\n                    meta = pool_meta[int(idx)]\n                    if 'ids' in meta:\n                        ids = tuple(int(ii) for ii in meta['ids'])\n                        for a in ids:\n                            add_edge(replaced, a, weight=0.8)\n                    improved = True\n                    trust = min(self.max_trust, trust * self.trust_expand)\n                else:\n                    # maybe replace global worst\n                    idx_w = int(np.argmax(pop_f))\n                    if f_prop < pop_f[idx_w]:\n                        pop[idx_w] = x_prop.copy()\n                        pop_f[idx_w] = f_prop\n                        improved = True\n                        trust = min(self.max_trust, trust * self.trust_expand)\n                        # connect parents to that index\n                        meta = pool_meta[int(idx)]\n                        if 'ids' in meta:\n                            ids = tuple(int(ii) for ii in meta['ids'])\n                            for a in ids:\n                                add_edge(idx_w, a, weight=0.9)\n                    else:\n                        # failure -> shrink trust\n                        trust = max(self.min_trust, trust * self.trust_shrink)\n\n                if f_prop < f_best:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n\n                # stop if budget done\n                if evals >= self.budget:\n                    break\n\n            # stagnation handling: if no improvement for few cycles, do small targeted burst of best-edge midpoints\n            if improved:\n                stagn = 0\n            else:\n                stagn += 1\n                if stagn >= 3 and evals < self.budget:\n                    # find top few edges and evaluate their midpoints (limited)\n                    tri = np.triu_indices(self.pop_size, k=1)\n                    if tri[0].size > 0:\n                        wflat = edge_w[tri]\n                        if wflat.size > 0:\n                            topk = min(6, wflat.size)\n                            topidx = np.argsort(wflat)[-topk:]\n                            for k in topidx:\n                                if evals >= self.budget: break\n                                a = int(tri[0][k]); b = int(tri[1][k])\n                                x_mid = 0.5 * (pop[a] + pop[b]) + 0.28 * trust * rng.standard_normal(dim)\n                                x_mid = np.clip(x_mid, lb, ub)\n                                f_mid = float(func(x_mid)); evals += 1\n                                if f_mid < np.max(pop_f):\n                                    idx_replace = int(np.argmax(pop_f))\n                                    pop[idx_replace] = x_mid.copy()\n                                    pop_f[idx_replace] = f_mid\n                                    add_edge(a, b, weight=1.2)\n                                    if f_mid < f_best:\n                                        f_best = f_mid; x_best = x_mid.copy()\n                    trust = max(self.min_trust, trust * 0.84)\n                    stagn = 0\n\n            # small population refresh: if many individuals are stale and budget allows, sample a very small number\n            if evals < self.budget and (np.mean(pop_f) > 0.9 * f_best + 1e-12) and rng.random() < 0.06:\n                # inject 1-2 random samples near best\n                for _ in range(min(2, self.budget - evals)):\n                    x_r = np.clip(x_best + 0.35 * trust * rng.standard_normal(dim), lb, ub)\n                    f_r = float(func(x_r)); evals += 1\n                    idx_w = int(np.argmax(pop_f))\n                    if f_r < pop_f[idx_w]:\n                        pop[idx_w] = x_r.copy(); pop_f[idx_w] = f_r\n                        if f_r < f_best:\n                            f_best = f_r; x_best = x_r.copy()\n                        # connect new node to best\n                        add_edge(idx_w, idx_best, weight=1.0)\n\n            # stop early if few evals remain to preserve for refinement\n            if self.budget - evals <= max(6, dim * 2):\n                break\n\n        # final local refinement: low-budget adaptive coordinate/direct search around best\n        step = 0.22 * range_norm\n        step_min = max(1e-8, 1e-6 * range_norm)\n        while evals < self.budget and step > step_min:\n            improved_local = False\n            order_dims = list(range(dim))\n            rng.shuffle(order_dims)\n            for d in order_dims:\n                if evals >= self.budget:\n                    break\n                # try +/- moves\n                xp = x_best.copy(); xp[d] = np.clip(xp[d] + step, lb[d], ub[d])\n                fp = float(func(xp)); evals += 1\n                if fp < f_best:\n                    f_best = fp; x_best = xp.copy(); improved_local = True\n                    continue\n                if evals >= self.budget: break\n                xm = x_best.copy(); xm[d] = np.clip(xm[d] - step, lb[d], ub[d])\n                fm = float(func(xm)); evals += 1\n                if fm < f_best:\n                    f_best = fm; x_best = xm.copy(); improved_local = True\n            if not improved_local:\n                step *= 0.5\n            else:\n                step = min(0.7 * range_norm, step * 1.12)\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm SparseLowDegreeEdgeSurrogate scored 0.247 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["45bacdb1-c496-4bcb-8a7b-2ccd06451f70"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.759692415071051}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7898920094454847}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.43694232033224334}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.060904033173867944}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0561296323879682}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.042956514423727055}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.000461290357290145}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.013312865914375083}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.021584578997850823}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.143940947131726}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8092866027896046}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.5695327639589387}], "aucs": [0.759692415071051, 0.7898920094454847, 0.43694232033224334, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.060904033173867944, 0.0561296323879682, 0.042956514423727055, 0.000461290357290145, 0.013312865914375083, 0.021584578997850823, 0.143940947131726, 0.8092866027896046, 0.5695327639589387]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 4365.0, "Edges": 4364.0, "Max Degree": 43.0, "Min Degree": 1.0, "Mean Degree": 1.9995418098510882, "Degree Variance": 2.506758094758237, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.540607872446438, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3246436651638387, "Depth Entropy": 2.1566061834418475, "Assortativity": 0.0, "Average Eccentricity": 17.60068728522337, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0002290950744558992, "Average Shortest Path": 10.796054147072319, "mean_complexity": 24.25, "total_complexity": 97.0, "mean_token_count": 947.25, "total_token_count": 3789.0, "mean_parameter_count": 5.25, "total_parameter_count": 21.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "b051bcc4-0ac3-4198-bf44-23ada9ae1faa", "fitness": 0.32338704142939345, "name": "AdaptiveLowRankQuadraticSurrogates", "description": "Adaptive Low-Rank Quadratic Surrogates (light) \u2014 prefer simpler local models (linear or diagonal-quadratic) selected by a cheap model-score; fit in very low random PCA subspaces, solve inexpensive 1D/axiswise minima, adapt trust radii by predicted vs actual reduction, and inject light global diversity. This reduces overfitting (lowers \"max degree\") while keeping powerful local curvature when supported by data.", "code": "import numpy as np\n\nclass AdaptiveLowRankQuadraticSurrogates:\n    \"\"\"\n    Adaptive Low-Rank Quadratic Surrogates (light)\n\n    Key changes vs full-quadratic ALRQS:\n      - Strong bias towards low-degree models: prefer constant -> linear -> diagonal-quadratic.\n      - Avoid full dense Hessian fits; diagonal quadratic gives curvature per-axis with far fewer params.\n      - Use PCA-based estimation of local intrinsic dim but clamp to 1-3 (very low).\n      - Simple model selection (RSS + complexity penalty) to avoid overfitting on small neighborhoods.\n      - Stable regularized solves and conservative trust-region steps.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_samples=None,\n                 elite_k=6, init_radius=1.0, min_radius=1e-6, max_radius=5.0,\n                 max_subdim=3, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if init_samples is None:\n            self.init_samples = int(min(50, max(10, self.budget // 10)))\n        else:\n            self.init_samples = int(init_samples)\n        self.elite_k = int(elite_k)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        # reduce max_subdim to encourage low-degree simple models\n        self.max_subdim = int(min(max_subdim, dim))\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def _orthonormal_basis(self, k):\n        A = self.rng.normal(size=(self.dim, k))\n        Q, _ = np.linalg.qr(A)\n        return Q[:, :k]\n\n    def _build_design_linear(self, S):\n        # columns: [1, s1..sk]\n        m, k = S.shape\n        return np.hstack([np.ones((m, 1)), S])\n\n    def _build_design_diagquad(self, S):\n        # columns: [1, s1..sk, 0.5*s1^2 .. 0.5*sk^2] (no cross terms)\n        m, k = S.shape\n        quad = 0.5 * (S ** 2)\n        return np.hstack([np.ones((m, 1)), S, quad])\n\n    def _fit_ridge(self, Phi, y, reg):\n        # solve (Phi^T Phi + reg I) theta = Phi^T y robustly\n        A = Phi.T @ Phi\n        diag_idx = np.diag_indices_from(A)\n        A[diag_idx] += reg + 1e-12\n        rhs = Phi.T @ y\n        try:\n            theta = np.linalg.solve(A, rhs)\n        except np.linalg.LinAlgError:\n            theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n        return theta\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0])); ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []\n        F = []\n        radii = []\n\n        # initial seeding\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            X.append(np.array(x, dtype=float)); F.append(f); radii.append(self.init_radius)\n\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x)); evals += 1\n            X.append(np.array(x, dtype=float)); F.append(f); radii.append(self.init_radius)\n\n        best_idx = int(np.argmin(F))\n        best_x = X[best_idx].copy()\n        best_f = float(F[best_idx])\n\n        stagnation = 0\n        iter_count = 0\n\n        while evals < self.budget:\n            iter_count += 1\n            n_archive = len(X)\n\n            # choose a center biased to better points\n            top_k = min(self.elite_k, n_archive)\n            idx_sorted = np.argsort(F)\n            top_idx = idx_sorted[:top_k]\n            top_fs = np.array([F[i] for i in top_idx], dtype=float)\n            scale = max(1e-8, np.std(top_fs))\n            w = np.exp(-(top_fs - top_fs.min()) / (scale + 1e-12))\n            probs = w / w.sum()\n            center_choice = int(self.rng.choice(top_idx, p=probs))\n            center_x = X[center_choice].copy()\n            center_f = float(F[center_choice])\n            center_r = float(radii[center_choice])\n\n            # determine local subspace dimension via PCA of neighborhood but bias low\n            X_arr = np.vstack(X)\n            dists = np.linalg.norm(X_arr - center_x[None, :], axis=1)\n            neigh_radius = max(center_r * 1.6, np.percentile(dists, 60) if n_archive >= 4 else center_r * 1.6)\n            within_idx = np.where(dists <= neigh_radius)[0].tolist()\n            # ensure we have at least a few samples\n            p_min = 6  # we prefer very small models so need modest sample count\n            if len(within_idx) < p_min:\n                order = np.argsort(dists)\n                for idx in order:\n                    if idx not in within_idx:\n                        within_idx.append(int(idx))\n                    if len(within_idx) >= p_min:\n                        break\n\n            X_neigh = X_arr[within_idx]\n            Y_neigh = np.array([F[i] for i in within_idx], dtype=float)\n            M = X_neigh - center_x[None, :]\n\n            # small PCA to estimate intrinsic dim\n            if M.shape[0] >= 2:\n                try:\n                    U, svals, _ = np.linalg.svd(M, full_matrices=False)\n                    explained = svals ** 2\n                    total = explained.sum() + 1e-12\n                    cum = np.cumsum(explained) / total\n                    # choose the smallest k such that cum >= 0.8, but clamp between 1 and max_subdim\n                    k_est = int(np.searchsorted(cum, 0.8) + 1)\n                    k_sub = min(max(1, k_est), min(self.max_subdim, self.dim))\n                except Exception:\n                    k_sub = min(2, self.max_subdim)\n            else:\n                k_sub = 1\n\n            # bias to keep k_sub small (1-3)\n            if k_sub > 1 and self.rng.random() < 0.6:\n                k_sub = max(1, k_sub - 1)\n\n            basis = self._orthonormal_basis(k_sub)  # (dim, k_sub)\n            S = (X_neigh - center_x[None, :]) @ basis  # (m, k_sub)\n\n            # candidate models: constant, linear, diagonal-quadratic\n            m = S.shape[0]\n            y = Y_neigh\n            var_y = max(1e-12, np.var(y))\n\n            # MODEL 0: constant\n            a0 = np.mean(y)\n            pred0 = np.full(m, a0)\n            rss0 = float(np.sum((y - pred0) ** 2))\n            score0 = rss0 + 1e-6 * var_y * 1.0  # penalize complexity slightly\n\n            # MODEL 1: linear\n            Phi_lin = self._build_design_linear(S)\n            p_lin = Phi_lin.shape[1]\n            reg_lin = 1e-8 * (var_y + 1e-6) * m\n            theta_lin = self._fit_ridge(Phi_lin, y, reg_lin)\n            pred_lin = Phi_lin @ theta_lin\n            rss_lin = float(np.sum((y - pred_lin) ** 2))\n            score_lin = rss_lin + 1e-6 * var_y * p_lin * m * 0.01\n\n            # MODEL 2: diagonal quadratic (only axis-wise second-order)\n            Phi_diag = self._build_design_diagquad(S)\n            p_diag = Phi_diag.shape[1]\n            reg_diag = 1e-8 * (var_y + 1e-6) * m\n            theta_diag = self._fit_ridge(Phi_diag, y, reg_diag)\n            pred_diag = Phi_diag @ theta_diag\n            rss_diag = float(np.sum((y - pred_diag) ** 2))\n            # penalize higher complexity a bit more so diag used only when helpful\n            score_diag = rss_diag + 1e-6 * var_y * p_diag * m * 0.02\n\n            # pick model with lowest score\n            scores = [score0, score_lin, score_diag]\n            choice = int(np.argmin(scores))\n\n            # Setup prediction and solver according to model\n            if choice == 0:\n                # constant model -> do a directed random small step (diversify)\n                # take a small step toward best if better than center or random otherwise\n                if best_f < center_f:\n                    dir_vec = best_x - center_x\n                    if np.linalg.norm(dir_vec) < 1e-12:\n                        step = self.rng.normal(size=self.dim)\n                    else:\n                        step = dir_vec\n                else:\n                    step = self.rng.normal(size=self.dim)\n                step = step / (np.linalg.norm(step) + 1e-12) * (0.8 * center_r * self.rng.random())\n                x_prop = center_x + step\n                x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n                pred_val = a0\n            elif choice == 1:\n                # linear model: value = a + b^T s\n                a_hat = float(theta_lin[0])\n                b_hat = np.array(theta_lin[1:], dtype=float)\n                # direction s = -b_hat scaled to trust radius\n                bnorm = np.linalg.norm(b_hat)\n                if bnorm < 1e-12:\n                    s_star = -0.6 * center_r * (self.rng.normal(size=k_sub))\n                else:\n                    # choose step length proportional to predicted curvature (unknown) -> conservative\n                    step_len = min(1.2 * center_r, 0.9 * center_r)\n                    s_star = -b_hat / (bnorm + 1e-12) * step_len\n                # map to ambient\n                x_prop = center_x + basis @ s_star\n                disp = x_prop - center_x\n                disp_norm = np.linalg.norm(disp)\n                max_step = max(1e-12, 1.5 * center_r)\n                if disp_norm > max_step:\n                    x_prop = center_x + disp * (max_step / disp_norm)\n                x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n                pred_val = float(a_hat + float(b_hat.dot(s_star)))\n            else:\n                # diagonal quadratic: theta = [a, b1..bk, q1..qk] where model = a + b^T s + 0.5 * sum(q * s^2)\n                a_hat = float(theta_diag[0])\n                b_hat = np.array(theta_diag[1:1 + k_sub], dtype=float)\n                q_hat = np.array(theta_diag[1 + k_sub:1 + k_sub + k_sub], dtype=float)\n                # try to solve each axis: q_i * s_i + b_i = 0 => s_i = -b_i/q_i (if q_i > 0)\n                s_star = np.zeros(k_sub, dtype=float)\n                denom_mask = np.abs(q_hat) > 1e-8\n                # for positive curvature use analytic; for small/negative curvature do damped step\n                for i in range(k_sub):\n                    if denom_mask[i] and q_hat[i] > 0:\n                        s_star[i] = -b_hat[i] / q_hat[i]\n                    else:\n                        # fallback: step along -b in that coordinate scaled conservatively\n                        s_star[i] = -0.8 * b_hat[i] / (np.linalg.norm(b_hat) + 1e-12) * center_r * 0.5\n                # if this gives huge move, scale down\n                s_norm = np.linalg.norm(s_star)\n                if s_norm > 1.5 * center_r:\n                    s_star = s_star * ((1.5 * center_r) / (s_norm + 1e-12))\n                x_prop = center_x + basis @ s_star\n                disp = x_prop - center_x\n                disp_norm = np.linalg.norm(disp)\n                max_step = max(1e-12, 1.5 * center_r)\n                if disp_norm > max_step:\n                    x_prop = center_x + disp * (max_step / disp_norm)\n                x_prop = np.minimum(np.maximum(x_prop, lb), ub)\n                pred_val = float(a_hat + b_hat.dot(s_star) + 0.5 * float(np.sum(q_hat * (s_star ** 2))))\n\n            # Evaluate candidate if budget remains\n            if evals >= self.budget:\n                break\n            f_prop = float(func(x_prop))\n            evals += 1\n\n            # Add to archive\n            X.append(x_prop.copy()); F.append(f_prop)\n            radii.append(max(self.min_radius, min(self.max_radius, center_r * 0.9)))\n\n            # predicted reduction and actual reduction\n            predicted_reduction = center_f - pred_val\n            actual_reduction = center_f - f_prop\n            denom = max(1e-12, abs(predicted_reduction))\n            rho = actual_reduction / denom if denom > 0 else (1.0 if actual_reduction > 0 else 0.0)\n\n            # update radius for the center\n            if rho > 0.75:\n                radii[center_choice] = min(self.max_radius, center_r * 1.5)\n                radii[-1] = min(self.max_radius, radii[-1] * 1.1)\n            elif rho > 0.25:\n                radii[center_choice] = min(self.max_radius, center_r * 1.1)\n            else:\n                radii[center_choice] = max(self.min_radius, center_r * 0.6)\n\n            # update best\n            if f_prop < best_f:\n                best_f = f_prop; best_x = x_prop.copy(); stagnation = 0\n            else:\n                stagnation += 1\n\n            # quick greedy probe occasionally along interpolation to best\n            if evals < self.budget and self.rng.random() < 0.13:\n                alpha = self.rng.random() * 0.6\n                x_probe = np.clip(alpha * center_x + (1 - alpha) * best_x, lb, ub)\n                f_probe = float(func(x_probe)); evals += 1\n                X.append(x_probe.copy()); F.append(f_probe); radii.append(self.init_radius * 0.8)\n                if f_probe < best_f:\n                    best_f = f_probe; best_x = x_probe.copy(); stagnation = 0\n\n            # If stagnation, add controlled global samples (very light)\n            if stagnation >= 8 and evals < self.budget:\n                n_global = min(3, max(1, self.dim // 6))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg)); evals += 1\n                    X.append(xg.copy()); F.append(fg); radii.append(self.init_radius)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagnation = 0\n                # if still stagnating, perform one larger jump around best_x\n                if stagnation >= 8 and evals < self.budget:\n                    jump_scale = 0.4 * (ub - lb)\n                    xj = best_x + self.rng.normal(size=self.dim) * jump_scale\n                    xj = np.minimum(np.maximum(xj, lb), ub)\n                    fj = float(func(xj)); evals += 1\n                    X.append(xj.copy()); F.append(fj); radii.append(self.init_radius)\n                    if fj < best_f:\n                        best_f = fj; best_x = xj.copy(); stagnation = 0\n                    else:\n                        # gentle radius reset\n                        radii = [max(self.min_radius, 0.7 * self.init_radius) for _ in radii]\n                        stagnation = 0\n\n            # trim archive to keep cost bounded\n            max_archive = max(200, 6 * self.dim + 50)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = int(max(30, 3 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                # biased random keep\n                probs = np.exp(-(np.array(F) - np.min(F)) / (1e-8 + max(1.0, np.std(F))))\n                probs /= probs.sum()\n                extra_n = min(len(X) - keep_best, max_archive - keep_best)\n                if extra_n > 0:\n                    extra = self.rng.choice(len(X), size=extra_n, replace=False, p=probs)\n                    for e in extra:\n                        keep_set.add(int(e))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]; F = [F[i] for i in keep_list]; radii = [radii[i] for i in keep_list]\n\n        best_i = int(np.argmin(F))\n        return float(F[best_i]), np.array(X[best_i], dtype=float)", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveLowRankQuadraticSurrogates scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0c98b60a-b401-4b27-b5de-4c183326b288"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.7725278754000073}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7580441777061548}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7331154417730037}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.008210210029272469}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03938108901911941}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0018418234863034622}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9543663087973737}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.804968020457715}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.7780506747719522}], "aucs": [0.7725278754000073, 0.7580441777061548, 0.7331154417730037, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.008210210029272469, 0.03938108901911941, 0.0018418234863034622, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.9543663087973737, 0.804968020457715, 0.7780506747719522]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3446.0, "Edges": 3445.0, "Max Degree": 68.0, "Min Degree": 1.0, "Mean Degree": 1.9994196169471852, "Degree Variance": 2.9315144629233583, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.161394448030988, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3119653923262373, "Depth Entropy": 2.083221659590702, "Assortativity": 0.0, "Average Eccentricity": 18.863029599535693, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0002901915264074289, "Average Shortest Path": 10.1244361481771, "mean_complexity": 9.571428571428571, "total_complexity": 67.0, "mean_token_count": 428.7142857142857, "total_token_count": 3001.0, "mean_parameter_count": 3.4285714285714284, "total_parameter_count": 24.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "6e79ae04-b7ad-446c-8f86-291cfd1fc410", "fitness": 0.5094949338102683, "name": "ProgressiveCoordinatePairwiseSearch", "description": "Progressive Coordinate-and-Pairwise Exploratory Search (PCPES) \u2014 keep a compact set of anchors, perform low-degree adaptive 1-D coordinate probes with per-coordinate step sizes, occasional small 2-D pairwise explorations and rare Cauchy jumps from the best; increase steps on success, shrink on failure, and replace stale anchors with near-best restarts to balance exploitation and cheap diversification.", "code": "import numpy as np\n\nclass ProgressiveCoordinatePairwiseSearch:\n    \"\"\"\n    Progressive Coordinate-and-Pairwise Exploratory Search (PCPES)\n    - budget: maximum number of function evaluations (must be >=1)\n    - dim: problem dimensionality\n    Optional args:\n      - pop_size: number of anchor points (compact, default ~5)\n      - init_step: initial per-coordinate step as fraction of range (default 0.25)\n      - success_mult / failure_mult: multiplicative adaptation of steps\n      - pair_prob: probability to do a pairwise 2D exploration for an anchor\n      - levy_prob: rare heavy-tailed jump probability from global best\n      - seed: RNG seed\n    The algorithm tries to keep operations low-degree (simple 1-D probes + occasional 2-D),\n    adapt per-coordinate step sizes, and uses cheap replacement/stagnation handling.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.25, success_mult=1.25, failure_mult=0.7,\n                 pair_prob=0.18, levy_prob=0.03, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # compact population to keep algorithm low-degree\n        if pop_size is None:\n            self.pop_size = max(3, min(8, int(round(5 + 0.08 * self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # adaptation parameters\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.pair_prob = float(pair_prob)\n        self.levy_prob = float(levy_prob)\n\n    def _get_bounds(self, func):\n        # if function exposes bounds use them, otherwise default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        # scale parameters relative to box size (L2 norm)\n        box_len = np.linalg.norm(ub - lb)\n        step_scale = max(1e-12, self.init_step_frac * box_len)\n\n        evals = 0\n\n        # initialize anchors uniformly and evaluate\n        anchors = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        anchor_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget: break\n            anchor_f[i] = float(func(anchors[i]))\n            evals += 1\n\n        # per-anchor, per-coordinate step sizes\n        steps = np.full((self.pop_size, dim), step_scale)\n        # simple activity counters for stagnation detection\n        no_improve_counts = np.zeros(self.pop_size, dtype=int)\n\n        # track global best\n        idx_best = int(np.argmin(anchor_f))\n        x_best = anchors[idx_best].copy()\n        f_best = float(anchor_f[idx_best])\n\n        # main iterative loop: each iteration visits anchors and does cheap probes\n        while evals < self.budget:\n            improved_any = False\n\n            # optionally perform a rare Levy jump from the best to test remote basin\n            if rng.random() < self.levy_prob and evals < self.budget:\n                # Cauchy-like (standard tan(pi*(u-0.5))) scaled modestly\n                u = rng.random(dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.18 * box_len * cauchy\n                x_jump = np.clip(x_best + jump, lb, ub)\n                f_jump = float(func(x_jump))\n                evals += 1\n                if f_jump < f_best:\n                    f_best = f_jump; x_best = x_jump.copy(); improved_any = True\n                    # replace worst anchor with this jump\n                    idx_w = int(np.argmax(anchor_f))\n                    anchors[idx_w] = x_jump.copy()\n                    anchor_f[idx_w] = f_jump\n                    steps[idx_w, :] = step_scale\n                # continue, don't consume anchor budget here beyond the single eval\n\n            # iterate anchors in random order to reduce bias\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget: break\n\n                x0 = anchors[idx].copy()\n                f0 = anchor_f[idx]\n                improved_anchor = False\n\n                # random coordinate order for this anchor\n                coords = rng.permutation(dim)\n\n                # 1-D adaptive coordinate probing (cheap, low-degree)\n                for c in coords:\n                    if evals >= self.budget: break\n\n                    delta = steps[idx, c]\n                    if delta <= 1e-12 * box_len:\n                        # too small, skip probing this coordinate\n                        continue\n\n                    # try positive direction first\n                    x_pos = x0.copy()\n                    x_pos[c] = np.clip(x0[c] + delta, lb[c], ub[c])\n                    f_pos = float(func(x_pos))\n                    evals += 1\n\n                    if f_pos < f0:\n                        # accept positive move\n                        anchors[idx] = x_pos\n                        anchor_f[idx] = f_pos\n                        x0 = x_pos.copy(); f0 = f_pos\n                        steps[idx, c] *= self.success_mult\n                        improved_anchor = True\n                        no_improve_counts[idx] = 0\n                        # continue probing same coordinate with increased step (aggressive if works)\n                        # but cap evals\n                        if evals >= self.budget: break\n                        # optional short line search: try another step in same sign\n                        x_pos2 = x0.copy()\n                        x_pos2[c] = np.clip(x0[c] + steps[idx, c], lb[c], ub[c])\n                        f_pos2 = float(func(x_pos2))\n                        evals += 1\n                        if f_pos2 < f0:\n                            anchors[idx] = x_pos2\n                            anchor_f[idx] = f_pos2\n                            x0 = x_pos2.copy(); f0 = f_pos2\n                            steps[idx, c] *= self.success_mult\n                        else:\n                            # if second step fails, slightly reduce the step so it doesn't blow up\n                            steps[idx, c] *= 0.95\n                        continue\n\n                    # try negative direction\n                    x_neg = x0.copy()\n                    x_neg[c] = np.clip(x0[c] - delta, lb[c], ub[c])\n                    f_neg = float(func(x_neg))\n                    evals += 1\n\n                    if f_neg < f0:\n                        anchors[idx] = x_neg\n                        anchor_f[idx] = f_neg\n                        x0 = x_neg.copy(); f0 = f_neg\n                        steps[idx, c] *= self.success_mult\n                        improved_anchor = True\n                        no_improve_counts[idx] = 0\n                        # short additional step in same sign\n                        if evals >= self.budget: break\n                        x_neg2 = x0.copy()\n                        x_neg2[c] = np.clip(x0[c] - steps[idx, c], lb[c], ub[c])\n                        f_neg2 = float(func(x_neg2))\n                        evals += 1\n                        if f_neg2 < f0:\n                            anchors[idx] = x_neg2\n                            anchor_f[idx] = f_neg2\n                            x0 = x_neg2.copy(); f0 = f_neg2\n                            steps[idx, c] *= self.success_mult\n                        else:\n                            steps[idx, c] *= 0.95\n                        continue\n\n                    # no improvement in either direction => shrink step\n                    steps[idx, c] *= self.failure_mult\n                    no_improve_counts[idx] += 1\n\n                # occasional 2-D pairwise exploration for this anchor (still low-degree: try 4 points)\n                if (rng.random() < self.pair_prob) and (evals + 4 <= self.budget) and dim >= 2:\n                    # pick two coords biased to ones with larger steps\n                    coord_scores = steps[idx, :] + 1e-12\n                    # sample two coordinates proportionally (without replacement)\n                    probs = coord_scores / coord_scores.sum()\n                    a = rng.choice(dim, p=probs)\n                    b = a\n                    while b == a:\n                        b = rng.choice(dim, p=probs)\n                    # small 2D cross of +/- on both coords\n                    base = anchors[idx].copy()\n                    cand_points = []\n                    deltas = np.array([steps[idx, a], steps[idx, b]])\n                    for sa in (+1.0, -1.0):\n                        for sb in (+1.0, -1.0):\n                            x2 = base.copy()\n                            x2[a] = np.clip(base[a] + sa * deltas[0], lb[a], ub[a])\n                            x2[b] = np.clip(base[b] + sb * deltas[1], lb[b], ub[b])\n                            cand_points.append(x2)\n                    # evaluate candidates (cheap batch-like sequentially but careful with budget)\n                    improved_local = False\n                    for x_cand in cand_points:\n                        if evals >= self.budget: break\n                        f_cand = float(func(x_cand))\n                        evals += 1\n                        if f_cand < anchor_f[idx]:\n                            anchors[idx] = x_cand.copy()\n                            anchor_f[idx] = f_cand\n                            steps[idx, a] *= self.success_mult\n                            steps[idx, b] *= self.success_mult\n                            improved_local = True\n                            improved_anchor = True\n                            no_improve_counts[idx] = 0\n                            # accept first improving candidate and stop pairwise exploration\n                            break\n                    if not improved_local:\n                        # penalize steps for both coords\n                        steps[idx, a] *= self.failure_mult\n                        steps[idx, b] *= self.failure_mult\n\n                # update global best if anchor improved\n                if anchor_f[idx] < f_best:\n                    f_best = float(anchor_f[idx])\n                    x_best = anchors[idx].copy()\n                    improved_any = True\n\n                # stagnation handling per-anchor: if many non-improvements, respawn near best\n                if no_improve_counts[idx] > max(8, 3 * dim) and evals < self.budget:\n                    no_improve_counts[idx] = 0\n                    # replace anchor with near-best perturbation (small gaussian) to refocus\n                    newp = x_best + 0.06 * (ub - lb) * rng.standard_normal(dim)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    anchors[idx] = newp\n                    anchor_f[idx] = f_new\n                    steps[idx, :] = step_scale  # reset steps\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy(); improved_any = True\n\n            # small population maintenance: keep steps within reasonable bounds\n            min_step = 1e-9 * box_len\n            max_step = 2.0 * box_len\n            steps = np.clip(steps, min_step, max_step)\n\n            # if nothing improved for a full pass, consider a random injection to worst anchor\n            if not improved_any and evals < self.budget and rng.random() < 0.12:\n                idx_w = int(np.argmax(anchor_f))\n                if evals < self.budget:\n                    new_rand = rng.uniform(lb, ub)\n                    f_rand = float(func(new_rand))\n                    evals += 1\n                    anchors[idx_w] = new_rand\n                    anchor_f[idx_w] = f_rand\n                    steps[idx_w, :] = step_scale\n                    if f_rand < f_best:\n                        f_best = f_rand; x_best = new_rand.copy()\n\n            # break early into final refinement if budget is small\n            if self.budget - evals < max(6, dim * 2):\n                break\n\n        # final coordinate pattern search on the best found point (deterministic, cheap)\n        mesh = 0.12 * box_len\n        mesh_min = 1e-7 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                # positive step\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                if evals >= self.budget: break\n                # negative step\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                f_try = float(func(x_try))\n                evals += 1\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # final store\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm ProgressiveCoordinatePairwiseSearch scored 0.509 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5713cbd-1130-4e8d-abdc-3c3ca8966b71"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.7415280899113179}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7362279532992235}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7462501658876903}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.49611151265919573}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.5438306885569704}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.5542900304342273}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.15269473568283598}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.13790681901010116}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.15387278885304223}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.12772948673272588}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.11892266659596884}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.13745622196048535}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9987105966277792}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9984191212950192}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9984731296474431}], "aucs": [0.7415280899113179, 0.7362279532992235, 0.7462501658876903, 0.49611151265919573, 0.5438306885569704, 0.5542900304342273, 0.15269473568283598, 0.13790681901010116, 0.15387278885304223, 0.12772948673272588, 0.11892266659596884, 0.13745622196048535, 0.9987105966277792, 0.9984191212950192, 0.9984731296474431]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2419.0, "Edges": 2418.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.9991732120711037, "Degree Variance": 2.097560292031477, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.517179023508138, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3291545499043895, "Depth Entropy": 2.086897034392209, "Assortativity": 0.0, "Average Eccentricity": 18.952046300124017, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00041339396444811904, "Average Shortest Path": 10.369539327306466, "mean_complexity": 18.0, "total_complexity": 54.0, "mean_token_count": 662.0, "total_token_count": 1986.0, "mean_parameter_count": 4.666666666666667, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "bcfd9736-e201-42dc-b4ff-41ebc7ae99ff", "fitness": 0.35993813853727397, "name": "RotatedCoordinateExtrapolation", "description": "Low-degree Rotated Coordinate Search with parabolic 1-D interpolation and compact orthogonal 2-D scans \u2014 prioritize cheap 1-D probes, fit local parabolas when available, keep rotated scans restricted to a 2-vector orthonormal basis, and adapt coordinate probabilities by success rate to maintain a very low algebraic degree while improving exploitation/diversification balance.", "code": "import numpy as np\n\nclass RotatedCoordinateExtrapolation:\n    \"\"\"\n    Refined Rotated Coordinate Extrapolation (low-degree)\n    - Strictly favors 1-D probes and low-rank (\u22642) rotated-plane scans.\n    - Uses parabolic interpolation on symmetric 1-D probes to estimate a local min.\n    - Maintains a tiny orthonormal direction memory (max 2) for rotated scans.\n    - Adapts per-dimension probe probabilities by success frequency (exponential recency).\n    - All candidates are clipped to bounds and budget is strictly honored.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_step=1.6, min_step=1e-6, max_step=2.5,\n                 elite_size=3, dir_memory=2, stagn_patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.elite_size = int(elite_size)\n        self.dir_memory = int(dir_memory)  # keep at most 2 for very low-degree rotated scans\n        self.stagn_patience = int(stagn_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shapes match dimension\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n\n        # handle trivial budgets\n        if self.budget <= 0:\n            x0 = np.clip(0.5 * (lb + ub), lb, ub)\n            return float(func(x0)), x0\n\n        # small initial sampling to seed best / elites (very small to preserve budget)\n        n_init = min(6, max(1, self.budget // 200 + 1))\n        xs = []\n        fs = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            xs.append(x.copy())\n            fs.append(f)\n\n        if len(fs) == 0:\n            x0 = lb + 0.5 * (ub - lb)\n            return float(func(x0)), x0\n\n        idx_best = int(np.argmin(fs))\n        x_best = xs[idx_best].copy()\n        f_best = float(fs[idx_best])\n\n        # elite archive as (f, x)\n        elite = sorted(zip(fs, xs), key=lambda t: t[0])[: self.elite_size]\n        elite = [(float(t[0]), t[1].copy()) for t in elite]\n\n        # compact orthonormal direction memory (max 2)\n        dir_mem = []\n\n        # per-dimension probe selection probabilities (start uniform)\n        dim_probs = np.ones(self.dim, dtype=float) / float(self.dim)\n        # per-dim success counts for soft adaptation\n        success_counts = np.zeros(self.dim, dtype=float)\n\n        step = float(self.init_step)\n        stagn = 0\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def push_dir(v):\n            # push normalized direction, maintain orthonormal set <= dir_memory\n            norm = np.linalg.norm(v)\n            if norm <= 1e-12:\n                return\n            u = v / norm\n            # orthogonalize vs existing mem\n            out = u.copy()\n            for i in range(len(dir_mem)):\n                proj = np.dot(out, dir_mem[i]) * dir_mem[i]\n                out = out - proj\n                if np.linalg.norm(out) < 1e-12:\n                    return  # nearly redundant\n            out = out / np.linalg.norm(out)\n            dir_mem.insert(0, out)\n            # re-orthonormalize and trim to dir_memory\n            if len(dir_mem) > 1:\n                # Gram-Schmidt small set\n                A = []\n                for v in dir_mem:\n                    w = v.copy()\n                    for q in A:\n                        w -= np.dot(w, q) * q\n                    nrm = np.linalg.norm(w)\n                    if nrm > 1e-12:\n                        A.append(w / nrm)\n                dir_mem[:] = A[:self.dir_memory]\n            if len(dir_mem) > self.dir_memory:\n                del dir_mem[self.dir_memory:]\n\n        def update_elite(xcand, fcand):\n            nonlocal elite\n            # add uniquely if improves\n            tol = 1e-8 + 1e-8 * np.linalg.norm(ub - lb)\n            unique = True\n            for _, xe in elite:\n                if np.linalg.norm(xcand - xe) < tol:\n                    unique = False\n                    break\n            if unique:\n                elite.append((float(fcand), xcand.copy()))\n                elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            improved = False\n\n            r = self.rng.random()\n\n            if r < 0.6:\n                # Primary: 1-D symmetric probe on a single coordinate, with optional parabolic interpolation\n                # choose coordinate by probabilities (soft)\n                if dim_probs.sum() <= 0:\n                    probs = np.ones(self.dim, dtype=float) / self.dim\n                else:\n                    probs = dim_probs / dim_probs.sum()\n                idx = int(self.rng.choice(self.dim, p=probs))\n                h = step\n                # try to evaluate symmetric points if budget allows both\n                pts = []\n                pts_coords = []\n                if remaining >= 2:\n                    x_minus = clip(x_best.copy())\n                    x_minus[idx] -= h\n                    x_plus = clip(x_best.copy())\n                    x_plus[idx] += h\n                    pts = [x_minus, x_plus]\n                    pts_coords = [-h, +h]\n                else:\n                    # only one eval left: sample the better side by a cheap heuristic (random)\n                    side = -1 if self.rng.random() < 0.5 else 1\n                    x_try = clip(x_best.copy())\n                    x_try[idx] += side * h\n                    pts = [x_try]\n                    pts_coords = [side * h]\n\n                # evaluate\n                local = []  # list of (delta, f, x)\n                for d, x_c in zip(pts_coords, pts):\n                    if evals >= self.budget:\n                        break\n                    fv = float(func(x_c))\n                    evals += 1\n                    local.append((d, fv, x_c.copy()))\n\n                # analyze\n                # if we have both sides, try parabolic interpolation for estimated minimum\n                if len(local) == 2:\n                    d1, f1, x1 = local[0]\n                    d2, f2, x2 = local[1]\n                    f0 = f_best\n                    # order: f(-h)=f1 maybe, f(+h)=f2 maybe depending on pts_coords\n                    # ensure d1 = -h, d2 = +h if possible\n                    if d1 > d2:\n                        # swap so d1=-h < d2=+h\n                        d1, d2, f1, f2, x1, x2 = d2, d1, f2, f1, x2, x1\n                    denom = (f1 - 2.0 * f0 + f2)\n                    if abs(denom) > 1e-12:\n                        # vertex offset t* h where t = (f1 - f2)/(2*(f1 - 2f0 + f2))\n                        t = (f1 - f2) / (2.0 * denom)\n                        # clamp t to [-2,2] to avoid runaway extrapolation\n                        t = float(np.clip(t, -2.0, 2.0))\n                        x_par = x_best.copy()\n                        x_par[idx] += t * h\n                        x_par = clip(x_par)\n                        if evals < self.budget and np.linalg.norm(x_par - x_best) > 1e-12:\n                            fpar = float(func(x_par))\n                            evals += 1\n                            if fpar < f_best:\n                                push_dir(x_par - x_best)\n                                x_best = x_par.copy()\n                                f_best = fpar\n                                improved = True\n                                success_counts[idx] += 1.0\n                            update_elite(x_par, fpar)\n                    # also accept direct side improvements\n                    for _, fv, xc in local:\n                        if fv < f_best:\n                            push_dir(xc - x_best)\n                            x_best = xc.copy()\n                            f_best = fv\n                            improved = True\n                            # increment success for this coordinate\n                            success_counts[idx] += 1.0\n                        update_elite(xc, fv)\n\n                else:\n                    # single evaluation case\n                    d, fv, xc = local[0]\n                    if fv < f_best:\n                        push_dir(xc - x_best)\n                        x_best = xc.copy()\n                        f_best = fv\n                        improved = True\n                        success_counts[idx] += 1.0\n                    update_elite(xc, fv)\n\n            elif r < 0.9:\n                # Secondary: 2-D rotated-plane scan but strictly low-rank: use at most two orthonormal vectors\n                # Construct small orthonormal basis: axis + one memory direction if available\n                # Choose primary axis at random but biased by spread among elites\n                if len(elite) >= 2:\n                    coords = np.vstack([e[1] for e in elite])\n                    spreads = np.std(coords, axis=0)\n                    if spreads.sum() > 0:\n                        probs = spreads + 1e-12\n                        probs = probs / probs.sum()\n                        idx = int(self.rng.choice(self.dim, p=probs))\n                    else:\n                        idx = int(self.rng.integers(self.dim))\n                else:\n                    idx = int(self.rng.integers(self.dim))\n                u = np.zeros(self.dim, dtype=float)\n                u[idx] = 1.0\n                # pick second direction: from dir_mem or random orthogonal\n                if dir_mem and self.rng.random() < 0.8:\n                    d = dir_mem[0]\n                    # make orthogonal to u\n                    proj = np.dot(u, d) * u\n                    v = d - proj\n                    nv = np.linalg.norm(v)\n                    if nv < 1e-12:\n                        # pick orthogonal axis (next index)\n                        v = np.zeros(self.dim)\n                        v[(idx + 1) % self.dim] = 1.0\n                        nv = 1.0\n                    v = v / nv\n                else:\n                    # random orthonormal vector orthogonal to u\n                    v = self.rng.normal(size=self.dim)\n                    v[idx] = 0.0\n                    nrm = np.linalg.norm(v)\n                    if nrm < 1e-12:\n                        v = np.zeros(self.dim); v[(idx + 1) % self.dim] = 1.0\n                    else:\n                        v = v / np.linalg.norm(v)\n\n                # sample a very small stencil on that 2D plane: center +/- u*step, center +/- v*(step*0.9)\n                cand_dirs = [u, -u, v, -v]\n                cand_pts = []\n                for dvec in cand_dirs:\n                    if evals >= self.budget:\n                        break\n                    xc = clip(x_best + step * dvec)\n                    cand_pts.append(xc)\n                # evaluate candidates but respect budget\n                best_local_f = f_best\n                best_local_x = x_best.copy()\n                for xc in cand_pts:\n                    if evals >= self.budget:\n                        break\n                    fv = float(func(xc))\n                    evals += 1\n                    update_elite(xc, fv)\n                    if fv < best_local_f:\n                        best_local_f = fv\n                        best_local_x = xc.copy()\n\n                if best_local_f < f_best:\n                    # accept and try a cautious combined step along direction (best_local_x - x_best)\n                    vstep = best_local_x - x_best\n                    push_dir(vstep)\n                    # attempt one extrapolation scaled by gamma\n                    gamma = 1.3\n                    if evals < self.budget:\n                        next_x = clip(x_best + gamma * vstep)\n                        if np.linalg.norm(next_x - best_local_x) > 1e-12:\n                            fnext = float(func(next_x))\n                            evals += 1\n                            update_elite(next_x, fnext)\n                            if fnext < best_local_f:\n                                x_best = next_x.copy()\n                                f_best = fnext\n                                improved = True\n                            else:\n                                x_best = best_local_x.copy()\n                                f_best = best_local_f\n                                improved = True\n                        else:\n                            x_best = best_local_x.copy()\n                            f_best = best_local_f\n                            improved = True\n\n            else:\n                # Tertiary: very low-degree recombination / global diversification\n                # Convex mix of two elites or pure opposition sample\n                if elite and len(elite) >= 2 and self.rng.random() < 0.85:\n                    ids = self.rng.choice(len(elite), size=2, replace=False)\n                    xa = elite[ids[0]][1]\n                    xb = elite[ids[1]][1]\n                    w = self.rng.random()\n                    x_new = clip(w * xa + (1 - w) * xb)\n                    # small gaussian jitter proportional to step\n                    jitter = self.rng.normal(scale=0.06 * max(step, 1e-12), size=self.dim)\n                    x_new = clip(x_new + jitter)\n                    if evals < self.budget:\n                        fnew = float(func(x_new))\n                        evals += 1\n                        update_elite(x_new, fnew)\n                        if fnew < f_best:\n                            push_dir(x_new - x_best)\n                            x_best = x_new.copy()\n                            f_best = fnew\n                            improved = True\n                else:\n                    # opposition-based candidate: reflect best in center + small jitter\n                    center = 0.5 * (lb + ub)\n                    x_opp = clip(center + (center - x_best) + self.rng.normal(scale=0.08 * max(step, 1e-12), size=self.dim))\n                    if evals < self.budget:\n                        fopp = float(func(x_opp))\n                        evals += 1\n                        update_elite(x_opp, fopp)\n                        if fopp < f_best:\n                            push_dir(x_opp - x_best)\n                            x_best = x_opp.copy()\n                            f_best = fopp\n                            improved = True\n\n            # update elites with current best\n            update_elite(x_best, f_best)\n\n            # adapt step sizes and per-dim probabilities\n            if improved:\n                step = min(self.max_step, step * 1.15)\n                stagn = 0\n                # boost dims that were successful\n                if success_counts.sum() > 0:\n                    # soft increase of probabilities where successes occurred\n                    alpha = 0.25\n                    dim_probs = (1 - alpha) * dim_probs + alpha * (success_counts + 1e-12)\n                # decay success counts slowly\n                success_counts *= 0.85\n            else:\n                step = max(self.min_step, step * 0.72)\n                stagn += 1\n                # softly decay per-dim bias to avoid lock-in\n                dim_probs = 0.97 * dim_probs + 0.03 * (np.ones(self.dim) / float(self.dim))\n\n            # occasional mirror/opposition quick check to escape symmetry (cheap)\n            if evals < self.budget and stagn >= 4 and self.rng.random() < 0.3:\n                x_mirror = clip(lb + ub - x_best)\n                f_mirror = float(func(x_mirror))\n                evals += 1\n                update_elite(x_mirror, f_mirror)\n                if f_mirror < f_best:\n                    push_dir(x_mirror - x_best)\n                    x_best = x_mirror.copy()\n                    f_best = f_mirror\n                    step = min(self.max_step, self.init_step)\n                    stagn = 0\n\n            # stronger diversification when stagnation persists but keep it low-degree\n            if stagn >= self.stagn_patience and evals < self.budget:\n                stagn = 0\n                # convex mix of elites + small jitter (cheap)\n                if len(elite) >= 2:\n                    k = min(len(elite), 3)\n                    ids = self.rng.choice(len(elite), size=k, replace=False)\n                    weights = self.rng.random(k)\n                    weights /= weights.sum()\n                    xm = np.zeros(self.dim)\n                    for w, i_e in zip(weights, ids):\n                        xm += w * elite[i_e][1]\n                    xm = clip(xm + self.rng.normal(scale=0.12 * max(step, 1e-12), size=self.dim))\n                    fxm = float(func(xm))\n                    evals += 1\n                    update_elite(xm, fxm)\n                    if fxm < f_best:\n                        push_dir(xm - x_best)\n                        x_best = xm.copy()\n                        f_best = fxm\n                        step = min(self.max_step, self.init_step)\n                        improved = True\n                # one near-global sample around random elite\n                if evals < self.budget:\n                    base = elite[self.rng.integers(len(elite))][1] if elite else x_best\n                    jump = 0.5 * (ub - lb)\n                    xj = clip(base + self.rng.normal(scale=1.0, size=self.dim) * jump)\n                    fj = float(func(xj))\n                    evals += 1\n                    update_elite(xj, fj)\n                    if fj < f_best:\n                        push_dir(xj - x_best)\n                        x_best = xj.copy()\n                        f_best = fj\n                        step = min(self.max_step, self.init_step)\n                        improved = True\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 10, "feedback": "The algorithm RotatedCoordinateExtrapolation scored 0.360 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a3b4a95-14a2-4906-a7fc-cf5f4ed2d5c6"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.28161400157625016}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.5725251318646971}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.6250194888367567}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.23793831652093544}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.4391560469739393}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.5948788657781676}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.049824641020667526}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.007647196823178826}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.028386025434572693}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.003961072864508974}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.040753777580450734}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.7748638106808412}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9093661163266991}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.8330875857774444}], "aucs": [0.28161400157625016, 0.5725251318646971, 0.6250194888367567, 0.23793831652093544, 0.4391560469739393, 0.5948788657781676, 0.049824641020667526, 0.007647196823178826, 0.028386025434572693, 0.003961072864508974, 0.040753777580450734, 4.999999999999449e-05, 0.7748638106808412, 0.9093661163266991, 0.8330875857774444]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3354.0, "Edges": 3353.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9994036970781157, "Degree Variance": 1.8425756730453655, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.941532258064516, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3388748869892715, "Depth Entropy": 2.1771710323279265, "Assortativity": 0.0, "Average Eccentricity": 17.546511627906977, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0002981514609421586, "Average Shortest Path": 11.043825152530303, "mean_complexity": 13.833333333333334, "total_complexity": 83.0, "mean_token_count": 478.3333333333333, "total_token_count": 2870.0, "mean_parameter_count": 3.0, "total_parameter_count": 18.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "c1235cdb-9ef7-4069-85ee-4be50b77d4ac", "fitness": 0.3665865937655939, "name": "DirectionalLowDegreeSearch", "description": "Directional Low-Degree Adaptive Search (DL-DAS) \u2014 combine cheap 1-D coordinate probes, single-step 1D extrapolations, and tiny 2-D low-degree line probes built from elite differences; aggressively keep operations to at-most single extrapolation/evaluation per improvement to reduce polynomial degree while adaptively growing/shrinking step sizes and using a tiny elite archive for safe diversification.", "code": "import numpy as np\n\nclass DirectionalLowDegreeSearch:\n    \"\"\"\n    Directional Low-Degree Adaptive Search (DL-DAS)\n\n    - Aim: keep all move primitives low-degree (mostly 1-D probes and single extra evaluations),\n      avoid multi-step extrapolation loops or wide-angle scans to reduce the algorithmic degree.\n    - Primitives:\n        * Coordinate central probes (\u00b1 step) with single controlled extrapolation if improvement.\n        * Low-degree 2-D probe using a single difference-of-elites direction and the coordinate axis:\n          evaluate at two symmetric offsets and accept a single-step extrapolation if helpful.\n        * Tiny elite archive and short direction memory to bias promising directions.\n        * Conservative adaptive step scaling on success/failure.\n    - Strict budget accounting: func calls are counted exactly and never exceeded.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_step=2.5, min_step=1e-6, max_step=2.5,\n                 elite_size=4, dir_memory=4, stagn_patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.elite_size = int(max(1, elite_size))\n        self.dir_memory = int(max(1, dir_memory))\n        self.stagn_patience = int(max(1, stagn_patience))\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        # default box [-5,5]^dim; if func supplies bounds try to use them\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shapes match dimension\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        # simple clip\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # small initial seeding, conservative\n        n_init = min(6, max(1, int(self.budget // 200 + 1)))\n        xs = []\n        fs = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            xs.append(x.copy())\n            fs.append(f)\n\n        if len(fs) == 0:\n            # no budget or impossible: return center\n            x0 = lb + 0.5 * (ub - lb)\n            return float(func(x0)), x0\n\n        # initialize best and elites\n        idx_best = int(np.argmin(fs))\n        x_best = xs[idx_best].copy()\n        f_best = float(fs[idx_best])\n\n        elite = sorted(zip(fs, xs), key=lambda t: t[0])[: self.elite_size]\n        elite = [(float(e[0]), e[1].copy()) for e in elite]\n\n        # direction memory (recent successful normalized vectors)\n        dir_mem = []\n\n        def push_dir(v):\n            nm = np.linalg.norm(v)\n            if nm > 1e-12:\n                u = v / nm\n                dir_mem.insert(0, u)\n                if len(dir_mem) > self.dir_memory:\n                    dir_mem.pop()\n\n        def update_elite(xcand, fcand):\n            nonlocal elite\n            # keep a compact sorted elite list\n            # insert only if unique enough\n            tol = 1e-8 + 1e-8 * np.linalg.norm(ub - lb)\n            unique = True\n            for _, xe in elite:\n                if np.linalg.norm(xcand - xe) < tol:\n                    unique = False\n                    break\n            if unique or len(elite) < self.elite_size:\n                elite.append((float(fcand), xcand.copy()))\n                elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n\n        step = float(self.init_step)\n        stagn = 0\n\n        # Main loop: choose low-degree actions\n        while evals < self.budget:\n            remaining = self.budget - evals\n            improved = False\n\n            r = self.rng.random()\n            # Action probabilities: coordinate probes are dominant, low-degree 2D probes second, tiny diversification rare\n            if r < 0.60 and remaining >= 1:\n                # Coordinate central probe (prefer dims with elite spread)\n                if len(elite) >= 2:\n                    coords = np.vstack([e[1] for e in elite])\n                    spreads = np.std(coords, axis=0)\n                    if spreads.sum() > 0:\n                        probs = spreads + 1e-12\n                        probs = probs / probs.sum()\n                        idx = int(self.rng.choice(self.dim, p=probs))\n                    else:\n                        idx = int(self.rng.integers(self.dim))\n                else:\n                    idx = int(self.rng.integers(self.dim))\n\n                offset = np.zeros(self.dim, dtype=float)\n                offset[idx] = step\n\n                # prefer central (both sides) if budget allows, else single side\n                if remaining >= 2:\n                    x_p = clip(x_best + offset)\n                    x_m = clip(x_best - offset)\n                    # evaluate both sides\n                    fc_p = float(func(x_p)); evals += 1\n                    fc_m = float(func(x_m)); evals += 1\n                    # choose best among center (known), p, m\n                    if fc_p < f_best or fc_m < f_best:\n                        # pick best side\n                        if fc_p <= fc_m:\n                            side_x, side_f, sign = x_p, fc_p, 1.0\n                        else:\n                            side_x, side_f, sign = x_m, fc_m, -1.0\n                        # accept side improvement\n                        x_best_old = x_best.copy()\n                        x_best = side_x.copy()\n                        f_best = float(side_f)\n                        improved = True\n                        push_dir(x_best - x_best_old)\n                        # single controlled extrapolation: one extra eval only\n                        if evals < self.budget:\n                            # produce a single extrapolated point but clamp magnitude\n                            extrap = 1.25 * sign * offset\n                            x_ex = clip(x_best + extrap)\n                            if np.linalg.norm(x_ex - x_best) > 1e-12:\n                                f_ex = float(func(x_ex)); evals += 1\n                                if f_ex < f_best:\n                                    push_dir(x_ex - x_best)\n                                    x_best = x_ex.copy()\n                                    f_best = float(f_ex)\n                    # else: no improvement\n                else:\n                    # only one side allowed (choose p)\n                    x_p = clip(x_best + offset)\n                    fc_p = float(func(x_p)); evals += 1\n                    if fc_p < f_best:\n                        x_best_old = x_best.copy()\n                        x_best = x_p.copy()\n                        f_best = float(fc_p)\n                        improved = True\n                        push_dir(x_best - x_best_old)\n                        # do NOT extrapolate if only one eval available (keep degree low)\n                update_elite(x_best, f_best)\n\n            elif r < 0.90 and remaining >= 2:\n                # Low-degree 2D probe: build a single direction from elite difference or dir_mem or random,\n                # then probe symmetric offsets along that direction combined with a coordinate axis.\n                # Keep evaluations to two per attempt + at most one extrapolation.\n                # Choose a coordinate axis\n                axis_idx = int(self.rng.integers(self.dim))\n                u = np.zeros(self.dim, dtype=float); u[axis_idx] = 1.0\n\n                # build a companion direction d (prefer elite differences, else dir_mem, else random)\n                if len(elite) >= 2 and self.rng.random() < 0.8:\n                    # pick two different elites\n                    ids = self.rng.choice(len(elite), size=2, replace=False)\n                    dvec = elite[ids[0]][1] - elite[ids[1]][1]\n                elif dir_mem and self.rng.random() < 0.7:\n                    dvec = dir_mem[0].copy()\n                else:\n                    dvec = self.rng.normal(size=self.dim)\n                # orthogonalize dvec to u to form a small plane basis\n                proj = u.dot(dvec) * u\n                v = dvec - proj\n                vn = np.linalg.norm(v)\n                if vn < 1e-12:\n                    # fallback: random orthogonal unit\n                    v = self.rng.normal(size=self.dim)\n                    v[axis_idx] = 0.0\n                    vn = np.linalg.norm(v)\n                    if vn < 1e-12:\n                        v = np.zeros(self.dim); v[(axis_idx+1) % self.dim] = 1.0\n                        vn = 1.0\n                v = v / vn\n\n                # two symmetric probes along combined axis+v direction (small angles)\n                alpha = 0.85  # small tilt to keep degree low and stable\n                dir1 = (np.sqrt(1 - alpha**2) * u) + (alpha * v)\n                dir2 = (np.sqrt(1 - alpha**2) * u) - (alpha * v)\n                x1 = clip(x_best + step * dir1)\n                x2 = clip(x_best + step * dir2)\n\n                f1 = float(func(x1)); evals += 1\n                # check budget\n                if evals < self.budget:\n                    f2 = float(func(x2)); evals += 1\n                else:\n                    f2 = f1 + 1.0  # sentinel worse\n\n                # accept best among them if better and do at most one single extrapolation\n                if f1 < f_best or f2 < f_best:\n                    if f1 <= f2:\n                        chosen_x, chosen_f, chosen_dir = x1, f1, dir1\n                    else:\n                        chosen_x, chosen_f, chosen_dir = x2, f2, dir2\n\n                    x_best_old = x_best.copy()\n                    x_best = chosen_x.copy()\n                    f_best = float(chosen_f)\n                    improved = True\n                    push_dir(x_best - x_best_old)\n                    # one extrapolation step\n                    if evals < self.budget:\n                        x_ex = clip(x_best + 1.2 * step * chosen_dir)\n                        if np.linalg.norm(x_ex - x_best) > 1e-12:\n                            f_ex = float(func(x_ex)); evals += 1\n                            if f_ex < f_best:\n                                push_dir(x_ex - x_best)\n                                x_best = x_ex.copy()\n                                f_best = float(f_ex)\n                    update_elite(x_best, f_best)\n                else:\n                    # no improvement -> record small negative feedback\n                    update_elite(x1, f1)\n                    update_elite(x2, f2)\n\n            else:\n                # Diversify: recombine elites or random sample (single eval)\n                if len(elite) >= 2 and self.rng.random() < 0.75:\n                    ids = self.rng.choice(len(elite), size=2, replace=False)\n                    xa = elite[ids[0]][1]\n                    xb = elite[ids[1]][1]\n                    w = self.rng.random()\n                    xnew = clip(w * xa + (1.0 - w) * xb + self.rng.normal(scale=0.04 * max(step, 1e-12), size=self.dim))\n                else:\n                    xnew = clip(lb + self.rng.random(self.dim) * (ub - lb))\n                if evals < self.budget:\n                    fnew = float(func(xnew)); evals += 1\n                    if fnew < f_best:\n                        push_dir(xnew - x_best)\n                        x_best = xnew.copy()\n                        f_best = float(fnew)\n                        improved = True\n                    update_elite(xnew, fnew)\n\n            # adapt step size conservatively to keep degree low/robust\n            if improved:\n                # modest growth\n                step = min(self.max_step, step * 1.12)\n                stagn = 0\n            else:\n                # shrink more aggressively on failures\n                step = max(self.min_step, step * 0.75)\n                stagn += 1\n\n            # occasional mirrored check (single eval) to escape symmetric traps\n            if evals < self.budget and stagn >= 5 and self.rng.random() < 0.3:\n                x_mirror = clip(lb + ub - x_best)\n                f_mirror = float(func(x_mirror)); evals += 1\n                if f_mirror < f_best:\n                    push_dir(x_mirror - x_best)\n                    x_best = x_mirror.copy()\n                    f_best = float(f_mirror)\n                    step = min(self.max_step, self.init_step)\n                    stagn = 0\n                    update_elite(x_best, f_best)\n                else:\n                    update_elite(x_mirror, f_mirror)\n\n            # stronger diversification if long stagnation (single or two evals)\n            if stagn >= self.stagn_patience and evals < self.budget:\n                # convex mix of elites (one eval)\n                if len(elite) >= 2:\n                    weights = self.rng.random(len(elite))\n                    weights /= weights.sum()\n                    mix = np.zeros(self.dim)\n                    for w, e in zip(weights, elite):\n                        mix += w * e[1]\n                    x_mix = clip(mix + self.rng.normal(scale=0.10 * max(step, 1e-12), size=self.dim))\n                    fmix = float(func(x_mix)); evals += 1\n                    if fmix < f_best:\n                        push_dir(x_mix - x_best)\n                        x_best = x_mix.copy()\n                        f_best = float(fmix)\n                        step = min(self.max_step, self.init_step)\n                        stagn = 0\n                        update_elite(x_best, f_best)\n                    else:\n                        update_elite(x_mix, fmix)\n                        # small reset of step to explore different scale\n                        step = max(self.min_step, 0.6 * self.init_step)\n                        stagn = 0\n                else:\n                    # random jump centered at best (one eval)\n                    jump_scale = 0.5 * (ub - lb)\n                    x_jump = clip(x_best + self.rng.normal(scale=1.0, size=self.dim) * jump_scale)\n                    fjump = float(func(x_jump)); evals += 1\n                    if fjump < f_best:\n                        push_dir(x_jump - x_best)\n                        x_best = x_jump.copy()\n                        f_best = float(fjump)\n                        step = min(self.max_step, self.init_step)\n                        stagn = 0\n                        update_elite(x_best, f_best)\n                    else:\n                        update_elite(x_jump, fjump)\n                        step = max(self.min_step, 0.6 * self.init_step)\n                        stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 11, "feedback": "The algorithm DirectionalLowDegreeSearch scored 0.367 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a3b4a95-14a2-4906-a7fc-cf5f4ed2d5c6"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.6570227599755296}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.4498136621389762}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.5479402275733822}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.2928814579456456}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.08994442486433074}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.13134327917868993}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.09123208203786437}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06480818054571191}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.08786272155683905}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07750184930222204}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0947517335329805}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9732107321023811}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9537609975226604}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.986674798206695}], "aucs": [0.6570227599755296, 0.4498136621389762, 0.5479402275733822, 0.2928814579456456, 0.08994442486433074, 0.13134327917868993, 0.09123208203786437, 0.06480818054571191, 4.999999999999449e-05, 0.08786272155683905, 0.07750184930222204, 0.0947517335329805, 0.9732107321023811, 0.9537609975226604, 0.986674798206695]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2784.0, "Edges": 2783.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.9992816091954022, "Degree Variance": 1.8678155758686745, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 9.000810372771475, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.334042297216243, "Depth Entropy": 2.208286640745148, "Assortativity": 0.0, "Average Eccentricity": 17.588721264367816, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00035919540229885057, "Average Shortest Path": 11.083637674964171, "mean_complexity": 11.166666666666666, "total_complexity": 67.0, "mean_token_count": 399.0, "total_token_count": 2394.0, "mean_parameter_count": 3.0, "total_parameter_count": 18.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "c59b4b63-97a8-4205-989c-5c1b43927983", "fitness": 0.26167701956359013, "name": "LowDegreeDirectionalEnsembleSearch", "description": "Low-Degree Directional Ensemble Search (LD-DES) \u2014 keep a small ensemble of simple linear probe directions, use cheap forward/back directional derivative probes and linear combined probes in 2D subspaces, adapt direction weights and a single step-size by success/failure, and diversify with small elite recombination and bounded random jumps. The method purposely avoids high-degree models (parabolas, high-order extrapolations), relying only on linear moves and sign/weighted directional information.", "code": "import numpy as np\n\nclass LowDegreeDirectionalEnsembleSearch:\n    \"\"\"\n    Low-Degree Directional Ensemble Search (LD-DES)\n\n    Key principles:\n      - Use only linear (degree-1) probes: forward/back directional samples, linear blends\n        of directions, convex mixes of elites + jitter, and symmetric mirror/random jumps.\n      - Maintain a small ensemble of normalized direction vectors with adaptive scores.\n      - Approximate directional derivative with two cheap probes (if budget allows) or one\n        probe, then move along the improving sign; accept only improving points.\n      - Combine two promising directions to probe simple 2D linear combinations (no parabolas).\n      - Adapt a single global step size by multiplicative growth/shrink based on success.\n      - Keep a tiny elite archive to form convex recombinations and to diversify when stagnating.\n      - Strictly respect the evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, init_step=1.0, min_step=1e-6, max_step=2.5,\n                 dir_count=8, elite_size=4, stagn_patience=10, rng=None):\n        \"\"\"\n        Parameters:\n          budget: maximum number of function evaluations\n          dim: dimensionality of problem\n          init_step: initial linear probe length (clamped by bounds)\n          min_step, max_step: step-size clamping\n          dir_count: number of directions in the ensemble\n          elite_size: how many best points to keep\n          stagn_patience: iterations without improvement before stronger diversification\n          rng: numpy.random.Generator (optional)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.dir_count = int(dir_count)\n        self.elite_size = int(elite_size)\n        self.stagn_patience = int(stagn_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure consistent shapes\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n\n        # initial seeding\n        n_init = min(8, max(1, self.budget // 40))\n        xs = []\n        fs = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            xs.append(x.copy())\n            fs.append(float(f))\n\n        if len(fs) == 0:\n            # no budget: evaluate center once and return\n            x0 = lb + 0.5 * (ub - lb)\n            return float(func(x0)), x0\n\n        # best so far\n        idx_best = int(np.argmin(fs))\n        x_best = xs[idx_best].copy()\n        f_best = float(fs[idx_best])\n\n        # elite archive: list of tuples (f, x)\n        elite = sorted(zip(fs, xs), key=lambda t: t[0])[: self.elite_size]\n        elite = [(float(t[0]), t[1].copy()) for t in elite]\n\n        # direction ensemble initialization (mix coordinate axes and random)\n        dirs = np.zeros((self.dir_count, self.dim), dtype=float)\n        # first few are coordinate axes if dim permits\n        for i in range(min(self.dir_count, self.dim)):\n            v = np.zeros(self.dim)\n            v[i] = 1.0\n            dirs[i] = v\n        # fill remaining with random normalized vectors\n        for i in range(min(self.dir_count, self.dim), self.dir_count):\n            v = self.rng.normal(size=self.dim)\n            norm = np.linalg.norm(v)\n            if norm < 1e-12:\n                v = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                v = v / norm\n            dirs[i] = v\n\n        # ensure directions are normalized nonzero\n        for i in range(self.dir_count):\n            nrm = np.linalg.norm(dirs[i])\n            if nrm < 1e-12:\n                dirs[i] = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                dirs[i] = dirs[i] / nrm\n\n        # scores to prefer productive directions\n        dir_scores = np.ones(self.dir_count, dtype=float)\n\n        step = float(self.init_step)\n        stagn = 0\n        iters = 0\n\n        # small helpers\n        def choose_dir_idx():\n            # sample index based on scores with softmax-ish weighting\n            scores = dir_scores - dir_scores.min() + 1e-12\n            probs = scores / scores.sum()\n            return int(self.rng.choice(self.dir_count, p=probs))\n\n        def orthonormalize_pool():\n            # light Gram-Schmidt to keep directions diverse\n            mat = dirs.copy()\n            for i in range(mat.shape[0]):\n                v = mat[i].copy()\n                for j in range(i):\n                    proj = np.dot(mat[j], v) * mat[j]\n                    v = v - proj\n                nrm = np.linalg.norm(v)\n                if nrm < 1e-12:\n                    # replace by random\n                    v = self.rng.normal(size=self.dim)\n                    nrm = np.linalg.norm(v)\n                    if nrm < 1e-12:\n                        v = np.ones(self.dim) / np.sqrt(self.dim)\n                        nrm = np.linalg.norm(v)\n                mat[i] = v / nrm\n            return mat\n\n        # main loop: respect budget strictly\n        while evals < self.budget:\n            remaining = self.budget - evals\n            improved = False\n            iters += 1\n\n            # decide action: directional probe (most common), 2D blend probe, or recombination/diversify\n            r = self.rng.random()\n            if r < 0.60:\n                # directional derivative probe (prefer directions with higher score)\n                idx = choose_dir_idx()\n                d = dirs[idx]\n                # scale step so it doesn't exceed box width\n                local_step = step * np.minimum(1.0, np.min((ub - lb) / (np.abs(d) + 1e-12)))\n                # ensure some minimal positive scalar\n                local_step = max(local_step, 1e-12)\n\n                # if at least 2 evals remain, probe both sides to estimate derivative\n                if remaining >= 2:\n                    x_plus = clip(x_best + local_step * d)\n                    x_minus = clip(x_best - local_step * d)\n                    f_plus = float(func(x_plus)); evals += 1\n                    # check budget again\n                    if evals < self.budget:\n                        f_minus = float(func(x_minus)); evals += 1\n                    else:\n                        f_minus = None\n                else:\n                    # only one probe left: evaluate one side (randomly choose sign)\n                    sign = 1 if self.rng.random() < 0.5 else -1\n                    x_probe = clip(x_best + sign * local_step * d)\n                    f_probe = float(func(x_probe)); evals += 1\n                    # emulate one-sided info\n                    if sign > 0:\n                        f_plus = f_probe; f_minus = None\n                    else:\n                        f_minus = f_probe; f_plus = None\n\n                # decide movement based on probes\n                candidate_x = None\n                candidate_f = None\n                # prefer the strictly better of probes\n                if f_plus is not None and f_minus is not None:\n                    if f_plus < f_minus and f_plus < f_best:\n                        candidate_x = clip(x_best + local_step * d)\n                        candidate_f = f_plus\n                    elif f_minus < f_plus and f_minus < f_best:\n                        candidate_x = clip(x_best - local_step * d)\n                        candidate_f = f_minus\n                    else:\n                        # if neither probe is better but difference suggests a downhill slope,\n                        # we try a small linear move along the downhill sign (one more eval if budget allows)\n                        if f_plus < f_minus and remaining >= 3:\n                            # downhill in +d, try small further step\n                            next_x = clip(x_best + 1.3 * local_step * d)\n                            if evals < self.budget:\n                                nf = float(func(next_x)); evals += 1\n                                if nf < f_best:\n                                    candidate_x = next_x; candidate_f = nf\n                        elif f_minus < f_plus and remaining >= 3:\n                            next_x = clip(x_best - 1.3 * local_step * d)\n                            if evals < self.budget:\n                                nf = float(func(next_x)); evals += 1\n                                if nf < f_best:\n                                    candidate_x = next_x; candidate_f = nf\n                else:\n                    # only one-sided probe available\n                    f_one = f_plus if f_plus is not None else f_minus\n                    sign = 1 if f_plus is not None else -1\n                    if f_one < f_best:\n                        candidate_x = clip(x_best + sign * local_step * d)\n                        candidate_f = f_one\n                    # otherwise try small extra step if budget allows\n                    elif remaining >= 1 and evals < self.budget and abs(f_one - f_best) > 1e-12:\n                        # if one-sided suggests downhill, attempt next small step\n                        if f_one < f_best:\n                            next_x = clip(x_best + 1.2 * sign * local_step * d)\n                            nf = float(func(next_x)); evals += 1\n                            if nf < f_best:\n                                candidate_x = next_x; candidate_f = nf\n\n                # accept candidate if it improves\n                if candidate_x is not None and candidate_f is not None and candidate_f < f_best:\n                    push_vec = candidate_x - x_best\n                    # update best\n                    x_best = candidate_x.copy()\n                    f_best = float(candidate_f)\n                    improved = True\n                    stagn = 0\n                    # boost chosen direction score and incorporate push into the pool\n                    dir_scores[idx] = dir_scores[idx] * 1.25 + 0.1\n                    # optionally replace a low-score direction with this push (promote good linear moves)\n                    if np.linalg.norm(push_vec) > 1e-12:\n                        u = push_vec / np.linalg.norm(push_vec)\n                        # find worst direction index and replace with this normalized vector sometimes\n                        worst_idx = int(np.argmin(dir_scores))\n                        if self.rng.random() < 0.35:\n                            dirs[worst_idx] = u\n                            dir_scores[worst_idx] = max(dir_scores) * 0.9 + 0.1\n                else:\n                    # no improvement from this direction probe\n                    dir_scores[idx] = max(1e-6, dir_scores[idx] * 0.92)\n                    stagn += 1\n\n            elif r < 0.85:\n                # 2D linear combined probes: pick two directions (prefer high scores)\n                if self.dir_count >= 2:\n                    idxs = self.rng.choice(self.dir_count, size=2, replace=False,\n                                           p=(dir_scores - dir_scores.min() + 1e-12) / (dir_scores - dir_scores.min() + 1e-12).sum())\n                    u = dirs[idxs[0]]\n                    v = dirs[idxs[1]]\n                else:\n                    idxs = [choose_dir_idx(), choose_dir_idx()]\n                    u = dirs[idxs[0]]\n                    v = dirs[idxs[1]]\n\n                # generate simple linear combos (only coefficients in {-1, 0, 1}) to remain low-degree\n                combos = [(1.0, 0.0), (-1.0, 0.0), (0.0, 1.0), (0.0, -1.0), (0.7, 0.7), (-0.7, 0.7), (0.7, -0.7)]\n                cand_xs = []\n                for a, b in combos:\n                    direction = a * u + b * v\n                    nrm = np.linalg.norm(direction)\n                    if nrm < 1e-12:\n                        continue\n                    direction = direction / nrm\n                    x_c = clip(x_best + step * direction)\n                    cand_xs.append(x_c)\n\n                best_local_x = None\n                best_local_f = f_best\n                for xc in cand_xs:\n                    if evals >= self.budget:\n                        break\n                    fc = float(func(xc)); evals += 1\n                    if fc < best_local_f:\n                        best_local_f = fc\n                        best_local_x = xc.copy()\n\n                if best_local_x is not None:\n                    # accept best from the small linear 2D scan\n                    push_vec = best_local_x - x_best\n                    x_best = best_local_x.copy()\n                    f_best = float(best_local_f)\n                    improved = True\n                    stagn = 0\n                    # increase scores of participating directions a bit\n                    for idx in idxs:\n                        dir_scores[idx] = dir_scores[idx] * 1.15 + 0.05\n                    # occasionally replace worst direction with the move direction\n                    if np.linalg.norm(push_vec) > 1e-12 and self.rng.random() < 0.3:\n                        worst = int(np.argmin(dir_scores))\n                        dirs[worst] = push_vec / np.linalg.norm(push_vec)\n                        dir_scores[worst] = max(dir_scores) * 0.6 + 0.1\n                else:\n                    # degrade scores slightly if no improvement\n                    for idx in idxs:\n                        dir_scores[idx] *= 0.94\n                    stagn += 1\n\n            else:\n                # recombination / diversification\n                if len(elite) >= 2 and self.rng.random() < 0.8:\n                    # convex mix of two elites plus small jitter proportional to step\n                    ids = self.rng.choice(len(elite), size=2, replace=False)\n                    w = self.rng.random()\n                    x_new = w * elite[ids[0]][1] + (1 - w) * elite[ids[1]][1]\n                    jitter = self.rng.normal(scale=0.08 * max(step, 1e-12), size=self.dim)\n                    x_new = clip(x_new + jitter)\n                    if evals < self.budget:\n                        fnew = float(func(x_new)); evals += 1\n                        if fnew < f_best:\n                            push_vec = x_new - x_best\n                            x_best = x_new.copy()\n                            f_best = float(fnew)\n                            improved = True\n                            stagn = 0\n                            # add push to a random low-score direction\n                            worst = int(np.argmin(dir_scores))\n                            if np.linalg.norm(push_vec) > 1e-12:\n                                dirs[worst] = push_vec / np.linalg.norm(push_vec)\n                                dir_scores[worst] = max(dir_scores) * 0.8 + 0.1\n                        # always consider adding to elite\n                        elite.append((float(fnew), x_new.copy()))\n                        elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n                else:\n                    # pure random local perturbation around best (small)\n                    scale = 0.12 * max(step, 1e-12)\n                    x_new = clip(x_best + self.rng.normal(scale=scale, size=self.dim))\n                    if evals < self.budget:\n                        fnew = float(func(x_new)); evals += 1\n                        if fnew < f_best:\n                            push_vec = x_new - x_best\n                            x_best = x_new.copy()\n                            f_best = float(fnew)\n                            improved = True\n                            stagn = 0\n                            # seed a direction with the push\n                            worst = int(np.argmin(dir_scores))\n                            if np.linalg.norm(push_vec) > 1e-12:\n                                dirs[worst] = push_vec / np.linalg.norm(push_vec)\n                                dir_scores[worst] = max(dir_scores) * 0.9 + 0.05\n                        elite.append((float(fnew), x_new.copy()))\n                        elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n\n            # update elite with current best occasionally\n            if evals < self.budget:\n                # keep small archive\n                elite.append((float(f_best), x_best.copy()))\n                elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n\n            # adapt global step\n            if improved:\n                step = min(self.max_step, step * 1.18)\n                stagn = 0\n            else:\n                step = max(self.min_step, step * 0.75)\n\n            # occasionally orthonormalize to reduce redundancy\n            if iters % 12 == 0:\n                dirs = orthonormalize_pool()\n\n            # mirror test to escape symmetric traps (a linear reflection)\n            if stagn >= 4 and evals < self.budget and self.rng.random() < 0.3:\n                x_mirror = clip(lb + ub - x_best)\n                fm = float(func(x_mirror)); evals += 1\n                if fm < f_best:\n                    x_best = x_mirror.copy()\n                    f_best = float(fm)\n                    improved = True\n                    step = min(self.max_step, self.init_step)\n                    stagn = 0\n                    # seed direction from mirror move\n                    worst = int(np.argmin(dir_scores))\n                    push = x_mirror - x_best\n                    if np.linalg.norm(push) > 1e-12:\n                        dirs[worst] = push / np.linalg.norm(push)\n                        dir_scores[worst] = max(dir_scores) * 0.9 + 0.05\n\n            # stronger diversification after long stagnation\n            if stagn >= self.stagn_patience and evals < self.budget:\n                # convex mix of elites\n                if len(elite) >= 2:\n                    weights = self.rng.random(len(elite))\n                    weights = weights / weights.sum()\n                    mix = np.zeros(self.dim)\n                    for w, e in zip(weights, elite):\n                        mix += w * e[1]\n                    x_mix = clip(mix + self.rng.normal(scale=0.18 * max(step, 1e-12), size=self.dim))\n                    if evals < self.budget:\n                        fmix = float(func(x_mix)); evals += 1\n                        if fmix < f_best:\n                            x_best = x_mix.copy()\n                            f_best = float(fmix)\n                            improved = True\n                            stagn = 0\n                            step = min(self.max_step, self.init_step)\n                            # seed direction\n                            worst = int(np.argmin(dir_scores))\n                            push = x_best - mix\n                            if np.linalg.norm(push) > 1e-12:\n                                dirs[worst] = push / np.linalg.norm(push)\n                                dir_scores[worst] = max(dir_scores) * 0.9 + 0.05\n                # a near-random jump centered at best (bounded by box)\n                if evals < self.budget:\n                    jump_scale = 0.6 * (ub - lb)\n                    x_jump = clip(x_best + self.rng.normal(scale=1.0, size=self.dim) * jump_scale)\n                    fjump = float(func(x_jump)); evals += 1\n                    if fjump < f_best:\n                        x_best = x_jump.copy()\n                        f_best = float(fjump)\n                        improved = True\n                        stagn = 0\n                        step = min(self.max_step, self.init_step)\n                        worst = int(np.argmin(dir_scores))\n                        push = x_jump - x_best\n                        if np.linalg.norm(push) > 1e-12:\n                            dirs[worst] = push / np.linalg.norm(push)\n                            dir_scores[worst] = max(dir_scores) * 0.9 + 0.05\n                    else:\n                        # lower step to change search scale and reset stagn\n                        step = max(self.min_step, 0.5 * self.init_step)\n                        stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 11, "feedback": "The algorithm LowDegreeDirectionalEnsembleSearch scored 0.262 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a3b4a95-14a2-4906-a7fc-cf5f4ed2d5c6"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9265490212831402}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9209153047440719}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9053179187945346}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.025067409671844287}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.01914495736676125}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.7056771746066566}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.10902591602111056}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.3131075909657327}], "aucs": [0.9265490212831402, 0.9209153047440719, 0.9053179187945346, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.025067409671844287, 4.999999999999449e-05, 0.01914495736676125, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.7056771746066566, 0.10902591602111056, 0.3131075909657327]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3693.0, "Edges": 3692.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9994584348767939, "Degree Variance": 1.7698345293446396, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 9.369591346153847, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3314786200682547, "Depth Entropy": 2.308941374424303, "Assortativity": 2.2144601026461808e-08, "Average Eccentricity": 20.18846466287571, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.00027078256160303275, "Average Shortest Path": 11.80740612308901, "mean_complexity": 19.2, "total_complexity": 96.0, "mean_token_count": 641.8, "total_token_count": 3209.0, "mean_parameter_count": 2.8, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "26270207-daab-4f0e-99f2-41a122778677", "fitness": 0.21773314935842283, "name": "LEDS", "description": "Linear Ensemble Directional Search (LEDS) \u2014 keep a compact elite set, perform many cheap low-degree linear operations (1-D directional probes, midpoints/extrapolations), pick the most promising candidates by linear prediction, and adapt simple multiplicative step sizes with occasional Cauchy escapes for diversification.", "code": "import numpy as np\n\nclass LEDS:\n    \"\"\"\n    Linear Ensemble Directional Search (LEDS)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    Key ideas:\n    - Maintain a small elite archive of best samples.\n    - Perform low-degree moves only: 1-D directional probes, midpoints, linear extrapolations.\n    - Rank many cheap virtual candidates by linear interpolation of elites' values,\n      evaluate only top-ranked candidates to save budget.\n    - Adapt a single multiplicative step-size by a simple success EMA.\n    - Occasional Cauchy (heavy-tailed) jumps to escape basins.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_samples=0.12, elite_size=None,\n                 step_init=None, jump_prob=0.04,\n                 target_success=0.2, ema_alpha=0.12,\n                 stagnation_restart=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # fractions and sizes\n        self.init_samples = float(init_samples)\n        if elite_size is None:\n            self.elite_size = max(4, min(12, int(2 + dim//2)))\n        else:\n            self.elite_size = int(elite_size)\n\n        # step-size parameters (single scalar, low-degree control)\n        self.step_init = None if step_init is None else float(step_init)\n        self.jump_prob = float(jump_prob)\n        self.target_success = float(target_success)\n        self.ema_alpha = float(ema_alpha)\n\n        # stagnation\n        self.stagnation_restart = int(stagnation_restart)\n\n    def _cauchy_dir(self, size):\n        z = self.rng.standard_cauchy(size=size).astype(float)\n        # sanitize\n        z = np.where(np.isfinite(z), z, self.rng.standard_normal(size=size))\n        nrm = np.linalg.norm(z)\n        if nrm == 0:\n            z = self.rng.standard_normal(size=size)\n            nrm = np.linalg.norm(z)\n        return z / (nrm + 1e-12)\n\n    def __call__(self, func):\n        # bounds must be available on func.bounds.lb/ub\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        evals = 0\n        # initial sampling budget\n        n_init = max(4, int(min(max(8, 2*dim), max(1, int(budget * self.init_samples)))))\n        n_init = min(n_init, budget)\n\n        elites = []  # list of (f, x)\n\n        # initial uniform sampling\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            elites.append((f, x.copy()))\n            if evals >= budget:\n                break\n\n        # sort elites\n        elites.sort(key=lambda t: t[0])\n        elites = elites[:self.elite_size]\n\n        # ensure we have at least one best\n        if len(elites) == 0:\n            # improbable but just in case\n            x0 = rng.uniform(lb, ub)\n            f0 = float(func(x0)); evals += 1\n            elites = [(f0, x0.copy())]\n\n        f_best, x_best = elites[0][0], elites[0][1].copy()\n\n        # range scale for step sizing\n        range_vec = ub - lb\n        range_scale = float(np.mean(range_vec))\n\n        # initial step size\n        if self.step_init is None:\n            step = max(1e-8, 0.15 * range_scale)\n        else:\n            step = float(self.step_init)\n\n        # success EMA for step adaptation\n        p_succ = 0.0\n\n        since_improvement = 0\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n\n            # Strategy selection: mix of directional probes and ensemble recombinations\n            # Use small batches to remain responsive.\n            batch = min( max(1, 6 + dim//3), remaining )\n\n            # Build inexpensive virtual candidates using linear combos of elites:\n            # midpoints and extrapolations. Use cheap linear prediction to rank them.\n            virtuals = []\n            n_el = len(elites)\n            # if too few elites, include random directions\n            if n_el >= 2:\n                # generate pairs up to some limit\n                max_pairs = min(30, n_el*(n_el-1)//2)\n                pairs = []\n                # sample some pairs (prefer best ones)\n                # deterministic mixing: pair best with random others and some random pairs\n                pairs.append((0, min(1, n_el-1)))\n                for _ in range(max_pairs-1):\n                    i = rng.integers(0, n_el)\n                    j = rng.integers(0, n_el)\n                    if i != j:\n                        pairs.append((i,j))\n                # create virtuals\n                for (i,j) in pairs:\n                    fi, xi = elites[i]\n                    fj, xj = elites[j]\n                    # midpoint\n                    xm = 0.5*(xi + xj)\n                    pred_m = 0.5*(fi + fj)  # linear prediction (very low-degree)\n                    virtuals.append((pred_m, xm, 'mid', i, j))\n                    # extrapolation from better towards worse\n                    if fi <= fj:\n                        base, other = xi, xj\n                        fbase, fother = fi, fj\n                    else:\n                        base, other = xj, xi\n                        fbase, fother = fj, fi\n                    # small extrapolation factors\n                    for alpha in (0.5, 1.0, 1.5):\n                        xe = base + alpha * (base - other)\n                        # predict extrap f by linear model: fbase + alpha*(fbase - fother)\n                        pred_e = fbase + alpha*(fbase - fother)\n                        virtuals.append((pred_e, xe, 'ext', i, j))\n            else:\n                # few elites: propose a couple of random midpoints / perturbations\n                for _ in range(8):\n                    xc = rng.uniform(lb, ub)\n                    virtuals.append((0.0, xc, 'rand', -1, -1))\n\n            # Add some directional probes around the best (1-D low-degree probes)\n            # Build a small orthogonal directional set using differences of elites and random vectors.\n            directions = []\n            # differences\n            for k in range(1, min(1 + n_el//2, n_el)):\n                vec = elites[k][1] - x_best\n                if np.linalg.norm(vec) > 1e-12:\n                    directions.append(vec / (np.linalg.norm(vec) + 1e-12))\n            # add some random orthonormal directions:\n            while len(directions) < min(4, dim):\n                v = rng.normal(size=dim)\n                # orthogonalize to existing directions (Gram-Schmidt)\n                for d in directions:\n                    v = v - np.dot(v, d) * d\n                nrm = np.linalg.norm(v)\n                if nrm > 1e-12:\n                    directions.append(v / nrm)\n                else:\n                    # fallback\n                    directions.append(self._cauchy_dir(dim))\n\n            # For each direction propose positive/negative step\n            for d in directions:\n                xp = x_best + step * d\n                xm = x_best - step * d\n                # clip / reflect to keep within bounds (simple clipping is fine for low-degree)\n                xp = np.clip(xp, lb, ub)\n                xm = np.clip(xm, lb, ub)\n                # predict using linear extrapolation: assume local slope ~0, use best value as baseline\n                virtuals.append((f_best, xp, 'dirp', -1, -1))\n                virtuals.append((f_best, xm, 'dirn', -1, -1))\n\n            # Filter/clean virtuals and rank by predicted value (lower better).\n            # Also remove duplicates (within small tol)\n            cand = []\n            seen = []\n            tol = 1e-9\n            for pred, x_v, tag, i, j in virtuals:\n                x_v = np.clip(np.asarray(x_v, dtype=float), lb, ub)\n                key = tuple(np.round(x_v, 9))  # cheap hashing\n                if key in seen:\n                    continue\n                seen.append(key)\n                cand.append((pred, x_v, tag, i, j))\n            if len(cand) == 0:\n                break\n\n            # Sort by predicted value and choose top_k to actually evaluate\n            cand.sort(key=lambda t: t[0])\n            top_k = min(max(1, batch), len(cand))\n            chosen = cand[:top_k]\n\n            # Evaluate chosen candidates, but do not exceed budget\n            for _, x_c, tag, i, j in chosen:\n                if evals >= budget:\n                    break\n                # occasional heavy-tailed perturbation applied to candidate (rare)\n                if rng.random() < self.jump_prob:\n                    # small Cauchy perturbation scaled by range\n                    d = self._cauchy_dir(dim) * (0.5 * range_scale)\n                    x_eval = np.clip(x_c + d, lb, ub)\n                else:\n                    x_eval = x_c\n                f_eval = float(func(x_eval))\n                evals += 1\n\n                improved = False\n                if f_eval < f_best - 1e-15:\n                    improved = True\n                    f_best = f_eval\n                    x_best = x_eval.copy()\n                    since_improvement = 0\n                else:\n                    since_improvement += 1\n\n                # Maintain elite archive: insert if good\n                inserted = False\n                if len(elites) < self.elite_size or f_eval < elites[-1][0]:\n                    elites.append((f_eval, x_eval.copy()))\n                    elites.sort(key=lambda t: t[0])\n                    elites = elites[:self.elite_size]\n                    inserted = True\n\n                # If this was a directional successful step, we can try a small extrapolation immediately (1-D low-degree)\n                if tag in ('dirp', 'dirn') and improved and evals < budget:\n                    # take an extra step of same sign\n                    dvec = x_eval - x_best\n                    if np.linalg.norm(dvec) > 1e-12:\n                        ddir = dvec / (np.linalg.norm(dvec) + 1e-12)\n                        x_extra = np.clip(x_eval + 0.6 * step * ddir, lb, ub)\n                        f_extra = float(func(x_extra))\n                        evals += 1\n                        if f_extra < f_best:\n                            f_best = f_extra\n                            x_best = x_extra.copy()\n                            # update elite\n                            elites.append((f_extra, x_extra.copy()))\n                            elites.sort(key=lambda t: t[0]); elites = elites[:self.elite_size]\n                            since_improvement = 0\n\n                # update step adaptation (simple EMA)\n                p_succ = (1.0 - self.ema_alpha) * p_succ + self.ema_alpha * (1.0 if improved else 0.0)\n                # multiplicative adjustment, clipped\n                factor = np.exp(0.7 * (p_succ - self.target_success) / max(1e-8, np.sqrt(dim)))\n                factor = float(np.clip(factor, 0.7, 1.5))\n                step = float(np.clip(step * factor, 1e-9 * range_scale + 1e-12, 3.0 * range_scale + 1e-12))\n\n                # early stop if budget used\n                if evals >= budget:\n                    break\n\n            # After the small batch, if no recent improvement, do targeted randomization\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # Try a small number of focused random diversifications\n                pushes = min(8, budget - evals)\n                for _ in range(pushes):\n                    # choose either a uniform restart or sample around a random elite\n                    if rng.random() < 0.5:\n                        center = rng.uniform(lb, ub)\n                    else:\n                        idx = rng.integers(0, len(elites))\n                        center = elites[idx][1]\n                    # moderate Cauchy or Gaussian perturbation\n                    if rng.random() < 0.5:\n                        x_try = np.clip(center + 2.0 * step * self._cauchy_dir(dim), lb, ub)\n                    else:\n                        x_try = np.clip(center + 1.2 * step * rng.normal(size=dim), lb, ub)\n                    f_try = float(func(x_try))\n                    evals += 1\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                        elites.append((f_try, x_try.copy()))\n                        elites.sort(key=lambda t: t[0]); elites = elites[:self.elite_size]\n                        # contract step a bit to refine\n                        step = max(step * 0.8, 1e-9 * range_scale)\n                        since_improvement = 0\n                        break\n                    if evals >= budget:\n                        break\n                # if still stuck, broaden search: bump step and increase jump prob temporarily\n                if since_improvement >= self.stagnation_restart:\n                    step = min(step * 2.0, 6.0 * range_scale)\n                    # increase jump prob but keep it modest\n                    self.jump_prob = min(0.35, self.jump_prob * 1.6)\n                    # also inject a random elite to diversify\n                    x_r = rng.uniform(lb, ub)\n                    f_r = float(func(x_r)); evals += 1\n                    elites.append((f_r, x_r.copy())); elites.sort(key=lambda t: t[0]); elites = elites[:self.elite_size]\n                    if f_r < f_best:\n                        f_best = f_r; x_best = x_r.copy(); since_improvement = 0\n\n            # tighten elite set occasionally (prune duplicates / very close points)\n            # combine near-identical elites\n            if len(elites) > 1:\n                new_elites = []\n                used = [False]*len(elites)\n                for i,(fi,xi) in enumerate(elites):\n                    if used[i]:\n                        continue\n                    # merge close ones\n                    merged_x = xi.copy()\n                    merged_f = fi\n                    count = 1\n                    used[i] = True\n                    for j in range(i+1, len(elites)):\n                        if used[j]:\n                            continue\n                        fj, xj = elites[j]\n                        if np.linalg.norm(xj - xi) < 1e-6 * (1.0 + np.linalg.norm(range_vec)):\n                            # keep better f\n                            merged_f = min(merged_f, fj)\n                            merged_x = merged_x if merged_f == fi else xj.copy()\n                            used[j] = True\n                            count += 1\n                    new_elites.append((merged_f, merged_x))\n                new_elites.sort(key=lambda t: t[0])\n                elites = new_elites[:self.elite_size]\n\n            # small safety: ensure best is consistent with elites\n            if elites and elites[0][0] < f_best - 1e-15:\n                f_best, x_best = elites[0][0], elites[0][1].copy()\n\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 11, "feedback": "The algorithm LEDS scored 0.218 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.17256410083736318}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.15971001717579003}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.14208631200895583}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.004450155781887788}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02377052528574297}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.00875589312713787}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.014484822132331487}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.008007232162077882}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0002916012661514644}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9145738431118914}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8844431165013888}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9327096209856239}], "aucs": [0.17256410083736318, 0.15971001717579003, 0.14208631200895583, 4.999999999999449e-05, 0.004450155781887788, 4.999999999999449e-05, 0.02377052528574297, 0.00875589312713787, 0.014484822132331487, 4.999999999999449e-05, 0.008007232162077882, 0.0002916012661514644, 0.9145738431118914, 0.8844431165013888, 0.9327096209856239]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2648.0, "Edges": 2647.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.9992447129909365, "Degree Variance": 2.0422955020490865, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.415767634854772, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3412231691815022, "Depth Entropy": 2.1003285930599485, "Assortativity": 1.0143130760244223e-08, "Average Eccentricity": 17.423716012084594, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00037764350453172205, "Average Shortest Path": 10.415726291064273, "mean_complexity": 20.333333333333332, "total_complexity": 61.0, "mean_token_count": 772.3333333333334, "total_token_count": 2317.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "6b6b7c94-f950-4bb0-b571-ad51e89fadcd", "fitness": "-inf", "name": "SDPAS", "description": "Separable Directional Pairwise Averaging Search (SDPAS) \u2014 a low-degree, linear-operation heuristic that mixes adaptive 1-D coordinate probes with pairwise midpoints and small linear extrapolations among a tiny elite set; per-dimension step-sizes adapt by success/failure and simple directional averages guide occasional multi-dimensional moves.", "code": "import numpy as np\n\nclass SDPAS:\n    \"\"\"\n    Separable Directional Pairwise Averaging Search (SDPAS)\n\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n    Optional tuning parameters can be passed as keyword arguments.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, rng=None,\n                 init_samples=None, elite_size=6,\n                 p_coord=0.62, p_pair=0.33, p_avg=0.05,\n                 sigma0=0.6, sigma_min=1e-6, sigma_max=5.0,\n                 success_inc=1.20, fail_dec=0.72,\n                 stagnation_restart=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng() if rng is None else rng\n\n        # initialization controls\n        self.init_samples = init_samples\n        self.elite_size = int(elite_size)\n\n        # operation probabilities (sum should be ~1.0)\n        self.p_coord = float(p_coord)\n        self.p_pair = float(p_pair)\n        self.p_avg = float(p_avg)\n\n        # per-dimension step sizes and clamps\n        self.sigma0 = float(sigma0)\n        self.sigma_min = float(sigma_min)\n        self.sigma_max = float(sigma_max)\n\n        # multiplicative adjustment on success/failure (per-dim)\n        self.success_inc = float(success_inc)\n        self.fail_dec = float(fail_dec)\n\n        # stagnation restart threshold (evals without improvement)\n        self.stagnation_restart = int(stagnation_restart)\n\n    def __call__(self, func):\n        # fixed bounds per problem statement\n        lb = -5.0\n        ub = 5.0\n        dim = self.dim\n        rng = self.rng\n        budget = self.budget\n\n        # bookkeeping\n        evals = 0\n        f_opt = float(\"inf\")\n        x_opt = np.zeros(dim, dtype=float)\n\n        # initial sample count\n        if self.init_samples is None:\n            n_init = min(budget, max(10, 2 * dim))\n        else:\n            n_init = min(budget, max(1, int(self.init_samples)))\n\n        # elites: list of tuples (f, x); keep sorted ascending by f\n        elites = []\n\n        # helper to evaluate safely and update elites\n        def _eval(x):\n            nonlocal evals, f_opt, x_opt, elites\n            if evals >= budget:\n                return None\n            # ensure numpy array\n            x = np.asarray(x, dtype=float)\n            # clip into bounds (safety)\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            evals += 1\n            # update best and elite list\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n            # insert into elites if good\n            if len(elites) < self.elite_size or f < elites[-1][0]:\n                elites.append((f, x.copy()))\n                elites.sort(key=lambda t: t[0])\n                if len(elites) > self.elite_size:\n                    elites.pop()\n            return f\n\n        # initial sampling: uniform\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub, size=dim)\n            _eval(x)\n            if evals >= budget:\n                break\n\n        # if no evaluations possible, return\n        if evals == 0:\n            return float(f_opt), np.asarray(x_opt, dtype=float)\n\n        # ensure we have at least one elite and best\n        if len(elites) == 0:\n            # evaluate a fallback point: center\n            _eval(np.zeros(dim))\n\n        # per-dimension adaptive step sizes\n        sigma = np.full(dim, self.sigma0 * (ub - lb) * 0.5 / 5.0)  # scaled to domain size\n        # ensure within allowed extents\n        sigma = np.clip(sigma, self.sigma_min, self.sigma_max)\n\n        # directional EMA of successful signed moves (low-degree guidance)\n        dir_ema = np.zeros(dim, dtype=float)\n        ema_alpha = 0.12\n\n        since_improvement = 0\n\n        # main loop: until budget exhausted\n        while evals < budget:\n            remaining = budget - evals\n            # choose operation\n            r = rng.random()\n            # pick a base point (prefer best but sometimes random elite)\n            if elites:\n                base_idx = 0 if rng.random() < 0.75 else rng.integers(low=0, high=len(elites))\n                base_x = elites[base_idx][1].copy()\n            else:\n                base_x = rng.uniform(lb, ub, size=dim)\n\n            performed = False\n            improved_this_round = False\n\n            # Coordinate probing: cheap 1-D moves (single eval each)\n            if r < self.p_coord:\n                # choose a coordinate with probability proportional to sigma (more explore on larger steps)\n                probs = sigma / (sigma.sum() + 1e-12)\n                i = int(rng.choice(dim, p=probs))\n                # try + and - in random order, but ensure we don't exceed budget\n                directions = [1.0, -1.0]\n                rng.shuffle(directions)\n                for sgn in directions:\n                    if evals >= budget:\n                        break\n                    x_try = base_x.copy()\n                    step = sgn * sigma[i]\n                    x_try[i] = x_try[i] + step\n                    x_try = np.clip(x_try, lb, ub)\n                    f_try = _eval(x_try)\n                    performed = True\n                    if f_try is not None and f_try < f_opt:\n                        # success: increase sigma for this coordinate\n                        sigma[i] = min(self.sigma_max, sigma[i] * self.success_inc)\n                        # update directional ema\n                        dir_step = np.zeros(dim); dir_step[i] = sgn\n                        dir_ema = (1.0 - ema_alpha) * dir_ema + ema_alpha * dir_step\n                        improved_this_round = True\n                        since_improvement = 0\n                        break\n                    else:\n                        # failure: shrink sigma\n                        sigma[i] = max(self.sigma_min, sigma[i] * self.fail_dec)\n                if not improved_this_round:\n                    since_improvement += 1\n\n            # Pairwise midpoints & small extrapolations (linear ops)\n            elif r < self.p_coord + self.p_pair and len(elites) >= 2:\n                # choose two distinct elites (prefer top ones)\n                a_idx = rng.integers(0, min(len(elites), max(2, self.elite_size)))\n                b_idx = rng.integers(0, min(len(elites), max(2, self.elite_size)))\n                # ensure distinct\n                if a_idx == b_idx:\n                    b_idx = (b_idx + 1) % max(1, len(elites))\n                xa = elites[a_idx][1]\n                xb = elites[b_idx][1]\n                # midpoint\n                m = 0.5 * (xa + xb)\n                if remaining > 0:\n                    f_m = _eval(m)\n                    performed = True\n                    if f_m is not None and f_m < f_opt:\n                        # nudging sigmas towards the direction of improvement\n                        d = xb - xa\n                        if np.linalg.norm(d) > 0:\n                            sgn = np.sign(d)\n                            dir_ema = (1.0 - ema_alpha) * dir_ema + ema_alpha * sgn\n                        # increase dimension-wise sigmas where |d| large\n                        sigma = np.minimum(self.sigma_max, sigma * (1.0 + 0.06 * (np.abs(d) / (np.abs(d).max() + 1e-12))))\n                        improved_this_round = True\n                        since_improvement = 0\n                    else:\n                        since_improvement += 1\n\n                # small extrapolation along the edge (linear extrapolation)\n                if remaining > 1 and evals < budget:\n                    beta = rng.uniform(0.12, 0.6)  # small linear extrapolation factor\n                    e = m + beta * (xb - xa)\n                    e = np.clip(e, lb, ub)\n                    f_e = _eval(e)\n                    performed = True\n                    if f_e is not None and f_e < f_opt:\n                        # success: modestly increase all sigmas\n                        sigma = np.minimum(self.sigma_max, sigma * 1.08)\n                        # update dir ema in direction of (xb - xa)\n                        d = xb - xa\n                        if np.linalg.norm(d) > 0:\n                            dir_ema = (1.0 - ema_alpha) * dir_ema + ema_alpha * np.sign(d)\n                        improved_this_round = True\n                        since_improvement = 0\n                    else:\n                        since_improvement += 1\n\n            # Average-guided multi-dim move (low-degree linear combination of sign EMA)\n            else:\n                # use EMA directional guidance to create a low-degree linear step\n                if np.linalg.norm(dir_ema) < 1e-12 or remaining <= 0:\n                    # fallback: small random coordinate perturbation around best\n                    center = x_opt\n                    i = rng.integers(0, dim)\n                    x_try = center.copy()\n                    x_try[i] += sigma[i] * rng.normal()\n                    x_try = np.clip(x_try, lb, ub)\n                    f_t = _eval(x_try)\n                    performed = True\n                    if f_t is not None and f_t < f_opt:\n                        sigma[i] = min(self.sigma_max, sigma[i] * self.success_inc)\n                        dir_ema[i] = (1.0 - ema_alpha) * dir_ema[i] + ema_alpha * np.sign(x_try[i] - center[i])\n                        improved_this_round = True\n                        since_improvement = 0\n                    else:\n                        sigma[i] = max(self.sigma_min, sigma[i] * self.fail_dec)\n                        since_improvement += 1\n                else:\n                    # normalized direction from EMA, scale by average sigma\n                    d = dir_ema.copy()\n                    # use only sign to keep low-degree linear moves\n                    d = np.sign(d)\n                    avg_sigma = float(np.mean(sigma))\n                    gamma = rng.uniform(0.8, 2.2)  # scale factor\n                    x_try = x_opt + d * (gamma * avg_sigma)\n                    x_try = np.clip(x_try, lb, ub)\n                    f_t = _eval(x_try)\n                    performed = True\n                    if f_t is not None and f_t < f_opt:\n                        # increase sigmas for coords that moved\n                        moved = np.abs(d) > 0\n                        sigma[moved] = np.minimum(self.sigma_max, sigma[moved] * self.success_inc)\n                        dir_ema = (1.0 - ema_alpha) * dir_ema + ema_alpha * d\n                        improved_this_round = True\n                        since_improvement = 0\n                    else:\n                        # shrink all sigmas a bit\n                        sigma = np.maximum(self.sigma_min, sigma * self.fail_dec)\n                        since_improvement += 1\n\n            # small housekeeping: keep sigma in bounds\n            sigma = np.clip(sigma, self.sigma_min, self.sigma_max)\n\n            # stagnation handling: if long without improvement, diversify with several linear midpoints and a random center\n            if since_improvement >= self.stagnation_restart and evals < budget:\n                # produce a few strategic linear diversifications (midpoints between best and some random elite/random point)\n                diversify_count = min(8, budget - evals)\n                for _ in range(diversify_count):\n                    if rng.random() < 0.6 and elites:\n                        partner = elites[rng.integers(0, len(elites))][1]\n                        candidate = 0.5 * (x_opt + partner) + rng.uniform(-0.3, 0.3, size=dim) * sigma\n                    else:\n                        candidate = rng.uniform(lb, ub, size=dim)\n                    _eval(candidate)\n                    if evals >= budget:\n                        break\n                # after diversification, increase some sigmas to encourage escapes\n                sigma = np.minimum(self.sigma_max, sigma * 1.8)\n                # reset EMA moderately\n                dir_ema *= 0.5\n                since_improvement = 0\n\n            # safety: if nothing performed (shouldn't happen), do a random sample\n            if (not performed) and evals < budget:\n                _eval(rng.uniform(lb, ub, size=dim))\n\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 11, "feedback": "In the code, line 894, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities do not sum to 1. See Notes section of docstring for more information.", "error": "In the code, line 894, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities do not sum to 1. See Notes section of docstring for more information.", "parent_ids": ["b86b676b-91f8-4b23-89ea-ffbb21612295"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2009.0, "Edges": 2008.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9990044798407167, "Degree Variance": 2.2658028914682333, "Transitivity": 0.0, "Max Depth": 20.0, "Min Depth": 2.0, "Mean Depth": 8.927252985884907, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3252328558572248, "Depth Entropy": 2.311705439452897, "Assortativity": 9.475789866352598e-09, "Average Eccentricity": 20.88302638128422, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.0004977600796416127, "Average Shortest Path": 10.644140213660044, "mean_complexity": 16.333333333333332, "total_complexity": 49.0, "mean_token_count": 586.3333333333334, "total_token_count": 1759.0, "mean_parameter_count": 6.0, "total_parameter_count": 18.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "5feae7f8-cc5b-4e93-b104-51c849a7111d", "fitness": 0.39543303291753606, "name": "LowDegreeEdgeGuidedSubspaceSearch", "description": "Low-Degree Edge-Guided Subspace Search (LEGSS) \u2014 favor strictly linear/affine moves: adaptive low-dimensional subspace probes steered by edge-derived linear directions (secant/affine extrapolations) and lightweight edge densification with simple linear predictions; remove higher-degree recombinations to keep models low-degree and robust.", "code": "import numpy as np\n\nclass LowDegreeEdgeGuidedSubspaceSearch:\n    \"\"\"\n    Low-Degree Edge-Guided Subspace Search (LEGSS)\n\n    Key ideas:\n      - Keep model degree low (linear/affine only): use secant-like edge directions and\n        linear interpolation predictions to propose candidates.\n      - Adaptive trust radius around current best; increase on success, shrink on failure.\n      - Subspace probes use directions constructed as linear combinations of elite differences\n        (affine steering) plus small Gaussian noise.\n      - Periodic edge-densification: generate many linear/extrapolated candidates along elite edges,\n        rank them by cheap linear predictions and distance penalty, evaluate only top few.\n      - Minimal orthogonal/noise perturbations (low-degree), no triangle barycentric or parabolic fits.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, init_radius=2.0, min_radius=1e-6,\n                 max_radius=2.5, stagnation_patience=6, elite_size=None,\n                 edge_phase_freq=8, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.stagnation_patience = int(stagnation_patience)\n        self.edge_phase_freq = int(edge_phase_freq)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        if elite_size is None:\n            self.elite_size = min(10, max(4, self.dim))\n        else:\n            self.elite_size = int(elite_size)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shape\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        evals = 0\n\n        # Seed: modest random sampling to populate archive\n        seed_pool = min(max(6, self.elite_size * 2), max(6, self.budget // 20))\n        seed_pool = max(2, seed_pool)\n        archive = []\n        for _ in range(seed_pool):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            archive.append((f, x.copy()))\n        if len(archive) == 0:\n            return None, None\n\n        archive.sort(key=lambda t: t[0])\n        archive = archive[:self.elite_size]\n        f_best, x_best = archive[0][0], archive[0][1].copy()\n\n        radius = float(self.init_radius)\n        stagnation = 0\n        major_iter = 0\n\n        # helper to update archive and best\n        def update_archive(fx, xx):\n            nonlocal archive, f_best, x_best\n            archive.append((fx, xx.copy()))\n            archive.sort(key=lambda t: t[0])\n            if len(archive) > self.elite_size:\n                archive = archive[:self.elite_size]\n            if fx < f_best:\n                f_best = float(fx)\n                x_best = xx.copy()\n\n        # Precompute scale for bound normalization\n        bound_range_mean = float(np.mean(ub - lb))\n        bound_norm = max(1e-12, np.linalg.norm(ub - lb))\n\n        # main loop\n        while evals < self.budget:\n            major_iter += 1\n            remaining = self.budget - evals\n\n            # SUBSPACE / AFFINE-STEERED PROBES\n            # bias k to small dims\n            raw = self.rng.beta(1.6, 5.5)\n            k = int(np.clip(np.ceil(raw * self.dim), 1, self.dim))\n\n            # batch size modest (favor more iterations, low-degree operations)\n            approx_batch = int(np.clip(4 + 3 * (k / max(1, self.dim)), 1, 20))\n            batch = max(1, min(remaining, approx_batch))\n\n            improved_in_batch = False\n\n            # Build a simple affine steering direction from elites:\n            # direction = weighted sum of (elite - x_best) with weights ~ exp(-(f - f_best)/scale)\n            E = len(archive)\n            elites = np.array([x for (_, x) in archive])\n            elites_f = np.array([f for (f, _) in archive])\n            # scale for weights: median spread of elite f values or small constant\n            fspread = max(1e-8, np.median(np.abs(elites_f - elites_f[0])) + 1e-8)\n            weights = np.exp(- (elites_f - elites_f[0]) / (fspread + 1e-12))\n            weights = weights.clip(min=1e-6)\n            # center at best -> steering vector towards (weighted mean of elites)\n            weighted_mean = np.average(elites, axis=0, weights=weights)\n            steering = weighted_mean - x_best\n            steering_norm = np.linalg.norm(steering)\n            if steering_norm > 1e-12:\n                steering_unit = steering / steering_norm\n            else:\n                steering_unit = None  # fallback to random directions\n\n            for _ in range(batch):\n                if evals >= self.budget:\n                    break\n\n                # pick coord subset\n                if k == self.dim:\n                    inds = np.arange(self.dim)\n                else:\n                    inds = self.rng.choice(self.dim, size=k, replace=False)\n\n                # base step: radius scaled, combine affine steering (if available) and small Gaussian\n                if steering_unit is not None:\n                    # directional magnitude proportional to radius, projected on chosen indices\n                    step_dir = steering_unit[inds]\n                    # random sign jitter to allow both directions sometimes\n                    sign_jitter = self.rng.choice([1.0, -1.0]) * (0.8 + 0.4 * self.rng.random())\n                    step = sign_jitter * radius * step_dir\n                    # add small Gaussian noise (kept very small to maintain linearity)\n                    noise = self.rng.normal(0.0, 0.08 * radius, size=k)\n                    step = step + noise\n                else:\n                    # pure isotropic small Gaussian if no steering\n                    step = self.rng.normal(0.0, 0.9 * radius / max(1.0, np.sqrt(k)), size=k)\n\n                x_cand = x_best.copy()\n                x_cand[inds] += step\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n\n                f_cand = float(func(x_cand))\n                evals += 1\n                update_archive(f_cand, x_cand)\n\n                if f_cand < f_best:\n                    improved_in_batch = True\n                    stagnation = 0\n                    # modestly increase radius on success (low-degree reward)\n                    radius = min(self.max_radius, radius * 1.22 + 1e-12)\n                # no parabolic or heavy local model \u2014 keep low-degree\n\n            if not improved_in_batch:\n                stagnation += 1\n                # shrink radius on failures\n                radius = max(self.min_radius, radius * 0.68)\n            else:\n                stagnation = 0\n\n            # occasional simple opposite test (low-degree: just reflect best)\n            if evals < self.budget and stagnation >= 3 and self.rng.random() < 0.18:\n                x_opp = lb + ub - x_best\n                x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                f_opp = float(func(x_opp))\n                evals += 1\n                update_archive(f_opp, x_opp)\n                if f_opp < f_best:\n                    radius = min(self.max_radius, radius * 1.18)\n                    stagnation = 0\n\n            # EDGE-DENSIFICATION PHASE (linear/extrapolation only)\n            do_edge_phase = (major_iter % self.edge_phase_freq == 0) or (stagnation >= max(2, self.stagnation_patience // 2))\n            if do_edge_phase and evals < self.budget and len(archive) >= 2:\n                # focus on a compact set of top elites (limit to keep degree low)\n                top_k = min(len(archive), max(3, self.elite_size))\n                elites = np.array([x for (f, x) in archive[:top_k]])\n                elites_f = np.array([f for (f, x) in archive[:top_k]])\n                E = len(elites)\n\n                candidate_list = []\n\n                # t-values for interpolation/extrapolation (kept small, linear only)\n                t_values = [0.0, 0.25, 0.5, 0.9, 1.1]  # includes mild extrapolation\n                for i in range(E):\n                    xi = elites[i]\n                    fi = elites_f[i]\n                    for j in range(i+1, E):\n                        xj = elites[j]\n                        fj = elites_f[j]\n                        v = xj - xi\n                        v_norm = np.linalg.norm(v) + 1e-12\n                        # prefer direction that goes toward lower predicted f by linear interpolation\n                        for t in t_values:\n                            # linear candidate along edge (affine)\n                            x_c = xi * (1.0 - t) + xj * t\n                            # small Gaussian perturbation (low-degree)\n                            x_c_pert = np.minimum(np.maximum(x_c + self.rng.normal(0, 0.02 * bound_range_mean, size=self.dim), lb), ub)\n\n                            # predicted linear interpolation f\n                            f_pred = fi * (1.0 - t) + fj * t\n\n                            # compute simple secant slope along edge: (fj - fi) / ||v||\n                            secant = (fj - fi) / v_norm\n                            # if secant indicates improvement moving from worse->better, propose small extrapolation from better endpoint\n                            # determine better endpoint\n                            if fi <= fj:\n                                better_x, better_f = xi, fi\n                                move_dir = (xj - xi) / v_norm\n                            else:\n                                better_x, better_f = xj, fj\n                                move_dir = (xi - xj) / v_norm\n                            # extrapolation distance scaled by radius but clipped to half edge length for safety\n                            ext_scale = min(1.0, 0.6 * radius / (v_norm + 1e-12))\n                            x_extrap = np.minimum(np.maximum(better_x + ext_scale * v_norm * move_dir, lb), ub)\n\n                            # linear prediction for extrapolation (affine secant)\n                            f_pred_extrap = better_f + secant * (ext_scale * v_norm)\n\n                            candidate_list.append((f_pred, x_c.copy()))\n                            candidate_list.append((f_pred - 0.005 * abs(fj - fi), x_c_pert.copy()))\n                            candidate_list.append((f_pred_extrap, x_extrap.copy()))\n\n                # deduplicate candidates (rounded)\n                uniq = {}\n                for fp, xc in candidate_list:\n                    key = tuple(np.round(xc, decimals=7))\n                    if key not in uniq or fp < uniq[key][0]:\n                        uniq[key] = (fp, xc)\n                candidates = list(uniq.values())\n\n                if len(candidates) > 0:\n                    xs = np.array([x for (fp, x) in candidates])\n                    fps = np.array([fp for (fp, x) in candidates])\n                    dists = np.linalg.norm(xs - x_best[None, :], axis=1)\n                    normalized_dist = dists / max(1e-12, bound_norm)\n                    # simple composite: predicted + small distance penalty (affine, low-degree)\n                    composite = fps + 0.10 * normalized_dist * (abs(f_best) + 1.0)\n                    order = np.argsort(composite)\n                    # evaluate a very small number of top linear candidates (conservative, low-degree)\n                    max_eval_edge_phase = min(max(3, self.elite_size), max(1, (remaining) // 8))\n                    max_eval_edge_phase = min(max_eval_edge_phase, len(order))\n                    evaluated_any = False\n                    for idx in order[:max_eval_edge_phase]:\n                        if evals >= self.budget:\n                            break\n                        x_try = candidates[idx][1]\n                        x_try = np.minimum(np.maximum(x_try, lb), ub)\n                        f_try = float(func(x_try))\n                        evals += 1\n                        update_archive(f_try, x_try)\n                        evaluated_any = True\n                        if f_try < f_best:\n                            radius = min(self.max_radius, radius * 1.28)\n                            stagnation = 0\n                    if not evaluated_any:\n                        radius = max(self.min_radius, radius * 0.85)\n                        stagnation += 1\n\n            # forced light diversification if stagnated\n            if stagnation >= self.stagnation_patience and evals < self.budget:\n                # sample a few global random points (very small number)\n                num_global = min(3, max(1, (self.budget - evals) // 20))\n                improved = False\n                for _ in range(num_global):\n                    if evals >= self.budget:\n                        break\n                    x_rand = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_rand = float(func(x_rand))\n                    evals += 1\n                    update_archive(f_rand, x_rand)\n                    if f_rand < f_best:\n                        improved = True\n                        radius = min(self.max_radius, self.init_radius)\n                        stagnation = 0\n                        break\n                if not improved and evals < self.budget:\n                    # small jump around a top elite\n                    anchor = archive[self.rng.integers(0, len(archive))][1]\n                    jump = self.rng.normal(0, 0.18 * bound_range_mean, size=self.dim)\n                    x_jump = np.minimum(np.maximum(anchor + jump, lb), ub)\n                    f_jump = float(func(x_jump))\n                    evals += 1\n                    update_archive(f_jump, x_jump)\n                    if f_jump < f_best:\n                        radius = min(self.max_radius, self.init_radius)\n                    else:\n                        radius = max(self.min_radius, self.init_radius * 0.55)\n                    stagnation = 0\n\n            # clamp radius\n            radius = float(np.clip(radius, self.min_radius, self.max_radius))\n\n            if evals >= self.budget:\n                break\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 11, "feedback": "The algorithm LowDegreeEdgeGuidedSubspaceSearch scored 0.395 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a66bcb4a-d4ad-4cd2-bcfd-3ff1a37e3197"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.6270349185642108}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.6076245616993117}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7049221876864133}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.2736441689750554}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.19062784985543046}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.21717578767035228}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.10330647223673972}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.07153536204812605}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.07718240540538135}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.06396046628179919}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07624587895124191}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.08604457352109307}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9559556536663972}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9304374690499421}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9457977381515466}], "aucs": [0.6270349185642108, 0.6076245616993117, 0.7049221876864133, 0.2736441689750554, 0.19062784985543046, 0.21717578767035228, 0.10330647223673972, 0.07153536204812605, 0.07718240540538135, 0.06396046628179919, 0.07624587895124191, 0.08604457352109307, 0.9559556536663972, 0.9304374690499421, 0.9457977381515466]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2603.0, "Edges": 2602.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9992316557817902, "Degree Variance": 2.0023044423017917, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.568776371308017, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3275491113061828, "Depth Entropy": 2.199878211936975, "Assortativity": 0.0, "Average Eccentricity": 17.98655397618133, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.000384172109104879, "Average Shortest Path": 10.638915719253756, "mean_complexity": 15.0, "total_complexity": 60.0, "mean_token_count": 572.0, "total_token_count": 2288.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "964ff4d1-eae7-4989-a9da-2f5488ddd047", "fitness": 0.6055376400708888, "name": "LowDegreeRotatedCoordinateSearch", "description": "Low-Degree Rotated Coordinate Search (LDRCS) \u2014 aggressive, low-degree coordinate probing with per-dimension step scales and lightweight rotated 2-D scans limited to a small fixed number of evaluations per iteration to favor robust, cheap exploitation and occasional small recombinations for diversification.", "code": "import numpy as np\n\nclass LowDegreeRotatedCoordinateSearch:\n    \"\"\"\n    Low-Degree Rotated Coordinate Search (LDRCS)\n    - Keep per-dimension step scales (low-degree: 1D probes + occasional single extrapolation).\n    - Use a tiny memory of recent successful directions to form cheap rotated 2-D probes.\n    - Limit the number of function evaluations per iteration (<= 3) to reduce max-degree operations.\n    - Maintain a small elite archive and use very cheap linear recombinations for diversification.\n    - Strictly respect the evaluation budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_step=1.0, min_step=1e-6, max_step=2.5,\n                 elite_size=4, dir_memory=5, stagn_patience=10, rng=None):\n        \"\"\"\n        Arguments:\n          budget: allowed function evaluations\n          dim: dimension\n          init_step: initial per-dim step magnitude\n          min_step, max_step: clamp step sizes\n          elite_size: small elite archive\n          dir_memory: number of recent successful direction unit vectors to remember\n          stagn_patience: iterations without improvement to trigger diversification\n          rng: numpy.random.Generator optional\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.elite_size = int(elite_size)\n        self.dir_memory = int(dir_memory)\n        self.stagn_patience = int(stagn_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        # allow callable to expose .bounds but don't rely on it\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = float(np.asarray(lb).ravel()[0]) if np.asarray(lb).size == 1 else np.array(lb, dtype=float)\n        ub = float(np.asarray(ub).ravel()[0]) if np.asarray(ub).size == 1 else np.array(ub, dtype=float)\n        if np.isscalar(lb):\n            lb = np.full(self.dim, lb, dtype=float)\n        if np.isscalar(ub):\n            ub = np.full(self.dim, ub, dtype=float)\n        return np.array(lb, dtype=float), np.array(ub, dtype=float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # enforce shapes\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n        if ub.shape[0] != self.dim:\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        rng = self.rng\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # small initial sampling to seed elites and get a feasible start\n        # keep very small to preserve budget\n        n_init = min(6, max(1, self.budget // 200 + 1))\n        xs = []\n        fs = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            xs.append(x.copy())\n            fs.append(f)\n\n        if len(fs) == 0:\n            # nothing evaluated, return center after consuming no budget\n            x0 = lb + 0.5 * (ub - lb)\n            return float(func(x0)), x0  # if budget zero this will violate but assume budget>0 in practice\n\n        idx_best = int(np.argmin(fs))\n        x_best = xs[idx_best].copy()\n        f_best = float(fs[idx_best])\n\n        # per-dimension step sizes (low-degree control)\n        steps = np.full(self.dim, float(self.init_step))\n        steps = np.minimum(steps, np.maximum(1e-12, ub - lb))\n\n        # elite archive: list of (f, x)\n        elite = sorted(zip(fs, xs), key=lambda t: t[0])[: self.elite_size]\n        elite = [(float(fv), np.array(xv, dtype=float).copy()) for fv, xv in elite]\n\n        # small direction memory (unit vectors)\n        dir_mem = []\n\n        stagn = 0\n        iters = 0\n\n        def push_dir(v):\n            if v is None:\n                return\n            norm = np.linalg.norm(v)\n            if norm > 1e-12:\n                u = v / norm\n                dir_mem.insert(0, u)\n                if len(dir_mem) > self.dir_memory:\n                    dir_mem.pop()\n\n        def update_elite(candidate_x, candidate_f):\n            nonlocal elite\n            candidate_x = np.array(candidate_x, dtype=float).copy()\n            candidate_f = float(candidate_f)\n            # simple uniqueness test\n            tol = 1e-8 + 1e-8 * np.linalg.norm(ub - lb)\n            add = True\n            for f_e, x_e in elite:\n                if np.linalg.norm(candidate_x - x_e) < tol:\n                    add = False\n                    break\n            if add:\n                elite.append((candidate_f, candidate_x))\n                elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n\n        # primary loop: each iteration uses at most 3 function evaluations (often 1-2)\n        while evals < self.budget:\n            remaining = self.budget - evals\n            improved = False\n            iters += 1\n\n            # choose mode with probabilities favouring cheap coordinate probes\n            r = rng.random()\n            if r < 0.60 and remaining >= 1:\n                # 1-D coordinate probe (prefer dims with larger step and/or larger spread in elites)\n                # compute simple weights from steps and elite spread\n                if len(elite) >= 2:\n                    coords = np.vstack([e[1] for e in elite])\n                    spreads = np.std(coords, axis=0) + 1e-12\n                else:\n                    spreads = np.ones(self.dim)\n                weights = spreads * (steps + 1e-12)\n                weights = weights / weights.sum()\n                idx = int(rng.choice(self.dim, p=weights))\n\n                dir_sign = 1 if rng.random() < 0.5 else -1\n                delta = np.zeros(self.dim)\n                delta[idx] = dir_sign * steps[idx]\n                # evaluate one side first (cheap)\n                x1 = clip(x_best + delta)\n                f1 = None\n                if evals < self.budget:\n                    f1 = float(func(x1))\n                    evals += 1\n                # if better accept and possibly try opposite (but keep low-degree)\n                if f1 is not None and f1 < f_best:\n                    # accept immediate improvement\n                    push_dir(x1 - x_best)\n                    x_best_old = x_best.copy()\n                    x_best = x1.copy()\n                    f_best = f1\n                    improved = True\n                    # try at most one extrapolation (single extra eval) along same signed axis\n                    if evals < self.budget:\n                        # make a short extrapolation (scale by 1.5)\n                        ext = clip(x_best + 0.5 * delta)  # half-step further in same sign\n                        if np.linalg.norm(ext - x_best) > 1e-12:\n                            fext = float(func(ext))\n                            evals += 1\n                            if fext < f_best:\n                                push_dir(ext - x_best_old)\n                                x_best = ext.copy()\n                                f_best = fext\n                                improved = True\n                    # increase step for this coordinate\n                    steps[idx] = min(self.max_step, steps[idx] * 1.20)\n                else:\n                    # evaluate opposite side if budget allows (keeps degree <=2)\n                    if remaining >= 2 and evals < self.budget:\n                        x2 = clip(x_best - delta)\n                        f2 = float(func(x2))\n                        evals += 1\n                        if f2 < f_best:\n                            push_dir(x2 - x_best)\n                            x_best_old = x_best.copy()\n                            x_best = x2.copy()\n                            f_best = f2\n                            improved = True\n                            # small extrapolation attempt\n                            if evals < self.budget:\n                                ext = clip(x_best - 0.5 * delta)\n                                if np.linalg.norm(ext - x_best) > 1e-12:\n                                    fext = float(func(ext))\n                                    evals += 1\n                                    if fext < f_best:\n                                        push_dir(ext - x_best_old)\n                                        x_best = ext.copy()\n                                        f_best = fext\n                                        improved = True\n                            steps[idx] = min(self.max_step, steps[idx] * 1.08)\n                        else:\n                            # no improvement on either side -> shrink step for this dim\n                            steps[idx] = max(self.min_step, steps[idx] * 0.63)\n                    else:\n                        # can't test opposite: shrink modestly\n                        steps[idx] = max(self.min_step, steps[idx] * 0.80)\n\n            elif r < 0.9 and remaining >= 1:\n                # rotated 2-D probe but keep it cheap: evaluate two directions in plane using at most 2 evals\n                # pick base axis u and a secondary direction v from dir_mem or random\n                u_idx = int(rng.integers(self.dim))\n                u = np.zeros(self.dim)\n                u[u_idx] = 1.0\n                if dir_mem and rng.random() < 0.75:\n                    d = dir_mem[rng.integers(len(dir_mem))]\n                else:\n                    d = rng.normal(size=self.dim)\n                    nd = np.linalg.norm(d)\n                    if nd < 1e-12:\n                        d = np.ones(self.dim) / np.sqrt(self.dim)\n                    else:\n                        d = d / nd\n                # orthogonalize d against u to get v\n                proj = (u @ d) * u\n                v = d - proj\n                vn = np.linalg.norm(v)\n                if vn < 1e-12:\n                    # fallback to neighboring axis\n                    v = np.zeros(self.dim)\n                    v[(u_idx + 1) % self.dim] = 1.0\n                else:\n                    v = v / vn\n\n                # build two rotated directions with small angle to favor low-degree curvature\n                alpha = 0.5  # control mixing between u and v (low-degree)\n                dir_a = alpha * u + (1.0 - alpha) * v\n                dir_b = alpha * u - (1.0 - alpha) * v\n                # scale by a representative step: geometric mean of two dims maybe\n                step_scale = np.sqrt(max(1e-12, steps[u_idx] * np.mean(steps)))\n                cand1 = clip(x_best + step_scale * dir_a)\n                cand2 = clip(x_best + step_scale * dir_b)\n\n                # evaluate at most two candidates (keeping low-degree)\n                f1 = None\n                f2 = None\n                if evals < self.budget:\n                    f1 = float(func(cand1))\n                    evals += 1\n                if evals < self.budget:\n                    f2 = float(func(cand2))\n                    evals += 1\n\n                # pick best among them and accept if improved\n                best_local_x = x_best\n                best_local_f = f_best\n                if f1 is not None and f1 < best_local_f:\n                    best_local_f = f1\n                    best_local_x = cand1.copy()\n                if f2 is not None and f2 < best_local_f:\n                    best_local_f = f2\n                    best_local_x = cand2.copy()\n\n                if best_local_f < f_best:\n                    push_dir(best_local_x - x_best)\n                    x_old = x_best.copy()\n                    x_best = best_local_x.copy()\n                    f_best = best_local_f\n                    improved = True\n                    # modest step increase along dimensions involved\n                    steps[u_idx] = min(self.max_step, steps[u_idx] * 1.12)\n                    # nudge other steps slightly up\n                    steps = np.minimum(self.max_step, steps * 1.03)\n\n            else:\n                # very cheap recombination/diversification: either mix elites or random near-best jitter\n                if elite and len(elite) >= 2 and rng.random() < 0.85 and remaining >= 1:\n                    # linear mix of two elites\n                    ids = rng.choice(len(elite), size=2, replace=False)\n                    x_a = elite[ids[0]][1]\n                    x_b = elite[ids[1]][1]\n                    w = rng.random()\n                    x_new = clip(w * x_a + (1.0 - w) * x_b)\n                    # tiny jitter scaled by median step to keep low-degree\n                    med_step = max(1e-12, np.median(steps))\n                    jitter = rng.normal(scale=0.06 * med_step, size=self.dim)\n                    x_new = clip(x_new + jitter)\n                    if evals < self.budget:\n                        fnew = float(func(x_new))\n                        evals += 1\n                        update_elite(x_new, fnew)\n                        if fnew < f_best:\n                            push_dir(x_new - x_best)\n                            x_best = x_new.copy()\n                            f_best = fnew\n                            improved = True\n                            steps = np.minimum(self.max_step, steps * 1.10)\n                else:\n                    # tiny random local sample around x_best (cheap)\n                    med_step = max(1e-12, np.median(steps))\n                    if evals < self.budget:\n                        x_new = clip(x_best + rng.normal(scale=0.12 * med_step, size=self.dim))\n                        fnew = float(func(x_new))\n                        evals += 1\n                        update_elite(x_new, fnew)\n                        if fnew < f_best:\n                            push_dir(x_new - x_best)\n                            x_best = x_new.copy()\n                            f_best = fnew\n                            improved = True\n                            steps = np.minimum(self.max_step, steps * 1.08)\n                        else:\n                            # small global shrink to encourage exploring other coordinates\n                            steps = np.maximum(self.min_step, steps * 0.97)\n\n            # update elite with current best occasionally\n            update_elite(x_best, f_best)\n\n            # adapt general behavior from improvement flag\n            if improved:\n                stagn = 0\n            else:\n                stagn += 1\n\n            # occasional mirrored check (very cheap, 1 eval) to exploit symmetry\n            if stagn >= 4 and evals < self.budget and rng.random() < 0.25:\n                x_mirror = clip(lb + ub - x_best)\n                fm = float(func(x_mirror))\n                evals += 1\n                update_elite(x_mirror, fm)\n                if fm < f_best:\n                    push_dir(x_mirror - x_best)\n                    x_best = x_mirror.copy()\n                    f_best = fm\n                    improved = True\n                    stagn = 0\n                    steps = np.minimum(self.max_step, steps * 1.05)\n\n            # stronger diversification when stagnation persists, but keep it degree-1 (one eval per attempt)\n            if stagn >= self.stagn_patience and evals < self.budget:\n                # try mixing best two elites with jitter (one eval)\n                if len(elite) >= 2:\n                    # mix best two\n                    x_mix = 0.5 * (elite[0][1] + elite[min(1, len(elite)-1)][1])\n                    jitter = rng.normal(scale=0.2 * np.median(steps), size=self.dim)\n                    x_mix = clip(x_mix + jitter)\n                    fmix = float(func(x_mix))\n                    evals += 1\n                    update_elite(x_mix, fmix)\n                    if fmix < f_best:\n                        push_dir(x_mix - x_best)\n                        x_best = x_mix.copy()\n                        f_best = fmix\n                        stagn = 0\n                        steps = np.minimum(self.max_step, steps * 1.1)\n                        continue\n                # if still stagnating, perform one near-random jump around best (single eval)\n                if evals < self.budget:\n                    jump_scale = 0.8 * (ub - lb)\n                    x_jump = clip(x_best + rng.normal(scale=0.6, size=self.dim) * jump_scale)\n                    fj = float(func(x_jump))\n                    evals += 1\n                    update_elite(x_jump, fj)\n                    if fj < f_best:\n                        push_dir(x_jump - x_best)\n                        x_best = x_jump.copy()\n                        f_best = fj\n                        stagn = 0\n                        steps = np.minimum(self.max_step, steps * 1.15)\n                    else:\n                        # reset some steps to encourage different exploration but remain low-degree\n                        steps = np.maximum(self.min_step, steps * 0.5)\n                        stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 11, "feedback": "The algorithm LowDegreeRotatedCoordinateSearch scored 0.606 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a3b4a95-14a2-4906-a7fc-cf5f4ed2d5c6"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9708169868638493}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9675808775842187}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9698373330125185}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9429717301676728}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9493337409231789}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9383859734068336}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06942645407554282}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.045275781905796064}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10357877293775541}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06746305137178554}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.08664702639338795}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.994339944360177}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9864153454294586}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9909415826311565}], "aucs": [0.9708169868638493, 0.9675808775842187, 0.9698373330125185, 0.9429717301676728, 0.9493337409231789, 0.9383859734068336, 0.06942645407554282, 4.999999999999449e-05, 0.045275781905796064, 0.10357877293775541, 0.06746305137178554, 0.08664702639338795, 0.994339944360177, 0.9864153454294586, 0.9909415826311565]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2965.0, "Edges": 2964.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9993254637436761, "Degree Variance": 1.9527820070581745, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.921969696969697, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.326689997129104, "Depth Entropy": 2.216827162309534, "Assortativity": 0.0, "Average Eccentricity": 18.75716694772344, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003372681281618887, "Average Shortest Path": 11.038373011267304, "mean_complexity": 12.5, "total_complexity": 75.0, "mean_token_count": 423.5, "total_token_count": 2541.0, "mean_parameter_count": 3.0, "total_parameter_count": 18.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "f5951c2b-b9ff-4352-bb47-76f19aa885c0", "fitness": 0.5955685389498775, "name": "SeparableLinearPush", "description": "Separable Linear Push (SLP) \u2014 maintain cheap per-dimension finite-difference slope estimates and perform only linear (degree-1) pushes and sparse 2-D linear scans; adapt per-dimension steps by simple success/failure rules and use occasional low-cost Gaussian jumps for diversification.", "code": "import numpy as np\n\nclass SeparableLinearPush:\n    \"\"\"\n    Separable Linear Push (SLP)\n\n    Key ideas:\n      - Keep only linear (degree-1) local models: per-dimension finite-difference slopes\n        estimated from a few cheap probes. No high-degree interpolation.\n      - Drive search by the signs/magnitudes of slope estimates: push along coordinates\n        where linear model predicts the largest improvement.\n      - Allow small 2-D linear scans (combining two coordinates) to capture simple interactions.\n      - Adapt each coordinate step independently: grow on success, shrink on failure.\n      - Occasional low-cost Gaussian/convex diversification when stagnating.\n      - Strict enforcement of evaluation budget.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_step=1.0, min_step=1e-6, max_step=2.5,\n                 grow=1.2, shrink=0.6, stagn_patience=12, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total function evaluations\n          dim: dimensionality\n          init_step: initial per-dimension step magnitude\n          min_step, max_step: per-dimension step clamp\n          grow/shrink: multiplicative factors for step adaptation\n          stagn_patience: iterations without improvement before stronger diversification\n          rng: numpy.random.Generator or seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.stagn_patience = int(stagn_patience)\n        if rng is None:\n            self.rng = np.random.default_rng()\n        elif isinstance(rng, (np.random.Generator,)):\n            self.rng = rng\n        else:\n            self.rng = np.random.default_rng(rng)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shapes\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        # quick exit if no budget\n        if self.budget <= 0:\n            x0 = lb + 0.5 * (ub - lb)\n            return float(func(x0)), x0.copy()\n\n        evals = 0\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n\n        # initial point: central random (use one eval)\n        x_best = clip(lb + self.rng.random(self.dim) * (ub - lb))\n        f_best = float(func(x_best))\n        evals += 1\n\n        # per-dimension step sizes and slope memory\n        step = np.full(self.dim, self.init_step, dtype=float)\n        # store last positive/negative neighbor points and function values (or np.nan)\n        pos_f = np.full(self.dim, np.nan, dtype=float)\n        neg_f = np.full(self.dim, np.nan, dtype=float)\n        # slope estimates: (f_pos - f_neg) / (2*step) when both available; else 0\n        slope = np.zeros(self.dim, dtype=float)\n\n        stagn = 0\n        iters = 0\n\n        # basis vectors for coordinate quick creation\n        eye = np.eye(self.dim)\n\n        # helper to safely call func under budget\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # main loop\n        while evals < self.budget:\n            iters += 1\n            improved = False\n\n            remaining = self.budget - evals\n\n            # occasionally refresh a cheap per-dim probe for slope estimates\n            # but keep number of probes small: aim to use about 1 probe per iteration\n            # Decision mix: mostly coordinate pushes, sometimes 2D linear scan, rare global jitter\n            r = self.rng.random()\n            if r < 0.62:\n                # Coordinate push driven by slope magnitude or random if no info\n                # compute priority: abs(slope) * step; tie-break by random\n                priorities = np.abs(slope) * step\n                if np.all(priorities == 0) or self.rng.random() < 0.15:\n                    # random coordinate\n                    idx = int(self.rng.integers(self.dim))\n                else:\n                    # pick highest priority with some randomness\n                    probs = priorities + 1e-12\n                    probs = probs / probs.sum()\n                    idx = int(self.rng.choice(self.dim, p=probs))\n\n                # decide probe order: prefer sign suggested by slope (negative slope -> decrease)\n                direction = 1.0\n                if slope[idx] > 0:\n                    # positive slope means increasing coordinate increases f, so try decreasing\n                    direction = -1.0\n                elif slope[idx] < 0:\n                    direction = 1.0\n                else:\n                    direction = 1.0 if self.rng.random() < 0.5 else -1.0\n\n                # Attempt forward/back probes within budget\n                # We'll probe preferred direction first (x_best + direction*step)\n                delta = direction * step[idx] * eye[idx]\n                x_probe = clip(x_best + delta)\n                f_probe = safe_eval(x_probe)\n                if f_probe is None:\n                    break\n\n                # update neighbor memory\n                if direction > 0:\n                    pos_f[idx] = f_probe\n                else:\n                    neg_f[idx] = f_probe\n\n                # update slope if both sides available\n                if not np.isnan(pos_f[idx]) and not np.isnan(neg_f[idx]) and (2 * step[idx]) > 0:\n                    slope[idx] = (pos_f[idx] - neg_f[idx]) / (2.0 * step[idx])\n\n                # improvement?\n                if f_probe < f_best:\n                    # accept probe and attempt simple linear extrapolation along coordinate direction\n                    prev_best = x_best.copy()\n                    x_best = x_probe.copy()\n                    f_best = f_probe\n                    improved = True\n                    push_dir = delta.copy()\n                    # try up to a small number of extrapolations (linear extrapolation only)\n                    mult = 1.8\n                    for _ in range(3):\n                        if evals >= self.budget:\n                            break\n                        next_x = clip(x_best + mult * push_dir)\n                        if np.linalg.norm(next_x - x_best) < 1e-12:\n                            break\n                        f_next = safe_eval(next_x)\n                        if f_next is None:\n                            break\n                        # update neighbor memory accordingly\n                        if np.dot(next_x - prev_best, eye[idx]) > 0:\n                            pos_f[idx] = f_next\n                        else:\n                            neg_f[idx] = f_next\n                        if f_next < f_best:\n                            prev_best = x_best.copy()\n                            x_best = next_x.copy()\n                            f_best = f_next\n                            # grow the step for this coordinate a bit\n                            step[idx] = min(self.max_step, step[idx] * self.grow)\n                            improved = True\n                        else:\n                            break\n                    # update slope if possible\n                    if not np.isnan(pos_f[idx]) and not np.isnan(neg_f[idx]) and (2*step[idx])>0:\n                        slope[idx] = (pos_f[idx] - neg_f[idx]) / (2.0 * step[idx])\n                else:\n                    # not improved: try the other side if budget allows and we haven't probed it recently\n                    other_delta = -delta\n                    x_other = clip(x_best + other_delta)\n                    # if the other side has not been evaluated recently (compare coordinates), try it\n                    # But only if we have at least one eval left\n                    if evals < self.budget:\n                        f_other = safe_eval(x_other)\n                        if f_other is None:\n                            break\n                        if (-direction) > 0:\n                            pos_f[idx] = f_other\n                        else:\n                            neg_f[idx] = f_other\n                        if not np.isnan(pos_f[idx]) and not np.isnan(neg_f[idx]) and (2*step[idx])>0:\n                            slope[idx] = (pos_f[idx] - neg_f[idx]) / (2.0 * step[idx])\n                        if f_other < f_best:\n                            x_best = x_other.copy()\n                            f_best = f_other\n                            improved = True\n                    # if both probes failed to improve, shrink step\n                    if not improved:\n                        step[idx] = max(self.min_step, step[idx] * self.shrink)\n\n            elif r < 0.9:\n                # Sparse 2-D linear scan: choose two coordinates (prefer top slopes)\n                # pick first coord proportional to abs(slope)*step, second random from remaining\n                priorities = np.abs(slope) * step\n                if np.all(priorities == 0) or self.rng.random() < 0.2:\n                    i = int(self.rng.integers(self.dim))\n                else:\n                    probs = priorities + 1e-12\n                    probs = probs / probs.sum()\n                    i = int(self.rng.choice(self.dim, p=probs))\n                # pick j different from i\n                j = int(self.rng.integers(self.dim - 1))\n                if j >= i:\n                    j += 1\n                # construct two sign guesses from slopes\n                si = -1.0 if slope[i] > 0 else 1.0\n                sj = -1.0 if slope[j] > 0 else 1.0\n                # consider a tiny set of linear combinations (only 3 points to keep low eval cost)\n                cand_dirs = [\n                    si * step[i] * eye[i],\n                    sj * step[j] * eye[j],\n                    si * step[i] * eye[i] + sj * step[j] * eye[j]\n                ]\n                # evaluate candidates sequentially until budget exhausted\n                best_local_x = x_best.copy()\n                best_local_f = f_best\n                for d in cand_dirs:\n                    if evals >= self.budget:\n                        break\n                    x_c = clip(x_best + d)\n                    f_c = safe_eval(x_c)\n                    if f_c is None:\n                        break\n                    # update pos/neg memory for involved dims\n                    # approximate by checking coordinate offset sign\n                    if d[i] > 0:\n                        pos_f[i] = min(pos_f[i], f_c) if not np.isnan(pos_f[i]) else f_c\n                    elif d[i] < 0:\n                        neg_f[i] = min(neg_f[i], f_c) if not np.isnan(neg_f[i]) else f_c\n                    if d[j] > 0:\n                        pos_f[j] = min(pos_f[j], f_c) if not np.isnan(pos_f[j]) else f_c\n                    elif d[j] < 0:\n                        neg_f[j] = min(neg_f[j], f_c) if not np.isnan(neg_f[j]) else f_c\n                    if f_c < best_local_f:\n                        best_local_f = f_c\n                        best_local_x = x_c.copy()\n                # accept best local if improved and attempt one small combined extrapolation\n                if best_local_f < f_best:\n                    prev_best = x_best.copy()\n                    x_best = best_local_x.copy()\n                    f_best = best_local_f\n                    improved = True\n                    # small combined extrapolation\n                    if evals < self.budget:\n                        combined_dir = x_best - prev_best\n                        if np.linalg.norm(combined_dir) > 1e-12:\n                            next_x = clip(x_best + 1.4 * combined_dir)\n                            f_next = safe_eval(next_x)\n                            if f_next is not None and f_next < f_best:\n                                x_best = next_x.copy()\n                                f_best = f_next\n                                improved = True\n                                # grow steps for i and j a little\n                                step[i] = min(self.max_step, step[i] * self.grow)\n                                step[j] = min(self.max_step, step[j] * self.grow)\n                else:\n                    # failure: shrink both steps slightly\n                    step[i] = max(self.min_step, step[i] * self.shrink)\n                    step[j] = max(self.min_step, step[j] * self.shrink)\n\n                # attempt to refresh slope estimates for i and j if both pos/neg exist\n                for k in (i, j):\n                    if not np.isnan(pos_f[k]) and not np.isnan(neg_f[k]) and (2 * step[k]) > 0:\n                        slope[k] = (pos_f[k] - neg_f[k]) / (2.0 * step[k])\n\n            else:\n                # diversification: small gaussian around best or convex mix of random points\n                if stagn >= self.stagn_patience and evals + 2 <= self.budget:\n                    # stronger jitter: draw two samples, keep best\n                    scale = 0.8 * np.maximum(step, 1e-12)\n                    x1 = clip(x_best + self.rng.normal(scale=scale))\n                    f1 = safe_eval(x1)\n                    if f1 is None:\n                        break\n                    x2 = clip(x_best + self.rng.normal(scale=1.6 * scale))\n                    f2 = None\n                    if evals < self.budget:\n                        f2 = safe_eval(x2)\n                    # choose best among new ones\n                    candidates = [(f1, x1)]\n                    if f2 is not None:\n                        candidates.append((f2, x2))\n                    candidates.append((f_best, x_best))\n                    candidates.sort(key=lambda t: t[0])\n                    if candidates[0][0] < f_best:\n                        x_best = candidates[0][1].copy()\n                        f_best = candidates[0][0]\n                        improved = True\n                    else:\n                        # reset some steps to avoid getting stuck, modestly increase some steps\n                        idxs = self.rng.choice(self.dim, size=max(1, self.dim // 6), replace=False)\n                        for k in idxs:\n                            step[k] = min(self.max_step, step[k] * 1.1)\n                else:\n                    # light random sample (one eval)\n                    avg_step = np.mean(step)\n                    jitter = self.rng.normal(scale=0.35 * avg_step, size=self.dim)\n                    x_new = clip(x_best + jitter)\n                    f_new = safe_eval(x_new)\n                    if f_new is None:\n                        break\n                    if f_new < f_best:\n                        x_best = x_new.copy()\n                        f_best = f_new\n                        improved = True\n                    else:\n                        # slight shrink of average steps\n                        step = np.maximum(self.min_step, step * (0.95))\n\n            # adaptation & stagnation bookkeeping\n            if improved:\n                stagn = 0\n                # on success, modestly grow steps where slope suggests good direction\n                # grow all steps a little to encourage progress\n                step = np.minimum(self.max_step, step * (1.0 + 0.08))\n            else:\n                stagn += 1\n                # global gentle shrink to refine when not improving\n                step = np.maximum(self.min_step, step * (1.0 - 0.05))\n\n            # ensure steps remain in bounds\n            step = np.clip(step, self.min_step, self.max_step)\n\n            # occasionally recompute slopes from stored pos/neg pairs (conservative)\n            for k in range(self.dim):\n                if not np.isnan(pos_f[k]) and not np.isnan(neg_f[k]) and (2*step[k]) > 0:\n                    slope[k] = (pos_f[k] - neg_f[k]) / (2.0 * step[k])\n\n            # strong diversification if prolonged stagnation\n            if stagn >= 3 * self.stagn_patience and evals < self.budget:\n                # try one global random restart near center or random point\n                if self.rng.random() < 0.5:\n                    x_rand = clip(lb + self.rng.random(self.dim) * (ub - lb))\n                else:\n                    # near center but randomize\n                    center = 0.5 * (lb + ub)\n                    rng_scale = 0.6 * (ub - lb)\n                    x_rand = clip(center + self.rng.normal(scale=rng_scale))\n                f_rand = safe_eval(x_rand)\n                if f_rand is None:\n                    break\n                if f_rand < f_best:\n                    x_best = x_rand.copy()\n                    f_best = f_rand\n                    stagn = 0\n                    # reinitialize some memory\n                    pos_f[:] = np.nan\n                    neg_f[:] = np.nan\n                    slope[:] = 0.0\n                    step[:] = np.minimum(self.max_step, self.init_step)\n                else:\n                    # partially reset steps to escape\n                    step = np.minimum(self.max_step, np.maximum(self.min_step, step * 1.4))\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 11, "feedback": "The algorithm SeparableLinearPush scored 0.596 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a3b4a95-14a2-4906-a7fc-cf5f4ed2d5c6"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9769937415139996}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9651261567444678}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9641146273123163}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.956010607489538}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9520873401182537}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9651929925056433}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08497169897119383}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0514606018100181}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.026532175625838095}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.02175814647108032}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0711610658756544}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9882233308262045}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9717066817394399}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9381389172445167}], "aucs": [0.9769937415139996, 0.9651261567444678, 0.9641146273123163, 0.956010607489538, 0.9520873401182537, 0.9651929925056433, 0.08497169897119383, 0.0514606018100181, 4.999999999999449e-05, 0.026532175625838095, 0.02175814647108032, 0.0711610658756544, 0.9882233308262045, 0.9717066817394399, 0.9381389172445167]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3000.0, "Edges": 2999.0, "Max Degree": 20.0, "Min Degree": 1.0, "Mean Degree": 1.9993333333333334, "Degree Variance": 1.821332888888889, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 9.446947674418604, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3251940049843447, "Depth Entropy": 2.270830857439757, "Assortativity": 0.0, "Average Eccentricity": 17.719666666666665, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0003333333333333333, "Average Shortest Path": 11.283679671001446, "mean_complexity": 22.5, "total_complexity": 90.0, "mean_token_count": 645.0, "total_token_count": 2580.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "49beef6d-dff3-43de-8c03-613d52610af4", "fitness": 0.5203734948396346, "name": "LowDegreeProgressiveAnchors", "description": "Low-Degree Progressive Anchor Search \u2014 maintain a compact anchor set and use strictly low-degree, sequential 1-D probes (at most two evaluations per coordinate), single-eval diagonal probes combining recent successful axes, rare opposition and Cauchy escapes, and lightweight adaptive step rules to maximize information per evaluation while keeping the operation degree minimal.", "code": "import numpy as np\n\nclass LowDegreeProgressiveAnchors:\n    \"\"\"\n    Low-Degree Progressive Anchor Search (LD-PAS)\n    - budget: maximum number of function evaluations\n    - dim: dimensionality\n    Optional args:\n      - pop_size: number of anchors (compact)\n      - init_step: initial per-coordinate step as fraction of box diagonal (default 0.20)\n      - success_mult / failure_mult: multiplicative adaptation\n      - diag_prob: probability of a single-eval diagonal probe per anchor visit\n      - opp_prob: probability to evaluate opposite-of-best (opposition-based sample)\n      - levy_prob: probability to attempt a single Cauchy jump from global best\n      - seed: RNG seed\n    Principles:\n      - Strictly sequential, low-degree probes: each coordinate visit uses at most 2 evals (first preferred sign then maybe opposite),\n        diagonal probes are single-eval, opposition and jumps are single-eval. This reduces the max degree while keeping adaptive behavior.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.20, success_mult=1.35, failure_mult=0.65,\n                 diag_prob=0.12, opp_prob=0.06, levy_prob=0.03, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        if pop_size is None:\n            # keep compact population depending on dim\n            self.pop_size = max(3, min(8, int(round(4 + 0.06 * self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.diag_prob = float(diag_prob)\n        self.opp_prob = float(opp_prob)\n        self.levy_prob = float(levy_prob)\n\n    def _get_bounds(self, func):\n        # default [-5,5] or use func.bounds if provided (common pattern in benchmarks)\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n\n        evals = 0\n\n        # initialize anchors uniformly across the box\n        anchors = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        anchor_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            anchor_f[i] = float(func(anchors[i]))\n            evals += 1\n\n        # if budget exhausted during initialization, return best seen\n        if evals >= self.budget:\n            finite_idx = np.where(np.isfinite(anchor_f))[0]\n            if finite_idx.size == 0:\n                # fallback (shouldn't normally happen since budget>=1)\n                self.x_opt = np.asarray(anchors[0], dtype=float)\n                self.f_opt = float(anchor_f[0])\n            else:\n                bi = int(np.argmin(anchor_f[finite_idx]))\n                idx = int(finite_idx[bi])\n                self.x_opt = np.asarray(anchors[idx], dtype=float)\n                self.f_opt = float(anchor_f[idx])\n            return self.f_opt, self.x_opt\n\n        # per-anchor per-dimension step sizes\n        steps = np.full((self.pop_size, dim), base_step)\n        # last successful sign memory for each anchor & coordinate: -1, 0, +1\n        last_sign = np.zeros((self.pop_size, dim), dtype=float)\n        # stagnation counter\n        stagn_count = np.zeros(self.pop_size, dtype=int)\n\n        # global best\n        idx_best = int(np.argmin(anchor_f))\n        x_best = anchors[idx_best].copy()\n        f_best = float(anchor_f[idx_best])\n\n        # main loop: sequential, low-degree probes\n        while evals < self.budget:\n            improved_any = False\n\n            # occasional opposition evaluation of global best (single eval)\n            if rng.random() < self.opp_prob and evals < self.budget:\n                x_opp = np.clip(lb + ub - x_best, lb, ub)\n                # avoid evaluating identical point\n                if not np.allclose(x_opp, x_best):\n                    f_opp = float(func(x_opp))\n                    evals += 1\n                    if f_opp < f_best:\n                        f_best = f_opp\n                        x_best = x_opp.copy()\n                        improved_any = True\n                        # replace worst anchor with this good opposite (cheap diversification)\n                        iw = int(np.argmax(anchor_f))\n                        anchors[iw] = x_opp.copy()\n                        anchor_f[iw] = f_opp\n                        steps[iw, :] = base_step\n                    # continue; single eval only\n\n            # rare single-eval Cauchy jump from best\n            if rng.random() < self.levy_prob and evals < self.budget:\n                u = rng.random(dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.14 * box_len * cauchy\n                x_jump = np.clip(x_best + jump, lb, ub)\n                if not np.allclose(x_jump, x_best):\n                    f_jump = float(func(x_jump))\n                    evals += 1\n                    if f_jump < f_best:\n                        f_best = f_jump\n                        x_best = x_jump.copy()\n                        improved_any = True\n                        iw = int(np.argmax(anchor_f))\n                        anchors[iw] = x_jump.copy()\n                        anchor_f[iw] = f_jump\n                        steps[iw, :] = base_step\n\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x0 = anchors[idx].copy()\n                f0 = anchor_f[idx]\n                improved_anchor = False\n\n                # random coordinate order\n                coords = rng.permutation(dim)\n\n                for c in coords:\n                    if evals >= self.budget:\n                        break\n\n                    delta = steps[idx, c]\n                    if delta <= 1e-12 * box_len:\n                        # too small to probe meaningfully\n                        continue\n\n                    # preferred sign: last success or random if none\n                    pref_sign = last_sign[idx, c] if last_sign[idx, c] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n\n                    # first try preferred sign (single eval)\n                    x_try = x0.copy()\n                    x_try[c] = np.clip(x0[c] + pref_sign * delta, lb[c], ub[c])\n                    # skip if identical due to clipping\n                    if np.allclose(x_try, x0):\n                        # try opposite direction instead as single eval if meaningful\n                        pref_sign = -pref_sign\n                        x_try[c] = np.clip(x0[c] + pref_sign * delta, lb[c], ub[c])\n                        if np.allclose(x_try, x0):\n                            # both clipped to same value, treat as no-op\n                            last_sign[idx, c] = 0.0\n                            steps[idx, c] *= self.failure_mult\n                            stagn_count[idx] += 1\n                            continue\n\n                    f_try = float(func(x_try))\n                    evals += 1\n\n                    if f_try < f0:\n                        # accept this single-eval improvement\n                        anchors[idx] = x_try.copy()\n                        anchor_f[idx] = f_try\n                        x0 = x_try.copy(); f0 = f_try\n                        last_sign[idx, c] = pref_sign\n                        steps[idx, c] *= self.success_mult\n                        improved_anchor = True\n                        stagn_count[idx] = 0\n\n                        # attempt a single extrapolation (one additional eval) along same sign (still low-degree)\n                        if evals < self.budget:\n                            x_ex = x0.copy()\n                            x_ex[c] = np.clip(x0[c] + pref_sign * steps[idx, c], lb[c], ub[c])\n                            if not np.allclose(x_ex, x0):\n                                f_ex = float(func(x_ex))\n                                evals += 1\n                                if f_ex < f0:\n                                    anchors[idx] = x_ex.copy()\n                                    anchor_f[idx] = f_ex\n                                    x0 = x_ex.copy(); f0 = f_ex\n                                    steps[idx, c] *= self.success_mult\n                                    stagn_count[idx] = 0\n                                else:\n                                    # small conservative shrink to avoid runaway growth\n                                    steps[idx, c] *= 0.92\n                        # continue to next coordinate\n                        continue\n\n                    # first attempt failed; try opposite sign only if budget remains (single eval)\n                    opp_sign = -pref_sign\n                    x_opp = x0.copy()\n                    x_opp[c] = np.clip(x0[c] + opp_sign * delta, lb[c], ub[c])\n                    if np.allclose(x_opp, x0):\n                        # both directions clipped to same point -> shrink and continue\n                        last_sign[idx, c] = 0.0\n                        steps[idx, c] *= self.failure_mult\n                        stagn_count[idx] += 1\n                        continue\n\n                    f_opp = float(func(x_opp))\n                    evals += 1\n\n                    if f_opp < f0:\n                        anchors[idx] = x_opp.copy()\n                        anchor_f[idx] = f_opp\n                        x0 = x_opp.copy(); f0 = f_opp\n                        last_sign[idx, c] = opp_sign\n                        steps[idx, c] *= self.success_mult\n                        improved_anchor = True\n                        stagn_count[idx] = 0\n                        # single extrapolation attempt in the same opp sign if budget allows\n                        if evals < self.budget:\n                            x_ex2 = x0.copy()\n                            x_ex2[c] = np.clip(x0[c] + opp_sign * steps[idx, c], lb[c], ub[c])\n                            if not np.allclose(x_ex2, x0):\n                                f_ex2 = float(func(x_ex2))\n                                evals += 1\n                                if f_ex2 < f0:\n                                    anchors[idx] = x_ex2.copy()\n                                    anchor_f[idx] = f_ex2\n                                    x0 = x_ex2.copy(); f0 = f_ex2\n                                    steps[idx, c] *= self.success_mult\n                                else:\n                                    steps[idx, c] *= 0.92\n                        continue\n\n                    # neither direction helped: shrink and forget sign\n                    last_sign[idx, c] = 0.0\n                    steps[idx, c] *= self.failure_mult\n                    stagn_count[idx] += 1\n\n                # single-eval diagonal probe combining two most \"active\" coords (at most one eval)\n                if dim >= 2 and rng.random() < self.diag_prob and evals < self.budget:\n                    # pick top two coordinates by step size\n                    order_coords = np.argsort(steps[idx])[::-1]\n                    a = int(order_coords[0])\n                    b = int(order_coords[1]) if order_coords.size > 1 else a\n                    if a != b:\n                        # choose signs from last_sign or random nonzero\n                        sa = last_sign[idx, a] if last_sign[idx, a] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                        sb = last_sign[idx, b] if last_sign[idx, b] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                        x_diag = x0.copy()\n                        x_diag[a] = np.clip(x0[a] + sa * steps[idx, a], lb[a], ub[a])\n                        x_diag[b] = np.clip(x0[b] + sb * steps[idx, b], lb[b], ub[b])\n                        # ensure it's not identical to x0\n                        if not np.allclose(x_diag, x0):\n                            f_diag = float(func(x_diag))\n                            evals += 1\n                            if f_diag < anchor_f[idx]:\n                                anchors[idx] = x_diag.copy()\n                                anchor_f[idx] = f_diag\n                                x0 = x_diag.copy(); f0 = f_diag\n                                # reward both coordinates mildly\n                                steps[idx, a] *= 1.15\n                                steps[idx, b] *= 1.15\n                                last_sign[idx, a] = sa\n                                last_sign[idx, b] = sb\n                                improved_anchor = True\n                                stagn_count[idx] = 0\n                            else:\n                                # small penalty\n                                steps[idx, a] *= self.failure_mult\n                                steps[idx, b] *= self.failure_mult\n\n                # update global best if anchor changed\n                if anchor_f[idx] < f_best:\n                    f_best = float(anchor_f[idx])\n                    x_best = anchors[idx].copy()\n                    improved_any = True\n\n                # stagnation handling: if anchor hasn't improved for a while, respawn near best\n                if stagn_count[idx] > max(10, 4 * dim) and evals < self.budget:\n                    stagn_count[idx] = 0\n                    # small gaussian perturbation around best or a uniform injection with some probability\n                    if rng.random() < 0.6:\n                        newp = x_best + 0.05 * (ub - lb) * rng.standard_normal(dim)\n                    else:\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    f_new = float(func(newp))\n                    evals += 1\n                    anchors[idx] = newp.copy()\n                    anchor_f[idx] = f_new\n                    steps[idx, :] = base_step\n                    last_sign[idx, :] = 0.0\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy(); improved_any = True\n\n            # clip steps to reasonable bounds\n            min_step = 1e-10 * box_len\n            max_step = 1.4 * box_len\n            steps = np.clip(steps, min_step, max_step)\n\n            # if no improvement for the full pass, occasionally replace worst anchor with a global-restart sample\n            if not improved_any and evals < self.budget and rng.random() < 0.14:\n                iw = int(np.argmax(anchor_f))\n                new_rand = rng.uniform(lb, ub)\n                f_rand = float(func(new_rand))\n                evals += 1\n                anchors[iw] = new_rand\n                anchor_f[iw] = f_rand\n                steps[iw, :] = base_step\n                last_sign[iw, :] = 0.0\n                if f_rand < f_best:\n                    f_best = f_rand; x_best = new_rand.copy(); improved_any = True\n\n            # small budget-aware break: if only a few evals left, go to final refinement\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final greedy coordinate mesh refinement on x_best (very low-degree: 1 eval per direction)\n        mesh = 0.10 * box_len\n        mesh_min = 1e-8 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                xp = x_best.copy()\n                xp[d] = np.clip(x_best[d] + mesh, lb[d], ub[d])\n                if not np.allclose(xp, x_best):\n                    fp = float(func(xp)); evals += 1\n                    if fp < f_best:\n                        f_best = fp; x_best = xp.copy(); improved = True\n                        # continue to next dimension (no opposite test to keep degree low)\n                        continue\n                if evals >= self.budget:\n                    break\n                xm = x_best.copy()\n                xm[d] = np.clip(x_best[d] - mesh, lb[d], ub[d])\n                if not np.allclose(xm, x_best):\n                    fm = float(func(xm)); evals += 1\n                    if fm < f_best:\n                        f_best = fm; x_best = xm.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # store final result\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm LowDegreeProgressiveAnchors scored 0.520 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6e79ae04-b7ad-446c-8f86-291cfd1fc410"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8482731583118917}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8565256632734077}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8590904670381598}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.43320173161799924}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.48483470403097007}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.4601292333393936}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1645160139650701}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.13460226105815787}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.16084037873629875}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1423633125916649}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.1316077582408145}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.13274485825568294}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9990272085416445}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9988190407711102}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.999026632822251}], "aucs": [0.8482731583118917, 0.8565256632734077, 0.8590904670381598, 0.43320173161799924, 0.48483470403097007, 0.4601292333393936, 0.1645160139650701, 0.13460226105815787, 0.16084037873629875, 0.1423633125916649, 0.1316077582408145, 0.13274485825568294, 0.9990272085416445, 0.9988190407711102, 0.999026632822251]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3119.0, "Edges": 3118.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.9993587688361654, "Degree Variance": 1.974991573433046, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.99157894736842, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3313038786656928, "Depth Entropy": 2.210206811091588, "Assortativity": 0.0, "Average Eccentricity": 19.49887784546329, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0003206155819172812, "Average Shortest Path": 11.150409016228414, "mean_complexity": 23.666666666666668, "total_complexity": 71.0, "mean_token_count": 856.3333333333334, "total_token_count": 2569.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "cdd8dd13-2a18-4dc0-8671-123b00e5276f", "fitness": 0.5790923144563249, "name": "SeparableSignedGradientPush", "description": "Separable Signed-Gradient Push (SSGP) \u2014 maintain lightweight per-dimension signed slope estimates (single-sided finite differences), perform strictly linear (degree-1) separable pushes guided by sign-consensus and confidence, adapt per-dimension step sizes by simple success/failure rules, and use cheap sign-projection moves and opposition/global probes for diversification.", "code": "import numpy as np\n\nclass SeparableSignedGradientPush:\n    \"\"\"\n    Separable Signed-Gradient Push (SSGP)\n\n    Main idea:\n      - Keep a small archive of evaluated points and per-dimension state:\n        a signed slope estimate and a confidence/weight for that estimate.\n      - Use only degree-1 moves: single-evaluation linear pushes along axes\n        (or simple signed linear combinations of axes). Initial slope\n        information is gathered with single-sided probes (one evaluation per dim).\n      - Moves are separable: combine per-dimension signed pushes into\n        candidate vectors (no quadratic fitting). Adapt per-dimension radii\n        multiplicatively on success/failure. Occasionally perform a\n        sign-consensus directional push (projection of sign pattern) and\n        cheap global/opposition probes for diversification.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10,\n                 init_radius=0.8, min_radius=1e-6, max_radius=5.0,\n                 init_samples=None, max_block=3, elite_k=6,\n                 stagnation_restart=10, rng=None):\n        \"\"\"\n        budget: budget of function evaluations\n        dim: dimensionality\n        init_radius: starting per-dimension step scale\n        min_radius, max_radius: bounds for radii\n        init_samples: initial random samples (defaults to small fraction)\n        max_block: maximum number of dims to push at once (keeps low-degree)\n        elite_k: selection pool size for center picking\n        stagnation_restart: iterations of no improvement that trigger diversification\n        rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.max_block = int(max(1, min(max_block, dim)))\n        self.elite_k = int(elite_k)\n        self.stagnation_restart = int(stagnation_restart)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        if init_samples is None:\n            # keep a small initial archive but not too large relative to budget\n            self.init_samples = int(min(40, max(6, self.budget // 12)))\n        else:\n            self.init_samples = int(init_samples)\n\n    def _get_bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n        if ub.shape[0] != self.dim:\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        X = []\n        F = []\n\n        # per-dimension radii and signed-slope estimates and confidence (0..1)\n        radii = np.full(self.dim, self.init_radius, dtype=float)\n        slope = np.zeros(self.dim, dtype=float)       # signed slope (finite-diff)\n        conf = np.zeros(self.dim, dtype=float)        # confidence in slope estimate (0..1)\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise RuntimeError(\"Budget exhausted\")\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # Initial archive: diverse random samples\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = safe_eval(x)\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n\n        # ensure at least one sample\n        if len(X) == 0 and evals < self.budget:\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = safe_eval(x)\n            X.append(np.array(x, dtype=float)); F.append(f)\n\n        best_idx = int(np.argmin(F))\n        best_x = X[best_idx].copy()\n        best_f = F[best_idx]\n\n        # initial single-sided slope probes on a small subset of dims (one eval per dim)\n        remaining = self.budget - evals\n        probe_dims = min(self.dim, max(1, remaining // 20, 3))\n        # pick dims randomly to probe (to not exhaust budget)\n        dims_to_probe = list(self.rng.choice(self.dim, size=probe_dims, replace=False))\n        center_x = best_x.copy()\n        center_f = best_f\n        for d in dims_to_probe:\n            if evals >= self.budget:\n                break\n            h = radii[d] * (0.6 + 0.8 * self.rng.random())\n            if h <= 0:\n                continue\n            x_plus = center_x.copy()\n            x_plus[d] = np.minimum(ub[d], center_x[d] + h)\n            if x_plus[d] == center_x[d]:\n                # tiny nudge\n                x_plus[d] = center_x[d] + np.sign(h) * 1e-9\n            f_plus = safe_eval(x_plus)\n            X.append(x_plus.copy()); F.append(f_plus)\n            # estimate slope (forward diff)\n            s = (f_plus - center_f) / (x_plus[d] - center_x[d] + 1e-16)\n            # small EMA style initialization\n            slope[d] = s\n            conf[d] = min(0.9, 0.3 + 0.7 * self.rng.random())\n\n            if f_plus < best_f:\n                best_f = f_plus; best_x = x_plus.copy()\n\n        stagnation = 0\n        iter_count = 0\n\n        # main loop: make strictly linear pushes\n        while evals < self.budget:\n            iter_count += 1\n            n_archive = len(X)\n            # pick center among elites with bias\n            top_k = min(self.elite_k, n_archive)\n            idx_sorted = np.argsort(F)\n            top_indices = idx_sorted[:top_k]\n            top_fs = np.array([F[i] for i in top_indices], dtype=float)\n            if top_fs.std() < 1e-12:\n                probs = np.ones(len(top_fs)) / len(top_fs)\n            else:\n                scores = -(top_fs - top_fs.min()) / (top_fs.std() + 1e-12)\n                exp_s = np.exp(scores - np.max(scores))\n                probs = exp_s / exp_s.sum()\n            center_choice = int(self.rng.choice(top_indices, p=probs))\n            center_x = X[center_choice].copy()\n            center_f = F[center_choice]\n\n            # choose block size (most often 1 to keep degree=1)\n            if self.rng.random() < 0.75:\n                block_size = 1\n            else:\n                block_size = int(self.rng.integers(1, self.max_block + 1))\n            remaining = self.budget - evals\n            # each block candidate costs 1 eval, but we will prefer small ones if budget tight\n            block_size = min(block_size, max(1, remaining // 1))\n\n            # pick dims weighted by radii * (1 - conf) so we explore uncertain dims\n            dim_scores = radii * (1.0 - conf + 1e-6)\n            if dim_scores.sum() <= 0:\n                dim_probs = np.ones(self.dim) / self.dim\n            else:\n                dim_probs = dim_scores / dim_scores.sum()\n            block_dims = list(self.rng.choice(self.dim, size=block_size, replace=False, p=dim_probs))\n\n            # build separable push: each dim gets sign based on slope sign and confidence\n            shift = np.zeros(self.dim, dtype=float)\n            used_dims = []\n            for d in block_dims:\n                # determine direction\n                if conf[d] > 0.2:\n                    # trust sign: move opposite to slope (descent)\n                    if slope[d] == 0:\n                        sign = -1 if self.rng.random() < 0.5 else 1\n                    else:\n                        sign = -np.sign(slope[d])\n                else:\n                    # low confidence: randomize but prefer negative of slope sign if available\n                    if slope[d] == 0 or self.rng.random() < 0.4:\n                        sign = -1 if self.rng.random() < 0.5 else 1\n                    else:\n                        sign = -np.sign(slope[d]) if self.rng.random() < 0.75 else np.sign(slope[d])\n                # step size scaled by radii and a random factor\n                h = radii[d] * (0.5 + self.rng.random() * 1.0)  # between 0.5..1.5 * radius\n                if h <= 1e-12:\n                    continue\n                shift[d] = sign * h\n                used_dims.append(d)\n\n            if len(used_dims) == 0:\n                # fallback: one small random probe\n                if evals >= self.budget:\n                    break\n                x_probe = np.clip(center_x + (self.rng.normal(size=self.dim) * (0.2 * np.mean(radii))), lb, ub)\n                f_probe = safe_eval(x_probe)\n                X.append(x_probe.copy()); F.append(f_probe)\n                if f_probe < best_f:\n                    best_f = f_probe; best_x = x_probe.copy(); stagnation = 0\n                else:\n                    stagnation += 1\n                # slightly decrease all radii to encourage different moves\n                radii *= 0.98\n                radii = np.clip(radii, self.min_radius, self.max_radius)\n                continue\n\n            # assemble candidate (separable linear push)\n            x_cand = center_x + shift\n            # enforce max displacement scale (avoid huge multi-dim jumps)\n            max_allowed = max(1e-12, 2.0 * np.mean(radii))\n            disp_norm = np.linalg.norm(x_cand - center_x)\n            if disp_norm > max_allowed:\n                x_cand = center_x + (x_cand - center_x) * (max_allowed / (disp_norm + 1e-16))\n            x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n\n            # evaluate candidate\n            if evals >= self.budget:\n                break\n            f_cand = safe_eval(x_cand)\n            X.append(x_cand.copy()); F.append(f_cand)\n\n            # update per-dim slope estimates using simple finite-diff attribution:\n            # for dims that were moved, we attribute the function change along that dim proportionally\n            delta_f = f_cand - center_f\n            # compute total absolute shift to proportionally assign credit\n            abs_shifts = np.abs(shift[used_dims])\n            total_abs = abs_shifts.sum() + 1e-16\n            for i, d in enumerate(used_dims):\n                s_obs = delta_f * (abs_shifts[i] / total_abs) / (shift[d] + 1e-16)  # approximate partial derivative\n                # update EMA: learning rate tied to confidence (low conf -> larger learning)\n                eta = 0.25 * (1.0 - conf[d]) + 0.05\n                slope[d] = (1.0 - eta) * slope[d] + eta * s_obs\n                # increase confidence if we actually moved in that dim and change was informative\n                conf[d] = min(1.0, conf[d] + 0.08 + 0.2 * (1.0 - conf[d]) * (abs_shifts[i] / (radii[d] + 1e-16)))\n\n            # adapt radii based on improvement relative to center\n            improvement = center_f - f_cand\n            if improvement > 1e-12:\n                # success: increase radii for used dims\n                for d in used_dims:\n                    factor = 1.10 + 0.2 * self.rng.random()\n                    radii[d] = min(self.max_radius, radii[d] * factor)\n                stagnation = 0\n            else:\n                # failure or no improvement: shrink radii for used dims\n                for d in used_dims:\n                    factor = 0.45 + 0.45 * self.rng.random()\n                    radii[d] = max(self.min_radius, radii[d] * factor)\n                stagnation += 1\n\n            # update best if improved overall\n            if f_cand < best_f:\n                best_f = f_cand; best_x = x_cand.copy(); stagnation = 0\n\n            # Occasionally perform a sign-consensus directional push (still linear)\n            if evals < self.budget and self.rng.random() < 0.12:\n                # consensus direction from slope signs weighted by confidence and radii\n                signs = -np.sign(slope)  # negative slope suggests decrease in positive direction\n                weights = conf * radii\n                if weights.sum() <= 1e-12:\n                    direc = (self.rng.normal(size=self.dim))\n                else:\n                    direc = signs * weights\n                # normalize and scale\n                norm = np.linalg.norm(direc)\n                if norm < 1e-12:\n                    direc = self.rng.normal(size=self.dim)\n                    norm = np.linalg.norm(direc)\n                direc = direc / (norm + 1e-16)\n                h_dir = np.mean(radii) * (0.6 + 0.9 * self.rng.random())\n                x_dir = np.clip(center_x + direc * h_dir, lb, ub)\n                if evals < self.budget:\n                    f_dir = safe_eval(x_dir); X.append(x_dir.copy()); F.append(f_dir)\n                    # update slope estimates projecting the observed change onto the direction\n                    df = f_dir - center_f\n                    if abs(h_dir) > 1e-16:\n                        # attribute to dims proportionally to absolute direc components\n                        abs_comp = np.abs(direc)\n                        denom = abs_comp.sum() + 1e-16\n                        for d in range(self.dim):\n                            frac = abs_comp[d] / denom\n                            s_obs = df * frac / (direc[d] * h_dir + 1e-16)\n                            eta = 0.12 * (1.0 - conf[d]) + 0.02\n                            slope[d] = (1.0 - eta) * slope[d] + eta * s_obs\n                            conf[d] = min(1.0, conf[d] + 0.04 * frac)\n                    if f_dir < best_f:\n                        best_f = f_dir; best_x = x_dir.copy(); stagnation = 0\n                    else:\n                        stagnation += 1\n\n            # If stagnation persists, do cheap diversification: global samples and opposition\n            if stagnation >= self.stagnation_restart and evals < self.budget:\n                n_global = min(4, max(1, self.dim // 3))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg = safe_eval(xg); X.append(xg.copy()); F.append(fg)\n                    if fg < best_f:\n                        best_f = fg; best_x = xg.copy(); stagnation = 0\n                # opposite of best_x w.r.t center of bounds\n                if evals < self.budget:\n                    center_bounds = 0.5 * (lb + ub)\n                    xopp = np.clip(center_bounds - (best_x - center_bounds), lb, ub)\n                    fopp = safe_eval(xopp); X.append(xopp.copy()); F.append(fopp)\n                    if fopp < best_f:\n                        best_f = fopp; best_x = xopp.copy(); stagnation = 0\n                # mild randomization of radii to escape local plateaus\n                radii = np.clip(radii * (0.6 + 0.8 * self.rng.random(self.dim)), self.min_radius, self.max_radius)\n                # slightly reduce slope confidence to encourage re-learning\n                conf = conf * 0.6\n                stagnation = 0\n\n            # Archive trimming\n            max_archive = max(200, 6 * self.dim + 50)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = int(max(30, 3 * self.dim))\n                keep_set = set(idx_sorted[:keep_best].tolist())\n                all_f = np.array(F)\n                # probabilistic extras preferring better points\n                std = all_f.std()\n                if std < 1e-12:\n                    probs = np.ones(len(all_f)) / len(all_f)\n                else:\n                    probs = np.exp(-(all_f - all_f.min()) / (std + 1e-12))\n                    probs = probs / probs.sum()\n                extras_needed = min(len(X) - keep_best, max_archive - keep_best)\n                if extras_needed > 0:\n                    extra_idx = self.rng.choice(len(X), size=extras_needed, replace=False, p=probs)\n                    for e in extra_idx:\n                        keep_set.add(int(e))\n                keep_list = sorted(list(keep_set))\n                X = [X[i] for i in keep_list]\n                F = [F[i] for i in keep_list]\n                # keep per-dim state unchanged (global across archive)\n\n        # finalize and return best found\n        best_idx = int(np.argmin(F))\n        return float(F[best_idx]), np.array(X[best_idx], dtype=float)", "configspace": "", "generation": 12, "feedback": "The algorithm SeparableSignedGradientPush scored 0.579 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["52858a4a-25dd-4a00-a79d-aaffdc317198"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9439806195121596}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9425922194721683}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.943613524423029}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8712997303495333}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8643907172479526}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8928068822034766}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06557090250332098}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.020981857952931726}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.06973844873918}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.03567847748216013}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.052124523732107964}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9942941052319835}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9945554145811561}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.994707293413713}], "aucs": [0.9439806195121596, 0.9425922194721683, 0.943613524423029, 0.8712997303495333, 0.8643907172479526, 0.8928068822034766, 0.06557090250332098, 0.020981857952931726, 0.06973844873918, 0.03567847748216013, 0.052124523732107964, 4.999999999999449e-05, 0.9942941052319835, 0.9945554145811561, 0.994707293413713]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3131.0, "Edges": 3130.0, "Max Degree": 42.0, "Min Degree": 1.0, "Mean Degree": 1.9993612264452252, "Degree Variance": 2.270839579192874, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.163584637268848, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.323036321983201, "Depth Entropy": 2.0849352581914733, "Assortativity": 3.83788726232413e-09, "Average Eccentricity": 16.961992973490897, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00031938677738741617, "Average Shortest Path": 10.196394500833161, "mean_complexity": 16.75, "total_complexity": 67.0, "mean_token_count": 692.0, "total_token_count": 2768.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "baa412d0-e0e0-4c35-b441-2d77e6b55032", "fitness": 0.5619752494504002, "name": "MicroMomentumProgressiveCoordinateSearch", "description": "Micro-Momentum Progressive Coordinate Search (MPCS) \u2014 extremely low-degree search that uses single-eval momentum-biased 1-D probes, occasional single-eval diagonal (pairwise) moves, and rare heavy-tail escapes; per-dimension momentum preserves successful signs and steps adapt multiplicatively to keep information per evaluation high.", "code": "import numpy as np\n\nclass MicroMomentumProgressiveCoordinateSearch:\n    \"\"\"\n    Micro-Momentum Progressive Coordinate Search (MPCS)\n\n    Key ideas:\n    - Keep operations extremely low-degree: mostly single-eval 1-D probes.\n    - Maintain per-anchor, per-dimension step sizes and a light \"momentum\" (preferred sign) memory.\n    - Use single-eval pairwise diagonal moves (not exhaustive 4-point scans) guided by momentum.\n    - Rare Cauchy-like heavy-tail jumps and occasional small gaussian respawns for diversification.\n    - Budget-respecting safe evaluation wrapper (never calls func when budget exhausted).\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.25, success_mult=1.30, failure_mult=0.6,\n                 pair_prob=0.14, heavy_prob=0.03, respawn_prob=0.10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        if pop_size is None:\n            self.pop_size = max(3, min(8, int(round(5 + 0.06 * self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        # adaptation params\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.pair_prob = float(pair_prob)\n        self.heavy_prob = float(heavy_prob)\n        self.respawn_prob = float(respawn_prob)\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        # scale relative to box (use L2 length)\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n\n        evals = 0\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            # ensure clipped\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            fv = float(func(x))\n            evals += 1\n            return fv\n\n        # initialize anchors uniformly (but respect budget)\n        anchors = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        anchor_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            anchor_f[i] = safe_eval(anchors[i])\n\n        # per-anchor per-dimension steps and momentum memory in [-1,1]\n        steps = np.full((self.pop_size, dim), base_step)\n        # momentum prefers sign: positive => +1, negative => -1, magnitude small means uncertain\n        momentum = np.zeros((self.pop_size, dim), dtype=float)\n        # stagnation counters\n        stagn = np.zeros(self.pop_size, dtype=int)\n\n        # best tracking\n        idx_best = int(np.nanargmin(anchor_f))\n        x_best = anchors[idx_best].copy()\n        f_best = float(anchor_f[idx_best])\n\n        # main loop: low-degree passes over anchors, single-eval probes\n        while evals < self.budget:\n            improved_any = False\n\n            # rare heavy-tail jump from best (single-eval)\n            if rng.random() < self.heavy_prob and evals < self.budget:\n                # Cauchy-like jump scaled to box\n                u = rng.random(dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.18 * box_len * cauchy\n                xj = np.clip(x_best + jump, lb, ub)\n                fj = safe_eval(xj)\n                if fj < f_best:\n                    f_best = fj; x_best = xj.copy(); improved_any = True\n                    # replace worst anchor\n                    idx_w = int(np.argmax(anchor_f))\n                    anchors[idx_w] = xj.copy()\n                    anchor_f[idx_w] = fj\n                    steps[idx_w, :] = base_step\n                    momentum[idx_w, :] = 0.0\n\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x0 = anchors[idx].copy()\n                f0 = anchor_f[idx]\n                improved_anchor = False\n\n                # random coordinate ordering (helps diversify)\n                coords = rng.permutation(dim)\n\n                # For each coordinate, we perform at most one evaluation:\n                # choose a sign biased by momentum; evaluate single direction only.\n                for c in coords:\n                    if evals >= self.budget:\n                        break\n\n                    delta = steps[idx, c]\n                    if delta <= 1e-12 * box_len:\n                        # too small to be meaningful; mark stagnation incrementally\n                        stagn[idx] += 1\n                        continue\n\n                    # determine preferred sign: use momentum (probabilistic)\n                    m = momentum[idx, c]\n                    pref_sign = 1 if m >= 0 else -1\n                    if rng.random() < (0.5 + 0.4 * abs(m)):  # stronger momentum -> more likely prefer sign\n                        sign = pref_sign\n                    else:\n                        sign = 1 if rng.random() < 0.5 else -1\n\n                    x_try = x0.copy()\n                    x_try[c] = np.clip(x0[c] + sign * delta, lb[c], ub[c])\n\n                    f_try = safe_eval(x_try)\n                    if f_try < f0:\n                        # accept\n                        anchors[idx] = x_try.copy()\n                        anchor_f[idx] = f_try\n                        x0 = x_try.copy(); f0 = f_try\n                        steps[idx, c] *= self.success_mult\n                        # strengthen momentum toward this sign\n                        momentum[idx, c] = 0.7 * momentum[idx, c] + 0.3 * float(sign)\n                        stagn[idx] = 0\n                        improved_anchor = True\n                        # continue to next coordinate (no second eval on same coordinate)\n                        continue\n                    else:\n                        # failure: shrink and nudge momentum away slightly\n                        steps[idx, c] *= self.failure_mult\n                        momentum[idx, c] = 0.9 * momentum[idx, c]  # decay momentum magnitude slowly\n                        stagn[idx] += 1\n\n                # single-eval pairwise diagonal: pick two coords and evaluate one diagonal move guided by momentum\n                if (rng.random() < self.pair_prob) and (evals < self.budget) and dim >= 2:\n                    # pick two coords favoring larger steps\n                    scores = steps[idx, :] + 1e-12\n                    probs = scores / scores.sum()\n                    a = rng.choice(dim, p=probs)\n                    b = a\n                    # pick b different from a\n                    while b == a:\n                        b = rng.choice(dim, p=probs)\n                    # determine diagonal sign for each using momentum (no extra evals)\n                    sa = 1 if momentum[idx, a] >= 0 else -1\n                    sb = 1 if momentum[idx, b] >= 0 else -1\n                    # small chance to invert one sign to encourage exploration\n                    if rng.random() < 0.12:\n                        sa *= -1\n                    if rng.random() < 0.12:\n                        sb *= -1\n\n                    x_diag = anchors[idx].copy()\n                    x_diag[a] = np.clip(x_diag[a] + sa * steps[idx, a], lb[a], ub[a])\n                    x_diag[b] = np.clip(x_diag[b] + sb * steps[idx, b], lb[b], ub[b])\n\n                    f_diag = safe_eval(x_diag)\n                    if f_diag < anchor_f[idx]:\n                        anchors[idx] = x_diag.copy()\n                        anchor_f[idx] = f_diag\n                        steps[idx, a] *= self.success_mult\n                        steps[idx, b] *= self.success_mult\n                        momentum[idx, a] = 0.7 * momentum[idx, a] + 0.3 * float(sa)\n                        momentum[idx, b] = 0.7 * momentum[idx, b] + 0.3 * float(sb)\n                        stagn[idx] = 0\n                        improved_anchor = True\n                    else:\n                        # penalize both steps mildly\n                        steps[idx, a] *= self.failure_mult\n                        steps[idx, b] *= self.failure_mult\n                        momentum[idx, a] *= 0.95\n                        momentum[idx, b] *= 0.95\n                        stagn[idx] += 1\n\n                # update global best\n                if anchor_f[idx] < f_best:\n                    f_best = float(anchor_f[idx])\n                    x_best = anchors[idx].copy()\n                    improved_any = True\n\n                # stagnation handling: respawn near best or random\n                if stagn[idx] > max(8, 2 * dim) and evals < self.budget:\n                    stagn[idx] = 0\n                    if rng.random() < self.respawn_prob:\n                        # respawn near best (small gaussian)\n                        newp = x_best + 0.04 * (ub - lb) * rng.standard_normal(dim)\n                        newp = np.clip(newp, lb, ub)\n                        f_new = safe_eval(newp)\n                        anchors[idx] = newp\n                        anchor_f[idx] = f_new\n                        steps[idx, :] = base_step\n                        momentum[idx, :] = 0.0\n                        if f_new < f_best:\n                            f_best = f_new; x_best = newp.copy(); improved_any = True\n                    else:\n                        # random injection\n                        newp = rng.uniform(lb, ub)\n                        f_new = safe_eval(newp)\n                        anchors[idx] = newp\n                        anchor_f[idx] = f_new\n                        steps[idx, :] = base_step\n                        momentum[idx, :] = 0.0\n                        if f_new < f_best:\n                            f_best = f_new; x_best = newp.copy(); improved_any = True\n\n            # maintain step bounds to avoid runaway\n            min_step = 1e-9 * box_len\n            max_step = 1.5 * box_len\n            steps = np.clip(steps, min_step, max_step)\n\n            # occasional simple replacement of worst with a small opposite of best (opposition)\n            if (not improved_any) and (evals < self.budget) and rng.random() < 0.12:\n                idx_w = int(np.argmax(anchor_f))\n                oppos = lb + ub - x_best  # simple opposition point\n                oppos = np.clip(oppos + 0.02 * (ub - lb) * rng.standard_normal(dim), lb, ub)\n                f_op = safe_eval(oppos)\n                anchors[idx_w] = oppos\n                anchor_f[idx_w] = f_op\n                steps[idx_w, :] = base_step\n                momentum[idx_w, :] = 0.0\n                if f_op < f_best:\n                    f_best = f_op; x_best = oppos.copy()\n\n            # break early to final refine when budget low\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final low-degree refinement around x_best: single-eval coordinate moves (one-sided)\n        mesh = 0.10 * box_len\n        mesh_min = 1e-7 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # try a single-sided positive probe (less degree)\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = safe_eval(x_try)\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                # else try negative only if budget allows and with lower priority\n                if evals < self.budget:\n                    x_try = x_best.copy()\n                    x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                    f_try = safe_eval(x_try)\n                    if f_try < f_best:\n                        f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # final results\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm MicroMomentumProgressiveCoordinateSearch scored 0.562 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6e79ae04-b7ad-446c-8f86-291cfd1fc410"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8801136979938966}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8695932364031639}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8694136271715545}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.7966945203416071}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.7976629752919049}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.813776384312991}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.053249934463986404}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.12333420424772212}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.05278582569911594}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.08503253752287887}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.08711087603379986}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07556906518846318}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9907392891062423}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9611468573358491}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9734057106428282}], "aucs": [0.8801136979938966, 0.8695932364031639, 0.8694136271715545, 0.7966945203416071, 0.7976629752919049, 0.813776384312991, 0.053249934463986404, 0.12333420424772212, 0.05278582569911594, 0.08503253752287887, 0.08711087603379986, 0.07556906518846318, 0.9907392891062423, 0.9611468573358491, 0.9734057106428282]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2408.0, "Edges": 2407.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.9991694352159468, "Degree Variance": 2.106311602520943, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.403967538322814, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3243739386855369, "Depth Entropy": 2.049863058639208, "Assortativity": 1.7103563399124676e-08, "Average Eccentricity": 18.697674418604652, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0004152823920265781, "Average Shortest Path": 10.34924714322981, "mean_complexity": 13.5, "total_complexity": 54.0, "mean_token_count": 502.75, "total_token_count": 2011.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "e2eb7d65-71fa-4031-ae0e-0f24195e0924", "fitness": 0.362364014204707, "name": "StochasticAdaptiveCoordinatePush", "description": "Stochastic Adaptive Coordinate Push (SACP) \u2014 strictly 1-D adaptive coordinate pushes with per-coordinate step scales and success-biased coordinate sampling; uses only single-coordinate perturbations (degree-1) and lightweight opposition/diversification on stagnation.", "code": "import numpy as np\n\nclass StochasticAdaptiveCoordinatePush:\n    \"\"\"\n    Stochastic Adaptive Coordinate Push (SACP)\n\n    Main idea:\n      - Maintain a single incumbent x_best and per-coordinate step sizes.\n      - Repeatedly pick one coordinate (biased by recent successes) and perform\n        1-D probes (forward then optionally backward). All evaluated moves change\n        only a single coordinate (max interaction degree = 1).\n      - Increase the step on success, shrink it on failure. Bias future picks toward\n        coordinates that recently produced improvements.\n      - On stagnation perform cheap opposition checks and a few global random samples.\n      - Strictly respect the evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, init_step=None,\n                 min_step=1e-8, max_step=None, stagnation_patience=8,\n                 rng=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          init_step: initial per-dim step (scalar or array). If None, set to (ub-lb)/8.\n          min_step: minimum allowed step\n          max_step: maximum allowed step (if None, set to (ub-lb))\n          stagnation_patience: iterations without improvement before diversification\n          rng: optional numpy.random.Generator for reproducibility\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step  # may be None until bounds known\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shapes\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        # initialize step sizes relative to bounds if not provided\n        span = ub - lb\n        span[span <= 0] = 1.0  # guard\n        if self.init_step is None:\n            init_step_arr = 0.125 * span  # moderate starting step (1/8 of range)\n        else:\n            init_step_arr = np.array(self.init_step)\n            if init_step_arr.shape == ():\n                init_step_arr = np.full(self.dim, float(init_step_arr))\n            # clip to sensible range\n            init_step_arr = np.minimum(np.maximum(init_step_arr, self.min_step), 0.5 * span)\n        if self.max_step is None:\n            max_step_arr = span.copy()\n        else:\n            max_step_arr = np.array(self.max_step)\n            if max_step_arr.shape == ():\n                max_step_arr = np.full(self.dim, float(max_step_arr))\n\n        step = init_step_arr.astype(float)\n        max_step_arr = max_step_arr.astype(float)\n\n        evals = 0\n        # initial incumbent\n        x_best = lb + self.rng.random(self.dim) * span\n        f_best = float(func(x_best))\n        evals += 1\n\n        # coordinate success tracker (recent successes bias selection)\n        # use small positive to avoid zero probabilities\n        success_score = np.full(self.dim, 1.0 / self.dim, dtype=float)\n\n        stagnation = 0  # iterations without improvement (count of coordinate selections)\n        iters = 0\n\n        # helper to pick a coordinate (softmax of success_score)\n        def pick_coord():\n            scores = np.maximum(success_score, 1e-12)\n            # sharpen occasionally to exploit promising coords\n            temp = 1.0 + 0.5 * (iters % 7 == 0)  # slight periodic sharpening\n            probs = scores ** temp\n            probs_sum = probs.sum()\n            if probs_sum <= 0 or np.isnan(probs_sum):\n                probs = np.full(self.dim, 1.0 / self.dim)\n            else:\n                probs = probs / probs_sum\n            return self.rng.choice(self.dim, p=probs)\n\n        # main loop: repeatedly attempt 1-D pushes\n        while evals < self.budget:\n            remaining = self.budget - evals\n            iters += 1\n\n            # pick coordinate to probe\n            i = pick_coord()\n\n            improved = False\n\n            # prepare single-coordinate forward probe if budget allows\n            if remaining <= 0:\n                break\n\n            # build forward candidate\n            x_fwd = x_best.copy()\n            step_val = float(step[i])\n            if step_val < self.min_step:\n                step_val = self.min_step\n            # Probing uses a randomized sign occasionally to escape plateaus\n            sign = 1.0\n            # small randomization in sign with low probability\n            if self.rng.random() < 0.03:\n                sign = -1.0\n            x_fwd[i] = x_fwd[i] + sign * step_val\n            # clip bounds\n            x_fwd[i] = float(min(max(x_fwd[i], lb[i]), ub[i]))\n\n            # evaluate forward\n            f_fwd = float(func(x_fwd))\n            evals += 1\n            remaining -= 1\n\n            if f_fwd < f_best:\n                # success: accept forward\n                f_best = f_fwd\n                x_best = x_fwd\n                improved = True\n                stagnation = 0\n                # increase step moderately (capped)\n                step[i] = min(max_step_arr[i], step[i] * (1.25 + 0.05 * self.rng.random()))\n                # reward coordinate score\n                success_score[i] = success_score[i] * 1.2 + 0.8\n                # small opportunistic exploitation: try a slightly larger single-coordinate probe\n                if remaining > 0 and step[i] * 1.5 <= max_step_arr[i]:\n                    probe = x_best.copy()\n                    probe[i] += 0.5 * step[i]\n                    probe[i] = float(min(max(probe[i], lb[i]), ub[i]))\n                    f_probe = float(func(probe))\n                    evals += 1\n                    remaining -= 1\n                    if f_probe < f_best:\n                        f_best = f_probe\n                        x_best = probe\n                        step[i] = min(max_step_arr[i], step[i] * 1.35)\n                        success_score[i] += 0.5\n                # continue main loop\n            else:\n                # no forward improvement; try backward if budget allows\n                if remaining > 0:\n                    x_bwd = x_best.copy()\n                    x_bwd[i] = x_bwd[i] - sign * step_val  # opposite direction from forward probe\n                    x_bwd[i] = float(min(max(x_bwd[i], lb[i]), ub[i]))\n                    f_bwd = float(func(x_bwd))\n                    evals += 1\n                    remaining -= 1\n                    if f_bwd < f_best:\n                        # accept backward\n                        f_best = f_bwd\n                        x_best = x_bwd\n                        improved = True\n                        stagnation = 0\n                        step[i] = min(max_step_arr[i], step[i] * (1.2 + 0.05 * self.rng.random()))\n                        success_score[i] = success_score[i] * 1.15 + 0.6\n                        # opportunistic small probe in same direction\n                        if remaining > 0 and step[i] * 1.4 <= max_step_arr[i]:\n                            probe = x_best.copy()\n                            probe[i] -= 0.4 * step[i]\n                            probe[i] = float(min(max(probe[i], lb[i]), ub[i]))\n                            f_probe = float(func(probe))\n                            evals += 1\n                            remaining -= 1\n                            if f_probe < f_best:\n                                f_best = f_probe\n                                x_best = probe\n                                step[i] = min(max_step_arr[i], step[i] * 1.3)\n                                success_score[i] += 0.4\n                    else:\n                        # failure on both sides: shrink step and penalize score slightly\n                        step[i] = max(self.min_step, step[i] * 0.55)\n                        success_score[i] = max(1e-6, success_score[i] * 0.85)\n                        stagnation += 1\n                else:\n                    # cannot try backward; treat as failure\n                    step[i] = max(self.min_step, step[i] * 0.8)\n                    success_score[i] = max(1e-6, success_score[i] * 0.9)\n                    stagnation += 1\n\n            # occasional mild global jitter to maintain exploration when progress is stalled\n            if stagnation > 0 and (stagnation % max(3, self.dim // 3) == 0) and evals < self.budget:\n                # try a single global random sample (degree = dim but only used rarely)\n                # To keep degree-1 philosophy, we instead jitter a few coordinates one at a time,\n                # but perform at most one extra evaluation here to limit degree growth.\n                x_rand = lb + self.rng.random(self.dim) * span\n                f_rand = float(func(x_rand))\n                evals += 1\n                if f_rand < f_best:\n                    f_best = f_rand\n                    x_best = x_rand\n                    stagnation = 0\n                    # reset some step sizes to medium values\n                    step = np.maximum(step, 0.5 * init_step_arr)\n                    success_score = np.maximum(success_score, 1.0 / self.dim)\n\n            # stagnation handling: opposition and small random samples\n            if stagnation >= self.stagnation_patience and evals < self.budget:\n                # opposition point: mirror within bounds\n                x_opp = lb + ub - x_best\n                x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                f_opp = float(func(x_opp))\n                evals += 1\n                if f_opp < f_best:\n                    f_best = f_opp\n                    x_best = x_opp\n                    stagnation = 0\n                    # boost steps a bit to capitalize on new region\n                    step = np.minimum(max_step_arr, step * 1.2)\n                    success_score = np.maximum(success_score, 1.0 / self.dim)\n                else:\n                    # try a small set of cheap global randoms (cap to not exceed budget)\n                    num_try = min(3, self.budget - evals)\n                    for _ in range(num_try):\n                        x_g = lb + self.rng.random(self.dim) * span\n                        f_g = float(func(x_g))\n                        evals += 1\n                        if f_g < f_best:\n                            f_best = f_g\n                            x_best = x_g\n                            stagnation = 0\n                            step = np.maximum(step, 0.5 * init_step_arr)\n                            success_score = np.maximum(success_score, 1.0 / self.dim)\n                            break\n                    # if still no improvement, gradually reset per-dim scores to uniform and enlarge steps moderately\n                    if stagnation >= self.stagnation_patience:\n                        success_score = np.full(self.dim, 1.0 / self.dim)\n                        step = np.minimum(max_step_arr, np.maximum(step, 0.5 * init_step_arr))\n                        stagnation = 0  # give fresh cycle\n\n            # tiny housekeeping: avoid numerical zeros in scores\n            success_score = np.maximum(success_score, 1e-12)\n            # cap steps\n            step = np.minimum(np.maximum(step, self.min_step), max_step_arr)\n\n        return f_best, x_best", "configspace": "", "generation": 12, "feedback": "The algorithm StochasticAdaptiveCoordinatePush scored 0.362 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.640838777179337}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.4053105553389388}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.5367688743739989}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.23651652884749907}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.13894492521595025}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10125474936432621}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08349816433434987}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.13264600542360128}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.09704772071265566}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.09009378401953694}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9923286803801996}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9900093400193521}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9901021078608593}], "aucs": [0.640838777179337, 0.4053105553389388, 0.5367688743739989, 4.999999999999449e-05, 0.23651652884749907, 4.999999999999449e-05, 0.13894492521595025, 0.10125474936432621, 0.08349816433434987, 0.13264600542360128, 0.09704772071265566, 0.09009378401953694, 0.9923286803801996, 0.9900093400193521, 0.9901021078608593]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1967.0, "Edges": 1966.0, "Max Degree": 22.0, "Min Degree": 1.0, "Mean Degree": 1.9989832231825115, "Degree Variance": 1.9511936789254523, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.323464912280702, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.323035554758044, "Depth Entropy": 2.179984687136217, "Assortativity": 0.0, "Average Eccentricity": 17.91001525165226, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0005083884087442806, "Average Shortest Path": 10.353713174810622, "mean_complexity": 9.75, "total_complexity": 39.0, "mean_token_count": 414.5, "total_token_count": 1658.0, "mean_parameter_count": 3.0, "total_parameter_count": 12.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "6744d912-3ade-4b20-a909-0c1648997e30", "fitness": 0.5463774014197115, "name": "MonoEvalAdaptiveDirectionalEnsemble", "description": "MonoEval Adaptive Directional Ensemble (MADE) \u2014 maintain a compact ensemble of anchors and perform strictly single-evaluation directional perturbations with sparse, adaptive coordinate-selection and per-dimension step-bandit updates to favor low-degree, cheap exploitation and occasional single-eval recombinations for diversification.", "code": "import numpy as np\n\nclass MonoEvalAdaptiveDirectionalEnsemble:\n    \"\"\"\n    MonoEval Adaptive Directional Ensemble (MADE)\n\n    Key ideas:\n      - Strictly single-eval proposals: every candidate evaluated uses exactly one function evaluation.\n      - Sparse directional moves: perturb 1-3 coordinates (mostly 1) to keep effective operation-degree minimal.\n      - Per-anchor, per-dimension adaptive step sizes updated via simple bandit-like success/failure rules.\n      - Preference weights per-dimension (per-anchor) bias coordinate selection toward productive axes.\n      - Occasional single-eval recombination between anchors (midpoints) and opposition samples for cheap diversification.\n      - Final single-eval coordinate mesh refinement.\n\n    Public interface:\n      __init__(budget, dim, ...) and __call__(func) -> (f_opt, x_opt)\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.18, success_mult=1.35, failure_mult=0.65,\n                 multi_dim_probs=(0.75, 0.20, 0.05), recomb_prob=0.08,\n                 opp_prob=0.05, respawn_prob=0.10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        # choose compact population relative to dim\n        if pop_size is None:\n            self.pop_size = max(3, min(9, int(round(3 + 0.05 * self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        # probabilities for selecting number of active coords: (p(k=1), p(k=2), p(k=3))\n        probs = np.asarray(multi_dim_probs, dtype=float)\n        if probs.size != 3 or np.any(probs < 0):\n            probs = np.array([0.75, 0.20, 0.05], dtype=float)\n        probs = probs / probs.sum()\n        self.multi_dim_probs = probs\n        self.recomb_prob = float(recomb_prob)\n        self.opp_prob = float(opp_prob)\n        self.respawn_prob = float(respawn_prob)\n\n        # minimal sanity\n        self.success_mult = max(1.01, self.success_mult)\n        self.failure_mult = min(0.99, self.failure_mult)\n\n    def _get_bounds(self, func):\n        # support optional func.bounds if provided, else use [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n\n        evals = 0\n\n        # initialize anchors uniformly across the box (evaluate each)\n        anchors = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        anchor_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            anchor_f[i] = float(func(anchors[i]))\n            evals += 1\n\n        # handle tiny budgets\n        if evals == 0:\n            # cannot evaluate anything, return a fallback\n            self.x_opt = anchors[0].copy()\n            self.f_opt = float(\"inf\")\n            return self.f_opt, self.x_opt\n\n        # identify best\n        idx_best = int(np.argmin(anchor_f))\n        x_best = anchors[idx_best].copy()\n        f_best = float(anchor_f[idx_best])\n\n        # per-anchor per-dimension step sizes and preference weights\n        steps = np.full((self.pop_size, dim), base_step)\n        # preference weights (higher means more likely to pick that coord)\n        prefs = np.ones((self.pop_size, dim), dtype=float)\n        # last successful sign memory (-1,0,+1)\n        last_sign = np.zeros((self.pop_size, dim), dtype=float)\n        # stagnation age counters\n        ages = np.zeros(self.pop_size, dtype=int)\n        # simple success counts to bias respawn choices\n        success_counts = np.zeros(self.pop_size, dtype=float)\n\n        # helper to choose k (1..3) active dims, clipped by dim\n        def sample_k():\n            if dim == 1:\n                return 1\n            elif dim == 2:\n                # map 3rd probability into k=2 when dim=2\n                p1, p2, p3 = self.multi_dim_probs\n                return 1 if rng.random() < p1 else 2\n            else:\n                r = rng.random()\n                if r < self.multi_dim_probs[0]:\n                    return 1\n                elif r < self.multi_dim_probs[0] + self.multi_dim_probs[1]:\n                    return 2\n                else:\n                    return 3\n\n        # main loop: strictly single-eval proposals\n        while evals < self.budget:\n            # occasionally evaluate an opposition of global best (single-eval)\n            if rng.random() < self.opp_prob and evals < self.budget:\n                x_opp = np.clip(lb + ub - x_best, lb, ub)\n                if not np.allclose(x_opp, x_best):\n                    f_opp = float(func(x_opp)); evals += 1\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy()\n                        # replace worst anchor cheaply\n                        worst = int(np.argmax(anchor_f))\n                        anchors[worst] = x_opp.copy(); anchor_f[worst] = f_opp\n                        steps[worst, :] = base_step\n                        prefs[worst, :] = 1.0\n                        last_sign[worst, :] = 0.0\n                        ages[worst] = 0\n\n            # occasional single-eval recombination: midpoint between two random anchors (single-eval)\n            if rng.random() < self.recomb_prob and evals < self.budget and self.pop_size >= 2:\n                i, j = rng.choice(self.pop_size, size=2, replace=False)\n                alpha = rng.random() * 0.8 + 0.1\n                candidate = np.clip(alpha * anchors[i] + (1.0 - alpha) * anchors[j], lb, ub)\n                if not np.allclose(candidate, anchors[i]) and not np.allclose(candidate, anchors[j]):\n                    f_c = float(func(candidate)); evals += 1\n                    # insert if good\n                    worst = int(np.argmax(anchor_f))\n                    if f_c < anchor_f[worst]:\n                        anchors[worst] = candidate.copy()\n                        anchor_f[worst] = f_c\n                        steps[worst, :] = base_step\n                        prefs[worst, :] = 1.0\n                        last_sign[worst, :] = 0.0\n                        ages[worst] = 0\n                    if f_c < f_best:\n                        f_best = f_c; x_best = candidate.copy()\n\n            # iterate anchors in random order\n            order = rng.permutation(self.pop_size)\n            improved_any = False\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                ages[idx] += 1\n                x0 = anchors[idx].copy()\n                f0 = anchor_f[idx]\n\n                # choose number of active dims (sparse)\n                k = sample_k()\n                k = min(k, dim)\n\n                # select coordinates according to preference weights (probability)\n                w = prefs[idx].clip(1e-8)\n                prob = w / w.sum()\n                if k == 1:\n                    coords = [int(rng.choice(dim, p=prob))]\n                else:\n                    # sample without replacement using categorical probabilities\n                    coords = []\n                    avail = np.arange(dim)\n                    p_local = prob.copy()\n                    for _ in range(k):\n                        if p_local.sum() <= 0:\n                            break\n                        c = int(rng.choice(avail, p=p_local / p_local.sum()))\n                        coords.append(c)\n                        # remove chosen coord\n                        mask = avail != c\n                        avail = avail[mask]\n                        p_local = p_local[mask]\n                        if avail.size == 0:\n                            break\n\n                if len(coords) == 0:\n                    continue\n\n                # build signed sparse direction using last_sign or random signs\n                signs = np.zeros(dim, dtype=float)\n                for c in coords:\n                    if last_sign[idx, c] != 0.0:\n                        # sometimes flip sign to test opposite: small prob\n                        if rng.random() < 0.12:\n                            signs[c] = -last_sign[idx, c]\n                        else:\n                            signs[c] = last_sign[idx, c]\n                    else:\n                        signs[c] = 1.0 if rng.random() < 0.5 else -1.0\n\n                # scale step by per-dim step sizes\n                delta = np.zeros(dim, dtype=float)\n                delta[coords] = signs[coords] * steps[idx, coords]\n\n                candidate = np.clip(x0 + delta, lb, ub)\n                # if clipping made it identical, penalize those coords and continue\n                if np.allclose(candidate, x0):\n                    # shrink steps for coords and lower prefs\n                    steps[idx, coords] *= self.failure_mult\n                    prefs[idx, coords] *= 0.85\n                    ages[idx] += 1\n                    continue\n\n                # single evaluation\n                f_c = float(func(candidate)); evals += 1\n\n                if f_c < f0:\n                    # accept\n                    anchors[idx] = candidate.copy()\n                    anchor_f[idx] = f_c\n                    x0 = candidate.copy(); f0 = f_c\n                    improved_any = True\n                    ages[idx] = 0\n                    success_counts[idx] += 1.0\n\n                    # reward steps and prefs for active coords\n                    steps[idx, coords] *= self.success_mult\n                    prefs[idx, coords] = prefs[idx, coords] * 1.15 + 0.01\n                    # record last sign\n                    last_sign[idx, coords] = signs[coords]\n                    # small mild boost to other coords (encourage exploration)\n                    noncoords = np.setdiff1d(np.arange(dim), coords, assume_unique=True)\n                    if noncoords.size > 0:\n                        prefs[idx, noncoords] *= 0.995\n                else:\n                    # rejection -> shrink the active coords' steps and decrease their preference\n                    steps[idx, coords] *= self.failure_mult\n                    prefs[idx, coords] *= 0.70\n                    # forget last_sign for those coords sometimes\n                    for c in coords:\n                        if rng.random() < 0.35:\n                            last_sign[idx, c] = 0.0\n\n                # update global best cheaply when anchor changed/improved\n                if anchor_f[idx] < f_best:\n                    f_best = float(anchor_f[idx])\n                    x_best = anchors[idx].copy()\n\n                # respawn stale anchors occasionally (single-eval)\n                if ages[idx] > max(12, 3 * dim) and evals < self.budget:\n                    ages[idx] = 0\n                    if rng.random() < self.respawn_prob:\n                        # respawn near global best with gaussian jitter or uniform random\n                        if rng.random() < 0.7:\n                            newp = x_best + 0.06 * (ub - lb) * rng.standard_normal(dim)\n                        else:\n                            newp = rng.uniform(lb, ub)\n                        newp = np.clip(newp, lb, ub)\n                        f_new = float(func(newp)); evals += 1\n                        anchors[idx] = newp.copy()\n                        anchor_f[idx] = f_new\n                        steps[idx, :] = base_step\n                        prefs[idx, :] = 1.0\n                        last_sign[idx, :] = 0.0\n                        ages[idx] = 0\n                        success_counts[idx] = 0.0\n                        if f_new < f_best:\n                            f_best = f_new; x_best = newp.copy()\n                        continue\n\n            # keep steps and prefs in reasonable range\n            min_step = 1e-12 * box_len\n            max_step = 1.6 * box_len\n            steps = np.clip(steps, min_step, max_step)\n            prefs = np.clip(prefs, 1e-6, 1e6)\n\n            # if no improvement for a pass, try inserting a direct sample near global best (single-eval)\n            if not improved_any and evals < self.budget and rng.random() < 0.18:\n                newp = x_best + 0.07 * (ub - lb) * rng.standard_normal(dim)\n                newp = np.clip(newp, lb, ub)\n                if not np.allclose(newp, x_best):\n                    f_new = float(func(newp)); evals += 1\n                    # replace worst anchor\n                    worst = int(np.argmax(anchor_f))\n                    anchors[worst] = newp.copy()\n                    anchor_f[worst] = f_new\n                    steps[worst, :] = base_step\n                    prefs[worst, :] = 1.0\n                    last_sign[worst, :] = 0.0\n                    ages[worst] = 0\n                    if f_new < f_best:\n                        f_best = f_new; x_best = newp.copy()\n\n            # budget-aware break to preserve evaluations for final refinement\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final single-eval coordinate mesh refinement on x_best (very low-degree: 1 eval per candidate)\n        mesh = 0.08 * box_len\n        mesh_min = 1e-9 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # positive direction (single-eval)\n                xp = x_best.copy()\n                xp[d] = np.clip(x_best[d] + mesh, lb[d], ub[d])\n                if not np.allclose(xp, x_best):\n                    fp = float(func(xp)); evals += 1\n                    if fp < f_best:\n                        f_best = fp; x_best = xp.copy(); improved = True\n                        continue\n                if evals >= self.budget:\n                    break\n                # negative direction\n                xm = x_best.copy()\n                xm[d] = np.clip(x_best[d] - mesh, lb[d], ub[d])\n                if not np.allclose(xm, x_best):\n                    fm = float(func(xm)); evals += 1\n                    if fm < f_best:\n                        f_best = fm; x_best = xm.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # store results\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm MonoEvalAdaptiveDirectionalEnsemble scored 0.546 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["49beef6d-dff3-43de-8c03-613d52610af4"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9306628721028837}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9314640135757054}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9343739152869616}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.6135391481669883}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.5773864697617533}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6146293468150428}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.12856971908716497}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.07507032765326771}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08878664886913978}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10666014534141655}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.09023687350595788}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.12320414455861428}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9919225807648465}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9941300712755811}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9950247445303477}], "aucs": [0.9306628721028837, 0.9314640135757054, 0.9343739152869616, 0.6135391481669883, 0.5773864697617533, 0.6146293468150428, 0.12856971908716497, 0.07507032765326771, 0.08878664886913978, 0.10666014534141655, 0.09023687350595788, 0.12320414455861428, 0.9919225807648465, 0.9941300712755811, 0.9950247445303477]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2730.0, "Edges": 2729.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9992673992673993, "Degree Variance": 2.1831496464463496, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.206235011990408, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.326764649503408, "Depth Entropy": 2.046623164435677, "Assortativity": 0.0, "Average Eccentricity": 18.835164835164836, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0003663003663003663, "Average Shortest Path": 10.51740832759521, "mean_complexity": 16.75, "total_complexity": 67.0, "mean_token_count": 539.25, "total_token_count": 2157.0, "mean_parameter_count": 3.5, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "d73fb929-d6e1-4776-b617-8f5cdba02304", "fitness": 0.607596313936012, "name": "StaggeredGreedyCoordinateExplorer", "description": "Staggered Greedy Coordinate Explorer (SGCE) \u2014 strictly low-degree (1-D) sequential coordinate probing with adaptive per-coordinate step sizes, priority-driven coordinate scheduling, and cheap one-coordinate extrapolations plus occasional mirrored/global diversification.", "code": "import numpy as np\n\nclass StaggeredGreedyCoordinateExplorer:\n    \"\"\"\n    Staggered Greedy Coordinate Explorer (SGCE)\n\n    Key ideas:\n      - Only modify one coordinate at a time (max interaction degree = 1).\n      - Maintain per-coordinate adaptive step sizes, increase on success, shrink on failure.\n      - Maintain a lightweight priority for coordinates based on recent successes/failures;\n        preferentially probe higher-priority coordinates more often.\n      - For each successful single-coordinate move, perform a cheap extrapolation probe\n        along the same coordinate (still degree-1).\n      - On stagnation, try mirrored opposite sample and a few global random samples.\n      - Strictly respect the evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, init_step=None, min_step=1e-6,\n                 max_step=None, stagnation_patience=6, rng=None):\n        \"\"\"\n        Args:\n          budget: maximum number of function evaluations allowed\n          dim: problem dimensionality\n          init_step: initial per-coordinate step size (scalar or array). If None, set to (ub-lb)/4 at runtime.\n          min_step: minimum allowed step\n          max_step: maximum allowed step (if None, set to (ub-lb)/2 at runtime)\n          stagnation_patience: iterations without improvement before broader diversification\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        # Try to read bounds from func.bounds if available; otherwise use [-5,5]\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` with at most self.budget evaluations.\n        Returns (f_best, x_best).\n        \"\"\"\n        lb, ub = self._get_bounds(func)\n        # Ensure bounds are vectors of correct length\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n        if ub.shape[0] != self.dim:\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        range_vec = ub - lb\n        # initialize step sizes\n        if self.init_step is None:\n            init_step = 0.25 * range_vec  # quarter of range\n        else:\n            init_step = np.array(self.init_step, dtype=float)\n            if init_step.shape == ():\n                init_step = np.full(self.dim, float(init_step))\n        # max step if not provided\n        if self.max_step is None:\n            max_step = 0.5 * range_vec\n        else:\n            max_step = np.array(self.max_step, dtype=float)\n            if max_step.shape == ():\n                max_step = np.full(self.dim, float(max_step))\n\n        step = np.clip(init_step, self.min_step, max_step)\n        # Priority and recent performance trackers\n        priority = np.zeros(self.dim, dtype=float)   # higher => probe more\n        success_count = np.zeros(self.dim, dtype=int)\n        failure_count = np.zeros(self.dim, dtype=int)\n\n        # bookkeeping\n        evals = 0\n        # initial point\n        x_best = lb + self.rng.random(self.dim) * range_vec\n        f_best = float(func(x_best))\n        evals += 1\n\n        stagnation = 0\n        iter_count = 0\n\n        # choose how many coordinates to probe in a sweep (fraction of dim)\n        base_probe_frac = max(0.05, min(0.4, 5.0 / max(1, self.dim)))  # adapt with dim\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            remaining = self.budget - evals\n            # number of coordinates to probe this sweep\n            p = max(1, int(np.ceil(self.dim * base_probe_frac)))\n            # sample coordinates with probability influenced by priority (softmax)\n            if np.any(priority != 0.0):\n                # stabilize priorities\n                scaled = priority - np.max(priority)\n                probs = np.exp(scaled)\n                probs = probs / probs.sum()\n            else:\n                probs = None\n\n            # Select a set of unique coordinates (without replacement) biased by probs if available\n            if probs is None:\n                coords = self.rng.choice(self.dim, size=min(self.dim, p), replace=False)\n            else:\n                # when dim small, fallback to uniform if numerical issues\n                try:\n                    coords = self.rng.choice(self.dim, size=min(self.dim, p), replace=False, p=probs)\n                except Exception:\n                    coords = self.rng.choice(self.dim, size=min(self.dim, p), replace=False)\n\n            improved_this_sweep = False\n\n            # Probe each selected coordinate sequentially (degree-1 modifications)\n            for i in coords:\n                if evals >= self.budget:\n                    break\n\n                # choose directional trial: prefer sign that historically improved (encoded in priority)\n                sign = 1 if self.rng.random() < 0.5 else -1\n                # small bias: if priority positive, try positive first, else negative\n                if priority[i] > 0.1:\n                    sign = 1\n                elif priority[i] < -0.1:\n                    sign = -1\n\n                s = float(step[i])\n                if s <= self.min_step:\n                    s = self.min_step\n\n                # single-sided probe first\n                x_c = x_best.copy()\n                x_c[i] = np.clip(x_c[i] + sign * s, lb[i], ub[i])\n                f_c = float(func(x_c))\n                evals += 1\n\n                if f_c < f_best:\n                    # accept\n                    f_best = f_c\n                    x_best = x_c\n                    improved_this_sweep = True\n                    stagnation = 0\n                    success_count[i] += 1\n                    failure_count[i] = 0\n                    # increase priority and step\n                    priority[i] += 1.0\n                    step[i] = float(min(max_step[i], step[i] * 1.25 + 1e-12))\n                    # cheap extrapolation along same coordinate (still degree-1)\n                    if evals < self.budget:\n                        ext = 0.5 * s\n                        x_e = x_best.copy()\n                        x_e[i] = np.clip(x_e[i] + sign * ext, lb[i], ub[i])\n                        f_e = float(func(x_e))\n                        evals += 1\n                        if f_e < f_best:\n                            f_best = f_e\n                            x_best = x_e\n                            # further reward\n                            priority[i] += 0.5\n                            step[i] = float(min(max_step[i], step[i] * 1.15 + 1e-12))\n                else:\n                    # first probe failed; try the opposite direction once (if budget permits)\n                    if evals < self.budget:\n                        x_c2 = x_best.copy()\n                        x_c2[i] = np.clip(x_c2[i] - sign * s, lb[i], ub[i])\n                        f_c2 = float(func(x_c2))\n                        evals += 1\n                        if f_c2 < f_best:\n                            f_best = f_c2\n                            x_best = x_c2\n                            improved_this_sweep = True\n                            stagnation = 0\n                            success_count[i] += 1\n                            failure_count[i] = 0\n                            priority[i] += 0.8\n                            step[i] = float(min(max_step[i], step[i] * 1.2 + 1e-12))\n                            # extrapolate a bit\n                            if evals < self.budget:\n                                ext = 0.5 * s\n                                x_e = x_best.copy()\n                                # try continuing in the successful direction (which is -sign)\n                                x_e[i] = np.clip(x_e[i] - sign * ext, lb[i], ub[i])\n                                f_e = float(func(x_e))\n                                evals += 1\n                                if f_e < f_best:\n                                    f_best = f_e\n                                    x_best = x_e\n                                    priority[i] += 0.3\n                                    step[i] = float(min(max_step[i], step[i] * 1.10 + 1e-12))\n                        else:\n                            # both sides failed: shrink step and penalize priority\n                            failure_count[i] += 1\n                            priority[i] -= 0.6\n                            step[i] = float(max(self.min_step, step[i] * 0.5))\n                    else:\n                        # no budget to try opposite, count it as failure\n                        failure_count[i] += 1\n                        priority[i] -= 0.6\n                        step[i] = float(max(self.min_step, step[i] * 0.5))\n\n                # keep priority values bounded to avoid numerical explosion\n                priority[i] = float(np.clip(priority[i], -10.0, 10.0))\n\n            # end for coords\n\n            # update stagnation\n            if not improved_this_sweep:\n                stagnation += 1\n            else:\n                stagnation = 0\n\n            # occasional very cheap mirrored check to escape symmetric traps\n            if evals < self.budget and stagnation >= 2 and self.rng.random() < 0.3:\n                # mirror around center of bounds (opposite)\n                x_opp = lb + ub - x_best\n                x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                f_opp = float(func(x_opp))\n                evals += 1\n                if f_opp < f_best:\n                    f_best = f_opp\n                    x_best = x_opp\n                    stagnation = 0\n                    # reset some steps to encourage exploration\n                    step = np.minimum(step * 1.2, max_step)\n                    priority *= 0.5\n\n            # forced diversification if stagnation persists\n            if stagnation >= self.stagnation_patience and evals < self.budget:\n                # try a small number of global uniform random samples (cheap)\n                num_global = min(4, max(1, self.dim // 4))\n                improved = False\n                for _ in range(num_global):\n                    if evals >= self.budget:\n                        break\n                    x_r = lb + self.rng.random(self.dim) * range_vec\n                    f_r = float(func(x_r))\n                    evals += 1\n                    if f_r < f_best:\n                        f_best = f_r\n                        x_best = x_r\n                        improved = True\n                        break\n                if improved:\n                    # reset step sizes modestly when improvement found globally\n                    step = np.clip(step * 1.1, self.min_step, max_step)\n                    priority *= 0.3\n                    stagnation = 0\n                else:\n                    # perform one \"near-random\" perturbation around current best\n                    if evals < self.budget:\n                        jump_scale = 0.2 * range_vec\n                        x_j = x_best + self.rng.normal(0.0, 1.0, size=self.dim) * jump_scale\n                        x_j = np.minimum(np.maximum(x_j, lb), ub)\n                        f_j = float(func(x_j))\n                        evals += 1\n                        if f_j < f_best:\n                            f_best = f_j\n                            x_best = x_j\n                            step = np.clip(step * 1.2, self.min_step, max_step)\n                            priority *= 0.2\n                            stagnation = 0\n                        else:\n                            # reset some priorities and moderately increase steps to escape\n                            priority = np.zeros_like(priority)\n                            step = np.clip(step * 1.5, self.min_step, max_step)\n                            stagnation = 0\n\n            # small adaptive adjustment to base_probe_frac so search breadth adapts\n            # if many successes recently, probe slightly more coords per sweep\n            total_successes = success_count.sum()\n            if total_successes > 0:\n                base_probe_frac = min(0.6, 0.05 + 0.12 * np.log1p(total_successes))\n            else:\n                base_probe_frac = max(0.05, base_probe_frac * 0.995)\n\n            # safety clamp steps within bounds\n            step = np.minimum(np.maximum(step, self.min_step), max_step)\n\n        # end main loop\n\n        return f_best, x_best", "configspace": "", "generation": 12, "feedback": "The algorithm StaggeredGreedyCoordinateExplorer scored 0.608 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9774225424661396}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9778956481839978}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9792233634352698}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8517830184668169}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8860342301664921}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9080131414096312}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1511024663199323}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06761418815860398}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.06761267291809858}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0741467869775424}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.08195286486522468}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.10385049579985683}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9958803543492462}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9957338784425478}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9956790570807783}], "aucs": [0.9774225424661396, 0.9778956481839978, 0.9792233634352698, 0.8517830184668169, 0.8860342301664921, 0.9080131414096312, 0.1511024663199323, 0.06761418815860398, 0.06761267291809858, 0.0741467869775424, 0.08195286486522468, 0.10385049579985683, 0.9958803543492462, 0.9957338784425478, 0.9956790570807783]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2060.0, "Edges": 2059.0, "Max Degree": 22.0, "Min Degree": 1.0, "Mean Degree": 1.9990291262135922, "Degree Variance": 1.9203874069186537, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.671940928270041, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.338560852935808, "Depth Entropy": 2.2730879244747024, "Assortativity": 0.0, "Average Eccentricity": 18.73883495145631, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0004854368932038835, "Average Shortest Path": 10.659549597551832, "mean_complexity": 15.0, "total_complexity": 45.0, "mean_token_count": 597.0, "total_token_count": 1791.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "f916748a-a3c0-486f-8075-a3106aeb935b", "fitness": 0.4568522464648036, "name": "SeparableSingleEvalAdaptiveSubspaceSearch", "description": "Separable Single-Eval Adaptive Subspace Search (SSE-ASS) \u2014 aggressively favor single-evaluation, low-degree (mostly 1-D) perturbations with per-coordinate adaptive step sizes and a tiny direction cache for low-cost recombinations and occasional staged diversification.", "code": "import numpy as np\n\nclass SeparableSingleEvalAdaptiveSubspaceSearch:\n    \"\"\"\n    Separable Single-Eval Adaptive Subspace Search (SSE-ASS)\n\n    Key ideas:\n      - Perform almost only single-evaluation probes: each candidate costs one function\n        evaluation (max per-candidate degree = 1).\n      - Favor very small subspaces (mostly 1-D, sometimes 2-3-D) with per-dimension adaptive\n        step sizes that grow on success and shrink on failure.\n      - Keep a tiny cache of successful direction vectors and occasionally try a single-eval\n        recombination built from cached directions (still degree 1).\n      - Use light-weight opposite and small global diversifications when stagnation occurs.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10,\n                 init_step=0.8, min_step=1e-6, max_step=5.0,\n                 stagnation_patience=10, rng=None, cache_size=6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.stagnation_patience = int(stagnation_patience)\n        self.cache_size = int(cache_size)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        evals = 0\n\n        # sensible small initial sampling (few samples to pick a robust start)\n        init_samples = 3 if self.budget >= 3 else 1\n        x_best = None\n        f_best = np.inf\n        for _ in range(init_samples):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n\n        # fallback in improbable case\n        if x_best is None:\n            x_best = lb + self.rng.random(self.dim) * (ub - lb)\n            f_best = float(func(x_best))\n            evals += 1\n\n        # per-dimension adaptive step sizes and simple success counters\n        step_sizes = np.full(self.dim, self.init_step, dtype=float)\n        successes = np.zeros(self.dim, dtype=float)  # smoothed success counts\n        failures = np.zeros(self.dim, dtype=float)\n        smoothing = 0.9\n\n        # tiny cache of recent successful direction vectors (normalized)\n        dir_cache = []\n\n        stagnation = 0\n\n        # helper: choose subspace dimension skewed to 1,2,3\n        def choose_k():\n            r = self.rng.random()\n            if r < 0.6:\n                return 1\n            elif r < 0.85:\n                return 2 if self.dim >= 2 else 1\n            elif r < 0.95:\n                return 3 if self.dim >= 3 else 1\n            else:\n                # rare larger but still small\n                return min(self.dim, 4)\n\n        while evals < self.budget:\n            remaining = self.budget - evals\n\n            # adapt selection probabilities favoring dims with recent successes (but ensure exploration)\n            sel_weights = successes + 1.0  # +1 prevents zero probs\n            sel_prob = sel_weights / sel_weights.sum()\n\n            k = choose_k()\n            # pick indices (if k==dim just all)\n            if k >= self.dim:\n                indices = np.arange(self.dim)\n            else:\n                # sample without replacement using probabilities\n                # numpy choice with p cannot sample without replacement with weights easily for >1\n                # use repeated sampling without replacement via simple approach\n                indices = []\n                probs = sel_prob.copy()\n                for _i in range(k):\n                    idx = self.rng.choice(self.dim, p=probs)\n                    indices.append(int(idx))\n                    probs[idx] = 0.0\n                    sum_p = probs.sum()\n                    if sum_p <= 0:\n                        probs = np.ones(self.dim)\n                    else:\n                        probs = probs / probs.sum()\n                indices = np.array(indices, dtype=int)\n\n            # build a single-eval candidate: use heavy-tailed perturbation (Cauchy) scaled by per-dim step sizes\n            # scale down by sqrt(k) to keep overall magnitude stable with k\n            # clip steps so they stay within bounds sensibly\n            sig = 1.0 / max(1.0, np.sqrt(k))\n            # sample Cauchy but guard extreme tails\n            raw = self.rng.standard_cauchy(size=k)\n            raw = np.clip(raw, -10.0, 10.0)  # avoid insane leaps\n            step = raw * (step_sizes[indices] * sig)\n\n            x_candidate = x_best.copy()\n            x_candidate[indices] += step\n            # project inside bounds\n            x_candidate = np.minimum(np.maximum(x_candidate, lb), ub)\n\n            f_candidate = float(func(x_candidate))\n            evals += 1\n\n            # acceptance\n            if f_candidate < f_best:\n                # success: update best, grow step sizes on involved dims\n                prev = x_best.copy()\n                f_best = f_candidate\n                x_best = x_candidate.copy()\n                stagnation = 0\n\n                # increase step sizes multiplicatively, with cap\n                growth = 1.25\n                step_sizes[indices] = np.minimum(self.max_step, step_sizes[indices] * growth)\n\n                # update smoothed success/failure counters\n                successes[indices] = smoothing * successes[indices] + (1.0 - smoothing) * 1.0\n                failures[indices] = smoothing * failures[indices]  # decay\n\n                # insert direction into cache (direction from prev to new)\n                d = x_best - prev\n                norm = np.linalg.norm(d)\n                if norm > 0:\n                    d_unit = d / norm\n                    dir_cache.insert(0, d_unit)\n                    if len(dir_cache) > self.cache_size:\n                        dir_cache.pop()\n\n                # tiny opportunistic recombination attempt (still a single-eval probe)\n                # Try once occasionally to exploit combinations of cached directions\n                if dir_cache and self.rng.random() < 0.15 and evals < self.budget:\n                    # draw small random linear combo of up to 3 cached directions\n                    take = min(len(dir_cache), self.rng.integers(1, min(4, len(dir_cache)) + 1))\n                    picks = self.rng.choice(len(dir_cache), size=take, replace=False)\n                    combo = np.zeros(self.dim)\n                    weight_scale = 0.6 * np.mean(step_sizes)  # small amplitude\n                    weights = self.rng.normal(0.0, 0.7, size=take)\n                    for ii, w in enumerate(weights):\n                        combo += w * dir_cache[picks[ii]]\n                    combo = combo * weight_scale / max(1e-12, np.linalg.norm(combo))\n                    x_combo = x_best + combo\n                    x_combo = np.minimum(np.maximum(x_combo, lb), ub)\n                    f_combo = float(func(x_combo))\n                    evals += 1\n                    if f_combo < f_best:\n                        # accept combo\n                        f_best = f_combo\n                        x_best = x_combo.copy()\n                        stagnation = 0\n                        # strengthen all dims slightly\n                        step_sizes = np.minimum(self.max_step, step_sizes * 1.10)\n\n            else:\n                # failure: shrink step sizes on involved dims\n                stagnation += 1\n                shrink = 0.80\n                step_sizes[indices] = np.maximum(self.min_step, step_sizes[indices] * shrink)\n\n                # update counters (smoothed)\n                successes[indices] = smoothing * successes[indices]\n                failures[indices] = smoothing * failures[indices] + (1.0 - smoothing) * 1.0\n\n            # Occasionally try the opposite point cheaply when stagnating\n            if evals < self.budget and stagnation >= 3 and self.rng.random() < 0.3:\n                x_opp = lb + ub - x_best\n                x_opp = np.minimum(np.maximum(x_opp, lb), ub)\n                f_opp = float(func(x_opp))\n                evals += 1\n                if f_opp < f_best:\n                    # accept\n                    f_best = f_opp\n                    x_best = x_opp.copy()\n                    # reset and enlarge some steps\n                    step_sizes = np.minimum(self.max_step, step_sizes * 1.2)\n                    stagnation = 0\n\n            # Forced diversification when patience exceeded: a small batch of global tries\n            if stagnation >= self.stagnation_patience and evals < self.budget:\n                num_global = min(3, self.budget - evals)\n                improved = False\n                for _ in range(num_global):\n                    x_rand = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_rand = float(func(x_rand))\n                    evals += 1\n                    if f_rand < f_best:\n                        f_best = f_rand\n                        x_best = x_rand.copy()\n                        improved = True\n                        break\n                if improved:\n                    # restore moderate step sizes around improved dims\n                    step_sizes = np.maximum(self.min_step, np.minimum(self.max_step, step_sizes * 1.1))\n                    stagnation = 0\n                else:\n                    # if no improvement, reset steps to moderate exploratory scale\n                    step_sizes = np.maximum(self.min_step, np.minimum(self.max_step,\n                                                                      np.full(self.dim, self.init_step * 0.9)))\n                    stagnation = 0  # give the search another phase\n\n            # keep steps in bounds\n            step_sizes = np.clip(step_sizes, self.min_step, self.max_step)\n\n            # If near-zero steps everywhere, re-seed some steps to avoid freeze\n            if np.all(step_sizes <= self.min_step * 10.0):\n                step_sizes += self.init_step * 0.25\n\n        return f_best, x_best", "configspace": "", "generation": 12, "feedback": "The algorithm SeparableSingleEvalAdaptiveSubspaceSearch scored 0.457 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5451537433199242}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.620408328425641}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.5685275449627265}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.18799429021525482}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.15190088716326722}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.13688983169724545}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.3022818611069923}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.3042062929042332}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.2982964585193718}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.2535559184600984}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.2654113968329197}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.2620166181334227}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9836577454244723}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9893041410517964}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9831786387546876}], "aucs": [0.5451537433199242, 0.620408328425641, 0.5685275449627265, 0.18799429021525482, 0.15190088716326722, 0.13688983169724545, 0.3022818611069923, 0.3042062929042332, 0.2982964585193718, 0.2535559184600984, 0.2654113968329197, 0.2620166181334227, 0.9836577454244723, 0.9893041410517964, 0.9831786387546876]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1772.0, "Edges": 1771.0, "Max Degree": 21.0, "Min Degree": 1.0, "Mean Degree": 1.9988713318284423, "Degree Variance": 1.9988700579366006, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.846441947565543, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3284140840885166, "Depth Entropy": 2.0190213737510034, "Assortativity": 1.472133662599443e-08, "Average Eccentricity": 15.804740406320542, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.000564334085778781, "Average Shortest Path": 9.700972400844812, "mean_complexity": 10.75, "total_complexity": 43.0, "mean_token_count": 386.75, "total_token_count": 1547.0, "mean_parameter_count": 3.25, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "73129811-f556-4b19-bf17-20e00fff3c3d", "fitness": 0.5749579542220897, "name": "LowDegreeAdaptiveSubspaceSearch", "description": "Low-Degree Adaptive Subspace Search (LD-ASES) \u2014 primarily single-evaluation, per-coordinate pushes with tiny multi-dim probes only rarely; adapt per-dimension step sizes from cheap successes/failures, keep an ultra-low evaluation degree (usually 1, occasionally 2), and use compact recent-success recombinations for lightweight diversification.", "code": "import numpy as np\n\nclass LowDegreeAdaptiveSubspaceSearch:\n    \"\"\"\n    Low-Degree Adaptive Subspace Search (LD-ASES)\n\n    Key points:\n      - Keep current best and per-dimension step sizes.\n      - Most proposals use a single evaluation: push along 1 (or rarely 2) coordinates.\n      - If a single push fails, optionally try the opposite direction (one extra eval).\n      - On success increase step(s); on failure shrink them.\n      - Store a tiny buffer of recent successful displacement directions to create\n        occasional low-degree recombinations (single eval).\n      - Rare global random samples used when stagnation occurs.\n      - Strict budget accounting.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_step=1.0, min_step=1e-6,\n                 max_step=5.0, stagnation_patience=10, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          init_step: initial per-dimension step magnitude (in domain units)\n          min_step: minimum allowed per-dim step\n          max_step: maximum allowed per-dim step\n          stagnation_patience: iterations without improvement before stronger diversification\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        # default bounds [-5,5]; try to read func.bounds if present\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure lengths\n        if lb.size != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n        if ub.size != self.dim:\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        # initial point: one random sample\n        x_best = lb + self.rng.random(self.dim) * (ub - lb)\n        f_best = float(func(x_best))\n        evals += 1\n\n        # per-dim step sizes\n        steps = np.full(self.dim, self.init_step, dtype=float)\n        # small success counters per-dim to bias selection\n        success_count = np.zeros(self.dim, dtype=float)\n        # buffer of recent successful displacements (normalized)\n        recent_dirs = []\n        max_recent = 8\n\n        stagnation_iters = 0\n        iter_count = 0\n\n        # helper to clip and eval safely\n        def eval_candidate(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            xc = np.minimum(np.maximum(x, lb), ub)\n            val = float(func(xc))\n            evals += 1\n            return val, xc\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            remaining = self.budget - evals\n\n            # Decide subspace dimension: mostly 1, occasionally 2, very rarely 3\n            r = self.rng.random()\n            if r < 0.78:\n                k = 1\n            elif r < 0.96:\n                k = 2\n            else:\n                k = 3\n            k = min(k, self.dim)\n\n            # choose indices to probe:\n            # Prefer dims with higher success_count but allow exploration\n            probs = success_count + 0.2  # baseline\n            probs = probs / probs.sum()\n            if k == self.dim:\n                indices = np.arange(self.dim)\n            else:\n                # sample without replacement using choice with probabilities\n                try:\n                    indices = self.rng.choice(self.dim, size=k, replace=False, p=probs)\n                except Exception:\n                    # fallback to uniform\n                    indices = self.rng.choice(self.dim, size=k, replace=False)\n\n            # make a single-eval proposal: random sign, slight gaussian multiplier to steps\n            multiplier = 0.7 + 0.6 * self.rng.random()  # between 0.7 and 1.3\n            signs = self.rng.choice([-1.0, 1.0], size=k)\n            x_candidate = x_best.copy()\n            for idx_i, sgn in zip(indices, signs):\n                x_candidate[idx_i] += sgn * steps[idx_i] * multiplier\n\n            f_candidate, x_candidate = eval_candidate(x_candidate)\n            if f_candidate is None:\n                break\n\n            improved = False\n            if f_candidate < f_best:\n                # accept\n                f_best = f_candidate\n                x_best = x_candidate\n                improved = True\n                stagnation_iters = 0\n                # reward steps: enlarge for touched dims\n                for idx in indices:\n                    steps[idx] = min(self.max_step, steps[idx] * 1.25)\n                    success_count[idx] += 1.0\n                # store normalized displacement for recombination (low memory)\n                disp = x_candidate - x_best  # zero because x_best already set -> compute from last center:\n                # to capture direction properly, compute displacement used in proposal:\n                disp = np.zeros(self.dim)\n                for idx_i, sgn in zip(indices, signs):\n                    disp[idx_i] = sgn * steps[idx_i] * multiplier\n                # normalize if nonzero\n                norm = np.linalg.norm(disp)\n                if norm > 0:\n                    recent_dirs.append(disp / norm)\n                    if len(recent_dirs) > max_recent:\n                        recent_dirs.pop(0)\n\n                # low-prob greedy half-step exploitation (adds 1 eval rarely)\n                if remaining > 1 and self.rng.random() < 0.28:\n                    x_probe = x_best.copy()\n                    for idx_i, sgn in zip(indices, signs):\n                        x_probe[idx_i] += 0.5 * sgn * steps[idx_i] * multiplier\n                    f_probe, x_probe = eval_candidate(x_probe)\n                    if f_probe is None:\n                        break\n                    if f_probe < f_best:\n                        f_best = f_probe\n                        x_best = x_probe\n                        # further reward\n                        for idx in indices:\n                            steps[idx] = min(self.max_step, steps[idx] * 1.12)\n            else:\n                # failed single-eval proposal: with modest probability try opposite direction (one more eval)\n                tried_opposite = False\n                if remaining > 0 and self.rng.random() < 0.52:\n                    x_opp = x_best.copy()\n                    for idx_i, sgn in zip(indices, signs):\n                        x_opp[idx_i] -= sgn * steps[idx_i] * multiplier  # opposite\n                    f_opp, x_opp = eval_candidate(x_opp)\n                    tried_opposite = True\n                    if f_opp is None:\n                        break\n                    if f_opp < f_best:\n                        f_best = f_opp\n                        x_best = x_opp\n                        improved = True\n                        stagnation_iters = 0\n                        for idx in indices:\n                            steps[idx] = min(self.max_step, steps[idx] * 1.18)\n                            success_count[idx] += 0.8\n                        # store direction\n                        disp = x_opp - x_best\n                        norm = np.linalg.norm(disp)\n                        if norm > 0:\n                            recent_dirs.append(disp / norm)\n                            if len(recent_dirs) > max_recent:\n                                recent_dirs.pop(0)\n                # if still not improved, penalize steps for touched dims\n                if not improved:\n                    for idx in indices:\n                        steps[idx] = max(self.min_step, steps[idx] * 0.62)\n                        # slight decay to success_count to keep exploration fresh\n                        success_count[idx] = max(0.0, success_count[idx] * 0.92)\n                    stagnation_iters += 1\n\n            # very occasional low-degree recombination using recent_dirs:\n            if evals < self.budget and recent_dirs and self.rng.random() < 0.06:\n                # pick up to two recent directions and combine with small scalar\n                rcount = len(recent_dirs)\n                a = recent_dirs[self.rng.integers(rcount)]\n                b = recent_dirs[self.rng.integers(rcount)] if rcount > 1 and self.rng.random() < 0.6 else None\n                mix = 0.6 + 0.8 * self.rng.random()  # 0.6..1.4\n                cand = x_best.copy()\n                cand += mix * a * (np.mean(steps) * 0.7)\n                if b is not None:\n                    cand += (0.3 + 0.6 * self.rng.random()) * b * (np.mean(steps) * 0.7)\n                f_cand, cand = eval_candidate(cand)\n                if f_cand is None:\n                    break\n                if f_cand < f_best:\n                    f_best = f_cand\n                    x_best = cand\n                    stagnation_iters = 0\n                    # modest global step reward\n                    steps = np.minimum(self.max_step, steps * 1.08)\n                    # push recent_dirs buffer\n                    norm = np.linalg.norm(cand - x_best)\n                    # (x_best updated to cand; we already appended directions earlier)\n                else:\n                    # slight shrink on failed recombination\n                    steps = np.maximum(self.min_step, steps * 0.985)\n\n            # If stagnation high, do lightweight global random sampling (still low degree)\n            if stagnation_iters >= self.stagnation_patience and evals < self.budget:\n                # do a small number of cheap independent random samples\n                num_global = min(max(2, self.dim // 3), self.budget - evals)\n                improved_here = False\n                for _ in range(num_global):\n                    xg = lb + self.rng.random(self.dim) * (ub - lb)\n                    fg, xg = eval_candidate(xg)\n                    if fg is None:\n                        break\n                    if fg < f_best:\n                        f_best = fg\n                        x_best = xg\n                        improved_here = True\n                        # reset steps to moderate exploration around new point\n                        steps = np.full(self.dim, min(self.max_step, max(self.init_step, np.mean(steps))), dtype=float)\n                        stagnation_iters = 0\n                        break\n                if not improved_here:\n                    # try a single near-random jump from best (1 eval)\n                    if evals < self.budget:\n                        jump_scale = np.maximum(0.2 * (ub - lb), 0.05 * (ub - lb))\n                        xj = x_best + self.rng.normal(0, 1.0, size=self.dim) * jump_scale\n                        fj, xj = eval_candidate(xj)\n                        if fj is None:\n                            break\n                        if fj < f_best:\n                            f_best = fj\n                            x_best = xj\n                            steps = np.full(self.dim, min(self.max_step, self.init_step), dtype=float)\n                            stagnation_iters = 0\n                        else:\n                            # gentle reset of some steps and counters\n                            steps = np.maximum(self.min_step, steps * 0.75)\n                            stagnation_iters = 0  # give a new cycle\n\n            # periodic small decay to success_count to prefer fresh successes\n            if iter_count % 20 == 0:\n                success_count *= 0.88\n\n            # ensure steps within bounds\n            steps = np.clip(steps, self.min_step, self.max_step)\n\n        return f_best, x_best", "configspace": "", "generation": 12, "feedback": "The algorithm LowDegreeAdaptiveSubspaceSearch scored 0.575 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.959265990720449}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9720692375288862}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9783808638868341}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8627981509020419}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8590398824770544}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8822934045718185}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.026553382756137167}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03764838672502846}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.037381028004827344}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.006142179935244685}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.010331917097419874}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.03567287856115331}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9851133547519582}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9912491451416678}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9804295102708256}], "aucs": [0.959265990720449, 0.9720692375288862, 0.9783808638868341, 0.8627981509020419, 0.8590398824770544, 0.8822934045718185, 0.026553382756137167, 0.03764838672502846, 0.037381028004827344, 0.006142179935244685, 0.010331917097419874, 0.03567287856115331, 0.9851133547519582, 0.9912491451416678, 0.9804295102708256]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1917.0, "Edges": 1916.0, "Max Degree": 22.0, "Min Degree": 1.0, "Mean Degree": 1.9989567031820552, "Degree Variance": 1.9290547279115096, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.305587229190422, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3300956606314158, "Depth Entropy": 2.1517834709392867, "Assortativity": 0.0, "Average Eccentricity": 17.25299947835159, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0005216484089723526, "Average Shortest Path": 10.207018185817915, "mean_complexity": 14.5, "total_complexity": 58.0, "mean_token_count": 411.5, "total_token_count": 1646.0, "mean_parameter_count": 3.25, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "0551dcff-1e39-4ac0-9547-8473ac5d6b75", "fitness": 0.5877255424689558, "name": "LowDegreeProgressiveAnchorsV2", "description": "Single-Eval Anchor-and-Blend (SEAB) \u2014 strictly single-evaluation atomic moves: maintain compact anchors, use one-eval directional probes, one-eval two-anchor blends and single-eval diagonal probes; adapt per-dimension steps by simple success/failure multipliers and respawn stale anchors to stay exploratory while keeping the operation degree = 1.", "code": "import numpy as np\n\nclass LowDegreeProgressiveAnchorsV2:\n    \"\"\"\n    Low-degree Progressive Anchors V2 (SEAB)\n    - budget: maximum number of function evaluations\n    - dim: dimensionality\n    Optional args:\n      - pop_size: number of anchors (compact)\n      - init_step: initial per-coordinate step as fraction of box diagonal (default 0.18)\n      - success_mult / failure_mult: multiplicative adaptation factors\n      - diag_prob: probability of a single-eval diagonal probe per anchor visit\n      - opp_prob: probability to evaluate opposite-of-best (opposition-based sample)\n      - blend_prob: probability to try a one-eval blend between best and an anchor\n      - respawn_noise: gaussian noise scale when respawning around best\n      - seed: RNG seed\n    Principles:\n      - All atomic moves are single-evaluation (max degree = 1).\n      - Rely on stored directional memory (last_sign) and anchor blends to produce informative, cheap candidates.\n      - Keep anchors compact and respawn stale anchors to preserve diversity.\n    \"\"\"\n\n    def __init__(self, budget, dim, pop_size=None,\n                 init_step=0.18, success_mult=1.3, failure_mult=0.7,\n                 diag_prob=0.10, opp_prob=0.06, blend_prob=0.10,\n                 respawn_noise=0.06, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        if pop_size is None:\n            self.pop_size = max(3, min(8, int(round(4 + 0.05 * self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.diag_prob = float(diag_prob)\n        self.opp_prob = float(opp_prob)\n        self.blend_prob = float(blend_prob)\n        self.respawn_noise = float(respawn_noise)\n\n        # outputs\n        self.x_opt = None\n        self.f_opt = None\n\n    def _get_bounds(self, func):\n        # default [-5,5] or use func.bounds if provided\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n\n        evals = 0\n\n        # initialize anchors uniformly across the box\n        anchors = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        anchor_f = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            anchor_f[i] = float(func(anchors[i]))\n            evals += 1\n\n        # ensure at least one evaluated point to return\n        finite_idx = np.where(np.isfinite(anchor_f))[0]\n        if finite_idx.size == 0:\n            # fallback evaluate first anchor if budget allows\n            if evals < self.budget:\n                anchor_f[0] = float(func(anchors[0])); evals += 1\n                finite_idx = np.where(np.isfinite(anchor_f))[0]\n            else:\n                # no budget \u2014 return zero vector\n                self.x_opt = np.clip(np.zeros(dim), lb, ub)\n                self.f_opt = float(np.inf)\n                return self.f_opt, self.x_opt\n\n        # per-anchor per-dimension step sizes\n        steps = np.full((self.pop_size, dim), base_step)\n        # last successful sign memory for each anchor & coordinate: -1, 0, +1\n        last_sign = np.zeros((self.pop_size, dim), dtype=float)\n        # stagnation counter\n        stagn_count = np.zeros(self.pop_size, dtype=int)\n\n        # global best\n        idx_best = int(np.nanargmin(anchor_f))\n        x_best = anchors[idx_best].copy()\n        f_best = float(anchor_f[idx_best])\n\n        # main loop: strictly single-eval probes\n        while evals < self.budget:\n            improved_any = False\n\n            # occasional opposition evaluation of global best (single eval)\n            if rng.random() < self.opp_prob and evals < self.budget:\n                x_opp = np.clip(lb + ub - x_best, lb, ub)\n                # avoid re-eval same point\n                if not np.allclose(x_opp, x_best):\n                    f_opp = float(func(x_opp)); evals += 1\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy(); improved_any = True\n                        # replace worst anchor cheaply\n                        iw = int(np.nanargmax(anchor_f))\n                        anchors[iw] = x_opp.copy(); anchor_f[iw] = f_opp\n                        steps[iw, :] = base_step\n                        last_sign[iw, :] = 0.0\n\n            # occasional one-eval blend between best and a random anchor\n            if rng.random() < self.blend_prob and evals < self.budget and self.pop_size > 1:\n                j = rng.integers(0, self.pop_size)\n                if j != idx_best:\n                    w = 0.65 + 0.7 * rng.random()  # weighted toward best but sometimes pushes beyond\n                    cand = np.clip(w * x_best + (1.0 - w) * anchors[j], lb, ub)\n                    # tiny random nudges to avoid exact duplicates\n                    if np.allclose(cand, x_best) or np.allclose(cand, anchors[j]):\n                        cand = np.clip(cand + 1e-6 * box_len * rng.standard_normal(dim), lb, ub)\n                    if not np.allclose(cand, x_best):\n                        f_cand = float(func(cand)); evals += 1\n                        if f_cand < f_best:\n                            f_best = f_cand; x_best = cand.copy(); improved_any = True\n                            iw = int(np.nanargmax(anchor_f))\n                            anchors[iw] = cand.copy(); anchor_f[iw] = f_cand\n                            steps[iw, :] = base_step\n                            last_sign[iw, :] = 0.0\n\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x0 = anchors[idx].copy()\n                f0 = anchor_f[idx]\n                improved_anchor = False\n\n                # random coordinate order for exploration fairness\n                coords = rng.permutation(dim)\n\n                for c in coords:\n                    if evals >= self.budget:\n                        break\n\n                    delta = steps[idx, c]\n                    if delta <= 1e-12 * box_len:\n                        continue\n\n                    # choose sign from memory or random\n                    sign = last_sign[idx, c] if last_sign[idx, c] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n\n                    # single-eval directional probe\n                    x_try = x0.copy()\n                    x_try[c] = np.clip(x0[c] + sign * delta, lb[c], ub[c])\n\n                    # if clipped to identical, record and shrink\n                    if np.allclose(x_try, x0):\n                        last_sign[idx, c] = 0.0\n                        steps[idx, c] *= self.failure_mult\n                        stagn_count[idx] += 1\n                        continue\n\n                    f_try = float(func(x_try)); evals += 1\n\n                    if f_try < f0:\n                        # accept single-eval improvement\n                        anchors[idx] = x_try.copy(); anchor_f[idx] = f_try\n                        x0 = x_try.copy(); f0 = f_try\n                        last_sign[idx, c] = sign\n                        steps[idx, c] *= self.success_mult\n                        stagn_count[idx] = 0\n                        improved_anchor = True\n                        # continue to next coordinate (still single-eval per coordinate)\n                    else:\n                        # no improvement: encourage opposite next time without extra eval\n                        last_sign[idx, c] = -sign\n                        steps[idx, c] *= self.failure_mult\n                        stagn_count[idx] += 1\n\n                # occasional single-eval diagonal probe combining two largest steps\n                if dim >= 2 and rng.random() < self.diag_prob and evals < self.budget:\n                    order_coords = np.argsort(steps[idx])[::-1]\n                    a = int(order_coords[0])\n                    b = int(order_coords[1]) if order_coords.size > 1 else a\n                    if a != b:\n                        sa = last_sign[idx, a] if last_sign[idx, a] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                        sb = last_sign[idx, b] if last_sign[idx, b] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                        x_diag = x0.copy()\n                        x_diag[a] = np.clip(x0[a] + sa * steps[idx, a], lb[a], ub[a])\n                        x_diag[b] = np.clip(x0[b] + sb * steps[idx, b], lb[b], ub[b])\n                        if not np.allclose(x_diag, x0):\n                            f_diag = float(func(x_diag)); evals += 1\n                            if f_diag < anchor_f[idx]:\n                                anchors[idx] = x_diag.copy(); anchor_f[idx] = f_diag\n                                x0 = x_diag.copy(); f0 = f_diag\n                                # reward coordinates mildly\n                                steps[idx, a] *= 1.12; steps[idx, b] *= 1.12\n                                last_sign[idx, a] = sa; last_sign[idx, b] = sb\n                                stagn_count[idx] = 0\n                                improved_anchor = True\n                            else:\n                                # shrink both\n                                steps[idx, a] *= self.failure_mult; steps[idx, b] *= self.failure_mult\n                                stagn_count[idx] += 1\n\n                # update global best\n                if anchor_f[idx] < f_best:\n                    f_best = float(anchor_f[idx]); x_best = anchors[idx].copy(); improved_any = True\n\n                # stagnation handling: respawn stale anchors (single-eval)\n                if stagn_count[idx] > max(12, 5 * dim) and evals < self.budget:\n                    stagn_count[idx] = 0\n                    if rng.random() < 0.7:\n                        # respawn near best with gaussian noise\n                        newp = x_best + self.respawn_noise * (ub - lb) * rng.standard_normal(dim)\n                    else:\n                        # full random injection\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    if not np.allclose(newp, anchors[idx]):\n                        f_new = float(func(newp)); evals += 1\n                        anchors[idx] = newp.copy(); anchor_f[idx] = f_new\n                        steps[idx, :] = base_step; last_sign[idx, :] = 0.0\n                        if f_new < f_best:\n                            f_best = f_new; x_best = newp.copy(); improved_any = True\n\n            # clip steps to sensible bounds\n            min_step = 1e-10 * box_len\n            max_step = 1.2 * box_len\n            steps = np.clip(steps, min_step, max_step)\n\n            # if full pass had no improvement, replace worst anchor occasionally (single-eval)\n            if not improved_any and evals < self.budget and rng.random() < 0.16:\n                iw = int(np.nanargmax(anchor_f))\n                new_rand = rng.uniform(lb, ub)\n                f_rand = float(func(new_rand)); evals += 1\n                anchors[iw] = new_rand; anchor_f[iw] = f_rand\n                steps[iw, :] = base_step; last_sign[iw, :] = 0.0\n                if f_rand < f_best:\n                    f_best = f_rand; x_best = new_rand.copy(); improved_any = True\n\n            # budget-aware break: stop heavy loops if few evals left\n            if self.budget - evals < max(4, dim // 2):\n                break\n\n        # final low-degree single-eval mesh refinement on x_best:\n        mesh = 0.08 * box_len\n        mesh_min = 1e-8 * box_len\n        # single-eval per coordinate per pass: choose direction from last_sign or random\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                s = 1.0\n                # pick preferred sign from aggregated last_sign across anchors (consensus) or best anchor\n                signs = last_sign[:, d]\n                if np.any(signs != 0):\n                    # weighted majority of nonzero signs\n                    nz = signs[signs != 0]\n                    s = np.sign(np.sum(nz))\n                    if s == 0:\n                        s = 1.0 if rng.random() < 0.5 else -1.0\n                else:\n                    s = 1.0 if rng.random() < 0.5 else -1.0\n\n                xp = x_best.copy()\n                xp[d] = np.clip(x_best[d] + s * mesh, lb[d], ub[d])\n                if not np.allclose(xp, x_best):\n                    fp = float(func(xp)); evals += 1\n                    if fp < f_best:\n                        f_best = fp; x_best = xp.copy(); improved = True\n                        # increase small local steps near best\n                        steps[:, d] = np.maximum(steps[:, d], 0.8 * mesh)\n                        continue\n                # do not attempt opposite to keep degree = 1\n\n            if not improved:\n                mesh *= 0.45  # shrink more aggressively to home in\n\n        # store final result\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm LowDegreeProgressiveAnchorsV2 scored 0.588 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["49beef6d-dff3-43de-8c03-613d52610af4"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9526223397072213}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9521910091699194}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9578088475689424}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.7162033560658223}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.7408479110049013}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6734880073152604}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.14115094168987674}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.17065218421885076}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.11485291618471716}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1267893119365162}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.11680409497432398}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.16429754267526553}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9956124568741376}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9961886250190554}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9963735926295268}], "aucs": [0.9526223397072213, 0.9521910091699194, 0.9578088475689424, 0.7162033560658223, 0.7408479110049013, 0.6734880073152604, 0.14115094168987674, 0.17065218421885076, 0.11485291618471716, 0.1267893119365162, 0.11680409497432398, 0.16429754267526553, 0.9956124568741376, 0.9961886250190554, 0.9963735926295268]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2682.0, "Edges": 2681.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9992542878448918, "Degree Variance": 1.9716623820192731, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.710355987055015, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3318980243874223, "Depth Entropy": 2.1653298790766407, "Assortativity": 0.0, "Average Eccentricity": 19.141685309470546, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003728560775540641, "Average Shortest Path": 11.139942718403125, "mean_complexity": 22.666666666666668, "total_complexity": 68.0, "mean_token_count": 760.3333333333334, "total_token_count": 2281.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "835c585d-4980-4335-8f83-5ee19cff472f", "fitness": 0.28431342986061026, "name": "OrthogonalSingleEvalAdaptiveTabu", "description": "Orthogonal Single-Eval Adaptive Tabu Search (OSAT) \u2014 always one-eval atomic moves that combine axis-signed signed-pushes, orthogonalized single-direction probes from recent successful moves, and a light tabu per-dimension with multiplicative step adaptation to keep degree = 1 and favor cheap robust exploitation.", "code": "import numpy as np\n\nclass OrthogonalSingleEvalAdaptiveTabu:\n    \"\"\"\n    Orthogonal Single-Eval Adaptive Tabu (OSAT)\n    - Strictly single-evaluation atomic moves after seeding: every iteration proposes exactly one candidate\n      and uses at most one function evaluation (degree = 1).\n    - Mixes axis-aligned signed pushes (with per-dim momentum and tabu), orthogonalized probes built from\n      recent successful directions, and occasional elite mixes / jitter for diversification.\n    - Maintains per-dim adaptive step sizes, lightweight per-dim slope-ish estimates (from accepted moves),\n      a small recent-direction memory, and an elite archive.\n    - Designed for noiseless unconstrained optimization in the fixed box [-5,5] (or func.bounds if provided).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_step=0.6, min_step=1e-6, max_step=2.5,\n                 elite_size=5, recent_mem=8, tabu_decay=0.92, stagn_patience=12, rng=None):\n        \"\"\"\n        Arguments:\n          budget: allowed function evaluations\n          dim: dimensionality\n          init_step: initial per-dim step magnitude (in fraction of bound range if bounds provided uniformly)\n          min_step, max_step: clamps for step sizes\n          elite_size: size of elite archive\n          recent_mem: number of recent successful direction unit vectors to remember\n          tabu_decay: multiplicative decay per iteration for per-dim tabu scores\n          stagn_patience: number of iterations without improvement before stronger diversification\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.elite_size = int(elite_size)\n        self.recent_mem = int(recent_mem)\n        self.tabu_decay = float(tabu_decay)\n        self.stagn_patience = int(stagn_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # enforce shape\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n        if ub.shape[0] != self.dim:\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        rng = self.rng\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # handle zero-budget: return center without evaluating\n        if self.budget <= 0:\n            x0 = lb + 0.5 * (ub - lb)\n            return float(np.inf), x0.copy()\n\n        # seed with a few cheap random samples (small to preserve budget)\n        n_init = min(6, max(1, self.budget // 200 + 1))\n        xs = []\n        fs = []\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            xs.append(x.copy())\n            fs.append(float(f))\n\n        # if no evaluation yet (budget exhausted), return center with inf score\n        if len(fs) == 0:\n            x0 = lb + 0.5 * (ub - lb)\n            return float(np.inf), x0.copy()\n\n        idx_best = int(np.argmin(fs))\n        x_best = xs[idx_best].copy()\n        f_best = float(fs[idx_best])\n\n        # per-dim steps: scaled by range and init_step fraction\n        range_vec = np.maximum(ub - lb, 1e-12)\n        steps = np.minimum(self.max_step, np.maximum(self.min_step, self.init_step * range_vec))\n\n        # elite archive (f, x)\n        elite = sorted(zip(fs, xs), key=lambda t: t[0])[: self.elite_size]\n        elite = [(float(fv), np.array(xv, dtype=float).copy()) for fv, xv in elite]\n\n        # recent successful direction unit vectors\n        recent_dirs = []\n\n        # lightweight per-dim slope-ish estimator and momentum\n        slope_est = np.zeros(self.dim, dtype=float)   # crude signed slope estimate\n        momentum = np.zeros(self.dim, dtype=float)    # accumulated signed pushes\n\n        # tabu-like score for dimensions to reduce repeated futile probing\n        tabu = np.zeros(self.dim, dtype=float)\n\n        stagn = 0\n        iters = 0\n\n        def push_recent_dir(v):\n            if v is None:\n                return\n            nrm = np.linalg.norm(v)\n            if nrm < 1e-12:\n                return\n            u = v / nrm\n            recent_dirs.insert(0, u)\n            if len(recent_dirs) > self.recent_mem:\n                recent_dirs.pop()\n\n        def update_elite(candidate_x, candidate_f):\n            nonlocal elite\n            candidate_x = np.array(candidate_x, dtype=float).copy()\n            candidate_f = float(candidate_f)\n            tol = 1e-8 + 1e-8 * np.linalg.norm(range_vec)\n            add = True\n            for f_e, x_e in elite:\n                if np.linalg.norm(candidate_x - x_e) < tol:\n                    add = False\n                    break\n            if add:\n                elite.append((candidate_f, candidate_x))\n                elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n\n        # single-eval main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            iters += 1\n            improved = False\n\n            # decay tabu slowly each iteration\n            tabu *= self.tabu_decay\n\n            # mode selection: prefer axis pushes, then orthogonal probes, then elite mixes / jitter\n            r = rng.random()\n\n            # compute some statistics\n            med_step = max(1e-12, np.median(steps))\n            step_weights = steps / (np.sum(steps) + 1e-20)\n            # increase weight on dims with larger |slope| or momentum, but penalize tabu\n            weight_boost = 1.0 + 2.5 * (np.abs(slope_est) / (np.abs(slope_est).mean() + 1e-12))\n            weights = step_weights * (1.0 + 0.5 * np.abs(momentum)) * weight_boost\n            # penalize tabu (higher tabu reduces selection probability)\n            tabu_factor = 1.0 / (1.0 + tabu)\n            weights = weights * tabu_factor\n            if np.all(weights <= 0) or not np.isfinite(weights).all():\n                weights = np.ones(self.dim)\n            weights = weights / np.sum(weights)\n\n            # choose one-eval candidate\n            x_new = None\n            used_dims = None  # dims primarily involved, for adaptation/tabu\n\n            if r < 0.55:\n                # Axis-aligned signed push (single-eval)\n                # choose a coordinate by weights biased by tabu and slope\n                idx = int(rng.choice(self.dim, p=weights))\n                # sign preference from momentum/slope, else random\n                pref = 0.0\n                if abs(momentum[idx]) > 0.5:\n                    pref = np.sign(momentum[idx])\n                elif abs(slope_est[idx]) > 0:\n                    pref = -np.sign(slope_est[idx])  # downhill direction: negative slope means decrease when moving positive\n                sign = int(pref) if pref != 0 else (1 if rng.random() < 0.5 else -1)\n                delta = np.zeros(self.dim)\n                delta[idx] = sign * steps[idx]\n                cand = clip(x_best + delta)\n                # if clip collapses (no move), jitter tiny random on that dim\n                if np.allclose(cand, x_best, atol=1e-12):\n                    cand = clip(x_best + delta + rng.normal(scale=1e-6 * range_vec, size=self.dim))\n                x_new = cand\n                used_dims = np.array([idx], dtype=int)\n\n            elif r < 0.9:\n                # Orthogonalized single-direction probe built from recent_dirs or random\n                if recent_dirs and rng.random() < 0.8:\n                    # combine two recent directions to create a new probe direction\n                    i = min(len(recent_dirs)-1, rng.integers(len(recent_dirs)))\n                    j = min(len(recent_dirs)-1, rng.integers(len(recent_dirs)))\n                    d1 = recent_dirs[i]\n                    d2 = recent_dirs[j]\n                    dir_raw = 0.6 * d1 + 0.4 * d2 * (1.0 - 0.5 * rng.random())\n                else:\n                    dir_raw = rng.normal(size=self.dim)\n                # attempt to orthogonalize partially to last direction to encourage new curvature probing\n                if recent_dirs:\n                    last = recent_dirs[0]\n                    proj = np.dot(last, dir_raw) * last\n                    dir_raw = dir_raw - 0.6 * proj\n                dn = np.linalg.norm(dir_raw)\n                if dn < 1e-12:\n                    # fallback to a random axis\n                    idx = int(rng.integers(self.dim))\n                    dir_vec = np.zeros(self.dim); dir_vec[idx] = 1.0\n                    used_dims = np.array([idx], dtype=int)\n                else:\n                    dir_vec = dir_raw / dn\n                    # determine dominant dims for adaptation bookkeeping\n                    used_dims = np.flatnonzero(np.abs(dir_vec) > (0.3 * np.max(np.abs(dir_vec))))\n                    if used_dims.size == 0:\n                        used_dims = np.array([int(np.argmax(np.abs(dir_vec)))], dtype=int)\n                # scale by median step and a small factor influenced by norms\n                scale = med_step * (1.0 + 0.25 * min(len(recent_dirs), self.recent_mem))\n                x_new = clip(x_best + dir_vec * scale)\n\n            else:\n                # elite mix / jitter: single-eval mixing of elites plus light jitter\n                if len(elite) >= 2 and rng.random() < 0.9:\n                    ids = rng.choice(len(elite), size=2, replace=False)\n                    xa = elite[ids[0]][1]\n                    xb = elite[ids[1]][1]\n                    w = rng.random()\n                    mix = w * xa + (1.0 - w) * xb\n                elif elite:\n                    mix = elite[0][1]\n                else:\n                    mix = x_best\n                jitter = rng.normal(scale=0.15 * med_step, size=self.dim)\n                x_new = clip(mix + jitter)\n                # set used dims to those with large jitter\n                used_dims = np.flatnonzero(np.abs(jitter) > 0.5 * np.max(np.abs(jitter)))\n                if used_dims.size == 0:\n                    used_dims = np.array([int(rng.integers(self.dim))], dtype=int)\n\n            # safety if x_new equals x_best (rare): try small random jitter rather than waste the evaluation\n            if x_new is None:\n                x_new = clip(x_best + rng.normal(scale=0.1 * med_step, size=self.dim))\n                used_dims = np.arange(self.dim)\n            if np.allclose(x_new, x_best, atol=1e-12):\n                # generate a tiny perturbation to make progress\n                x_new = clip(x_best + rng.normal(scale=1e-6 * med_step + 1e-12, size=self.dim))\n\n            # single evaluation (degree = 1)\n            # ensure we don't exceed budget\n            if evals >= self.budget:\n                break\n            f_new = float(func(x_new))\n            evals += 1\n\n            # update elite\n            update_elite(x_new, f_new)\n\n            # decide acceptance\n            if f_new < f_best - 1e-15:\n                # accept improvement\n                delta = x_new - x_best\n                # estimate directional slope-ish per-dim (signed)\n                denom = np.sum(delta * delta) + 1e-12\n                # directional slope: (f_new - f_old) projected into per-dim contributions\n                contrib = (f_new - f_best) * (delta / (denom + 1e-30))\n                # update slope_est as moving average\n                slope_est = 0.85 * slope_est + 0.15 * contrib\n                # update momentum: move sign towards improvement\n                momentum = 0.8 * momentum\n                momentum[used_dims] += np.sign(delta[used_dims]) * 1.0\n                push_recent_dir(delta)\n                x_best_old = x_best.copy()\n                x_best = x_new.copy()\n                f_best = f_new\n                improved = True\n                stagn = 0\n                # increase steps on used dims modestly\n                for d in np.atleast_1d(used_dims):\n                    steps[d] = min(self.max_step, steps[d] * (1.06 + 0.02 * rng.random()))\n                # slightly nudge all steps upward (small optimism after success)\n                steps = np.minimum(self.max_step, steps * (1.01 + 0.005 * rng.random()))\n                # reduce tabu for used dims (reward)\n                tabu[used_dims] *= 0.6\n            else:\n                # rejection: update slope_est weakly in opposite direction (learn from failure)\n                delta_fail = x_new - x_best\n                denom_fail = np.sum(delta_fail * delta_fail) + 1e-12\n                contrib_fail = (f_new - f_best) * (delta_fail / (denom_fail + 1e-30))\n                slope_est = 0.97 * slope_est + 0.03 * contrib_fail\n                # penalize used dims in tabu to reduce immediate retries\n                tabu[used_dims] += 1.0 + 2.0 * rng.random()\n                # shrink steps on the dims involved moderately\n                for d in np.atleast_1d(used_dims):\n                    steps[d] = max(self.min_step, steps[d] * (0.80 - 0.05 * rng.random()))\n                # slight global shrink when repeatedly failing\n                steps = np.maximum(self.min_step, steps * (0.995))\n                stagn += 1\n\n            # Occasionally try mirrored candidate of best (single eval) if stagnating mildly\n            if stagn >= 6 and evals < self.budget and rng.random() < 0.18:\n                x_mirror = clip(lb + ub - x_best)\n                fm = float(func(x_mirror))\n                evals += 1\n                update_elite(x_mirror, fm)\n                if fm < f_best:\n                    push_recent_dir(x_mirror - x_best)\n                    slope_est = 0.9 * slope_est + 0.1 * ((fm - f_best) * (x_mirror - x_best) / (np.sum((x_mirror - x_best)**2) + 1e-12))\n                    x_best = x_mirror.copy()\n                    f_best = fm\n                    stagn = 0\n                    # reward steps a little\n                    steps = np.minimum(self.max_step, steps * 1.05)\n                else:\n                    # penalize global exploration budget by small shrink\n                    steps = np.maximum(self.min_step, steps * 0.98)\n\n            # stronger diversification when prolonged stagnation, but still single-eval attempts\n            if stagn >= self.stagn_patience and evals < self.budget:\n                # try mixing best two elites with bigger jitter once\n                if len(elite) >= 2:\n                    mix = 0.5 * (elite[0][1] + elite[min(1, len(elite)-1)][1])\n                    jitter = rng.normal(scale=0.4 * np.median(steps), size=self.dim)\n                    x_mix = clip(mix + jitter)\n                    fmix = float(func(x_mix))\n                    evals += 1\n                    update_elite(x_mix, fmix)\n                    if fmix < f_best:\n                        push_recent_dir(x_mix - x_best)\n                        x_best = x_mix.copy()\n                        f_best = fmix\n                        stagn = 0\n                        steps = np.minimum(self.max_step, steps * 1.12)\n                        continue  # continue main loop\n                # if still stagnating, single random jump around center of elites or around best\n                if evals < self.budget:\n                    if len(elite) >= 2:\n                        center = 0.5 * (elite[0][1] + elite[min(1, len(elite)-1)][1])\n                    else:\n                        center = x_best\n                    jump = rng.normal(scale=0.9, size=self.dim) * range_vec * 0.6\n                    x_jump = clip(center + jump)\n                    fj = float(func(x_jump))\n                    evals += 1\n                    update_elite(x_jump, fj)\n                    if fj < f_best:\n                        push_recent_dir(x_jump - x_best)\n                        x_best = x_jump.copy()\n                        f_best = fj\n                        stagn = 0\n                        steps = np.minimum(self.max_step, steps * 1.15)\n                    else:\n                        # reset some steps to encourage different exploration\n                        steps = np.maximum(self.min_step, steps * 0.45)\n                        stagn = 0\n\n            # ensure steps remain within bounds\n            steps = np.minimum(self.max_step, np.maximum(self.min_step, steps))\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 13, "feedback": "The algorithm OrthogonalSingleEvalAdaptiveTabu scored 0.284 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["964ff4d1-eae7-4989-a9da-2f5488ddd047"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.97687617332192}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9158742186713125}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9585474469975844}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.11598075597431523}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.1414688622308986}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.044071909953872446}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.012700300724054814}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.013835199798128639}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.8287761020000255}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.12336544299359198}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.13300503524345042}], "aucs": [0.97687617332192, 0.9158742186713125, 0.9585474469975844, 0.11598075597431523, 0.1414688622308986, 0.044071909953872446, 4.999999999999449e-05, 4.999999999999449e-05, 0.012700300724054814, 4.999999999999449e-05, 4.999999999999449e-05, 0.013835199798128639, 0.8287761020000255, 0.12336544299359198, 0.13300503524345042]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3026.0, "Edges": 3025.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.9993390614672835, "Degree Variance": 2.1797748440591325, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.48102189781022, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3086039386628812, "Depth Entropy": 2.205423420590518, "Assortativity": 0.0, "Average Eccentricity": 18.61169861202908, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.00033046926635822867, "Average Shortest Path": 10.652191639400677, "mean_complexity": 10.0, "total_complexity": 60.0, "mean_token_count": 453.3333333333333, "total_token_count": 2720.0, "mean_parameter_count": 3.1666666666666665, "total_parameter_count": 19.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "6cd3ea29-a941-4770-ab4b-0e5e0cca3077", "fitness": 0.592909218246615, "name": "MicroMomentumSingleCoordinateSearch", "description": "Strict single-coordinate micro-momentum search \u2014 all atomic moves change exactly one coordinate per evaluation, using per-dimension adaptive steps, momentum-biased one-sided pushes, priority-driven coordinate selection, and single-coordinate respawns for diversification.", "code": "import numpy as np\n\nclass MicroMomentumSingleCoordinateSearch:\n    \"\"\"\n    Micro-Momentum Single-Coordinate Search (MMSCS)\n\n    Main ideas:\n    - Strictly single-coordinate atomic moves: every evaluated probe differs from its anchor\n      in exactly one coordinate. This minimizes the operation \"degree\" and keeps evaluations\n      very cheap and informative per dimension.\n    - Maintain a small population of anchors. Initialize them by single-coordinate perturbations\n      from a single base sample to keep initial moves degree-1 as well.\n    - Per-anchor, per-dimension adaptive step sizes and a tiny \"momentum\" memory (preferred sign).\n    - Priority-driven coordinate selection: prefer coords with larger step * uncertainty.\n    - Stagnation handled via single-coordinate respawns (randomizing one coordinate) or\n      single-coordinate large heavy-tail pushes \u2014 still degree-1.\n    - Budget-respecting safe evaluations.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.12, success_mult=1.35, failure_mult=0.55,\n                 heavy_prob=0.02, respawn_prob=0.12, seed=None):\n        # required interface\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        # population\n        if pop_size is None:\n            # keep population small relative to dim and budget\n            self.pop_size = max(2, min(8, int(round(4 + 0.05 * self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        # adaptation parameters\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.heavy_prob = float(heavy_prob)\n        self.respawn_prob = float(respawn_prob)\n\n    def _get_bounds(self, func):\n        # respect function-provided bounds if present; otherwise use default [-5,5]^dim\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        # scale relative to box (use L2 length)\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n\n        evals = 0\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            fv = float(func(x))\n            evals += 1\n            return fv\n\n        # --- Initialize anchors using only single-coordinate changes from one base point ---\n        anchors = np.empty((self.pop_size, dim), dtype=float)\n        anchor_f = np.full(self.pop_size, np.inf)\n        # base sample (full-dim first eval)\n        base = rng.uniform(lb, ub)\n        anchors[0] = base.copy()\n        anchor_f[0] = safe_eval(anchors[0])\n        # create other anchors by changing one coordinate at a time from the base\n        for i in range(1, self.pop_size):\n            if evals >= self.budget:\n                # if budget exhausted, fill remaining anchors with base (no evals)\n                anchors[i] = anchors[0].copy()\n                anchor_f[i] = anchor_f[0]\n                continue\n            anc = anchors[0].copy()\n            c = rng.integers(0, dim)\n            # set that coordinate to a random value in bounds (single-coordinate change)\n            anc[c] = rng.uniform(lb[c], ub[c])\n            anchors[i] = anc\n            anchor_f[i] = safe_eval(anc)\n\n        # per-anchor per-dimension steps and momentum memory in [-1,1]\n        steps = np.full((self.pop_size, dim), base_step)\n        momentum = np.zeros((self.pop_size, dim), dtype=float)  # preferred sign, small magnitude = uncertain\n        stagn = np.zeros(self.pop_size, dtype=int)  # stagnation counters\n\n        # best tracking\n        idx_best = int(np.nanargmin(anchor_f))\n        x_best = anchors[idx_best].copy()\n        f_best = float(anchor_f[idx_best])\n\n        # main loop: strictly single-coordinate probes\n        while evals < self.budget:\n            improved_any = False\n\n            # occasionally try a heavy-tail single-coordinate push from best (still degree-1)\n            if rng.random() < self.heavy_prob and evals < self.budget:\n                # choose one coordinate to perturb heavily using Cauchy-like one-d coordinate\n                c = rng.integers(0, dim)\n                u = rng.random()\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # scale heavy jump relative to box but only on a single coordinate\n                jump = 0.22 * box_len * cauchy\n                xj = x_best.copy()\n                xj[c] = np.clip(x_best[c] + jump, lb[c], ub[c])\n                fj = safe_eval(xj)\n                if fj < f_best:\n                    f_best = fj; x_best = xj.copy(); improved_any = True\n                    # replace the worst anchor by this single-coordinate improved point\n                    idx_w = int(np.nanargmax(anchor_f))\n                    anchors[idx_w] = xj.copy()\n                    anchor_f[idx_w] = fj\n                    steps[idx_w, :] = base_step\n                    momentum[idx_w, :] = 0.0\n\n            # iterate anchors in randomized priority: worse anchors get more attention sometimes\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x0 = anchors[idx].copy()\n                f0 = anchor_f[idx]\n                improved_anchor = False\n\n                # compute per-coordinate priority: prefer larger steps and larger momentum uncertainty & stagnation\n                # priority = step * (1 + |momentum| * 0.6) * (1 + stagnation_factor)\n                mom_strength = np.abs(momentum[idx])\n                stagn_factor = 1.0 + (stagn[idx] / (4.0 + dim))\n                priority = steps[idx] * (1.0 + 0.6 * mom_strength) * stagn_factor\n                # to avoid deterministic behaviour, add tiny noise\n                priority = priority + 1e-12 * rng.random(dim)\n\n                # pick coordinate probabilistically (but biased to high priority)\n                prob = priority / np.sum(priority)\n                c = rng.choice(dim, p=prob)\n\n                # choose sign biased by momentum for this coordinate\n                m = momentum[idx, c]\n                pref_sign = 1 if m >= 0 else -1\n                if rng.random() < (0.5 + 0.45 * min(1.0, abs(m))):\n                    sign = pref_sign\n                else:\n                    sign = 1 if rng.random() < 0.5 else -1\n\n                delta = steps[idx, c]\n                if delta <= 1e-12 * box_len:\n                    # too small to move meaningfully -> increment stagnation and consider respawn\n                    stagn[idx] += 1\n                else:\n                    x_try = x0.copy()\n                    x_try[c] = np.clip(x0[c] + sign * delta, lb[c], ub[c])\n                    f_try = safe_eval(x_try)\n\n                    if f_try < f0:\n                        # accept improvement\n                        anchors[idx] = x_try.copy()\n                        anchor_f[idx] = f_try\n                        x0 = x_try.copy(); f0 = f_try\n                        steps[idx, c] = min(steps[idx, c] * self.success_mult, 1.5 * box_len)\n                        # strengthen momentum toward sign, capped\n                        momentum[idx, c] = np.clip(0.75 * momentum[idx, c] + 0.25 * float(sign), -1.0, 1.0)\n                        stagn[idx] = 0\n                        improved_anchor = True\n                    else:\n                        # failure: shrink step and decay momentum magnitude\n                        steps[idx, c] = max(steps[idx, c] * self.failure_mult, 1e-12 * box_len)\n                        momentum[idx, c] = 0.88 * momentum[idx, c]\n                        stagn[idx] += 1\n\n                # update global best\n                if anchor_f[idx] < f_best:\n                    f_best = float(anchor_f[idx])\n                    x_best = anchors[idx].copy()\n                    improved_any = True\n\n                # handle stagnation with strict single-coordinate respawn or large single-coord jump\n                if stagn[idx] > max(8, 3 * dim) and evals < self.budget:\n                    stagn[idx] = 0\n                    # probabilistic single-coordinate respawn: randomize one coordinate only\n                    if rng.random() < self.respawn_prob:\n                        rc = rng.integers(0, dim)\n                        newp = anchors[idx].copy()\n                        newp[rc] = rng.uniform(lb[rc], ub[rc])\n                        f_new = safe_eval(newp)\n                        anchors[idx] = newp\n                        anchor_f[idx] = f_new\n                        steps[idx, rc] = base_step\n                        momentum[idx, rc] = 0.0\n                        # reset other steps mildly\n                        steps[idx, :] = np.maximum(steps[idx, :], base_step * 0.2)\n                        if f_new < f_best:\n                            f_best = f_new; x_best = newp.copy(); improved_any = True\n                    else:\n                        # single-coordinate large push: pick coordinate and do a heavy single-dim perturbation\n                        rc = rng.integers(0, dim)\n                        u = rng.random()\n                        cauchy = np.tan(np.pi * (u - 0.5))\n                        jump = 0.18 * box_len * cauchy\n                        newp = anchors[idx].copy()\n                        newp[rc] = np.clip(anchors[idx, rc] + jump, lb[rc], ub[rc])\n                        f_new = safe_eval(newp)\n                        anchors[idx] = newp\n                        anchor_f[idx] = f_new\n                        steps[idx, rc] = base_step\n                        momentum[idx, rc] = 0.0\n                        if f_new < f_best:\n                            f_best = f_new; x_best = newp.copy(); improved_any = True\n\n            # clip steps to safe bounds\n            min_step = 1e-10 * box_len\n            max_step = 1.0 * box_len\n            steps = np.clip(steps, min_step, max_step)\n\n            # if nothing improved in a full pass, perform a targeted single-coordinate opposition on the worst anchor\n            if (not improved_any) and (evals < self.budget) and rng.random() < 0.14:\n                idx_w = int(np.nanargmax(anchor_f))\n                # opposition only along one coordinate to keep degree-1\n                rc = rng.integers(0, dim)\n                oppos_val = lb[rc] + ub[rc] - anchors[idx_w, rc]\n                oppos = anchors[idx_w].copy()\n                # small gaussian jitter on that coordinate\n                oppos[rc] = np.clip(oppos_val + 0.03 * (ub[rc] - lb[rc]) * rng.standard_normal(), lb[rc], ub[rc])\n                f_op = safe_eval(oppos)\n                anchors[idx_w] = oppos\n                anchor_f[idx_w] = f_op\n                steps[idx_w, rc] = base_step\n                momentum[idx_w, rc] = 0.0\n                if f_op < f_best:\n                    f_best = f_op; x_best = oppos.copy()\n\n            # early break to final refinement when budget low\n            if self.budget - evals < max(6, dim):\n                break\n\n        # Final low-degree (single-coordinate) refinement around x_best: one-sided progressive mesh\n        mesh = 0.10 * box_len\n        mesh_min = 1e-8 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # single-sided positive probe\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + mesh, lb[d], ub[d])\n                f_try = safe_eval(x_try)\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy(); improved = True\n                    continue\n                # single-sided negative probe\n                if evals < self.budget:\n                    x_try = x_best.copy()\n                    x_try[d] = np.clip(x_try[d] - mesh, lb[d], ub[d])\n                    f_try = safe_eval(x_try)\n                    if f_try < f_best:\n                        f_best = f_try; x_best = x_try.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        # store and return\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 13, "feedback": "The algorithm MicroMomentumSingleCoordinateSearch scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["baa412d0-e0e0-4c35-b441-2d77e6b55032"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9197291555487732}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9295488848325748}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9221300217463096}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8809131023823304}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8843537378760381}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8816124829677947}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08058673876344258}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08894810557503297}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0863805485302177}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.08729305293119283}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.10136207220904814}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07895960836305782}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9900521960391488}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9843601310394628}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9774084348948011}], "aucs": [0.9197291555487732, 0.9295488848325748, 0.9221300217463096, 0.8809131023823304, 0.8843537378760381, 0.8816124829677947, 0.08058673876344258, 0.08894810557503297, 0.0863805485302177, 0.08729305293119283, 0.10136207220904814, 0.07895960836305782, 0.9900521960391488, 0.9843601310394628, 0.9774084348948011]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2366.0, "Edges": 2365.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9991546914623837, "Degree Variance": 2.1808953125033494, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.237037037037037, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3202777841051612, "Depth Entropy": 2.071756931691881, "Assortativity": 1.498841305301559e-08, "Average Eccentricity": 18.64835164835165, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00042265426880811494, "Average Shortest Path": 10.283701271894474, "mean_complexity": 10.75, "total_complexity": 43.0, "mean_token_count": 501.25, "total_token_count": 2005.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "71c959dc-5827-41ec-9bac-06642da14323", "fitness": 0.11974058788084868, "name": "SeparableLinearPushLite", "description": "Separable Linear Push \u2014 Ultra-low-degree single-eval coordinate pushes with EMA slope estimates, momentum-biased coordinate scheduling, at-most-one extra confirmation extrapolation (degree \u2264 2), and compact single-eval diversification.", "code": "import numpy as np\n\nclass SeparableLinearPushLite:\n    \"\"\"\n    Separable Linear Push \u2014 Lite variant (SLP-Lite)\n\n    Main ideas / refinements over original SLP:\n      - Strictly ultra-low evaluation degree: most iterations perform exactly 1 eval\n        (a single coordinate probe). Optionally a single extra confirmation/extrapolation\n        evaluation is allowed (so max degree per iteration = 2). No small 2-D scans.\n      - Use lightweight exponential-moving-average (EMA) slope estimates from one-sided\n        finite differences to rank coordinates; maintain a per-dimension momentum sign\n        to bias successful directions.\n      - Adaptive per-dimension step sizes (grow on success, shrink on failure).\n      - Very cheap diversification (single-eval opposition / Gaussian jitter).\n      - Strong budget enforcement: func called at most self.budget times.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_step=0.8, min_step=1e-6, max_step=2.5,\n                 grow=1.25, shrink=0.6, ema_alpha=0.25, momentum_decay=0.9,\n                 stagn_patience=20, rng=None):\n        \"\"\"\n        Args:\n          budget: total number of function evaluations allowed\n          dim: dimensionality of the problem\n          init_step: initial per-dimension step magnitude\n          min_step, max_step: per-dimension step clamp\n          grow/shrink: multiplicative factors to adapt steps on success/failure\n          ema_alpha: EMA weight for slope updates from single-sided probes (0..1)\n          momentum_decay: per-iter decay for momentum accumulator (0..1)\n          stagn_patience: iterations without improvement before stronger diversification\n          rng: numpy.random.Generator or seed (or None)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.ema_alpha = float(ema_alpha)\n        self.momentum_decay = float(momentum_decay)\n        self.stagn_patience = int(stagn_patience)\n        if rng is None:\n            self.rng = np.random.default_rng()\n        elif isinstance(rng, np.random.Generator):\n            self.rng = rng\n        else:\n            self.rng = np.random.default_rng(rng)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        # quick return if no budget\n        if self.budget <= 0:\n            x0 = lb + 0.5 * (ub - lb)\n            return float(func(x0)), x0.copy()\n\n        evals = 0\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n\n        # initial point: random within bounds (1 eval)\n        x_best = clip(lb + self.rng.random(self.dim) * (ub - lb))\n        f_best = float(func(x_best))\n        evals += 1\n\n        # per-dimension step sizes, EMA slope estimates, momentum accumulator\n        step = np.full(self.dim, self.init_step, dtype=float)\n        slope = np.zeros(self.dim, dtype=float)       # EMA estimate of d f / d x_i (one-sided)\n        momentum = np.zeros(self.dim, dtype=float)    # accumulates recent successful signed moves\n\n        stagn = 0\n        iters = 0\n        eye = np.eye(self.dim)\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # main loop: each iteration aims to use 1 eval; optional extra eval allowed (max 2)\n        while evals < self.budget:\n            iters += 1\n            improved = False\n\n            # schedule diversification when stagnating\n            if stagn >= self.stagn_patience and self.rng.random() < 0.4:\n                # single-eval opposition or gaussian jitter (only one eval)\n                if self.rng.random() < 0.5:\n                    # opposition point: reflect around midpoint\n                    center = 0.5 * (lb + ub)\n                    x_try = clip(2.0 * center - x_best)\n                else:\n                    avg_step = np.mean(step)\n                    x_try = clip(x_best + self.rng.normal(scale=0.6 * avg_step, size=self.dim))\n                f_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_best:\n                    x_best = x_try.copy()\n                    f_best = f_try\n                    improved = True\n                else:\n                    # modestly randomize some steps to escape plateaus\n                    idxs = self.rng.choice(self.dim, size=max(1, self.dim // 8), replace=False)\n                    for k in idxs:\n                        step[k] = min(self.max_step, step[k] * (1.0 + 0.15 * self.rng.random()))\n                    # slight global shrink to refine\n                    step = np.maximum(self.min_step, step * 0.98)\n            else:\n                # pick coordinate: priority = abs(slope)*step + momentum_bonus\n                pri = np.abs(slope) * step + 1e-12\n                # add a small momentum bias to prefer recently-successful signed moves\n                mom_bonus = 0.25 * np.abs(momentum)\n                pri = pri + mom_bonus\n                if np.all(pri == pri[0]):  # all identical (flat)\n                    idx = int(self.rng.integers(self.dim))\n                else:\n                    probs = pri / pri.sum()\n                    idx = int(self.rng.choice(self.dim, p=probs))\n\n                # propose direction: sign opposite to slope (we minimize) but use momentum occasionally\n                if slope[idx] > 0:\n                    suggested = -1.0\n                elif slope[idx] < 0:\n                    suggested = 1.0\n                else:\n                    suggested = 1.0 if self.rng.random() < 0.5 else -1.0\n\n                # momentum may flip/strengthen direction (probabilistic)\n                if np.abs(momentum[idx]) > 0 and self.rng.random() < 0.6:\n                    dir_from_mom = np.sign(momentum[idx])\n                    # combine with slope suggestion, prefer momentum if consistent\n                    if dir_from_mom == suggested:\n                        direction = suggested\n                    else:\n                        # choose momentum with small prob proportional to its magnitude\n                        if self.rng.random() < min(0.35, np.abs(momentum[idx])/(np.abs(momentum[idx])+1e-12)):\n                            direction = dir_from_mom\n                        else:\n                            direction = suggested\n                else:\n                    direction = suggested\n\n                # small random flip to maintain exploration\n                if self.rng.random() < 0.07:\n                    direction = -direction\n\n                # primary single-eval probe\n                delta = direction * step[idx] * eye[idx]\n                x_probe = clip(x_best + delta)\n                f_probe = safe_eval(x_probe)\n                if f_probe is None:\n                    break\n\n                # compute one-sided slope estimate: (f_probe - f_best) / (delta_x)\n                dx = (x_probe[idx] - x_best[idx])\n                if abs(dx) > 0:\n                    grad_est = (f_probe - f_best) / dx\n                else:\n                    grad_est = 0.0\n                # update EMA slope (we keep sign consistent with derivative)\n                slope[idx] = (1.0 - self.ema_alpha) * slope[idx] + self.ema_alpha * grad_est\n\n                # if probe improved, accept. Optionally attempt one extrapolation (single extra eval)\n                if f_probe < f_best:\n                    # accept probe\n                    prev = x_best.copy()\n                    x_best = x_probe.copy()\n                    f_best = f_probe\n                    improved = True\n                    # update momentum positively for this coordinate\n                    momentum[idx] = momentum[idx] * self.momentum_decay + 1.0 * direction\n                    # try one quick extrapolation with limited chance and only if budget remains\n                    if evals < self.budget and self.rng.random() < 0.28:\n                        # extrapolate by factor (1.6) of last delta but only one extra eval\n                        ext_factor = 1.6\n                        x_ext = clip(x_best + ext_factor * delta)\n                        f_ext = safe_eval(x_ext)\n                        if f_ext is None:\n                            break\n                        # compute extended gradient estimate if meaningful\n                        dx2 = (x_ext[idx] - x_best[idx])\n                        if abs(dx2) > 0:\n                            grad_est2 = (f_ext - f_best) / dx2\n                            slope[idx] = (1.0 - self.ema_alpha) * slope[idx] + self.ema_alpha * grad_est2\n                        if f_ext < f_best:\n                            x_best = x_ext.copy()\n                            f_best = f_ext\n                            # successful extrapolation -> grow step\n                            step[idx] = min(self.max_step, step[idx] * self.grow)\n                            momentum[idx] = momentum[idx] * self.momentum_decay + 1.5 * direction\n                        else:\n                            # unsuccessful extrapolation: slightly shrink\n                            step[idx] = max(self.min_step, step[idx] * self.shrink)\n                    else:\n                        # simple success: grow step\n                        step[idx] = min(self.max_step, step[idx] * self.grow)\n                else:\n                    # no improvement: treat as failure, shrink step and update momentum\n                    step[idx] = max(self.min_step, step[idx] * self.shrink)\n                    # negative reinforcement to momentum\n                    momentum[idx] = momentum[idx] * self.momentum_decay - 0.5 * direction\n\n            # drift momentum towards zero (decay)\n            momentum *= self.momentum_decay\n\n            # bookkeeping of stagnation\n            if improved:\n                stagn = 0\n                # modest global growth to encourage continued progress\n                step = np.minimum(self.max_step, step * 1.03)\n            else:\n                stagn += 1\n                # gentle global shrink to refine\n                step = np.maximum(self.min_step, step * 0.985)\n\n            # occasional cheap single-eval random blend of two dims (1 eval)\n            if self.rng.random() < 0.04 and evals < self.budget:\n                # choose two dims and mix small fractions (single sample)\n                i = int(self.rng.integers(self.dim))\n                j = int(self.rng.integers(self.dim - 1))\n                if j >= i:\n                    j += 1\n                mix = self.rng.uniform(0.2, 0.8)\n                delta = np.zeros(self.dim)\n                delta[i] = mix * step[i] * (1.0 if self.rng.random() < 0.5 else -1.0)\n                delta[j] = (1.0 - mix) * step[j] * (1.0 if self.rng.random() < 0.5 else -1.0)\n                x_mix = clip(x_best + delta)\n                f_mix = safe_eval(x_mix)\n                if f_mix is None:\n                    break\n                if f_mix < f_best:\n                    x_best = x_mix.copy()\n                    f_best = f_mix\n                    stagn = 0\n                    # modest grow for involved dims\n                    step[i] = min(self.max_step, step[i] * self.grow)\n                    step[j] = min(self.max_step, step[j] * self.grow)\n\n            # longer stagnation: single-eval random restart near center\n            if stagn >= 4 * self.stagn_patience and evals < self.budget:\n                center = 0.5 * (lb + ub)\n                rng_scale = 0.35 * (ub - lb)\n                x_restart = clip(center + self.rng.normal(scale=rng_scale))\n                f_restart = safe_eval(x_restart)\n                if f_restart is None:\n                    break\n                if f_restart < f_best:\n                    x_best = x_restart.copy()\n                    f_best = f_restart\n                    stagn = 0\n                    # reset slope and momentum lightly\n                    slope[:] = 0.0\n                    momentum[:] = 0.0\n                    step[:] = np.minimum(self.max_step, np.full(self.dim, self.init_step))\n                else:\n                    # increase steps to escape\n                    step = np.minimum(self.max_step, step * 1.3)\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 13, "feedback": "The algorithm SeparableLinearPushLite scored 0.120 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5951c2b-b9ff-4352-bb47-76f19aa885c0"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5121319202221757}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.4528520189478449}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.6793389180746622}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.038865134932732914}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.04056859067812968}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.07190223535718576}], "aucs": [0.5121319202221757, 0.4528520189478449, 0.6793389180746622, 4.999999999999449e-05, 4.999999999999449e-05, 0.038865134932732914, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.04056859067812968, 0.07190223535718576]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2135.0, "Edges": 2134.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.9990632318501171, "Degree Variance": 1.9587813238705853, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.456676860346585, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3174876624169627, "Depth Entropy": 2.1877783138211067, "Assortativity": 0.0, "Average Eccentricity": 17.74473067915691, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.000468384074941452, "Average Shortest Path": 10.459531747616927, "mean_complexity": 12.5, "total_complexity": 50.0, "mean_token_count": 472.25, "total_token_count": 1889.0, "mean_parameter_count": 4.25, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "2ce99025-003a-46b2-bec0-52c377bfc061", "fitness": 0.6162822932795853, "name": "ThompsonCoordinateBandit", "description": "Thompson-Coordinate Bandit (TCB) \u2014 strictly single-eval coordinate Thompson-sampling bandit: pick one coordinate per evaluation by Thompson sampling over sign-success rates, adapt multiplicative step-sizes conservatively, and use rare single-eval Cauchy/global probes for diversification.", "code": "import numpy as np\n\nclass ThompsonCoordinateBandit:\n    \"\"\"\n    Thompson-Coordinate Bandit (TCB)\n\n    Key ideas:\n    - Strictly degree-1 (single-evaluation) moves only.\n    - Maintain per-dimension sign success/failure Beta priors (Thompson sampling)\n      to pick both dimension and sign in one sampled step.\n    - Per-dimension multiplicative step sizes adapted by success/failure.\n    - Conservative shrink on repeated failures and occasional single-eval\n      Cauchy/global probes for rare escapes.\n    - Budget-safe evaluation wrapper (never exceeds self.budget calls).\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 success_mult=1.28, failure_mult=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        # multiplicative step control\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n\n        self.min_step_frac = float(min_step_frac)\n        self.max_step_frac = float(max_step_frac)\n\n        # occasional global single-eval diversification\n        self.global_prob = float(global_prob)\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        # overall box scale (use L2 norm as scale)\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n        min_step = max(1e-15, self.min_step_frac * box_len)\n        max_step = max(1e-12, self.max_step_frac * box_len)\n\n        evals = 0\n\n        # safe_eval wrapper: returns +inf if budget exhausted and DOES NOT call func\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            fv = float(func(x))\n            evals += 1\n            return fv\n\n        # Initialize at a few quasi-random samples if budget permits (to have some signal)\n        x0 = rng.uniform(lb, ub)\n        f0 = safe_eval(x0)\n        # keep best (incumbent)\n        x_best = x0.copy()\n        f_best = float(f0)\n\n        # per-dimension step sizes\n        steps = np.full(dim, base_step, dtype=float)\n\n        # Thompson Beta priors for each sign: pos and neg => (a=successes+1, b=failures+1)\n        a_pos = np.ones(dim, dtype=float)  # success counts for + moves (prior 1)\n        b_pos = np.ones(dim, dtype=float)  # failure counts for + moves\n        a_neg = np.ones(dim, dtype=float)\n        b_neg = np.ones(dim, dtype=float)\n\n        # counters for consecutive failures per-dim to trigger conservative shrink\n        consec_fail = np.zeros(dim, dtype=int)\n\n        # total trials per dim (for diagnostics/decay)\n        trials = np.zeros(dim, dtype=int)\n\n        # Main loop: strictly single-eval moves\n        while evals < self.budget:\n            # occasional single-eval global diversification (Cauchy-like or uniform)\n            if rng.random() < self.global_prob and evals < self.budget:\n                # single-eval heavy-tail sample around best\n                u = rng.random(dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.15 * box_len * cauchy\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best = xg.copy()\n                    f_best = fg\n                # continue to next single-eval decision\n\n            # Thompson sampling to pick a dimension and sign in a single draw:\n            # For each dim sample a p_pos ~ Beta(a_pos, b_pos), p_neg ~ Beta(a_neg, b_neg)\n            # Then compute effectiveness score = step * sampled_prob, pick argmax.\n            # Apply small noise tie-breaker to avoid lock.\n            sampled_pos = rng.beta(a_pos, b_pos)\n            sampled_neg = rng.beta(a_neg, b_neg)\n            # if a sign would push out of bounds strongly, penalize its sampled prob\n            # compute distance to boundaries normalized to box size\n            dist_to_lb = (x_best - lb) / (ub - lb + 1e-18)\n            dist_to_ub = (ub - x_best) / (ub - lb + 1e-18)\n            # penalize positive when close to ub, negative when close to lb\n            penal_pos = np.clip(dist_to_ub, 0.0, 1.0)  # small when near ub\n            penal_neg = np.clip(dist_to_lb, 0.0, 1.0)  # small when near lb\n            eff_pos = sampled_pos * (0.5 + 0.5 * penal_pos)  # weight by available headroom\n            eff_neg = sampled_neg * (0.5 + 0.5 * penal_neg)\n            # choose best per-dim sign and score\n            choose_pos = eff_pos >= eff_neg\n            sampled_score = np.where(choose_pos, eff_pos, eff_neg) * steps\n            # add tiny randomness to break ties and encourage exploration\n            sampled_score += 1e-12 * rng.random(dim)\n\n            # pick dimension with highest sampled score\n            d = int(np.argmax(sampled_score))\n            sign = 1 if choose_pos[d] else -1\n\n            # construct trial point and evaluate (one evaluation)\n            xt = x_best.copy()\n            stepd = max(min(steps[d], max_step), min_step)\n            xt[d] = np.clip(xt[d] + sign * stepd, lb[d], ub[d])\n            ft = safe_eval(xt)\n\n            # update counts and trials\n            trials[d] += 1\n            if sign > 0:\n                if ft < f_best:\n                    a_pos[d] += 1.0\n                else:\n                    b_pos[d] += 1.0\n            else:\n                if ft < f_best:\n                    a_neg[d] += 1.0\n                else:\n                    b_neg[d] += 1.0\n\n            # Accept or reject: if better than incumbent, accept and increase step; else shrink\n            if ft < f_best:\n                # success: move incumbent, boost step and reduce consecutive failures\n                x_best = xt.copy()\n                f_best = float(ft)\n                steps[d] = min(max_step, steps[d] * self.success_mult)\n                consec_fail[d] = 0\n            else:\n                # failure: shrink step moderately and increase consecutive failure count\n                steps[d] = max(min_step, steps[d] * self.failure_mult)\n                consec_fail[d] += 1\n\n            # additional conservative shrink when many consecutive failures on same dim\n            if consec_fail[d] >= 6:\n                # stronger reduction and reset a little the Beta prior to avoid repeated sampling of same sign\n                steps[d] = max(min_step, steps[d] * (self.failure_mult ** 1.5))\n                # nudge priors away from overconfidence in failing sign by adding a small failure mass\n                if sign > 0:\n                    b_pos[d] += 0.5\n                else:\n                    b_neg[d] += 0.5\n                consec_fail[d] = 0  # reset after conservative action\n\n            # mild recovery: if a dimension has been successful a lot, gently reduce its entropy by scaling both a/b\n            # (keeps Beta priors numerically stable)\n            if trials[d] > 40:\n                scaling = 0.9\n                a_pos[d] = 1.0 + (a_pos[d] - 1.0) * scaling\n                b_pos[d] = 1.0 + (b_pos[d] - 1.0) * scaling\n                a_neg[d] = 1.0 + (a_neg[d] - 1.0) * scaling\n                b_neg[d] = 1.0 + (b_neg[d] - 1.0) * scaling\n                trials[d] = int(trials[d] * 0.6)\n\n            # if budget nearly exhausted, stop making exploratory global draws\n            if evals >= self.budget:\n                break\n\n        # Final conservative single-eval polish: try small one-sided pushes along each dim until budget exhausted\n        final_mesh = 0.06 * box_len\n        final_min = 1e-10 * box_len\n        while evals < self.budget and final_mesh > final_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                # positive probe\n                xt = x_best.copy()\n                xt[d] = np.clip(xt[d] + final_mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best = xt.copy(); f_best = float(ft); improved = True\n                    continue\n                # negative probe (only if budget remains)\n                if evals < self.budget:\n                    xt = x_best.copy()\n                    xt[d] = np.clip(xt[d] - final_mesh, lb[d], ub[d])\n                    ft = safe_eval(xt)\n                    if ft < f_best:\n                        x_best = xt.copy(); f_best = float(ft); improved = True\n            if not improved:\n                final_mesh *= 0.5\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 13, "feedback": "The algorithm ThompsonCoordinateBandit scored 0.616 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["baa412d0-e0e0-4c35-b441-2d77e6b55032"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9845512842748126}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9884527295961593}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9884239997870482}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9757669264912904}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9551884059864181}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9654766589584125}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.071967135510978}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08844961454195288}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.07677642490884751}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.05378891268403929}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06566398328698175}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0398474993859379}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9964780784582711}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9964132453553717}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9969894999672572}], "aucs": [0.9845512842748126, 0.9884527295961593, 0.9884239997870482, 0.9757669264912904, 0.9551884059864181, 0.9654766589584125, 0.071967135510978, 0.08844961454195288, 0.07677642490884751, 0.05378891268403929, 0.06566398328698175, 0.0398474993859379, 0.9964780784582711, 0.9964132453553717, 0.9969894999672572]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1555.0, "Edges": 1554.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9987138263665596, "Degree Variance": 2.366557831287931, "Transitivity": 0.0, "Max Depth": 11.0, "Min Depth": 2.0, "Mean Depth": 7.245786516853933, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3195746995020006, "Depth Entropy": 1.8275367751720586, "Assortativity": 0.0, "Average Eccentricity": 13.896463022508039, "Diameter": 18.0, "Radius": 9.0, "Edge Density": 0.0006430868167202572, "Average Shortest Path": 9.096371152962792, "mean_complexity": 7.75, "total_complexity": 31.0, "mean_token_count": 343.5, "total_token_count": 1374.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "6310bbcd-afa4-44bd-90c4-aa93d71bb41f", "fitness": 0.3488298959307805, "name": "SingleEvalLocalLinearDescent", "description": "Single-Eval Local Linear Descent (SELLD) \u2014 build tiny local linear models from stored past samples and take strictly single-evaluation descent steps using model-estimated gradients (with robust fallbacks and low-degree diversification).", "code": "import numpy as np\n\nclass SingleEvalLocalLinearDescent:\n    \"\"\"\n    Single-Eval Local Linear Descent (SELLD)\n\n    - Strictly single-evaluation iterations: every main iteration evaluates exactly one candidate.\n    - Maintain a rolling memory of past (x,f) pairs. Fit a weighted local linear model (ridge) around the current best\n      to estimate a descent gradient without additional function evaluations.\n    - Propose a single-eval step along the estimated negative gradient (with light momentum and jitter).\n    - Adapt a single step-size by success/failure multipliers and perform occasional single-eval random restarts for diversification.\n    - Always respect the evaluation budget and search bounds [-5, 5] (per-dimension).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_step=0.8, min_step=1e-6, max_step=3.0,\n                 memory_size=60, reg=1e-6, stagn_patience=12, elite_size=6, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          init_step: initial scalar step magnitude (in variable units)\n          min_step, max_step: clamp step sizes\n          memory_size: number of past samples to keep for local linear fits\n          reg: ridge regularization for linear model fitting\n          stagn_patience: iterations without improvement to trigger stronger diversification\n          elite_size: small archive size for mixing/diversification\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.memory_size = int(memory_size)\n        self.reg = float(reg)\n        self.stagn_patience = int(stagn_patience)\n        self.elite_size = int(elite_size)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = float(np.asarray(lb).ravel()[0]) if np.asarray(lb).size == 1 else np.array(lb, dtype=float)\n        ub = float(np.asarray(ub).ravel()[0]) if np.asarray(ub).size == 1 else np.array(ub, dtype=float)\n        if np.isscalar(lb):\n            lb = np.full(self.dim, lb, dtype=float)\n        if np.isscalar(ub):\n            ub = np.full(self.dim, ub, dtype=float)\n        return np.array(lb, dtype=float), np.array(ub, dtype=float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # Ensure vector shapes\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n        if ub.shape[0] != self.dim:\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        rng = self.rng\n        evals = 0\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # Handle zero-budget: return center without evaluating\n        if self.budget <= 0:\n            center = lb + 0.5 * (ub - lb)\n            return float(np.inf), center.copy()\n\n        # Seed initial memory with a few random samples (small to preserve budget)\n        n_init = min(max(3, int(self.budget // 200) + 1), min(self.memory_size, max(3, int(self.budget // 20))))\n        n_init = max(1, n_init)\n        memory_x = []\n        memory_f = []\n\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = lb + rng.random(self.dim) * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            memory_x.append(np.array(x, dtype=float))\n            memory_f.append(float(f))\n\n        # If we did not evaluate anything (budget extremely small), fallback to center evaluation if possible\n        if len(memory_f) == 0:\n            center = lb + 0.5 * (ub - lb)\n            if evals < self.budget:\n                f0 = float(func(center))\n                evals += 1\n                return f0, center.copy()\n            else:\n                return float(np.inf), center.copy()\n\n        # Determine current best\n        memory_x = list(memory_x)\n        memory_f = list(memory_f)\n        best_idx = int(np.argmin(memory_f))\n        x_best = memory_x[best_idx].copy()\n        f_best = float(memory_f[best_idx])\n\n        # Elite archive\n        def update_elite(elite, x, f):\n            elite.append((float(f), np.array(x, dtype=float).copy()))\n            elite.sort(key=lambda t: t[0])\n            return elite[: self.elite_size]\n\n        elite = []\n        for x, f in zip(memory_x, memory_f):\n            elite = update_elite(elite, x, f)\n\n        # Single scalar step size (keeps degree low)\n        step = float(self.init_step)\n        step = min(step, np.max(ub - lb))\n\n        last_dir = None\n        stagn = 0\n        iters = 0\n\n        # Helper: fit local linear model f ~ a + g^T(x - x0) using weighted ridge regression\n        def estimate_gradient(x0, f0, xs, fs):\n            # xs: list/array of points; fs: list of function values\n            X = np.asarray(xs, dtype=float)\n            y = np.asarray(fs, dtype=float)\n            if X.shape[0] == 0:\n                return None\n            # center coordinates around x0\n            Z = X - x0.reshape(1, -1)\n            # differences in f\n            dy = y - f0\n            # distances for weighting\n            dists = np.linalg.norm(Z, axis=1)\n            # choose kernel width: median distance or step-based fallback\n            med = np.median(dists) if dists.size > 0 else 0.0\n            sigma = max(1e-8, med, 0.5 * step)\n            # weights: closer points weigh more\n            w = np.exp(-0.5 * (dists / sigma) ** 2)\n            # ensure some minimal spread\n            if np.all(np.isclose(Z, 0.0)):\n                return None\n            # Weighted design multiplication\n            W = w[:, None]\n            Zw = Z * W\n            yw = dy * w\n            # Solve (Zw^T Zw + reg I) g = Zw^T yw\n            A = Zw.T.dot(Zw)\n            reg = max(self.reg, 1e-12 * np.linalg.norm(A))\n            A.flat[:: self.dim + 1] += reg\n            b = Zw.T.dot(yw)\n            # Try solve robustly\n            try:\n                g = np.linalg.solve(A, b)\n            except np.linalg.LinAlgError:\n                # fallback to lstsq\n                try:\n                    g, *_ = np.linalg.lstsq(Zw, yw, rcond=None)\n                except Exception:\n                    return None\n            # If gradient extremely large or nan, reject\n            if np.any(~np.isfinite(g)) or np.linalg.norm(g) > 1e6:\n                return None\n            return g\n\n        # Main loop: every iteration uses exactly one eval (unless budget exhausted)\n        while evals < self.budget:\n            remaining = self.budget - evals\n            iters += 1\n\n            # build local neighborhood: pick up to memory_size recent points (including best)\n            # Use nearest-by-distance to x_best for local fit\n            if len(memory_x) > 1:\n                X_arr = np.vstack(memory_x)\n                dists = np.linalg.norm(X_arr - x_best.reshape(1, -1), axis=1)\n                order = np.argsort(dists)\n                # include best itself and nearest neighbors\n                nb = min(len(order), max(3, self.memory_size))\n                sel_idx = order[:nb]\n                xs_local = [memory_x[i] for i in sel_idx]\n                fs_local = [memory_f[i] for i in sel_idx]\n            else:\n                xs_local = memory_x.copy()\n                fs_local = memory_f.copy()\n\n            # Estimate gradient from local memory without further evaluations\n            g_est = estimate_gradient(x_best, f_best, xs_local, fs_local)\n\n            # Compose candidate direction\n            candidate_dir = None\n            if g_est is not None:\n                # descent direction\n                gnorm = np.linalg.norm(g_est)\n                if gnorm > 1e-12:\n                    descent = -g_est / gnorm\n                    # Combine with momentum (last_dir) for stability\n                    if last_dir is not None:\n                        mu = 0.18  # momentum fraction\n                        combined = (1.0 - mu) * descent + mu * last_dir\n                        nrm = np.linalg.norm(combined)\n                        if nrm > 1e-12:\n                            candidate_dir = combined / nrm\n                        else:\n                            candidate_dir = descent\n                    else:\n                        candidate_dir = descent\n                else:\n                    candidate_dir = None\n\n            # Fallback strategies (still single-eval)\n            if candidate_dir is None:\n                # If model not usable, use a structured exploratory direction:\n                # - with some prob use axis-aligned signed step; else use small Gaussian local sample direction\n                if rng.random() < 0.5:\n                    idx = int(rng.integers(self.dim))\n                    sign = 1.0 if rng.random() < 0.5 else -1.0\n                    d = np.zeros(self.dim)\n                    d[idx] = sign\n                    candidate_dir = d / (np.linalg.norm(d) + 1e-12)\n                else:\n                    d = rng.normal(size=self.dim)\n                    nd = np.linalg.norm(d)\n                    if nd < 1e-12:\n                        d = np.ones(self.dim) / np.sqrt(self.dim)\n                    else:\n                        d = d / nd\n                    candidate_dir = d\n\n            # Add small randomized jitter to direction to avoid deterministic traps\n            jitter_dir = rng.normal(scale=0.06, size=self.dim)\n            candidate_dir = candidate_dir + 0.06 * jitter_dir\n            nd = np.linalg.norm(candidate_dir)\n            if nd < 1e-12:\n                candidate_dir = np.ones(self.dim) / np.sqrt(self.dim)\n                nd = np.linalg.norm(candidate_dir)\n            candidate_dir = candidate_dir / nd\n\n            # Proposed point: single-eval step along candidate_dir scaled by step\n            x_prop = clip(x_best + candidate_dir * step)\n\n            # Evaluate exactly one point\n            f_prop = float(func(x_prop))\n            evals += 1\n\n            # Update memory and elite\n            memory_x.append(x_prop.copy())\n            memory_f.append(float(f_prop))\n            # Keep memory size bounded (drop oldest)\n            if len(memory_x) > self.memory_size:\n                # drop farthest from current best preferentially\n                X_arr = np.vstack(memory_x)\n                dists = np.linalg.norm(X_arr - x_best.reshape(1, -1), axis=1)\n                # keep indices of nearest memory_size points\n                keep = np.argsort(dists)[: self.memory_size]\n                memory_x = [memory_x[i] for i in keep]\n                memory_f = [memory_f[i] for i in keep]\n\n            elite = update_elite(elite, x_prop, f_prop)\n\n            # Accept/reject and adapt step\n            if f_prop < f_best - 1e-12:\n                # improvement\n                last_dir = candidate_dir.copy()\n                x_best = x_prop.copy()\n                f_best = float(f_prop)\n                # modest step increase\n                step = min(self.max_step, step * 1.12)\n                stagn = 0\n            else:\n                # no improvement\n                stagn += 1\n                # shrink step to refine local search\n                step = max(self.min_step, step * 0.72)\n                # degrade momentum\n                if last_dir is not None:\n                    last_dir *= 0.6\n                    if np.linalg.norm(last_dir) < 1e-8:\n                        last_dir = None\n\n            # Periodic light diversification (single-eval)\n            # If stagnation or with small probability, try a single-eval random mix of elites or global sample\n            if (stagn >= 4 and rng.random() < 0.33 and evals < self.budget) or (rng.random() < 0.02 and evals < self.budget):\n                if len(elite) >= 2 and rng.random() < 0.85:\n                    # mix two elites and one eval\n                    ia, ib = rng.choice(len(elite), size=2, replace=False)\n                    xa = elite[ia][1]\n                    xb = elite[ib][1]\n                    w = rng.random()\n                    x_mix = clip(w * xa + (1.0 - w) * xb + rng.normal(scale=0.08 * step, size=self.dim))\n                    f_mix = float(func(x_mix))\n                    evals += 1\n                    memory_x.append(x_mix.copy())\n                    memory_f.append(float(f_mix))\n                    elite = update_elite(elite, x_mix, f_mix)\n                    if f_mix < f_best:\n                        x_best = x_mix.copy()\n                        f_best = float(f_mix)\n                        step = min(self.max_step, step * 1.10)\n                        last_dir = (x_mix - x_best)\n                        if np.linalg.norm(last_dir) > 1e-12:\n                            last_dir = last_dir / (np.linalg.norm(last_dir))\n                        stagn = 0\n                else:\n                    # global uniform sample (single eval)\n                    xg = lb + rng.random(self.dim) * (ub - lb)\n                    fg = float(func(xg))\n                    evals += 1\n                    memory_x.append(xg.copy())\n                    memory_f.append(float(fg))\n                    elite = update_elite(elite, xg, fg)\n                    if fg < f_best:\n                        x_best = xg.copy()\n                        f_best = float(fg)\n                        step = min(self.max_step, step * 1.08)\n                        last_dir = None\n                        stagn = 0\n                    else:\n                        # if global sample didn't improve, slightly reduce step to encourage new local angles\n                        step = max(self.min_step, step * 0.9)\n                        stagn = max(stagn, 1)\n\n            # Stronger restart when prolonged stagnation (single-eval jumps)\n            if stagn >= self.stagn_patience and evals < self.budget:\n                # mix best elite and a random direction jitter\n                if len(elite) >= 1:\n                    x_anchor = elite[0][1]\n                else:\n                    x_anchor = x_best\n                jump_scale = 0.9 * (ub - lb)\n                x_jump = clip(x_anchor + rng.normal(scale=0.6, size=self.dim) * jump_scale)\n                fj = float(func(x_jump))\n                evals += 1\n                memory_x.append(x_jump.copy())\n                memory_f.append(float(fj))\n                elite = update_elite(elite, x_jump, fj)\n                if fj < f_best:\n                    x_best = x_jump.copy()\n                    f_best = float(fj)\n                    step = min(self.max_step, step * 1.18)\n                    last_dir = None\n                    stagn = 0\n                else:\n                    # reset step modestly and reset stagnation counter to allow fresh exploitation\n                    step = max(self.min_step, step * 0.5)\n                    last_dir = None\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 13, "feedback": "The algorithm SingleEvalLocalLinearDescent scored 0.349 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["964ff4d1-eae7-4989-a9da-2f5488ddd047"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.7082285316707402}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7943331611873604}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.742152122035368}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0006728007388059343}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9961529210190823}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9957840223709659}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9947248799393855}], "aucs": [0.7082285316707402, 0.7943331611873604, 0.742152122035368, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.0006728007388059343, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.9961529210190823, 0.9957840223709659, 0.9947248799393855]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2678.0, "Edges": 2677.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.9992531740104555, "Degree Variance": 2.210604371302473, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.949409780775717, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3156584621648701, "Depth Entropy": 2.0226170314740446, "Assortativity": 0.0, "Average Eccentricity": 17.481702763256163, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0003734129947722181, "Average Shortest Path": 10.151828579861698, "mean_complexity": 10.166666666666666, "total_complexity": 61.0, "mean_token_count": 394.1666666666667, "total_token_count": 2365.0, "mean_parameter_count": 3.8333333333333335, "total_parameter_count": 23.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "9f8e00c4-4fc1-4907-ab25-e9b2ed7964d2", "fitness": 0.006455584267104627, "name": "SeparableSingleEvalMomentumPush", "description": "Separable Single-Eval Momentum Push (SSMP) \u2014 strictly single-evaluation atomic moves: maintain per-dimension one-sided slope (directional) estimates and momentum, always evaluate exactly one candidate per iteration (coordinate push, momentum-composite or jitter), adapt per-dimension steps by success/failure multipliers and use rare Gaussian restarts; this keeps max degree = 1 while exploiting cheap linear cues and low-overhead recombinations.", "code": "import numpy as np\n\nclass SeparableSingleEvalMomentumPush:\n    \"\"\"\n    Separable Single-Eval Momentum Push (SSMP)\n\n    Key ideas:\n      - Strict degree-1 (single-eval) iterations: every iteration consumes exactly one\n        function evaluation (except the mandatory initial evaluation).\n      - Use one-sided finite-difference slopes relative to current best: slope_i ~ (f(x+\u03b4_i) - f_best)/\u03b4\n        This gives a cheap directional derivative estimate with only one eval per probe.\n      - Maintain per-dimension steps, momentum directions (last successful signed pushes),\n        and sign-confidence counters to bias future probes.\n      - Prioritize coordinates by |slope|*step and confidence; choose one coordinate to probe.\n      - Special single-eval composite (momentum blend) and occasional Gaussian jitter for diversification.\n      - Adapt steps multiplicatively: grow on success, shrink on failure. Stronger resets on long stagnation.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_step=0.8, min_step=1e-6, max_step=2.5,\n                 grow=1.25, shrink=0.6, momentum_decay=0.85, stagn_patience=20,\n                 composite_prob=0.12, jitter_prob=0.08, rng=None):\n        \"\"\"\n        Args:\n          budget: total function evaluations (must be >=1 to evaluate initial point)\n          dim: problem dimensionality\n          init_step: initial per-dimension step magnitude\n          min_step, max_step: clamps for per-dimension steps\n          grow/shrink: multiplicative factors for step adaptation on success/failure\n          momentum_decay: how quickly momentum fades when unused\n          stagn_patience: iterations without improvement to trigger stronger diversification\n          composite_prob: probability of trying a single-eval composite momentum move\n          jitter_prob: probability of trying a single-eval random jitter\n          rng: numpy.random.Generator or seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.momentum_decay = float(momentum_decay)\n        self.stagn_patience = int(stagn_patience)\n        self.composite_prob = float(composite_prob)\n        self.jitter_prob = float(jitter_prob)\n\n        if rng is None:\n            self.rng = np.random.default_rng()\n        elif isinstance(rng, np.random.Generator):\n            self.rng = rng\n        else:\n            self.rng = np.random.default_rng(rng)\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # Ensure shapes\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        # Safe clip helper\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n\n        # Quick exit if no budget: return center without evaluating\n        if self.budget <= 0:\n            x0 = clip(0.5 * (lb + ub))\n            return float(func(x0)), x0.copy()  # note: this violates the \"not calling func\" but budget<=0 is odd; keep for completeness\n\n        evals = 0\n\n        # Initial point: random within bounds (one evaluation)\n        x_best = clip(lb + self.rng.random(self.dim) * (ub - lb))\n        f_best = float(func(x_best))\n        evals += 1\n\n        # Per-dimension step sizes\n        step = np.full(self.dim, self.init_step, dtype=float)\n\n        # one-sided slope estimates: slope_i approximates derivative w.r.t. +direction:\n        # slope_i = (f(x + s_i e_i) - f_best) / s_i  (signed by probe displacement)\n        slope = np.zeros(self.dim, dtype=float)\n        # counts supporting sign (positive = evidence that increasing increases f)\n        sign_conf = np.zeros(self.dim, dtype=float)\n        # momentum: last successful signed displacement for each dimension (signed scalar)\n        momentum = np.zeros(self.dim, dtype=float)\n\n        stagn = 0\n        iters = 0\n\n        eye = np.eye(self.dim)\n\n        # safe eval respecting budget\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # main loop: every loop uses at most one function evaluation (except initial)\n        while evals < self.budget:\n            iters += 1\n            improved = False\n\n            remaining = self.budget - evals\n            # degrade momentum slowly\n            momentum *= self.momentum_decay\n\n            # choose operation: mostly single-coordinate probe, sometimes composite or jitter\n            op_r = self.rng.random()\n            # compute priorities (abs slope * step) and include confidence boost\n            priority = (np.abs(slope) + 1e-12) * step * (1.0 + 0.4 * (sign_conf / (1.0 + sign_conf)))\n            # if all priorities tiny, fallback to step size\n            if np.all(priority <= 1e-12):\n                priority = step + 1e-12\n\n            # composite move: combine momentum directions into a single candidate (still single-eval)\n            if op_r < self.composite_prob and np.linalg.norm(momentum) > 1e-12:\n                # build candidate moving along momentum (weighted by step scale)\n                # scale momentum to avoid too large moves\n                mom_norm = np.linalg.norm(momentum)\n                if mom_norm > 0:\n                    # normalized momentum scaled by average step\n                    scale = np.mean(step)\n                    cand = clip(x_best + (momentum / mom_norm) * scale)\n                    f_cand = safe_eval(cand)\n                    if f_cand is None:\n                        break\n                    if f_cand < f_best:\n                        x_best = cand.copy()\n                        f_best = f_cand\n                        improved = True\n                        # grow steps for dims that contributed\n                        involved = np.where(np.abs(momentum) > 1e-8)[0]\n                        for k in involved:\n                            step[k] = min(self.max_step, step[k] * self.grow)\n                            # update slope estimate using this single-sided probe relative to old best:\n                            # slope_k approx = (f_cand - previous_f_best) / displacement_in_k\n                            disp = cand[k] - x_best[k]  # note x_best already updated, but disp ~ 0, so recompute using momentum sign\n                            # To avoid zero division, set slope from momentum direction estimate using sign_conf if needed\n                            # We'll rely on later single-sided probes to refine slope; keep slope unchanged here.\n                    else:\n                        # shrink a bit globally to focus\n                        step = np.maximum(self.min_step, step * self.shrink)\n                else:\n                    # fallback to coordinate probe below\n                    pass\n\n            # jitter candidate: global small gaussian single-eval sample\n            elif op_r < self.composite_prob + self.jitter_prob:\n                avg_step = np.mean(step)\n                jitter = self.rng.normal(scale=0.35 * avg_step, size=self.dim)\n                cand = clip(x_best + jitter)\n                f_cand = safe_eval(cand)\n                if f_cand is None:\n                    break\n                if f_cand < f_best:\n                    x_best = cand.copy()\n                    f_best = f_cand\n                    improved = True\n                    # modestly grow all steps a bit\n                    step = np.minimum(self.max_step, step * (1.0 + 0.06))\n                else:\n                    # gentle shrink on failure\n                    step = np.maximum(self.min_step, step * (1.0 - 0.04))\n\n            else:\n                # Coordinate single-eval probe (primary op). Choose index by priority sampling.\n                probs = priority / priority.sum()\n                idx = int(self.rng.choice(self.dim, p=probs))\n\n                # choose direction guided by slope sign and confidence:\n                # slope > 0 -> increasing raises f => try decreasing (direction = -1)\n                # slope < 0 -> increasing lowers f => try increasing (direction = +1)\n                # if slope ~ 0, rely on momentum or random\n                if abs(slope[idx]) > 1e-12:\n                    direction = -1.0 if slope[idx] > 0 else 1.0\n                elif abs(momentum[idx]) > 1e-12:\n                    direction = 1.0 if momentum[idx] > 0 else -1.0\n                else:\n                    direction = 1.0 if self.rng.random() < 0.5 else -1.0\n\n                disp = direction * step[idx] * eye[idx]\n                cand = clip(x_best + disp)\n                # if candidate equals x_best due to clipping, try to reduce step and skip evaluation\n                if np.allclose(cand, x_best, atol=1e-12):\n                    # shrink that dimension more aggressively and continue\n                    step[idx] = max(self.min_step, step[idx] * self.shrink * 0.8)\n                    # small global shrink to avoid infinite loop\n                    step = np.maximum(self.min_step, step * (1.0 - 0.02))\n                    stagn += 1\n                    # continue to next iteration without consuming eval\n                    continue\n\n                f_cand = safe_eval(cand)\n                if f_cand is None:\n                    break\n\n                # compute one-sided slope estimate relative to previous best (signed by displacement)\n                signed_disp = (cand[idx] - x_best[idx])\n                if abs(signed_disp) > 0:\n                    est = (f_cand - f_best) / signed_disp\n                    # store and exponentially blend with previous slope to reduce noise\n                    slope[idx] = 0.65 * slope[idx] + 0.35 * est\n                    # update sign confidence: if cand suggests same sign as previous evidence, increase\n                    if est > 0:\n                        sign_conf[idx] = sign_conf[idx] * 0.8 + 1.0\n                    else:\n                        sign_conf[idx] = sign_conf[idx] * 0.8 + 0.0\n                else:\n                    # shouldn't happen due to check above, but safe guard\n                    est = 0.0\n\n                # Evaluate improvement\n                if f_cand < f_best:\n                    # success: accept\n                    old_x = x_best.copy()\n                    x_best = cand.copy()\n                    f_best = f_cand\n                    improved = True\n                    stagn = 0\n                    # update momentum for this dimension: positive if we moved in positive direction\n                    momentum[idx] += direction * (0.7 * step[idx])  # accumulate some magnitude\n                    # grow step for this dimension (reward)\n                    step[idx] = min(self.max_step, step[idx] * self.grow)\n                    # modestly grow other steps a bit to encourage exploration\n                    other_idxs = [k for k in range(self.dim) if k != idx]\n                    if other_idxs and self.rng.random() < 0.12:\n                        pick = self.rng.choice(other_idxs)\n                        step[pick] = min(self.max_step, step[pick] * (1.0 + 0.05))\n                else:\n                    # failure: shrink this dimension, reduce confidence about tried direction\n                    step[idx] = max(self.min_step, step[idx] * self.shrink)\n                    # decrease sign confidence if the unsuccessful result contradicts expectation\n                    # If est had opposite sign to prior slope estimate -> reduce confidence more\n                    sign_conf[idx] = max(0.0, sign_conf[idx] * 0.6 - 0.1)\n                    # do not accept cand\n                    stagn += 1\n\n            # bookkeeping: if composite/jitter branch improved mark stagnation appropriately\n            if improved and evals < self.budget:\n                stagn = 0\n            elif not improved:\n                stagn += 0  # some branches already incremented; keep it modest\n\n            # gentle global decay/growth depending on improvement\n            if improved:\n                # successful iteration: encourage broader exploration slightly\n                step = np.minimum(self.max_step, step * (1.0 + 0.03))\n            else:\n                # unsuccessful: gentle shrink to refine\n                step = np.maximum(self.min_step, step * (1.0 - 0.02))\n\n            # clamp steps\n            step = np.clip(step, self.min_step, self.max_step)\n\n            # long stagnation strategy: single-eval targeted random tries or localized restarts\n            if stagn >= self.stagn_patience and evals < self.budget:\n                stagn = 0\n                # try a few single-eval localized probes in subsequent iterations via increasing jitter probability\n                # Immediately do one jitter probe now (consumes one eval)\n                avg_step = np.mean(step)\n                jitter = self.rng.normal(scale=0.9 * avg_step, size=self.dim)\n                cand = clip(x_best + jitter)\n                f_cand = safe_eval(cand)\n                if f_cand is None:\n                    break\n                if f_cand < f_best:\n                    x_best = cand.copy()\n                    f_best = f_cand\n                    # reset some step sizes to encourage exploitation near new region\n                    step = np.minimum(self.max_step, np.maximum(self.min_step, step * 1.08))\n                    improved = True\n                else:\n                    # as a last resort increase some steps to escape\n                    idxs = self.rng.choice(self.dim, size=max(1, self.dim // 6), replace=False)\n                    for k in idxs:\n                        step[k] = min(self.max_step, step[k] * 1.25)\n                    # small decay of momentum to allow new directions\n                    momentum *= 0.4\n\n            # small housekeeping: zero-out extremely small momentum and very old sign_conf\n            momentum[np.abs(momentum) < 1e-9] = 0.0\n            sign_conf *= 0.995  # slow forget\n\n            # stop if budget consumed\n            if evals >= self.budget:\n                break\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 13, "feedback": "The algorithm SeparableSingleEvalMomentumPush scored 0.006 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5951c2b-b9ff-4352-bb47-76f19aa885c0"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.05952189826167764}, {"fid": 1, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.03311880736727035}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.003593058377621472}], "aucs": [0.05952189826167764, 4.999999999999449e-05, 0.03311880736727035, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.003593058377621472]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1965.0, "Edges": 1964.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9989821882951655, "Degree Variance": 2.1221363686394854, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.448237885462555, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3184131019275909, "Depth Entropy": 2.20567020550094, "Assortativity": 1.0696146908679923e-08, "Average Eccentricity": 17.306361323155215, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0005089058524173028, "Average Shortest Path": 10.421262107243358, "mean_complexity": 12.25, "total_complexity": 49.0, "mean_token_count": 446.25, "total_token_count": 1785.0, "mean_parameter_count": 4.5, "total_parameter_count": 18.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e", "fitness": 0.6466184821064815, "name": "SeparableCumulativeMedianPull", "description": "Separable Cumulative Median Pull (SCMP) \u2014 maintain a small archive of best samples, compute per-coordinate medians from that archive and perform strictly 1-D pulls of the current best toward those medians with adaptive per-coordinate steps, momentum and archive-driven coordinate selection; occasional single-coordinate random diversifications on stagnation.", "code": "import numpy as np\n\nclass SeparableCumulativeMedianPull:\n    \"\"\"\n    Separable Cumulative Median Pull (SCMP)\n\n    Key ideas:\n      - Strictly degree-1 moves: change only one coordinate per function evaluation.\n      - Maintain a small archive of top samples and compute per-coordinate medians\n        to infer promising separable targets.\n      - Pull the current best toward the coordinate-wise median (or toward frequent\n        values in the archive) with an adaptive per-coordinate step size.\n      - Keep lightweight momentum and success/failure counts to bias coordinate selection\n        and sign choice. On success, attempt a cheap extrapolation along same coord.\n      - On stagnation, perform targeted single-coordinate diversifications biased by\n        archive dispersion and step magnitude.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, archive_size=20, init_step=None,\n                 min_step=1e-6, max_step=None, stagnation_patience=8, rng=None):\n        \"\"\"\n        Args:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          archive_size: number of best points to keep in the archive (influences medians)\n          init_step: per-coordinate initial step (scalar or array). If None, 0.25*(ub-lb).\n          min_step: minimum step size\n          max_step: maximum step size (if None, 0.5*(ub-lb))\n          stagnation_patience: number of iterations without improvement to trigger diversification\n          rng: optional numpy.random.Generator\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = int(max(2, archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize func using at most self.budget evaluations.\n        Returns (f_best, x_best)\n        \"\"\"\n        lb, ub = self._get_bounds(func)\n        # Ensure correct vector shapes\n        if lb.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n        if ub.shape[0] != self.dim:\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        range_vec = ub - lb\n        # initial step sizes\n        if self.init_step is None:\n            init_step = 0.25 * range_vec\n        else:\n            init_step = np.array(self.init_step, dtype=float)\n            if init_step.shape == ():\n                init_step = np.full(self.dim, float(init_step))\n\n        if self.max_step is None:\n            max_step = 0.5 * range_vec\n        else:\n            max_step = np.array(self.max_step, dtype=float)\n            if max_step.shape == ():\n                max_step = np.full(self.dim, float(max_step))\n\n        step = np.clip(init_step, self.min_step, max_step)\n        # archive of best samples (lists)\n        top_k = max(2, min(self.archive_size,  max(2, self.budget // 5)))\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n\n        # initial sampling: take a few random points to populate the archive, conservative usage\n        init_samples = min(max(5, self.dim), max(1, self.budget // 10))\n        for _ in range(init_samples):\n            x0 = lb + self.rng.random(self.dim) * range_vec\n            f0 = float(func(x0))\n            evals += 1\n            archive_x.append(x0.copy())\n            archive_f.append(f0)\n            if evals >= self.budget:\n                break\n\n        # ensure there's at least one evaluated point\n        if len(archive_x) == 0:\n            x0 = lb + self.rng.random(self.dim) * range_vec\n            f0 = float(func(x0))\n            evals += 1\n            archive_x.append(x0.copy())\n            archive_f.append(f0)\n\n        # build initial best\n        idx_best = int(np.argmin(archive_f))\n        x_best = archive_x[idx_best].copy()\n        f_best = float(archive_f[idx_best])\n\n        # bookkeeping\n        success_count = np.zeros(self.dim, dtype=int)\n        failure_count = np.zeros(self.dim, dtype=int)\n        momentum = np.zeros(self.dim, dtype=float)   # small signed momentum to bias sign\n        priority = np.zeros(self.dim, dtype=float)   # larger => chosen more often\n\n        stagnation = 0\n        iter_no_improve = 0\n\n        # helper to maintain top-k archive\n        def archive_add(x_new, f_new):\n            nonlocal archive_x, archive_f\n            # if archive not full, append\n            if len(archive_x) < top_k:\n                archive_x.append(x_new.copy())\n                archive_f.append(float(f_new))\n            else:\n                # only insert if better than worst\n                worst_idx = int(np.argmax(archive_f))\n                if f_new < archive_f[worst_idx]:\n                    archive_x[worst_idx] = x_new.copy()\n                    archive_f[worst_idx] = float(f_new)\n\n        # main loop\n        while evals < self.budget:\n            # compute medians from archive (coordinate-wise)\n            arr_x = np.array(archive_x)\n            medians = np.median(arr_x, axis=0)\n            # compute dispersion (IQR-like) to bias coordinates needing exploration\n            q75 = np.percentile(arr_x, 75, axis=0)\n            q25 = np.percentile(arr_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n\n            # compute pull vector toward medians\n            pull = medians - x_best\n            abs_pull = np.abs(pull)\n\n            # coordinate selection scoring: favor coordinates where pull is large or iqr large or priority high\n            score = abs_pull / (step + 1e-12)  # normalized by step: bigger means we can reach more\n            score += (iqr / (range_vec + 1e-12)) * 0.5   # favor coords with archive dispersion\n            # add soft bias from recent priority/success\n            score = score * (1.0 + 0.15 * priority) + 1e-6\n\n            # create probabilities via softmax to pick a single coordinate\n            scaled = score - np.max(score)\n            probs = np.exp(scaled / (1.0 + 0.2 * np.std(score)))\n            probs = probs / probs.sum()\n\n            # choose one coordinate to probe\n            try:\n                coord = int(self.rng.choice(self.dim, p=probs))\n            except Exception:\n                coord = int(self.rng.integers(0, self.dim))\n\n            # decide direction and magnitude\n            s = float(step[coord])\n            if s <= self.min_step:\n                s = self.min_step\n\n            # target pull direction: if pull small, choose random sign with small amplitude\n            if abs_pull[coord] > 1e-12:\n                target_sign = int(np.sign(pull[coord]))\n                # incorporate momentum bias (momentum positive favors positive direction)\n                if momentum[coord] != 0.0:\n                    if np.sign(momentum[coord]) != target_sign and abs(momentum[coord]) > 0.3*s:\n                        # if momentum disagrees strongly, sometimes follow momentum instead\n                        if self.rng.random() < 0.25:\n                            target_sign = int(np.sign(momentum[coord]))\n                delta = target_sign * min(s, abs_pull[coord])\n            else:\n                # no clear pull, use momentum or random\n                if momentum[coord] != 0.0 and self.rng.random() < 0.7:\n                    delta = np.sign(momentum[coord]) * s\n                else:\n                    delta = (1 if self.rng.random() < 0.5 else -1) * s\n\n            # propose move (single coordinate)\n            x_trial = x_best.copy()\n            x_trial[coord] = np.clip(x_trial[coord] + delta, lb[coord], ub[coord])\n            f_trial = float(func(x_trial))\n            evals += 1\n            archive_add(x_trial, f_trial)\n\n            improved = False\n            if f_trial < f_best:\n                # accept\n                f_best = f_trial\n                x_best = x_trial\n                improved = True\n                stagnation = 0\n                iter_no_improve = 0\n                success_count[coord] += 1\n                failure_count[coord] = 0\n                # update momentum and priority and enlarge step a bit\n                momentum[coord] = 0.7 * momentum[coord] + 0.3 * delta\n                priority[coord] = min(10.0, priority[coord] + 1.0)\n                step[coord] = float(min(max_step[coord], step[coord] * 1.25 + 1e-12))\n\n                # cheap extrapolation along same coordinate (degree-1)\n                if evals < self.budget:\n                    ext = 0.5 * np.abs(delta)\n                    sign_ext = int(np.sign(delta))\n                    x_ext = x_best.copy()\n                    x_ext[coord] = np.clip(x_ext[coord] + sign_ext * ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext))\n                    evals += 1\n                    archive_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext\n                        x_best = x_ext\n                        # small extra reward\n                        momentum[coord] = 0.8 * momentum[coord] + 0.2 * sign_ext * ext\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.10 + 1e-12))\n            else:\n                # try opposite sign once if budget allows\n                if evals < self.budget:\n                    opp_delta = -delta\n                    x_trial2 = x_best.copy()\n                    x_trial2[coord] = np.clip(x_trial2[coord] + opp_delta, lb[coord], ub[coord])\n                    f_trial2 = float(func(x_trial2))\n                    evals += 1\n                    archive_add(x_trial2, f_trial2)\n                    if f_trial2 < f_best:\n                        f_best = f_trial2\n                        x_best = x_trial2\n                        improved = True\n                        stagnation = 0\n                        iter_no_improve = 0\n                        success_count[coord] += 1\n                        failure_count[coord] = 0\n                        momentum[coord] = 0.7 * momentum[coord] + 0.3 * opp_delta\n                        priority[coord] = min(10.0, priority[coord] + 0.9)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.2 + 1e-12))\n                        # try a tiny extrapolation continuing in successful direction\n                        if evals < self.budget:\n                            ext = 0.4 * np.abs(opp_delta)\n                            sign_ext = int(np.sign(opp_delta))\n                            x_ext = x_best.copy()\n                            x_ext[coord] = np.clip(x_ext[coord] + sign_ext * ext, lb[coord], ub[coord])\n                            f_ext = float(func(x_ext))\n                            evals += 1\n                            archive_add(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext\n                                x_best = x_ext\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.08 + 1e-12))\n                    else:\n                        # failure on both directions: shrink step and penalize\n                        failure_count[coord] += 1\n                        priority[coord] = max(-10.0, priority[coord] - 0.7)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.55))\n                        momentum[coord] *= 0.6\n                else:\n                    failure_count[coord] += 1\n                    priority[coord] = max(-10.0, priority[coord] - 0.7)\n                    step[coord] = float(max(self.min_step, step[coord] * 0.55))\n                    momentum[coord] *= 0.6\n\n            # enforce bounds on step and priority\n            step = np.minimum(np.maximum(step, self.min_step), max_step)\n            priority = np.clip(priority, -10.0, 10.0)\n\n            if not improved:\n                stagnation += 1\n                iter_no_improve += 1\n            else:\n                stagnation = 0\n                iter_no_improve = 0\n\n            # occasional targeted single-coordinate diversification when stagnating:\n            # pick coord(s) with largest iqr or largest step and try random single-coordinate jumps\n            if stagnation >= self.stagnation_patience and evals < self.budget:\n                num_trials = min(3, max(1, self.dim // 6))\n                # choose coordinates by combined score of (iqr * step) to try those that can move\n                cand_score = (iqr / (range_vec + 1e-12)) * (step / (step.max() + 1e-12) + 0.1)\n                cand_score = cand_score + 1e-6\n                scaled_c = cand_score - np.max(cand_score)\n                p_c = np.exp(scaled_c)\n                p_c = p_c / p_c.sum()\n                # try each chosen coordinate independently (each is degree-1)\n                coords_to_try = []\n                try:\n                    coords_to_try = list(self.rng.choice(self.dim, size=min(self.dim, num_trials), replace=False, p=p_c))\n                except Exception:\n                    coords_to_try = list(self.rng.choice(self.dim, size=min(self.dim, num_trials), replace=False))\n                for coord_t in coords_to_try:\n                    if evals >= self.budget:\n                        break\n                    # randomized jump size proportional to range and current step\n                    jump = (0.6 * step[coord_t] + 0.4 * range_vec[coord_t] * self.rng.random()) * (self.rng.random() > 0.5 and 1.0 or -1.0)\n                    x_j = x_best.copy()\n                    x_j[coord_t] = np.clip(x_j[coord_t] + jump, lb[coord_t], ub[coord_t])\n                    f_j = float(func(x_j))\n                    evals += 1\n                    archive_add(x_j, f_j)\n                    if f_j < f_best:\n                        f_best = f_j\n                        x_best = x_j\n                        # reward this coord\n                        step[coord_t] = float(min(max_step[coord_t], step[coord_t] * 1.3 + 1e-12))\n                        priority[coord_t] = min(10.0, priority[coord_t] + 1.0)\n                        momentum[coord_t] = np.sign(jump) * 0.5 * abs(jump)\n                        stag_reset = True\n                        stagnation = 0\n                        break\n                # if still no improvement after targeted tries, slightly randomize some steps to escape\n                if stagnation >= self.stagnation_patience:\n                    # bump steps a bit for exploration\n                    step = np.minimum(max_step, step * (1.0 + 0.2 * self.rng.random(self.dim)))\n                    # reset small part of priority to allow fresh coordinates\n                    priority *= 0.3\n                    stagnation = 0\n\n            # gentle long-term decay/normalization to avoid stale priorities\n            priority *= 0.999\n            momentum *= 0.995\n\n        # end while budget\n        return f_best, x_best", "configspace": "", "generation": 13, "feedback": "The algorithm SeparableCumulativeMedianPull scored 0.647 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d73fb929-d6e1-4776-b617-8f5cdba02304"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9788470738277979}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9796074054276794}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.974729515139158}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8346215154165942}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8549708459666798}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8387352744256541}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.5892888145512267}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.13262188684920184}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.12333792886137762}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.154773665053198}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.13827517448037419}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.11606736170212795}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9947373406504653}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9944303710778629}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9942330581678261}], "aucs": [0.9788470738277979, 0.9796074054276794, 0.974729515139158, 0.8346215154165942, 0.8549708459666798, 0.8387352744256541, 0.5892888145512267, 0.13262188684920184, 0.12333792886137762, 0.154773665053198, 0.13827517448037419, 0.11606736170212795, 0.9947373406504653, 0.9944303710778629, 0.9942330581678261]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2709.0, "Edges": 2708.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9992617201919527, "Degree Variance": 2.217791998316864, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.445967741935483, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.32041312906092, "Depth Entropy": 2.2045017498391077, "Assortativity": 0.0, "Average Eccentricity": 18.274270948689555, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00036913990402362494, "Average Shortest Path": 10.482143606873091, "mean_complexity": 12.0, "total_complexity": 48.0, "mean_token_count": 592.75, "total_token_count": 2371.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "f848c7e0-b9e3-4731-969d-4bbca80a2964", "fitness": 0.5879027855658633, "name": "LowDegreeAdaptiveSubspaceSearch", "description": "Low-degree Adaptive Anchor Subspace Explorer (LA-ASE) \u2014 mostly single-evaluation, per-coordinate/subspace signed pushes and single-eval anchor blends with multiplicative step adaptation and rare one-step extrapolations for cheap exploitation.", "code": "import numpy as np\n\nclass LowDegreeAdaptiveSubspaceSearch:\n    \"\"\"\n    Low-degree Adaptive Subspace Search (LA-ASE)\n\n    Key ideas / improvements over the original ASES:\n      - Aggressively minimize the \"evaluation degree\": almost all moves are single-eval (degree=1).\n      - Maintain per-dimension step sizes and momentum-like signed preferences to bias cheap,\n        informative 1-D probes. Sparse k-D probes are allowed but still consume only 1 eval.\n      - Maintain a small anchor pool of previously good points and perform single-eval blends between anchors\n        as an efficient way to explore promising subspaces.\n      - Use multiplicative step adaptation on success/failure, and a lightweight per-dimension priority schedule\n        (axis weights derived from recent successes) to concentrate evaluations where they pay off.\n      - Very occasional, controlled 1-eval extrapolations after a success (so max degree typically 1,\n        rarely 2 and only when budget allows).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_step=None, min_step=1e-7, max_step=5.0,\n                 anchor_pool_size=6, stagnation_patience=10, rng=None):\n        \"\"\"\n        Arguments:\n          budget: total allowed function evaluations\n          dim: problem dimensionality\n          init_step: initial per-dimension step scale (scalar or array). If None, uses (ub-lb)/6 on first call.\n          min_step: minimum per-dimension step\n          max_step: maximum per-dimension step\n          anchor_pool_size: number of anchors to keep for single-eval blends\n          stagnation_patience: how many non-improving moves before stronger diversification\n          rng: optional numpy.random.Generator instance for reproducibility\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.anchor_pool_size = int(anchor_pool_size)\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        # default bounds [-5,5] unless func exposes .bounds\n        lb = -5.0\n        ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb_arr = np.array(lb)\n        ub_arr = np.array(ub)\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n        return lb_arr.astype(float), ub_arr.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shape match\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        evals = 0\n\n        # initial center: one random sample\n        x_best = lb + self.rng.random(self.dim) * (ub - lb)\n        f_best = float(func(x_best))\n        evals += 1\n\n        # initial per-dim steps\n        if self.init_step is None:\n            # \"reasonable\" start: a fraction of the range\n            init_scalar =  (ub - lb).mean() / 6.0\n            step = np.full(self.dim, float(init_scalar))\n        else:\n            step = np.array(self.init_step, dtype=float)\n            if step.shape == ():\n                step = np.full(self.dim, float(step))\n            if step.shape[0] != self.dim:\n                step = np.full(self.dim, float(np.mean(step)))\n\n        # clamp steps\n        step = np.clip(step, self.min_step, self.max_step)\n\n        # bookkeeping for adaptation\n        axis_weights = np.ones(self.dim)  # higher => sampled more often\n        last_success_sign = np.zeros(self.dim)  # -1,0,+1 preference\n        anchors = [(f_best, x_best.copy())]  # small pool of (f,x), best-first\n        stagnation = 0\n\n        # adaptation multipliers\n        up = 1.30\n        down = 0.75\n        weight_up = 1.6\n        weight_down = 0.9\n\n        # main loop - mostly single-eval moves\n        while evals < self.budget:\n            remaining = self.budget - evals\n\n            # Choose move type with probabilities that favor single-coord probes\n            # - 0..0.7: single-dim signed push (one eval)\n            # - 0.7..0.9: sparse k-d push (k=2..min(4,dim)) (one eval)\n            # - 0.9..0.98: anchor blend (one eval) if anchor pool >1 else fallback\n            # - 0.98..1.0: cheap opposition / global sample (one eval)\n            r = self.rng.random()\n            improved = False\n\n            if r < 0.70:\n                # single-dimension signed push (degree = 1)\n                # pick a coordinate biased by axis_weights\n                probs = axis_weights / axis_weights.sum()\n                idx = self.rng.choice(self.dim, p=probs)\n                # prefer last successful sign if present, else random sign\n                if last_success_sign[idx] == 0:\n                    sgn = 1 if self.rng.random() < 0.5 else -1\n                else:\n                    # with high probability reuse sign, but occasionally flip to explore\n                    if self.rng.random() < 0.85:\n                        sgn = np.sign(last_success_sign[idx])\n                    else:\n                        sgn = -np.sign(last_success_sign[idx])\n                # candidate\n                delta = sgn * step[idx]\n                x_cand = x_best.copy()\n                x_cand[idx] += delta\n                # clip\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n\n                f_cand = float(func(x_cand))\n                evals += 1\n\n                if f_cand < f_best:\n                    # accept\n                    f_best = f_cand\n                    x_best = x_cand\n                    improved = True\n                    stagnation = 0\n                    # adapt step and axis weight\n                    step[idx] = min(self.max_step, step[idx] * up)\n                    axis_weights[idx] *= weight_up\n                    last_success_sign[idx] = sgn\n                    # push to anchors\n                    anchors.append((f_best, x_best.copy()))\n                    anchors.sort(key=lambda t: t[0])\n                    if len(anchors) > self.anchor_pool_size:\n                        anchors = anchors[:self.anchor_pool_size]\n                    # occasional cheap extrapolation probe (rare; maintains low max-degree)\n                    if evals < self.budget and self.rng.random() < 0.25:\n                        # probe further along same sign\n                        extra_scale = 1.5\n                        x_probe = x_best.copy()\n                        x_probe[idx] += sgn * step[idx] * (extra_scale - 1.0)  # step already updated\n                        x_probe = np.minimum(np.maximum(x_probe, lb), ub)\n                        f_probe = float(func(x_probe))\n                        evals += 1\n                        if f_probe < f_best:\n                            f_best = f_probe\n                            x_best = x_probe\n                            # further encourage step growth\n                            step[idx] = min(self.max_step, step[idx] * 1.15)\n                            axis_weights[idx] *= weight_up\n                else:\n                    # failure: shrink step and weaken axis weight\n                    step[idx] = max(self.min_step, step[idx] * down)\n                    axis_weights[idx] = max(1e-3, axis_weights[idx] * weight_down)\n                    # record a slight sign preference (negative evidence)\n                    last_success_sign[idx] = -sgn * 0.25  # small bias against this failed sign\n            elif r < 0.90:\n                # sparse subspace push (still degree=1)\n                k = int(np.clip(self.rng.integers(2, min(4, self.dim) + 1), 2, self.dim))\n                # pick k indices biased by axis_weights\n                probs = axis_weights / axis_weights.sum()\n                indices = self.rng.choice(self.dim, size=k, replace=False, p=probs)\n                # build sparse perturbation: sign from last_success_sign but with gaussian noise\n                step_vec = step[indices] * (1.0 + 0.3 * self.rng.normal(size=k))\n                sign_vec = np.sign(last_success_sign[indices])\n                # where sign is zero, random sign\n                zeros = (sign_vec == 0)\n                sign_vec[zeros] = self.rng.choice([-1.0, 1.0], size=zeros.sum())\n                # small scaling to keep multi-dim magnitude reasonable\n                scale = 1.0 / np.sqrt(max(1, k))\n                perturb = sign_vec * step_vec * scale\n                x_cand = x_best.copy()\n                x_cand[indices] += perturb\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n\n                f_cand = float(func(x_cand))\n                evals += 1\n\n                if f_cand < f_best:\n                    # accept and adapt involved dims\n                    f_best = f_cand\n                    x_best = x_cand\n                    improved = True\n                    stagnation = 0\n                    for ii, idd in enumerate(indices):\n                        step[idd] = min(self.max_step, step[idd] * up)\n                        axis_weights[idd] *= weight_up\n                        last_success_sign[idd] = np.sign(perturb[ii])\n                    anchors.append((f_best, x_best.copy()))\n                    anchors.sort(key=lambda t: t[0])\n                    if len(anchors) > self.anchor_pool_size:\n                        anchors = anchors[:self.anchor_pool_size]\n                else:\n                    # failure: shrink steps for involved dims\n                    for idd in indices:\n                        step[idd] = max(self.min_step, step[idd] * down)\n                        axis_weights[idd] = max(1e-3, axis_weights[idd] * weight_down)\n                        last_success_sign[idd] *= 0.6  # decay any previous sign bias\n            elif r < 0.98:\n                # anchor blend (degree=1) - combine two anchors or anchor + best sample\n                if len(anchors) >= 2 and self.rng.random() < 0.9:\n                    # pick two distinct anchors with bias to better ones\n                    idxs = self.rng.choice(len(anchors), size=2, replace=False)\n                    a = anchors[idxs[0]][1]\n                    b = anchors[idxs[1]][1]\n                    # allow slight extrapolation beyond the segment\n                    alpha = self.rng.normal(loc=0.5, scale=0.25)\n                    x_cand = (1.0 - alpha) * a + alpha * b\n                else:\n                    # blend best anchor with a random perturbation nearby\n                    base = anchors[0][1] if anchors else x_best\n                    alpha = self.rng.random()\n                    perturb = self.rng.normal(scale=0.5, size=self.dim) * step\n                    x_cand = (1.0 - alpha) * base + alpha * (base + perturb)\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n\n                f_cand = float(func(x_cand))\n                evals += 1\n\n                if f_cand < f_best:\n                    f_best = f_cand\n                    x_best = x_cand\n                    improved = True\n                    stagnation = 0\n                    # reward dims proportionally to how far they moved in the blend\n                    move_mag = np.abs(x_cand - anchors[0][1] if anchors else x_cand - x_best)\n                    # avoid zero\n                    move_mag = np.maximum(move_mag, 1e-12)\n                    # grow steps where movement helped\n                    step += (move_mag / move_mag.mean()) * (up - 1.0) * 0.5 * step\n                    step = np.clip(step, self.min_step, self.max_step)\n                    axis_weights *= 0.97\n                    if anchors:\n                        anchors.append((f_best, x_best.copy()))\n                        anchors.sort(key=lambda t: t[0])\n                        if len(anchors) > self.anchor_pool_size:\n                            anchors = anchors[:self.anchor_pool_size]\n                else:\n                    # small shrinkage to encourage different blends next time\n                    step *= down ** 0.05\n                    axis_weights *= 0.995\n            else:\n                # opposition or a global sample - cheap diversification (degree=1)\n                if self.rng.random() < 0.6:\n                    # opposition inside bounds\n                    x_cand = lb + ub - x_best\n                else:\n                    # pure random global sample\n                    x_cand = lb + self.rng.random(self.dim) * (ub - lb)\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                f_cand = float(func(x_cand))\n                evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand\n                    x_best = x_cand\n                    improved = True\n                    stagnation = 0\n                    # reset some steps to encourage exploration around new area\n                    step = np.clip(step * 1.1 + (ub - lb).mean() * 0.01, self.min_step, self.max_step)\n                    anchors.append((f_best, x_best.copy()))\n                    anchors.sort(key=lambda t: t[0])\n                    if len(anchors) > self.anchor_pool_size:\n                        anchors = anchors[:self.anchor_pool_size]\n\n            # end move types\n\n            if not improved:\n                stagnation += 1\n            else:\n                # mild global decay of stagnation on improvement\n                stagnation = max(0, stagnation - 0)\n\n            # occasional stronger diversification if stagnating\n            if stagnation >= self.stagnation_patience and evals < self.budget:\n                # try a few single-eval randoms (but cap so we don't blow budget)\n                tries = min(3, remaining)\n                improved_local = False\n                for _ in range(tries):\n                    x_rand = lb + self.rng.random(self.dim) * (ub - lb)\n                    f_rand = float(func(x_rand))\n                    evals += 1\n                    if f_rand < f_best:\n                        f_best = f_rand\n                        x_best = x_rand\n                        improved_local = True\n                        break\n                if improved_local:\n                    stagnation = 0\n                    # reset some step sizes to moderate values around new point\n                    step = np.clip(step * 1.1, self.min_step, self.max_step)\n                    anchors.append((f_best, x_best.copy()))\n                    anchors.sort(key=lambda t: t[0])\n                    if len(anchors) > self.anchor_pool_size:\n                        anchors = anchors[:self.anchor_pool_size]\n                else:\n                    # a near-random jump (one eval)\n                    if evals < self.budget:\n                        jump_scale = 0.3 * (ub - lb)\n                        x_jump = x_best + self.rng.normal(0.0, 1.0, size=self.dim) * jump_scale\n                        x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                        f_jump = float(func(x_jump))\n                        evals += 1\n                        if f_jump < f_best:\n                            f_best = f_jump\n                            x_best = x_jump\n                            stagnation = 0\n                            step = np.clip(step * 1.2, self.min_step, self.max_step)\n                            anchors.append((f_best, x_best.copy()))\n                            anchors.sort(key=lambda t: t[0])\n                            if len(anchors) > self.anchor_pool_size:\n                                anchors = anchors[:self.anchor_pool_size]\n                        else:\n                            # encourage exploration by slightly increasing steps\n                            step = np.clip(step * 1.12, self.min_step, self.max_step)\n                            stagnation = 0  # give next cycles chance after diversification\n\n            # keep axis_weights from exploding\n            axis_weights = np.clip(axis_weights, 1e-6, 1e6)\n\n            # final safety: ensure steps within bounds\n            step = np.clip(step, self.min_step, self.max_step)\n\n        # finished or budget exhausted\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 13, "feedback": "The algorithm LowDegreeAdaptiveSubspaceSearch scored 0.588 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0e0f14b7-0341-4b06-a151-5442f5c3b114"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9575305659082975}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9455167993914229}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9553360601385684}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9272409076452099}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8692013465065651}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8932929586366256}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.04767914376060667}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03975139235741121}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.038575215272502295}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.04886445430172204}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06256182100635543}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07182334819731095}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9849207518789993}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9863490255635654}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.989897992922788}], "aucs": [0.9575305659082975, 0.9455167993914229, 0.9553360601385684, 0.9272409076452099, 0.8692013465065651, 0.8932929586366256, 0.04767914376060667, 0.03975139235741121, 0.038575215272502295, 0.04886445430172204, 0.06256182100635543, 0.07182334819731095, 0.9849207518789993, 0.9863490255635654, 0.989897992922788]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2601.0, "Edges": 2600.0, "Max Degree": 20.0, "Min Degree": 1.0, "Mean Degree": 1.9992310649750096, "Degree Variance": 1.8861970250403501, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 9.329931972789115, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.330270440888124, "Depth Entropy": 2.277813189868667, "Assortativity": 0.0, "Average Eccentricity": 18.108419838523645, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00038446751249519417, "Average Shortest Path": 11.12448762310354, "mean_complexity": 16.333333333333332, "total_complexity": 49.0, "mean_token_count": 761.3333333333334, "total_token_count": 2284.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "max degree"}
{"id": "0c2ccbf7-9070-4a2c-938c-d7548ac0c9e6", "fitness": 0.5835566656405858, "name": "SeparableLinearPushV2", "description": "Separable Linear Push v2 \u2014 prioritized per-coordinate mirrored probes with lightweight momentum extrapolation and compact 2-D scans; adapt per-dimension steps aggressively on success and gently on failure, plus staged jitter restarts for robust diversification.", "code": "import numpy as np\n\nclass SeparableLinearPushV2:\n    \"\"\"\n    Separable Linear Push v2 (SLP-v2)\n\n    - Per-dimension finite-difference slope estimates from mirrored probes (pos/neg).\n    - Prioritized single-coordinate pushes (choose coords by |slope|*step) with\n      1-step momentum extrapolation on success.\n    - Sparse 2-D linear scans to capture simple interactions.\n    - Adaptive per-dim multiplicative step growth/shrink and staged jitter restarts.\n    - Strict budget enforcement.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_step=0.8, min_step=1e-6, max_step=2.5,\n                 grow=1.3, shrink=0.6, stagn_patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.stagn_patience = int(stagn_patience)\n        self.rng = np.random.default_rng() if rng is None else (rng if isinstance(rng, np.random.Generator) else np.random.default_rng(rng))\n\n    def _bounds(self, func):\n        lb, ub = -5.0, 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n\n        if self.budget <= 0:\n            x0 = clip((lb + ub) * 0.5)\n            return float(func(x0)), x0.copy()\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # init\n        x_best = clip(lb + self.rng.random(self.dim) * (ub - lb))\n        f_best = float(func(x_best)); evals += 1\n\n        step = np.full(self.dim, self.init_step, dtype=float)\n        pos = np.full(self.dim, np.nan); neg = np.full(self.dim, np.nan)\n        slope = np.zeros(self.dim)\n        stagn = 0\n\n        eye = np.eye(self.dim)\n\n        while evals < self.budget:\n            improved = False\n            remaining = self.budget - evals\n\n            # choose action: coordinate push (most), 2D scan (some), jitter (rare)\n            r = self.rng.random()\n            # recompute priorities\n            pri = np.abs(slope) * step + 1e-12\n            if r < 0.66:\n                # single-coordinate prioritized push\n                if np.all(pri == pri.flat[0]) or self.rng.random() < 0.12:\n                    idx = int(self.rng.integers(self.dim))\n                else:\n                    p = pri / pri.sum()\n                    idx = int(self.rng.choice(self.dim, p=p))\n\n                # preferred direction: downhill per slope or random if unknown\n                if slope[idx] > 0:\n                    dir_sign = -1.0\n                elif slope[idx] < 0:\n                    dir_sign = 1.0\n                else:\n                    dir_sign = 1.0 if self.rng.random() < 0.5 else -1.0\n\n                delta = dir_sign * step[idx] * eye[idx]\n                x1 = clip(x_best + delta)\n                f1 = safe_eval(x1)\n                if f1 is None:\n                    break\n                # update pos/neg\n                if dir_sign > 0:\n                    pos[idx] = f1\n                else:\n                    neg[idx] = f1\n                if f1 < f_best:\n                    # accept and try one extrapolation (momentum)\n                    x_prev = x_best.copy()\n                    x_best = x1.copy(); f_best = f1; improved = True\n                    extra = 1.6\n                    if evals < self.budget:\n                        x2 = clip(x_best + extra * (x_best - x_prev))\n                        f2 = safe_eval(x2)\n                        if f2 is None:\n                            break\n                        # update pos/neg for idx by comparing coordinate offset\n                        off = x2[idx] - x_prev[idx]\n                        if off > 0:\n                            pos[idx] = f2\n                        elif off < 0:\n                            neg[idx] = f2\n                        if f2 < f_best:\n                            x_best = x2.copy(); f_best = f2\n                            step[idx] = min(self.max_step, step[idx] * self.grow)\n                        else:\n                            # modest growth on single success without extrapolation pay-off\n                            step[idx] = min(self.max_step, step[idx] * 1.05)\n                else:\n                    # try mirrored side if budget allows\n                    x_other = clip(x_best - delta)\n                    if evals < self.budget:\n                        f_other = safe_eval(x_other)\n                        if f_other is None:\n                            break\n                        if dir_sign < 0:\n                            pos[idx] = f_other\n                        else:\n                            neg[idx] = f_other\n                        if f_other < f_best:\n                            x_best = x_other.copy(); f_best = f_other; improved = True\n                    if not improved:\n                        step[idx] = max(self.min_step, step[idx] * self.shrink)\n\n                # update slope if both available\n                if not np.isnan(pos[idx]) and not np.isnan(neg[idx]) and (2*step[idx])>0:\n                    slope[idx] = (pos[idx] - neg[idx]) / (2.0 * step[idx])\n\n            elif r < 0.94:\n                # sparse 2-D linear scan: pick top coord and another random\n                # pick first by priority, second random (distinct)\n                if np.all(pri == pri.flat[0]) or self.rng.random() < 0.15:\n                    i = int(self.rng.integers(self.dim))\n                else:\n                    p = pri / pri.sum()\n                    i = int(self.rng.choice(self.dim, p=p))\n                j = int(self.rng.integers(self.dim - 1))\n                if j >= i: j += 1\n\n                si = -1.0 if slope[i] > 0 else 1.0\n                sj = -1.0 if slope[j] > 0 else 1.0\n                dirs = [si * step[i] * eye[i], sj * step[j] * eye[j], si * step[i] * eye[i] + sj * step[j] * eye[j]]\n                best_local_f = f_best; best_local_x = x_best.copy()\n                for d in dirs:\n                    if evals >= self.budget: break\n                    xc = clip(x_best + d)\n                    fc = safe_eval(xc)\n                    if fc is None: break\n                    # update pos/neg conservatively (take best seen)\n                    di = int(np.nonzero(np.abs(d))[0][0]) if np.any(d) else None\n                    # for both dims i and j, update by sign\n                    for k in (i, j):\n                        if d[k] > 0:\n                            pos[k] = fc if np.isnan(pos[k]) else min(pos[k], fc)\n                        elif d[k] < 0:\n                            neg[k] = fc if np.isnan(neg[k]) else min(neg[k], fc)\n                    if fc < best_local_f:\n                        best_local_f = fc; best_local_x = xc.copy()\n                if best_local_f < f_best:\n                    prev = x_best.copy()\n                    x_best = best_local_x.copy(); f_best = best_local_f; improved = True\n                    # small extrapolation along combined move\n                    if evals < self.budget:\n                        nx = clip(x_best + 1.3 * (x_best - prev))\n                        fn = safe_eval(nx)\n                        if fn is None: break\n                        if fn < f_best:\n                            x_best = nx.copy(); f_best = fn\n                            step[i] = min(self.max_step, step[i] * self.grow)\n                            step[j] = min(self.max_step, step[j] * self.grow)\n                else:\n                    step[i] = max(self.min_step, step[i] * self.shrink)\n                    step[j] = max(self.min_step, step[j] * self.shrink)\n\n                # recompute slopes\n                for k in (i, j):\n                    if not np.isnan(pos[k]) and not np.isnan(neg[k]) and (2*step[k])>0:\n                        slope[k] = (pos[k] - neg[k]) / (2.0 * step[k])\n\n            else:\n                # diversification: jitter scaled by median step or stronger restart when stagnating\n                if stagn >= self.stagn_patience and remaining >= 2:\n                    scale = 0.6 * np.maximum(step, 1e-12)\n                    x1 = clip(x_best + self.rng.normal(scale=scale))\n                    f1 = safe_eval(x1)\n                    if f1 is None: break\n                    x2 = clip(x_best + self.rng.normal(scale=1.4*scale)) if evals < self.budget else None\n                    f2 = safe_eval(x2) if x2 is not None else None\n                    # choose best among candidates\n                    cand = [(f_best, x_best), (f1, x1)]\n                    if f2 is not None: cand.append((f2, x2))\n                    cand.sort(key=lambda t: t[0])\n                    if cand[0][0] < f_best:\n                        x_best = cand[0][1].copy(); f_best = cand[0][0]; improved = True\n                        # seed pos/neg lightly from this new area\n                        pos[:] = np.nan; neg[:] = np.nan; slope[:] = 0.0\n                    else:\n                        # gentle expansion of some steps\n                        ix = self.rng.choice(self.dim, size=max(1, self.dim//8), replace=False)\n                        for k in ix: step[k] = min(self.max_step, step[k] * 1.2)\n                else:\n                    med = np.median(step)\n                    x_new = clip(x_best + self.rng.normal(scale=0.35*med, size=self.dim))\n                    f_new = safe_eval(x_new)\n                    if f_new is None: break\n                    if f_new < f_best:\n                        x_best = x_new.copy(); f_best = f_new; improved = True\n                    else:\n                        step = np.maximum(self.min_step, step * 0.98)\n\n            # adapt steps and stagnation bookkeeping\n            if improved:\n                stagn = 0\n                # modest global growth to keep momentum\n                step = np.minimum(self.max_step, step * 1.06)\n            else:\n                stagn += 1\n                step = np.maximum(self.min_step, step * 0.96)\n\n            # enforce bounds\n            step = np.clip(step, self.min_step, self.max_step)\n\n            # conservative recompute of all slopes when possible\n            for k in range(self.dim):\n                if not np.isnan(pos[k]) and not np.isnan(neg[k]) and (2*step[k])>0:\n                    slope[k] = (pos[k] - neg[k]) / (2.0 * step[k])\n\n            # strong restart if prolonged stagnation\n            if stagn >= 4 * self.stagn_patience and evals < self.budget:\n                if self.rng.random() < 0.5:\n                    xr = clip(lb + self.rng.random(self.dim) * (ub - lb))\n                else:\n                    center = 0.5 * (lb + ub)\n                    xr = clip(center + self.rng.normal(scale=0.5 * (ub - lb)))\n                fr = safe_eval(xr)\n                if fr is None: break\n                if fr < f_best:\n                    x_best = xr.copy(); f_best = fr\n                    pos[:] = np.nan; neg[:] = np.nan; slope[:] = 0.0\n                    step[:] = np.minimum(self.max_step, self.init_step)\n                    stagn = 0\n                else:\n                    step = np.minimum(self.max_step, step * 1.3)\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 14, "feedback": "The algorithm SeparableLinearPushV2 scored 0.584 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5951c2b-b9ff-4352-bb47-76f19aa885c0"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9717098500263112}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9646057590096206}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9589784802685528}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9658461589041785}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.931387310000511}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9335379355677189}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.04634671713865213}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.01547799136071748}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.052243906709385945}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.03752421313866561}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.040611826958617536}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.00399017867223761}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9431471024887047}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9382764215460152}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9496661328188989}], "aucs": [0.9717098500263112, 0.9646057590096206, 0.9589784802685528, 0.9658461589041785, 0.931387310000511, 0.9335379355677189, 0.04634671713865213, 0.01547799136071748, 0.052243906709385945, 0.03752421313866561, 0.040611826958617536, 0.00399017867223761, 0.9431471024887047, 0.9382764215460152, 0.9496661328188989]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2629.0, "Edges": 2628.0, "Max Degree": 20.0, "Min Degree": 1.0, "Mean Degree": 1.99923925446938, "Degree Variance": 1.7748187442027155, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 9.399668325041459, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3269653207350043, "Depth Entropy": 2.278156381755417, "Assortativity": 0.0, "Average Eccentricity": 18.0418410041841, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0003803727653100038, "Average Shortest Path": 11.178170771739866, "mean_complexity": 19.75, "total_complexity": 79.0, "mean_token_count": 571.5, "total_token_count": 2286.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "3b2d4dbb-816f-4284-b7dc-44afb78bef24", "fitness": 0.5776069059463455, "name": "ProgressiveAnchorsSEAB2", "description": "Adaptive Anchor-and-Blend (compact SEAB+) \u2014 a compact single-eval anchor pool that biases coordinate probes by learned importance, uses one-eval blends/opposition/Levy jumps and light diag probes, and respawns stale anchors near the best to keep cheap, focused exploration.", "code": "import numpy as np\n\nclass ProgressiveAnchorsSEAB2:\n    def __init__(self, budget, dim, pop_size=None,\n                 init_step=0.14, success_mult=1.33, failure_mult=0.65,\n                 blend_p=0.12, opp_p=0.06, levy_p=0.035, diag_p=0.10,\n                 respawn_noise=0.06, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.pop = pop_size if pop_size is not None else max(3, min(7, 3 + self.dim // 6))\n        self.init_step_frac = float(init_step)\n        self.smul = float(success_mult); self.fmul = float(failure_mult)\n        self.blend_p = float(blend_p); self.opp_p = float(opp_p)\n        self.levy_p = float(levy_p); self.diag_p = float(diag_p)\n        self.respawn_noise = float(respawn_noise)\n        self.x_opt = None; self.f_opt = None\n\n    def __call__(self, func):\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0); ub = np.full(self.dim, 5.0)\n\n        box = ub - lb\n        box_len = np.linalg.norm(box)\n        base = max(1e-12, self.init_step_frac * box_len)\n\n        evals = 0\n        rng = self.rng\n\n        # init anchors\n        A = rng.uniform(lb, ub, size=(self.pop, self.dim))\n        F = np.full(self.pop, np.inf)\n        for i in range(self.pop):\n            if evals >= self.budget: break\n            F[i] = float(func(A[i])); evals += 1\n\n        if not np.isfinite(F).any():\n            if evals < self.budget:\n                F[0] = float(func(A[0])); evals += 1\n            else:\n                self.x_opt = np.clip(np.zeros(self.dim), lb, ub); self.f_opt = float(np.inf); return self.f_opt, self.x_opt\n\n        # state\n        steps = np.full((self.pop, self.dim), base)\n        sign = np.zeros((self.pop, self.dim), dtype=float)  # last successful direction\n        age = np.zeros(self.pop, dtype=int)\n        coord_score = np.ones(self.dim, dtype=float)  # importance weights\n\n        ib = int(np.nanargmin(F)); xb = A[ib].copy(); fb = float(F[ib])\n\n        # main loop: keep evaluations moderate per pass (at most ~pop + small extras)\n        while evals < self.budget:\n            improved = False\n\n            # occasional opposition of best\n            if rng.random() < self.opp_p and evals < self.budget:\n                xopp = np.clip(lb + ub - xb, lb, ub)\n                if not np.allclose(xopp, xb):\n                    fopp = float(func(xopp)); evals += 1\n                    if fopp < fb:\n                        fb, xb, improved = fopp, xopp.copy(), True\n                        worst = int(np.nanargmax(F)); A[worst] = xopp.copy(); F[worst] = fopp\n                        steps[worst, :] = base; sign[worst, :] = 0.0\n\n            # occasional blend (one-eval)\n            if rng.random() < self.blend_p and self.pop > 1 and evals < self.budget:\n                j = int(rng.integers(0, self.pop))\n                if j != ib:\n                    w = 0.6 + 0.7 * rng.random()\n                    cand = np.clip(w * xb + (1-w) * A[j], lb, ub)\n                    if np.allclose(cand, xb) or np.allclose(cand, A[j]):\n                        cand = np.clip(cand + 1e-6 * box_len * rng.standard_normal(self.dim), lb, ub)\n                    if not np.allclose(cand, xb):\n                        fc = float(func(cand)); evals += 1\n                        if fc < fb:\n                            fb, xb, improved = fc, cand.copy(), True\n                            worst = int(np.nanargmax(F)); A[worst] = cand.copy(); F[worst] = fc\n                            steps[worst, :] = base; sign[worst, :] = 0.0\n\n            # occasional Levy-like long probe from best to escape deep traps\n            if rng.random() < self.levy_p and evals < self.budget:\n                # Cauchy scaled to box\n                jump = box_len * (0.6 + rng.random() * 0.9) * rng.standard_cauchy(self.dim)\n                cand = np.clip(xb + 0.06 * jump, lb, ub)\n                if not np.allclose(cand, xb):\n                    fc = float(func(cand)); evals += 1\n                    if fc < fb:\n                        fb, xb, improved = fc, cand.copy(), True\n                        worst = int(np.nanargmax(F)); A[worst] = cand.copy(); F[worst] = fc\n                        steps[worst, :] = base; sign[worst, :] = 0.0\n\n            # one coordinate probe per anchor per pass, chosen by learned coord_score and anchor age\n            order = rng.permutation(self.pop)\n            for idx in order:\n                if evals >= self.budget: break\n                age[idx] += 1\n                x0 = A[idx].copy(); f0 = F[idx]\n                # pick coord biased by coord_score * steps (favor uncertain large steps)\n                weights = coord_score * (steps[idx] + 1e-12)\n                weights = np.maximum(weights, 1e-12)\n                c = int(rng.choice(self.dim, p=weights / weights.sum()))\n                # choose sign: prefer remembered, else random\n                s = sign[idx, c] if sign[idx, c] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                xtry = x0.copy()\n                xtry[c] = np.clip(x0[c] + s * steps[idx, c], lb[c], ub[c])\n                if np.allclose(xtry, x0):\n                    sign[idx, c] = 0.0; steps[idx, c] *= self.fmul; age[idx] += 1\n                else:\n                    ftry = float(func(xtry)); evals += 1\n                    if ftry < f0:\n                        A[idx] = xtry.copy(); F[idx] = ftry; x0 = xtry.copy(); f0 = ftry\n                        sign[idx, c] = s; steps[idx, c] *= self.smul\n                        coord_score[c] += 1.0  # reward coord\n                        age[idx] = 0; improved = True\n                        if ftry < fb:\n                            fb, xb = ftry, xtry.copy()\n                    else:\n                        sign[idx, c] = -s; steps[idx, c] *= self.fmul\n                        coord_score[c] *= 0.96  # small penalty\n\n                # occasional small diag probe combining top two coords for that anchor\n                if self.dim >= 2 and rng.random() < self.diag_p and evals < self.budget:\n                    top = np.argsort(steps[idx])[::-1][:2]\n                    a, b = int(top[0]), int(top[1]) if top.size > 1 else int(top[0])\n                    sa = sign[idx, a] if sign[idx, a] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                    sb = sign[idx, b] if sign[idx, b] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                    xdiag = x0.copy()\n                    xdiag[a] = np.clip(x0[a] + sa * steps[idx, a], lb[a], ub[a])\n                    xdiag[b] = np.clip(x0[b] + sb * steps[idx, b], lb[b], ub[b])\n                    if not np.allclose(xdiag, x0):\n                        fdiag = float(func(xdiag)); evals += 1\n                        if fdiag < F[idx]:\n                            A[idx] = xdiag.copy(); F[idx] = fdiag; steps[idx, a] *= 1.12; steps[idx, b] *= 1.12\n                            sign[idx, a], sign[idx, b] = sa, sb; age[idx] = 0; improved = True\n                            if fdiag < fb: fb, xb = fdiag, xdiag.copy()\n                        else:\n                            steps[idx, a] *= self.fmul; steps[idx, b] *= self.fmul\n\n                # age-based respawn of stale anchors (single-eval)\n                if age[idx] > max(10, 4 * self.dim) and evals < self.budget:\n                    age[idx] = 0\n                    if rng.random() < 0.75:\n                        newp = xb + self.respawn_noise * box * rng.standard_normal(self.dim)\n                    else:\n                        newp = rng.uniform(lb, ub)\n                    newp = np.clip(newp, lb, ub)\n                    if not np.allclose(newp, A[idx]):\n                        fn = float(func(newp)); evals += 1\n                        A[idx] = newp.copy(); F[idx] = fn; steps[idx, :] = base; sign[idx, :] = 0.0\n                        if fn < fb: fb, xb = fn, newp.copy(); improved = True\n\n            # occasional replacement of worst by a perturbation of best when stuck\n            if not improved and evals < self.budget and rng.random() < 0.18:\n                iw = int(np.nanargmax(F))\n                newp = np.clip(xb + 0.08 * box * rng.standard_normal(self.dim), lb, ub)\n                if not np.allclose(newp, A[iw]):\n                    fn = float(func(newp)); evals += 1\n                    A[iw] = newp.copy(); F[iw] = fn; steps[iw, :] = base; sign[iw, :] = 0.0\n                    if fn < fb: fb, xb = fn, newp.copy(); improved = True\n\n            # keep steps sane\n            mn = 1e-10 * box_len; mx = 1.2 * box_len\n            steps = np.clip(steps, mn, mx)\n            # budget-aware quick exit\n            if self.budget - evals < max(3, self.dim // 3):\n                break\n\n        # final local single-eval sweep around xb using consensus sign\n        mesh = 0.07 * box_len; mesh_min = 1e-8 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            any_imp = False\n            # consensus sign from anchors\n            cons = np.sign((sign != 0).astype(float).sum(axis=0))\n            for d in range(self.dim):\n                if evals >= self.budget: break\n                s = cons[d] if cons[d] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                xp = xb.copy(); xp[d] = np.clip(xb[d] + s * mesh, lb[d], ub[d])\n                if np.allclose(xp, xb): continue\n                fp = float(func(xp)); evals += 1\n                if fp < fb:\n                    fb, xb, any_imp = fp, xp.copy(), True\n                    steps[:, d] = np.maximum(steps[:, d], 0.8 * mesh)\n            if not any_imp:\n                mesh *= 0.45\n\n        self.x_opt = np.asarray(xb, dtype=float); self.f_opt = float(fb)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 14, "feedback": "The algorithm ProgressiveAnchorsSEAB2 scored 0.578 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0551dcff-1e39-4ac0-9547-8473ac5d6b75"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9606363048773761}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.96248551236522}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9401329435045718}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.6485123130779371}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.744367196893336}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6617055041854663}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1124611959546653}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1477432208769771}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.1326377805547414}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.12383169493221535}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.11041460723320018}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.13528629997369623}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9956942205943795}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9938014222720855}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9943933718993129}], "aucs": [0.9606363048773761, 0.96248551236522, 0.9401329435045718, 0.6485123130779371, 0.744367196893336, 0.6617055041854663, 0.1124611959546653, 0.1477432208769771, 0.1326377805547414, 0.12383169493221535, 0.11041460723320018, 0.13528629997369623, 0.9956942205943795, 0.9938014222720855, 0.9943933718993129]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2855.0, "Edges": 2854.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9992994746059545, "Degree Variance": 1.9656737649559413, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.710068130204391, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3311505131574284, "Depth Entropy": 2.0902995254365297, "Assortativity": 0.0, "Average Eccentricity": 17.47985989492119, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0003502626970227671, "Average Shortest Path": 10.886416950063634, "mean_complexity": 33.0, "total_complexity": 66.0, "mean_token_count": 1224.5, "total_token_count": 2449.0, "mean_parameter_count": 7.5, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "222d9ec4-d304-4559-9eb6-2586b4c62aca", "fitness": 0.16425506393476264, "name": "AdaptivePairwiseRotationalPush", "description": "Adaptive Pairwise Rotational Push (APRP) \u2014 mix single-coordinate micro-pushes with biased 2-D rotated pushes formed from an adaptive per-dimension step palette and a tiny archive of successful deltas; successes boost both involved steps and seed archive-driven directional bias, failures shrink steps; occasional heavy-tail global jumps for escape.", "code": "import numpy as np\n\nclass AdaptivePairwiseRotationalPush:\n    \"\"\"\n    Adaptive Pairwise Rotational Push (APRP)\n\n    - Budget-safe optimizer for continuous unconstrained problems in [-5,5] per-dim by default.\n    - Mixes single-dimension micro-pushes and 2-D rotated pushes on chosen coordinate pairs.\n    - Keeps a tiny archive of successful deltas to bias pair selection and small rotation angles.\n    - Multiplicative step adaptation on success/failure, occasional heavy-tail global probes.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 success_mult=1.25, failure_mult=0.7,\n                 pair_prob=0.55, global_prob=0.03,\n                 archive_size=8, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.pair_prob = float(pair_prob)\n        self.global_prob = float(global_prob)\n        self.archive_size = int(max(1, archive_size))\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n        min_step = 1e-12 * box_len\n        max_step = 1.5 * box_len\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            fv = float(func(x))\n            evals += 1\n            return fv\n\n        # init\n        x = rng.uniform(lb, ub)\n        f = safe_eval(x)\n        x_best = x.copy(); f_best = float(f)\n\n        # per-dim adaptive steps\n        steps = np.full(dim, base_step, dtype=float)\n\n        # tiny circular archive of successful deltas (for bias)\n        archive = np.zeros((self.archive_size, dim), dtype=float)\n        arch_count = 0\n        arch_pos = 0\n\n        # simple momentum (last accepted delta)\n        last_delta = np.zeros(dim, dtype=float)\n\n        while evals < self.budget:\n            # occasional heavy-tail global escape\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.12 * box_len * cauchy\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    last_delta = xg - x_best\n                    x_best = xg.copy(); f_best = fg\n                    # inflate steps slightly where jump moved\n                    moved = np.abs(last_delta) > 1e-12\n                    steps[moved] = np.minimum(max_step, steps[moved] * self.success_mult)\n                continue\n\n            # choose action: pairwise rotated push or single-dim micro-push\n            do_pair = (dim >= 2) and (rng.random() < self.pair_prob)\n            if do_pair:\n                # compute dimension importance scores from steps and archive variance\n                if arch_count > 0:\n                    var = np.var(archive[:arch_count], axis=0)\n                else:\n                    var = np.ones(dim) * 1e-6\n                scores = steps * (1.0 + var / (var.mean() + 1e-18))\n                # pick first dim weighted by scores\n                p = scores.clip(min=0)\n                p_sum = p.sum()\n                if p_sum <= 0:\n                    i = rng.integers(dim)\n                else:\n                    r = rng.random() * p_sum\n                    cum = 0.0\n                    i = 0\n                    for idx, val in enumerate(p):\n                        cum += val\n                        if r <= cum:\n                            i = idx; break\n                # pick second dim different from i with weighted scores\n                p2 = p.copy(); p2[i] = 0.0\n                p2s = p2.sum()\n                if p2s <= 0:\n                    j = (i + 1) % dim\n                else:\n                    r = rng.random() * p2s\n                    cum = 0.0\n                    j = 0\n                    for idx, val in enumerate(p2):\n                        cum += val\n                        if r <= cum:\n                            j = idx; break\n\n                # angle bias from archive principal directions projected to (i,j)\n                theta_bias = 0.0\n                if arch_count > 0:\n                    proj = archive[:arch_count][:, [i, j]]\n                    # small principal direction\n                    u, s, vh = np.linalg.svd(proj, full_matrices=False)\n                    if s.size > 0 and s[0] > 1e-12:\n                        vec = vh[0]\n                        theta_bias = np.arctan2(vec[1], vec[0])\n                # sample small rotation around bias\n                theta = rng.normal(theta_bias, 0.6)  # moderate spread\n                # construct rotated 2D push\n                si = steps[i]; sj = steps[j]\n                vi = si * np.cos(theta)\n                vj = sj * np.sin(theta)\n                v = np.zeros(dim, dtype=float)\n                v[i] = vi; v[j] = vj\n                # occasionally flip sign of whole vector for exploration\n                if rng.random() < 0.25:\n                    v = -v\n                xt = np.clip(x_best + v, lb, ub)\n                ft = safe_eval(xt)\n\n                if ft < f_best:\n                    last_delta = xt - x_best\n                    x_best = xt.copy(); f_best = float(ft)\n                    # success: boost involved steps modestly\n                    steps[i] = min(max_step, steps[i] * self.success_mult)\n                    steps[j] = min(max_step, steps[j] * self.success_mult)\n                    # add delta to archive\n                    archive[arch_pos] = last_delta\n                    arch_pos = (arch_pos + 1) % self.archive_size\n                    arch_count = min(self.archive_size, arch_count + 1)\n                else:\n                    # failure: shrink both\n                    steps[i] = max(min_step, steps[i] * self.failure_mult)\n                    steps[j] = max(min_step, steps[j] * self.failure_mult)\n                    # if repeated tiny steps, nudge one dimension randomly\n                    if steps[i] <= 1.5 * min_step and steps[j] <= 1.5 * min_step:\n                        k = rng.integers(dim)\n                        steps[k] = min(max_step, steps[k] * 1.5)\n                # continue loop\n                continue\n\n            # single-dimension micro-push\n            # select dim weighted by steps and archive variance\n            if arch_count > 0:\n                var = np.var(archive[:arch_count], axis=0)\n            else:\n                var = np.ones(dim) * 1e-6\n            scores = steps * (1.0 + var / (var.mean() + 1e-18))\n            psum = scores.sum()\n            if psum <= 0:\n                d = rng.integers(dim)\n            else:\n                r = rng.random() * psum\n                cum = 0.0\n                d = 0\n                for idx, val in enumerate(scores):\n                    cum += val\n                    if r <= cum:\n                        d = idx; break\n            # one-sided micro push with small momentum bias\n            sd = max(min(steps[d], max_step), min_step)\n            sign = 1 if rng.random() < 0.5 else -1\n            # bias sign by last_delta on that dim\n            if last_delta[d] > 0 and rng.random() < 0.7:\n                sign = 1\n            if last_delta[d] < 0 and rng.random() < 0.7:\n                sign = -1\n            xt = x_best.copy()\n            xt[d] = np.clip(xt[d] + sign * sd, lb[d], ub[d])\n            ft = safe_eval(xt)\n            if ft < f_best:\n                last_delta = xt - x_best\n                x_best = xt.copy(); f_best = float(ft)\n                steps[d] = min(max_step, steps[d] * self.success_mult)\n                archive[arch_pos] = last_delta\n                arch_pos = (arch_pos + 1) % self.archive_size\n                arch_count = min(self.archive_size, arch_count + 1)\n            else:\n                steps[d] = max(min_step, steps[d] * self.failure_mult)\n\n        # final tiny polish: try tiny probes along each dim until budget used\n        probe = 0.03 * box_len\n        while evals < self.budget and probe > 1e-12:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = np.clip(x_best + np.eye(dim)[d] * probe, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best = xt.copy(); f_best = float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = np.clip(x_best - np.eye(dim)[d] * probe, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best = xt.copy(); f_best = float(ft); improved = True\n            if not improved:\n                probe *= 0.5\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 14, "feedback": "The algorithm AdaptivePairwiseRotationalPush scored 0.164 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2ce99025-003a-46b2-bec0-52c377bfc061"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5382328563689465}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.09961996090355374}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.49114978776291907}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.022709478170403807}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.0041542961014134905}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.03572216131443562}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.03660364120930937}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.040904848109025616}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.05114828842303287}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.03926194968059504}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.10386745138158582}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8809602772041315}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.11939096239208702}], "aucs": [0.5382328563689465, 0.09961996090355374, 0.49114978776291907, 4.999999999999449e-05, 0.022709478170403807, 0.0041542961014134905, 0.03572216131443562, 0.03660364120930937, 0.040904848109025616, 0.05114828842303287, 0.03926194968059504, 4.999999999999449e-05, 0.10386745138158582, 0.8809602772041315, 0.11939096239208702]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2012.0, "Edges": 2011.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9990059642147118, "Degree Variance": 2.2007942405210885, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.826182618261826, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3235432528322064, "Depth Entropy": 1.948543202464546, "Assortativity": 0.0, "Average Eccentricity": 16.392644135188867, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0004970178926441351, "Average Shortest Path": 9.567606790880772, "mean_complexity": 12.0, "total_complexity": 48.0, "mean_token_count": 433.25, "total_token_count": 1733.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "1bcb1936-423e-41c9-9aa2-ad5ec868270f", "fitness": 0.22064542455006672, "name": "AdaptiveSeparableContrastivePull", "description": "Adaptive Separable Contrastive Pull (ASCP) \u2014 per-coordinate contrastive pulls toward an archive-weighted center with cheap 1-D bracket probes, multiplicative step adaptation, and archive-shaped single-coordinate Gaussian diversification.", "code": "import numpy as np\n\nclass AdaptiveSeparableContrastivePull:\n    \"\"\"\n    Adaptive Separable Contrastive Pull (ASCP)\n\n    - Strictly (mostly) degree-1 moves: each evaluation changes one coordinate.\n    - Maintain a small top-k archive. Build an archive-weighted center (per-dim).\n    - Choose a coordinate by contrast magnitude normalized by step and archive dispersion.\n    - Probe the predicted sign first (one eval). On failure optionally try opposite sign.\n    - Adapt per-coordinate multiplicative steps; occasional archive-shaped single-dim Gaussian jumps on stagnation.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, archive_size=12, init_step=None,\n                 min_step=1e-6, max_step=None, stagnation_patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float); ub = np.array(ub, dtype=float)\n        if lb.shape == (): lb = np.full(self.dim, float(lb))\n        if ub.shape == (): ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        rng_dim = self.dim\n        span = ub - lb\n        if self.init_step is None:\n            step = 0.2 * span\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (self.dim,) else np.full(self.dim, float(s))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (self.dim,) else np.full(self.dim, float(m))\n\n        step = np.clip(step, self.min_step, max_step)\n\n        top_k = min(self.archive_size, max(2, self.budget // 6))\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n\n        # seed archive with a few samples\n        n0 = min(max(4, self.dim), max(1, self.budget // 12))\n        for _ in range(n0):\n            x = lb + rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n            if evals >= self.budget: break\n\n        if len(archive_x) == 0:\n            x = lb + rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n\n        # best so far\n        idx = int(np.argmin(archive_f))\n        x_best = archive_x[idx].copy()\n        f_best = float(archive_f[idx])\n\n        stagn = 0\n\n        def add_archive(xn, fn):\n            nonlocal archive_x, archive_f\n            if len(archive_x) < top_k:\n                archive_x.append(xn.copy()); archive_f.append(float(fn))\n            else:\n                worst = int(np.argmax(archive_f))\n                if fn < archive_f[worst]:\n                    archive_x[worst] = xn.copy(); archive_f[worst] = float(fn)\n\n        while evals < self.budget:\n            # archive statistics\n            A = np.array(archive_x)\n            # weights: favor better archive members (inverse rank)\n            idxs = np.argsort(archive_f)\n            ranks = np.empty_like(idxs); ranks[idxs] = np.arange(len(idxs))\n            weights = 1.0 / (1.0 + ranks)\n            weights = weights / weights.sum()\n            center = np.sum(A * weights[:, None], axis=0)   # archive-weighted center\n            q75 = np.percentile(A, 75, axis=0); q25 = np.percentile(A, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n\n            contrast = center - x_best\n            abs_contrast = np.abs(contrast)\n\n            # coordinate preference: contrast normalized by step plus archive dispersion term\n            score = abs_contrast / (step + 1e-12) + 0.4 * (iqr / (span + 1e-12))\n            score = score + 1e-6\n            # softmax selection temperature depends on global spread\n            temp = 1.0 + 3.0 * (np.median(iqr) / (span.mean() + 1e-12))\n            scaled = (score - score.max()) / temp\n            probs = np.exp(scaled); probs = probs / probs.sum()\n            try:\n                coord = int(rng.choice(self.dim, p=probs))\n            except Exception:\n                coord = int(rng.integers(0, self.dim))\n\n            s = max(step[coord], self.min_step)\n            # target sign from contrast, with random flip based on small noise\n            if abs_contrast[coord] > 1e-12:\n                sign = int(np.sign(contrast[coord]))\n                if rng.random() < 0.08:\n                    sign = -sign\n            else:\n                sign = 1 if rng.random() < 0.5 else -1\n\n            delta = sign * min(s, abs_contrast[coord] if abs_contrast[coord] > 0 else s)\n            # one-eval probe in predicted direction\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            add_archive(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try; improved = True\n                step[coord] = float(min(max_step[coord], step[coord] * 1.3))\n                stagn = 0\n                # cheap extrapolation (small) if budget allows\n                if evals < self.budget:\n                    ext = 0.4 * abs(delta)\n                    x_ext = x_best.copy()\n                    x_ext[coord] = np.clip(x_ext[coord] + np.sign(delta) * ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_archive(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n            else:\n                # try opposite sign once\n                if evals < self.budget:\n                    opp = -delta if delta != 0 else (-s if rng.random() < 0.5 else s)\n                    x_try2 = x_best.copy()\n                    x_try2[coord] = np.clip(x_try2[coord] + opp, lb[coord], ub[coord])\n                    f_try2 = float(func(x_try2)); evals += 1\n                    add_archive(x_try2, f_try2)\n                    if f_try2 < f_best:\n                        f_best = f_try2; x_best = x_try2; improved = True\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.25))\n                        stagn = 0\n                if not improved:\n                    # both directions failed: shrink this coord step\n                    step[coord] = float(max(self.min_step, step[coord] * 0.55))\n                    stagn += 1\n\n            # clamp steps\n            step = np.minimum(np.maximum(step, self.min_step), max_step)\n\n            # stagnation-triggered archive-shaped single-coordinate Gaussian jumps\n            if stagn >= self.stagnation_patience and evals < self.budget:\n                # pick a coordinate weighted by iqr*step\n                scores2 = (iqr / (span + 1e-12)) * (step / (step.max() + 1e-12) + 0.05)\n                scores2 = scores2 + 1e-8\n                s2 = scores2 / scores2.sum()\n                k = min(3, max(1, self.dim // 6))\n                try:\n                    coords = list(rng.choice(self.dim, size=k, replace=False, p=s2))\n                except Exception:\n                    coords = list(rng.choice(self.dim, size=k, replace=False))\n                progressed = False\n                for c in coords:\n                    if evals >= self.budget: break\n                    scale = max(1e-3, iqr[c]) + 0.2 * step[c]\n                    jump = rng.normal(loc=center[c], scale=scale)\n                    xj = x_best.copy()\n                    xj[c] = np.clip(jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    add_archive(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj; progressed = True\n                        step[c] = float(min(max_step[c], step[c] * 1.35))\n                        break\n                if not progressed:\n                    # soften steps to explore fresh coords\n                    step = np.minimum(max_step, step * (1.0 + 0.2 * rng.random(self.dim)))\n                stagn = 0\n\n        return f_best, x_best", "configspace": "", "generation": 14, "feedback": "The algorithm AdaptiveSeparableContrastivePull scored 0.221 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.3211129977749251}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.054271383703189735}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.3955094685889322}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.5904540583220665}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.6095844177041696}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.7092761908503151}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08456381737176744}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06728279861203468}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0037837878484319187}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.09519967676909802}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.05486795600928418}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.1242757022033536}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.07716047008327753}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.12228864241015436}], "aucs": [0.3211129977749251, 0.054271383703189735, 0.3955094685889322, 0.5904540583220665, 0.6095844177041696, 0.7092761908503151, 0.08456381737176744, 0.06728279861203468, 0.0037837878484319187, 0.09519967676909802, 0.05486795600928418, 4.999999999999449e-05, 0.1242757022033536, 0.07716047008327753, 0.12228864241015436]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1982.0, "Edges": 1981.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.9989909182643795, "Degree Variance": 2.202824410613789, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.890642615558061, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3246478708452558, "Depth Entropy": 2.063190092274283, "Assortativity": 0.0, "Average Eccentricity": 16.02169525731584, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.0005045408678102926, "Average Shortest Path": 9.811706163141164, "mean_complexity": 9.75, "total_complexity": 39.0, "mean_token_count": 436.0, "total_token_count": 1744.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "06919430-09b9-4f76-a599-bb0da3f9b870", "fitness": 0.016431633814982687, "name": "SparseOrbitalRecenter", "description": "Sparse Orbital Recenter (SOR) \u2014 perform cheap 2D orbital probes in adaptive important planes plus occasional 1D pulls and archive blends; adapt orbit radius and plane weights to focus cheap circular sweeps around the current best.", "code": "import numpy as np\n\nclass SparseOrbitalRecenter:\n    \"\"\"\n    Sparse Orbital Recenter (SOR)\n    - Mostly single-eval moves: cheap 2D orbital probes in selected coordinate planes.\n    - Adaptive orbit radius and per-plane/axis importance weights concentrate effort.\n    - Occasional 1D pulls and archive blends provide fine exploitation and light diversification.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, init_radius=None, min_radius=1e-6, max_radius=5.0,\n                 archive_size=6, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_radius = init_radius\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.archive_size = int(archive_size)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb); ub = np.array(ub)\n        if lb.shape == (): lb = np.full(self.dim, float(lb))\n        if ub.shape == (): ub = np.full(self.dim, float(ub))\n        return lb.astype(float), ub.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        if lb.size != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        # initial sample\n        x_best = lb + self.rng.random(self.dim) * (ub - lb)\n        f_best = float(func(x_best)); evals += 1\n\n        # radius and axis/plane importance\n        if self.init_radius is None:\n            radius = float((ub - lb).mean() / 10.0)\n        else:\n            radius = float(self.init_radius)\n        radius = float(np.clip(radius, self.min_radius, self.max_radius))\n\n        axis_w = np.ones(self.dim)\n        plane_w = np.ones((self.dim, self.dim))  # symmetric but we'll sample i<j\n        last_dir = np.zeros(self.dim)\n\n        archive = [(f_best, x_best.copy())]\n\n        # multipliers\n        r_up = 1.22; r_down = 0.80\n        a_up = 1.4; a_down = 0.85\n\n        while evals < self.budget:\n            remaining = self.budget - evals\n            r = self.rng.random()\n\n            improved = False\n\n            # 60%: 2D orbital probe in a plane (single eval)\n            if r < 0.60:\n                # choose plane (i,j) biased by axis_w product\n                probs = axis_w / axis_w.sum()\n                i = self.rng.choice(self.dim, p=probs)\n                # pick j different from i with bias\n                probs_j = axis_w.copy()\n                probs_j[i] = 0.0\n                probs_j = probs_j / probs_j.sum()\n                j = self.rng.choice(self.dim, p=probs_j)\n                # choose angle and radial profile: small circle around x_best in (i,j)\n                theta = self.rng.random() * 2 * np.pi\n                # allow slight anisotropy by axis scale\n                ai = (ub[i] - lb[i]) / (ub.mean() - lb.mean() + 1e-12)\n                aj = (ub[j] - lb[j]) / (ub.mean() - lb.mean() + 1e-12)\n                vec = np.zeros(self.dim)\n                vec[i] = np.cos(theta) * ai\n                vec[j] = np.sin(theta) * aj\n                x_cand = x_best + vec * radius\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                f_cand = float(func(x_cand)); evals += 1\n\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True\n                    radius = min(self.max_radius, radius * r_up)\n                    axis_w[i] *= a_up; axis_w[j] *= a_up\n                    plane_w[i, j] *= 1.3; plane_w[j, i] = plane_w[i, j]\n                    last_dir[i] = np.sign(x_cand[i] - x_best[i])\n                    last_dir[j] = np.sign(x_cand[j] - x_best[j])\n                    archive.append((f_best, x_best.copy()))\n                    archive.sort(key=lambda t: t[0])\n                    archive = archive[:self.archive_size]\n                else:\n                    radius = max(self.min_radius, radius * r_down)\n                    axis_w[i] *= a_down; axis_w[j] *= a_down\n\n            # 25%: single-coordinate pull (cheap 1D exploitation)\n            elif r < 0.85:\n                probs = axis_w / axis_w.sum()\n                idx = self.rng.choice(self.dim, p=probs)\n                # sign bias from last_dir else random\n                sgn = int(np.sign(last_dir[idx])) if last_dir[idx] != 0 else (1 if self.rng.random() < 0.5 else -1)\n                # random magnitude between 0.5 and 1.5 of radius projected to that axis\n                mag = radius * (0.5 + self.rng.random())\n                x_cand = x_best.copy()\n                x_cand[idx] += sgn * mag\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True\n                    radius = min(self.max_radius, radius * r_up)\n                    axis_w[idx] *= a_up\n                    last_dir[idx] = sgn\n                    archive.append((f_best, x_best.copy()))\n                    archive.sort(key=lambda t: t[0]); archive = archive[:self.archive_size]\n                else:\n                    radius = max(self.min_radius, radius * r_down)\n                    axis_w[idx] *= a_down\n                    last_dir[idx] *= 0.7\n\n            # 10%: archive blend (combine two good points, single eval)\n            elif r < 0.95 and len(archive) >= 2:\n                # sample two archive indices with bias to better ones\n                weights = np.array([1.0 / (1 + k) for k in range(len(archive))])\n                weights = weights / weights.sum()\n                ids = self.rng.choice(len(archive), size=2, replace=False, p=weights)\n                a = archive[ids[0]][1]; b = archive[ids[1]][1]\n                alpha = 0.4 + 0.6 * self.rng.random()  # favor interior blends\n                if self.rng.random() < 0.12:\n                    alpha = 0.5 + 0.6 * (self.rng.random() - 0.5)  # occasional extrapolate\n                x_cand = (1 - alpha) * a + alpha * b\n                # tiny jitter proportional to radius\n                x_cand += self.rng.normal(0, 0.15 * radius, size=self.dim)\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True\n                    radius = min(self.max_radius, radius * 1.08)\n                    axis_w *= 0.98\n                    archive.append((f_best, x_best.copy())); archive.sort(key=lambda t: t[0]); archive = archive[:self.archive_size]\n                else:\n                    radius = max(self.min_radius, radius * 0.93)\n                    axis_w *= 0.995\n\n            # 5%: global random or opposition\n            else:\n                if self.rng.random() < 0.6:\n                    x_cand = lb + ub - x_best  # opposition\n                else:\n                    x_cand = lb + self.rng.random(self.dim) * (ub - lb)\n                x_cand = np.minimum(np.maximum(x_cand, lb), ub)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True\n                    radius = max(self.min_radius, radius * 1.15)\n                    axis_w *= 1.02\n                    archive.append((f_best, x_best.copy())); archive.sort(key=lambda t: t[0]); archive = archive[:self.archive_size]\n                else:\n                    radius = max(self.min_radius, radius * 0.95)\n                    axis_w *= 0.99\n\n            # light normalization and safety\n            axis_w = np.clip(axis_w, 1e-6, 1e6)\n            radius = float(np.clip(radius, self.min_radius, self.max_radius))\n\n            # tiny random perturbation of weights to avoid lock\n            if self.rng.random() < 0.02:\n                axis_w *= (1.0 + 0.06 * (self.rng.normal(size=self.dim)))\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 14, "feedback": "The algorithm SparseOrbitalRecenter scored 0.016 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f848c7e0-b9e3-4731-969d-4bbca80a2964"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.10007308189021402}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.055767174227076666}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.0662327489810074}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.007596314464317189}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.014428234497896875}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.0019269531642281956}], "aucs": [0.10007308189021402, 0.055767174227076666, 0.0662327489810074, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.007596314464317189, 0.014428234497896875, 0.0019269531642281956]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1897.0, "Edges": 1896.0, "Max Degree": 20.0, "Min Degree": 1.0, "Mean Degree": 1.9989457037427518, "Degree Variance": 2.1275687355864448, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.798133022170362, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3083635839638517, "Depth Entropy": 2.2232276352492413, "Assortativity": 0.0, "Average Eccentricity": 17.109646810753823, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0005271481286241434, "Average Shortest Path": 10.072624107796232, "mean_complexity": 8.333333333333334, "total_complexity": 25.0, "mean_token_count": 567.0, "total_token_count": 1701.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "b30d9b85-7770-4ac7-84c5-bc04438bc1dd", "fitness": 0.21982834898993733, "name": "SeparableLinearPushPP", "description": "SLP++ \u2014 compact Separable Linear Push with exponential-smoothing slopes, momentum-biased coordinate selection, lightweight adaptive extrapolation and budget-aware sparse 2\u2011D scans for cheap exploitation and occasional Gaussian/global restarts.", "code": "import numpy as np\n\nclass SeparableLinearPushPP:\n    def __init__(self, budget=1000, dim=10, init_step=1.0, min_step=1e-6, max_step=2.5,\n                 grow=1.25, shrink=0.6, smooth=0.35, stagn_patience=12, rng=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.init_step = float(init_step); self.min_step = float(min_step); self.max_step = float(max_step)\n        self.grow = float(grow); self.shrink = float(shrink); self.smooth = float(smooth)\n        self.stagn_patience = int(stagn_patience)\n        if rng is None:\n            self.rng = np.random.default_rng()\n        elif isinstance(rng, np.random.Generator):\n            self.rng = rng\n        else:\n            self.rng = np.random.default_rng(rng)\n\n    def __call__(self, func):\n        lb = np.full(self.dim, -5.0); ub = np.full(self.dim, 5.0)\n        # safe eval wrapper\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return None\n            v = float(func(x))\n            evals += 1\n            return v\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n\n        # init\n        x_best = clip(lb + self.rng.random(self.dim) * (ub - lb))\n        f_best = safe_eval(x_best)\n        if f_best is None: return float('inf'), x_best.copy()\n        step = np.full(self.dim, self.init_step)\n        pos_f = np.full(self.dim, np.nan); neg_f = np.full(self.dim, np.nan)\n        slope = np.zeros(self.dim)\n        last_dir = np.zeros(self.dim)  # momentum sign\n        stagn = 0\n\n        eye = np.eye(self.dim)\n        it = 0\n        while evals < self.budget:\n            it += 1\n            improved = False\n            remaining = self.budget - evals\n\n            # choose mode: coord push (70%), 2D scan (20%), jitter/restart (10%)\n            r = self.rng.random()\n            if r < 0.70:\n                # coordinate push\n                pri = np.abs(slope) * step + 1e-12\n                if np.all(pri == pri[0]) and self.rng.random() < 0.5:\n                    idx = int(self.rng.integers(self.dim))\n                else:\n                    probs = pri / pri.sum()\n                    idx = int(self.rng.choice(self.dim, p=probs))\n\n                # preferred direction: go opposite sign of slope (reduce f)\n                if slope[idx] > 0: pref = -1.0\n                elif slope[idx] < 0: pref = 1.0\n                else:\n                    # bias by last successful dir (momentum) else random\n                    if last_dir[idx] != 0 and self.rng.random() < 0.7:\n                        pref = last_dir[idx]\n                    else:\n                        pref = 1.0 if self.rng.random() < 0.5 else -1.0\n\n                delta = pref * step[idx] * eye[idx]\n                x1 = clip(x_best + delta); f1 = safe_eval(x1)\n                if f1 is None: break\n                if pref > 0: pos_f[idx] = f1\n                else: neg_f[idx] = f1\n\n                # update slope if both sides known\n                if not np.isnan(pos_f[idx]) and not np.isnan(neg_f[idx]) and (2*step[idx])>0:\n                    s_new = (pos_f[idx] - neg_f[idx]) / (2.0 * step[idx])\n                    slope[idx] = (1.0 - self.smooth) * slope[idx] + self.smooth * s_new\n\n                if f1 < f_best:\n                    # accept and try light extrapolation with momentum\n                    x_best = x1.copy(); f_best = f1; improved = True\n                    last_dir[idx] = pref\n                    # geometric extrapolation up to 3 times\n                    mult = 1.6\n                    for _ in range(3):\n                        if evals >= self.budget: break\n                        nx = clip(x_best + mult * delta)\n                        if np.allclose(nx, x_best): break\n                        fn = safe_eval(nx)\n                        if fn is None: break\n                        if pref > 0: pos_f[idx] = min(pos_f[idx], fn) if not np.isnan(pos_f[idx]) else fn\n                        else: neg_f[idx] = min(neg_f[idx], fn) if not np.isnan(neg_f[idx]) else fn\n                        if fn < f_best:\n                            x_best = nx.copy(); f_best = fn; improved = True\n                            step[idx] = min(self.max_step, step[idx] * self.grow)\n                        else:\n                            break\n                else:\n                    # try opposite side if budget allows (cheap info)\n                    if remaining > 1:\n                        x2 = clip(x_best - delta); f2 = safe_eval(x2)\n                        if f2 is None: break\n                        if (-pref) > 0: pos_f[idx] = f2\n                        else: neg_f[idx] = f2\n                        if not np.isnan(pos_f[idx]) and not np.isnan(neg_f[idx]) and (2*step[idx])>0:\n                            s_new = (pos_f[idx] - neg_f[idx]) / (2.0 * step[idx])\n                            slope[idx] = (1.0 - self.smooth) * slope[idx] + self.smooth * s_new\n                        if f2 < f_best:\n                            x_best = x2.copy(); f_best = f2; improved = True\n                            last_dir[idx] = -pref\n                    if not improved:\n                        last_dir[idx] = 0\n                        step[idx] = max(self.min_step, step[idx] * self.shrink)\n\n            elif r < 0.90:\n                # sparse 2D linear scan: pick one high-priority and one random\n                pri = np.abs(slope) * step + 1e-12\n                probs = pri / pri.sum() if pri.sum() > 0 else None\n                i = int(self.rng.choice(self.dim, p=probs) if probs is not None else self.rng.integers(self.dim))\n                j = int(self.rng.integers(self.dim - 1))\n                if j >= i: j += 1\n                si = -1.0 if slope[i] > 0 else 1.0\n                sj = -1.0 if slope[j] > 0 else 1.0\n                cand = [si*step[i]*eye[i], sj*step[j]*eye[j], si*step[i]*eye[i] + sj*step[j]*eye[j]]\n                best_local_f = f_best; best_local_x = x_best.copy()\n                for d in cand:\n                    if evals >= self.budget: break\n                    xc = clip(x_best + d); fc = safe_eval(xc)\n                    if fc is None: break\n                    # update pos/neg conservatively\n                    if d[i] > 0: pos_f[i] = fc if np.isnan(pos_f[i]) else min(pos_f[i], fc)\n                    elif d[i] < 0: neg_f[i] = fc if np.isnan(neg_f[i]) else min(neg_f[i], fc)\n                    if d[j] > 0: pos_f[j] = fc if np.isnan(pos_f[j]) else min(pos_f[j], fc)\n                    elif d[j] < 0: neg_f[j] = fc if np.isnan(neg_f[j]) else min(neg_f[j], fc)\n                    if fc < best_local_f:\n                        best_local_f, best_local_x = fc, xc.copy()\n                if best_local_f < f_best:\n                    prev = x_best.copy()\n                    x_best = best_local_x.copy(); f_best = best_local_f; improved = True\n                    # tiny combined extrapolation\n                    if evals < self.budget:\n                        comb = x_best - prev\n                        if np.linalg.norm(comb) > 1e-12:\n                            xn = clip(x_best + 1.3 * comb); fn = safe_eval(xn)\n                            if fn is not None and fn < f_best:\n                                x_best = xn.copy(); f_best = fn; improved = True\n                                step[i] = min(self.max_step, step[i] * self.grow)\n                                step[j] = min(self.max_step, step[j] * self.grow)\n                else:\n                    step[i] = max(self.min_step, step[i] * self.shrink)\n                    step[j] = max(self.min_step, step[j] * self.shrink)\n                # refresh slopes\n                for k in (i, j):\n                    if not np.isnan(pos_f[k]) and not np.isnan(neg_f[k]) and (2*step[k])>0:\n                        s_new = (pos_f[k] - neg_f[k]) / (2.0 * step[k])\n                        slope[k] = (1.0 - self.smooth) * slope[k] + self.smooth * s_new\n\n            else:\n                # diversification: jitter or stronger restart when stagnated\n                if stagn >= self.stagn_patience and remaining >= 2:\n                    scale = 0.6 * np.maximum(step, 1e-12)\n                    x1 = clip(x_best + self.rng.normal(scale=scale)); f1 = safe_eval(x1)\n                    if f1 is None: break\n                    x2 = clip(x_best + self.rng.normal(scale=1.6*scale)) if evals < self.budget else None\n                    f2 = safe_eval(x2) if x2 is not None else None\n                    cand = [(f_best, x_best), (f1, x1)]\n                    if f2 is not None: cand.append((f2, x2))\n                    cand.sort(key=lambda t: t[0])\n                    if cand[0][0] < f_best:\n                        x_best = cand[0][1].copy(); f_best = cand[0][0]; improved = True\n                    else:\n                        # modestly randomize subset of steps\n                        idxs = self.rng.choice(self.dim, size=max(1, self.dim//6), replace=False)\n                        for k in idxs: step[k] = min(self.max_step, step[k] * 1.2)\n                        # partial reset of slope memory to allow new directions\n                        sel = self.rng.choice(self.dim, size=max(1, self.dim//8), replace=False)\n                        pos_f[sel] = np.nan; neg_f[sel] = np.nan; slope[sel] = 0.0\n                else:\n                    # light jitter\n                    avg = np.mean(step)\n                    xj = clip(x_best + self.rng.normal(scale=0.35*avg, size=self.dim))\n                    fj = safe_eval(xj)\n                    if fj is None: break\n                    if fj < f_best:\n                        x_best = xj.copy(); f_best = fj; improved = True\n                    else:\n                        step = np.maximum(self.min_step, step * 0.97)\n\n            # adapt global bookkeeping\n            if improved:\n                stagn = 0\n                # modest growth on success (keeps exploration)\n                step = np.minimum(self.max_step, step * (1.0 + 0.06))\n            else:\n                stagn += 1\n                step = np.maximum(self.min_step, step * (1.0 - 0.04))\n\n            step = np.clip(step, self.min_step, self.max_step)\n\n            # periodic slope refresh from stored pos/neg\n            for k in range(self.dim):\n                if not np.isnan(pos_f[k]) and not np.isnan(neg_f[k]) and (2*step[k])>0:\n                    s_new = (pos_f[k] - neg_f[k]) / (2.0 * step[k])\n                    slope[k] = (1.0 - self.smooth) * slope[k] + self.smooth * s_new\n\n            # rare global restart if seriously stuck\n            if stagn >= 4 * self.stagn_patience and evals < self.budget:\n                if self.rng.random() < 0.5:\n                    xr = clip(lb + self.rng.random(self.dim) * (ub - lb))\n                else:\n                    center = 0.5 * (lb + ub); rng_scale = 0.5 * (ub - lb)\n                    xr = clip(center + self.rng.normal(scale=rng_scale))\n                fr = safe_eval(xr)\n                if fr is None: break\n                if fr < f_best:\n                    x_best = xr.copy(); f_best = fr\n                    pos_f[:] = np.nan; neg_f[:] = np.nan; slope[:] = 0.0\n                    step[:] = np.minimum(self.max_step, self.init_step)\n                    stagn = 0\n                else:\n                    step = np.minimum(self.max_step, step * 1.3)\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 14, "feedback": "The algorithm SeparableLinearPushPP scored 0.220 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5951c2b-b9ff-4352-bb47-76f19aa885c0"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.7259942951671481}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7740058002696056}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7642698453427827}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.04954351747850272}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.18451140822549428}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06042506864495356}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.04851913086372206}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.04398785478618361}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0385212268263172}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.2212795948202848}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.032773149132887536}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.353444343291178}], "aucs": [0.7259942951671481, 0.7740058002696056, 0.7642698453427827, 0.04954351747850272, 0.18451140822549428, 4.999999999999449e-05, 0.06042506864495356, 4.999999999999449e-05, 4.999999999999449e-05, 0.04851913086372206, 0.04398785478618361, 0.0385212268263172, 0.2212795948202848, 0.032773149132887536, 0.353444343291178]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2875.0, "Edges": 2874.0, "Max Degree": 22.0, "Min Degree": 1.0, "Mean Degree": 1.999304347826087, "Degree Variance": 1.8281734291115312, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 9.752252252252251, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3175011965276362, "Depth Entropy": 2.2557095094736486, "Assortativity": 0.0, "Average Eccentricity": 17.768, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00034782608695652176, "Average Shortest Path": 11.318524220144624, "mean_complexity": 28.333333333333332, "total_complexity": 85.0, "mean_token_count": 830.0, "total_token_count": 2490.0, "mean_parameter_count": 4.666666666666667, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "6d5a5531-3f76-4ec9-9270-7f4a002d1ff0", "fitness": 0.428431741809147, "name": "CompactOrthonormalPullSearch", "description": "Compact Orthonormal Pull Search (COPS) \u2014 maintain a tiny bank of orthonormal search directions built from recent successful displacements and perform strictly single-direction pulls (one eval per probe) with adaptive per-direction step sizes, success-driven direction re-learning and cheap axis respawns for diversification.", "code": "import numpy as np\n\nclass CompactOrthonormalPullSearch:\n    \"\"\"\n    COPS: single-direction pulls along a compact orthonormal bank learned from recent successes.\n    All atomic probes move along exactly one stored direction vector (degree-1 in the direction basis).\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, k=None, init_step=0.08,\n                 success_mult=1.4, failure_mult=0.6, rebuild_every=6,\n                 respawn_prob=0.12, heavy_prob=0.02, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.k = int(k) if k is not None else max(2, min(8, self.dim))\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.rebuild_every = int(rebuild_every)\n        self.respawn_prob = float(respawn_prob)\n        self.heavy_prob = float(heavy_prob)\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n        evals = 0\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            v = float(func(x))\n            evals += 1\n            return v\n\n        # init incumbent\n        x = rng.uniform(lb, ub)\n        f = safe_eval(x)\n\n        # init direction bank D (k x dim), start with some axes then random orthonormalize\n        k = min(self.k, dim)\n        D = np.zeros((k, dim))\n        # first few axes\n        for i in range(min(k, dim)):\n            e = np.zeros(dim); e[i] = 1.0\n            D[i] = e\n        if k > dim:\n            D = D[:dim].copy()\n            k = dim\n        if k < self.k:\n            # pad with random orthonormal vectors\n            extra = self.k - k\n            R = rng.standard_normal((extra, dim))\n            # orthonormalize with QR of stacked matrix (existing + R)\n            M = np.vstack((D[:k], R))\n            Q, _ = np.linalg.qr(M.T)\n            D = Q.T[:self.k]\n            k = self.k\n\n        # normalize and ensure shape\n        def normalize_rows(A):\n            n = np.linalg.norm(A, axis=1, keepdims=True)\n            n[n == 0] = 1.0\n            return A / n\n\n        D = normalize_rows(D)\n\n        steps = np.full(D.shape[0], base_step)\n        momentum = np.zeros(D.shape[0])   # prefer sign per direction\n        recency = np.zeros(D.shape[0], dtype=int)\n        recent_disp = []  # store recent successful displacement vectors\n        successes = 0\n\n        # main loop: single-direction probes\n        while evals < self.budget:\n            # occasional heavy Cauchy single-direction probe from incumbent\n            if rng.random() < self.heavy_prob and evals < self.budget:\n                i = rng.integers(0, D.shape[0])\n                u = rng.random(); cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.18 * box_len * cauchy\n                x_try = x + jump * D[i]\n                x_try = np.clip(x_try, lb, ub)\n                f_try = safe_eval(x_try)\n                if f_try < f:\n                    x, f = x_try, f_try\n                    # incorporate displacement\n                    recent_disp.append(x - (x - jump * D[i]))  # equal to jump * D[i]\n                    successes += 1\n\n            # choose direction by weighted priority (step * (1 + recency*0.3))\n            priority = steps * (1.0 + 0.3 * recency) + 1e-12 * rng.random(steps.size)\n            prob = priority / np.sum(priority)\n            i = rng.choice(steps.size, p=prob)\n\n            # sign biased by momentum\n            m = momentum[i]\n            if rng.random() < 0.5 + 0.45 * np.clip(abs(m), 0, 1):\n                sign = 1 if m >= 0 else -1\n            else:\n                sign = 1 if rng.random() < 0.5 else -1\n\n            delta = steps[i]\n            if delta <= 1e-12 * box_len:\n                # tiny, respawn this direction occasionally\n                recency[i] = max(0, recency[i] - 1)\n                if rng.random() < self.respawn_prob:\n                    v = rng.standard_normal(dim)\n                    # make orthogonal-ish to existing bank\n                    for j in range(D.shape[0]):\n                        v -= np.dot(v, D[j]) * D[j]\n                    if np.linalg.norm(v) < 1e-12:\n                        v = rng.standard_normal(dim)\n                    v /= np.linalg.norm(v)\n                    D[i] = v\n                    steps[i] = base_step\n                    momentum[i] = 0.0\n                else:\n                    steps[i] *= self.success_mult  # nudge upwards\n                continue\n\n            x_try = x + sign * delta * D[i]\n            x_try = np.clip(x_try, lb, ub)\n            f_try = safe_eval(x_try)\n\n            if f_try < f:\n                # accept\n                disp = x_try - x\n                x, f = x_try.copy(), f_try\n                steps[i] = min(steps[i] * self.success_mult, box_len)\n                momentum[i] = 0.8 * momentum[i] + 0.2 * sign\n                recency[i] += 1\n                recent_disp.append(disp)\n                successes += 1\n            else:\n                # fail\n                steps[i] = max(steps[i] * self.failure_mult, 1e-12 * box_len)\n                momentum[i] *= 0.85\n                recency[i] = max(0, recency[i] - 1)\n\n            # rebuild direction bank from recent successes periodically to learn good directions\n            if successes >= self.rebuild_every and len(recent_disp) >= min(self.rebuild_every, dim):\n                V = np.vstack(recent_disp[-(2 * self.k + 1):])  # recent displacement rows\n                # SVD to get principal directions\n                try:\n                    _, _, Vt = np.linalg.svd(V, full_matrices=False)\n                    newD = Vt[:self.k]\n                    newD = normalize_rows(newD)\n                    # replace worst-performing directions probabilistically\n                    worst_idx = np.argsort(recency)[:max(1, self.k // 3)]\n                    for j, row in enumerate(newD):\n                        tgt = worst_idx[j % worst_idx.size]\n                        D[tgt] = row\n                        steps[tgt] = max(steps[tgt], base_step * 0.6)\n                        momentum[tgt] = 0.0\n                except Exception:\n                    # fallback: small random perturbations\n                    for j in range(D.shape[0]):\n                        if rng.random() < 0.2:\n                            v = rng.standard_normal(dim)\n                            for q in range(D.shape[0]):\n                                v -= np.dot(v, D[q]) * D[q]\n                            if np.linalg.norm(v) > 0:\n                                D[j] = v / np.linalg.norm(v)\n                successes = 0\n                recent_disp = recent_disp[-(4 * self.k + 1):]\n\n            # occasional axis sniff (diversify with an axis)\n            if rng.random() < self.respawn_prob and evals < self.budget:\n                j = rng.integers(0, D.shape[0])\n                e = np.zeros(dim); e[rng.integers(0, dim)] = 1.0\n                D[j] = e\n                steps[j] = base_step\n                momentum[j] = 0.0\n\n            # keep steps bounded\n            mn = 1e-11 * box_len; mx = 1.0 * box_len\n            steps = np.clip(steps, mn, mx)\n\n            # low-budget stop-to-refine\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final single-coordinate refinement in axis space (cheap)\n        mesh = 0.08 * box_len\n        mesh_min = 1e-8 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget:\n                    break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f = xt, ft; improved = True; continue\n                xt = x.copy(); xt[d] = np.clip(xt[d] - mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f = xt, ft; improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.x_opt = np.asarray(x, dtype=float)\n        self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 14, "feedback": "The algorithm CompactOrthonormalPullSearch scored 0.428 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6cd3ea29-a941-4770-ab4b-0e5e0cca3077"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.6836408429631178}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.6562124818468422}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.6793923410471819}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.2172128983077548}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.19264084962034345}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.15752395095075067}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.15290204873362456}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1661778156705006}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.17345091280188896}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.11229420792532707}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.10877080120618265}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.14974628300578952}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9963021823159368}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9860654963017058}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9941430144402577}], "aucs": [0.6836408429631178, 0.6562124818468422, 0.6793923410471819, 0.2172128983077548, 0.19264084962034345, 0.15752395095075067, 0.15290204873362456, 0.1661778156705006, 0.17345091280188896, 0.11229420792532707, 0.10877080120618265, 0.14974628300578952, 0.9963021823159368, 0.9860654963017058, 0.9941430144402577]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2100.0, "Edges": 2099.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.999047619047619, "Degree Variance": 2.1257133786848073, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 7.841995841995842, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.323958094898701, "Depth Entropy": 2.0706138254160087, "Assortativity": 0.0, "Average Eccentricity": 17.947619047619046, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0004761904761904762, "Average Shortest Path": 10.009039225027792, "mean_complexity": 9.0, "total_complexity": 45.0, "mean_token_count": 359.0, "total_token_count": 1795.0, "mean_parameter_count": 3.4, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "77249296-42be-468b-a3a8-60f1c387f192", "fitness": 0.5176589873309172, "name": "ThompsonCoordinateBandit", "description": "Thompson-Coordinate Bandit v2 \u2014 per-coordinate Thompson sampling over signed moves, but updates priors proportionally to improvement magnitude, adds light momentum and headroom-aware scoring, and uses rarer heavy-tail global probes; strictly single-eval moves and budget-safe.", "code": "import numpy as np\n\nclass ThompsonCoordinateBandit:\n    \"\"\"\n    Compact improved TCB:\n    - Single-eval moves only.\n    - Per-dim Beta priors for +/-, update magnitude-weighted on success.\n    - Headroom-aware scoring, light momentum, adaptive multiplicative steps.\n    - Occasional heavy-tail global probes. Budget-safe.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 success_mult=1.25, failure_mult=0.65,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.02, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac)\n        self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob)\n\n    def __call__(self, func):\n        dim = self.dim\n        rng = self.rng\n\n        # bounds\n        lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n        min_step = max(1e-15, self.min_step_frac * box_len)\n        max_step = max(1e-12, self.max_step_frac * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            fv = float(func(x))\n            evals += 1\n            return fv\n\n        # small randomized initialization (use a few samples if budget allows)\n        x_best = rng.uniform(lb, ub)\n        f_best = safe_eval(x_best)\n        for _ in range(min(4, max(0, self.budget//20))):\n            if evals >= self.budget: break\n            x = rng.uniform(lb, ub)\n            f = safe_eval(x)\n            if f < f_best:\n                x_best, f_best = x.copy(), f\n\n        # per-dim state\n        steps = np.full(dim, base_step)\n        a_pos = np.ones(dim); b_pos = np.ones(dim)\n        a_neg = np.ones(dim); b_neg = np.ones(dim)\n        last_dir = np.zeros(dim)    # momentum in [-1,1]\n        consec_fail = np.zeros(dim, int)\n        trials = np.zeros(dim, int)\n\n        while evals < self.budget:\n            # occasional heavy-tail global probe\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim)\n                c = np.tan(np.pi * (u - 0.5))\n                jump = 0.12 * box_len * c\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                # continue single-eval loop\n\n            # sample Beta for each sign\n            sp = rng.beta(a_pos, b_pos)\n            sn = rng.beta(a_neg, b_neg)\n\n            # headroom penalization (0..1)\n            dist = (x_best - lb) / (ub - lb + 1e-18)\n            head_neg = np.clip(dist, 0.0, 1.0)\n            head_pos = np.clip(1.0 - dist, 0.0, 1.0)\n\n            # score combining sampled strength, headroom, step and momentum bias\n            momentum = 1.0 + 0.4 * last_dir * 1.0  # favors previous successful direction\n            score_pos = sp * head_pos * steps * momentum\n            score_neg = sn * head_neg * steps * (-momentum)  # negative sign encodes direction effect\n            # pick per-dim best sign\n            choose_pos = score_pos >= np.abs(score_neg)\n            signed_score = np.where(choose_pos, score_pos, np.abs(score_neg))\n            d = int(np.argmax(signed_score + 1e-14 * rng.random(dim)))  # tie-breaker\n            sign = 1 if choose_pos[d] else -1\n\n            # propose single-dim move\n            stepd = float(np.clip(steps[d], min_step, max_step))\n            xt = x_best.copy()\n            xt[d] = np.clip(xt[d] + sign * stepd, lb[d], ub[d])\n            ft = safe_eval(xt)\n\n            trials[d] += 1\n            # improvement magnitude (relative)\n            improv = max(0.0, (f_best - ft) / (abs(f_best) + 1.0))\n            if ft < f_best:\n                # reward priors proportional to improvement magnitude (stronger update on bigger gains)\n                weight = 1.0 + 6.0 * improv\n                if sign > 0:\n                    a_pos[d] += weight\n                else:\n                    a_neg[d] += weight\n                # boost step and momentum toward successful sign\n                steps[d] = min(max_step, steps[d] * self.success_mult)\n                last_dir[d] = 0.6 * last_dir[d] + 0.4 * sign\n                consec_fail[d] = 0\n                x_best, f_best = xt.copy(), float(ft)\n            else:\n                # failure: modest penalty to that sign\n                if sign > 0:\n                    b_pos[d] += 0.9\n                else:\n                    b_neg[d] += 0.9\n                steps[d] = max(min_step, steps[d] * self.failure_mult)\n                last_dir[d] *= 0.7\n                consec_fail[d] += 1\n\n            # conservative extra shrink if repeated failures\n            if consec_fail[d] >= 7:\n                steps[d] = max(min_step, steps[d] * (self.failure_mult ** 1.5))\n                consec_fail[d] = 0\n                # nudge opposite sign a bit (encourage switch)\n                if sign > 0:\n                    b_pos[d] += 0.5\n                else:\n                    b_neg[d] += 0.5\n\n            # mild decay of priors to avoid numerical blowup\n            if trials[d] > 50:\n                decay = 0.85\n                a_pos[d] = 1.0 + (a_pos[d] - 1.0) * decay\n                b_pos[d] = 1.0 + (b_pos[d] - 1.0) * decay\n                a_neg[d] = 1.0 + (a_neg[d] - 1.0) * decay\n                b_neg[d] = 1.0 + (b_neg[d] - 1.0) * decay\n                trials[d] = int(trials[d] * 0.6)\n\n        # final tiny polish: small per-dim one-sided tries while budget remains\n        mesh = 0.05 * box_len\n        while evals < self.budget and mesh > 1e-12 * box_len:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 14, "feedback": "The algorithm ThompsonCoordinateBandit scored 0.518 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2ce99025-003a-46b2-bec0-52c377bfc061"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9794841350001888}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.96873969787464}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9777867488403487}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9478289392988057}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9640947894581609}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.96266413439308}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0997074649022055}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1296181901522676}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.04947155753359167}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0918128080161652}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.09181731231553292}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.016284108327823565}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.6230060507935862}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.18660504519840138}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.6759638278589606}], "aucs": [0.9794841350001888, 0.96873969787464, 0.9777867488403487, 0.9478289392988057, 0.9640947894581609, 0.96266413439308, 0.0997074649022055, 0.1296181901522676, 0.04947155753359167, 0.0918128080161652, 0.09181731231553292, 0.016284108327823565, 0.6230060507935862, 0.18660504519840138, 0.6759638278589606]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1533.0, "Edges": 1532.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9986953685583821, "Degree Variance": 2.4083479391631553, "Transitivity": 0.0, "Max Depth": 12.0, "Min Depth": 2.0, "Mean Depth": 7.321178120617111, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.311823580443923, "Depth Entropy": 1.8367001289187348, "Assortativity": 0.0, "Average Eccentricity": 14.272667971298109, "Diameter": 19.0, "Radius": 10.0, "Edge Density": 0.0006523157208088715, "Average Shortest Path": 8.84485275207404, "mean_complexity": 8.666666666666666, "total_complexity": 26.0, "mean_token_count": 441.3333333333333, "total_token_count": 1324.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "5a73bbe4-abab-43b1-aedc-5cd165d81328", "fitness": 0.4809279066872818, "name": "DirectionalTrustRegionBandit", "description": "Directional Trust-Region Bandit (DTRB) \u2014 fit tiny local linear models from a rolling memory to estimate a descent direction (cheap directional trust-region steps), complemented by sparse coordinate bandit pushes and orthogonal escapes; adapt per-dimension trust radii by simple success/failure multipliers.", "code": "import numpy as np\n\nclass DirectionalTrustRegionBandit:\n    \"\"\"\n    Directional Trust-Region Bandit (DTRB)\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, mem_size=None, init_step=None,\n                 min_step=1e-6, max_step=5.0, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.mem_size = mem_size if mem_size is not None else min(2 * self.dim + 3, 40)\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def __call__(self, func):\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n\n        evals = 0\n\n        # init\n        x_best = lb + self.rng.random(self.dim) * (ub - lb)\n        f_best = float(func(x_best)); evals += 1\n\n        # per-dim trust radii\n        if self.init_step is None:\n            step = np.full(self.dim, ((ub - lb).mean() / 6.0))\n        else:\n            s = np.asarray(self.init_step, dtype=float)\n            step = np.full(self.dim, float(s) if s.shape == () else np.mean(s))\n            step = np.full(self.dim, step) if step.shape == () else step\n        step = np.clip(step, self.min_step, self.max_step)\n\n        # bookkeeping\n        mem_x = [x_best.copy()]\n        mem_f = [f_best]\n        axis_score = np.ones(self.dim)\n        up = 1.35; down = 0.7\n        stagn = 0\n\n        # helper: fit local linear model f \u2248 a + g^T (x - x_best)\n        def estimate_grad():\n            if len(mem_x) < min(self.dim + 1, 4):\n                return None\n            X = np.vstack(mem_x) - x_best  # (n,dim)\n            y = np.array(mem_f) - f_best   # (n,)\n            # small regularization for stability\n            try:\n                g, *_ = np.linalg.lstsq(X, y, rcond=None)\n            except Exception:\n                return None\n            if np.allclose(g, 0):\n                return None\n            return g\n\n        while evals < self.budget:\n            remaining = self.budget - evals\n            r = self.rng.random()\n            improved = False\n\n            if r < 0.50:\n                # directional trust-region step using estimated gradient\n                g = estimate_grad()\n                if g is None:\n                    # fallback: coordinate push\n                    r = 0.55\n                else:\n                    # propose step opposite gradient, scaled so per-dim move ~ step\n                    gn = g.copy()\n                    # normalize using L2 to avoid huge moves\n                    gn_norm = np.linalg.norm(gn)\n                    if gn_norm == 0:\n                        # degenerate -> fallback\n                        r = 0.55\n                    else:\n                        direction = -gn / gn_norm\n                        # scale so that abs(move) ~ step on average\n                        avg_step = step.mean()\n                        move = direction * avg_step * (1.0 + 0.2 * self.rng.standard_normal())\n                        x_cand = np.clip(x_best + move, lb, ub)\n                        f_cand = float(func(x_cand)); evals += 1\n                        if f_cand < f_best:\n                            f_best = f_cand; x_best = x_cand; improved = True; stagn = 0\n                            # reward dims moved\n                            rel = np.abs(move) / (np.abs(move).mean() + 1e-12)\n                            step = np.minimum(self.max_step, step * (1.0 + 0.18 * rel))\n                            axis_score *= 0.95\n                            axis_score += 0.2 * rel\n                            # push memory\n                            mem_x.append(x_best.copy()); mem_f.append(f_best)\n                        else:\n                            # shrink trust radii along gradient components\n                            shrink = (np.abs(direction) > 1e-8)\n                            step[shrink] = np.maximum(self.min_step, step[shrink] * down)\n                            axis_score *= 0.9\n                            stagn += 1\n\n            if 0.50 <= r < 0.85:\n                # sparse coordinate bandit push (single-eval)\n                probs = axis_score / axis_score.sum()\n                idx = self.rng.choice(self.dim, p=probs)\n                sgn = 1 if self.rng.random() < 0.5 else -1\n                move = np.zeros(self.dim); move[idx] = sgn * step[idx] * (1.0 + 0.15 * self.rng.standard_normal())\n                x_cand = np.clip(x_best + move, lb, ub)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True; stagn = 0\n                    step[idx] = min(self.max_step, step[idx] * up)\n                    axis_score[idx] *= 1.4\n                    mem_x.append(x_best.copy()); mem_f.append(f_best)\n                else:\n                    step[idx] = max(self.min_step, step[idx] * down)\n                    axis_score[idx] *= 0.6\n                    stagn += 1\n\n            if 0.85 <= r < 0.95:\n                # orthogonal escape: sample a random vector and project to be orthogonal to estimated grad (if available)\n                g = estimate_grad()\n                v = self.rng.normal(size=self.dim)\n                if g is not None and np.linalg.norm(g) > 1e-12:\n                    g_unit = g / (np.linalg.norm(g) + 1e-12)\n                    v = v - g_unit * (v @ g_unit)  # orthogonal component\n                    if np.linalg.norm(v) < 1e-12:\n                        v = self.rng.normal(size=self.dim)\n                # scale to average step\n                v = v / (np.linalg.norm(v) + 1e-12) * step.mean() * (0.8 + 0.6 * self.rng.random())\n                x_cand = np.clip(x_best + v, lb, ub)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True; stagn = 0\n                    step *= 1.08\n                    axis_score *= 0.95\n                    mem_x.append(x_best.copy()); mem_f.append(f_best)\n                else:\n                    step *= 0.98\n                    stagn += 1\n\n            if r >= 0.95:\n                # anchor blend or random global\n                if self.rng.random() < 0.6 and len(mem_x) > 1:\n                    # blend best with a random recent memory\n                    j = self.rng.integers(len(mem_x))\n                    base = mem_x[j]\n                    alpha = self.rng.normal(loc=0.5, scale=0.25)\n                    x_cand = np.clip((1 - alpha) * x_best + alpha * base, lb, ub)\n                else:\n                    x_cand = lb + self.rng.random(self.dim) * (ub - lb)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True; stagn = 0\n                    step = np.minimum(self.max_step, step * 1.12)\n                    axis_score *= 0.9\n                    mem_x.append(x_best.copy()); mem_f.append(f_best)\n                else:\n                    stagn += 1\n\n            # maintain memory size\n            if len(mem_x) > self.mem_size:\n                # keep best and some recent\n                idxs = np.argsort(mem_f)[:max(1, min(3, len(mem_x)//5))]\n                keep = set(idxs.tolist())\n                # also keep last ones\n                for i in range(len(mem_x)-10, len(mem_x)):\n                    if i >= 0:\n                        keep.add(i)\n                new_mem_x = [mem_x[i] for i in sorted(keep)]\n                new_mem_f = [mem_f[i] for i in sorted(keep)]\n                # if still too big trim worst\n                if len(new_mem_x) > self.mem_size:\n                    ords = np.argsort(new_mem_f)[:self.mem_size]\n                    new_mem_x = [new_mem_x[i] for i in ords]\n                    new_mem_f = [new_mem_f[i] for i in ords]\n                mem_x = new_mem_x; mem_f = new_mem_f\n\n            # push latest to memory if improved or occasionally to enrich sample diversity\n            if improved or (self.rng.random() < 0.03 and evals < self.budget):\n                mem_x.append(x_best.copy()); mem_f.append(f_best)\n\n            # enforce bounds on step and axis_score\n            step = np.clip(step, self.min_step, self.max_step)\n            axis_score = np.clip(axis_score, 1e-6, 1e6)\n\n            # stagnation-driven small random restart\n            if stagn > max(10, self.dim) and evals < self.budget:\n                tries = min(3, self.budget - evals)\n                found = False\n                for _ in range(tries):\n                    xr = lb + self.rng.random(self.dim) * (ub - lb)\n                    fr = float(func(xr)); evals += 1\n                    if fr < f_best:\n                        f_best, x_best = fr, xr.copy()\n                        found = True; stagn = 0\n                        step = np.clip(step * 1.08, self.min_step, self.max_step)\n                        mem_x.append(x_best.copy()); mem_f.append(f_best)\n                        break\n                if not found:\n                    # gentle enlarge trust radii to escape\n                    step = np.clip(step * 1.2, self.min_step, self.max_step)\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 15, "feedback": "The algorithm DirectionalTrustRegionBandit scored 0.481 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f848c7e0-b9e3-4731-969d-4bbca80a2964"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.899749365476964}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9109644114045677}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9179681520897951}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.6249250969968474}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.3766797043883754}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.4586456809721027}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.008911110185277571}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.014441427108852989}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0003941177243059757}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.009119285708565794}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9974395388060816}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9974543192085717}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9971263902389179}], "aucs": [0.899749365476964, 0.9109644114045677, 0.9179681520897951, 0.6249250969968474, 0.3766797043883754, 0.4586456809721027, 0.008911110185277571, 0.014441427108852989, 0.0003941177243059757, 4.999999999999449e-05, 0.009119285708565794, 4.999999999999449e-05, 0.9974395388060816, 0.9974543192085717, 0.9971263902389179]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1949.0, "Edges": 1948.0, "Max Degree": 19.0, "Min Degree": 1.0, "Mean Degree": 1.9989738327347357, "Degree Variance": 1.8860943805364132, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.4920814479638, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3331086081734185, "Depth Entropy": 2.1202881782994285, "Assortativity": 0.0, "Average Eccentricity": 18.116983068240124, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.000513083632632119, "Average Shortest Path": 10.360461796340566, "mean_complexity": 14.666666666666666, "total_complexity": 44.0, "mean_token_count": 583.3333333333334, "total_token_count": 1750.0, "mean_parameter_count": 3.3333333333333335, "total_parameter_count": 10.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "45fea7f0-0303-4bfd-b609-a20e785c492d", "fitness": 0.23621489042585828, "name": "MomentumWeightedSingleCoordinateSearch", "description": "Momentum-Weighted Single-Coordinate Pulls \u2014 strictly degree-1 moves that learn global per-dimension success statistics and anchor-local momentum, combine single-coordinate pulls toward promising medians/anchors, and use compact, informed respawns and occasional heavy Cauchy one-dim jumps for robust exploration.", "code": "import numpy as np\n\nclass MomentumWeightedSingleCoordinateSearch:\n    \"\"\"\n    Strict single-coordinate optimizer with global per-dim success EMA,\n    anchor-local momentum & adaptive steps, single-coordinate pulls toward\n    medians, informed respawns and rare heavy Cauchy one-dim jumps.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.12, success_mult=1.4, failure_mult=0.6,\n                 heavy_prob=0.03, respawn_prob=0.15, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        if pop_size is None:\n            self.pop_size = max(2, min(6, 3 + self.dim // 6))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.heavy_prob = float(heavy_prob)\n        self.respawn_prob = float(respawn_prob)\n\n    def __call__(self, func):\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n\n        evals = 0\n        rng = self.rng\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x))\n            evals += 1\n            return v\n\n        # --- init: one base then single-coordinate variants to keep initial moves degree-1 ---\n        anchors = np.zeros((self.pop_size, self.dim), float)\n        fvals = np.full(self.pop_size, np.inf)\n        base = rng.uniform(lb, ub)\n        anchors[0] = base.copy()\n        fvals[0] = safe_eval(anchors[0])\n        for i in range(1, self.pop_size):\n            if evals >= self.budget:\n                anchors[i] = anchors[0].copy()\n                fvals[i] = fvals[0]\n                continue\n            a = anchors[0].copy()\n            c = rng.integers(0, self.dim)\n            a[c] = rng.uniform(lb[c], ub[c])\n            anchors[i] = a\n            fvals[i] = safe_eval(a)\n\n        # per-anchor local multipliers and momentum, global per-dim success EMA & step baseline\n        steps = np.full((self.pop_size, self.dim), base_step)\n        momentum = np.zeros((self.pop_size, self.dim), float)\n        stagn = np.zeros(self.pop_size, int)\n        global_success = np.full(self.dim, 0.5)  # EMA [0,1], higher => prefer coord\n        ema_alpha = 0.18\n\n        # best\n        ib = int(np.nanargmin(fvals))\n        x_best = anchors[ib].copy()\n        f_best = float(fvals[ib])\n\n        # main loop (only single-dim changes)\n        while evals < self.budget:\n            # occasional heavy one-dim Cauchy jump from best\n            if rng.random() < self.heavy_prob and evals < self.budget:\n                c = rng.integers(0, self.dim)\n                u = rng.random()\n                cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.2 * box_len * cauchy\n                xj = x_best.copy()\n                xj[c] = np.clip(x_best[c] + jump, lb[c], ub[c])\n                fj = safe_eval(xj)\n                if fj < f_best:\n                    # replace worst anchor\n                    iw = int(np.nanargmax(fvals))\n                    anchors[iw] = xj.copy()\n                    fvals[iw] = fj\n                    steps[iw, :] = base_step\n                    momentum[iw, :] = 0.0\n                    f_best, x_best = fj, xj.copy()\n\n            # select an anchor biased to worse anchors (they need attention)\n            weights_a = fvals - f_best\n            if np.all(np.isfinite(weights_a)):\n                wsum = weights_a.sum()\n                if wsum <= 0 or not np.isfinite(wsum):\n                    probs_a = np.repeat(1.0 / self.pop_size, self.pop_size)\n                else:\n                    probs_a = np.clip(weights_a / wsum, 0, None)\n                    s = probs_a.sum()\n                    if s <= 0: probs_a[:] = 1.0 / self.pop_size\n                    else: probs_a /= s\n            else:\n                probs_a = np.repeat(1.0 / self.pop_size, self.pop_size)\n            # pick one anchor and perform a sequence of single-dim trials (cheap loop)\n            idx = rng.choice(self.pop_size, p=probs_a)\n            x0 = anchors[idx].copy()\n            f0 = fvals[idx]\n            improved_anchor = False\n\n            # coordinate priority: combine global_success EMA, local step size, momentum magnitude, and population diversity\n            diversity = np.std(anchors, axis=0)\n            priority = global_success * (steps[idx] / (base_step + 1e-18)) * (1.0 + 0.7 * np.abs(momentum[idx])) + 0.6 * (diversity / (np.ptp(anchors, axis=0).max() + 1e-12))\n            if not np.any(priority > 0):\n                probc = np.repeat(1.0 / self.dim, self.dim)\n            else:\n                probc = priority.clip(min=0)\n                s = probc.sum()\n                if s <= 0: probc[:] = 1.0 / self.dim\n                else: probc /= s\n\n            c = rng.choice(self.dim, p=probc)\n            m = momentum[idx, c]\n            pref = 1 if m >= 0 else -1\n            if rng.random() < (0.5 + 0.4 * min(1.0, abs(m))):\n                sign = pref\n            else:\n                sign = 1 if rng.random() < 0.5 else -1\n\n            # sometimes do a single-coordinate pull toward the population median (degree-1)\n            if rng.random() < 0.18:\n                target = np.median(anchors[:, c])\n                # move x0[c] fractionally toward target but only change that coordinate\n                frac = 0.35 * (1.0 + global_success[c])  # more pull if success is high\n                x_try = x0.copy()\n                x_try[c] = np.clip(x0[c] + frac * (target - x0[c]), lb[c], ub[c])\n            else:\n                delta = steps[idx, c]\n                if delta <= 1e-14 * box_len:\n                    # tiny step -> treat as failure/no move\n                    x_try = x0.copy()\n                    x_try[c] = x_try[c]\n                else:\n                    x_try = x0.copy()\n                    x_try[c] = np.clip(x0[c] + sign * delta, lb[c], ub[c])\n\n            f_try = safe_eval(x_try)\n            success = f_try < f0\n\n            # update anchor\n            if success:\n                anchors[idx] = x_try.copy()\n                fvals[idx] = f_try\n                steps[idx, c] = min(steps[idx, c] * self.success_mult, box_len)\n                momentum[idx, c] = np.clip(0.7 * momentum[idx, c] + 0.3 * float(sign), -1.0, 1.0)\n                stagn[idx] = 0\n            else:\n                # mirrored retry: try opposite sign once with reduced step\n                if x_try[c] != x0[c] and evals < self.budget:\n                    x_alt = x0.copy()\n                    x_alt[c] = np.clip(x0[c] - sign * 0.6 * steps[idx, c], lb[c], ub[c])\n                    f_alt = safe_eval(x_alt)\n                    if f_alt < f_try:\n                        # accept alternative if better\n                        anchors[idx] = x_alt.copy()\n                        fvals[idx] = f_alt\n                        steps[idx, c] = min(steps[idx, c] * self.success_mult, box_len)\n                        momentum[idx, c] = np.clip(0.6 * momentum[idx, c] + 0.4 * float(-sign), -1.0, 1.0)\n                        stagn[idx] = 0\n                        success = True\n                        f_try = f_alt\n                    else:\n                        # both failed\n                        steps[idx, c] = max(steps[idx, c] * self.failure_mult, 1e-14 * box_len)\n                        momentum[idx, c] *= 0.9\n                        stagn[idx] += 1\n                else:\n                    steps[idx, c] = max(steps[idx, c] * self.failure_mult, 1e-14 * box_len)\n                    momentum[idx, c] *= 0.9\n                    stagn[idx] += 1\n\n            # update global success EMA for coordinate c\n            global_success[c] = (1 - ema_alpha) * global_success[c] + ema_alpha * (1.0 if success else 0.0)\n\n            # global best update\n            if fvals[idx] < f_best:\n                f_best = float(fvals[idx])\n                x_best = anchors[idx].copy()\n\n            # stagnation respawn: replace one coordinate guided by best and population spread\n            if stagn[idx] > max(6, 3 * self.dim) and evals < self.budget:\n                stagn[idx] = 0\n                rc = rng.integers(0, self.dim)\n                if rng.random() < self.respawn_prob:\n                    # sample near best using spread of anchors\n                    spread = max(1e-6, np.std(anchors[:, rc]))\n                    v = x_best[rc] + spread * rng.standard_normal()\n                    v = np.clip(v, lb[rc], ub[rc])\n                else:\n                    # uniform respawn on that coordinate\n                    v = rng.uniform(lb[rc], ub[rc])\n                newp = anchors[idx].copy()\n                newp[rc] = v\n                f_new = safe_eval(newp)\n                anchors[idx] = newp\n                fvals[idx] = f_new\n                steps[idx, rc] = base_step\n                momentum[idx, rc] = 0.0\n                if f_new < f_best:\n                    f_best = f_new; x_best = newp.copy()\n\n            # step bounds and light global cooling\n            steps = np.clip(steps, 1e-16 * box_len, box_len)\n            # quick early-exit if budget nearly exhausted\n            if self.budget - evals < max(6, self.dim):\n                break\n\n        # final single-coordinate refinement around best with progressive mesh\n        mesh = 0.08 * box_len\n        mesh_min = 1e-9 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(self.dim):\n                if evals >= self.budget:\n                    break\n                xp = x_best.copy(); xn = x_best.copy()\n                xp[d] = np.clip(xp[d] + mesh, lb[d], ub[d])\n                fp = safe_eval(xp)\n                if fp < f_best:\n                    f_best = fp; x_best = xp.copy(); improved = True; continue\n                xn[d] = np.clip(xn[d] - mesh, lb[d], ub[d])\n                fn = safe_eval(xn)\n                if fn < f_best:\n                    f_best = fn; x_best = xn.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.x_opt = np.asarray(x_best, float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 15, "feedback": "The algorithm MomentumWeightedSingleCoordinateSearch scored 0.236 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6cd3ea29-a941-4770-ab4b-0e5e0cca3077"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.16723313617973945}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.15186716750679474}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.1529841874264526}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.022478555156877134}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.012044719716229335}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.025248800889665857}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.008815220918746869}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.02124545282849377}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.014421490537620207}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9901317024513848}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9881610706189223}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9884418521569466}], "aucs": [0.16723313617973945, 0.15186716750679474, 0.1529841874264526, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.022478555156877134, 0.012044719716229335, 0.025248800889665857, 0.008815220918746869, 0.02124545282849377, 0.014421490537620207, 0.9901317024513848, 0.9881610706189223, 0.9884418521569466]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2542.0, "Edges": 2541.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.999213217938631, "Degree Variance": 2.06923620237446, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.051063829787234, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.321823625992325, "Depth Entropy": 1.9945761402936133, "Assortativity": 0.0, "Average Eccentricity": 16.831628638867034, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0003933910306845004, "Average Shortest Path": 9.895958367741502, "mean_complexity": 15.0, "total_complexity": 45.0, "mean_token_count": 729.6666666666666, "total_token_count": 2189.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "14367c47-fdf3-466b-a696-3fdc1daba054", "fitness": 0.5868651647800209, "name": "PORAS", "description": "Progressive Orthogonal Rotations with Adaptive Steps (PORAS) \u2014 mix cheap per-coordinate pushes with tiny rotated 2-D probes built from recent successful directions and a compact elite archive; adapt per-dimension scales conservatively and trigger single-eval mixes/jumps on stagnation.", "code": "import numpy as np\n\nclass PORAS:\n    \"\"\"\n    PORAS(budget, dim): Progressive Orthogonal Rotations with Adaptive Steps.\n    Compact, budget-aware optimizer for [-5,5]^d continuous noiseless functions.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_step=1.0, min_step=1e-6, max_step=2.5,\n                 elite_size=4, dir_memory=4, stagn_patience=12, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.elite_size = int(elite_size)\n        self.dir_memory = int(dir_memory)\n        self.stagn_patience = int(stagn_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb, ub = -5.0, 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.asarray(lb); ub = np.asarray(ub)\n        if lb.size == 1: lb = np.full(self.dim, float(lb))\n        if ub.size == 1: ub = np.full(self.dim, float(ub))\n        return lb.astype(float), ub.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n        evals = 0\n        rng = self.rng\n\n        # tiny initialization\n        n_init = min(4, max(1, self.budget // 200 + 1))\n        xs = []\n        fs = []\n        for _ in range(n_init):\n            if evals >= self.budget: break\n            x = lb + rng.random(self.dim) * (ub - lb)\n            fs.append(float(func(x))); xs.append(x.copy()); evals += 1\n\n        if len(fs) == 0:\n            # budget zero or nothing evaluated: return center (do not call func)\n            x0 = lb + 0.5 * (ub - lb)\n            return float(\"nan\"), x0.copy()\n\n        idx = int(np.argmin(fs)); x_best = xs[idx].copy(); f_best = float(fs[idx])\n\n        steps = np.minimum(self.max_step, np.full(self.dim, self.init_step))\n        steps = np.minimum(steps, ub - lb + 1e-12)\n\n        # elite archive as list of (f,x)\n        elite = sorted(zip(fs, xs), key=lambda t: t[0])[: self.elite_size]\n        elite = [(float(fv), np.array(xv, dtype=float).copy()) for fv, xv in elite]\n\n        dir_mem = []  # recent unit directions\n        stagn = 0\n\n        def push_dir(v):\n            if v is None: return\n            n = np.linalg.norm(v)\n            if n <= 1e-12: return\n            u = v / n\n            dir_mem.insert(0, u)\n            if len(dir_mem) > self.dir_memory: dir_mem.pop()\n\n        def update_elite(x, f):\n            nonlocal elite\n            x = np.array(x, dtype=float)\n            f = float(f)\n            # quick uniqueness: if too close to any elite skip\n            for fe, xe in elite:\n                if np.linalg.norm(x - xe) < 1e-8: return\n            elite.append((f, x))\n            elite = sorted(elite, key=lambda t: t[0])[: self.elite_size]\n\n        # main loop: each iteration consumes <=3 evals\n        while evals < self.budget:\n            remaining = self.budget - evals\n            improved = False\n\n            # weighted coordinate sampling favoring large step and spread\n            if rng.random() < 0.62 and remaining >= 1:\n                if len(elite) >= 2:\n                    coords = np.vstack([e[1] for e in elite]); spreads = np.std(coords, axis=0) + 1e-12\n                else:\n                    spreads = np.ones(self.dim)\n                w = spreads * (steps + 1e-12)\n                w = w / w.sum()\n                i = int(rng.choice(self.dim, p=w))\n                sgn = 1 if rng.random() < 0.5 else -1\n                delta = np.zeros(self.dim); delta[i] = sgn * steps[i]\n                # probe one side\n                if evals < self.budget:\n                    x1 = clip(x_best + delta); f1 = float(func(x1)); evals += 1\n                    if f1 < f_best:\n                        push_dir(x1 - x_best); x_old = x_best.copy(); x_best, f_best = x1.copy(), f1; improved = True\n                        # cheap extrapolation (one more eval)\n                        if evals < self.budget:\n                            ext = clip(x_best + 0.5 * delta); fext = float(func(ext)); evals += 1\n                            if fext < f_best:\n                                push_dir(ext - x_old); x_best, f_best = ext.copy(), fext\n                        steps[i] = min(self.max_step, steps[i] * 1.18)\n                    else:\n                        # try opposite if budget allows\n                        if remaining >= 2 and evals < self.budget:\n                            x2 = clip(x_best - delta); f2 = float(func(x2)); evals += 1\n                            if f2 < f_best:\n                                push_dir(x2 - x_best); x_old = x_best.copy(); x_best, f_best = x2.copy(), f2; improved = True\n                                if evals < self.budget:\n                                    ext = clip(x_best - 0.5 * delta); fext = float(func(ext)); evals += 1\n                                    if fext < f_best:\n                                        push_dir(ext - x_old); x_best, f_best = ext.copy(), fext\n                                steps[i] = min(self.max_step, steps[i] * 1.06)\n                            else:\n                                steps[i] = max(self.min_step, steps[i] * 0.60)\n                        else:\n                            steps[i] = max(self.min_step, steps[i] * 0.80)\n\n            # rotated cheap 2-D probe using recent direction memory or random\n            elif rng.random() < 0.9 and remaining >= 1:\n                a_idx = int(rng.integers(self.dim))\n                a = np.zeros(self.dim); a[a_idx] = 1.0\n                if dir_mem and rng.random() < 0.7:\n                    d = dir_mem[rng.integers(len(dir_mem))]\n                else:\n                    d = rng.normal(size=self.dim); dn = np.linalg.norm(d)\n                    d = (d / max(dn, 1e-12))\n                # orthogonalize\n                proj = (a @ d) * a\n                v = d - proj; vn = np.linalg.norm(v)\n                if vn < 1e-12:\n                    v = np.zeros(self.dim); v[(a_idx + 1) % self.dim] = 1.0\n                else:\n                    v = v / vn\n                alpha = 0.45\n                dir1 = alpha * a + (1 - alpha) * v\n                dir2 = alpha * a - (1 - alpha) * v\n                scale = np.sqrt(max(1e-12, steps[a_idx] * np.mean(steps)))\n                c1 = clip(x_best + scale * dir1); c2 = clip(x_best + scale * dir2)\n                f1 = f2 = None\n                if evals < self.budget:\n                    f1 = float(func(c1)); evals += 1\n                if evals < self.budget:\n                    f2 = float(func(c2)); evals += 1\n                cand_x, cand_f = x_best, f_best\n                if f1 is not None and f1 < cand_f: cand_x, cand_f = c1.copy(), f1\n                if f2 is not None and f2 < cand_f: cand_x, cand_f = c2.copy(), f2\n                if cand_f < f_best:\n                    push_dir(cand_x - x_best); x_best, f_best = cand_x.copy(), cand_f; improved = True\n                    steps[a_idx] = min(self.max_step, steps[a_idx] * 1.10)\n                    steps = np.minimum(self.max_step, steps * 1.02)\n\n            # elite mixing / tiny jitter or local gaussian jitter\n            else:\n                if elite and len(elite) >= 2 and rng.random() < 0.85 and remaining >= 1:\n                    i1, i2 = rng.choice(len(elite), size=2, replace=False)\n                    xmix = clip(rng.random() * elite[i1][1] + (1 - rng.random()) * elite[i2][1])\n                    med = max(1e-12, np.median(steps))\n                    xmix = clip(xmix + rng.normal(scale=0.06 * med, size=self.dim))\n                    if evals < self.budget:\n                        fm = float(func(xmix)); evals += 1; update_elite(xmix, fm)\n                        if fm < f_best:\n                            push_dir(xmix - x_best); x_best, f_best = xmix.copy(), fm; improved = True; steps = np.minimum(self.max_step, steps * 1.10)\n                else:\n                    med = max(1e-12, np.median(steps))\n                    if evals < self.budget:\n                        xn = clip(x_best + rng.normal(scale=0.12 * med, size=self.dim))\n                        fn = float(func(xn)); evals += 1; update_elite(xn, fn)\n                        if fn < f_best:\n                            push_dir(xn - x_best); x_best, f_best = xn.copy(), fn; improved = True; steps = np.minimum(self.max_step, steps * 1.08)\n                        else:\n                            steps = np.maximum(self.min_step, steps * 0.97)\n\n            update_elite(x_best, f_best)\n            stagn = 0 if improved else (stagn + 1)\n\n            # mirrored cheap check for symmetry\n            if stagn >= 4 and evals < self.budget and rng.random() < 0.28:\n                xm = clip(lb + ub - x_best); fm = float(func(xm)); evals += 1; update_elite(xm, fm)\n                if fm < f_best:\n                    push_dir(xm - x_best); x_best, f_best = xm.copy(), fm; stagn = 0; steps = np.minimum(self.max_step, steps * 1.04)\n\n            # stagnation-driven single-eval diversifications (keep degree=1)\n            if stagn >= self.stagn_patience and evals < self.budget:\n                if len(elite) >= 2:\n                    mix = 0.5 * (elite[0][1] + elite[min(1, len(elite)-1)][1])\n                    mix = clip(mix + rng.normal(scale=0.22 * np.median(steps), size=self.dim))\n                    fm = float(func(mix)); evals += 1; update_elite(mix, fm)\n                    if fm < f_best:\n                        push_dir(mix - x_best); x_best, f_best = mix.copy(), fm; stagn = 0; steps = np.minimum(self.max_step, steps * 1.12); continue\n                # near-random jump\n                jump_scale = 0.7 * (ub - lb)\n                jump = clip(x_best + rng.normal(scale=0.6, size=self.dim) * jump_scale)\n                fj = float(func(jump)); evals += 1; update_elite(jump, fj)\n                if fj < f_best:\n                    push_dir(jump - x_best); x_best, f_best = jump.copy(), fj; stagn = 0; steps = np.minimum(self.max_step, steps * 1.15)\n                else:\n                    steps = np.maximum(self.min_step, steps * 0.5)\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 15, "feedback": "The algorithm PORAS scored 0.587 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["964ff4d1-eae7-4989-a9da-2f5488ddd047"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9718486752653691}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9685257387635823}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.942488623849104}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9311077529126022}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9368545406686951}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9078012802557537}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.047740712935614105}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.025367917923877603}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.049072579207840916}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.014088384738113024}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.04606730916014479}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9902457394655705}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9867238621955294}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.984994354358519}], "aucs": [0.9718486752653691, 0.9685257387635823, 0.942488623849104, 0.9311077529126022, 0.9368545406686951, 0.9078012802557537, 0.047740712935614105, 4.999999999999449e-05, 0.025367917923877603, 0.049072579207840916, 0.014088384738113024, 0.04606730916014479, 0.9902457394655705, 0.9867238621955294, 0.984994354358519]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2688.0, "Edges": 2687.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.9992559523809523, "Degree Variance": 1.8638387321074261, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 9.077943615257048, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3248941921218667, "Depth Entropy": 2.284926444911348, "Assortativity": 0.0, "Average Eccentricity": 18.616815476190474, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003720238095238095, "Average Shortest Path": 11.095727665833731, "mean_complexity": 12.8, "total_complexity": 64.0, "mean_token_count": 465.4, "total_token_count": 2327.0, "mean_parameter_count": 3.4, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "8f133031-8bcd-4ff3-ac53-c6a33f25dff1", "fitness": "-inf", "name": "StaggeredGreedyMomentumArchiveExplorer", "description": "Staggered Greedy Momentum-Archive Explorer (SGMAE) \u2014 single-coordinate greedy probes with per-dimension momentum & adaptive steps, a tiny elite archive that drives per-coordinate medians/variances for informed pulls and occasional diagonal-covariance global jumps for escape.", "code": "import numpy as np\n\nclass StaggeredGreedyMomentumArchiveExplorer:\n    def __init__(self, budget=10000, dim=10, init_step=None, min_step=1e-6,\n                 max_step=None, patience=6, archive_size=8, rng=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.init_step = init_step; self.min_step = float(min_step)\n        self.max_step = max_step\n        self.patience = int(patience)\n        self.archive_size = int(archive_size)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb, ub = -5.0, 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, float); ub = np.array(ub, float)\n        if lb.shape == (): lb = np.full(self.dim, lb.item())\n        if ub.shape == (): ub = np.full(self.dim, ub.item())\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func); rng = self.rng\n        span = ub - lb\n        if self.init_step is None:\n            step = np.full(self.dim, 0.25) * span\n        else:\n            step = np.array(self.init_step, float)\n            if step.shape == (): step = np.full(self.dim, float(step))\n        max_step = (np.full(self.dim, 0.5) * span) if self.max_step is None else np.array(self.max_step, float)\n        if max_step.shape == (): max_step = np.full(self.dim, float(max_step))\n        step = np.clip(step, self.min_step, max_step)\n\n        # state\n        x_best = lb + rng.random(self.dim) * span\n        f_best = float(func(x_best)); evals = 1\n        archive = [(f_best, x_best.copy())]\n        momentum = np.zeros(self.dim)\n        priority = np.zeros(self.dim)\n        stagn = 0\n        iter_idx = 0\n        base_frac = max(0.05, min(0.4, 5.0 / max(1, self.dim)))\n\n        while evals < self.budget:\n            iter_idx += 1\n            remaining = self.budget - evals\n            p = max(1, int(np.ceil(self.dim * base_frac)))\n            # sampling distribution from priority (softmax-ish)\n            tmp = priority - priority.max()\n            probs = np.exp(tmp)\n            probs = probs / probs.sum()\n            # pick coords without replacement (fallback uniform)\n            try:\n                coords = rng.choice(self.dim, size=min(self.dim, p), replace=False, p=probs)\n            except Exception:\n                coords = rng.choice(self.dim, size=min(self.dim, p), replace=False)\n            improved = False\n            # stagger order: shuffle with deterministic tilt by iter_idx to reduce repeated patterns\n            rng.shuffle(coords)\n            for i in coords:\n                if evals >= self.budget: break\n                s = max(step[i], self.min_step)\n                # prefer direction from momentum, else random\n                sign = 1 if momentum[i] >= 0 else -1\n                if momentum[i] == 0 and rng.random() < 0.5: sign = -sign\n                # try preferred side\n                x = x_best.copy(); x[i] = np.clip(x[i] + sign * s, lb[i], ub[i])\n                f = float(func(x)); evals += 1\n                if f < f_best:\n                    # accept and lightly boost\n                    f_best, x_best = f, x\n                    improved = True; stagn = 0\n                    momentum[i] = 0.7 * momentum[i] + 0.3 * sign\n                    priority[i] = 0.8 * priority[i] + 0.6\n                    step[i] = float(min(max_step[i], step[i] * 1.25 + 1e-12))\n                    # cheap extrapolation\n                    if evals < self.budget:\n                        ext = 0.6 * s\n                        xe = x_best.copy(); xe[i] = np.clip(xe[i] + sign * ext, lb[i], ub[i])\n                        fe = float(func(xe)); evals += 1\n                        if fe < f_best:\n                            f_best, x_best = fe, xe\n                            momentum[i] += 0.2 * sign\n                            step[i] = float(min(max_step[i], step[i] * 1.15))\n                    # archive insert\n                    archive.append((f_best, x_best.copy()))\n                else:\n                    # try opposite once\n                    if evals < self.budget:\n                        x2 = x_best.copy(); x2[i] = np.clip(x2[i] - sign * s, lb[i], ub[i])\n                        f2 = float(func(x2)); evals += 1\n                        if f2 < f_best:\n                            f_best, x_best = f2, x2\n                            improved = True; stagn = 0\n                            momentum[i] = 0.7 * momentum[i] - 0.3 * sign\n                            priority[i] = 0.8 * priority[i] + 0.5\n                            step[i] = float(min(max_step[i], step[i] * 1.2))\n                            if evals < self.budget:\n                                ext = 0.6 * s\n                                xe = x_best.copy(); xe[i] = np.clip(xe[i] - sign * ext, lb[i], ub[i])\n                                fe = float(func(xe)); evals += 1\n                                if fe < f_best:\n                                    f_best, x_best = fe, xe\n                                    momentum[i] -= 0.1 * sign\n                                    step[i] = float(min(max_step[i], step[i] * 1.1))\n                            archive.append((f_best, x_best.copy()))\n                        else:\n                            # failure shrink and penalize\n                            priority[i] = 0.85 * priority[i] - 0.4\n                            momentum[i] *= 0.6\n                            step[i] = float(max(self.min_step, step[i] * 0.5))\n                    else:\n                        # no budget for opposite\n                        priority[i] *= 0.9; step[i] = float(max(self.min_step, step[i] * 0.9))\n\n                # keep bounds\n                priority[i] = float(np.clip(priority[i], -8.0, 8.0))\n                step[i] = float(np.clip(step[i], self.min_step, max_step[i]))\n\n            # trim/keep archive best few distinct samples\n            archive.sort(key=lambda t: t[0])\n            unique = []\n            seen = set()\n            for f,x in archive:\n                key = tuple(np.round(x, 6))\n                if key not in seen:\n                    unique.append((f,x)); seen.add(key)\n                if len(unique) >= self.archive_size: break\n            archive = unique\n\n            if improved:\n                stagn = 0\n            else:\n                stagn += 1\n\n            # occasional archive-driven median pulls (cheap, per-dim)\n            if stagn >= 2 and evals < self.budget and len(archive) >= 3 and rng.random() < 0.35:\n                arr = np.vstack([a[1] for a in archive])\n                med = np.median(arr, axis=0)\n                # pick a few dims to pull toward median\n                nd = max(1, self.dim // 6)\n                idx = rng.choice(self.dim, size=nd, replace=False)\n                for i in idx:\n                    if evals >= self.budget: break\n                    # pull partially toward median along i\n                    x = x_best.copy()\n                    delta = (med[i] - x[i]) * 0.5\n                    if abs(delta) < 1e-12: continue\n                    x[i] = np.clip(x[i] + delta, lb[i], ub[i])\n                    f = float(func(x)); evals += 1\n                    if f < f_best:\n                        f_best, x_best = f, x\n                        improved = True; stagn = 0\n                        momentum[i] = 0.6 * np.sign(delta)\n                        step[i] = float(min(max_step[i], step[i] * 1.2))\n                        archive.append((f_best, x_best.copy()))\n\n            # stronger escape: diagonal gaussian based on archive spread\n            if stagn >= self.patience and evals < self.budget:\n                arr = np.vstack([a[1] for a in archive])\n                if arr.shape[0] >= 2:\n                    spread = np.maximum(np.std(arr, axis=0), 1e-8 * span)\n                else:\n                    spread = 0.1 * span\n                # limit magnitude relative to steps\n                cov_scale = np.minimum(spread, 1.0 * step)\n                ntries = min(4, remaining)\n                for _ in range(ntries):\n                    if evals >= self.budget: break\n                    xg = x_best + rng.normal(0, 1, self.dim) * cov_scale\n                    xg = np.minimum(np.maximum(xg, lb), ub)\n                    fg = float(func(xg)); evals += 1\n                    if fg < f_best:\n                        f_best, x_best = fg, xg; improved = True; stagn = 0\n                        step = np.clip(step * 1.2, self.min_step, max_step)\n                        archive.append((f_best, x_best.copy()))\n                        break\n                if not improved:\n                    # increase steps to force wider search\n                    step = np.clip(step * 1.3, self.min_step, max_step)\n                    priority *= 0.3\n                    momentum *= 0.2\n                    stagn = 0\n\n            # adapt probe fraction slightly\n            successes = sum(1 for s in archive if s[0] < f_best + 1e-12)\n            if successes > 0:\n                base_frac = min(0.6, 0.06 + 0.08 * np.log1p(len(archive)))\n            else:\n                base_frac = max(0.05, base_frac * 0.995)\n\n        return f_best, x_best", "configspace": "", "generation": 15, "feedback": "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/blade_result_b6acaacaf014449197cd8b410f253f9f.pkl'.\nOn line 116:                     else:.\n", "error": "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/blade_result_b6acaacaf014449197cd8b410f253f9f.pkl'.\nOn line 116:                     else:.\n", "parent_ids": ["d73fb929-d6e1-4776-b617-8f5cdba02304"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2282.0, "Edges": 2281.0, "Max Degree": 22.0, "Min Degree": 1.0, "Mean Degree": 1.9991235758106924, "Degree Variance": 1.9360202660611838, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.794285714285714, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3316074637418818, "Depth Entropy": 2.2274910612554124, "Assortativity": 2.6504852339965872e-08, "Average Eccentricity": 17.719982471516214, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00043821209465381246, "Average Shortest Path": 10.692136119703944, "mean_complexity": 16.666666666666668, "total_complexity": 50.0, "mean_token_count": 665.0, "total_token_count": 1995.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "4a569da3-dff0-437a-a1e1-9e9baee72739", "fitness": 0.6218575264459555, "name": "ThompsonCoordinateMomentum", "description": "Thompson-Coordinate Momentum (TCM) \u2014 a compact Thompson-sampling coordinate-bandit that blends per-dimension Beta sign posteriors with a lightweight momentum signal, conservative multiplicative step adaptation, and rare L\u00e9vy-style escapes to quickly exploit useful directions while staying robust to stagnation.", "code": "import numpy as np\n\nclass ThompsonCoordinateMomentum:\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 success_mult=1.25, failure_mult=0.65,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, momentum_gain=0.22, momentum_decay=0.85,\n                 seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob)\n        self.momentum_gain = float(momentum_gain); self.momentum_decay = float(momentum_decay)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds (defaults -5..5)\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box_scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = max(1e-12, self.max_step_frac * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base, float)\n        a_pos = np.ones(dim); b_pos = np.ones(dim); a_neg = np.ones(dim); b_neg = np.ones(dim)\n        momentum = np.zeros(dim)\n        consec_fail = np.zeros(dim, int)\n        trials = np.zeros(dim, int)\n\n        while evals < self.budget:\n            # rare L\u00e9vy-like escape\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim)\n                jump = 0.12 * box_scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n\n            # sample Beta for each sign\n            sp = rng.beta(a_pos, b_pos); sn = rng.beta(a_neg, b_neg)\n            # bias via momentum: positive momentum favors positive moves and vice versa\n            mb = np.clip(momentum, -0.999, 0.999)\n            sp = sp * (1.0 + 0.5 * mb)          # more weight if momentum>0\n            sn = sn * (1.0 - 0.5 * mb)          # less weight if momentum>0\n            # account for headroom to bounds (avoid pushing into walls)\n            headroom_pos = (ub - x_best) / (ub - lb + 1e-18); headroom_neg = (x_best - lb) / (ub - lb + 1e-18)\n            score_pos = sp * (0.3 + 0.7 * headroom_pos)\n            score_neg = sn * (0.3 + 0.7 * headroom_neg)\n            choose_pos = score_pos >= score_neg\n            score = np.where(choose_pos, score_pos, score_neg) * steps\n            score += 1e-15 * rng.random(dim)\n            d = int(np.argmax(score)); sign = 1 if choose_pos[d] else -1\n\n            # propose single-eval coordinate move\n            s = np.clip(steps[d], min_step, max_step)\n            xt = x_best.copy(); xt[d] = np.clip(xt[d] + sign * s, lb[d], ub[d])\n            ft = safe_eval(xt)\n            trials[d] += 1\n\n            # update priors & momentum\n            if sign > 0:\n                if ft < f_best: a_pos[d] += 1.0\n                else: b_pos[d] += 1.0\n            else:\n                if ft < f_best: a_neg[d] += 1.0\n                else: b_neg[d] += 1.0\n\n            if ft < f_best:\n                # success: accept, boost step and reinforce momentum\n                x_best, f_best = xt.copy(), float(ft)\n                steps[d] = min(max_step, steps[d] * self.success_mult)\n                momentum[d] = momentum[d] * self.momentum_decay + sign * self.momentum_gain\n                consec_fail[d] = 0\n                # opportunistic one-step extrapolation if budget allows: cheap greedy probe\n                if evals < self.budget:\n                    extra = x_best.copy(); extra[d] = np.clip(extra[d] + sign * steps[d], lb[d], ub[d])\n                    fextra = safe_eval(extra)\n                    if fextra < f_best:\n                        x_best, f_best = extra.copy(), float(fextra)\n            else:\n                # failure: shrink and weaken momentum a bit\n                steps[d] = max(min_step, steps[d] * self.failure_mult)\n                momentum[d] *= (self.momentum_decay ** 1.5)\n                consec_fail[d] += 1\n\n            # conservative strong shrink on repeated fails\n            if consec_fail[d] >= 6:\n                steps[d] = max(min_step, steps[d] * (self.failure_mult ** 1.5))\n                if sign > 0: b_pos[d] += 0.5\n                else: b_neg[d] += 0.5\n                consec_fail[d] = 0\n\n            # mild prior compression for stability\n            if trials[d] > 48:\n                scl = 0.9\n                a_pos[d] = 1.0 + (a_pos[d] - 1.0) * scl\n                b_pos[d] = 1.0 + (b_pos[d] - 1.0) * scl\n                a_neg[d] = 1.0 + (a_neg[d] - 1.0) * scl\n                b_neg[d] = 1.0 + (b_neg[d] - 1.0) * scl\n                trials[d] = int(trials[d] * 0.6)\n\n        # final light polish: small symmetric probes per-dim while budget remains\n        fin = 0.05 * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best: x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best: x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 15, "feedback": "The algorithm ThompsonCoordinateMomentum scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2ce99025-003a-46b2-bec0-52c377bfc061"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9875631711363656}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9868231476451716}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9871220268432187}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9682199656937096}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9646413321405338}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9747905884444446}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.05369036224148971}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10420713136962845}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.05227947102332642}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.06758603114660322}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.11469014551313339}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07938182812895533}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9950468475672257}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.996171323579578}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9956495242159482}], "aucs": [0.9875631711363656, 0.9868231476451716, 0.9871220268432187, 0.9682199656937096, 0.9646413321405338, 0.9747905884444446, 0.05369036224148971, 0.10420713136962845, 0.05227947102332642, 0.06758603114660322, 0.11469014551313339, 0.07938182812895533, 0.9950468475672257, 0.996171323579578, 0.9956495242159482]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1670.0, "Edges": 1669.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9988023952095808, "Degree Variance": 2.415567428018215, "Transitivity": 0.0, "Max Depth": 12.0, "Min Depth": 2.0, "Mean Depth": 7.4113110539845755, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.31789874778724, "Depth Entropy": 1.8682015109899843, "Assortativity": 0.0, "Average Eccentricity": 14.381437125748503, "Diameter": 19.0, "Radius": 10.0, "Edge Density": 0.0005988023952095808, "Average Shortest Path": 9.052920641640625, "mean_complexity": 10.333333333333334, "total_complexity": 31.0, "mean_token_count": 491.0, "total_token_count": 1473.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "87c3d726-aa1c-4e82-97c3-71647d82e61f", "fitness": 0.24713274131321603, "name": "SparseDirectionalTempering", "description": "Sparse Directional Tempering (SDT) \u2014 maintain per-dimension \"temperatures\" to control sparse, low-rank Gaussian probes biased by a short-term preferred direction from recent successes; adapt temps multiplicatively on success/failure and use tiny anchor blends and rare global resets for diversification.", "code": "import numpy as np\n\nclass SparseDirectionalTempering:\n    \"\"\"\n    Sparse Directional Tempering (SDT)\n\n    Idea: Most evaluations are single-eval sparse Gaussian probes focused on a few coordinates\n    chosen by adaptive per-dimension \"temperatures\". Successful probes update a short-term\n    preferred direction (momentum-like) that biases subsequent low-rank probes. Temperatures\n    grow on success and shrink on failure; occasional 1-eval anchor blends and global draws\n    provide diversification. Works in [-5,5] default bounds and respects budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, init_temp=None, min_temp=1e-6, max_temp=5.0,\n                 anchor_pool=5, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_temp = init_temp\n        self.min_temp = float(min_temp)\n        self.max_temp = float(max_temp)\n        self.anchor_pool = int(anchor_pool)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == (): lb = np.full(self.dim, float(lb))\n        if ub.shape == (): ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        if lb.size != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        evals = 0\n        x_best = lb + self.rng.random(self.dim) * (ub - lb)\n        f_best = float(func(x_best)); evals += 1\n\n        # temperatures control std dev per-dim (relative)\n        if self.init_temp is None:\n            temp = np.full(self.dim, ((ub - lb).mean() / 8.0))\n        else:\n            t = np.array(self.init_temp, dtype=float)\n            temp = t if t.shape == (self.dim,) else np.full(self.dim, float(t.mean() if t.size else t))\n\n        temp = np.clip(temp, self.min_temp, self.max_temp)\n\n        # adaptation params\n        up = 1.30\n        down = 0.72\n        # short memory preferred direction (average of recent accepted step vectors)\n        pref_buf = []\n        pref_len = 6\n        anchors = [(f_best, x_best.copy())]\n\n        while evals < self.budget:\n            remaining = self.budget - evals\n            r = self.rng.random()\n            improved = False\n\n            # probabilities: 0..0.6 single-sparse; 0.6..0.85 low-rank bias; 0.85..0.96 anchor blend; rest global\n            if r < 0.60:\n                # single-sparse: pick k dims (usually 1 or 2) weighted by temp\n                probs = temp / temp.sum()\n                k = 1 if self.rng.random() < 0.75 or self.dim == 1 else min(self.dim, 1 + int(self.rng.choice([1,2], p=[0.6,0.4])))\n                idx = self.rng.choice(self.dim, size=k, replace=False, p=probs)\n                # gaussian perturbation scaled by temps and by dimension count (normalize)\n                scale = 1.0 / np.sqrt(max(1, k))\n                delta = np.zeros(self.dim)\n                delta[idx] = self.rng.normal(0.0, 1.0, size=k) * temp[idx] * scale\n                x_cand = np.clip(x_best + delta, lb, ub)\n                f_cand = float(func(x_cand)); evals += 1\n\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True\n                    # increase temps for used dims, slightly reward others mildly\n                    temp[idx] = np.minimum(self.max_temp, temp[idx] * up)\n                    temp = np.minimum(self.max_temp, temp * 1.02)\n                    # update preferred direction buffer\n                    step = x_best - (anchors[0][1] if anchors else x_best)\n                    pref_buf.append(step); \n                    if len(pref_buf) > pref_len: pref_buf.pop(0)\n                    anchors.append((f_best, x_best.copy())); anchors.sort(key=lambda t: t[0])\n                    if len(anchors) > self.anchor_pool: anchors = anchors[:self.anchor_pool]\n                else:\n                    # failure: shrink temps for used dims\n                    temp[idx] = np.maximum(self.min_temp, temp[idx] * down)\n                    temp = np.maximum(self.min_temp, temp * 0.995)\n\n            elif r < 0.85:\n                # low-rank biased probe: build a small-rank direction from preferred direction + noise\n                rank = min(3, self.dim)\n                if pref_buf:\n                    pref = np.mean(pref_buf, axis=0)\n                    # normalize and mix with noise\n                    pref_norm = pref / (np.linalg.norm(pref) + 1e-12)\n                    noise = self.rng.normal(0.0, 1.0, size=self.dim)\n                    dir_vec = 0.7 * pref_norm + 0.3 * noise / (np.linalg.norm(noise) + 1e-12)\n                else:\n                    dir_vec = self.rng.normal(0.0, 1.0, size=self.dim)\n                # scale per-dim by temp, but keep overall step size modest\n                scaled = dir_vec * temp\n                scaled *= ( (ub - lb).mean() / (np.linalg.norm(scaled) + 1e-12) ) * 0.5\n                # sparsify: zero out small entries with probability depending on temp\n                keep_prob = np.clip(temp / (temp.max() + 1e-12), 0.05, 0.9)\n                mask = self.rng.random(self.dim) < keep_prob\n                if not mask.any():\n                    mask[self.rng.integers(self.dim)] = True\n                delta = scaled * mask\n                x_cand = np.clip(x_best + delta, lb, ub)\n                f_cand = float(func(x_cand)); evals += 1\n\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True\n                    # reward dims proportionally to absolute move\n                    m = np.abs(delta)\n                    if m.sum() > 0:\n                        temp += (m / m.max()) * (up - 1.0) * temp.mean() * 0.2\n                    temp = np.minimum(temp, self.max_temp)\n                    pref_buf.append(x_best - anchors[0][1] if anchors else x_best * 0.0)\n                    if len(pref_buf) > pref_len: pref_buf.pop(0)\n                    anchors.append((f_best, x_best.copy())); anchors.sort(key=lambda t: t[0])\n                    if len(anchors) > self.anchor_pool: anchors = anchors[:self.anchor_pool]\n                else:\n                    # shrink where moved\n                    temp = np.maximum(self.min_temp, temp * (down ** 0.5))\n\n            elif r < 0.96:\n                # anchor blend: interpolate between best anchor and another or small perturbation\n                if len(anchors) >= 2 and self.rng.random() < 0.85:\n                    i, j = self.rng.choice(len(anchors), size=2, replace=False)\n                    a, b = anchors[i][1], anchors[j][1]\n                    alpha = np.clip(self.rng.normal(0.5, 0.18), -0.2, 1.2)\n                    x_cand = np.clip(a * (1 - alpha) + b * alpha, lb, ub)\n                else:\n                    base = anchors[0][1] if anchors else x_best\n                    x_cand = np.clip(base + self.rng.normal(0.0, 0.3, size=self.dim) * temp, lb, ub)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True\n                    temp = np.minimum(self.max_temp, temp * 1.08)\n                    anchors.append((f_best, x_best.copy())); anchors.sort(key=lambda t: t[0])\n                    if len(anchors) > self.anchor_pool: anchors = anchors[:self.anchor_pool]\n                else:\n                    temp = np.maximum(self.min_temp, temp * 0.98)\n\n            else:\n                # global diversification: opposition or random\n                if self.rng.random() < 0.6:\n                    x_cand = np.clip(lb + ub - x_best, lb, ub)\n                else:\n                    x_cand = lb + self.rng.random(self.dim) * (ub - lb)\n                f_cand = float(func(x_cand)); evals += 1\n                if f_cand < f_best:\n                    f_best = f_cand; x_best = x_cand; improved = True\n                    temp = np.clip(temp * 1.12 + (ub - lb).mean() * 0.01, self.min_temp, self.max_temp)\n                    anchors.append((f_best, x_best.copy())); anchors.sort(key=lambda t: t[0])\n                    if len(anchors) > self.anchor_pool: anchors = anchors[:self.anchor_pool]\n                else:\n                    # slightly cool temps to focus search\n                    temp = np.maximum(self.min_temp, temp * 0.95)\n\n            # keep temps bounded\n            temp = np.clip(temp, self.min_temp, self.max_temp)\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 15, "feedback": "The algorithm SparseDirectionalTempering scored 0.247 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f848c7e0-b9e3-4731-969d-4bbca80a2964"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.20730918079975458}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.2516977992749698}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.18226069568589143}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02547731174548351}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.013091764131994199}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.02765776505374684}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.027814275246723086}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0016536842247036887}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.021511717587576085}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9765191541959553}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9852667484953582}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.986581023256084}], "aucs": [0.20730918079975458, 0.2516977992749698, 0.18226069568589143, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.02547731174548351, 0.013091764131994199, 0.02765776505374684, 0.027814275246723086, 0.0016536842247036887, 0.021511717587576085, 0.9765191541959553, 0.9852667484953582, 0.986581023256084]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1918.0, "Edges": 1917.0, "Max Degree": 17.0, "Min Degree": 1.0, "Mean Degree": 1.9989572471324295, "Degree Variance": 1.8206454194443509, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 9.35632183908046, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3314936098017325, "Depth Entropy": 2.3273599265237688, "Assortativity": 0.0, "Average Eccentricity": 17.982273201251303, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0005213764337851929, "Average Shortest Path": 10.830921185398413, "mean_complexity": 12.333333333333334, "total_complexity": 37.0, "mean_token_count": 593.0, "total_token_count": 1779.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "372710bf-5bc2-45d6-a0ba-b72fe6011b88", "fitness": 0.39589271121511194, "name": "MicroMomentumSingleCoordinateSearchV2", "description": "Micro-Momentum Single-Coordinate Search v2 \u2014 adaptive, low-degree (strictly mostly 1\u2011D) search with per-anchor per-dimension momentum, success-scaled step boosts, priority/UCB-like coordinate scheduling, and compact single-coordinate respawns/heavy-tail pushes for robust diversification.", "code": "import numpy as np\n\nclass MicroMomentumSingleCoordinateSearchV2:\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.08, success_mult=1.40, failure_mult=0.60,\n                 heavy_prob=0.03, respawn_prob=0.14, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        if pop_size is None:\n            self.pop_size = max(2, min(6, int(3 + 0.04 * self.dim)))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.heavy_prob = float(heavy_prob)\n        self.respawn_prob = float(respawn_prob)\n\n    def _bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0); ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func); dim = self.dim; rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n        evals = 0\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # initialize anchors by single-coordinate variants of a base point\n        anchors = np.empty((self.pop_size, dim), float)\n        af = np.full(self.pop_size, np.inf)\n        base = rng.uniform(lb, ub)\n        anchors[0] = base.copy(); af[0] = safe_eval(anchors[0])\n        for i in range(1, self.pop_size):\n            if evals >= self.budget:\n                anchors[i] = anchors[0].copy(); af[i] = af[0]; continue\n            a = anchors[0].copy()\n            c = rng.integers(0, dim)\n            a[c] = np.clip(a[c] + rng.normal(0, base_step), lb[c], ub[c])\n            anchors[i] = a; af[i] = safe_eval(a)\n\n        steps = np.maximum(base_step * (0.5 + rng.random((self.pop_size, dim))), 1e-12 * box_len)\n        mom = np.zeros((self.pop_size, dim), float)\n        stagn = np.zeros(self.pop_size, int)\n\n        ibest = int(np.nanargmin(af)); xbest = anchors[ibest].copy(); fbest = float(af[ibest])\n\n        # main loop (mostly single-coordinate operations)\n        while evals < self.budget:\n            # pick anchor biased to underperformers to focus improvement while keeping elites alive\n            weights = (af - fbest) - np.min(af - fbest) + 1e-12\n            weights = weights ** 1.0\n            idx = int(rng.choice(self.pop_size, p=weights / weights.sum()))\n\n            # select coordinate by priority: step * (1 + |mom|*w) * (1 + stagn_factor)\n            wmom = 1.2\n            stagn_factor = 1.0 + 0.2 * stagn[idx]\n            priority = steps[idx] * (1.0 + wmom * np.abs(mom[idx])) * stagn_factor + 1e-14 * rng.random(dim)\n            c = int(rng.choice(dim, p=priority / priority.sum()))\n\n            # decide move sign with momentum bias\n            m = mom[idx, c]\n            prob_pref = 0.52 + 0.44 * (min(1.0, abs(m)))\n            if rng.random() < prob_pref:\n                sign = 1 if m >= 0 else -1\n            else:\n                sign = 1 if rng.random() < 0.5 else -1\n\n            # heavy-tail single-coordinate probe occasionally\n            if rng.random() < self.heavy_prob and evals < self.budget:\n                u = rng.random(); cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.18 * box_len * cauchy\n                x = anchors[idx].copy(); x[c] = np.clip(x[c] + jump, lb[c], ub[c])\n                fv = safe_eval(x)\n            else:\n                delta = steps[idx, c]\n                if delta <= 1e-12 * box_len:\n                    # tiny step -> consider respawn on that coordinate\n                    if rng.random() < self.respawn_prob and evals < self.budget:\n                        newp = anchors[idx].copy()\n                        newp[c] = rng.uniform(lb[c], ub[c])\n                        fv = safe_eval(newp)\n                        anchors[idx] = newp; af[idx] = fv\n                        steps[idx, c] = base_step; mom[idx, c] = 0.0; stagn[idx] = 0\n                        if fv < fbest: fbest, xbest = fv, newp.copy()\n                        continue\n                    else:\n                        # nudge slightly to escape\n                        delta = base_step\n                x = anchors[idx].copy(); x[c] = np.clip(x[c] + sign * delta, lb[c], ub[c])\n                fv = safe_eval(x)\n\n            # update anchor\n            f0 = af[idx]\n            if fv < f0:\n                # improvement: accept, encourage step scaled by improvement magnitude\n                improv = max(1e-12, (f0 - fv) / (1.0 + abs(f0)))\n                factor = self.success_mult * (1.0 + 0.25 * min(5.0, improv))\n                steps[idx, c] = min(steps[idx, c] * factor, 1.5 * box_len)\n                mom[idx, c] = np.clip(0.85 * mom[idx, c] + 0.15 * float(np.sign(x[c] - anchors[idx, c])), -1.0, 1.0)\n                anchors[idx] = x.copy(); af[idx] = fv; stagn[idx] = 0\n                if fv < fbest: fbest, xbest = fv, x.copy()\n            else:\n                # failure: shrink and decay momentum\n                steps[idx, c] = max(steps[idx, c] * self.failure_mult, 1e-12 * box_len)\n                mom[idx, c] *= 0.80\n                stagn[idx] += 1\n\n            # targeted respawn for persistently stagnant anchors (single-coordinate respawn or heavy push)\n            if stagn[idx] > max(6, dim // 2) and evals < self.budget:\n                stagn[idx] = 0\n                rc = int(rng.integers(0, dim))\n                if rng.random() < self.respawn_prob:\n                    p = anchors[idx].copy(); p[rc] = rng.uniform(lb[rc], ub[rc])\n                    fv = safe_eval(p); anchors[idx] = p; af[idx] = fv\n                else:\n                    u = rng.random(); cauchy = np.tan(np.pi * (u - 0.5))\n                    jump = 0.16 * box_len * cauchy\n                    p = anchors[idx].copy(); p[rc] = np.clip(anchors[idx, rc] + jump, lb[rc], ub[rc])\n                    fv = safe_eval(p); anchors[idx] = p; af[idx] = fv\n                steps[idx, rc] = base_step; mom[idx, rc] = 0.0\n                if fv < fbest: fbest, xbest = fv, anchors[idx].copy()\n\n            # safety limits on steps\n            steps[idx] = np.clip(steps[idx], 1e-12 * box_len, 1.0 * box_len)\n\n            # small early exit to final refinement if few evals remain\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final strict single-coordinate local refinement (progressive mesh)\n        mesh = 0.09 * box_len; mesh_min = 1e-8 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                p = xbest.copy(); p[d] = np.clip(p[d] + mesh, lb[d], ub[d]); fv = safe_eval(p)\n                if fv < fbest: fbest, xbest, improved = fv, p.copy(), True; continue\n                if evals >= self.budget: break\n                p = xbest.copy(); p[d] = np.clip(p[d] - mesh, lb[d], ub[d]); fv = safe_eval(p)\n                if fv < fbest: fbest, xbest, improved = fv, p.copy(), True\n            if not improved: mesh *= 0.5\n\n        self.f_opt = float(fbest); self.x_opt = np.asarray(xbest, float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 15, "feedback": "The algorithm MicroMomentumSingleCoordinateSearchV2 scored 0.396 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6cd3ea29-a941-4770-ab4b-0e5e0cca3077"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.637079533675601}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.6323325167672922}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.6400389310685567}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.1482312954660081}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.13399674456228472}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.2607440869135813}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0834138415124267}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10290841476412771}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0864411042675205}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.07988257029976253}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.09581820023972565}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.08834595071535045}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9859139404618699}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.984728078508285}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9785154590042865}], "aucs": [0.637079533675601, 0.6323325167672922, 0.6400389310685567, 0.1482312954660081, 0.13399674456228472, 0.2607440869135813, 0.0834138415124267, 0.10290841476412771, 0.0864411042675205, 0.07988257029976253, 0.09581820023972565, 0.08834595071535045, 0.9859139404618699, 0.984728078508285, 0.9785154590042865]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2164.0, "Edges": 2163.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9990757855822552, "Degree Variance": 2.1182985912990597, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 7.904904904904905, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3120171784485903, "Depth Entropy": 2.008920906362905, "Assortativity": 0.0, "Average Eccentricity": 17.95887245841035, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0004621072088724584, "Average Shortest Path": 10.119299289085554, "mean_complexity": 9.25, "total_complexity": 37.0, "mean_token_count": 468.25, "total_token_count": 1873.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "51e9aa93-4b4d-4535-baba-261121ae39d8", "fitness": 0.5877365546871611, "name": "SparseDirectionalAnchorSearch", "description": "Sparse Directional Anchor Search (SDAS) \u2014 maintain a tiny anchor set, estimate sparse per-coordinate signals and pairwise correlations from anchor differences, then perform mostly single-coordinate, occasionally correlated 2-D pushes with multiplicative step adaptation and targeted respawns.", "code": "import numpy as np\n\nclass SparseDirectionalAnchorSearch:\n    def __init__(self, budget=1000, dim=10, pop=5, init_step=0.12,\n                 success_mult=1.35, failure_mult=0.6, pair_prob=0.22,\n                 respawn_prob=0.10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop = max(2, int(pop))\n        self.init_step = float(init_step)\n        self.succ = float(success_mult)\n        self.fail = float(failure_mult)\n        self.pair_prob = float(pair_prob)\n        self.respawn_prob = float(respawn_prob)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        dim = self.dim\n        budget = self.budget\n        rng = self.rng\n        lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return np.inf\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            v = float(func(x))\n            evals += 1\n            return v\n\n        # init anchors\n        anchors = rng.uniform(lb, ub, size=(self.pop, dim))\n        af = np.array([safe_eval(anchors[i]) for i in range(self.pop)])\n        steps = np.full((self.pop, dim), base_step)\n        corr = np.zeros((dim, dim), dtype=float)   # symmetric pairwise success counts\n        # simple per-dim momentum sign memory\n        mem = np.zeros((self.pop, dim), dtype=float)\n\n        ibest = int(np.argmin(af))\n        xbest = anchors[ibest].copy(); fbest = float(af[ibest])\n\n        while evals < budget:\n            # choose anchor: bias to worse anchors to promote repair\n            worst = af.max()\n            weights = (worst - af) + 1e-9\n            p = weights / weights.sum()\n            idx = int(rng.choice(self.pop, p=p))\n\n            x0 = anchors[idx].copy(); f0 = af[idx]\n            # coordinate importance from anchor cloud: var * mean step\n            var = anchors.var(axis=0)\n            importance = var + 1e-12\n            importance *= (steps[idx] / (steps[idx].mean() + 1e-12))\n\n            # decide single or pair probe\n            if rng.random() < self.pair_prob and dim >= 2:\n                # pick pair: prefer recorded correlations or joint importance\n                # candidate pairs sampled, pick best by score\n                K = min(6, dim*(dim-1)//2)\n                best_pair = (0,1)\n                best_score = -1.0\n                for _ in range(K):\n                    i = int(rng.choice(dim, p=(importance/importance.sum())))\n                    j = int(rng.choice(dim, p=(importance/importance.sum())))\n                    if i == j: continue\n                    score = corr[i,j] + importance[i]*importance[j]\n                    if score > best_score:\n                        best_score = score; best_pair = (i,j)\n                i,j = best_pair\n                # decide signs: try to move toward best anchor's relative coords when plausible\n                s_i = -1 if xbest[i] < x0[i] else 1\n                s_j = -1 if xbest[j] < x0[j] else 1\n                # small random flip chance\n                if rng.random() < 0.25:\n                    s_i *= -1\n                if rng.random() < 0.25:\n                    s_j *= -1\n                # step lengths scaled by per-dim step\n                dx = np.zeros(dim)\n                dx[i] = s_i * steps[idx, i]\n                dx[j] = s_j * steps[idx, j]\n                xt = np.clip(x0 + dx, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f0:\n                    anchors[idx] = xt; af[idx] = ft\n                    steps[idx, i] = min(steps[idx, i]*self.succ, box_len)\n                    steps[idx, j] = min(steps[idx, j]*self.succ, box_len)\n                    corr[i,j] += 1; corr[j,i] += 1\n                    mem[idx, i] = 0.8*mem[idx, i] + 0.2*s_i\n                    mem[idx, j] = 0.8*mem[idx, j] + 0.2*s_j\n                else:\n                    steps[idx, i] = max(steps[idx, i]*self.fail, 1e-12*box_len)\n                    steps[idx, j] = max(steps[idx, j]*self.fail, 1e-12*box_len)\n                    mem[idx, i] *= 0.85; mem[idx, j] *= 0.85\n            else:\n                # single-coordinate probe\n                probs = importance / importance.sum()\n                c = int(rng.choice(dim, p=probs))\n                # prefer direction suggested by memory or by best\n                prefer = 0.5 + 0.45 * np.tanh(abs(mem[idx,c]))\n                if rng.random() < prefer:\n                    sign = 1 if mem[idx,c] >= 0 else -1\n                else:\n                    # move toward best if that would change coordinate\n                    sign = -1 if xbest[c] < x0[c] else 1\n                    if rng.random() < 0.15:\n                        sign *= -1\n                dx = np.zeros(dim); dx[c] = sign * steps[idx, c]\n                xt = np.clip(x0 + dx, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f0:\n                    anchors[idx] = xt; af[idx] = ft\n                    steps[idx, c] = min(steps[idx, c]*self.succ, box_len)\n                    mem[idx, c] = 0.9*mem[idx, c] + 0.1*sign\n                    # update pairwise signal using best as reference (if different)\n                    diff = (xt - xbest)\n                    # for coords with same sign change, boost corr\n                    for k in range(dim):\n                        if k != c and diff[k]*diff[c] > 0:\n                            corr[c,k] += 0.25; corr[k,c] += 0.25\n                else:\n                    steps[idx, c] = max(steps[idx, c]*self.fail, 1e-12*box_len)\n                    mem[idx, c] *= 0.8\n\n            # occasional respawn of very stale/worst anchors\n            if rng.random() < self.respawn_prob and evals < budget:\n                iw = int(np.argmax(af))\n                rc = int(rng.integers(0, dim))\n                newp = anchors[iw].copy()\n                newp[rc] = rng.uniform(lb[rc], ub[rc])\n                fn = safe_eval(newp)\n                anchors[iw] = newp; af[iw] = fn\n                steps[iw, rc] = base_step\n                mem[iw, rc] = 0.0\n\n            # update global best\n            ibest = int(np.argmin(af))\n            if af[ibest] < fbest:\n                fbest = float(af[ibest]); xbest = anchors[ibest].copy()\n\n            # light clipping of steps\n            steps = np.clip(steps, 1e-12*box_len, 1.0*box_len)\n\n            # early finalize when budget nearly exhausted\n            if budget - evals < max(6, dim):\n                break\n\n        # small final single-dim polish around xbest\n        mesh = 0.08 * box_len\n        while evals < budget and mesh > 1e-8*box_len:\n            improved = False\n            for d in range(dim):\n                if evals >= budget: break\n                xp = xbest.copy(); xp[d] = np.clip(xp[d] + mesh, lb[d], ub[d])\n                fp = safe_eval(xp)\n                if fp < fbest:\n                    fbest = fp; xbest = xp.copy(); improved = True; continue\n                xm = xbest.copy(); xm[d] = np.clip(xm[d] - mesh, lb[d], ub[d])\n                fm = safe_eval(xm)\n                if fm < fbest:\n                    fbest = fm; xbest = xm.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.x_opt = xbest\n        self.f_opt = float(fbest)\n        return self.f_opt, np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 15, "feedback": "The algorithm SparseDirectionalAnchorSearch scored 0.588 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6cd3ea29-a941-4770-ab4b-0e5e0cca3077"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9536248503818778}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9357813106811262}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.929594325053664}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8898951570817106}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8984745399731777}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.894577102214599}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06373821757041442}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.07993839848513928}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.04755257194665263}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0406093650091619}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07530011730555852}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07042677094133398}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.988309118643549}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9731540983547962}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9750723766646546}], "aucs": [0.9536248503818778, 0.9357813106811262, 0.929594325053664, 0.8898951570817106, 0.8984745399731777, 0.894577102214599, 0.06373821757041442, 0.07993839848513928, 0.04755257194665263, 0.0406093650091619, 0.07530011730555852, 0.07042677094133398, 0.988309118643549, 0.9731540983547962, 0.9750723766646546]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1968.0, "Edges": 1967.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9989837398373984, "Degree Variance": 2.2449176664022734, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.153421633554084, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3037966167283865, "Depth Entropy": 2.040532832265349, "Assortativity": 0.0, "Average Eccentricity": 15.220528455284553, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0005081300813008131, "Average Shortest Path": 9.56044242191278, "mean_complexity": 11.666666666666666, "total_complexity": 35.0, "mean_token_count": 545.6666666666666, "total_token_count": 1637.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "7a0a043e-4f30-49e3-b8c2-d79e059cbdb1", "fitness": 0.6112654830404445, "name": "MicroMomentumSingleCoordinateSearchV2", "description": "Micro-Momentum Single-Coordinate Search v2 \u2014 strict 1-D probes with per-anchor per-dim momentum + EWMA success signals, immediate follow-through pushes on wins, and lightweight Cauchy respawns for robust escapes.", "code": "import numpy as np\n\nclass MicroMomentumSingleCoordinateSearchV2:\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.12, success_mult=1.35, failure_mult=0.55,\n                 heavy_prob=0.025, respawn_prob=0.12, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        if pop_size is None:\n            self.pop_size = max(2, min(8, int(3 + 0.04 * self.dim)))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.heavy_prob = float(heavy_prob)\n        self.respawn_prob = float(respawn_prob)\n\n    def _bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step_frac * box_len)\n        evals = 0\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # init anchors: one full-dim sample + single-coordinate variants\n        anchors = np.empty((self.pop_size, dim), float)\n        vals = np.full(self.pop_size, np.inf)\n        base = rng.uniform(lb, ub)\n        anchors[0] = base\n        vals[0] = safe_eval(anchors[0])\n        for i in range(1, self.pop_size):\n            if evals >= self.budget:\n                anchors[i] = anchors[0].copy(); vals[i] = vals[0]; continue\n            a = anchors[0].copy()\n            c = rng.integers(0, dim)\n            a[c] = rng.uniform(lb[c], ub[c])\n            anchors[i] = a\n            vals[i] = safe_eval(a)\n\n        steps = np.full((self.pop_size, dim), base_step)\n        momentum = np.zeros((self.pop_size, dim), float)  # [-1,1] sign bias\n        ewma_gain = np.full((self.pop_size, dim), 0.0)    # positive avg improvement magnitude\n        stagn = np.zeros(self.pop_size, int)\n\n        ibest = int(np.nanargmin(vals))\n        xbest = anchors[ibest].copy(); fbest = float(vals[ibest])\n\n        # main loop: single-coordinate atomic moves\n        while evals < self.budget:\n            # occasional heavy one-d jump from best\n            if rng.random() < self.heavy_prob and evals < self.budget:\n                c = rng.integers(0, dim)\n                u = rng.random(); cauchy = np.tan(np.pi * (u - 0.5))\n                jump = 0.2 * box_len * cauchy\n                xp = xbest.copy(); xp[c] = np.clip(xbest[c] + jump, lb[c], ub[c])\n                fp = safe_eval(xp)\n                if fp < fbest:\n                    fbest = fp; xbest = xp.copy()\n                    # replace worst anchor\n                    iw = int(np.nanargmax(vals))\n                    anchors[iw] = xp.copy(); vals[iw] = fp\n                    steps[iw,:] = base_step; momentum[iw,:] = 0.0; stagn[iw]=0\n\n            order = rng.permutation(self.pop_size)\n            improved = False\n            for idx in order:\n                if evals >= self.budget: break\n                x0 = anchors[idx].copy(); f0 = vals[idx]\n                # priority per-dim: step * (1 + |mom|*alpha) * (1 + ewma_gain_scaled) * (1 + stagn_factor)\n                mom = np.abs(momentum[idx])\n                gain = ewma_gain[idx] / (1e-12 + (np.max(ewma_gain[idx]) if np.max(ewma_gain[idx])>0 else 1.0))\n                stagn_factor = 1.0 + (stagn[idx] / (4.0 + dim))\n                priority = steps[idx] * (1.0 + 0.7 * mom) * (1.0 + 1.2 * gain) * stagn_factor\n                if np.sum(priority) <= 0:\n                    c = rng.integers(0, dim)\n                else:\n                    p = priority / np.sum(priority)\n                    # robust small-noise sampling\n                    c = rng.choice(dim, p=p)\n                m = momentum[idx, c]\n                pref = 1 if m >= 0 else -1\n                sign_prob = 0.5 + 0.45 * min(1.0, abs(m))\n                sign = pref if rng.random() < sign_prob else (1 if rng.random() < 0.5 else -1)\n                d = steps[idx, c]\n                if d <= 1e-12 * box_len:\n                    stagn[idx] += 1\n                    # small bump respawn occasionally\n                    if rng.random() < 0.05 and evals < self.budget:\n                        newp = anchors[idx].copy()\n                        rc = rng.integers(0, dim)\n                        newp[rc] = rng.uniform(lb[rc], ub[rc])\n                        fn = safe_eval(newp)\n                        anchors[idx] = newp; vals[idx] = fn; steps[idx,rc]=base_step; momentum[idx,rc]=0.0\n                        if fn < fbest: fbest=fn; xbest=newp.copy(); improved=True\n                    continue\n\n                xt = x0.copy()\n                xt[c] = np.clip(x0[c] + sign * d, lb[c], ub[c])\n                ft = safe_eval(xt)\n                if ft < f0:\n                    # accept\n                    anchors[idx] = xt.copy(); vals[idx] = ft\n                    # adapt step and momentum\n                    steps[idx, c] = min(steps[idx, c] * self.success_mult, box_len)\n                    momentum[idx, c] = np.clip(0.7 * momentum[idx, c] + 0.3 * float(sign), -1.0, 1.0)\n                    # ewma of absolute improvement (positive)\n                    imp = max(0.0, f0 - ft)\n                    ewma_gain[idx, c] = 0.6 * ewma_gain[idx, c] + 0.4 * imp\n                    stagn[idx] = 0\n                    improved = True\n                    # immediate follow-through push in same direction (aggressive single-dim exploit)\n                    if evals < self.budget:\n                        d2 = min(steps[idx, c] * 1.6, box_len)\n                        x2 = anchors[idx].copy()\n                        x2[c] = np.clip(anchors[idx, c] + sign * d2, lb[c], ub[c])\n                        f2 = safe_eval(x2)\n                        if f2 < vals[idx]:\n                            anchors[idx] = x2.copy(); vals[idx] = f2\n                            steps[idx, c] = min(steps[idx, c] * self.success_mult, box_len)\n                            momentum[idx, c] = np.clip(0.7 * momentum[idx, c] + 0.3 * float(sign), -1.0, 1.0)\n                            ewma_gain[idx, c] = 0.6 * ewma_gain[idx, c] + 0.4 * max(0.0, ft - f2)\n                else:\n                    # failure\n                    steps[idx, c] = max(steps[idx, c] * self.failure_mult, 1e-12 * box_len)\n                    momentum[idx, c] *= 0.86\n                    stagn[idx] += 1\n\n                # update global best\n                if vals[idx] < fbest:\n                    fbest = float(vals[idx]); xbest = anchors[idx].copy()\n                # stagnation handling\n                if stagn[idx] > max(6, 3*dim) and evals < self.budget:\n                    stagn[idx] = 0\n                    if rng.random() < self.respawn_prob:\n                        rc = rng.integers(0, dim)\n                        newp = anchors[idx].copy(); newp[rc] = rng.uniform(lb[rc], ub[rc])\n                        fn = safe_eval(newp)\n                        anchors[idx] = newp; vals[idx] = fn\n                        steps[idx, :] = np.maximum(steps[idx, :], base_step * 0.2)\n                        momentum[idx, :] = 0.0\n                        if fn < fbest: fbest=fn; xbest=newp.copy(); improved=True\n                    else:\n                        rc = rng.integers(0, dim)\n                        u = rng.random(); cauchy = np.tan(np.pi*(u-0.5))\n                        jump = 0.16 * box_len * cauchy\n                        newp = anchors[idx].copy()\n                        newp[rc] = np.clip(anchors[idx, rc] + jump, lb[rc], ub[rc])\n                        fn = safe_eval(newp)\n                        anchors[idx] = newp; vals[idx] = fn\n                        steps[idx, rc] = base_step; momentum[idx, rc] = 0.0\n                        if fn < fbest: fbest=fn; xbest=newp.copy(); improved=True\n\n            # clip steps\n            steps = np.clip(steps, 1e-10 * box_len, box_len)\n            # if no improvement, occasional targeted opposition on worst anchor\n            if (not improved) and (evals < self.budget) and rng.random() < 0.15:\n                iw = int(np.nanargmax(vals))\n                rc = rng.integers(0, dim)\n                opp = anchors[iw].copy()\n                oppv = lb[rc] + ub[rc] - anchors[iw, rc]\n                opp[rc] = np.clip(oppv + 0.02*(ub[rc]-lb[rc])*rng.standard_normal(), lb[rc], ub[rc])\n                fo = safe_eval(opp)\n                anchors[iw] = opp; vals[iw] = fo\n                steps[iw, rc] = base_step; momentum[iw, rc] = 0.0\n                if fo < fbest: fbest = fo; xbest = opp.copy(); improved = True\n\n            # quick exit to final refine if few evals left\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final single-coord mesh refinement around best\n        mesh = 0.08 * box_len; mesh_min = 1e-8 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            any_imp = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = xbest.copy(); xt[d] = np.clip(xbest[d] + mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fbest:\n                    fbest = ft; xbest = xt.copy(); any_imp = True; continue\n                if evals >= self.budget: break\n                xt = xbest.copy(); xt[d] = np.clip(xbest[d] - mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fbest:\n                    fbest = ft; xbest = xt.copy(); any_imp = True\n            if not any_imp:\n                mesh *= 0.5\n\n        self.f_opt = float(fbest); self.x_opt = np.asarray(xbest, float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 16, "feedback": "The algorithm MicroMomentumSingleCoordinateSearchV2 scored 0.611 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6cd3ea29-a941-4770-ab4b-0e5e0cca3077"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9460123955825066}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9439052703972008}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9416350115603189}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.914105033567467}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9064210457489946}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9162857472290746}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.11492340639866083}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.09343108998232497}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.09715273181829376}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.11333931022054589}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.10628915442010167}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.12552569618523446}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9875168484600372}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9785219147072656}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.983917589328641}], "aucs": [0.9460123955825066, 0.9439052703972008, 0.9416350115603189, 0.914105033567467, 0.9064210457489946, 0.9162857472290746, 0.11492340639866083, 0.09343108998232497, 0.09715273181829376, 0.11333931022054589, 0.10628915442010167, 0.12552569618523446, 0.9875168484600372, 0.9785219147072656, 0.983917589328641]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2797.0, "Edges": 2796.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9992849481587416, "Degree Variance": 2.149445323523889, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.525581395348837, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3174595044665423, "Depth Entropy": 2.122736755547038, "Assortativity": 2.5507855720330377e-08, "Average Eccentricity": 18.378977475867, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003575259206292456, "Average Shortest Path": 10.49368652188657, "mean_complexity": 12.5, "total_complexity": 50.0, "mean_token_count": 596.0, "total_token_count": 2384.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "a27331f4-6f07-4524-95e5-91ccd01efc4c", "fitness": 0.08809606054020508, "name": "EnsembleGuidedCoordinateRecombination", "description": "Ensemble Guided Coordinate-Recombination (EGCR) \u2014 a tiny ensemble of candidate points that mix cheap coordinate-wise pulls, pairwise mid-point recombinations and focused per-dimension credit assignment to rapidly concentrate evaluations in promising subspaces.", "code": "import numpy as np\n\nclass EnsembleGuidedCoordinateRecombination:\n    \"\"\"\n    Ensemble Guided Coordinate-Recombination (EGCR)\n\n    Compact ensemble of cheap local movers:\n      - maintain a small pool of candidates (ensemble)\n      - mostly single-coordinate signed pulls driven by per-dim credits (cheap, 1-eval)\n      - occasional pairwise mid-point recombination + tiny jitter to explore promising subspaces\n      - multiplicative step adaptation per-dimension and per-candidate\n      - replace-worst or replace-self acceptance to keep pool diverse while focusing\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, pool_size=6, init_step=None,\n                 min_step=1e-7, max_step=5.0, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pool_size = max(2, int(pool_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _get_bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb); ub = np.array(ub)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb.astype(float), ub.astype(float)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.size != self.dim:\n            lb = np.full(self.dim, float(lb.ravel()[0]))\n        if ub.size != self.dim:\n            ub = np.full(self.dim, float(ub.ravel()[0]))\n\n        evals = 0\n        P = min(self.pool_size, max(2, self.budget))\n        # init ensemble: random samples (use at least one random + one center)\n        xs = lb + self.rng.random((P, self.dim)) * (ub - lb)\n        fs = np.empty(P, dtype=float)\n        for i in range(P):\n            fs[i] = float(func(xs[i]))\n            evals += 1\n            if evals >= self.budget:\n                # return best so far\n                idx = int(np.argmin(fs[:i+1]))\n                return float(fs[idx]), xs[idx].copy()\n\n        # bookkeeping\n        best_idx = int(np.argmin(fs))\n        best_x = xs[best_idx].copy()\n        best_f = float(fs[best_idx])\n\n        # steps: per-dim base scale (shared) and per-candidate multiplier\n        if self.init_step is None:\n            base = np.full(self.dim, (ub - lb).mean() / 8.0)\n        else:\n            base = np.array(self.init_step, dtype=float)\n            if base.shape == ():\n                base = np.full(self.dim, float(base))\n            if base.size != self.dim:\n                base = np.full(self.dim, float(np.mean(base)))\n        base = np.clip(base, self.min_step, self.max_step)\n        cand_mult = np.ones(P)\n        # per-dim credits (higher => picked more often)\n        credits = np.ones(self.dim)\n        # momentum sign bias per-dim (small)\n        bias = np.zeros(self.dim)\n\n        up = 1.35; down = 0.7\n        replace_prob = 0.15  # when recombining, replace worst sometimes\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # pick an actor candidate biased to better fitness\n            fnorm = fs - fs.min()\n            # temperature scales with spread\n            temp = max(1e-3, fnorm.std() + 1e-3)\n            scores = np.exp(-fnorm / temp)\n            probs = scores / scores.sum()\n            actor = int(self.rng.choice(P, p=probs))\n\n            # choose move: most often single-dim pull, else recombine, else global jitter\n            r = self.rng.random()\n            improved = False\n\n            if r < 0.72:\n                # single-dimension signed pull\n                p = credits / credits.sum()\n                dim = int(self.rng.choice(self.dim, p=p))\n                # determine sign: use bias with random flip occasionally\n                if abs(bias[dim]) < 1e-6:\n                    sgn = 1.0 if self.rng.random() < 0.5 else -1.0\n                else:\n                    sgn = np.sign(bias[dim]) if self.rng.random() < 0.85 else -np.sign(bias[dim])\n\n                delta = sgn * base[dim] * cand_mult[actor]\n                x_c = xs[actor].copy()\n                x_c[dim] = np.clip(x_c[dim] + delta, lb[dim], ub[dim])\n                f_c = float(func(x_c)); evals += 1\n\n                if f_c < fs[actor]:\n                    # accept into actor; reward\n                    fs[actor] = f_c; xs[actor] = x_c\n                    improved = True\n                    # grow step for that dim and candidate\n                    cand_mult[actor] = min(10.0, cand_mult[actor] * 1.12)\n                    base[dim] = min(self.max_step, base[dim] * up)\n                    credits[dim] *= 1.5\n                    bias[dim] = 0.9 * bias[dim] + 0.5 * sgn\n                else:\n                    # failure: shrink\n                    cand_mult[actor] = max(1e-3, cand_mult[actor] * 0.85)\n                    base[dim] = max(self.min_step, base[dim] * down)\n                    credits[dim] = max(1e-4, credits[dim] * 0.85)\n                    bias[dim] *= 0.7  # decay\n\n            elif r < 0.92 and P >= 2:\n                # pairwise recombination: midpoint plus tiny jitter\n                other = int(self.rng.choice([i for i in range(P) if i != actor]))\n                alpha = self.rng.normal(0.5, 0.18)  # allows slight extrapolation\n                alpha = float(np.clip(alpha, -0.5, 1.5))\n                base_move = (1 - alpha) * xs[actor] + alpha * xs[other]\n                jitter = self.rng.normal(0.0, 0.6, size=self.dim) * base * cand_mult[actor] * 0.4\n                x_c = np.clip(base_move + jitter, lb, ub)\n                f_c = float(func(x_c)); evals += 1\n\n                # replace policy: if better than actor accept; else occasionally replace worst\n                if f_c < fs[actor]:\n                    fs[actor] = f_c; xs[actor] = x_c; improved = True\n                    # reward dims proportional to movement magnitude\n                    mv = np.abs(x_c - xs[actor])\n                    w = (mv / (mv.mean() + 1e-12))\n                    base = np.clip(base * (1 + 0.12 * w), self.min_step, self.max_step)\n                    credits += 0.2 * (w / (w.sum() + 1e-12))\n                    cand_mult[actor] = min(10.0, cand_mult[actor] * 1.05)\n                else:\n                    if self.rng.random() < replace_prob:\n                        worst = int(np.argmax(fs))\n                        if f_c < fs[worst]:\n                            fs[worst] = f_c; xs[worst] = x_c; improved = True\n                            cand_mult[worst] = max(1e-3, cand_mult[worst] * 0.9)\n                            credits *= 0.98\n\n            else:\n                # global jitter / opposition / random restart (diversify)\n                if self.rng.random() < 0.5:\n                    # opposition around actor\n                    x_c = np.clip(lb + ub - xs[actor] + self.rng.normal(0, 0.03, size=self.dim) * base, lb, ub)\n                else:\n                    x_c = lb + self.rng.random(self.dim) * (ub - lb)\n                f_c = float(func(x_c)); evals += 1\n\n                # replace worst if improved\n                worst = int(np.argmax(fs))\n                if f_c < fs[worst]:\n                    fs[worst] = f_c; xs[worst] = x_c; improved = True\n                    cand_mult[worst] = 1.0\n                    credits *= 0.995\n                # weak encouragement otherwise: slightly expand steps to escape\n                else:\n                    base = np.clip(base * 1.05, self.min_step, self.max_step)\n\n            # update global best\n            cur_best_idx = int(np.argmin(fs))\n            if fs[cur_best_idx] < best_f:\n                best_f = float(fs[cur_best_idx]); best_x = xs[cur_best_idx].copy()\n\n            # mild normalization of credits to avoid explosion\n            if credits.max() > 1e3:\n                credits *= 1e-3\n            if credits.sum() == 0:\n                credits += 1.0\n\n            # ensure numerical bounds\n            base = np.clip(base, self.min_step, self.max_step)\n            cand_mult = np.clip(cand_mult, 1e-6, 1e2)\n\n        return float(best_f), best_x.copy()", "configspace": "", "generation": 16, "feedback": "The algorithm EnsembleGuidedCoordinateRecombination scored 0.088 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f848c7e0-b9e3-4731-969d-4bbca80a2964"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.2675946721521756}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.16989808174512233}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.3096665663829832}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.007079970032662253}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.000976905152198948}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.030020713728456605}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.019318836277979257}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0033678577158762213}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0030512186002655817}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.15826421290760573}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.06670005328583462}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.28535182012191607}], "aucs": [0.2675946721521756, 0.16989808174512233, 0.3096665663829832, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.007079970032662253, 0.000976905152198948, 0.030020713728456605, 0.019318836277979257, 0.0033678577158762213, 0.0030512186002655817, 0.15826421290760573, 0.06670005328583462, 0.28535182012191607]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1848.0, "Edges": 1847.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.998917748917749, "Degree Variance": 1.9361460148797809, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.422802850356295, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3258868420034786, "Depth Entropy": 2.1978498967972633, "Assortativity": 0.0, "Average Eccentricity": 17.213203463203463, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0005411255411255411, "Average Shortest Path": 10.284415232845118, "mean_complexity": 11.0, "total_complexity": 33.0, "mean_token_count": 556.3333333333334, "total_token_count": 1669.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "82379331-5855-488c-a7d5-53d7afbbd2bb", "fitness": 0.6207325518891316, "name": "MirroredAdaptiveQuadratic", "description": "Mirrored Adaptive Quadratic Descent \u2014 use cheap symmetric (mirrored) coordinate probes to estimate 1-D curvature and improvement, adapt per-coordinate step-sizes multiplicatively, accept extrapolations on detected negative curvature, and perform rare heavy-tailed global jumps for diversification.", "code": "import numpy as np\n\nclass MirroredAdaptiveQuadratic:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 success_mult=1.3, failure_mult=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, extrapolate_gain=1.6,\n                 polish_frac=0.04, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob)\n        self.extrapolate_gain = float(extrapolate_gain)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box_scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = max(1e-12, self.max_step_frac * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base, float)\n        trials = np.zeros(dim, int); succ = np.zeros(dim, int)\n        curv = np.zeros(dim, float)  # running estimate of curvature along coord\n        alpha = 0.14  # curvature EWMA\n\n        while evals < self.budget:\n            # rare heavy-tailed global diversification\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim)\n                jump = 0.12 * box_scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                continue\n\n            # select coordinate: favor larger step and less-tried dims\n            scores = steps * (1.0 + 1.0 / (1 + trials))\n            d = int(np.argmax(scores + 1e-12 * rng.random(dim)))\n\n            s = float(np.clip(steps[d], min_step, max_step))\n            # if we have >=2 budget, do mirrored probe to get curvature estimate\n            if evals <= self.budget - 2:\n                xp = x_best.copy(); xm = x_best.copy()\n                xp[d] = np.clip(xp[d] + s, lb[d], ub[d])\n                xm[d] = np.clip(xm[d] - s, lb[d], ub[d])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[d] += 2\n                # acceptance logic\n                if fp < f_best or fm < f_best:\n                    if fp < fm:\n                        x_best, f_best = xp.copy(), float(fp)\n                        succ[d] += 1\n                        steps[d] = min(max_step, steps[d] * self.success_mult)\n                    else:\n                        x_best, f_best = xm.copy(), float(fm)\n                        succ[d] += 1\n                        steps[d] = min(max_step, steps[d] * self.success_mult)\n                    # small opportunistic extrapolation on improvement and available budget\n                    if evals < self.budget:\n                        ext = x_best.copy()\n                        sign = np.sign(x_best[d] - (xp[d] if fp<fm else xm[d]))\n                        if sign == 0: sign = 1.0\n                        ext[d] = np.clip(ext[d] + sign * s * self.extrapolate_gain, lb[d], ub[d])\n                        fext = safe_eval(ext)\n                        if fext < f_best:\n                            x_best, f_best = ext.copy(), float(fext)\n                        trials[d] += 1\n                else:\n                    # no improvement: estimate curvature and adapt\n                    f0_approx = f_best  # center value (may not be exactly f at center)\n                    est_curv = (fp + fm - 2.0 * f0_approx) / (s * s + 1e-24)\n                    curv[d] = (1 - alpha) * curv[d] + alpha * est_curv\n                    # if negative curvature (valley along direction), try extrapolation once if budget allows\n                    if est_curv < -1e-12 and evals < self.budget:\n                        sign = -np.sign(est_curv)  # positive if valley opens with decrease both sides? keep simple\n                        ext = x_best.copy()\n                        ext[d] = np.clip(ext[d] + sign * s * self.extrapolate_gain, lb[d], ub[d])\n                        fext = safe_eval(ext); trials[d] += 1\n                        if fext < f_best:\n                            x_best, f_best = ext.copy(), float(fext)\n                            succ[d] += 1\n                            steps[d] = min(max_step, steps[d] * self.success_mult)\n                        else:\n                            steps[d] = max(min_step, steps[d] * self.failure_mult)\n                    else:\n                        steps[d] = max(min_step, steps[d] * self.failure_mult)\n            else:\n                # single-eval remaining: try one-sided best direction by sampling plus or minus\n                if rng.random() < 0.5:\n                    xt = x_best.copy(); xt[d] = np.clip(xt[d] + s, lb[d], ub[d])\n                else:\n                    xt = x_best.copy(); xt[d] = np.clip(xt[d] - s, lb[d], ub[d])\n                ft = safe_eval(xt); trials[d] += 1\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); succ[d] += 1\n                    steps[d] = min(max_step, steps[d] * self.success_mult)\n                else:\n                    steps[d] = max(min_step, steps[d] * self.failure_mult)\n\n            # occasional mild reset of steps for very stale coordinates\n            if trials[d] > 80 and succ[d] == 0:\n                steps[d] = min(max_step, steps[d] * 1.5)\n                trials[d] = int(trials[d] * 0.6)\n\n        # final symmetric polish: small probes per-dim while budget remains\n        fin = self.polish_frac * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 16, "feedback": "The algorithm MirroredAdaptiveQuadratic scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a569da3-dff0-437a-a1e1-9e9baee72739"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9749455969129244}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9733331414150619}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9749816034684525}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9411641342669269}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.946315672349232}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9408127934632399}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.058242699589996616}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08503798382736116}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.10380402157751267}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.13857349046109335}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.09906060574944242}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0847973558246049}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9974364275395938}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9962851802409689}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.996197571650562}], "aucs": [0.9749455969129244, 0.9733331414150619, 0.9749816034684525, 0.9411641342669269, 0.946315672349232, 0.9408127934632399, 0.058242699589996616, 0.08503798382736116, 0.10380402157751267, 0.13857349046109335, 0.09906060574944242, 0.0847973558246049, 0.9974364275395938, 0.9962851802409689, 0.996197571650562]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1855.0, "Edges": 1854.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.998921832884097, "Degree Variance": 2.0420473550758858, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.346650998824911, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3257339812190103, "Depth Entropy": 2.131438263095731, "Assortativity": 1.4230032046475274e-08, "Average Eccentricity": 15.635579514824798, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0005390835579514825, "Average Shortest Path": 10.154889697223458, "mean_complexity": 12.0, "total_complexity": 36.0, "mean_token_count": 523.3333333333334, "total_token_count": 1570.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "1a1a79ab-60bf-4999-8701-a662a670de18", "fitness": 0.19949391351216522, "name": "DiagonalQuadraticExplorer", "description": "Diagonal-Quadratic Explorer (DQE) \u2014 fit cheap 1-D quadratics per coordinate from three local probes, take analytic minimizers (when reliable), and occasionally do tiny 2-D diagonal scans plus Cauchy escapes; adapt per-dim steps by simple grow/shrink.", "code": "import numpy as np\n\nclass DiagonalQuadraticExplorer:\n    def __init__(self, budget=1000, dim=10, init_step=0.8, min_step=1e-6, max_step=2.5,\n                 grow=1.25, shrink=0.6, patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.patience = int(patience)\n        self.rng = np.random.default_rng() if rng is None else (rng if isinstance(rng, np.random.Generator) else np.random.default_rng(rng))\n\n    def _bounds(self, func):\n        lb, ub = -5.0, 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == (): lb = np.full(self.dim, float(lb))\n        if ub.shape == (): ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        lb, ub = np.broadcast_to(lb, (self.dim,)), np.broadcast_to(ub, (self.dim,))\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n\n        if self.budget <= 0:\n            x0 = clip(0.5 * (lb + ub))\n            return float(func(x0)), x0.copy()\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # init\n        x_best = clip(lb + self.rng.random(self.dim) * (ub - lb))\n        f_best = float(func(x_best)); evals += 1\n\n        step = np.full(self.dim, self.init_step, dtype=float)\n        f_plus = np.full(self.dim, np.nan)\n        f_minus = np.full(self.dim, np.nan)\n        anchor = x_best.copy()  # these f_plus/f_minus are relative to this anchor\n        stagn = 0\n\n        eye = np.eye(self.dim)\n        it = 0\n        while evals < self.budget:\n            it += 1\n            improved = False\n            remaining = self.budget - evals\n\n            # invalidate cache if anchor moved\n            if not np.allclose(anchor, x_best, atol=1e-12):\n                f_plus.fill(np.nan); f_minus.fill(np.nan); anchor = x_best.copy()\n\n            r = self.rng.random()\n            # mostly 1-D quadratic fits\n            if r < 0.68:\n                # pick coordinate: prefer ones with large finite diff if present\n                grads = np.zeros(self.dim)\n                known = (~np.isnan(f_plus)) & (~np.isnan(f_minus))\n                h = step.copy()\n                grads[known] = (f_plus[known] - f_minus[known]) / (2.0 * h[known])\n                probs = np.abs(grads) + 1e-12\n                if probs.sum() == 0 or self.rng.random() < 0.15:\n                    idx = int(self.rng.integers(self.dim))\n                else:\n                    probs /= probs.sum()\n                    idx = int(self.rng.choice(self.dim, p=probs))\n\n                hi = max(step[idx], 1e-12)\n                # ensure we have both side probes when budget allows\n                if np.isnan(f_plus[idx]) and remaining >= 1:\n                    x_p = clip(x_best + hi * eye[idx])\n                    fp = safe_eval(x_p)\n                    if fp is None: break\n                    f_plus[idx] = fp\n                if np.isnan(f_minus[idx]) and (self.budget - evals) >= 1:\n                    x_m = clip(x_best - hi * eye[idx])\n                    fm = safe_eval(x_m)\n                    if fm is None: break\n                    f_minus[idx] = fm\n\n                # if both available, fit quadratic\n                if (not np.isnan(f_plus[idx])) and (not np.isnan(f_minus[idx])):\n                    f0 = f_best\n                    fp = f_plus[idx]; fm = f_minus[idx]\n                    denom = (fp + fm - 2.0 * f0)\n                    xstar_off = None\n                    if abs(denom) > 1e-12:\n                        xstar_off = hi * (fm - fp) / (2.0 * denom)  # formula: h*(f- - f+) / (2*(f+ + f- -2f0))\n                    # guard magnitude: don't extrapolate wildly\n                    if xstar_off is None or not np.isfinite(xstar_off) or abs(xstar_off) > 3.0 * hi:\n                        # fallback: take best of the two neighbors\n                        if fp < f_best or fm < f_best:\n                            if fp < fm:\n                                cand = clip(x_best + hi * eye[idx]); fc = fp\n                            else:\n                                cand = clip(x_best - hi * eye[idx]); fc = fm\n                            if fc < f_best:\n                                x_best = cand.copy(); f_best = float(fc); improved = True\n                                step[idx] = min(self.max_step, step[idx] * self.grow)\n                            else:\n                                step[idx] = max(self.min_step, step[idx] * self.shrink)\n                        else:\n                            step[idx] = max(self.min_step, step[idx] * self.shrink)\n                    else:\n                        x_try = clip(x_best + xstar_off * eye[idx])\n                        f_try = safe_eval(x_try)\n                        if f_try is None: break\n                        if f_try < f_best:\n                            x_best = x_try.copy(); f_best = f_try; improved = True\n                            step[idx] = min(self.max_step, step[idx] * self.grow)\n                        else:\n                            # maybe one neighbor was better\n                            if fp < f_best:\n                                x_best = clip(x_best + hi * eye[idx]); f_best = fp; improved = True\n                                step[idx] = min(self.max_step, step[idx] * self.grow)\n                            elif fm < f_best:\n                                x_best = clip(x_best - hi * eye[idx]); f_best = fm; improved = True\n                                step[idx] = min(self.max_step, step[idx] * self.grow)\n                            else:\n                                step[idx] = max(self.min_step, step[idx] * self.shrink)\n                else:\n                    # not enough budget to form quadratic: try one probe direction if available\n                    if (not np.isnan(f_plus[idx])) and f_plus[idx] < f_best:\n                        x_best = clip(x_best + hi * eye[idx]); f_best = f_plus[idx]; improved = True\n                    elif (not np.isnan(f_minus[idx])) and f_minus[idx] < f_best:\n                        x_best = clip(x_best - hi * eye[idx]); f_best = f_minus[idx]; improved = True\n                    else:\n                        # try a single random probe\n                        if remaining >= 1:\n                            sign = 1.0 if self.rng.random() < 0.5 else -1.0\n                            x_try = clip(x_best + sign * hi * eye[idx])\n                            f_try = safe_eval(x_try)\n                            if f_try is None: break\n                            if sign > 0:\n                                f_plus[idx] = f_try\n                            else:\n                                f_minus[idx] = f_try\n                            if f_try < f_best:\n                                x_best = x_try.copy(); f_best = f_try; improved = True\n                                step[idx] = min(self.max_step, step[idx] * self.grow)\n                            else:\n                                step[idx] = max(self.min_step, step[idx] * self.shrink)\n\n            # sparse 2-D diagonal scan\n            elif r < 0.92:\n                if self.dim < 2:\n                    continue\n                # pick a strong and a random\n                weights = np.abs((np.nan_to_num((f_plus - f_minus) / (2.0 * step))) )\n                if weights.sum() == 0 or self.rng.random() < 0.2:\n                    i = int(self.rng.integers(self.dim))\n                else:\n                    p = weights + 1e-12\n                    p /= p.sum()\n                    i = int(self.rng.choice(self.dim, p=p))\n                j = int(self.rng.integers(self.dim - 1))\n                if j >= i: j += 1\n                hi = step[i]; hj = step[j]\n                # evaluate three small diagonal candidates: +i, +j, +i+ +j (keep cheap)\n                cand_dirs = [hi * eye[i], hj * eye[j], hi * eye[i] + hj * eye[j]]\n                best_local_f = f_best; best_local_x = x_best.copy()\n                for d in cand_dirs:\n                    if evals >= self.budget: break\n                    x_c = clip(x_best + d)\n                    fc = safe_eval(x_c)\n                    if fc is None: break\n                    if fc < best_local_f:\n                        best_local_f = fc; best_local_x = x_c.copy()\n                if best_local_f < f_best:\n                    prev = x_best.copy()\n                    x_best = best_local_x.copy(); f_best = best_local_f; improved = True\n                    # tiny extrapolation along combined direction\n                    if evals < self.budget:\n                        dirv = x_best - prev\n                        if np.linalg.norm(dirv) > 1e-12:\n                            x_e = clip(x_best + 1.3 * dirv)\n                            fe = safe_eval(x_e)\n                            if fe is not None and fe < f_best:\n                                x_best = x_e.copy(); f_best = fe\n                                step[i] = min(self.max_step, step[i] * self.grow)\n                                step[j] = min(self.max_step, step[j] * self.grow)\n                    else:\n                        step[i] = max(self.min_step, step[i] * self.shrink)\n                        step[j] = max(self.min_step, step[j] * self.shrink)\n                else:\n                    step[i] = max(self.min_step, step[i] * self.shrink)\n                    step[j] = max(self.min_step, step[j] * self.shrink)\n\n            # diversification\n            else:\n                if stagn >= self.patience and remaining >= 1:\n                    # Cauchy-like heavy tail jump\n                    scale = np.maximum(step, 1e-8)\n                    jump = self.rng.standard_cauchy(size=self.dim) * 0.4 * scale\n                    xj = clip(x_best + jump)\n                    fj = safe_eval(xj)\n                    if fj is None: break\n                    if fj < f_best:\n                        x_best = xj.copy(); f_best = fj; improved = True\n                        step = np.minimum(self.max_step, step * 1.2)\n                    else:\n                        step = np.minimum(self.max_step, np.maximum(self.min_step, step * 1.5))\n                else:\n                    # light gaussian sample\n                    avg = np.mean(step)\n                    xg = clip(x_best + self.rng.normal(scale=0.35 * avg, size=self.dim))\n                    fg = safe_eval(xg)\n                    if fg is None: break\n                    if fg < f_best:\n                        x_best = xg.copy(); f_best = fg; improved = True\n                    else:\n                        step *= 0.97\n                        step = np.clip(step, self.min_step, self.max_step)\n\n            # bookkeeping\n            if improved:\n                stagn = 0\n                # small global growth to encourage exploration after success\n                step = np.minimum(self.max_step, step * 1.05)\n                # invalidate cached offsets because anchor moved\n                f_plus.fill(np.nan); f_minus.fill(np.nan); anchor = x_best.copy()\n            else:\n                stagn += 1\n                step = np.maximum(self.min_step, step * 0.96)\n\n            # guard\n            step = np.clip(step, self.min_step, self.max_step)\n\n            # optional fast refresh of any stale +/- values for coords we care about\n            # (kept minimal to save evals)\n\n            # strong restart on long stagnation\n            if stagn >= 4 * self.patience and evals < self.budget:\n                if self.rng.random() < 0.5:\n                    xr = clip(lb + self.rng.random(self.dim) * (ub - lb))\n                else:\n                    center = 0.5 * (lb + ub)\n                    xr = clip(center + self.rng.normal(scale=0.5 * (ub - lb)))\n                fr = safe_eval(xr)\n                if fr is None: break\n                if fr < f_best:\n                    x_best = xr.copy(); f_best = fr; stagn = 0\n                    step[:] = np.minimum(self.max_step, self.init_step)\n                    f_plus.fill(np.nan); f_minus.fill(np.nan); anchor = x_best.copy()\n                else:\n                    step = np.minimum(self.max_step, np.maximum(self.min_step, step * 1.4))\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 16, "feedback": "The algorithm DiagonalQuadraticExplorer scored 0.199 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5951c2b-b9ff-4352-bb47-76f19aa885c0"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9928000457388081}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8408791311526372}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.5428666846637171}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.1139307459130181}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.1196322242803397}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08248467470229592}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08525826922287327}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.0010502027679692993}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.025753134869671013}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.000978162967930074}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.054870801002177005}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.07681384114474299}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.05481184510423387}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.0002289391520650108}], "aucs": [0.9928000457388081, 0.8408791311526372, 0.5428666846637171, 0.1139307459130181, 4.999999999999449e-05, 0.1196322242803397, 0.08248467470229592, 0.08525826922287327, 0.0010502027679692993, 0.025753134869671013, 0.000978162967930074, 0.054870801002177005, 0.07681384114474299, 0.05481184510423387, 0.0002289391520650108]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2897.0, "Edges": 2896.0, "Max Degree": 20.0, "Min Degree": 1.0, "Mean Degree": 1.999309630652399, "Degree Variance": 1.7266132617401813, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 9.661055853098699, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3335090408450843, "Depth Entropy": 2.3442129998194443, "Assortativity": 1.5810935651169907e-08, "Average Eccentricity": 18.198481187435277, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00034518467380048324, "Average Shortest Path": 11.548164227806629, "mean_complexity": 18.0, "total_complexity": 72.0, "mean_token_count": 622.0, "total_token_count": 2488.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "cbe19ddb-b156-4e57-8db5-9ab05addb504", "fitness": 0.1132107003566102, "name": "SeparableLinearPush", "description": "Separable Linear Push with Momentum & Sparse Couple-Scans \u2014 per-coordinate finite-difference slopes drive greedy 1-D pushes, lightweight momentum aggregates useful directions for occasional multi-dim pushes, adaptive multiplicative step control, sparse 2-D linear scans and budget-aware Gaussian restarts for diversification.", "code": "import numpy as np\n\nclass SeparableLinearPush:\n    \"\"\"\n    Compact Separable Linear Push with momentum and sparse 2-D scans.\n    \"\"\"\n    def __init__(self, budget, dim, init_step=0.8, min_step=1e-6, max_step=2.5,\n                 grow=1.25, shrink=0.6, patience=12, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_step = float(max_step)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.patience = int(patience)\n        self.rng = np.random.default_rng() if rng is None else (rng if isinstance(rng, np.random.Generator) else np.random.default_rng(rng))\n\n    def __call__(self, func):\n        # bounds\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == (): lb = np.full(self.dim, lb)\n        if ub.shape == (): ub = np.full(self.dim, ub)\n        clip = lambda x: np.minimum(np.maximum(x, lb), ub)\n\n        # quick initial\n        if self.budget <= 0:\n            x = clip(0.5 * (lb + ub))\n            return float(func(x)), x.copy()\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return None\n            v = float(func(x))\n            evals += 1\n            return v\n\n        x_best = clip(lb + self.rng.random(self.dim) * (ub - lb))\n        f_best = float(func(x_best)); evals += 1\n\n        step = np.full(self.dim, self.init_step)\n        pos_f = np.full(self.dim, np.nan)\n        neg_f = np.full(self.dim, np.nan)\n        slope = np.zeros(self.dim)\n        momentum = np.zeros(self.dim)\n        mom_decay = 0.85\n        stagn = 0\n\n        eye = np.eye(self.dim)\n\n        while evals < self.budget:\n            # choose action: coord push (70%), 2D scan (20%), momentum/diversify (10%)\n            r = self.rng.random()\n            improved = False\n\n            # refresh priorities\n            pri = np.abs(slope) * step + 1e-12\n\n            if r < 0.7:\n                # coordinate push\n                if np.all(pri == pri[0]) and self.rng.random() < 0.25:\n                    idx = int(self.rng.integers(self.dim))\n                else:\n                    probs = pri / pri.sum()\n                    idx = int(self.rng.choice(self.dim, p=probs))\n\n                # prefer sign from slope (negative slope -> decrease)\n                s = -1.0 if slope[idx] > 0 else (1.0 if slope[idx] < 0 else (1.0 if self.rng.random() < 0.5 else -1.0))\n                delta = s * step[idx] * eye[idx]\n                x1 = clip(x_best + delta)\n                f1 = safe_eval(x1)\n                if f1 is None: break\n                if s > 0: pos_f[idx] = f1\n                else: neg_f[idx] = f1\n                if f1 < f_best:\n                    x_prev = x_best.copy(); f_best = f1; x_best = x1.copy(); improved = True\n                    momentum = mom_decay * momentum + 0.3 * (x_best - x_prev)\n                    # extrapolate a few times along delta\n                    mul = 1.6\n                    for _ in range(3):\n                        if evals >= self.budget: break\n                        x2 = clip(x_best + mul * delta)\n                        if np.allclose(x2, x_best): break\n                        f2 = safe_eval(x2)\n                        if f2 is None: break\n                        if f2 < f_best:\n                            x_prev = x_best.copy(); x_best = x2.copy(); f_best = f2\n                            step[idx] = min(self.max_step, step[idx] * self.grow)\n                            momentum = mom_decay * momentum + 0.25 * (x_best - x_prev)\n                            improved = True\n                        else:\n                            break\n                else:\n                    # try other side if budget permits\n                    x2 = clip(x_best - delta)\n                    if evals < self.budget:\n                        f2 = safe_eval(x2)\n                        if f2 is None: break\n                        if (-s) > 0: pos_f[idx] = f2\n                        else: neg_f[idx] = f2\n                        if f2 < f_best:\n                            x_best = x2.copy(); f_best = f2; improved = True\n                    if not improved:\n                        step[idx] = max(self.min_step, step[idx] * self.shrink)\n\n                # update slope\n                if not np.isnan(pos_f[idx]) and not np.isnan(neg_f[idx]) and (2*step[idx])>0:\n                    slope[idx] = (pos_f[idx] - neg_f[idx]) / (2.0 * step[idx])\n\n            elif r < 0.9:\n                # sparse 2-D linear scan: pick i by priority, j random\n                probs = pri / pri.sum()\n                i = int(self.rng.choice(self.dim, p=probs))\n                j = int(self.rng.integers(self.dim - 1))\n                if j >= i: j += 1\n                si = -1.0 if slope[i] > 0 else 1.0\n                sj = -1.0 if slope[j] > 0 else 1.0\n                dirs = [si*step[i]*eye[i], sj*step[j]*eye[j], si*step[i]*eye[i] + sj*step[j]*eye[j]]\n                local_best = (f_best, x_best.copy())\n                for d in dirs:\n                    if evals >= self.budget: break\n                    xc = clip(x_best + d)\n                    fc = safe_eval(xc)\n                    if fc is None: break\n                    # update pos/neg conservatively\n                    if d[i] > 0: pos_f[i] = fc if np.isnan(pos_f[i]) else min(pos_f[i], fc)\n                    elif d[i] < 0: neg_f[i] = fc if np.isnan(neg_f[i]) else min(neg_f[i], fc)\n                    if d[j] > 0: pos_f[j] = fc if np.isnan(pos_f[j]) else min(pos_f[j], fc)\n                    elif d[j] < 0: neg_f[j] = fc if np.isnan(neg_f[j]) else min(neg_f[j], fc)\n                    if fc < local_best[0]: local_best = (fc, xc.copy())\n                if local_best[0] < f_best:\n                    x_prev = x_best.copy(); f_best, x_best = local_best[0], local_best[1].copy(); improved = True\n                    momentum = mom_decay * momentum + 0.2 * (x_best - x_prev)\n                    step[i] = min(self.max_step, step[i] * self.grow)\n                    step[j] = min(self.max_step, step[j] * self.grow)\n                else:\n                    step[i] = max(self.min_step, step[i] * self.shrink)\n                    step[j] = max(self.min_step, step[j] * self.shrink)\n                # slopes update\n                for k in (i, j):\n                    if not np.isnan(pos_f[k]) and not np.isnan(neg_f[k]) and (2*step[k])>0:\n                        slope[k] = (pos_f[k] - neg_f[k]) / (2.0 * step[k])\n\n            else:\n                # momentum/diversify\n                if np.linalg.norm(momentum) > 1e-12 and self.rng.random() < 0.6:\n                    # try a multi-dim push along momentum direction scaled to average step\n                    avg = np.mean(step)\n                    dirn = momentum / (np.linalg.norm(momentum) + 1e-12)\n                    x_try = clip(x_best + 0.9 * avg * dirn)\n                    f_try = safe_eval(x_try)\n                    if f_try is None: break\n                    if f_try < f_best:\n                        x_prev = x_best.copy(); x_best = x_try.copy(); f_best = f_try; improved = True\n                        step = np.minimum(self.max_step, step * (1.0 + 0.06))\n                        momentum = mom_decay * momentum + 0.3 * (x_best - x_prev)\n                    else:\n                        momentum *= 0.5\n                else:\n                    # Gaussian jitter / small restart when stagnating\n                    if stagn >= self.patience and evals + 2 <= self.budget:\n                        scale = 0.6 * np.maximum(step, 1e-12)\n                        x1 = clip(x_best + self.rng.normal(scale=scale))\n                        f1 = safe_eval(x1); \n                        if f1 is None: break\n                        x2 = None; f2 = None\n                        if evals < self.budget:\n                            x2 = clip(x_best + self.rng.normal(scale=1.4*scale)); f2 = safe_eval(x2)\n                            if f2 is None: f2 = np.inf\n                        best_c = min([ (f_best, x_best) , (f1, x1)] + ([] if x2 is None else [(f2, x2)]), key=lambda t:t[0])\n                        if best_c[0] < f_best:\n                            x_best = best_c[1].copy(); f_best = best_c[0]; improved = True\n                            momentum = np.zeros(self.dim)\n                        else:\n                            # modestly inflate some steps to escape\n                            ids = self.rng.choice(self.dim, size=max(1,self.dim//8), replace=False)\n                            step[ids] = np.minimum(self.max_step, step[ids]*1.3)\n                            momentum *= 0.2\n                    else:\n                        xj = clip(x_best + self.rng.normal(scale=0.3*np.mean(step), size=self.dim))\n                        fj = safe_eval(xj)\n                        if fj is None: break\n                        if fj < f_best:\n                            x_best = xj.copy(); f_best = fj; improved = True\n                            momentum = mom_decay * momentum + 0.15 * (xj - x_best)\n                        else:\n                            step *= 0.97\n\n            # bookkeeping\n            if improved:\n                stagn = 0\n                step = np.minimum(self.max_step, step * (1.0 + 0.08))\n            else:\n                stagn += 1\n                step = np.maximum(self.min_step, step * (1.0 - 0.05))\n\n            step = np.clip(step, self.min_step, self.max_step)\n\n            # periodic conservative slope recompute (no extra evals; uses stored pos/neg)\n            for k in range(self.dim):\n                if not np.isnan(pos_f[k]) and not np.isnan(neg_f[k]) and (2*step[k])>0:\n                    slope[k] = (pos_f[k] - neg_f[k]) / (2.0 * step[k])\n\n            # long stagnation: random nearby restart\n            if stagn >= 4 * self.patience and evals < self.budget:\n                if self.rng.random() < 0.5:\n                    x_r = clip(lb + self.rng.random(self.dim)*(ub - lb))\n                else:\n                    center = 0.5*(lb+ub)\n                    x_r = clip(center + self.rng.normal(scale=0.5*(ub-lb)))\n                fr = safe_eval(x_r)\n                if fr is None: break\n                if fr < f_best:\n                    x_best = x_r.copy(); f_best = fr\n                    pos_f[:] = np.nan; neg_f[:] = np.nan; slope[:] = 0.0\n                    step[:] = np.minimum(self.max_step, self.init_step)\n                    momentum[:] = 0.0\n                    stagn = 0\n                else:\n                    step = np.minimum(self.max_step, step * 1.4)\n                    stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 16, "feedback": "The algorithm SeparableLinearPush scored 0.113 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5951c2b-b9ff-4352-bb47-76f19aa885c0"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.5558686845256462}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7119489876835496}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.34729019293150887}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.019571317961642132}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.04883355795612021}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.014197764290685866}, {"fid": 5, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}], "aucs": [0.5558686845256462, 0.7119489876835496, 0.34729019293150887, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.019571317961642132, 4.999999999999449e-05, 4.999999999999449e-05, 0.04883355795612021, 0.014197764290685866, 4.999999999999449e-05]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2730.0, "Edges": 2729.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9992673992673993, "Degree Variance": 1.8652009284976319, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 9.58366375892149, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3197675119215515, "Depth Entropy": 2.2877871182789336, "Assortativity": 1.163898309262034e-08, "Average Eccentricity": 18.5978021978022, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003663003663003663, "Average Shortest Path": 11.131751087559076, "mean_complexity": 26.0, "total_complexity": 78.0, "mean_token_count": 798.0, "total_token_count": 2394.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "161922f9-374f-4a09-82dc-5507db3a0597", "fitness": 0.2777616231505856, "name": "ScrambleEnsembleAnchors", "description": "Scramble-Ensemble Anchors (SEA) \u2014 maintain a tiny anchor ensemble and perform low-cost scrambled subspace pulls toward ensemble medians with additive step adaptation, occasional heavy-tail global jumps, and randomized small subspace recombinations.", "code": "import numpy as np\n\nclass ScrambleEnsembleAnchors:\n    \"\"\"\n    Scramble-Ensemble Anchors (SEA)\n    - tiny ensemble of anchors (cheap population)\n    - randomized subspace \"scrambles\" toward ensemble median with per-dim additive steps\n    - occasional heavy-tail global jumps for escapes\n    - budget-safe, works on [-5,5] default box\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, anchors=3,\n                 init_scale=0.18, add_step=0.06, decay=0.7,\n                 global_prob=0.04, min_step_frac=1e-9, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.anchors = max(1, int(anchors))\n        self.init_scale = float(init_scale)\n        self.add_step = float(add_step)\n        self.decay = float(decay)\n        self.global_prob = float(global_prob)\n        self.min_step_frac = float(min_step_frac)\n        self.rng = np.random.default_rng(seed)\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_vec = ub - lb\n        box_scale = max(1e-12, np.linalg.norm(box_vec))\n        base_step = max(1e-12, self.init_scale * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = box_scale * 1.5\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            v = float(func(x))\n            evals += 1\n            return v\n\n        # initialize small ensemble (respect budget)\n        K = min(self.anchors, max(1, self.budget // 5))\n        anchors_x = np.empty((K, dim), dtype=float)\n        anchors_f = np.empty(K, dtype=float)\n        for i in range(K):\n            x = rng.uniform(lb, ub)\n            anchors_x[i] = x\n            anchors_f[i] = safe_eval(x)\n            if evals >= self.budget:\n                break\n        # if some anchors not set due to tiny budget, fill with best seen\n        if np.isinf(anchors_f).any():\n            mask = np.isfinite(anchors_f)\n            if mask.any():\n                best_idx = int(np.argmin(anchors_f[mask]))\n                fill_x = anchors_x[mask][best_idx].copy()\n            else:\n                fill_x = np.clip((lb+ub)/2, lb, ub)\n            for i in range(K):\n                if not np.isfinite(anchors_f[i]):\n                    anchors_x[i] = fill_x.copy()\n                    anchors_f[i] = safe_eval(anchors_x[i])\n                    if evals >= self.budget:\n                        break\n\n        # incumbent\n        best_idx = int(np.argmin(anchors_f))\n        x_best = anchors_x[best_idx].copy()\n        f_best = float(anchors_f[best_idx])\n\n        # per-dim additive steps (keeps different from multiplicative-only)\n        steps = np.full(dim, base_step, dtype=float)\n\n        # small counters for stagnation\n        stagn = np.zeros(K, dtype=int)\n\n        # main loop\n        while evals < self.budget:\n            # occasional heavy-tail global jump\n            if rng.random() < self.global_prob and evals < self.budget:\n                # Cauchy-like via ratio of normals\n                z = rng.standard_normal(dim)\n                y = rng.standard_normal(dim)\n                cauchy = z / (np.where(np.abs(y) < 1e-12, 1e-12, y))\n                jump = 0.25 * box_scale * cauchy * (0.5 + rng.random())\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    f_best = fg; x_best = xg.copy()\n                    # replace worst anchor with this good one\n                    worst = int(np.argmax(anchors_f))\n                    anchors_x[worst] = xg.copy(); anchors_f[worst] = fg\n                continue\n\n            # pick an anchor probabilistically biased to better ones\n            scores = np.exp((np.max(anchors_f) - anchors_f) / (1e-8 + abs(np.max(anchors_f)) + 1.0))\n            probs = scores / scores.sum()\n            i = int(rng.choice(K, p=probs))\n            xi = anchors_x[i].copy()\n\n            # compute ensemble median and diversity\n            median = np.median(anchors_x, axis=0)\n            spread = np.maximum(1e-12, np.std(anchors_x, axis=0))\n            # choose subspace size: geometric-like favoring small subspaces\n            p_dim = 0.25 + 0.5 * rng.random()  # expected fraction\n            k = max(1, int(round(p_dim * dim)))\n            dims = rng.choice(dim, size=k, replace=False)\n\n            # scrambled pull: move selected dims toward median with random mixing + noise\n            lam = 0.12 + 0.6 * rng.random()  # mixing factor\n            noise = rng.normal(0.0, 1.0, size=k) * (steps[dims] / (0.5 + rng.random()))\n            xt = xi.copy()\n            xt[dims] = xi[dims] + lam * (median[dims] - xi[dims]) + noise\n            xt = np.clip(xt, lb, ub)\n            ft = safe_eval(xt)\n\n            # update anchor i with replacement strategy: better wins or probabilistic replace\n            improved = False\n            if ft < anchors_f[i]:\n                anchors_x[i] = xt.copy()\n                anchors_f[i] = ft\n                stagn[i] = 0\n                improved = True\n            else:\n                # occasional soft replacement if similar and random\n                stagn[i] += 1\n                if stagn[i] > 8 and rng.random() < 0.18:\n                    anchors_x[i] = xt.copy(); anchors_f[i] = ft; stagn[i] = 0\n\n            # update global best\n            if ft < f_best:\n                f_best = ft; x_best = xt.copy()\n\n            # per-dim additive adaptation: successes add some step, failures decay\n            if improved:\n                steps[dims] = np.minimum(max_step, steps[dims] + self.add_step * box_scale * (0.5 + rng.random()))\n            else:\n                steps[dims] = np.maximum(min_step, steps[dims] * (self.decay ** (0.5 + rng.random())))\n\n            # small anchor recombination: if ensemble too clustered, respawn worst\n            if (np.max(anchors_f) - np.min(anchors_f)) < 1e-6 and rng.random() < 0.1 and evals < self.budget:\n                w = int(np.argmax(anchors_f))\n                anchors_x[w] = np.clip((x_best + rng.normal(0, 0.02*box_scale, size=dim)), lb, ub)\n                anchors_f[w] = safe_eval(anchors_x[w])\n                if anchors_f[w] < f_best:\n                    f_best = anchors_f[w]; x_best = anchors_x[w].copy()\n\n        # final cheap polish: sweep small single-dim pushes while budget allows\n        final = 0.04 * box_scale\n        while evals < self.budget and final > 1e-12:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + final, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    f_best = ft; x_best = xt.copy(); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - final, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    f_best = ft; x_best = xt.copy(); improved = True\n            if not improved:\n                final *= 0.5\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 16, "feedback": "The algorithm ScrambleEnsembleAnchors scored 0.278 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2ce99025-003a-46b2-bec0-52c377bfc061"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.2835566885753946}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.32303210573852714}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.3313132262347962}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.03691001107901759}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.067350926892497}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.04982338057670077}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.03549612495533583}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.05363699853294346}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.046130814860592895}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.97196151760552}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9911536433933539}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9759089088141057}], "aucs": [0.2835566885753946, 0.32303210573852714, 0.3313132262347962, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.03691001107901759, 0.067350926892497, 0.04982338057670077, 0.03549612495533583, 0.05363699853294346, 0.046130814860592895, 0.97196151760552, 0.9911536433933539, 0.9759089088141057]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1776.0, "Edges": 1775.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9988738738738738, "Degree Variance": 2.132881614722831, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.542929292929293, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3296798528917912, "Depth Entropy": 1.9342081130688935, "Assortativity": 4.866637605907463e-09, "Average Eccentricity": 15.25, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0005630630630630631, "Average Shortest Path": 9.642796599416318, "mean_complexity": 9.5, "total_complexity": 38.0, "mean_token_count": 398.5, "total_token_count": 1594.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "8e4d543e-a18a-4def-9b56-2a20aed9fccf", "fitness": 0.6216755353232127, "name": "SeparableCumulativeMedianPull", "description": "Separable Cumulative Median Pull v2 \u2014 lightweight archive-driven per-coordinate pulls with compact momentum, adaptive multiplicative steps, dispersion-weighted coordinate sampling and occasional targeted escapes; strictly 1-D moves for cheap, robust exploitation and small randomized diversifications on stagnation.", "code": "import numpy as np\n\nclass SeparableCumulativeMedianPull:\n    def __init__(self, budget=10000, dim=10, archive_size=16, init_step=None,\n                 min_step=1e-6, max_step=None, stagnation_patience=8, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == (): lb = np.full(self.dim, lb.item())\n        if ub.shape == (): ub = np.full(self.dim, ub.item())\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        D = self.dim\n        rng = self.rng\n        rng_random = rng.random\n        rng_choice = rng.choice\n        rng_integers = rng.integers\n\n        span = ub - lb\n        if self.init_step is None:\n            init_s = 0.25 * span\n        else:\n            init_s = np.asarray(self.init_step, float)\n            if init_s.shape == (): init_s = np.full(D, float(init_s))\n        if self.max_step is None:\n            max_s = 0.5 * span\n        else:\n            max_s = np.asarray(self.max_step, float)\n            if max_s.shape == (): max_s = np.full(D, float(max_s))\n        step = np.clip(init_s, self.min_step, max_s)\n\n        B = self.budget\n        evals = 0\n        top_k = min(self.archive_size, max(2, B // 5))\n        archive_x = []\n        archive_f = []\n\n        # initial cheap sampling\n        init_n = min(max(4, D), max(1, B // 12))\n        for _ in range(init_n):\n            if evals >= B: break\n            x = lb + rng_random(D) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n        if len(archive_x) == 0:\n            x = lb + rng_random(D) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n\n        idx = int(np.argmin(archive_f))\n        x_best = archive_x[idx].copy(); f_best = float(archive_f[idx])\n\n        momentum = np.zeros(D)\n        priority = np.zeros(D)\n        success = np.zeros(D, dtype=int)\n        failure = np.zeros(D, dtype=int)\n\n        stagn = 0\n\n        def add_archive(xn, fn):\n            nonlocal archive_x, archive_f\n            if len(archive_x) < top_k:\n                archive_x.append(xn.copy()); archive_f.append(float(fn)); return\n            worst = int(np.argmax(archive_f))\n            if fn < archive_f[worst]:\n                archive_x[worst] = xn.copy(); archive_f[worst] = float(fn)\n\n        while evals < B:\n            arr = np.array(archive_x)\n            med = np.median(arr, axis=0)\n            q75 = np.percentile(arr, 75, axis=0); q25 = np.percentile(arr, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            # coordinate preference: normalized pull + dispersion + priority\n            score = abs_pull / (step + 1e-12) + 0.5 * (iqr / (span + 1e-12))\n            score = score * (1.0 + 0.12 * priority) + 1e-8\n            # softmax-like probabilities with light temperature tied to score spread\n            smax = np.max(score); sc = score - smax\n            temp = 1.0 + 0.18 * np.std(score)\n            probs = np.exp(sc / temp)\n            probs = probs / probs.sum()\n\n            try:\n                coord = int(rng_choice(D, p=probs))\n            except Exception:\n                coord = int(rng_integers(0, D))\n            s = max(step[coord], self.min_step)\n\n            # decide sign: follow pull unless tiny, else follow momentum or random\n            if abs_pull[coord] > 1e-9:\n                sign = int(np.sign(pull[coord]))\n                if momentum[coord] != 0 and rng_random() < 0.22 and np.sign(momentum[coord]) != sign:\n                    sign = int(np.sign(momentum[coord]))\n                delta = sign * min(s, abs_pull[coord])\n            else:\n                if momentum[coord] != 0 and rng_random() < 0.7:\n                    delta = np.sign(momentum[coord]) * s\n                else:\n                    delta = (1 if rng_random() < 0.5 else -1) * s\n\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            add_archive(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try; improved = True\n                momentum[coord] = 0.75 * momentum[coord] + 0.25 * delta\n                priority[coord] = min(8.0, priority[coord] + 1.0)\n                step[coord] = float(min(max_s[coord], step[coord] * 1.28 + 1e-12))\n                success[coord] += 1; failure[coord] = 0\n                # cheap forward extrapolation\n                if evals < B:\n                    ext = 0.45 * np.abs(delta); sign_ext = int(np.sign(delta))\n                    x_ext = x_best.copy()\n                    x_ext[coord] = np.clip(x_ext[coord] + sign_ext * ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_archive(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext\n                        momentum[coord] = 0.85 * momentum[coord] + 0.15 * sign_ext * ext\n                        step[coord] = float(min(max_s[coord], step[coord] * 1.12 + 1e-12))\n            else:\n                # try opposite direction once\n                if evals < B:\n                    opp = -delta\n                    x2 = x_best.copy()\n                    x2[coord] = np.clip(x2[coord] + opp, lb[coord], ub[coord])\n                    f2 = float(func(x2)); evals += 1\n                    add_archive(x2, f2)\n                    if f2 < f_best:\n                        f_best = f2; x_best = x2; improved = True\n                        momentum[coord] = 0.7 * momentum[coord] + 0.3 * opp\n                        priority[coord] = min(8.0, priority[coord] + 0.8)\n                        step[coord] = float(min(max_s[coord], step[coord] * 1.18 + 1e-12))\n                        success[coord] += 1; failure[coord] = 0\n                        if evals < B:\n                            ext = 0.36 * abs(opp); se = int(np.sign(opp))\n                            x_ext = x_best.copy()\n                            x_ext[coord] = np.clip(x_ext[coord] + se * ext, lb[coord], ub[coord])\n                            f_ext = float(func(x_ext)); evals += 1\n                            add_archive(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext; x_best = x_ext\n                                step[coord] = float(min(max_s[coord], step[coord] * 1.08 + 1e-12))\n                    else:\n                        failure[coord] += 1\n                        priority[coord] = max(-8.0, priority[coord] - 0.6)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.56))\n                        momentum[coord] *= 0.62\n                else:\n                    failure[coord] += 1\n                    priority[coord] = max(-8.0, priority[coord] - 0.6)\n                    step[coord] = float(max(self.min_step, step[coord] * 0.56))\n                    momentum[coord] *= 0.62\n\n            # normalize and decay\n            step = np.minimum(np.maximum(step, self.min_step), max_s)\n            priority *= 0.998\n            momentum *= 0.993\n\n            if improved:\n                stagn = 0\n            else:\n                stagn += 1\n\n            # targeted escape on stagnation: few randomized single-coordinate jumps\n            if stagn >= self.stagnation_patience and evals < B:\n                # pick coords with large iqr*step score\n                cand = (iqr / (span + 1e-12)) * (step / (step.max() + 1e-12) + 0.12)\n                cand = cand + 1e-8\n                sc = cand - cand.max(); p = np.exp(sc); p = p / p.sum()\n                k = min(3, max(1, D // 6))\n                try:\n                    picks = list(rng_choice(D, size=k, replace=False, p=p))\n                except Exception:\n                    picks = list(rng_choice(D, size=k, replace=False))\n                any_imp = False\n                for c in picks:\n                    if evals >= B: break\n                    jump = (0.6 * step[c] + 0.4 * span[c] * rng_random()) * (1 if rng_random() < 0.5 else -1)\n                    xj = x_best.copy()\n                    xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    add_archive(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj\n                        step[c] = float(min(max_s[c], step[c] * 1.32 + 1e-12))\n                        priority[c] = min(8.0, priority[c] + 0.9)\n                        momentum[c] = np.sign(jump) * 0.5 * abs(jump)\n                        any_imp = True\n                        break\n                if not any_imp:\n                    # small randomized step reset to escape plateaus\n                    step = np.minimum(max_s, step * (1.0 + 0.22 * rng_random(D)))\n                    priority *= 0.28\n                stagn = 0\n\n        return f_best, x_best", "configspace": "", "generation": 16, "feedback": "The algorithm SeparableCumulativeMedianPull scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9691912166271829}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.975569102904815}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9766510105013623}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8382435196135777}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8224061526812438}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8263129602182934}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1477374013059698}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.15342417313228618}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.16381824953798663}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.14342239081788033}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.15992712016736277}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.1624184681661498}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9955168540053115}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9942519707406144}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9962424394281528}], "aucs": [0.9691912166271829, 0.975569102904815, 0.9766510105013623, 0.8382435196135777, 0.8224061526812438, 0.8263129602182934, 0.1477374013059698, 0.15342417313228618, 0.16381824953798663, 0.14342239081788033, 0.15992712016736277, 0.1624184681661498, 0.9955168540053115, 0.9942519707406144, 0.9962424394281528]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2473.0, "Edges": 2472.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.9991912656692277, "Degree Variance": 2.29276117368837, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.285333333333334, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3173762060254055, "Depth Entropy": 2.1687453676827113, "Assortativity": 0.0, "Average Eccentricity": 17.7864941366761, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0004043671653861706, "Average Shortest Path": 10.130861851687547, "mean_complexity": 10.75, "total_complexity": 43.0, "mean_token_count": 541.0, "total_token_count": 2164.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "4161cdb6-4e26-4eb2-a406-816633d1e944", "fitness": "-inf", "name": "MicroMomentumAdaptiveBandit", "description": "Micro-Momentum Adaptive Bandit (MMAB) \u2014 strictly single-coordinate atomic moves combining per-anchor adaptive steps, momentum-biased sign selection, and a lightweight coordinate-bandit (priority sampling from step \u00d7 uncertainty) with rare Cauchy single-coordinate escapes and single-coordinate respawns.", "code": "import numpy as np\n\nclass MicroMomentumAdaptiveBandit:\n    def __init__(self, budget=1000, dim=10, pop_size=None,\n                 init_step=0.12, success_mult=1.4, failure_mult=0.6,\n                 heavy_prob=0.025, respawn_prob=0.12, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.heavy_prob = float(heavy_prob)\n        self.respawn_prob = float(respawn_prob)\n        if pop_size is None:\n            self.pop_size = max(2, min(8, int(round(4 + 0.04 * self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n    def _bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_step * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        # initialize anchors: one base full-dim eval, others single-coordinate variants\n        anchors = np.empty((self.pop_size, dim), float)\n        af = np.full(self.pop_size, np.inf)\n        base = rng.uniform(lb, ub)\n        anchors[0] = base.copy(); af[0] = safe_eval(anchors[0])\n\n        for i in range(1, self.pop_size):\n            if evals >= self.budget:\n                anchors[i] = anchors[0].copy(); af[i] = af[0]; continue\n            a = anchors[0].copy()\n            c = rng.integers(0, dim)\n            a[c] = rng.uniform(lb[c], ub[c])\n            anchors[i] = a; af[i] = safe_eval(a)\n\n        steps = np.full((self.pop_size, dim), base_step)\n        # small randomness to break symmetry\n        steps *= (1.0 + 0.05 * rng.standard_normal((self.pop_size, dim)))\n        momentum = np.zeros((self.pop_size, dim), float)  # in [-1,1]\n        stagn = np.zeros(self.pop_size, int)\n        succ = np.ones((self.pop_size, dim), float)  # success counters (smoothed)\n        fail = np.ones((self.pop_size, dim), float)\n\n        ibest = int(np.nanargmin(af))\n        xbest = anchors[ibest].copy(); fbest = float(af[ibest])\n\n        while evals < self.budget:\n            # occasional heavy single-coordinate escape from best\n            if rng.random() < self.heavy_prob and evals < self.budget:\n                c = rng.integers(0, dim)\n                cauchy = np.tan(np.pi * (rng.random() - 0.5))\n                jump = 0.22 * box_len * cauchy\n                xj = xbest.copy(); xj[c] = np.clip(xbest[c] + jump, lb[c], ub[c])\n                fj = safe_eval(xj)\n                if fj < fbest:\n                    fbest = fj; xbest = xj.copy()\n                    iw = int(np.nanargmax(af))\n                    anchors[iw] = xj.copy(); af[iw] = fj\n                    steps[iw, :] = base_step; momentum[iw, :] = 0.0\n\n            # pick an anchor to work on: prefer worse & stagnated anchors occasionally\n            weights = (af - fbest + 1e-12)\n            # emphasize stagnation so stuck anchors get attention\n            weights *= (1.0 + 0.3 * (stagn / (1 + stagn)))\n            if np.sum(weights) <= 0:\n                idx = rng.integers(0, self.pop_size)\n            else:\n                p = weights / np.sum(weights)\n                idx = int(rng.choice(self.pop_size, p=p))\n\n            x0 = anchors[idx].copy(); f0 = af[idx]\n\n            # coordinate-bandit priority: step * (1 + |m|) * (1 + sqrt((fail+1)/(succ+1)))\n            unc = np.sqrt((fail[idx] + 1.0) / (succ[idx] + 1.0))\n            priority = steps[idx] * (1.0 + 0.6 * np.abs(momentum[idx])) * (1.0 + 0.8 * unc)\n            # small noise to break ties\n            priority += 1e-12 * rng.random(dim)\n            prob = priority / np.sum(priority)\n            c = int(rng.choice(dim, p=prob))\n\n            # sign choice biased by momentum\n            m = momentum[idx, c]\n            pref = 1 if m >= 0 else -1\n            if rng.random() < (0.5 + 0.4 * min(1.0, abs(m))):\n                s = pref\n            else:\n                s = 1 if rng.random() < 0.5 else -1\n\n            d = steps[idx, c]\n            if d <= 1e-12 * box_len:\n                stagn[idx] += 1\n            else:\n                xt = x0.copy()\n                xt[c] = np.clip(x0[c] + s * d, lb[c], ub[c])\n                ft = safe_eval(xt)\n                if ft < f0:\n                    anchors[idx] = xt.copy(); af[idx] = ft\n                    steps[idx, c] = min(steps[idx, c] * self.success_mult, 1.5 * box_len)\n                    momentum[idx, c] = np.clip(0.7 * momentum[idx, c] + 0.3 * float(s), -1.0, 1.0)\n                    succ[idx, c] += 1.0\n                    fail[idx, c] *= 0.9\n                    stagn[idx] = 0\n                else:\n                    steps[idx, c] = max(steps[idx, c] * self.failure_mult, 1e-12 * box_len)\n                    momentum[idx, c] *= 0.88\n                    fail[idx, c] += 1.0\n                    succ[idx, c] *= 0.99\n                    stagn[idx] += 1\n\n            # global best update\n            im = int(np.nanargmin(af))\n            if af[im] < fbest:\n                fbest = float(af[im]); xbest = anchors[im].copy()\n\n            # stagnation handling for the anchor\n            if stagn[idx] > max(6, 2 * dim) and evals < self.budget:\n                stagn[idx] = 0\n                if rng.random() < self.respawn_prob:\n                    rc = rng.integers(0, dim)\n                    newp = anchors[idx].copy()\n                    newp[rc] = rng.uniform(lb[rc], ub[rc])\n                    fn = safe_eval(newp)\n                    anchors[idx] = newp; af[idx] = fn\n                    steps[idx, rc] = base_step; momentum[idx, rc] = 0.0\n                    steps[idx, :] = np.maximum(steps[idx, :], 0.2 * base_step)\n                else:\n                    rc = rng.integers(0, dim)\n                    cauchy = np.tan(np.pi * (rng.random() - 0.5))\n                    jump = 0.18 * box_len * cauchy\n                    newp = anchors[idx].copy()\n                    newp[rc] = np.clip(anchors[idx, rc] + jump, lb[rc], ub[rc])\n                    fn = safe_eval(newp)\n                    anchors[idx] = newp; af[idx] = fn\n                    steps[idx, rc] = base_step; momentum[idx, rc] = 0.0\n\n            # occasional replacement: inject a perturbed best into worst anchor\n            if rng.random() < 0.04 and evals < self.budget:\n                iw = int(np.nanargmax(af))\n                rp = xbest.copy()\n                rc = rng.integers(0, dim)\n                rp[rc] = np.clip(xbest[rc] + 0.06 * (ub[rc] - lb[rc]) * rng.standard_normal(), lb[rc], ub[rc])\n                fw = safe_eval(rp)\n                anchors[iw] = rp; af[iw] = fw\n                steps[iw, :] = base_step; momentum[iw, :] = 0.0\n\n            # clip steps\n            steps = np.clip(steps, 1e-10 * box_len, 1.0 * box_len)\n\n            # quick exit to final refine when budget low\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final single-coordinate progressive refinement\n        mesh = 0.10 * box_len\n        mesh_min = 1e-8 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = xbest.copy(); xt[d] = np.clip(xt[d] + mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fbest:\n                    fbest = ft; xbest = xt.copy(); improved = True; continue\n                if evals >= self.budget: break\n                xt = xbest.copy(); xt[d] = np.clip(xt[d] - mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fbest:\n                    fbest = ft; xbest = xt.copy(); improved = True\n            if not improved: mesh *= 0.5\n\n        self.f_opt = float(fbest); self.x_opt = np.asarray(xbest, float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 16, "feedback": "In the code, line 892, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities are not non-negative", "error": "In the code, line 892, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities are not non-negative", "parent_ids": ["6cd3ea29-a941-4770-ab4b-0e5e0cca3077"], "operator": "refine", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2385.0, "Edges": 2384.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.99916142557652, "Degree Variance": 2.251571623836962, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.8125, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3118252759125382, "Depth Entropy": 1.9370203027922457, "Assortativity": 0.0, "Average Eccentricity": 18.11823899371069, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0004192872117400419, "Average Shortest Path": 9.840739099235996, "mean_complexity": 9.5, "total_complexity": 38.0, "mean_token_count": 518.75, "total_token_count": 2075.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "b28d80a7-bbab-4c9b-b4d3-c98b4242b824", "fitness": 0.47676316580876954, "name": "DirectionalAdaptiveMicroSwarm", "description": "Directional Adaptive Micro-Swarm \u2014 a tiny swarm of anchors that favors coordinate moves guided by momentum and vector differences to the best point, with occasional compact rotated pulls and cheap respawns for robust exploitation/exploration.", "code": "import numpy as np\n\nclass DirectionalAdaptiveMicroSwarm:\n    def __init__(self, budget=1000, dim=10, pop=None, init_frac=0.12,\n                 success_mult=1.4, failure_mult=0.6, rotate_prob=0.06,\n                 respawn_prob=0.12, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.pop = max(3, min(7, int(pop) if pop is not None else 2 + dim//4))\n        self.init_frac = float(init_frac)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.rotate_prob = float(rotate_prob)\n        self.respawn_prob = float(respawn_prob)\n\n    def _bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box = ub - lb\n        box_len = np.linalg.norm(box)\n        base_step = max(1e-12, self.init_frac * box_len)\n        evals = 0\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # anchors\n        X = rng.uniform(lb, ub, size=(self.pop, dim))\n        f = np.array([safe_eval(X[i]) if evals < self.budget else np.inf for i in range(self.pop)], float)\n        steps = np.full((self.pop, dim), base_step)\n        mom = np.zeros((self.pop, dim), float)  # [-1,1] direction bias\n        ew = np.zeros((self.pop, dim), float)   # EWMA improvement magnitude\n        stagn = np.zeros(self.pop, int)\n\n        ib = int(np.nanargmin(f))\n        xb = X[ib].copy(); fb = float(f[ib])\n\n        while evals < self.budget:\n            order = rng.permutation(self.pop)\n            any_imp = False\n            for i in order:\n                if evals >= self.budget: break\n                xi = X[i].copy(); fi = f[i]\n                diff = xb - xi\n                # coordinate priority: step * (1 + |mom|*0.8) * (1 + |diff|/range) * (1 + ew_scaled)\n                diffw = np.abs(diff) / (1e-12 + box)\n                ew_scaled = ew[i] / (1e-12 + (np.max(ew[i]) if np.max(ew[i])>0 else 1.0))\n                pr = steps[i] * (1.0 + 0.8 * np.abs(mom[i])) * (1.0 + diffw) * (1.0 + 1.0 * ew_scaled)\n                if pr.sum() <= 0:\n                    c = rng.integers(0, dim)\n                else:\n                    p = pr / pr.sum()\n                    c = rng.choice(dim, p=p)\n                # decide sign: prefer sign toward best and momentum\n                prefer = np.sign(diff[c]) if diff[c] != 0 else (1 if rng.random() < 0.5 else -1)\n                m = mom[i, c]\n                pref_score = 0.55 + 0.45 * min(1.0, abs(m))\n                sign = prefer if rng.random() < pref_score else (-prefer if rng.random() < 0.5 else prefer)\n                d = steps[i, c]\n                if d <= 1e-14 * box_len:\n                    stagn[i] += 1\n                    if rng.random() < 0.08 and evals < self.budget:\n                        # tiny respawn on a coordinate\n                        rc = rng.integers(0, dim)\n                        newp = xi.copy(); newp[rc] = rng.uniform(lb[rc], ub[rc])\n                        fn = safe_eval(newp)\n                        X[i] = newp; f[i] = fn; steps[i, rc] = base_step; mom[i, rc] = 0.0\n                        if fn < fb: fb = fn; xb = newp.copy(); any_imp = True\n                    continue\n\n                # rotated 2D probe sometimes: single eval on diagonal combining two coords\n                if rng.random() < self.rotate_prob and evals < self.budget:\n                    c2 = rng.integers(0, dim-1)\n                    c2 = c2 if c2 < c else c2+1\n                    s2 = 1 if rng.random() < 0.5 else -1\n                    step1 = d\n                    step2 = steps[i, c2]\n                    alpha = rng.uniform(0.4, 1.0)\n                    xt = xi.copy()\n                    xt[c] = np.clip(xi[c] + sign * alpha * step1, lb[c], ub[c])\n                    xt[c2] = np.clip(xi[c2] + s2 * (1-alpha) * step2, lb[c2], ub[c2])\n                    ft = safe_eval(xt)\n                    if ft < fi:\n                        X[i] = xt; f[i] = ft\n                        steps[i, c] = min(steps[i, c] * self.success_mult, box_len)\n                        steps[i, c2] = min(steps[i, c2] * self.success_mult, box_len)\n                        mom[i, c] = np.clip(0.75*mom[i, c] + 0.25*sign, -1.0, 1.0)\n                        mom[i, c2] = np.clip(0.75*mom[i, c2] + 0.25*s2, -1.0, 1.0)\n                        imp = max(0.0, fi - ft)\n                        ew[i, c] = 0.6*ew[i, c] + 0.4*imp\n                        ew[i, c2] = 0.6*ew[i, c2] + 0.4*imp\n                        stagn[i] = 0\n                        any_imp = True\n                    else:\n                        steps[i, c] = max(steps[i, c]*self.failure_mult, 1e-14*box_len)\n                        steps[i, c2] = max(steps[i, c2]*self.failure_mult, 1e-14*box_len)\n                        mom[i, c] *= 0.9; mom[i, c2] *= 0.9\n                        stagn[i] += 1\n                else:\n                    # simple one-d probe\n                    xt = xi.copy()\n                    xt[c] = np.clip(xi[c] + sign * d, lb[c], ub[c])\n                    ft = safe_eval(xt)\n                    if ft < fi:\n                        X[i] = xt; f[i] = ft\n                        steps[i, c] = min(steps[i, c] * self.success_mult, box_len)\n                        mom[i, c] = np.clip(0.8*mom[i, c] + 0.2*sign, -1.0, 1.0)\n                        imp = max(0.0, fi - ft)\n                        ew[i, c] = 0.6*ew[i, c] + 0.4*imp\n                        stagn[i] = 0\n                        any_imp = True\n                        # quick follow-through\n                        if evals < self.budget:\n                            d2 = min(steps[i, c]*1.5, box_len)\n                            x2 = X[i].copy(); x2[c] = np.clip(X[i, c] + sign * d2, lb[c], ub[c])\n                            f2 = safe_eval(x2)\n                            if f2 < f[i]:\n                                X[i] = x2; f[i] = f2\n                                steps[i, c] = min(steps[i, c] * self.success_mult, box_len)\n                                mom[i, c] = np.clip(0.8*mom[i, c] + 0.2*sign, -1.0, 1.0)\n                                ew[i, c] = 0.6*ew[i, c] + 0.4*max(0.0, ft - f2)\n                    else:\n                        steps[i, c] = max(steps[i, c] * self.failure_mult, 1e-14*box_len)\n                        mom[i, c] *= 0.92\n                        stagn[i] += 1\n\n                # maybe replace worst with a recombination of best and this anchor on success\n                if f[i] < fb:\n                    fb = float(f[i]); xb = X[i].copy()\n\n                if stagn[i] > max(5, dim//2) and evals < self.budget:\n                    stagn[i] = 0\n                    if rng.random() < self.respawn_prob:\n                        # respawn by convex recombination of best and random anchor\n                        j = rng.integers(0, self.pop)\n                        mix = rng.random()\n                        newp = np.clip(mix*xb + (1-mix)*X[j] + 0.02*rng.standard_normal(dim)*box, lb, ub)\n                        fn = safe_eval(newp)\n                        X[i] = newp; f[i] = fn\n                        steps[i, :] = np.maximum(steps[i, :], base_step*0.4)\n                        mom[i, :] = 0.0\n                        if fn < fb: fb = fn; xb = newp.copy(); any_imp = True\n                    else:\n                        # larger random jump on one coord\n                        rc = rng.integers(0, dim)\n                        jump = (rng.random() - 0.5) * 0.6 * box[rc]\n                        newp = X[i].copy(); newp[rc] = np.clip(X[i, rc] + jump, lb[rc], ub[rc])\n                        fn = safe_eval(newp)\n                        X[i] = newp; f[i] = fn\n                        steps[i, rc] = base_step; mom[i, rc] = 0.0\n                        if fn < fb: fb = fn; xb = newp.copy(); any_imp = True\n\n            # keep steps sane\n            steps = np.clip(steps, 1e-14 * box_len, box_len)\n\n            # small targeted opposition move if population stalled\n            if (not any_imp) and (evals < self.budget) and rng.random() < 0.12:\n                iw = int(np.nanargmax(f))\n                rc = rng.integers(0, dim)\n                oppv = lb[rc] + ub[rc] - X[iw, rc]\n                newp = X[iw].copy()\n                newp[rc] = np.clip(oppv + 0.01*box[rc]*rng.standard_normal(), lb[rc], ub[rc])\n                fn = safe_eval(newp)\n                X[iw] = newp; f[iw] = fn\n                steps[iw, rc] = base_step; mom[iw, rc] = 0.0\n                if fn < fb: fb = fn; xb = newp.copy()\n\n            if self.budget - evals < max(6, dim):\n                break\n\n        # final coordinate mesh refine around xb\n        mesh = 0.06 * box_len; mesh_min = 1e-9 * box_len\n        while evals < self.budget and mesh > mesh_min:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = xb.copy(); xt[d] = np.clip(xb[d] + mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fb:\n                    fb = ft; xb = xt.copy(); improved = True; continue\n                if evals >= self.budget: break\n                xt = xb.copy(); xt[d] = np.clip(xb[d] - mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fb:\n                    fb = ft; xb = xt.copy(); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.f_opt = float(fb); self.x_opt = np.asarray(xb, float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 17, "feedback": "The algorithm DirectionalAdaptiveMicroSwarm scored 0.477 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7a0a043e-4f30-49e3-b8c2-d79e059cbdb1"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.7739031970266842}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7727195538769742}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7469717595066276}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.30451917229202086}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.43382875189196934}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.4571131003507306}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.15834874660144538}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1095980813718972}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.11364222874823049}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.11501445967395685}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.10576329034705811}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.09636594564088952}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9867377032776545}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9902090676760349}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9867124288493693}], "aucs": [0.7739031970266842, 0.7727195538769742, 0.7469717595066276, 0.30451917229202086, 0.43382875189196934, 0.4571131003507306, 0.15834874660144538, 0.1095980813718972, 0.11364222874823049, 0.11501445967395685, 0.10576329034705811, 0.09636594564088952, 0.9867377032776545, 0.9902090676760349, 0.9867124288493693]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 3029.0, "Edges": 3028.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9993397160779136, "Degree Variance": 2.0950804488055983, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 9.152758132956153, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.309790502947802, "Depth Entropy": 2.2103783908564556, "Assortativity": 0.0, "Average Eccentricity": 17.915483657972928, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0003301419610432486, "Average Shortest Path": 10.92681010033786, "mean_complexity": 13.0, "total_complexity": 52.0, "mean_token_count": 634.25, "total_token_count": 2537.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "e2364080-196b-4222-b922-c12f0cb3d0bc", "fitness": 0.6092621233996897, "name": "AdaptiveExponentialSubspaceDescent", "description": "Adaptive Exponential Subspace Descent (AESD) \u2014 pick small random subspaces, probe along random normalized directions with exponentially growing step sizes to rapidly exploit useful directions, adapt per-coordinate step scales, and occasionally perform heavy-tail global jumps for escapes.", "code": "import numpy as np\n\nclass AdaptiveExponentialSubspaceDescent:\n    \"\"\"\n    AESD: adaptive exponential step growth on random low-dimensional subspaces.\n    - Strict box [-5,5] default bounds support.\n    - Budget-safe evaluations.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_frac=0.02,\n                 success_mult=1.5, failure_mult=0.7,\n                 global_prob=0.03, max_dbl=6, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_frac = float(init_frac)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.global_prob = float(global_prob)\n        self.max_dbl = int(max_dbl)\n        self.rng = np.random.default_rng(seed)\n\n    def _bounds(self, func):\n        if getattr(func, \"bounds\", None) is not None and \\\n           hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        dim = self.dim\n        rng = self.rng\n\n        scale = np.linalg.norm(ub - lb)\n        base_step = max(1e-12, self.init_frac * scale)\n        min_step = 1e-12 * scale\n        max_step = 1.5 * scale\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # init\n        x_best = rng.uniform(lb, ub)\n        f_best = safe_eval(x_best)\n\n        steps = np.full(dim, base_step, dtype=float)\n\n        # main loop\n        while evals < self.budget:\n            # occasional heavy-tail global jump\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim)\n                c = np.tan(np.pi * (u - 0.5))\n                jump = 0.2 * scale * c\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                # continue\n\n            # choose subspace size: usually 1 or 2, sometimes up to 4\n            if rng.random() < 0.75 or dim == 1:\n                k = 1\n            else:\n                k = min(dim, 1 + rng.integers(1, 4))\n            idx = rng.choice(dim, size=k, replace=False)\n\n            # random (Gaussian) direction on subspace, normalized on subspace\n            v = np.zeros(dim, dtype=float)\n            rv = rng.normal(size=k)\n            norm = np.linalg.norm(rv)\n            if norm == 0:\n                rv = np.ones(k)\n                norm = np.linalg.norm(rv)\n            rv /= norm\n            v[idx] = rv\n\n            # base step derived from involved coordinates' steps\n            step0 = max(min(np.mean(steps[idx]), max_step), min_step)\n\n            best_local_x = x_best.copy()\n            best_local_f = f_best\n            improved = False\n\n            # exponential probing: try step0 * 2^t along +v and -v separately (greedy)\n            for sign in (1.0, -1.0):\n                s = step0\n                t = 0\n                while evals < self.budget and t < self.max_dbl:\n                    xt = np.clip(best_local_x + sign * s * v, lb, ub)\n                    ft = safe_eval(xt)\n                    if ft < best_local_f:\n                        # accept this probe and attempt to grow further along same ray\n                        best_local_x = xt.copy()\n                        best_local_f = ft\n                        improved = True\n                        s *= 2.0\n                        t += 1\n                        # continue doubling until failure or max_dbl\n                        continue\n                    else:\n                        # failure: stop doubling in this direction\n                        break\n\n            # If we found an improvement in local probing, update global incumbent and boost steps\n            if improved and best_local_f < f_best:\n                # accept largest local improvement\n                x_best = best_local_x.copy()\n                f_best = float(best_local_f)\n                steps[idx] = np.minimum(max_step, steps[idx] * self.success_mult)\n            else:\n                # no improvement: shrink steps on touched coords\n                steps[idx] = np.maximum(min_step, steps[idx] * self.failure_mult)\n\n            # small randomized local polish occasionally\n            if rng.random() < 0.12 and evals < self.budget:\n                d = rng.integers(dim)\n                delta = steps[d] * (1.0 + rng.normal(scale=0.5))\n                xt = np.clip(x_best + (rng.choice([-1.0, 1.0]) * delta) * np.eye(1, dim, d).reshape(-1), lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft)\n                    steps[d] = min(max_step, steps[d] * self.success_mult)\n                else:\n                    steps[d] = max(min_step, steps[d] * self.failure_mult)\n\n        # final tiny coordinate sweep polish\n        eps = 1e-3 * scale\n        d = 0\n        while evals < self.budget:\n            xt = x_best.copy()\n            xt[d] = np.clip(xt[d] + eps, lb[d], ub[d])\n            ft = safe_eval(xt)\n            if ft < f_best:\n                x_best, f_best = xt.copy(), float(ft)\n                continue\n            xt = x_best.copy()\n            xt[d] = np.clip(xt[d] - eps, lb[d], ub[d])\n            ft = safe_eval(xt)\n            if ft < f_best:\n                x_best, f_best = xt.copy(), float(ft)\n                continue\n            d = (d + 1) % dim\n            # reduce eps occasionally to try smaller moves\n            if d == 0:\n                eps *= 0.5\n                if eps < 1e-9 * scale:\n                    break\n\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 17, "feedback": "The algorithm AdaptiveExponentialSubspaceDescent scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2ce99025-003a-46b2-bec0-52c377bfc061"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9729119689031223}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.970786581483672}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9785230306370083}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9563443207351459}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9562399150446265}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9625897759879584}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.049345479150072036}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08808097587457364}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.07412922445978343}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.08473954634782199}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.05992822687434296}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9956616545462238}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9948733983213962}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.994727752629598}], "aucs": [0.9729119689031223, 0.970786581483672, 0.9785230306370083, 0.9563443207351459, 0.9562399150446265, 0.9625897759879584, 4.999999999999449e-05, 0.049345479150072036, 0.08808097587457364, 0.07412922445978343, 0.08473954634782199, 0.05992822687434296, 0.9956616545462238, 0.9948733983213962, 0.994727752629598]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1346.0, "Edges": 1345.0, "Max Degree": 21.0, "Min Degree": 1.0, "Mean Degree": 1.9985141158989599, "Degree Variance": 1.9925683716432379, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.463815789473684, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3319834826940369, "Depth Entropy": 1.9694123279523748, "Assortativity": 0.0, "Average Eccentricity": 16.028231797919762, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.0007429420505200594, "Average Shortest Path": 9.48920386440341, "mean_complexity": 7.75, "total_complexity": 31.0, "mean_token_count": 293.0, "total_token_count": 1172.0, "mean_parameter_count": 3.5, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "09a62062-9dd4-48da-aad0-e98e79fa0af6", "fitness": 0.5548433941703876, "name": "ThompsonCoordinateBanditV3", "description": "Thompson-Coordinate Bandit v3 \u2014 compact single-eval coordinate Thompson sampling with lightweight momentum, adaptive multiplicative steps, headroom-aware sign penalization, occasional Cauchy escapes and tiny uniform restarts to overcome stagnation.", "code": "import numpy as np\n\nclass ThompsonCoordinateBanditV3:\n    \"\"\"\n    Compact Thompson-Coordinate Bandit v3\n\n    - Strictly single-eval moves.\n    - Per-dim Beta priors for +/-, sampled to pick (dim,sign).\n    - Adaptive multiplicative steps with bounds, lightweight momentum to bias useful directions.\n    - Headroom-aware penalization (avoid pushing out of box).\n    - Occasional Cauchy escapes and tiny uniform restarts on stagnation.\n    \"\"\"\n    def __init__(self, budget=1000, dim=10, init_step=0.12, success_mult=1.25,\n                 failure_mult=0.65, min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.035, restart_prob=0.007, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.succ_mult = float(success_mult)\n        self.fail_mult = float(failure_mult)\n        self.min_frac = float(min_step_frac)\n        self.max_frac = float(max_step_frac)\n        self.global_prob = float(global_prob)\n        self.restart_prob = float(restart_prob)\n\n    def __call__(self, func):\n        # bounds detection (default [-5,5])\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        box = ub - lb\n        scale = max(1e-12, np.linalg.norm(box))\n        base = max(1e-12, self.init_step * scale)\n        min_step = max(1e-15, self.min_frac * scale)\n        max_step = max(1e-12, self.max_frac * scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x))\n            evals += 1\n            return v\n\n        # init: center + one random if possible\n        x = np.clip(np.zeros(self.dim), lb, ub)\n        f = safe_eval(x)\n        if evals < self.budget:\n            xr = self.rng.uniform(lb, ub)\n            fr = safe_eval(xr)\n            if fr < f: x, f = xr.copy(), fr\n        x_best = x.copy(); f_best = float(f)\n\n        # per-dim state\n        steps = np.full(self.dim, base)\n        a_pos = np.ones(self.dim); b_pos = np.ones(self.dim)\n        a_neg = np.ones(self.dim); b_neg = np.ones(self.dim)\n        momentum = np.zeros(self.dim)    # in [-1,1], biases sign selection\n        no_improve = 0\n\n        while evals < self.budget:\n            # occasional heavy-tail escape\n            if self.rng.random() < self.global_prob and evals < self.budget:\n                u = self.rng.random(self.dim)\n                c = np.tan(np.pi * (u - 0.5))\n                jump = 0.12 * scale * c\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                    no_improve = 0\n                else:\n                    no_improve += 1\n                continue\n\n            # tiny uniform restart on stagnation\n            if no_improve > 50 and self.rng.random() < self.restart_prob and evals < self.budget:\n                xr = self.rng.uniform(lb, ub)\n                fr = safe_eval(xr)\n                if fr < f_best:\n                    x_best, f_best = xr.copy(), fr\n                    no_improve = 0\n                else:\n                    no_improve += 1\n                continue\n\n            # Thompson sampling per-sign per-dim\n            p_pos = self.rng.beta(a_pos, b_pos)\n            p_neg = self.rng.beta(a_neg, b_neg)\n\n            # headroom: penalize moves toward bounds\n            headroom_pos = np.clip((ub - x_best) / (box + 1e-18), 0.0, 1.0)\n            headroom_neg = np.clip((x_best - lb) / (box + 1e-18), 0.0, 1.0)\n\n            # combine with momentum (momentum ~ [-1,1], add small bias toward last-winning sign)\n            pos_score = (p_pos + 0.25 * np.clip(momentum, 0, 1)) * headroom_pos * steps\n            neg_score = (p_neg + 0.25 * np.clip(-momentum, 0, 1)) * headroom_neg * steps\n\n            # choose best dim/sign\n            # tiny jitter to break ties\n            pos_score += 1e-14 * self.rng.random(self.dim)\n            neg_score += 1e-14 * self.rng.random(self.dim)\n            combined = np.maximum(pos_score, neg_score)\n            d = int(np.argmax(combined))\n            sign = 1 if pos_score[d] >= neg_score[d] else -1\n\n            # propose single-eval coordinate move\n            s = np.clip(steps[d], min_step, max_step)\n            xt = x_best.copy()\n            xt[d] = np.clip(xt[d] + sign * s, lb[d], ub[d])\n            ft = safe_eval(xt)\n\n            # update priors and state\n            if sign > 0:\n                if ft < f_best:\n                    a_pos[d] += 1.0\n                else:\n                    b_pos[d] += 1.0\n            else:\n                if ft < f_best:\n                    a_neg[d] += 1.0\n                else:\n                    b_neg[d] += 1.0\n\n            if ft < f_best:\n                # accept\n                x_best, f_best = xt.copy(), float(ft)\n                steps[d] = min(max_step, steps[d] * self.succ_mult)\n                # update momentum toward successful sign\n                momentum[d] = 0.7 * momentum[d] + 0.3 * sign\n                no_improve = 0\n            else:\n                # reject\n                steps[d] = max(min_step, steps[d] * self.fail_mult)\n                # decay momentum slightly away from failing sign\n                momentum[d] *= 0.92\n                no_improve += 1\n\n            # occasional mild prior shrink to avoid runaway counts\n            if (a_pos[d] + b_pos[d] + a_neg[d] + b_neg[d]) > 200:\n                a_pos[d] = 1.0 + (a_pos[d]-1.0)*0.85\n                b_pos[d] = 1.0 + (b_pos[d]-1.0)*0.85\n                a_neg[d] = 1.0 + (a_neg[d]-1.0)*0.85\n                b_neg[d] = 1.0 + (b_neg[d]-1.0)*0.85\n\n        # final cheap polish: one-sided small probes per-dim while budget remains\n        mesh = 0.05 * scale\n        while evals < self.budget and mesh > 1e-12 * scale:\n            improved = False\n            for d in range(self.dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - mesh, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True\n            if not improved:\n                mesh *= 0.5\n\n        self.x_opt = np.asarray(x_best, float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 17, "feedback": "The algorithm ThompsonCoordinateBanditV3 scored 0.555 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2ce99025-003a-46b2-bec0-52c377bfc061"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9868078260703351}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9831048642338879}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9776640673734125}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9475417143736091}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.950371588740096}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.959451924066464}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0822280877192898}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.07442941744451781}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08502579624882778}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.05672759024890095}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.03875926711016908}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.05075208368829276}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.935450525001837}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9656430772826894}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.22869308295348512}], "aucs": [0.9868078260703351, 0.9831048642338879, 0.9776640673734125, 0.9475417143736091, 0.950371588740096, 0.959451924066464, 0.0822280877192898, 0.07442941744451781, 0.08502579624882778, 0.05672759024890095, 0.03875926711016908, 0.05075208368829276, 0.935450525001837, 0.9656430772826894, 0.22869308295348512]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1642.0, "Edges": 1641.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9987819732034104, "Degree Variance": 2.282580733219493, "Transitivity": 0.0, "Max Depth": 12.0, "Min Depth": 2.0, "Mean Depth": 7.416993464052288, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3217616621802377, "Depth Entropy": 1.87272192401531, "Assortativity": 0.0, "Average Eccentricity": 14.40803897685749, "Diameter": 19.0, "Radius": 10.0, "Edge Density": 0.0006090133982947625, "Average Shortest Path": 9.128142950771974, "mean_complexity": 10.666666666666666, "total_complexity": 32.0, "mean_token_count": 482.3333333333333, "total_token_count": 1447.0, "mean_parameter_count": 4.666666666666667, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "b6dfd3db-5c46-4441-a54a-34c5ad9bdd18", "fitness": 0.6264503378473399, "name": "AdaptiveOrthogonalPerturbationDescent", "description": "Adaptive Orthogonal Perturbation Descent (AOPD) \u2014 pick small random orthogonal subspaces (mostly 1-D), perform cheap 2-point directional probes biased by archive medians, adapt per-coordinate step-sizes multiplicatively and use periodic median-pulls and Cauchy jumps to escape stagnation.", "code": "import numpy as np\n\nclass AdaptiveOrthogonalPerturbationDescent:\n    \"\"\"\n    Adaptive Orthogonal Perturbation Descent (AOPD)\n\n    - Mostly degree-1 (single-coordinate) probes, occasionally 2-D orthogonal probes.\n    - Maintain a small archive of best samples; use coordinate-wise medians to bias probe directions.\n    - Use 1-2 cheap function evaluations per iteration, adapt per-coordinate step sizes multiplicatively.\n    - On success enlarge step for involved coords; on failure shrink them. Periodic median-pulls and\n      Cauchy jumps help escape stagnation.\n    \"\"\"\n    def __init__(self, budget, dim, archive_size=12, init_step=None,\n                 min_step=1e-6, max_step=None, stagnation_patience=12, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def __call__(self, func):\n        lb = -5.0\n        ub = 5.0\n        lb = np.full(self.dim, float(lb))\n        ub = np.full(self.dim, float(ub))\n        span = ub - lb\n\n        # initial per-coordinate steps\n        if self.init_step is None:\n            step = np.full(self.dim, 0.25 * span)\n        else:\n            s0 = np.array(self.init_step, dtype=float)\n            step = s0 if s0.shape == (self.dim,) else np.full(self.dim, float(s0))\n\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            ms = np.array(self.max_step, dtype=float)\n            max_step = ms if ms.shape == (self.dim,) else np.full(self.dim, float(ms))\n\n        step = np.clip(step, self.min_step, max_step)\n\n        # archive of best points\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n\n        # initial sample(s)\n        n0 = max(1, min(self.dim, self.budget // 15 + 1))\n        for _ in range(n0):\n            x0 = lb + self.rng.random(self.dim) * span\n            f0 = float(func(x0))\n            evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n            if evals >= self.budget:\n                break\n\n        if len(archive_x) == 0:\n            x0 = lb + self.rng.random(self.dim) * span\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n\n        best_idx = int(np.argmin(archive_f))\n        x_best = archive_x[best_idx].copy()\n        f_best = float(archive_f[best_idx])\n\n        no_improve = 0\n\n        def archive_add(xn, fn):\n            if len(archive_x) < self.archive_size:\n                archive_x.append(xn.copy()); archive_f.append(float(fn))\n            else:\n                w = int(np.argmax(archive_f))\n                if fn < archive_f[w]:\n                    archive_x[w] = xn.copy(); archive_f[w] = float(fn)\n\n        # main loop\n        while evals < self.budget:\n            # choose subspace size: mostly 1, sometimes 2 (if dim>=2)\n            k = 2 if (self.dim >= 2 and self.rng.random() < 0.18) else 1\n            idx = self.rng.choice(self.dim, size=k, replace=False)\n            # random orthogonal direction in that subspace\n            d = np.zeros(self.dim)\n            v = self.rng.normal(size=k)\n            vn = np.linalg.norm(v)\n            if vn == 0:\n                v = np.ones(k); vn = np.linalg.norm(v)\n            d[idx] = v / vn\n\n            # bias by archive median projection\n            if len(archive_x) >= 2:\n                med = np.median(np.array(archive_x), axis=0)\n                proj = float(np.dot(med - x_best, d))\n            else:\n                proj = 0.0\n\n            # base step for this probe = mean of steps on involved coords, jitter slightly\n            s_base = max(self.min_step, float(step[idx].mean() * (1.0 + 0.18 * (self.rng.random() - 0.5))))\n            # choose sign biased by projection (if proj>0 prefer +)\n            if proj > 1e-12:\n                sign = 1 if self.rng.random() < 0.75 else -1\n            elif proj < -1e-12:\n                sign = -1 if self.rng.random() < 0.75 else 1\n            else:\n                sign = 1 if self.rng.random() < 0.5 else -1\n\n            s = sign * s_base\n\n            # probe forward\n            x1 = x_best + s * d\n            x1 = np.minimum(np.maximum(x1, lb), ub)\n            if evals >= self.budget:\n                break\n            f1 = float(func(x1)); evals += 1\n            archive_add(x1, f1)\n\n            improved = False\n            if f1 < f_best:\n                # accept and enlarge steps on involved coords\n                f_best = f1; x_best = x1.copy()\n                step[idx] = np.minimum(max_step[idx], step[idx] * 1.25 + 1e-12)\n                no_improve = 0\n                improved = True\n                # cheap extrapolation (same direction, half size) if budget allows\n                if evals < self.budget:\n                    xe = x_best + 0.5 * s * d\n                    xe = np.minimum(np.maximum(xe, lb), ub)\n                    fe = float(func(xe)); evals += 1\n                    archive_add(xe, fe)\n                    if fe < f_best:\n                        f_best = fe; x_best = xe.copy()\n                        step[idx] = np.minimum(max_step[idx], step[idx] * 1.1 + 1e-12)\n            else:\n                # try opposite sign once\n                if evals < self.budget:\n                    x2 = x_best - s * d\n                    x2 = np.minimum(np.maximum(x2, lb), ub)\n                    f2 = float(func(x2)); evals += 1\n                    archive_add(x2, f2)\n                    if f2 < f_best:\n                        f_best = f2; x_best = x2.copy()\n                        step[idx] = np.minimum(max_step[idx], step[idx] * 1.20 + 1e-12)\n                        no_improve = 0\n                        improved = True\n                if not improved:\n                    # failure: shrink steps on involved coords\n                    step[idx] = np.maximum(self.min_step, step[idx] * 0.65)\n                    no_improve += 1\n\n            # maintain archive size and best pointer\n            if len(archive_x) > 0:\n                bi = int(np.argmin(archive_f))\n                f_best_archive = float(archive_f[bi])\n                if f_best_archive < f_best:\n                    f_best = f_best_archive; x_best = archive_x[bi].copy()\n\n            # stagnation handling\n            if no_improve >= self.stagnation_patience and evals < self.budget:\n                no_improve = 0\n                # attempt median pull (cheap directed move)\n                if len(archive_x) >= 2:\n                    med = np.median(np.array(archive_x), axis=0)\n                    pull = 0.45 * (med - x_best)\n                    if np.linalg.norm(pull) > 0:\n                        xp = x_best + pull\n                        xp = np.minimum(np.maximum(xp, lb), ub)\n                        fpu = float(func(xp)); evals += 1\n                        archive_add(xp, fpu)\n                        if fpu < f_best:\n                            f_best = fpu; x_best = xp.copy()\n                            # inflate steps on coordinates that moved\n                            moved = np.where(np.abs(pull) > 1e-12)[0]\n                            if moved.size > 0:\n                                step[moved] = np.minimum(max_step[moved], step[moved] * 1.4)\n                            continue\n                # otherwise or if median pull failed: heavy-tailed jump (Cauchy) scaled by span\n                if evals < self.budget:\n                    scale = 0.6 * span\n                    jump = self.rng.standard_cauchy(self.dim) * scale\n                    # temper extremes to bounds\n                    jump = np.clip(jump, -2.0 * span, 2.0 * span)\n                    xj = x_best + jump * (self.rng.random(self.dim) < 0.6)\n                    xj = np.minimum(np.maximum(xj, lb), ub)\n                    fj = float(func(xj)); evals += 1\n                    archive_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step = np.minimum(max_step, step * (1.0 + 0.5 * self.rng.random(self.dim)))\n                        continue\n                    else:\n                        # gentle step reset to allow fresh exploration\n                        step = np.minimum(max_step, np.maximum(self.min_step, step * (1.0 + 0.25 * (self.rng.random(self.dim) - 0.5))))\n                        # demote archive influence slightly\n                        if len(archive_x) > 3:\n                            # randomly discard one worst to diversify archive\n                            w = int(np.argmax(archive_f))\n                            archive_x.pop(w); archive_f.pop(w)\n\n            # small global decay to avoid runaway steps\n            step = np.minimum(max_step, np.maximum(self.min_step, step * (1.0 - 1e-4)))\n            # safety: if nearly exhausted budget, break\n            if evals >= self.budget:\n                break\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 17, "feedback": "The algorithm AdaptiveOrthogonalPerturbationDescent scored 0.626 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9800451616164693}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9808444322200287}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9818162546502593}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9596161356356447}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9670836732978842}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9603738034749605}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0697066494010039}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10906285308266084}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.11459806748342949}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10881798933744069}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.081876876233555}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.09525267181316888}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9962941590935221}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9964047998560247}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9949615405140463}], "aucs": [0.9800451616164693, 0.9808444322200287, 0.9818162546502593, 0.9596161356356447, 0.9670836732978842, 0.9603738034749605, 0.0697066494010039, 0.10906285308266084, 0.11459806748342949, 0.10881798933744069, 0.081876876233555, 0.09525267181316888, 0.9962941590935221, 0.9964047998560247, 0.9949615405140463]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1895.0, "Edges": 1894.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.9989445910290238, "Degree Variance": 1.922954031230638, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.403055229142186, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3226511074590375, "Depth Entropy": 2.204977922347569, "Assortativity": 0.0, "Average Eccentricity": 19.82532981530343, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.0005277044854881266, "Average Shortest Path": 10.314405998110963, "mean_complexity": 14.0, "total_complexity": 42.0, "mean_token_count": 556.6666666666666, "total_token_count": 1670.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "cd6944c9-d56c-46d8-bafb-55356bae9dd3", "fitness": 0.6173162464399721, "name": "MirroredAdaptiveQuadratic", "description": "Mirror-guided coordinate descent with compact per-dim momentum, EWMA curvature, multiplicative step adaptation and rare Cauchy L\u00e9vy jumps for robust exploitation and escapes.", "code": "import numpy as np\n\nclass MirroredAdaptiveQuadratic:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 success_mult=1.33, failure_mult=0.62,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, extrapolate_gain=1.6,\n                 polish_frac=0.04, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob); self.extrapolate_gain = float(extrapolate_gain)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box_scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = max(1e-12, self.max_step_frac * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base, float)\n        trials = np.zeros(dim, int); succ = np.zeros(dim, int)\n        curv = np.zeros(dim, float)\n        mom = np.zeros(dim, float)      # momentum in [-1,1]\n        alpha = 0.16                    # EWMA for curvature\n        while evals < self.budget:\n            # rare heavy-tailed global escape\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim) - 0.5\n                jump = 0.12 * box_scale * np.tan(np.pi * u)\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                continue\n\n            # pick coordinate: prefer large step, low trials, and momentum\n            score = steps * (1.0 + 1.0/(1.0+trials)) * (1.0 + 0.25*np.abs(mom))\n            d = int(np.argmax(score + 1e-12 * rng.random(dim)))\n\n            s = float(np.clip(steps[d], min_step, max_step))\n            # incorporate anchor shift from momentum for trial center\n            anchor = x_best.copy()\n            anchor[d] = np.clip(anchor[d] + 0.12 * s * mom[d], lb[d], ub[d])\n\n            # mirrored two-sided probe when possible\n            if evals <= self.budget - 2:\n                xp = anchor.copy(); xm = anchor.copy()\n                xp[d] = np.clip(xp[d] + s, lb[d], ub[d])\n                xm[d] = np.clip(xm[d] - s, lb[d], ub[d])\n                fp = safe_eval(xp); fm = safe_eval(xm); trials[d] += 2\n                # accept best side\n                if fp < f_best or fm < f_best:\n                    if fp < fm:\n                        chosen, fch, side = xp.copy(), float(fp), +1.0\n                    else:\n                        chosen, fch, side = xm.copy(), float(fm), -1.0\n                    x_best, f_best = chosen, fch\n                    succ[d] += 1\n                    mom[d] = 0.7*mom[d] + 0.3*side\n                    steps[d] = min(max_step, steps[d] * self.success_mult)\n                    # opportunistic extrapolation using sign + momentum\n                    if evals < self.budget:\n                        ext = x_best.copy()\n                        ext[d] = np.clip(ext[d] + side * s * (self.extrapolate_gain + 0.2*abs(mom[d])), lb[d], ub[d])\n                        fext = safe_eval(ext); trials[d] += 1\n                        if fext < f_best:\n                            x_best, f_best = ext.copy(), float(fext)\n                            succ[d] += 1\n                            steps[d] = min(max_step, steps[d] * self.success_mult)\n                else:\n                    # curvature estimate using center approx (use f_best as proxy)\n                    est_curv = (fp + fm - 2.0 * f_best) / (s*s + 1e-24)\n                    curv[d] = (1-alpha)*curv[d] + alpha*est_curv\n                    # negative curvature -> try extrapolate along the downhill side\n                    if est_curv < -1e-12 and evals < self.budget:\n                        side = -np.sign(fp - fm) if (fp!=fm) else np.sign(mom[d]) or 1.0\n                        ext = anchor.copy()\n                        ext[d] = np.clip(ext[d] + side * s * self.extrapolate_gain, lb[d], ub[d])\n                        fext = safe_eval(ext); trials[d] += 1\n                        if fext < f_best:\n                            x_best, f_best = ext.copy(), float(fext)\n                            succ[d] += 1\n                            mom[d] = 0.6*mom[d] + 0.4*side\n                            steps[d] = min(max_step, steps[d] * self.success_mult)\n                        else:\n                            steps[d] = max(min_step, steps[d] * self.failure_mult)\n                    else:\n                        steps[d] = max(min_step, steps[d] * self.failure_mult)\n                        mom[d] *= 0.9\n            else:\n                # single eval remaining -> try best one-sided + momentum\n                if rng.random() < 0.5:\n                    xt = anchor.copy(); xt[d] = np.clip(xt[d] + s, lb[d], ub[d]); side = +1.0\n                else:\n                    xt = anchor.copy(); xt[d] = np.clip(xt[d] - s, lb[d], ub[d]); side = -1.0\n                ft = safe_eval(xt); trials[d] += 1\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); succ[d] += 1\n                    mom[d] = 0.7*mom[d] + 0.3*side\n                    steps[d] = min(max_step, steps[d] * self.success_mult)\n                else:\n                    steps[d] = max(min_step, steps[d] * self.failure_mult)\n                    mom[d] *= 0.9\n\n            # mild rescue for stale dims\n            if trials[d] > 80 and succ[d] == 0:\n                steps[d] = min(max_step, steps[d] * 1.4)\n                trials[d] = int(trials[d] * 0.6)\n\n        # final polish: small systematic +/- probes while budget\n        fin = self.polish_frac * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 17, "feedback": "The algorithm MirroredAdaptiveQuadratic scored 0.617 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["82379331-5855-488c-a7d5-53d7afbbd2bb"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9747338740507127}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9710677439763619}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9653204007503801}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9369693183944957}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9310651854978635}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9354103836806792}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.12107359414840102}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08187162832857398}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.11452355116795299}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.09907266731373843}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06355042952723577}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07416638631586026}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9964657234331784}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9975486514013631}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9969041586127855}], "aucs": [0.9747338740507127, 0.9710677439763619, 0.9653204007503801, 0.9369693183944957, 0.9310651854978635, 0.9354103836806792, 0.12107359414840102, 0.08187162832857398, 0.11452355116795299, 0.09907266731373843, 0.06355042952723577, 0.07416638631586026, 0.9964657234331784, 0.9975486514013631, 0.9969041586127855]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2046.0, "Edges": 2045.0, "Max Degree": 25.0, "Min Degree": 1.0, "Mean Degree": 1.9990224828934506, "Degree Variance": 2.052784968213972, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.435194942044257, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.325192642058879, "Depth Entropy": 2.1460642328725577, "Assortativity": 5.726693680299218e-09, "Average Eccentricity": 17.40175953079179, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0004887585532746823, "Average Shortest Path": 10.204029091291494, "mean_complexity": 12.0, "total_complexity": 36.0, "mean_token_count": 582.6666666666666, "total_token_count": 1748.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "f41a8cbb-59fa-4b18-963e-9895d3465403", "fitness": 0.263622787613021, "name": "AdaptiveRotatingSubspaceSearch", "description": "Adaptive Rotating Subspace Search (ARSS) \u2014 mostly 1-D adaptive per-dimension steps with occasional small random 2-D rotations, momentum-driven sign bias, archive-driven recombination and rare L\u00e9vy escapes for robust, cheap global exploration.", "code": "import numpy as np\n\nclass AdaptiveRotatingSubspaceSearch:\n    def __init__(self, budget=1000, dim=10, step0=0.08,\n                 succ=1.22, fail=0.68, min_frac=1e-9, max_frac=1.4,\n                 escape_prob=0.035, levy_scale=0.12, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.step0 = float(step0); self.succ = float(succ); self.fail = float(fail)\n        self.min_frac = float(min_frac); self.max_frac = float(max_frac)\n        self.escape_prob = float(escape_prob); self.levy_scale = float(levy_scale)\n\n    def __call__(self, func):\n        rng = self.rng; dim = self.dim\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n        box = np.maximum(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.step0 * box)\n        min_step = max(1e-15, self.min_frac * box)\n        max_step = max(1e-12, self.max_frac * box)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base)\n        mom = np.zeros(dim)\n        last_moves = []  # archive of recent successful displacement vectors\n        no_improve = 0\n\n        while evals < self.budget:\n            # rare heavy-tailed escape\n            if rng.random() < self.escape_prob and evals < self.budget:\n                u = rng.random(dim)\n                jump = self.levy_scale * box * np.tan(np.pi * (u - 0.5))\n                # random subspace mask (small fraction of dims)\n                k = max(1, int(1 + dim * 0.12))\n                mask = np.zeros(dim, bool); mask[rng.choice(dim, k, replace=False)] = True\n                xg = np.clip(x_best + jump * mask, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                    no_improve = 0\n                else:\n                    no_improve += 1\n                continue\n\n            # choose subspace size: mostly 1-D, occasionally 2-D\n            if rng.random() < 0.85 or dim == 1:\n                k = 1\n            else:\n                k = 2\n\n            # sampling dims using step-weighted probabilities (favor larger steps)\n            p = steps + 1e-12\n            p = p / p.sum()\n            idx = rng.choice(dim, size=k, replace=False, p=p)\n            # construct direction: sign biased by momentum\n            sgn = np.sign(mom[idx])\n            # where momentum is near zero, pick random sign\n            rnd = rng.random(len(idx))\n            sgn = np.where(np.abs(sgn) < 1e-6, np.where(rnd < 0.5, -1.0, 1.0), np.sign(sgn))\n            # small random rotation mixing if k==2\n            if k == 2:\n                theta = rng.normal(0, 0.35)\n                rot = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n                v = rot.dot(np.array(sgn))\n                dir_vec = np.zeros(dim); dir_vec[idx] = v\n                scale = np.sqrt((steps[idx] ** 2).sum())\n                cand = x_best + np.clip(dir_vec, -1, 1) * scale\n            else:\n                d = idx[0]\n                cand = x_best.copy(); cand[d] = np.clip(cand[d] + sgn[0] * steps[d], lb[d], ub[d])\n\n            cand = np.clip(cand, lb, ub)\n            fc = safe_eval(cand)\n            if fc < f_best:\n                # accept\n                disp = cand - x_best\n                x_best, f_best = cand.copy(), float(fc)\n                no_improve = 0\n                # update steps for involved dims\n                for j in np.nonzero(disp)[0]:\n                    steps[j] = min(max_step, steps[j] * self.succ)\n                    mom[j] = mom[j] * 0.6 + np.sign(disp[j]) * 0.9\n                last_moves.append(disp)\n                if len(last_moves) > 6: last_moves.pop(0)\n                # opportunistic extrapolation along disp (one cheap probe)\n                if evals < self.budget:\n                    ext = np.clip(x_best + disp, lb, ub)\n                    fe = safe_eval(ext)\n                    if fe < f_best:\n                        x_best, f_best = ext.copy(), float(fe)\n            else:\n                # reject: shrink steps of probed dims and decay momentum\n                no_improve += 1\n                for j in idx:\n                    steps[j] = max(min_step, steps[j] * self.fail)\n                    mom[j] *= 0.7\n\n            # recombination when stagnating: try sum of recent successful moves\n            if no_improve >= 10 and last_moves:\n                agg = np.sum(last_moves[-3:], axis=0)\n                if np.any(agg):\n                    norm = np.linalg.norm(agg)\n                    if norm > 0:\n                        trial = np.clip(x_best + 0.5 * (agg / norm) * box * 0.06, lb, ub)\n                        ft = safe_eval(trial)\n                        if ft < f_best:\n                            x_best, f_best = trial.copy(), float(ft)\n                            for j in np.nonzero(trial - x_best)[0]:\n                                steps[j] = min(max_step, steps[j] * self.succ)\n                            no_improve = 0\n                        else:\n                            # mild global shrink\n                            steps *= 0.98\n                            no_improve += 1\n\n            # mild annealing of steps and momentum regularization\n            steps = np.clip(steps, min_step, max_step)\n            mom *= 0.995\n\n        # final polish: tiny coordinate probes\n        fin = 1e-3 * box\n        while evals < self.budget and fin > 1e-12 * box:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 17, "feedback": "The algorithm AdaptiveRotatingSubspaceSearch scored 0.264 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a569da3-dff0-437a-a1e1-9e9baee72739"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.18779160623062618}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.18058961965937725}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.20175106245181607}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.03011331168278153}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.061777799552238166}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.059842468548572114}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.044320483134544175}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.04296156249516658}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.042288188905131974}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.03173902892482461}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.024335900515480402}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.06047414438792198}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9967551137487897}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9956163476833554}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9939851762746887}], "aucs": [0.18779160623062618, 0.18058961965937725, 0.20175106245181607, 0.03011331168278153, 0.061777799552238166, 0.059842468548572114, 0.044320483134544175, 0.04296156249516658, 0.042288188905131974, 0.03173902892482461, 0.024335900515480402, 0.06047414438792198, 0.9967551137487897, 0.9956163476833554, 0.9939851762746887]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1679.0, "Edges": 1678.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.9988088147706968, "Degree Variance": 2.085763917587577, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 7.926797385620915, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3318088305342841, "Depth Entropy": 2.0864671017838456, "Assortativity": 0.0, "Average Eccentricity": 17.03752233472305, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0005955926146515784, "Average Shortest Path": 9.83501019748261, "mean_complexity": 12.0, "total_complexity": 36.0, "mean_token_count": 495.3333333333333, "total_token_count": 1486.0, "mean_parameter_count": 4.666666666666667, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "e50ae5cc-1f9b-4a78-84ea-43b33f8436e0", "fitness": 0.202885488565055, "name": "DualScaleArchiveAnnealer", "description": "Dual-Scale Archive Annealer (DSAA) \u2014 mix compact archive-directed vector jumps with cheap coordinate pulls and occasional 3-point quadratic interpolation, adapt per-dim step scales multiplicatively and anneal exploration.", "code": "import numpy as np\n\nclass DualScaleArchiveAnnealer:\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 grow=1.3, shrink=0.6, archive_size=5,\n                 global_prob=0.04, interp_prob=0.22, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.grow = float(grow); self.shrink = float(shrink)\n        self.K = int(archive_size)\n        self.global_prob = float(global_prob)\n        self.interp_prob = float(interp_prob)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box_scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step * box_scale)\n        min_step = 1e-12 * box_scale; max_step = 0.8 * box_scale\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base, float)\n        temp = np.full(dim, 1.0, float)   # exploration annealer\n        archive = [(f_best, x_best.copy())]\n\n        # helper to maintain archive of best K\n        def arch_add(fx, x):\n            archive.append((fx, x.copy()))\n            archive.sort(key=lambda t: t[0])\n            if len(archive) > self.K: archive.pop()\n\n        # main loop\n        while evals < self.budget:\n            # occasional archive-guided vector jump (one-eval)\n            if rng.random() < self.global_prob and len(archive) > 1 and evals < self.budget:\n                # centroid of good points and a heavy-tailed scale\n                pts = np.stack([p for _, p in archive], 0)\n                centroid = pts.mean(0)\n                direction = centroid - x_best\n                if np.allclose(direction, 0):\n                    # random heavy-tailed jump\n                    step = 0.25 * box_scale * np.tan(np.pi * (rng.random(dim) - 0.5))\n                    xt = np.clip(x_best + step, lb, ub)\n                else:\n                    # scale direction and add Cauchy noise\n                    sc = 0.9 * rng.random() + 0.1\n                    noise = 0.04 * box_scale * np.tan(np.pi * (rng.random(dim) - 0.5))\n                    xt = np.clip(x_best + sc * direction + noise, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft)\n                    arch_add(f_best, x_best)\n\n            # choose coordinate proportional to step*temp (favor large/uncertain)\n            weights = steps * (0.5 + 0.5 * temp)\n            if weights.sum() <= 0: weights = np.ones(dim)\n            probs = weights / weights.sum()\n            d = rng.choice(dim, p=probs)\n\n            # choose sign biased by headroom\n            head_pos = (ub[d] - x_best[d]) / (ub[d] - lb[d] + 1e-18)\n            head_neg = (x_best[d] - lb[d]) / (ub[d] - lb[d] + 1e-18)\n            sign = 1 if rng.random() < 0.5*(1+ (head_pos - head_neg)) else -1\n\n            # propose single-eval coordinate move with small gaussian perturbation\n            noise = (rng.normal() * 0.25) * steps[d]\n            s = np.clip(steps[d] * (1.0 + 0.6 * rng.random()), min_step, max_step)\n            xt = x_best.copy(); xt[d] = np.clip(xt[d] + sign * s + noise, lb[d], ub[d])\n            ft = safe_eval(xt)\n\n            # success -> grow step and opportunistic quadratic interpolation sometimes\n            if ft < f_best:\n                # accept\n                prev = x_best.copy(); f_prev = f_best\n                x_best, f_best = xt.copy(), float(ft)\n                arch_add(f_best, x_best)\n                steps[d] = min(max_step, steps[d] * self.grow)\n                temp[d] = max(0.01, temp[d] * 0.92)  # cool down\n                # occasional quadratic interpolation along coordinate using symmetric probe\n                if rng.random() < self.interp_prob and evals < self.budget:\n                    h = max(1e-12, steps[d])\n                    left = prev.copy(); left[d] = np.clip(prev[d] - sign * h, lb[d], ub[d])\n                    fl = safe_eval(left)\n                    fr = ft\n                    f0 = f_prev\n                    denom = (fl - 2*f0 + fr)\n                    if denom != 0 and np.isfinite(denom):\n                        delta = 0.5 * (fl - fr) / denom * (sign*h)\n                        xq = np.clip(prev.copy().astype(float), lb, ub)\n                        xq[d] = np.clip(prev[d] + delta, lb[d], ub[d])\n                        fq = safe_eval(xq)\n                        if fq < f_best:\n                            x_best, f_best = xq.copy(), float(fq)\n                            arch_add(f_best, x_best)\n            else:\n                # failure: shrink and heat a bit\n                steps[d] = max(min_step, steps[d] * self.shrink)\n                temp[d] = min(5.0, temp[d] * 1.04)\n\n            # mild global annealing on steps and temps\n            steps *= (1.0 - 1e-4)  # tiny decay to avoid runaway\n            temp *= 0.999\n\n        # final polish: tiny symmetric probes per-dim\n        fin = 0.04 * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best: x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best: x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 17, "feedback": "The algorithm DualScaleArchiveAnnealer scored 0.203 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a569da3-dff0-437a-a1e1-9e9baee72739"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.3902888590416719}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.2350531496651771}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.2536358511721475}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.05332674751051758}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.07321290374473688}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08100371465224399}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.05634099309110574}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06013257543926753}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.030421252399607912}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.3601353437682886}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.7389050885838342}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.7106758494072261}], "aucs": [0.3902888590416719, 0.2350531496651771, 0.2536358511721475, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.05332674751051758, 0.07321290374473688, 0.08100371465224399, 0.05634099309110574, 0.06013257543926753, 0.030421252399607912, 0.3601353437682886, 0.7389050885838342, 0.7106758494072261]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1671.0, "Edges": 1670.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.9988031119090366, "Degree Variance": 2.0275269935510583, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.951443569553806, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3282097334281013, "Depth Entropy": 2.05831086929233, "Assortativity": 0.0, "Average Eccentricity": 16.311789347695992, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0005984440454817474, "Average Shortest Path": 9.854203263132622, "mean_complexity": 8.5, "total_complexity": 34.0, "mean_token_count": 368.0, "total_token_count": 1472.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "204bbe4a-302b-4f60-a40c-bc58129e2234", "fitness": 0.2534524650162333, "name": "AdaptiveRandomSubspaceQuadratic", "description": "Adaptive Random-Subspace Quadratic Descent (ARSQD) \u2014 probe small random subspaces with mirrored probes, fit a cheap quadratic in that subspace, take a regularized Newton-like step, adapt per-dimension step scales and momentum, with occasional Cauchy escapes.", "code": "import numpy as np\n\nclass AdaptiveRandomSubspaceQuadratic:\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 success_mult=1.25, failure_mult=0.7,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, momentum_gain=0.18, momentum_decay=0.85,\n                 subspace_max=3, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob)\n        self.momentum_gain = float(momentum_gain); self.momentum_decay = float(momentum_decay)\n        self.subspace_max = int(max(1, min(subspace_max, self.dim)))\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box_scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = max(1e-12, self.max_step_frac * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base, float)\n        momentum = np.zeros(dim)\n        consec_fail = np.zeros(dim, int)\n        trials = np.zeros(dim, int)\n\n        while evals < self.budget:\n            # occasional heavy-tailed global escape\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim)\n                jump = 0.15 * box_scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n\n            # choose subspace size (1..subspace_max) biased to small\n            probs = np.array([0.6, 0.3, 0.1][:self.subspace_max])\n            probs = probs / probs.sum()\n            k = int(rng.choice(np.arange(1, self.subspace_max+1), p=probs))\n\n            # ensure budget for mirrored probes (2*k) else fallback to single-coordinate trial\n            if evals + 2*k >= self.budget:\n                # single-coordinate fallback: pick dimension weighted by step sizes*variance\n                weights = steps + 1e-12\n                d = int(rng.choice(dim, p=(weights/weights.sum())))\n                s = np.clip(steps[d], min_step, max_step)\n                sign = 1 if rng.random() < 0.5 else -1\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + sign * s, lb[d], ub[d])\n                ft = safe_eval(xt); trials[d] += 1\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), ft\n                    steps[d] = min(max_step, steps[d] * self.success_mult)\n                    momentum[d] = momentum[d] * self.momentum_decay + sign * self.momentum_gain\n                    consec_fail[d] = 0\n                else:\n                    steps[d] = max(min_step, steps[d] * self.failure_mult)\n                    momentum[d] *= self.momentum_decay\n                    consec_fail[d] += 1\n                continue\n\n            # build k orthonormal directions (random subspace)\n            A = rng.normal(size=(dim, k))\n            Q, _ = np.linalg.qr(A, mode='reduced')\n            # scale for probes: use average steps of coordinates with large projection\n            proj_mags = np.mean(np.abs(Q), axis=0)\n            s = float(np.clip(np.mean(steps) * (0.8 + 0.4*rng.random()), min_step, max_step))\n\n            f_plus = np.empty(k); f_minus = np.empty(k)\n            for i in range(k):\n                xp = np.clip(x_best + s * Q[:, i], lb, ub); f_plus[i] = safe_eval(xp)\n                xm = np.clip(x_best - s * Q[:, i], lb, ub); f_minus[i] = safe_eval(xm)\n\n            # directional first and second differences\n            dvec = (f_plus - f_minus) / (2.0 * s)\n            cvec = (f_plus - 2.0 * f_best + f_minus) / (s * s)\n\n            # regularized small subspace Newton: (diag(cvec)+lam) * alpha = -dvec\n            lam = np.maximum(1e-6, 0.5 * np.abs(cvec).mean() + 1e-8)\n            Hs = np.diag(cvec + lam)\n            # solve robustly\n            try:\n                alpha = np.linalg.solve(Hs, -dvec)\n            except np.linalg.LinAlgError:\n                alpha = -dvec / (cvec + lam + 1e-12)\n\n            dx = Q @ alpha  # proposed move in full space\n            # trust-region: limit norm\n            tr = 2.0 * s * max(1.0, k**0.5)\n            ndx = np.linalg.norm(dx)\n            if ndx > tr and ndx > 0:\n                dx = dx * (tr / ndx)\n\n            xt = np.clip(x_best + dx, lb, ub)\n            ft = safe_eval(xt)\n\n            # attribute trials to coordinates proportionally to abs(Q*alpha)\n            contrib = np.abs(dx)\n            idxs = np.where(contrib > 0)[0]\n            for i in idxs: trials[i] += 1\n\n            if ft < f_best:\n                # accept and adapt\n                x_best, f_best = xt.copy(), float(ft)\n                # boost steps where contribution was meaningful\n                if contrib.sum() > 0:\n                    thr = max(1e-12, np.percentile(contrib, 60))\n                    uplift = (contrib >= thr)\n                    steps[uplift] = np.minimum(max_step, steps[uplift] * self.success_mult)\n                    steps[~uplift] = np.maximum(min_step, steps[~uplift] * (1.0 + 0.05*(self.success_mult-1.0)))\n                else:\n                    steps = np.minimum(max_step, steps * self.success_mult)\n                # momentum aligned with accepted move\n                if ndx > 0:\n                    momentum = momentum * self.momentum_decay + (dx / (ndx + 1e-12)) * self.momentum_gain\n                else:\n                    momentum *= self.momentum_decay\n                consec_fail[:] = 0\n                # opportunistic extrapolation along dx if budget allows\n                if evals < self.budget:\n                    extra = np.clip(x_best + dx, lb, ub)\n                    fextra = safe_eval(extra)\n                    if fextra < f_best:\n                        x_best, f_best = extra.copy(), float(fextra)\n            else:\n                # reject: shrink scales in coordinates that contributed\n                if contrib.sum() > 0:\n                    shrink_idx = contrib >= (0.25 * contrib.max() if contrib.max()>0 else 0)\n                    steps[shrink_idx] = np.maximum(min_step, steps[shrink_idx] * self.failure_mult)\n                    steps[~shrink_idx] = np.maximum(min_step, steps[~shrink_idx] * (1.0 - 0.03*(1.0-self.failure_mult)))\n                    momentum *= self.momentum_decay\n                    consec_fail[shrink_idx] += 1\n                else:\n                    steps = np.maximum(min_step, steps * self.failure_mult)\n                    momentum *= self.momentum_decay\n                    consec_fail += 1\n\n            # occasional stronger shrink if repeated fails on particular dims\n            too_many = np.where(consec_fail >= 6)[0]\n            for d in too_many:\n                steps[d] = max(min_step, steps[d] * (self.failure_mult ** 1.5))\n                consec_fail[d] = 0\n\n            # mild decay of overly large priors/steps\n            big = steps > 0.5 * box_scale\n            steps[big] = steps[big] * 0.95\n\n        # final polish: small symmetric per-dim probes\n        fin = 0.04 * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 17, "feedback": "The algorithm AdaptiveRandomSubspaceQuadratic scored 0.253 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a569da3-dff0-437a-a1e1-9e9baee72739"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9745040554183811}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9741855821620068}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8716072922963833}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0033704860008155757}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.030860323952675794}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.0814268228546593}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.8005980171369256}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.06488439542165236}], "aucs": [0.9745040554183811, 0.9741855821620068, 0.8716072922963833, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.0033704860008155757, 4.999999999999449e-05, 4.999999999999449e-05, 0.030860323952675794, 4.999999999999449e-05, 0.0814268228546593, 0.8005980171369256, 0.06488439542165236]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2153.0, "Edges": 2152.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.9990710636321412, "Degree Variance": 2.3409187840814045, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.899295065458207, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.322004257034526, "Depth Entropy": 1.9886666499162915, "Assortativity": 1.2651077788847136e-08, "Average Eccentricity": 15.881560613098003, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.00046446818392940084, "Average Shortest Path": 9.64042910644264, "mean_complexity": 12.333333333333334, "total_complexity": 37.0, "mean_token_count": 629.0, "total_token_count": 1887.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "075ef6e2-2d9f-4114-9f88-faf8bf23b6c5", "fitness": 0.6115496089355468, "name": "AnchoredPairwiseRecombinationDescent", "description": "Anchored Pairwise Recombination Descent (APRD) \u2014 maintain a small anchor archive, do cheap mirrored 1-D probes around anchors, adapt per-dimension steps multiplicatively, and occasionally perform pairwise recombination along anchor differences plus Cauchy jumps on stagnation.", "code": "import numpy as np\n\nclass AnchoredPairwiseRecombinationDescent:\n    \"\"\"\n    APRD: anchored mirrored coordinate probes + pairwise recombination and Cauchy escapes.\n    \"\"\"\n    def __init__(self, budget, dim, archive_size=10, init_step=None,\n                 min_step=1e-6, max_step=None, stagnation=10, rng=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step); self.stagnation = int(stagnation)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        if init_step is None:\n            self.init_step = 0.2 * (5.0 - (-5.0))\n        else:\n            self.init_step = float(init_step)\n        if max_step is None:\n            self.max_step = 0.5 * (5.0 - (-5.0))\n        else:\n            self.max_step = float(max_step)\n\n    def __call__(self, func):\n        lb = -5.0; ub = 5.0\n        span = ub - lb\n        # state\n        steps = np.full(self.dim, float(self.init_step))\n        steps = np.clip(steps, self.min_step, self.max_step)\n        archive_x = []\n        archive_f = []\n        evals = 0\n\n        def push(x, f):\n            nonlocal archive_x, archive_f\n            if len(archive_x) < self.archive_size:\n                archive_x.append(x.copy()); archive_f.append(float(f))\n            else:\n                w = int(np.argmax(archive_f))\n                if f < archive_f[w]:\n                    archive_x[w] = x.copy(); archive_f[w] = float(f)\n\n        # initial random anchors\n        n0 = max(2, min(self.archive_size, max(1, self.budget // 20)))\n        for _ in range(n0):\n            if evals >= self.budget: break\n            x = lb + self.rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            push(x, f)\n        if len(archive_x) == 0:\n            x = lb + self.rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            push(x, f)\n\n        bi = int(np.argmin(archive_f)); x_best = archive_x[bi].copy(); f_best = float(archive_f[bi])\n        no_imp = 0\n\n        # helper to try x and update\n        def try_point(x):\n            nonlocal evals, f_best, x_best\n            if evals >= self.budget: return None\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x)); evals += 1\n            push(x, f)\n            if f < f_best:\n                f_best = f; x_best = x.copy()\n                return True\n            return False\n\n        while evals < self.budget:\n            # choose an anchor: rank-weighted toward best\n            nA = len(archive_x)\n            ranks = np.argsort(np.array(archive_f))\n            # sample index proportional to exponential of -rank\n            beta = 0.6\n            weights = np.exp(-beta * np.arange(nA))\n            weights = weights / weights.sum()\n            a_idx = self.rng.choice(ranks, p=weights)\n            anchor = archive_x[a_idx].copy()\n\n            # mostly single-coordinate mirrored probe; sometimes 2-D or recombination\n            r = self.rng.random()\n            if r < 0.72:\n                # mirrored coordinate probe\n                k = int(self.rng.integers(0, self.dim))\n                s = steps[k] * (1.0 + 0.15 * (self.rng.random() - 0.5))\n                # forward\n                xf = anchor.copy(); xf[k] += s\n                improved_fwd = try_point(xf)\n                if improved_fwd:\n                    steps[k] = min(self.max_step, steps[k] * 1.30)\n                    no_imp = 0\n                    # cheap exploitation: small extrapolation\n                    if evals < self.budget:\n                        xe = x_best.copy(); xe[k] += 0.6 * s\n                        if try_point(xe):\n                            steps[k] = min(self.max_step, steps[k] * 1.12)\n                    continue\n                # backward\n                xb = anchor.copy(); xb[k] -= s\n                improved_bwd = False\n                if evals < self.budget:\n                    improved_bwd = try_point(xb)\n                    if improved_bwd:\n                        steps[k] = min(self.max_step, steps[k] * 1.25)\n                        no_imp = 0\n                        continue\n                # failure\n                steps[k] = max(self.min_step, steps[k] * 0.70)\n                no_imp += 1\n            elif r < 0.92 and nA >= 2:\n                # pairwise recombination along difference of two anchors\n                i, j = self.rng.choice(nA, size=2, replace=False)\n                x1, x2 = archive_x[i], archive_x[j]\n                alpha = self.rng.uniform(-0.4, 1.4)\n                xr = x1 + alpha * (x2 - x1)\n                if try_point(xr):\n                    # amplify steps on coordinates that contributed\n                    moved = np.abs(x2 - x1) > 1e-12\n                    if moved.any():\n                        steps[moved] = np.minimum(self.max_step, steps[moved] * (1.15 + 0.2 * self.rng.random()))\n                    no_imp = 0\n                    continue\n                else:\n                    # mild shrink on dims with large disagreement\n                    moved = np.abs(x2 - x1) > 0.5 * span\n                    if moved.any():\n                        steps[moved] = np.maximum(self.min_step, steps[moved] * 0.82)\n                    no_imp += 1\n            else:\n                # small random linear probe across 2 coords if dim>=2, else single\n                k = 2 if self.dim >= 2 else 1\n                idx = self.rng.choice(self.dim, size=k, replace=False)\n                v = self.rng.normal(size=k)\n                v = v / (np.linalg.norm(v) + 1e-12)\n                d = np.zeros(self.dim); d[idx] = v\n                s = steps[idx].mean() * (0.9 + 0.25 * self.rng.random())\n                xp = anchor + s * d\n                if try_point(xp):\n                    # increase steps for involved coords\n                    steps[idx] = np.minimum(self.max_step, steps[idx] * 1.20)\n                    no_imp = 0\n                    continue\n                else:\n                    steps[idx] = np.maximum(self.min_step, steps[idx] * 0.75)\n                    no_imp += 1\n\n            # occasionally refresh archive with random probe\n            if (self.rng.random() < 0.04) and evals < self.budget:\n                xr = lb + self.rng.random(self.dim) * span\n                if try_point(xr):\n                    steps = np.minimum(self.max_step, steps * (1.0 + 0.3 * self.rng.random(self.dim)))\n                    no_imp = 0\n\n            # stagnation: median pull or Cauchy jump\n            if no_imp >= self.stagnation and evals < self.budget:\n                no_imp = 0\n                if len(archive_x) >= 3:\n                    med = np.median(np.array(archive_x), axis=0)\n                    pull = 0.35 * (med - x_best)\n                    if np.linalg.norm(pull) > 1e-12:\n                        xp = x_best + pull\n                        if try_point(xp):\n                            moved = np.abs(pull) > 1e-12\n                            steps[moved] = np.minimum(self.max_step, steps[moved] * 1.3)\n                            continue\n                # heavy-tailed retry\n                jump = self.rng.standard_cauchy(self.dim)\n                jump = np.clip(jump, -3.0, 3.0) * (0.5 * span)\n                mask = self.rng.random(self.dim) < 0.6\n                xj = x_best + jump * mask\n                if try_point(xj):\n                    steps = np.minimum(self.max_step, steps * (1.0 + 0.5 * self.rng.random(self.dim)))\n                    continue\n                else:\n                    # gentle reset of some steps\n                    steps = np.minimum(self.max_step, np.maximum(self.min_step, steps * (0.9 + 0.2 * self.rng.random(self.dim))))\n                    # trim worst archive member to encourage diversity\n                    if len(archive_x) > 3:\n                        w = int(np.argmax(archive_f)); archive_x.pop(w); archive_f.pop(w)\n\n            # small decay\n            steps = np.minimum(self.max_step, np.maximum(self.min_step, steps * (1.0 - 1e-4)))\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 18, "feedback": "The algorithm AnchoredPairwiseRecombinationDescent scored 0.612 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6dfd3db-5c46-4441-a54a-34c5ad9bdd18"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9696502252635059}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9707557712404302}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9682408248781356}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9362171198822887}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.935575328259243}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9443527533668143}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08174061387466103}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08472082106515322}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.12078689615819349}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.06330403011456764}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06153231933597658}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.06537989769453889}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9934267308006475}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9873661908944203}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9901946112046227}], "aucs": [0.9696502252635059, 0.9707557712404302, 0.9682408248781356, 0.9362171198822887, 0.935575328259243, 0.9443527533668143, 0.08174061387466103, 0.08472082106515322, 0.12078689615819349, 0.06330403011456764, 0.06153231933597658, 0.06537989769453889, 0.9934267308006475, 0.9873661908944203, 0.9901946112046227]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1801.0, "Edges": 1800.0, "Max Degree": 21.0, "Min Degree": 1.0, "Mean Degree": 1.9988895058300944, "Degree Variance": 1.857855513054781, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.50121359223301, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3237849628795864, "Depth Entropy": 2.2118477217391748, "Assortativity": 0.0, "Average Eccentricity": 19.24597445863409, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.000555247084952804, "Average Shortest Path": 10.443864519711271, "mean_complexity": 9.5, "total_complexity": 38.0, "mean_token_count": 410.75, "total_token_count": 1643.0, "mean_parameter_count": 3.5, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "48a2954a-fdcf-49d4-ac15-3ec15640c194", "fitness": 0.6219007091102273, "name": "ThompsonCoordinateMomentum", "description": "Thompson-Coordinate Momentum v3 \u2014 compact Thompson-sampled coordinate/sign bandit with a tiny best-archive median pull, EWMA-sized-aware step scaling, conservative multiplicative adaptation and rare L\u00e9vy escapes for robust exploitation and escapes.", "code": "import numpy as np\n\nclass ThompsonCoordinateMomentum:\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 success_mult=1.25, failure_mult=0.7,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, median_prob=0.04,\n                 momentum_gain=0.22, momentum_decay=0.85,\n                 seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob); self.median_prob = float(median_prob)\n        self.mg = float(momentum_gain); self.md = float(momentum_decay)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box = np.maximum(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * box)\n        min_step = max(1e-15, self.min_step_frac * box)\n        max_step = max(1e-12, self.max_step_frac * box)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # state\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base)\n        a_pos = np.ones(dim); b_pos = np.ones(dim); a_neg = np.ones(dim); b_neg = np.ones(dim)\n        a_coord = np.ones(dim); b_coord = np.ones(dim)  # coord-selection Beta\n        momentum = np.zeros(dim)\n        imp_ema = np.zeros(dim)  # EWMA of improvement magnitude\n        archive = [(x_best.copy(), f_best)]\n\n        while evals < self.budget:\n            # rare global L\u00e9vy-like jump\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim)\n                jump = 0.12 * box * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                    archive.append((x_best.copy(), f_best))\n                    archive = sorted(archive, key=lambda t: t[1])[:6]\n\n            # occasional archive median pull\n            if rng.random() < self.median_prob and len(archive) > 1 and evals < self.budget:\n                pts = np.vstack([p for p, _ in archive])\n                xm = np.median(pts, axis=0)\n                xm = np.clip(x_best + 0.5 * (xm - x_best), lb, ub)  # partial pull\n                fm = safe_eval(xm)\n                if fm < f_best:\n                    x_best, f_best = xm.copy(), fm\n                    archive.append((x_best.copy(), f_best)); archive = sorted(archive, key=lambda t: t[1])[:6]\n\n            # sample sign-beliefs and coord-belief\n            sp = rng.beta(a_pos, b_pos); sn = rng.beta(a_neg, b_neg)\n            cb = rng.beta(a_coord, b_coord)\n            # headroom to avoid walls\n            hr_pos = (ub - x_best) / (ub - lb + 1e-18); hr_neg = (x_best - lb) / (ub - lb + 1e-18)\n            # score per-dim combining coordinate interest, sign gap, headroom and step size\n            sign_gap = np.abs(sp - sn) + 1e-12\n            score = cb * sign_gap * (0.3 + 0.7 * (hr_pos + hr_neg) * 0.5) * steps\n            d = int(np.argmax(score + 1e-18 * rng.random(dim)))\n\n            # decide sign by sampled beliefs biased by momentum\n            m = np.clip(momentum[d], -0.95, 0.95)\n            ppos = sp[d] * (1 + 0.6 * m); pneg = sn[d] * (1 - 0.6 * m)\n            sign = 1 if ppos >= pneg else -1\n\n            s = float(np.clip(steps[d], min_step, max_step))\n            xt = x_best.copy(); xt[d] = np.clip(xt[d] + sign * s, lb[d], ub[d])\n            ft = safe_eval(xt)\n            # update coordinate selection Beta\n            if ft < f_best:\n                a_coord[d] += 1.0\n            else:\n                b_coord[d] += 1.0\n\n            # update sign priors\n            if sign > 0:\n                if ft < f_best: a_pos[d] += 1.0\n                else: b_pos[d] += 1.0\n            else:\n                if ft < f_best: a_neg[d] += 1.0\n                else: b_neg[d] += 1.0\n\n            if ft < f_best:\n                # success: accept, adapt step and momentum, update EWMA and archive\n                delta = f_best - ft\n                x_best, f_best = xt.copy(), float(ft)\n                steps[d] = min(max_step, steps[d] * self.success_mult * (1.0 + 0.3 * (imp_ema[d] / (imp_ema[d] + 1e-12))))\n                momentum[d] = momentum[d] * self.md + sign * self.mg\n                imp_ema[d] = 0.2 * abs(delta) + 0.8 * imp_ema[d]\n                archive.append((x_best.copy(), f_best)); archive = sorted(archive, key=lambda t: t[1])[:6]\n                # cheap extrapolation\n                if evals < self.budget:\n                    extra = x_best.copy(); extra[d] = np.clip(extra[d] + sign * steps[d], lb[d], ub[d])\n                    fextra = safe_eval(extra)\n                    if fextra < f_best:\n                        x_best, f_best = extra.copy(), float(fextra)\n                        archive.append((x_best.copy(), f_best)); archive = sorted(archive, key=lambda t: t[1])[:6]\n            else:\n                # failure: shrink, decay momentum mildly\n                steps[d] = max(min_step, steps[d] * self.failure_mult)\n                momentum[d] *= np.sqrt(self.md)\n                # encourage exploring other coords by slightly increasing b for this sign\n                if sign > 0: b_pos[d] += 0.2\n                else: b_neg[d] += 0.2\n\n            # mild regularization of Betas to avoid extremes\n            if np.sum(a_coord + b_coord) > 200:\n                a_coord = 1.0 + 0.9 * (a_coord - 1.0); b_coord = 1.0 + 0.9 * (b_coord - 1.0)\n\n        # final symmetric polish\n        fin = 0.05 * box\n        while evals < self.budget and fin > 1e-12 * box:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                for sgn in (1, -1):\n                    xt = x_best.copy(); xt[d] = np.clip(xt[d] + sgn * fin, lb[d], ub[d])\n                    ft = safe_eval(xt)\n                    if ft < f_best:\n                        x_best, f_best, improved = xt.copy(), float(ft), True\n                        break\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 18, "feedback": "The algorithm ThompsonCoordinateMomentum scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a569da3-dff0-437a-a1e1-9e9baee72739"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9829369419875458}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.985255591979697}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9829811191682513}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9682265332829798}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9662912937916307}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9689647607149144}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.07909157062220251}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08206470505410912}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.04941395958195949}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.08200645622429004}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.04934675545090794}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.13898658690344257}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9968105518227647}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9982506882474411}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9978831218212755}], "aucs": [0.9829369419875458, 0.985255591979697, 0.9829811191682513, 0.9682265332829798, 0.9662912937916307, 0.9689647607149144, 0.07909157062220251, 0.08206470505410912, 0.04941395958195949, 0.08200645622429004, 0.04934675545090794, 0.13898658690344257, 0.9968105518227647, 0.9982506882474411, 0.9978831218212755]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1837.0, "Edges": 1836.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9989112683723462, "Degree Variance": 2.3287957662148857, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.756213017751479, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.317942379464331, "Depth Entropy": 2.0348888487629524, "Assortativity": 6.0968109228221605e-09, "Average Eccentricity": 16.046815459989112, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0005443658138268917, "Average Shortest Path": 9.5359749900081, "mean_complexity": 11.666666666666666, "total_complexity": 35.0, "mean_token_count": 548.3333333333334, "total_token_count": 1645.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "0788f73c-777a-44c0-ab77-3be3c1901027", "fitness": 0.6194050462364092, "name": "SeparableCumulativeMedianPull", "description": "Separable Cumulative Median Pull v3 \u2014 compact per-coordinate median pulls with multiplicative step adaptation, lightweight momentum/priority, and targeted heavy-tailed single-coordinate escapes when stagnating.", "code": "import numpy as np\n\nclass SeparableCumulativeMedianPull:\n    \"\"\"\n    Compact Separable Cumulative Median Pull (SCMP v3)\n\n    - Strictly single-coordinate evaluations.\n    - Maintain small top-k archive; use coordinate-wise medians & dispersion to pull best.\n    - Adaptive per-dim multiplicative steps, lightweight momentum & priority.\n    - On stagnation use targeted heavy-tailed single-coordinate diversifications.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, archive_size=12, init_step=None,\n                 min_step=1e-6, stagnation_patience=8, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == (): lb = np.full(self.dim, float(lb))\n        if ub.shape == (): ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        rng = self.rng\n        span = ub - lb\n        # init step\n        if self.init_step is None:\n            step = 0.25 * span\n        else:\n            s = np.array(self.init_step, dtype=float)\n            step = s if s.shape == (self.dim,) else np.full(self.dim, float(s))\n        step = np.maximum(step, self.min_step)\n        max_step = 0.5 * span\n\n        top_k = min(self.archive_size, max(2, self.budget // 6))\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n        # initial sampling to seed archive\n        n0 = min(max(3, self.dim), max(1, self.budget // 12))\n        for _ in range(n0):\n            x0 = lb + rng.random(self.dim) * span\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n            if evals >= self.budget: break\n\n        if len(archive_x) == 0:  # ensure at least one\n            x0 = lb + rng.random(self.dim) * span\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n\n        # best so far\n        idx = int(np.argmin(archive_f))\n        x_best = archive_x[idx].copy(); f_best = float(archive_f[idx])\n\n        # per-dim bookkeeping\n        momentum = np.zeros(self.dim)\n        priority = np.zeros(self.dim)\n        stagn = 0\n\n        def add_archive(x, f):\n            # maintain top-k (smaller f better)\n            if len(archive_x) < top_k:\n                archive_x.append(x.copy()); archive_f.append(float(f)); return\n            worst = int(np.argmax(archive_f))\n            if f < archive_f[worst]:\n                archive_x[worst] = x.copy(); archive_f[worst] = float(f)\n\n        eps = 1e-12\n        while evals < self.budget:\n            arr = np.array(archive_x)\n            med = np.median(arr, axis=0)\n            q75 = np.percentile(arr, 75, axis=0); q25 = np.percentile(arr, 25, axis=0)\n            iqr = np.maximum(q75 - q25, eps)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            # score coordinates: normalized pull, dispersion and priority\n            score = abs_pull / (step + eps)\n            score += 0.45 * (iqr / (span + eps))\n            score += 0.12 * np.tanh(priority)\n            # softmax-ish probabilities\n            temp = 1.0 + 0.18 * np.std(score)\n            s = score - score.max()\n            probs = np.exp(s / temp)\n            probs = probs / (probs.sum() + eps)\n\n            # choose coordinate\n            try:\n                coord = int(rng.choice(self.dim, p=probs))\n            except Exception:\n                coord = int(rng.integers(0, self.dim))\n\n            sstep = max(step[coord], self.min_step)\n            # decide delta: pull toward median or use momentum/random\n            if abs_pull[coord] > 1e-9:\n                sign = int(np.sign(pull[coord]))\n                # sometimes follow momentum if strong\n                if momentum[coord] != 0 and np.sign(momentum[coord]) != sign and abs(momentum[coord]) > 0.4 * sstep and rng.random() < 0.27:\n                    sign = int(np.sign(momentum[coord]))\n                delta = sign * min(sstep, abs_pull[coord])\n            else:\n                if momentum[coord] != 0 and rng.random() < 0.7:\n                    delta = np.sign(momentum[coord]) * sstep\n                else:\n                    delta = (1 if rng.random() < 0.5 else -1) * sstep\n\n            # single-coordinate trial\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            add_archive(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try; improved = True\n                stagn = 0\n                priority[coord] = min(10.0, priority[coord] + 1.0)\n                momentum[coord] = 0.75 * momentum[coord] + 0.25 * delta\n                step[coord] = min(max_step[coord], step[coord] * 1.22)\n                # small cheap extrapolation (if budget allows)\n                if evals < self.budget:\n                    ext = 0.45 * np.sign(delta) * abs(delta)\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_archive(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext\n                        step[coord] = min(max_step[coord], step[coord] * 1.08)\n                        momentum[coord] = 0.8 * momentum[coord] + 0.2 * ext\n            else:\n                # try opposite sign once\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    add_archive(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp; improved = True\n                        stagn = 0\n                        priority[coord] = min(10.0, priority[coord] + 0.9)\n                        momentum[coord] = 0.7 * momentum[coord] - 0.3 * delta\n                        step[coord] = min(max_step[coord], step[coord] * 1.18)\n                    else:\n                        # both directions failed\n                        priority[coord] = max(-10.0, priority[coord] - 0.6)\n                        step[coord] = max(self.min_step, step[coord] * 0.56)\n                        momentum[coord] *= 0.55\n                else:\n                    priority[coord] = max(-10.0, priority[coord] - 0.6)\n                    step[coord] = max(self.min_step, step[coord] * 0.56)\n                    momentum[coord] *= 0.55\n\n            step = np.clip(step, self.min_step, max_step)\n            priority *= 0.998\n            momentum *= 0.995\n\n            if not improved:\n                stagn += 1\n            else:\n                stagn = 0\n\n            # targeted heavy-tailed diversification when stagnating\n            if stagn >= self.stagnation_patience and evals < self.budget:\n                # pick up to 2 coords with largest (iqr * step)\n                cand = (iqr / (span + eps)) * (step / (step.max() + eps) + 0.1)\n                k = min(2, self.dim)\n                idxs = np.argsort(-cand)[:k]\n                for c in idxs:\n                    if evals >= self.budget: break\n                    # heavy-tailed single-coordinate jump (Cauchy scaled)\n                    r = np.tanh(np.abs(rng.standard_cauchy()) / 3.0)  # stabilize tail\n                    sign = 1 if rng.random() < 0.5 else -1\n                    jump = sign * (0.5 * step[c] + 0.8 * r * span[c])\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    add_archive(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj\n                        step[c] = min(max_step[c], step[c] * 1.35)\n                        priority[c] = min(10.0, priority[c] + 1.2)\n                        momentum[c] = 0.7 * momentum[c] + 0.3 * jump\n                        stagn = 0\n                        break\n                # small randomization to break symmetry if still stuck\n                if stagn >= self.stagnation_patience:\n                    step = np.minimum(max_step, step * (1.0 + 0.18 * rng.random(self.dim)))\n                    priority *= 0.35\n                    stagn = 0\n\n        return f_best, x_best", "configspace": "", "generation": 18, "feedback": "The algorithm SeparableCumulativeMedianPull scored 0.619 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9778054794465812}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9767889583293955}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9785418190700522}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8310297410938275}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8245590451052986}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8455870105477145}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.16936562032658276}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.11391013658638405}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.15078188618445243}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.12531006121438182}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.13820316360213436}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.17095730070426685}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9964803230501079}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9961331405541047}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.995622007730854}], "aucs": [0.9778054794465812, 0.9767889583293955, 0.9785418190700522, 0.8310297410938275, 0.8245590451052986, 0.8455870105477145, 0.16936562032658276, 0.11391013658638405, 0.15078188618445243, 0.12531006121438182, 0.13820316360213436, 0.17095730070426685, 0.9964803230501079, 0.9961331405541047, 0.995622007730854]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2143.0, "Edges": 2142.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.999066728884741, "Degree Variance": 2.1735875564431963, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.08265306122449, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.321893599351182, "Depth Entropy": 2.0665532618662916, "Assortativity": 1.1051411895636339e-08, "Average Eccentricity": 16.088194120391975, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.0004666355576294914, "Average Shortest Path": 9.930645146532715, "mean_complexity": 9.75, "total_complexity": 39.0, "mean_token_count": 466.25, "total_token_count": 1865.0, "mean_parameter_count": 3.5, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "2c3f1cc7-361b-4268-a133-77a297fc5409", "fitness": 0.585056659451727, "name": "RotatedPairedMirrored", "description": "Rotated Paired Mirrored Search \u2014 perform cheap mirrored probes along random 1\u20132D rotated directions (linear combos of coords), adapt per-dimension steps by projection-weighted multiplicative updates, keep a light momentum direction for extrapolation and occasional heavy-tailed global escapes.", "code": "import numpy as np\n\nclass RotatedPairedMirrored:\n    def __init__(self, budget=1000, dim=10, init_step=0.06,\n                 success_mult=1.25, failure_mult=0.7,\n                 global_prob=0.03, extrapolate=1.5, polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.global_prob = float(global_prob); self.extrapolate = float(extrapolate)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step * box)\n        min_step = 1e-15 * box\n        max_step = 1.2 * box\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x_best = rng.uniform(lb, ub)\n        f_best = safe_eval(x_best)\n        steps = np.full(dim, base, float)\n        momentum = np.zeros(dim, float)\n        last_dir = np.zeros(dim, float)\n\n        while evals < self.budget:\n            # occasional heavy-tailed global escape\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim) - 0.5\n                jump = 0.12 * box * np.tan(np.pi * (u.clip(0,1) - 0.5))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    momentum = 0.6 * momentum + 0.4 * (xg - x_best); x_best, f_best = xg.copy(), fg\n                continue\n\n            # pick a 1- or 2-d random subspace (mostly 2D for rotated probes)\n            if dim == 1 or rng.random() < 0.25:\n                # 1D: pick coord by large step preference\n                idx = int(np.argmax(steps + 1e-12 * rng.random(dim)))\n                mask = np.zeros(dim); mask[idx] = 1.0\n            else:\n                # pick two distinct coords weighted by step sizes\n                probs = steps / steps.sum()\n                i = rng.choice(dim, p=probs)\n                j = rng.choice(dim-1, p=np.delete(probs, i)/ (1-probs[i]))\n                j = j if j < i else j+1\n                mask = np.zeros(dim); mask[i] = 1.0; mask[j] = 1.0\n\n            # construct a random unit direction within selected subspace\n            v = rng.normal(size=dim) * mask\n            nrm = np.linalg.norm(v)\n            if nrm == 0:\n                # fallback to best coordinate direction\n                idx = int(np.argmax(steps + 1e-12 * rng.random(dim)))\n                v = np.zeros(dim); v[idx] = 1.0; nrm = 1.0\n            u = v / nrm\n\n            # choose scalar step from projection-weighted average of per-dim steps\n            proj = np.abs(u) * steps\n            s = float(np.clip(proj.sum(), min_step, max_step))\n\n            # mirrored probe if budget allows 2 evals\n            if evals <= self.budget - 2:\n                xp = np.clip(x_best + s * u, lb, ub)\n                xm = np.clip(x_best - s * u, lb, ub)\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                # accept best side\n                if fp < f_best or fm < f_best:\n                    if fp < fm:\n                        newx, newf = xp.copy(), float(fp)\n                    else:\n                        newx, newf = xm.copy(), float(fm)\n                    # update momentum and steps (projection-weighted)\n                    delta = newx - x_best\n                    weight = np.abs(u)\n                    x_best, f_best = newx, newf\n                    momentum = 0.7 * momentum + 0.3 * delta\n                    last_dir = u.copy()\n                    # boost steps on involved dims proportionally\n                    mult = self.success_mult\n                    steps = np.minimum(max_step, steps * (mult ** (weight * 1.5)))\n                    # opportunistic extrapolation along momentum/last_dir if budget left\n                    if evals < self.budget:\n                        ext_dir = momentum if np.linalg.norm(momentum) > 1e-16 else last_dir\n                        ed = ext_dir / (np.linalg.norm(ext_dir)+1e-24)\n                        ext = np.clip(x_best + s * self.extrapolate * ed, lb, ub)\n                        fe = safe_eval(ext)\n                        if fe < f_best:\n                            steps = np.minimum(max_step, steps * (mult ** (np.abs(ed) * 1.2)))\n                            momentum = 0.8 * momentum + 0.2 * (ext - x_best)\n                            x_best, f_best = ext.copy(), float(fe)\n                else:\n                    # no improvement: reduce steps on involved dims\n                    steps = np.maximum(min_step, steps * (self.failure_mult ** (np.abs(u) + 0.2)))\n            else:\n                # single eval left: try one side randomly\n                if rng.random() < 0.5:\n                    xt = np.clip(x_best + s * u, lb, ub)\n                else:\n                    xt = np.clip(x_best - s * u, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    delta = xt - x_best\n                    weight = np.abs(u)\n                    x_best, f_best = xt.copy(), float(ft)\n                    momentum = 0.7 * momentum + 0.3 * delta\n                    steps = np.minimum(max_step, steps * (self.success_mult ** (weight * 1.2)))\n                else:\n                    steps = np.maximum(min_step, steps * (self.failure_mult ** (np.abs(u) + 0.2)))\n\n            # gentle occasional re-scaling to avoid collapse\n            if rng.random() < 0.02:\n                # inflate a random coord with poor recent movement\n                idx = int(np.argmin(np.abs(momentum) + 1e-12 * rng.random(dim)))\n                steps[idx] = min(max_step, steps[idx] * 1.5)\n\n        # final polish: tiny coordinate-wise mirrored sweeps\n        fin = self.polish_frac * box\n        while evals < self.budget and fin > 1e-14 * box:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = np.clip(x_best + fin * np.eye(1, dim, d).ravel(), lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = np.clip(x_best - fin * np.eye(1, dim, d).ravel(), lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 18, "feedback": "The algorithm RotatedPairedMirrored scored 0.585 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["82379331-5855-488c-a7d5-53d7afbbd2bb"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9812750947138246}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9755780374339958}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9714643439239271}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8319829150785647}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8181589399517692}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9355057261215115}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.018788360920119707}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10905712599346229}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.016900670503409265}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.05211163520218087}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.05832584323339107}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.030127245469193764}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9897422431824363}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9913395867072674}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9954921233408519}], "aucs": [0.9812750947138246, 0.9755780374339958, 0.9714643439239271, 0.8319829150785647, 0.8181589399517692, 0.9355057261215115, 0.018788360920119707, 0.10905712599346229, 0.016900670503409265, 0.05211163520218087, 0.05832584323339107, 0.030127245469193764, 0.9897422431824363, 0.9913395867072674, 0.9954921233408519]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1754.0, "Edges": 1753.0, "Max Degree": 22.0, "Min Degree": 1.0, "Mean Degree": 1.998859749144812, "Degree Variance": 1.9042176279921832, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.31069182389937, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3321259559846126, "Depth Entropy": 2.1723044203629884, "Assortativity": 0.0, "Average Eccentricity": 18.509692132269098, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0005701254275940707, "Average Shortest Path": 10.242295176016876, "mean_complexity": 11.666666666666666, "total_complexity": 35.0, "mean_token_count": 514.3333333333334, "total_token_count": 1543.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "54126816-8feb-409f-9b9a-dab378a4b31f", "fitness": 0.6214139130125914, "name": "SCQNT", "description": "Stochastic Coordinate Quasi-Newton with Low-rank Trust (SCQNT) \u2014 mostly cheap mirrored 1-D probes with adaptive per-dim steps & momentum, while occasionally building a tiny low-rank model from recent finite-difference moves to propose informed multi-dim quasi-Newton steps; rare heavy-tailed jumps for escapes.", "code": "import numpy as np\n\nclass SCQNT:\n    def __init__(self, budget=1000, dim=10, seed=None,\n                 init_step=0.06, success_mult=1.34, fail_mult=0.6,\n                 global_prob=0.03, subspace_prob=0.12, history=18):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.success_mult = float(success_mult); self.fail_mult = float(fail_mult)\n        self.global_prob = float(global_prob); self.subspace_prob = float(subspace_prob)\n        self.hist_max = int(history)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step * scale)\n        min_step = 1e-12 * scale; max_step = 1.2 * scale\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x = rng.uniform(lb, ub); fx = safe_eval(x)\n        steps = np.full(dim, base)\n        trials = np.zeros(dim, int); succ = np.zeros(dim, int)\n        mom = np.zeros(dim, float)\n        # tiny history of (dx, df) for low-rank model\n        DX = []; DF = []\n\n        while evals < self.budget:\n            # rare heavy-tailed global escape\n            if rng.random() < self.global_prob:\n                u = rng.random(dim) - 0.5\n                jump = 0.12 * scale * np.tan(np.pi * u)\n                xg = np.clip(x + jump, lb, ub); fg = safe_eval(xg)\n                if fg < fx: x, fx = xg.copy(), fg\n                continue\n\n            # occasional low-rank quasi-Newton proposal\n            if rng.random() < self.subspace_prob and len(DX) >= 4 and evals <= self.budget - 2:\n                A = np.vstack(DX)\n                try:\n                    _, svals, vt = np.linalg.svd(A, full_matrices=False)\n                except Exception:\n                    svals = None\n                if svals is not None and svals[0] > 1e-8:\n                    v = vt[0]\n                    # propose mirrored along v with step scaled by median steps\n                    ssub = max(min_step, min(max_step, np.median(steps) * (1.0 + 0.5*abs(np.dot(mom, v)))))\n                    xp = np.clip(x + ssub * v, lb, ub); xm = np.clip(x - ssub * v, lb, ub)\n                    fp = safe_eval(xp); fm = safe_eval(xm)\n                    # record diffs\n                    DX.append(xp - x); DF.append(fp - fx)\n                    DX.append(xm - x); DF.append(fm - fx)\n                    if len(DX) > self.hist_max:\n                        DX = DX[-self.hist_max:]; DF = DF[-self.hist_max:]\n                    # accept best\n                    if fp < fx or fm < fx:\n                        if fp < fm:\n                            x, fx = xp.copy(), fp; side = +1.0\n                        else:\n                            x, fx = xm.copy(), fm; side = -1.0\n                        # inflate steps along coordinates aligned with v\n                        w = np.abs(v); steps = np.minimum(max_step, steps * (1.0 + 0.5*w* (1+abs(side))))\n                        mom = 0.6*mom + 0.4*side*v\n                        continue\n                    else:\n                        steps *= self.fail_mult\n                        mom *= 0.9\n                        continue\n\n            # pick coordinate (stochastic preference for large step & low trials & momentum)\n            score = steps * (1.0 + 1.0/(1.0+trials)) * (1.0 + 0.35*np.abs(mom))\n            probs = score / (score.sum() + 1e-24)\n            d = rng.choice(dim, p=probs)\n\n            s = float(np.clip(steps[d], min_step, max_step))\n            # anchor uses small momentum bias\n            anchor = x.copy()\n            anchor[d] = np.clip(anchor[d] + 0.08 * s * mom[d], lb[d], ub[d])\n\n            # mirrored probe\n            if evals <= self.budget - 2:\n                xp = anchor.copy(); xm = anchor.copy()\n                xp[d] = np.clip(xp[d] + s, lb[d], ub[d])\n                xm[d] = np.clip(xm[d] - s, lb[d], ub[d])\n                fp = safe_eval(xp); fm = safe_eval(xm); trials[d] += 2\n                # record low-cost finite-diff\n                DX.append((xp - xm) * 0.5)  # direction approx\n                DF.append(fp - fm)\n                if len(DX) > self.hist_max:\n                    DX = DX[-self.hist_max:]; DF = DF[-self.hist_max:]\n                if fp < fx or fm < fx:\n                    if fp < fm:\n                        x, fx, side = xp.copy(), fp, +1.0\n                    else:\n                        x, fx, side = xm.copy(), fm, -1.0\n                    succ[d] += 1\n                    mom[d] = 0.7*mom[d] + 0.3*side\n                    steps[d] = min(max_step, steps[d] * self.success_mult)\n                    # small opportunistic extrapolation (one eval)\n                    if evals < self.budget:\n                        ext = x.copy(); ext[d] = np.clip(ext[d] + side * s * 1.3, lb[d], ub[d])\n                        fext = safe_eval(ext); trials[d] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext\n                            steps[d] = min(max_step, steps[d] * self.success_mult)\n                else:\n                    # curvature proxy; reduce step if positive curvature else try quick extrapolate\n                    curv = (fp + fm - 2.0*fx) / (s*s + 1e-24)\n                    if curv < -1e-12 and evals < self.budget:\n                        side = -np.sign(fp - fm) if fp != fm else (np.sign(mom[d]) or 1.0)\n                        ext = anchor.copy(); ext[d] = np.clip(ext[d] + side * s * 1.6, lb[d], ub[d])\n                        fext = safe_eval(ext); trials[d] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext\n                            succ[d] += 1\n                            mom[d] = 0.6*mom[d] + 0.4*side\n                            steps[d] = min(max_step, steps[d] * self.success_mult)\n                        else:\n                            steps[d] = max(min_step, steps[d] * self.fail_mult)\n                            mom[d] *= 0.9\n                    else:\n                        steps[d] = max(min_step, steps[d] * self.fail_mult)\n                        mom[d] *= 0.92\n            else:\n                # single eval left -> try best side by momentum\n                side = np.sign(mom[d]) if mom[d] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                xt = anchor.copy(); xt[d] = np.clip(xt[d] + side*s, lb[d], ub[d])\n                ft = safe_eval(xt); trials[d] += 1\n                if ft < fx:\n                    x, fx = xt.copy(), ft\n                    succ[d] += 1\n                    mom[d] = 0.7*mom[d] + 0.3*side\n                    steps[d] = min(max_step, steps[d] * self.success_mult)\n                else:\n                    steps[d] = max(min_step, steps[d] * self.fail_mult)\n                    mom[d] *= 0.9\n\n        # final light polish while budget remains\n        eps = 0.03 * scale\n        while evals < self.budget and eps > 1e-12 * scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + eps, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fx: x, fx, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] - eps, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fx: x, fx, improved = xt.copy(), ft, True\n            if not improved: eps *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 18, "feedback": "The algorithm SCQNT scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd6944c9-d56c-46d8-bafb-55356bae9dd3"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9880297970667361}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9785503051071839}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9771854634282302}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9719428523483195}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9699077289699204}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9718170381597445}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.10911830288390645}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06377662976877307}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.04838326173993046}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.07934895084308391}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06188808423115277}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.11491724430163908}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9953421663698285}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9961541220511213}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9948467479192993}], "aucs": [0.9880297970667361, 0.9785503051071839, 0.9771854634282302, 0.9719428523483195, 0.9699077289699204, 0.9718170381597445, 0.10911830288390645, 0.06377662976877307, 0.04838326173993046, 0.07934895084308391, 0.06188808423115277, 0.11491724430163908, 0.9953421663698285, 0.9961541220511213, 0.9948467479192993]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2212.0, "Edges": 2211.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9990958408679929, "Degree Variance": 2.04701545736064, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.689756097560975, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3260978118528195, "Depth Entropy": 2.1354225504422533, "Assortativity": 8.267702556100492e-09, "Average Eccentricity": 18.106690777576855, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0004520795660036166, "Average Shortest Path": 10.411517948642453, "mean_complexity": 13.333333333333334, "total_complexity": 40.0, "mean_token_count": 626.3333333333334, "total_token_count": 1879.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "0ecf33e2-b710-41c3-b0ee-d48183420be5", "fitness": 0.28768100938431573, "name": "DirectionalMemoryDescent", "description": "Directional Memory Descent (DMD) \u2014 maintain a compact set of learned search directions (a \"memory\") whose strengths grow when they produce wins; probe along memory directions and occasional coordinate probes, adapt step scales multiplicatively, and inject tempered Cauchy jumps on stagnation.", "code": "import numpy as np\n\nclass DirectionalMemoryDescent:\n    \"\"\"\n    Directional Memory Descent (compact, low-overhead gradient-free method)\n\n    - Keep a small memory of unit search directions and per-direction strengths.\n    - Most iterations: probe along one memory direction (forward/back), using a single-eval\n      greedy probe then optionally the opposite sign; adapt per-direction scale/strength.\n    - Occasionally do single-coordinate probes for fine-grained adjustments.\n    - On success, reinforce the used direction (increase strength) and slightly enlarge its scale.\n      On failure, decay scale and decrease strength.\n    - When a direction yields consistent improvement, refresh memory by injecting the\n      displacement vector between new best and previous best (normalized), replacing the weakest.\n    - Periodic tempered Cauchy jumps to escape stagnation.\n    \"\"\"\n    def __init__(self, budget, dim, mem_size=6, init_scale=None,\n                 min_scale=1e-6, max_scale=None, stagn_patience=14, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.mem_size = max(1, int(mem_size))\n        self.min_scale = float(min_scale)\n        self.stagn_patience = int(stagn_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        # initial scale per direction (or per-coordinate if provided)\n        self.init_scale = init_scale\n        if max_scale is None:\n            self.max_scale = 5.0  # absolute cap (working with bounds [-5,5])\n        else:\n            self.max_scale = float(max_scale)\n\n    def __call__(self, func):\n        lb = -5.0; ub = 5.0\n        lbv = np.full(self.dim, lb); ubv = np.full(self.dim, ub)\n        span = ubv - lbv\n\n        # init scales\n        if self.init_scale is None:\n            scale_vec = np.full(self.mem_size, 0.18 * span.mean())\n        else:\n            s = float(self.init_scale)\n            scale_vec = np.full(self.mem_size, s)\n\n        scale_vec = np.clip(scale_vec, self.min_scale, self.max_scale)\n\n        # initialize direction memory: random orthonormal-ish vectors\n        D = self.rng.normal(size=(self.mem_size, self.dim))\n        # normalize rows\n        D /= np.linalg.norm(D, axis=1, keepdims=True) + 1e-12\n\n        strengths = np.ones(self.mem_size)  # higher => more likely selected\n\n        archive_x = []\n        archive_f = []\n        evals = 0\n\n        # initial sampling: a few random points\n        n0 = max(1, min(self.budget, 1 + self.dim // 4))\n        for _ in range(n0):\n            x = lbv + self.rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n            if evals >= self.budget:\n                break\n\n        if len(archive_x) == 0:\n            x = lbv + self.rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n\n        bi = int(np.argmin(archive_f))\n        x_best = archive_x[bi].copy()\n        f_best = float(archive_f[bi])\n\n        no_improve = 0\n        prev_best = x_best.copy()\n\n        # helper to add to archive\n        def archive_add(xn, fn):\n            if len(archive_x) < 32:\n                archive_x.append(xn.copy()); archive_f.append(float(fn))\n            else:\n                worst = int(np.argmax(archive_f))\n                if fn < archive_f[worst]:\n                    archive_x[worst] = xn.copy(); archive_f[worst] = float(fn)\n\n        # main loop\n        while evals < self.budget:\n            # choose action: memory probe mostly, sometimes coordinate probe\n            use_mem = (self.rng.random() < 0.82 and self.mem_size > 0)\n            improved = False\n\n            if use_mem:\n                # select a memory direction by strength-weighted sampling\n                probs = strengths / (np.sum(strengths) + 1e-12)\n                i = int(self.rng.choice(self.mem_size, p=probs))\n                d = D[i]\n                s = float(scale_vec[i] * (1.0 + 0.12 * (self.rng.random() - 0.5)))\n                # bias sign by projection of archive median\n                if len(archive_x) >= 3:\n                    med = np.median(np.array(archive_x), axis=0)\n                    p = float(np.dot(med - x_best, d))\n                else:\n                    p = 0.0\n                if p > 1e-12:\n                    sign = 1 if self.rng.random() < 0.72 else -1\n                elif p < -1e-12:\n                    sign = -1 if self.rng.random() < 0.72 else 1\n                else:\n                    sign = 1 if self.rng.random() < 0.5 else -1\n                step = sign * s\n\n                # single-eval greedy probe: try forward only first (cheap)\n                x_try = x_best + step * d\n                x_try = np.minimum(np.maximum(x_try, lbv), ubv)\n                if evals >= self.budget:\n                    break\n                f_try = float(func(x_try)); evals += 1\n                archive_add(x_try, f_try)\n                if f_try < f_best:\n                    f_best = f_try; x_best = x_try.copy()\n                    # reinforce direction and enlarge scale a bit\n                    strengths[i] = min(10.0, strengths[i] * 1.22 + 1e-12)\n                    scale_vec[i] = min(self.max_scale, scale_vec[i] * 1.18 + 1e-12)\n                    no_improve = 0; improved = True\n                    # maybe try small extrapolation (half step) if budget remains\n                    if evals < self.budget and self.rng.random() < 0.38:\n                        xe = x_best + 0.5 * step * d\n                        xe = np.minimum(np.maximum(xe, lbv), ubv)\n                        fe = float(func(xe)); evals += 1\n                        archive_add(xe, fe)\n                        if fe < f_best:\n                            f_best = fe; x_best = xe.copy()\n                            scale_vec[i] = min(self.max_scale, scale_vec[i] * 1.08)\n                else:\n                    # try opposite sign once (cheap two-sided check)\n                    if evals < self.budget:\n                        x_op = x_best - step * d\n                        x_op = np.minimum(np.maximum(x_op, lbv), ubv)\n                        f_op = float(func(x_op)); evals += 1\n                        archive_add(x_op, f_op)\n                        if f_op < f_best:\n                            f_best = f_op; x_best = x_op.copy()\n                            strengths[i] = min(10.0, strengths[i] * 1.18 + 1e-12)\n                            scale_vec[i] = min(self.max_scale, scale_vec[i] * 1.12 + 1e-12)\n                            no_improve = 0; improved = True\n                        else:\n                            # decay scale and strength on failure\n                            scale_vec[i] = max(self.min_scale, scale_vec[i] * 0.68)\n                            strengths[i] = max(0.02, strengths[i] * 0.86)\n                            no_improve += 1\n                    else:\n                        no_improve += 1\n\n                # if direction led to a positional change, consider injecting displacement\n                if improved:\n                    disp = x_best - prev_best\n                    prev_best = x_best.copy()\n                    nd = np.linalg.norm(disp)\n                    if nd > 1e-12:\n                        newdir = disp / nd\n                        # replace weakest memory with probability depending on strength gap\n                        weakest = int(np.argmin(strengths))\n                        if strengths[weakest] < np.median(strengths) or self.rng.random() < 0.25:\n                            D[weakest] = newdir\n                            scale_vec[weakest] = max(self.min_scale, nd * 0.9)\n                            strengths[weakest] = max(0.5, np.mean(strengths) * 0.3 + 0.2)\n            else:\n                # coordinate probe: pick one coordinate, try + and - (two evals)\n                j = int(self.rng.integers(0, self.dim))\n                coord_step = 0.9 * (0.12 * span.mean())  # conservative coordinate step\n                s = coord_step * (1.0 + 0.25 * (self.rng.random() - 0.5))\n                xp = x_best.copy(); xm = x_best.copy()\n                xp[j] = np.clip(xp[j] + s, lbv[j], ubv[j])\n                xm[j] = np.clip(xm[j] - s, lbv[j], ubv[j])\n                if evals >= self.budget:\n                    break\n                fp = float(func(xp)); evals += 1; archive_add(xp, fp)\n                better = None\n                if fp < f_best:\n                    f_best = fp; x_best = xp.copy(); better = 'p'\n                    no_improve = 0\n                else:\n                    if evals < self.budget:\n                        fm = float(func(xm)); evals += 1; archive_add(xm, fm)\n                        if fm < f_best:\n                            f_best = fm; x_best = xm.copy(); better = 'm'\n                            no_improve = 0\n                        else:\n                            no_improve += 1\n                # if coordinate probe wins, consider injecting its unit vector into memory\n                if better is not None and self.mem_size > 0:\n                    unit = np.zeros(self.dim); unit[j] = 1.0 if better == 'p' else -1.0\n                    weakest = int(np.argmin(strengths))\n                    D[weakest] = unit\n                    scale_vec[weakest] = max(self.min_scale, 0.8 * s)\n                    strengths[weakest] = max(strengths[weakest], 0.6)\n\n            # maintain best from archive\n            if len(archive_x) > 0:\n                bi = int(np.argmin(archive_f))\n                if archive_f[bi] < f_best:\n                    f_best = float(archive_f[bi]); x_best = archive_x[bi].copy()\n\n            # stagnation handling\n            if no_improve >= self.stagn_patience and evals < self.budget:\n                no_improve = 0\n                # try a tempered median pull if archive has diversity\n                if len(archive_x) >= 3:\n                    med = np.median(np.array(archive_x), axis=0)\n                    pull = 0.35 * (med - x_best)\n                    if np.linalg.norm(pull) > 1e-12:\n                        xp = np.clip(x_best + pull, lbv, ubv)\n                        fpu = float(func(xp)); evals += 1; archive_add(xp, fpu)\n                        if fpu < f_best:\n                            f_best = fpu; x_best = xp.copy()\n                            # amplify scales of directions aligned with pull\n                            align = np.abs(D @ pull) / (np.linalg.norm(pull) + 1e-12)\n                            scale_vec = np.minimum(self.max_scale, scale_vec * (1.0 + 0.6 * align))\n                            strengths = np.minimum(10.0, strengths * (1.0 + 0.3 * align))\n                            continue\n                # otherwise do a tempered Cauchy jump\n                if evals < self.budget:\n                    scale = 0.45 * span.mean()\n                    jump = self.rng.standard_cauchy(self.dim) * scale\n                    # mask some coords to keep jumps partial\n                    mask = (self.rng.random(self.dim) < 0.6).astype(float)\n                    xj = x_best + jump * mask\n                    xj = np.clip(xj, lbv, ubv)\n                    fj = float(func(xj)); evals += 1; archive_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        # refresh memory: replace the weakest with the displacement direction\n                        disp = xj - prev_best\n                        nd = np.linalg.norm(disp)\n                        if nd > 1e-12:\n                            weakest = int(np.argmin(strengths))\n                            D[weakest] = disp / nd\n                            scale_vec[weakest] = min(self.max_scale, nd)\n                            strengths[weakest] = 1.0\n                        prev_best = x_best.copy()\n                    else:\n                        # mild randomize strengths/scales to encourage exploration\n                        strengths = np.maximum(0.02, strengths * (1.0 + 0.12 * (self.rng.random(self.mem_size) - 0.5)))\n                        scale_vec = np.clip(scale_vec * (1.0 + 0.18 * (self.rng.random(self.mem_size) - 0.5)),\n                                            self.min_scale, self.max_scale)\n                        # occasionally drop the worst archive item\n                        if len(archive_x) > 6 and self.rng.random() < 0.2:\n                            w = int(np.argmax(archive_f)); archive_x.pop(w); archive_f.pop(w)\n\n            # gentle decay to avoid runaway scales\n            scale_vec = np.clip(scale_vec * (1.0 - 3e-4), self.min_scale, self.max_scale)\n\n            # re-normalize memory directions occasionally to avoid numerical drift\n            if self.rng.random() < 0.08:\n                norms = np.linalg.norm(D, axis=1, keepdims=True) + 1e-12\n                D = D / norms\n\n            # safety break if budget exhausted\n            if evals >= self.budget:\n                break\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 18, "feedback": "The algorithm DirectionalMemoryDescent scored 0.288 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6dfd3db-5c46-4441-a54a-34c5ad9bdd18"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.3384538612169913}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.22921459211047646}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.22238170454863626}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.09510992991186906}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1233924354075987}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.07134720581347043}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.07405559156469421}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.11285850834240796}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.09200961849743527}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9877580305402212}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9825101377295931}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9859735250813422}], "aucs": [0.3384538612169913, 0.22921459211047646, 0.22238170454863626, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.09510992991186906, 0.1233924354075987, 0.07134720581347043, 0.07405559156469421, 0.11285850834240796, 0.09200961849743527, 0.9877580305402212, 0.9825101377295931, 0.9859735250813422]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2517.0, "Edges": 2516.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.9992054032578466, "Degree Variance": 2.0293994480756914, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.94575678040245, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3285084887520424, "Depth Entropy": 2.217316511492548, "Assortativity": 1.5257075545997838e-08, "Average Eccentricity": 19.36948748510131, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0003972983710766786, "Average Shortest Path": 10.742942269199018, "mean_complexity": 17.333333333333332, "total_complexity": 52.0, "mean_token_count": 755.3333333333334, "total_token_count": 2266.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "57efc6c5-0de7-4f77-95ae-0e884f6ddb81", "fitness": 0.5600686509001704, "name": "SeparableTriangulationMedianBlend", "description": "Separable Triangulation + Median-Blend (compact) \u2014 per-coordinate 3-point triangulation (mirrored probes + quadratic fit) guided by archive medians/dispersion, adaptive steps and occasional median/global blends to escape plateaus.", "code": "import numpy as np\n\nclass SeparableTriangulationMedianBlend:\n    def __init__(self, budget=10000, dim=10, archive_size=12, init_step=None,\n                 min_step=1e-6, max_step=None, stagn_patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.stagn_patience = int(stagn_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb); ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.asarray(lb, float); ub = np.asarray(ub, float)\n        if lb.shape == (): lb = np.full(self.dim, lb.item())\n        if ub.shape == (): ub = np.full(self.dim, ub.item())\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        D = self.dim\n        rng = self.rng\n        rand = rng.random\n        choice = rng.choice\n\n        span = ub - lb\n        if self.init_step is None:\n            step = 0.18 * span\n        else:\n            s0 = np.asarray(self.init_step, float)\n            step = s0 if s0.shape != () else np.full(D, float(s0))\n        if self.max_step is None:\n            max_step = 0.6 * span\n        else:\n            tmp = np.asarray(self.max_step, float)\n            max_step = tmp if tmp.shape != () else np.full(D, float(tmp))\n        step = np.clip(step, self.min_step, max_step)\n\n        B = self.budget\n        evals = 0\n        top_k = min(self.archive_size, max(2, B // 6))\n        archive_x = []\n        archive_f = []\n\n        # initial sampling (cheap)\n        n0 = min(max(4, D), max(1, B // 12))\n        for _ in range(n0):\n            if evals >= B: break\n            x = lb + rand(D) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n        if len(archive_x) == 0:\n            x = lb + rand(D) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n\n        idx = int(np.argmin(archive_f))\n        x_best = archive_x[idx].copy(); f_best = float(archive_f[idx])\n\n        stagn = 0\n        momentum = np.zeros(D)\n\n        def add_arc(xn, fn):\n            nonlocal archive_x, archive_f\n            if len(archive_x) < top_k:\n                archive_x.append(xn.copy()); archive_f.append(float(fn)); return\n            worst = int(np.argmax(archive_f))\n            if fn < archive_f[worst]:\n                archive_x[worst] = xn.copy(); archive_f[worst] = float(fn)\n\n        # main loop\n        while evals < B:\n            arr = np.array(archive_x)\n            med = np.median(arr, axis=0)\n            q75 = np.percentile(arr, 75, axis=0); q25 = np.percentile(arr, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            # coordinate selection: prefer large pull normalized by step and dispersion\n            score = abs_pull / (step + 1e-12) + 1.2 * (iqr / (span + 1e-12))\n            score = score + 1e-9\n            # tempered probabilities\n            temp = 1.0 + 0.14 * np.std(score)\n            sc = score - score.max()\n            probs = np.exp(sc / temp); probs = probs / probs.sum()\n            try:\n                c = int(choice(D, p=probs))\n            except Exception:\n                c = int(rng.integers(0, D))\n\n            s = float(max(step[c], self.min_step))\n            # mirrored probes: -s and +s around current best on coord c\n            x_minus = x_best.copy(); x_plus = x_best.copy()\n            x_minus[c] = np.clip(x_best[c] - s, lb[c], ub[c])\n            x_plus[c] = np.clip(x_best[c] + s, lb[c], ub[c])\n\n            # decide order: prefer side towards median\n            if pull[c] > 0:\n                order = (x_plus, x_minus)\n            else:\n                order = (x_minus, x_plus)\n\n            f0 = f_best\n            f1 = float(func(order[0])); evals += 1\n            add_arc(order[0], f1)\n            if evals >= B:\n                # update if better and exit\n                if f1 < f_best:\n                    f_best = f1; x_best = order[0].copy()\n                break\n            f2 = float(func(order[1])); evals += 1\n            add_arc(order[1], f2)\n\n            # map f_minus,f0,f_plus consistently at -s,0,+s\n            if pull[c] > 0:\n                fp = f1; fm = f2\n                xp = order[0]; xm = order[1]\n            else:\n                fp = f2; fm = f1\n                xp = order[1]; xm = order[0]\n\n            improved = False\n            # quadratic minimizer formula for symmetric probes at \u00b1s:\n            denom = 2.0 * (fm + fp - 2.0 * f0)\n            if abs(denom) > 1e-12:\n                offset = s * (fm - fp) / denom  # offset in [-~s,~s] often\n                # trust reasonable vertex positions up to 1.5*s\n                if abs(offset) <= 1.5 * s and evals < B:\n                    x_opt = x_best.copy()\n                    x_opt[c] = np.clip(x_best[c] + offset, lb[c], ub[c])\n                    fopt = float(func(x_opt)); evals += 1\n                    add_arc(x_opt, fopt)\n                    if fopt < f_best:\n                        f_best = fopt; x_best = x_opt.copy(); improved = True\n                        # step shrink when precise curvature helped\n                        step[c] = float(max(self.min_step, min(max_step[c], step[c] * 0.82)))\n                        momentum[c] = 0.6 * momentum[c] + 0.4 * offset\n                    else:\n                        # no improvement -> modest shrink\n                        step[c] = float(max(self.min_step, step[c] * 0.92))\n                        momentum[c] *= 0.84\n                else:\n                    # vertex outside trusted window -> take better of probes\n                    if fp < f_best:\n                        f_best = fp; x_best = xp.copy(); improved = True\n                        step[c] = float(min(max_step[c], step[c] * 1.08))\n                        momentum[c] = 0.5 * momentum[c] + 0.3 * (xp[c] - x_best[c])\n                    elif fm < f_best:\n                        f_best = fm; x_best = xm.copy(); improved = True\n                        step[c] = float(min(max_step[c], step[c] * 1.08))\n                        momentum[c] = 0.5 * momentum[c] + 0.3 * (xm[c] - x_best[c])\n                    else:\n                        step[c] = float(max(self.min_step, step[c] * 0.82))\n                        momentum[c] *= 0.8\n            else:\n                # nearly linear along probe: pick better side or small random push\n                if fp < f0 or fm < f0:\n                    if fp < fm:\n                        if fp < f_best:\n                            f_best = fp; x_best = xp.copy(); improved = True\n                        step[c] = float(min(max_step[c], step[c] * 1.06))\n                    else:\n                        if fm < f_best:\n                            f_best = fm; x_best = xm.copy(); improved = True\n                        step[c] = float(min(max_step[c], step[c] * 1.06))\n                else:\n                    # promote exploration\n                    if rand() < 0.18 and evals < B:\n                        jump = (0.6 * step[c] + 0.4 * span[c] * rand()) * (1 if rand() < 0.5 else -1)\n                        xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                        fj = float(func(xj)); evals += 1\n                        add_arc(xj, fj)\n                        if fj < f_best:\n                            f_best = fj; x_best = xj.copy(); improved = True\n                            step[c] = float(min(max_step[c], step[c] * 1.18))\n                            momentum[c] = 0.6 * momentum[c] + 0.4 * jump\n                        else:\n                            step[c] = float(max(self.min_step, step[c] * 0.74))\n                            momentum[c] *= 0.72\n\n            # moderation and bounds\n            step = np.minimum(np.maximum(step, self.min_step), max_step)\n            momentum *= 0.96\n\n            if improved:\n                stagn = 0\n            else:\n                stagn += 1\n\n            # occasional light median pull\n            if rand() < 0.08 and evals < B:\n                mshift = 0.22 * pull\n                xm = np.clip(x_best + mshift, lb, ub)\n                fm = float(func(xm)); evals += 1\n                add_arc(xm, fm)\n                if fm < f_best:\n                    f_best = fm; x_best = xm.copy()\n                    step = np.minimum(max_step, step * 1.06)\n                    momentum += 0.15 * mshift\n                    stagn = 0\n\n            # stronger archive-driven restart on stagnation\n            if stagn >= self.stagn_patience and evals < B:\n                center = np.median(np.array(archive_x), axis=0)\n                sigma = 0.28 * (iqr + 1e-9) + 0.02 * span\n                tries = min(6, max(1, (B - evals) // 6))\n                improved_any = False\n                for _ in range(tries):\n                    if evals >= B: break\n                    xr = np.clip(center + rng.normal(0, 1, D) * sigma, lb, ub)\n                    fr = float(func(xr)); evals += 1\n                    add_arc(xr, fr)\n                    if fr < f_best:\n                        f_best = fr; x_best = xr.copy(); improved_any = True\n                        step = np.minimum(max_step, step * 1.12)\n                        momentum = 0.5 * (momentum + (xr - x_best))\n                        break\n                if not improved_any:\n                    # gentle randomize some steps\n                    step = np.minimum(max_step, step * (1.0 + 0.18 * rand(D)))\n                stagn = 0\n\n        return f_best, x_best", "configspace": "", "generation": 18, "feedback": "The algorithm SeparableTriangulationMedianBlend scored 0.560 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8e4d543e-a18a-4def-9b56-2a20aed9fccf"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9916478329315185}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9837409365580789}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9190965819103798}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.6113552118581532}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.6351878191470082}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6848722239561559}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.16186481558321342}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.051358063716118174}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.13488488070436577}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.08087830882860692}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.11716146339107325}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0718266014904908}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9939009209780353}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.988688951732784}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9745651507165731}], "aucs": [0.9916478329315185, 0.9837409365580789, 0.9190965819103798, 0.6113552118581532, 0.6351878191470082, 0.6848722239561559, 0.16186481558321342, 0.051358063716118174, 0.13488488070436577, 0.08087830882860692, 0.11716146339107325, 0.0718266014904908, 0.9939009209780353, 0.988688951732784, 0.9745651507165731]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2468.0, "Edges": 2467.0, "Max Degree": 39.0, "Min Degree": 1.0, "Mean Degree": 1.9991896272285252, "Degree Variance": 2.323338079114448, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.330365093499555, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3200035150493936, "Depth Entropy": 2.193645486892052, "Assortativity": 0.0, "Average Eccentricity": 16.08995137763371, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.0004051863857374392, "Average Shortest Path": 10.183051285066608, "mean_complexity": 12.25, "total_complexity": 49.0, "mean_token_count": 536.0, "total_token_count": 2144.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "7b8ff9a2-76db-4ccd-9d7c-5ec4f4578d39", "fitness": 0.6223340530809427, "name": "ThompsonCoordinateBandit", "description": "Directional Thompson Bandit \u2014 per-coordinate Thompson sampling over signed Beta priors with lightweight per-dim momentum, conservative multiplicative step adaptation, occasional heavy-tail single-eval escapes and periodic median pulls from a tiny archive for guided moves.", "code": "import numpy as np\n\nclass ThompsonCoordinateBandit:\n    def __init__(self, budget, dim, init_step=0.12,\n                 success_mult=1.25, failure_mult=0.7,\n                 global_prob=0.035, median_period=20, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.global_prob = float(global_prob)\n        self.median_period = int(median_period)\n        self.rng = np.random.default_rng(seed)\n\n    def _bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base = max(1e-12, self.init_step_frac * box_len)\n        min_step = max(1e-15, 1e-9 * box_len)\n        max_step = max(1e-12, 1.5 * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        x0 = rng.uniform(lb, ub)\n        f0 = safe_eval(x0)\n        x_best = x0.copy(); f_best = float(f0)\n        # tiny archive of two best to do cheap median pulls\n        x_second = x_best.copy(); f_second = f_best\n\n        steps = np.full(dim, base, float)\n        a_pos = np.ones(dim); b_pos = np.ones(dim)\n        a_neg = np.ones(dim); b_neg = np.ones(dim)\n        trials = np.zeros(dim, int)\n        last_sign = np.zeros(dim, int)\n        consec_fail = np.zeros(dim, int)\n        it = 0\n\n        while evals < self.budget:\n            # occasional heavy-tail single-eval global escape\n            if rng.random() < self.global_prob and evals < self.budget:\n                c = rng.standard_cauchy(dim)\n                jump = 0.12 * box_len * c\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_second, f_second = x_best.copy(), f_best\n                    x_best, f_best = xg.copy(), fg\n\n            # periodic cheap median pull from two-best archive\n            if (it % max(1, self.median_period) == 0) and (f_second < np.inf) and (not np.allclose(x_second, x_best)):\n                xm = 0.5 * (x_best + x_second)\n                fm = safe_eval(xm)\n                if fm < f_best:\n                    x_second, f_second = x_best.copy(), f_best\n                    x_best, f_best = xm.copy(), fm\n\n            # Thompson sampling per sign\n            ppos = rng.beta(a_pos, b_pos)\n            pneg = rng.beta(a_neg, b_neg)\n            # headroom penalty to avoid hitting bounds\n            head_ub = (ub - x_best) / (ub - lb + 1e-18)\n            head_lb = (x_best - lb) / (ub - lb + 1e-18)\n            eff_pos = ppos * (0.5 + 0.5 * np.clip(head_ub, 0, 1))\n            eff_neg = pneg * (0.5 + 0.5 * np.clip(head_lb, 0, 1))\n            # small momentum and exploration bonus for under-tried dims\n            bonus = 1.0 / (1.0 + np.sqrt(trials))\n            mom = 0.12 * (last_sign > 0)  # small bias for recent + successes\n            mom_neg = 0.12 * (last_sign < 0)\n            score_pos = eff_pos * steps + mom + 1e-10 * bonus\n            score_neg = eff_neg * steps + mom_neg + 1e-10 * bonus\n            choose_pos = score_pos >= score_neg\n            score = np.where(choose_pos, score_pos, score_neg)\n            d = int(np.argmax(score))\n            sign = 1 if choose_pos[d] else -1\n\n            # one-eval coordinate move\n            stepd = float(np.clip(steps[d], min_step, max_step))\n            xt = x_best.copy()\n            xt[d] = np.clip(xt[d] + sign * stepd, lb[d], ub[d])\n            ft = safe_eval(xt)\n            trials[d] += 1\n\n            # update Beta priors\n            if sign > 0:\n                if ft < f_best:\n                    a_pos[d] += 1.0\n                else:\n                    b_pos[d] += 1.0\n            else:\n                if ft < f_best:\n                    a_neg[d] += 1.0\n                else:\n                    b_neg[d] += 1.0\n\n            # accept or reject\n            if ft < f_best:\n                x_second, f_second = x_best.copy(), f_best\n                x_best, f_best = xt.copy(), float(ft)\n                steps[d] = min(max_step, steps[d] * self.success_mult)\n                consec_fail[d] = 0\n                last_sign[d] = sign\n            else:\n                steps[d] = max(min_step, steps[d] * self.failure_mult)\n                consec_fail[d] += 1\n                last_sign[d] = -sign  # slightly prefer opposite next time\n\n            # conservative corrective action on repeated failures\n            if consec_fail[d] >= 6:\n                steps[d] = max(min_step, steps[d] * (self.failure_mult ** 1.5))\n                if sign > 0:\n                    b_pos[d] += 0.5\n                else:\n                    b_neg[d] += 0.5\n                consec_fail[d] = 0\n\n            # mild shrink of Beta mass to avoid overconfidence\n            if trials[d] > 50:\n                for arr in ((a_pos, b_pos), (a_neg, b_neg)):\n                    arr[0][d] = 1.0 + (arr[0][d] - 1.0) * 0.9\n                    arr[1][d] = 1.0 + (arr[1][d] - 1.0) * 0.9\n                trials[d] = int(trials[d] * 0.6)\n\n            it += 1\n\n        # cheap final polish: small symmetric one-sided passes until budget used\n        final = max(1e-12, 0.05 * box_len)\n        while evals < self.budget and final > 1e-12:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + final, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - final, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True\n            if not improved:\n                final *= 0.5\n\n        self.x_opt = np.asarray(x_best, float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 18, "feedback": "The algorithm ThompsonCoordinateBandit scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2ce99025-003a-46b2-bec0-52c377bfc061"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9893022067545653}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9878900102376711}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9883253869754131}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9782405053292381}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9768123667646177}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.976682005773988}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08516504740379716}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.052254954198001746}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.09173367360943407}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.06566933306663736}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07674811383515934}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07676942854585045}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9961786683556498}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.996826683054607}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9964124123095106}], "aucs": [0.9893022067545653, 0.9878900102376711, 0.9883253869754131, 0.9782405053292381, 0.9768123667646177, 0.976682005773988, 0.08516504740379716, 0.052254954198001746, 0.09173367360943407, 0.06566933306663736, 0.07674811383515934, 0.07676942854585045, 0.9961786683556498, 0.996826683054607, 0.9964124123095106]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1706.0, "Edges": 1705.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9988276670574443, "Degree Variance": 2.4173491531852966, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.391139240506329, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3135457397648782, "Depth Entropy": 1.8538921850110068, "Assortativity": 0.0, "Average Eccentricity": 14.935521688159437, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0005861664712778429, "Average Shortest Path": 9.199430679368659, "mean_complexity": 8.75, "total_complexity": 35.0, "mean_token_count": 370.5, "total_token_count": 1482.0, "mean_parameter_count": 3.5, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "acf350dc-8fdd-4870-af96-f6c251de308b", "fitness": 0.08953995069699558, "name": "SparseRotationalDirectionalEvolution", "description": "Sparse Rotational Directional Evolution \u2014 maintain a small evolving set of low-rank search directions, perform cheap mirrored probes along one selected direction per step, adapt per-direction step sizes and momentum, occasionally rotate directions and pull toward archive medians for guided diversification.", "code": "import numpy as np\n\nclass SparseRotationalDirectionalEvolution:\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 success_mult=1.3, failure_mult=0.65,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 rot_prob=0.08, median_prob=0.04,\n                 mom_gain=0.22, mom_decay=0.85,\n                 dirs=None, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.succ_mult = float(success_mult); self.fail_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.rot_prob = float(rot_prob); self.median_prob = float(median_prob)\n        self.mg = float(mom_gain); self.md = float(mom_decay)\n        # number of persistent directions\n        self.K = int(dirs) if dirs is not None else max(2, min(12, self.dim//2 + 1))\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng; K = self.K\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * box)\n        min_step = max(1e-15, self.min_step_frac * box)\n        max_step = max(1e-12, self.max_step_frac * box)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x = rng.uniform(lb, ub)\n        fx = safe_eval(x)\n        steps = np.full(K, base)\n        succ = np.ones(K) * 0.5  # soft success rates\n        fail = np.ones(K) * 0.5\n        mom = np.zeros(K)\n        # directions: random orthonormal-ish\n        D = rng.normal(size=(K, dim))\n        D /= (np.linalg.norm(D, axis=1, keepdims=True) + 1e-12)\n\n        archive = [(x.copy(), fx)]\n\n        while evals < self.budget:\n            # occasional median pull from archive\n            if rng.random() < self.median_prob and len(archive) > 1 and evals < self.budget:\n                pts = np.vstack([p for p,_ in archive])\n                xm = np.median(pts, axis=0)\n                cand = np.clip(x + 0.5*(xm - x), lb, ub)\n                fc = safe_eval(cand)\n                if fc < fx:\n                    x, fx = cand.copy(), float(fc)\n                    archive.append((x.copy(), fx)); archive = sorted(archive, key=lambda t: t[1])[:8]\n\n            # compute selection scores for directions\n            # headroom along direction: dot of abs(d) with available room\n            room = (ub - x) + (x - lb)  # 2*room vector\n            absD = np.abs(D)\n            hr = np.clip(absD.dot(room) / (box + 1e-12), 0.0, 1e9)\n            score = (succ / (succ + fail + 1e-12)) * (1.0 + np.abs(mom)) * steps * (0.2 + 0.8 * (hr/(hr+1e-12)))\n            j = int(np.argmax(score + 1e-12 * rng.random(K)))\n\n            d = D[j]\n            s = float(np.clip(steps[j], min_step, max_step))\n            # mirrored probe (two evals if budget allows)\n            xp = np.clip(x + s * d, lb, ub)\n            xm = np.clip(x - s * d, lb, ub)\n            fp = safe_eval(xp)\n            fm = safe_eval(xm)\n            # choose best of probes\n            if fp < fm:\n                bestp, fb = xp, fp\n                other, fo = xm, fm\n                sign = 1.0\n            else:\n                bestp, fb = xm, fm\n                other, fo = xp, fp\n                sign = -1.0\n\n            improved = fb < fx\n            # update succ/fail exponential counts\n            if improved:\n                succ[j] = 0.8 * succ[j] + 0.2 * 1.0\n                fail[j] = 0.8 * fail[j] + 0.2 * 0.0\n                # accept\n                x, fx = bestp.copy(), float(fb)\n                archive.append((x.copy(), fx)); archive = sorted(archive, key=lambda t: t[1])[:8]\n                # increase step and momentum\n                steps[j] = min(max_step, steps[j] * (self.succ_mult * (1.0 + 0.25 * (succ[j]/(succ[j]+fail[j]+1e-12)))))\n                mom[j] = mom[j] * self.md + sign * self.mg\n                # cheap extrapolation along same dir if budget remains\n                if evals < self.budget:\n                    extra = np.clip(x + sign * steps[j] * d, lb, ub)\n                    fe = safe_eval(extra)\n                    if fe < fx:\n                        x, fx = extra.copy(), float(fe)\n                        archive.append((x.copy(), fx)); archive = sorted(archive, key=lambda t: t[1])[:8]\n            else:\n                succ[j] = 0.9 * succ[j]\n                fail[j] = 0.9 * fail[j] + 0.1\n                steps[j] = max(min_step, steps[j] * self.fail_mult)\n                mom[j] *= np.sqrt(self.md)\n\n            # occasional small rotation / rejuvenation of chosen direction\n            if rng.random() < self.rot_prob:\n                noise = rng.normal(scale=0.12, size=dim)\n                dnew = d + 0.7 * noise\n                dnew /= (np.linalg.norm(dnew) + 1e-12)\n                D[j] = dnew\n\n            # mild mixing: replace worst direction occasionally with direction toward archive median\n            if rng.random() < 0.03 and len(archive) > 1:\n                pts = np.vstack([p for p,_ in archive])\n                med = np.median(pts, axis=0)\n                v = med - x\n                if np.any(np.abs(v) > 1e-12):\n                    k = int(np.argmin(succ/(succ+fail+1e-12)))\n                    D[k] = v / (np.linalg.norm(v) + 1e-12)\n                    steps[k] = base\n\n        # final coordinate polish (small symmetric steps)\n        fin = 0.05 * box\n        while evals < self.budget and fin > 1e-12 * box:\n            improved = False\n            for i in range(dim):\n                if evals >= self.budget: break\n                for sgn in (1.0, -1.0):\n                    xt = x.copy(); xt[i] = np.clip(xt[i] + sgn * fin, lb[i], ub[i])\n                    ft = safe_eval(xt)\n                    if ft < fx:\n                        x, fx, improved = xt.copy(), float(ft), True\n                        archive.append((x.copy(), fx)); archive = sorted(archive, key=lambda t: t[1])[:8]\n                        break\n            if not improved:\n                fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 19, "feedback": "The algorithm SparseRotationalDirectionalEvolution scored 0.090 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["48a2954a-fdcf-49d4-ac15-3ec15640c194"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.21206743907375258}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.20920104605427403}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.24190285139463963}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.003926753310275255}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.003462671072213541}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.10574071576423338}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.10145964134791263}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.46498814243763265}], "aucs": [0.21206743907375258, 0.20920104605427403, 0.24190285139463963, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.003926753310275255, 0.003462671072213541, 4.999999999999449e-05, 4.999999999999449e-05, 0.10574071576423338, 0.10145964134791263, 0.46498814243763265]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1839.0, "Edges": 1838.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9989124524197934, "Degree Variance": 2.25557249858882, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 7.914590747330961, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3252645198356143, "Depth Entropy": 2.1258265704804273, "Assortativity": 0.0, "Average Eccentricity": 18.31973898858075, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.000543773790103317, "Average Shortest Path": 9.980976792870706, "mean_complexity": 10.666666666666666, "total_complexity": 32.0, "mean_token_count": 551.3333333333334, "total_token_count": 1654.0, "mean_parameter_count": 5.666666666666667, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "8b45a635-80bc-4163-b040-d6e3acb62b6c", "fitness": 0.6078683334817699, "name": "StochasticLowRankSearch", "description": "Stochastic Low-Rank Search (SLRS) \u2014 maintain a tiny pool of useful search directions (low-rank subspace), perform cheap mirrored probes along pool directions and occasional axis sweeps, adapt per-direction and per-coordinate step sizes multiplicatively, replace pool vectors with successful displacements and use rare Cauchy global jumps for escapes.", "code": "import numpy as np\n\nclass StochasticLowRankSearch:\n    def __init__(self, budget=1000, dim=10, pool_size=None,\n                 init_step=0.08, success_mult=1.35, failure_mult=0.65,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, extrapolate_gain=1.6,\n                 polish_frac=0.04, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob)\n        self.extrapolate_gain = float(extrapolate_gain)\n        self.polish_frac = float(polish_frac)\n        self.pool_size = int(pool_size) if pool_size is not None else min(6, max(2, self.dim//2))\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box_scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = max(1e-12, self.max_step_frac * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        # pool of directions (unit vectors)\n        m = max(1, min(self.pool_size, dim))\n        pool = rng.normal(size=(m, dim))\n        pool /= np.linalg.norm(pool, axis=1, keepdims=True) + 1e-12\n        steps_pool = np.full(m, base, float)\n        trials_pool = np.zeros(m, int); succ_pool = np.zeros(m, int)\n        # axis steps\n        steps_coord = np.full(dim, base, float)\n        trials_coord = np.zeros(dim, int); succ_coord = np.zeros(dim, int)\n\n        while evals < self.budget:\n            # rare heavy-tailed global diversification (Cauchy-like)\n            if rng.random() < self.global_prob and evals < self.budget:\n                u = rng.random(dim)\n                jump = 0.12 * box_scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                continue\n\n            # choose pool direction vs coordinate sweep\n            use_pool = (rng.random() < 0.72 and m > 0)\n            if use_pool:\n                # pick pool index by encouraging large step & few trials\n                scores = steps_pool * (1.0 + 1.0 / (1 + trials_pool))\n                idx = int(np.argmax(scores + 1e-12 * rng.random(m)))\n                s = float(np.clip(steps_pool[idx], min_step, max_step))\n                u = pool[idx]\n                # mirrored if at least 2 evals left\n                if evals <= self.budget - 2:\n                    xp = np.clip(x_best + s * u, lb, ub)\n                    xm = np.clip(x_best - s * u, lb, ub)\n                    fp = safe_eval(xp); fm = safe_eval(xm)\n                    trials_pool[idx] += 2\n                    # accept better side\n                    if fp < f_best or fm < f_best:\n                        if fp < fm:\n                            prev = x_best.copy(); x_best, f_best = xp.copy(), fp\n                            disp = x_best - prev\n                        else:\n                            prev = x_best.copy(); x_best, f_best = xm.copy(), fm\n                            disp = x_best - prev\n                        succ_pool[idx] += 1\n                        steps_pool[idx] = min(max_step, steps_pool[idx] * self.success_mult)\n                        # replace a weak pool vector with successful displacement direction\n                        norm = np.linalg.norm(disp)\n                        if norm > 1e-12:\n                            replace_idx = int(np.argmin(succ_pool + 1e-9 * rng.random(m)))\n                            pool[replace_idx] = disp / (norm + 1e-12)\n                            steps_pool[replace_idx] = steps_pool[idx]\n                            trials_pool[replace_idx] = 0; succ_pool[replace_idx] = 0\n                        # opportunistic extrapolation\n                        if evals < self.budget:\n                            ext = x_best + np.sign(np.dot(disp, u)) * u * s * self.extrapolate_gain\n                            ext = np.clip(ext, lb, ub)\n                            fext = safe_eval(ext)\n                            if fext < f_best:\n                                x_best, f_best = ext.copy(), fext\n                    else:\n                        steps_pool[idx] = max(min_step, steps_pool[idx] * self.failure_mult)\n                else:\n                    # single-eval remaining: try one side\n                    if rng.random() < 0.5:\n                        xt = np.clip(x_best + s * u, lb, ub)\n                    else:\n                        xt = np.clip(x_best - s * u, lb, ub)\n                    ft = safe_eval(xt); trials_pool[idx] += 1\n                    if ft < f_best:\n                        prev = x_best.copy(); x_best, f_best = xt.copy(), ft\n                        disp = x_best - prev\n                        succ_pool[idx] += 1\n                        steps_pool[idx] = min(max_step, steps_pool[idx] * self.success_mult)\n                        norm = np.linalg.norm(disp)\n                        if norm > 1e-12:\n                            replace_idx = int(np.argmin(succ_pool + 1e-9 * rng.random(m)))\n                            pool[replace_idx] = disp / (norm + 1e-12)\n                    else:\n                        steps_pool[idx] = max(min_step, steps_pool[idx] * self.failure_mult)\n            else:\n                # axis-aligned cheap mirrored probe (coordinate exploitation)\n                scores = steps_coord * (1.0 + 1.0 / (1 + trials_coord))\n                d = int(np.argmax(scores + 1e-12 * rng.random(dim)))\n                s = float(np.clip(steps_coord[d], min_step, max_step))\n                if evals <= self.budget - 2:\n                    xp = x_best.copy(); xm = x_best.copy()\n                    xp[d] = np.clip(xp[d] + s, lb[d], ub[d])\n                    xm[d] = np.clip(xm[d] - s, lb[d], ub[d])\n                    fp = safe_eval(xp); fm = safe_eval(xm)\n                    trials_coord[d] += 2\n                    if fp < f_best or fm < f_best:\n                        if fp < fm:\n                            prev = x_best.copy(); x_best, f_best = xp.copy(), fp\n                            disp = x_best - prev\n                        else:\n                            prev = x_best.copy(); x_best, f_best = xm.copy(), fm\n                            disp = x_best - prev\n                        succ_coord[d] += 1\n                        steps_coord[d] = min(max_step, steps_coord[d] * self.success_mult)\n                        # add axis displacement to pool occasionally\n                        if np.linalg.norm(disp) > 1e-12 and rng.random() < 0.25 and m > 0:\n                            replace_idx = int(np.argmin(succ_pool + 1e-9 * rng.random(m)))\n                            pool[replace_idx] = disp / (np.linalg.norm(disp) + 1e-12)\n                            steps_pool[replace_idx] = steps_coord[d]\n                            trials_pool[replace_idx] = 0; succ_pool[replace_idx] = 0\n                        if evals < self.budget:\n                            # small extrapolation along found axis\n                            ext = x_best.copy()\n                            sign = np.sign(disp[d]) if disp[d] != 0 else 1.0\n                            ext[d] = np.clip(ext[d] + sign * s * self.extrapolate_gain, lb[d], ub[d])\n                            fext = safe_eval(ext)\n                            if fext < f_best:\n                                x_best, f_best = ext.copy(), fext\n                    else:\n                        steps_coord[d] = max(min_step, steps_coord[d] * self.failure_mult)\n                else:\n                    # single eval remaining\n                    if rng.random() < 0.5:\n                        xt = x_best.copy(); xt[d] = np.clip(xt[d] + s, lb[d], ub[d])\n                    else:\n                        xt = x_best.copy(); xt[d] = np.clip(xt[d] - s, lb[d], ub[d])\n                    ft = safe_eval(xt); trials_coord[d] += 1\n                    if ft < f_best:\n                        prev = x_best.copy(); x_best, f_best = xt.copy(), ft\n                        disp = x_best - prev\n                        succ_coord[d] += 1\n                        steps_coord[d] = min(max_step, steps_coord[d] * self.success_mult)\n                        if np.linalg.norm(disp) > 1e-12 and m > 0 and rng.random() < 0.2:\n                            replace_idx = int(np.argmin(succ_pool + 1e-9 * rng.random(m)))\n                            pool[replace_idx] = disp / (np.linalg.norm(disp) + 1e-12)\n                    else:\n                        steps_coord[d] = max(min_step, steps_coord[d] * self.failure_mult)\n\n            # mild rejuvenation of stagnant coordinates/pool entries\n            if rng.random() < 0.04:\n                worst_c = int(np.argmax(trials_coord - succ_coord))\n                steps_coord[worst_c] = min(max_step, steps_coord[worst_c] * 1.4)\n                trials_coord[worst_c] = int(trials_coord[worst_c] * 0.6)\n            if m > 0 and rng.random() < 0.03:\n                worst_p = int(np.argmax(trials_pool - succ_pool))\n                steps_pool[worst_p] = min(max_step, steps_pool[worst_p] * 1.4)\n                trials_pool[worst_p] = int(trials_pool[worst_p] * 0.6)\n\n        # final polish: tiny axis sweeps\n        fin = self.polish_frac * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 19, "feedback": "The algorithm StochasticLowRankSearch scored 0.608 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["82379331-5855-488c-a7d5-53d7afbbd2bb"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9245227617855604}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9281447866598347}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9338383767586588}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8254389079069175}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8269672140121636}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8585896121787769}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0933470864487912}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.11861805972780226}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.14788723727240283}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.14710635866575883}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.16428621582488323}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.16501655404282933}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9938937028299097}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9939949912751189}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9963731368371401}], "aucs": [0.9245227617855604, 0.9281447866598347, 0.9338383767586588, 0.8254389079069175, 0.8269672140121636, 0.8585896121787769, 0.0933470864487912, 0.11861805972780226, 0.14788723727240283, 0.14710635866575883, 0.16428621582488323, 0.16501655404282933, 0.9938937028299097, 0.9939949912751189, 0.9963731368371401]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2678.0, "Edges": 2677.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9992531740104555, "Degree Variance": 1.995518486313675, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 9.022094926350245, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3336508863861194, "Depth Entropy": 2.2463635709752046, "Assortativity": 0.0, "Average Eccentricity": 18.72404779686333, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003734129947722181, "Average Shortest Path": 10.895350903598072, "mean_complexity": 17.333333333333332, "total_complexity": 52.0, "mean_token_count": 761.0, "total_token_count": 2283.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "df63ca1c-4962-4832-b309-0f63dcef9c6f", "fitness": 0.3006899689237971, "name": "ProbabilisticMirroredCoordinateDescent", "description": "Probabilistic Mirrored Coordinate Descent (PMCD) \u2014 use cheap mirrored 1-D probes to estimate per-coordinate improvement-signs and confidence, adapt per-dim step-sizes multiplicatively, bias coordinate choice by archive dispersion + estimated uncertainty, and perform occasional median-pulls and controlled long jumps on stagnation.", "code": "import numpy as np\n\nclass ProbabilisticMirroredCoordinateDescent:\n    \"\"\"\n    Probabilistic Mirrored Coordinate Descent (PMCD)\n\n    - Strictly 1-D probes when possible: mirrored (+step, -step) to get a cheap sign/confidence.\n    - Maintain a tiny top-k archive to compute coordinate-wise medians and dispersion (IQR).\n    - Adapt per-coordinate step sizes based on success/failure streaks and confidence from mirrored probes.\n    - Coordinate selection mixes pull-to-median, dispersion-driven exploration and uncertainty-driven sampling.\n    - Occasional median-pulls and occasional large randomized jumps to escape stagnation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None, init_step=None, min_step=1e-6,\n                 max_step=None, archive_size=12, stagnation_patience=12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.archive_size = max(2, int(archive_size))\n        self.stagnation_patience = int(stagnation_patience)\n\n    def _get_bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == (): lb = np.full(self.dim, float(lb))\n        if ub.shape == (): ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        rng.shuffle(np.arange(1))  # seed use to avoid flake\n\n        rng2 = rng\n\n        span = ub - lb\n        if self.init_step is None:\n            step = 0.18 * span\n        else:\n            init = np.array(self.init_step, dtype=float)\n            step = init if init.shape != () else np.full(self.dim, float(init))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            mm = np.array(self.max_step, dtype=float)\n            max_step = mm if mm.shape != () else np.full(self.dim, float(mm))\n        step = np.clip(step, self.min_step, max_step)\n\n        # small initial archive\n        top_k = min(self.archive_size, max(2, self.budget // 6))\n        archive_x = []\n        archive_f = []\n        evals = 0\n\n        # seed with a handful of random points\n        init_n = min(max(4, self.dim), max(1, self.budget // 15))\n        for _ in range(init_n):\n            x = lb + rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n            if evals >= self.budget: break\n        if len(archive_x) == 0:\n            x = lb + rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n\n        # best\n        best_idx = int(np.argmin(archive_f))\n        x_best = archive_x[best_idx].copy()\n        f_best = float(archive_f[best_idx])\n\n        # bookkeeping\n        succ = np.zeros(self.dim, dtype=int)\n        fail = np.zeros(self.dim, dtype=int)\n        uncert = np.ones(self.dim, dtype=float)  # higher => more uncertain\n        stagn = 0\n\n        def add_archive(xn, fn):\n            nonlocal archive_x, archive_f\n            fn = float(fn)\n            if len(archive_x) < top_k:\n                archive_x.append(xn.copy()); archive_f.append(fn)\n            else:\n                wi = int(np.argmax(archive_f))\n                if fn < archive_f[wi]:\n                    archive_x[wi] = xn.copy(); archive_f[wi] = fn\n\n        while evals < self.budget:\n            arr = np.array(archive_x)\n            med = np.median(arr, axis=0)\n            q75 = np.percentile(arr, 75, axis=0); q25 = np.percentile(arr, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n\n            # weigh coordinates: pull-to-median, dispersion, and uncertainty\n            pull = np.abs(med - x_best)\n            score = (pull / (step + 1e-12)) * 0.9 + (iqr / (span + 1e-12)) * 0.6 + 0.8 * uncert\n            score = np.maximum(score, 1e-9)\n            probs = score / score.sum()\n\n            # pick a coordinate\n            try:\n                coord = int(rng.choice(self.dim, p=probs))\n            except Exception:\n                coord = int(rng.integers(0, self.dim))\n\n            s = float(step[coord])\n            if s < self.min_step: s = self.min_step\n\n            # mirrored probe if budget allows two evals (gives sign/confidence)\n            use_mirrored = (evals + 2 <= self.budget)\n            if use_mirrored:\n                x_p = x_best.copy(); x_m = x_best.copy()\n                x_p[coord] = np.clip(x_p[coord] + s, lb[coord], ub[coord])\n                x_m[coord] = np.clip(x_m[coord] - s, lb[coord], ub[coord])\n                f_p = float(func(x_p)); f_m = float(func(x_m)); evals += 2\n                add_archive(x_p, f_p); add_archive(x_m, f_m)\n                # prefer the better side; infer a central finite-difference slope\n                if f_p < f_m:\n                    preferred = 1; pref_f = f_p; pref_x = x_p\n                else:\n                    preferred = -1; pref_f = f_m; pref_x = x_m\n                slope_conf = np.abs(f_p - f_m) / (abs(f_p) + abs(f_m) + 1e-12)\n                # if mirrored probe already improved best, accept and maybe extrapolate\n                if pref_f < f_best:\n                    x_best = pref_x.copy(); f_best = pref_f\n                    succ[coord] += 1; fail[coord] = 0\n                    uncert[coord] = max(0.1, uncert[coord] * 0.6)\n                    step[coord] = float(min(max_step[coord], step[coord] * (1.1 + 0.5 * slope_conf)))\n                    stagn = 0\n                    # small extrapolation attempt if budget left\n                    if evals < self.budget:\n                        ext = np.clip(x_best.copy(), lb, ub)\n                        ext[coord] = np.clip(ext[coord] + preferred * 0.7 * s, lb[coord], ub[coord])\n                        f_ext = float(func(ext)); evals += 1\n                        add_archive(ext, f_ext)\n                        if f_ext < f_best:\n                            x_best = ext; f_best = f_ext\n                            step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                    continue  # move to next iter\n                else:\n                    # no improvement; propose a cautious move toward preferred side with reduced size\n                    delta = preferred * s * (0.6 + 0.4 * slope_conf)\n                    x_trial = x_best.copy()\n                    x_trial[coord] = np.clip(x_trial[coord] + delta, lb[coord], ub[coord])\n                    # we might reuse pref_x if equal, otherwise evaluate\n                    f_trial = float(func(x_trial)); evals += 1\n                    add_archive(x_trial, f_trial)\n                    if f_trial < f_best:\n                        x_best = x_trial.copy(); f_best = f_trial\n                        succ[coord] += 1; fail[coord] = 0\n                        uncert[coord] = max(0.2, uncert[coord] * 0.7)\n                        step[coord] = float(min(max_step[coord], step[coord] * (1.08 + 0.3 * slope_conf)))\n                        stagn = 0\n                    else:\n                        # failure: shrink, increase uncertainty\n                        fail[coord] += 1\n                        uncert[coord] = min(10.0, uncert[coord] * (1.2 + 0.5 * (1 - slope_conf)))\n                        step[coord] = float(max(self.min_step, step[coord] * 0.55))\n                        stagn += 1\n            else:\n                # single-eval cheap probe: try median-pull or a signed step suggested by momentum of median\n                direction = np.sign(med[coord] - x_best[coord])\n                if direction == 0:\n                    direction = 1 if rng.random() < 0.5 else -1\n                delta = direction * s * (0.9 if rng.random() < 0.7 else 1.4)\n                x_trial = x_best.copy()\n                x_trial[coord] = np.clip(x_trial[coord] + delta, lb[coord], ub[coord])\n                f_trial = float(func(x_trial)); evals += 1\n                add_archive(x_trial, f_trial)\n                if f_trial < f_best:\n                    x_best = x_trial.copy(); f_best = f_trial\n                    succ[coord] += 1; fail[coord] = 0\n                    uncert[coord] = max(0.2, uncert[coord] * 0.7)\n                    step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                    stagn = 0\n                else:\n                    fail[coord] += 1\n                    uncert[coord] = min(10.0, uncert[coord] * 1.3)\n                    step[coord] = float(max(self.min_step, step[coord] * 0.6))\n                    stagn += 1\n\n            # occasional median-pull: directly try moving toward archive median on a chosen coord\n            if stagn >= (self.stagnation_patience // 2) and evals < self.budget:\n                # pick coord with largest pull\n                pulls = np.abs(med - x_best)\n                c = int(np.argmax(pulls))\n                trial = x_best.copy()\n                # step toward median but clipped\n                move = np.sign(med[c] - x_best[c]) * min(step[c]*1.4, pulls[c])\n                trial[c] = np.clip(trial[c] + move, lb[c], ub[c])\n                f_trial = float(func(trial)); evals += 1\n                add_archive(trial, f_trial)\n                if f_trial < f_best:\n                    x_best = trial.copy(); f_best = f_trial\n                    step[c] = float(min(max_step[c], step[c] * 1.15))\n                    uncert[c] = max(0.2, uncert[c] * 0.6)\n                    stagn = 0\n                else:\n                    step[c] = float(max(self.min_step, step[c] * 0.7))\n                    uncert[c] = min(10.0, uncert[c] * 1.1)\n                    stagn += 1\n\n            # if persistent stagnation, try a larger randomized coordinate jump\n            if stagn >= self.stagnation_patience and evals < self.budget:\n                how_many = min(3, max(1, self.dim // 8))\n                coords = rng.choice(self.dim, size=how_many, replace=False)\n                for c in coords:\n                    if evals >= self.budget: break\n                    mag = (0.4 * step[c] + 0.6 * span[c] * rng.random()) * (1 if rng.random() < 0.5 else -1)\n                    xt = x_best.copy()\n                    xt[c] = np.clip(xt[c] + mag, lb[c], ub[c])\n                    ft = float(func(xt)); evals += 1\n                    add_archive(xt, ft)\n                    if ft < f_best:\n                        x_best = xt.copy(); f_best = ft\n                        step[c] = float(min(max_step[c], step[c] * 1.4))\n                        uncert[c] = max(0.2, uncert[c] * 0.5)\n                        stagn = 0\n                        break\n                # slight global jitter to steps to diversify\n                step = np.minimum(max_step, step * (1.0 + 0.15 * rng.random(self.dim)))\n                uncert *= 0.85\n                if stagn >= self.stagnation_patience:\n                    stagn = 0  # reset after heavy probe\n\n            # gentle recovery: successes reduce uncertainty, failures increase it slightly\n            uncert = np.clip(uncert, 0.05, 50.0)\n            # keep steps in bounds\n            step = np.clip(step, self.min_step, max_step)\n\n        return f_best, x_best", "configspace": "", "generation": 19, "feedback": "The algorithm ProbabilisticMirroredCoordinateDescent scored 0.301 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.47167707149604543}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.24164934298874918}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.40493733187590064}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.11084140053958469}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.12019100215802025}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.10929986396186431}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.04891956514902429}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0429352375032519}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07624807895901853}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9505261734265785}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9831797374506326}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9497947283482862}], "aucs": [0.47167707149604543, 0.24164934298874918, 0.40493733187590064, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.11084140053958469, 0.12019100215802025, 0.10929986396186431, 0.04891956514902429, 0.0429352375032519, 0.07624807895901853, 0.9505261734265785, 0.9831797374506326, 0.9497947283482862]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2611.0, "Edges": 2610.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9992340099578705, "Degree Variance": 2.1470695013481103, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.321848739495799, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3207992581836028, "Depth Entropy": 2.0885837424749467, "Assortativity": 0.0, "Average Eccentricity": 16.022979701263882, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.00038299502106472615, "Average Shortest Path": 10.095341988140362, "mean_complexity": 10.0, "total_complexity": 40.0, "mean_token_count": 569.75, "total_token_count": 2279.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "b6c820cb-e543-4734-8411-e6a5e8a250d7", "fitness": "-inf", "name": "ConsensusMirroredDescent", "description": "Consensus Mirrored Descent \u2014 build a tiny archive of recent successful signed moves to form a lightweight consensus direction, use cheap mirrored (\u00b1s) directional probes to estimate directional derivatives and curvature, blend with per-coordinate adaptive steps and momentum, and occasionally perform L\u00e9vy-like escapes.", "code": "import numpy as np\nfrom collections import deque\n\nclass ConsensusMirroredDescent:\n    def __init__(self, budget=1000, dim=10, init_step=0.12,\n                 success_mult=1.3, failure_mult=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.5,\n                 consensus_prob=0.35, global_prob=0.03,\n                 momentum_gain=0.2, momentum_decay=0.85,\n                 archive_size=8, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.consensus_prob = float(consensus_prob)\n        self.global_prob = float(global_prob)\n        self.m_gain = float(momentum_gain); self.m_decay = float(momentum_decay)\n        self.archive_size = int(archive_size)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        box_scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = max(1e-12, self.max_step_frac * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base, float)\n        momentum = np.zeros(dim)\n        consec_fail = np.zeros(dim, int)\n        # small archive of recent successful signed moves: store (direction vector, improvement)\n        wins = deque(maxlen=self.archive_size)\n\n        while evals < self.budget:\n            # occasional global heavy-tailed escape\n            if rng.random() < self.global_prob and evals + 1 < self.budget:\n                u = rng.random(dim)\n                # Cauchy-like jump\n                jump = 0.12 * box_scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n                # continue loop\n\n            # form consensus direction from wins\n            if len(wins) >= 2 and rng.random() < self.consensus_prob:\n                # weighted average of past successful signed moves (improvement-weighted)\n                vecs = np.vstack([w[0] * w[1] for w in wins])\n                cand_dir = np.sum(vecs, axis=0)\n                nd = np.linalg.norm(cand_dir)\n                if nd > 0:\n                    cand_dir = cand_dir / nd\n                else:\n                    cand_dir = None\n            else:\n                cand_dir = None\n\n            # choose candidate direction: consensus or a single coordinate (most promising)\n            if cand_dir is not None and rng.random() < 0.75:\n                # propose mirrored probe along consensus (two evals if budget allows)\n                s = np.clip(np.mean(steps), min_step, max_step)\n                if evals + 2 > self.budget:\n                    # fall back to single-coordinate cheap probe\n                    cand_dir = None\n                else:\n                    xp = np.clip(x_best + s * cand_dir, lb, ub)\n                    xm = np.clip(x_best - s * cand_dir, lb, ub)\n                    fp = safe_eval(xp); fm = safe_eval(xm)\n                    # estimate directional derivative and curvature\n                    deriv = (fp - fm) / (2.0 * s)\n                    curvature = (fp + fm - 2.0 * f_best) / (s * s)\n                    # if derivative negative, move along -deriv direction (gradient descent)\n                    if deriv < 0 or fp < f_best or fm < f_best:\n                        # choose better side\n                        if fp < fm and fp < f_best:\n                            move = xp; fmove = fp; step_vec = s * cand_dir\n                        elif fm < f_best:\n                            move = xm; fmove = fm; step_vec = -s * cand_dir\n                        else:\n                            # derivative indicates decreasing in + direction\n                            dir_sign = -np.sign(deriv)\n                            move = np.clip(x_best + dir_sign * s * cand_dir, lb, ub)\n                            fmove = safe_eval(move) if evals < self.budget else np.inf\n                            step_vec = dir_sign * s * cand_dir\n                        if fmove < f_best:\n                            # success: accept and register win\n                            improvement = max(1e-12, f_best - fmove)\n                            x_best, f_best = move.copy(), float(fmove)\n                            wins.append((step_vec / (np.linalg.norm(step_vec) + 1e-18), improvement))\n                            # boost per-dim steps in directions of step_vec\n                            influence = np.abs(step_vec)\n                            # increase steps proportionally to influence\n                            steps = np.minimum(max_step, steps * (1.0 + 0.6 * (influence / (np.max(influence)+1e-18))))\n                            momentum = momentum * self.m_decay + (step_vec / (np.linalg.norm(step_vec)+1e-18)) * self.m_gain\n                            consec_fail[:] = 0\n                            continue\n                    # if mirrored probe not helpful, shrink global steps slightly\n                    steps *= self.failure_mult\n                    momentum *= (self.m_decay ** 1.5)\n                    continue\n\n            # pick a single coordinate to probe: score = step * headroom * (1+|momentum|)\n            headroom_pos = (ub - x_best) / (ub - lb + 1e-18)\n            headroom_neg = (x_best - lb) / (ub - lb + 1e-18)\n            score_pos = steps * headroom_pos * (1.0 + np.abs(momentum))\n            score_neg = steps * headroom_neg * (1.0 + np.abs(momentum))\n            # pick coord and sign by top score\n            all_scores = np.maximum(score_pos, score_neg)\n            d = int(np.argmax(all_scores))\n            sign = 1 if score_pos[d] >= score_neg[d] else -1\n            s = np.clip(steps[d], min_step, max_step)\n\n            # if only one eval left, try single-sided probe\n            if evals + 1 >= self.budget:\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + sign * s, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft)\n                break\n\n            # mirrored cheap probe on single coordinate: two evals to estimate derivative\n            xp = x_best.copy(); xp[d] = np.clip(xp[d] + s, lb[d], ub[d])\n            xm = x_best.copy(); xm[d] = np.clip(xm[d] - s, lb[d], ub[d])\n            fp = safe_eval(xp); fm = safe_eval(xm)\n            deriv = (fp - fm) / (2.0 * s)\n            curvature = (fp + fm - 2.0 * f_best) / (s * s)\n\n            moved = False\n            # if either side improves accept that side and adapt step\n            if fp < f_best or fm < f_best:\n                if fp < fm:\n                    xt, ft, chosen_sign = xp, fp, 1\n                else:\n                    xt, ft, chosen_sign = xm, fm, -1\n                x_best, f_best = xt.copy(), float(ft)\n                # update step for that dimension\n                steps[d] = min(max_step, steps[d] * self.success_mult)\n                # momentum update targeted to dim\n                momentum[d] = momentum[d] * self.m_decay + chosen_sign * self.m_gain\n                # register win vector\n                sv = np.zeros(dim); sv[d] = chosen_sign * s\n                wins.append((sv / (np.linalg.norm(sv)+1e-18), max(1e-12, (abs(f_best - ft)))))\n                consec_fail[d] = 0\n                moved = True\n            else:\n                # no improvement: attempt extrapolation if curvature negative (possible valley)\n                if curvature < -1e-6:\n                    # extrapolate along direction of negative curvature\n                    ext = x_best.copy(); ext[d] = np.clip(ext[d] - np.sign(deriv) * 2.0 * s, lb[d], ub[d])\n                    fext = safe_eval(ext)\n                    if fext < f_best:\n                        x_best, f_best = ext.copy(), float(fext)\n                        steps[d] = min(max_step, steps[d] * (self.success_mult * 0.9))\n                        momentum[d] = momentum[d] * self.m_decay + (-np.sign(deriv)) * self.m_gain\n                        moved = True\n                if not moved:\n                    steps[d] = max(min_step, steps[d] * self.failure_mult)\n                    momentum[d] *= (self.m_decay ** 1.2)\n                    consec_fail[d] += 1\n\n            # stronger shrink after repeated fails on a coordinate\n            if consec_fail[d] >= 5:\n                steps[d] = max(min_step, steps[d] * (self.failure_mult ** 1.7))\n                consec_fail[d] = 0\n\n            # gentle global normalization to keep steps in range\n            steps = np.clip(steps, min_step, max_step)\n\n        # final light polish: small symmetric per-dim probes while budget remains\n        fin = 0.04 * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 19, "feedback": "In the code, line 50, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: wins = deque(maxlen=self.archive_size)", "error": "In the code, line 50, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: wins = deque(maxlen=self.archive_size)", "parent_ids": ["4a569da3-dff0-437a-a1e1-9e9baee72739"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2324.0, "Edges": 2323.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9991394148020654, "Degree Variance": 2.212563803282962, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.342056074766354, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3291621441413224, "Depth Entropy": 2.1881616906713055, "Assortativity": 0.0, "Average Eccentricity": 19.67814113597246, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0004302925989672978, "Average Shortest Path": 10.326299231734144, "mean_complexity": 15.333333333333334, "total_complexity": 46.0, "mean_token_count": 659.6666666666666, "total_token_count": 1979.0, "mean_parameter_count": 5.666666666666667, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "86d13d75-4636-4c2f-9196-d0f90890a1f4", "fitness": 0.6450305689642717, "name": "ParabolicAdaptiveMirroredDescent", "description": "Parabolic Adaptive Mirrored Descent (PAMD) \u2014 mirrored 3-point probes per coordinate, fit a parabola to obtain an analytic 1-D minimum, adapt steps multiplicatively, and use rare Cauchy jumps plus light polish for robust escapes and efficient exploitation.", "code": "import numpy as np\n\nclass ParabolicAdaptiveMirroredDescent:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 success_mult=1.4, failure_mult=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, max_extrap=2.0, polish_frac=0.03,\n                 seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.succ_mult = float(success_mult); self.fail_mult = float(failure_mult)\n        self.min_pf = float(min_step_frac); self.max_pf = float(max_step_frac)\n        self.global_prob = float(global_prob)\n        self.max_extrap = float(max_extrap)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * scale)\n        min_step = max(1e-15, self.min_pf * scale)\n        max_step = max(1e-12, self.max_pf * scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        steps = np.full(dim, base, float)\n        trials = np.zeros(dim, int); succ = np.zeros(dim, int)\n        recent = []  # store small history of (dx, df)\n\n        while evals < self.budget:\n            if rng.random() < self.global_prob:\n                u = rng.random(dim)\n                jump = 0.12 * scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f:\n                    x, f = xg.copy(), fg\n                continue\n\n            # choose coord: prefer larger steps and less-tried dims\n            scores = steps / (1.0 + 0.2 * trials)\n            d = int(np.argmax(scores + 1e-12 * rng.random(dim)))\n            s = float(np.clip(steps[d], min_step, max_step))\n\n            # mirrored probe if possible\n            if evals <= self.budget - 2:\n                xp = x.copy(); xm = x.copy()\n                xp[d] = np.clip(xp[d] + s, lb[d], ub[d])\n                xm[d] = np.clip(xm[d] - s, lb[d], ub[d])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[d] += 2\n\n                # immediate improvement\n                if fp < f or fm < f:\n                    if fp < fm:\n                        dx = xp - x; df = fp - f; x, f = xp.copy(), float(fp)\n                    else:\n                        dx = xm - x; df = fm - f; x, f = xm.copy(), float(fm)\n                    recent.append((dx, df))\n                    if len(recent) > 20: recent.pop(0)\n                    succ[d] += 1\n                    steps[d] = min(max_step, steps[d] * self.succ_mult)\n\n                    # try analytic parabola around new center if budget allows and s not tiny\n                    if evals < self.budget and s > 1e-12 * scale:\n                        # compute using last triple: values at -s,0,s are fm, f (new center), fp\n                        f0 = f\n                        denom = (fp + fm - 2.0 * f0)\n                        if abs(denom) > 1e-16:\n                            x_star = -s * (fp - fm) / (2.0 * denom)  # offset from center\n                            x_star = np.clip(x_star, -self.max_extrap * s, self.max_extrap * s)\n                            if abs(x_star) > 1e-12:\n                                xt = x.copy(); xt[d] = np.clip(xt[d] + x_star, lb[d], ub[d])\n                                ft = safe_eval(xt)\n                                trials[d] += 1\n                                if ft < f:\n                                    dx = xt - x; df = ft - f; x, f = xt.copy(), float(ft)\n                                    recent.append((dx, df)); succ[d] += 1\n                                    steps[d] = min(max_step, steps[d] * self.succ_mult)\n                else:\n                    # no immediate improvement => try parabola minimum if convex and well-conditioned\n                    denom = (fp + fm - 2.0 * f)\n                    if denom > 1e-14:\n                        x_star = -s * (fp - fm) / (2.0 * denom)\n                        x_star = np.clip(x_star, -self.max_extrap * s, self.max_extrap * s)\n                        if abs(x_star) > 1e-12 and evals < self.budget:\n                            xt = x.copy(); xt[d] = np.clip(xt[d] + x_star, lb[d], ub[d])\n                            ft = safe_eval(xt); trials[d] += 1\n                            if ft < f:\n                                dx = xt - x; df = ft - f; x, f = xt.copy(), float(ft)\n                                recent.append((dx, df)); succ[d] += 1\n                                steps[d] = min(max_step, steps[d] * self.succ_mult)\n                                continue\n                    # otherwise shrink step\n                    steps[d] = max(min_step, steps[d] * self.fail_mult)\n\n            else:\n                # last-eval strategies: one-sided probe\n                if rng.random() < 0.5:\n                    xt = x.copy(); xt[d] = np.clip(xt[d] + s, lb[d], ub[d])\n                else:\n                    xt = x.copy(); xt[d] = np.clip(xt[d] - s, lb[d], ub[d])\n                ft = safe_eval(xt); trials[d] += 1\n                if ft < f:\n                    dx = xt - x; df = ft - f; x, f = xt.copy(), float(ft)\n                    recent.append((dx, df)); succ[d] += 1\n                    steps[d] = min(max_step, steps[d] * self.succ_mult)\n                else:\n                    steps[d] = max(min_step, steps[d] * self.fail_mult)\n\n            # mild rescue: boost stale dims with no success\n            if trials[d] > 60 and succ[d] == 0:\n                steps[d] = min(max_step, steps[d] * 1.6)\n                trials[d] = int(trials[d] * 0.5)\n\n        # final cheap polish\n        fin = self.polish_frac * scale\n        while evals < self.budget and fin > 1e-12 * scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 19, "feedback": "The algorithm ParabolicAdaptiveMirroredDescent scored 0.645 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["82379331-5855-488c-a7d5-53d7afbbd2bb"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.993174454234379}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9880875479795753}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9899936385184729}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9596134735662374}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9615549564233574}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9610009423334644}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.16780211242774679}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.19735977409459637}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.13852482355385343}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10376708723063988}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.10374076118427833}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.12086380279468589}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9967053825455996}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9964844824179453}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9967852951592443}], "aucs": [0.993174454234379, 0.9880875479795753, 0.9899936385184729, 0.9596134735662374, 0.9615549564233574, 0.9610009423334644, 0.16780211242774679, 0.19735977409459637, 0.13852482355385343, 0.10376708723063988, 0.10374076118427833, 0.12086380279468589, 0.9967053825455996, 0.9964844824179453, 0.9967852951592443]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1998.0, "Edges": 1997.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.998998998998999, "Degree Variance": 2.004003002001, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.695509309967141, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3316272645267517, "Depth Entropy": 2.2480439037399296, "Assortativity": 8.429462578756122e-09, "Average Eccentricity": 16.87987987987988, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0005005005005005005, "Average Shortest Path": 10.534182655364429, "mean_complexity": 12.666666666666666, "total_complexity": 38.0, "mean_token_count": 562.3333333333334, "total_token_count": 1687.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "c85282eb-f8db-492d-b392-cf1c68d7a151", "fitness": 0.6310716658822826, "name": "DirectionalThompsonMirrored", "description": "Directional Thompson with Mirrored Probes and Dispersion Weighting \u2014 per-coordinate signed-Beta Thompson sampling augmented by occasional mirrored (two-sided) probes to detect curvature, an entropy-like dispersion weight to prioritize informative coordinates, tiny best-archive median pulls and sporadic Cauchy escapes for robust cheap 1-D exploitation and low-cost informed extrapolations.", "code": "import numpy as np\n\nclass DirectionalThompsonMirrored:\n    def __init__(self, budget, dim, init_step=0.12, success_mult=1.22,\n                 failure_mult=0.75, global_prob=0.035, median_period=18,\n                 mirror_prob=0.12, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.init_step_frac = float(init_step)\n        self.succ = float(success_mult); self.fail = float(failure_mult)\n        self.global_prob = float(global_prob); self.median_period = int(median_period)\n        self.mirror_prob = float(mirror_prob)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0); ub = np.full(self.dim, 5.0)\n\n        box_len = np.linalg.norm(ub - lb)\n        base = max(1e-12, self.init_step_frac * box_len)\n        min_step = max(1e-15, 1e-9 * box_len); max_step = max(1e-12, 1.5 * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        rng = self.rng; d = self.dim\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        x_best = x.copy(); f_best = float(f)\n        # small best-archive (keep up to 3)\n        archive = [(x_best.copy(), f_best)]\n\n        steps = np.full(d, base, float)\n        a_pos = np.ones(d); b_pos = np.ones(d)\n        a_neg = np.ones(d); b_neg = np.ones(d)\n        trials = np.zeros(d, int)\n        last_sign = np.zeros(d, int)\n        consec_fail = np.zeros(d, int)\n        disp = np.zeros(d, float)   # EWMA of absolute improvements (dispersion)\n        it = 0\n\n        while evals < self.budget:\n            # occasional heavy-tailed global escape\n            if rng.random() < self.global_prob and evals < self.budget:\n                c = rng.standard_cauchy(d)\n                jump = 0.09 * box_len * c\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                    x_best, f_best = xg.copy(), float(fg)\n\n            # periodic cheap median pull from archive\n            if (it % max(1, self.median_period) == 0) and len(archive) >= 2:\n                pts = np.stack([p for p, _ in archive[:3]])\n                xm = np.median(pts, axis=0)\n                if not np.allclose(xm, x_best):\n                    fm = safe_eval(xm)\n                    if fm < f_best:\n                        archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                        x_best, f_best = xm.copy(), float(fm)\n\n            # Thompson sampling per sign\n            ppos = rng.beta(a_pos, b_pos)\n            pneg = rng.beta(a_neg, b_neg)\n            head_ub = (ub - x_best) / (ub - lb + 1e-18)\n            head_lb = (x_best - lb) / (ub - lb + 1e-18)\n            eff_pos = ppos * (0.6 + 0.4 * np.clip(head_ub, 0, 1))\n            eff_neg = pneg * (0.6 + 0.4 * np.clip(head_lb, 0, 1))\n\n            # dispersion weighting: favor dims with recent informative moves but also under-tried\n            weight = 0.5 * (1.0 + disp / (1e-12 + disp.max() if disp.max() > 0 else 1.0)) + 0.5 / (1.0 + np.sqrt(trials))\n            mom = 0.11 * (last_sign > 0); momn = 0.11 * (last_sign < 0)\n            score_pos = eff_pos * steps * weight + mom\n            score_neg = eff_neg * steps * weight + momn\n\n            choose_pos = score_pos >= score_neg\n            score = np.where(choose_pos, score_pos, score_neg)\n            idx = int(np.argmax(score))\n            sign = 1 if choose_pos[idx] else -1\n            step = float(np.clip(steps[idx], min_step, max_step))\n\n            # mirrored probe occasionally to estimate curvature/extrapolate\n            do_mirror = (rng.random() < self.mirror_prob) and (evals + 2 <= self.budget)\n            if do_mirror:\n                xp = x_best.copy(); xm = x_best.copy()\n                xp[idx] = np.clip(xp[idx] + step, lb[idx], ub[idx])\n                xm[idx] = np.clip(xm[idx] - step, lb[idx], ub[idx])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[idx] += 2\n                # parabola fit: f(0)=f_best, f(+)=fp, f(-)=fm -> curvature a = (fp + fm - 2 f0)/(2 step^2)\n                a = (fp + fm - 2.0 * f_best) / (2.0 * step * step + 1e-20)\n                # if negative curvature (valley) try extrapolation to 2*step\n                if a < 0:\n                    x2 = x_best.copy(); x2[idx] = np.clip(x2[idx] + 2.0 * (np.sign(fp - fm) or sign) * step, lb[idx], ub[idx])\n                    f2 = safe_eval(x2)\n                    trials[idx] += 1\n                    # choose best among xp,xm,x2\n                    cand = [(fp, xp), (fm, xm), (f2, x2)]\n                    cand.sort(key=lambda z: z[0])\n                    bestf, bestx = cand[0]\n                    if bestf < f_best:\n                        archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                        # update priors by sign of accepted move\n                        s = 1 if np.allclose(bestx[idx], xp[idx]) else -1 if np.allclose(bestx[idx], xm[idx]) else np.sign(bestx[idx] - x_best[idx])\n                        if s > 0: a_pos[idx] += 1\n                        else: a_neg[idx] += 1\n                        x_best, f_best = bestx.copy(), float(bestf)\n                        steps[idx] = min(max_step, steps[idx] * self.succ)\n                        consec_fail[idx] = 0; last_sign[idx] = int(s)\n                        disp[idx] = 0.9 * disp[idx] + 0.1 * max(0.0, f - f_best)\n                        it += 1; continue\n                # otherwise pick the better side if it improved\n                if fp < f_best or fm < f_best:\n                    if fp < fm:\n                        chosen_x, chosen_f, sgn = xp, fp, 1\n                    else:\n                        chosen_x, chosen_f, sgn = xm, fm, -1\n                    archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                    if chosen_f < f_best:\n                        if sgn > 0: a_pos[idx] += 1\n                        else: a_neg[idx] += 1\n                        x_best, f_best = chosen_x.copy(), float(chosen_f)\n                        steps[idx] = min(max_step, steps[idx] * self.succ)\n                        consec_fail[idx] = 0; last_sign[idx] = int(sgn)\n                        disp[idx] = 0.9 * disp[idx] + 0.1 * max(0.0, f - f_best)\n                    else:\n                        # both worse -> shrink\n                        steps[idx] = max(min_step, steps[idx] * self.fail)\n                        consec_fail[idx] += 1; last_sign[idx] = -sign\n                    it += 1; continue\n\n            # single-eval coordinate probe (cheap)\n            xt = x_best.copy(); xt[idx] = np.clip(xt[idx] + sign * step, lb[idx], ub[idx])\n            ft = safe_eval(xt)\n            trials[idx] += 1\n\n            # update Beta priors\n            if sign > 0:\n                if ft < f_best: a_pos[idx] += 1.0\n                else: b_pos[idx] += 1.0\n            else:\n                if ft < f_best: a_neg[idx] += 1.0\n                else: b_neg[idx] += 1.0\n\n            if ft < f_best:\n                archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                x_best, f_best = xt.copy(), float(ft)\n                steps[idx] = min(max_step, steps[idx] * self.succ)\n                consec_fail[idx] = 0; last_sign[idx] = int(sign)\n                disp[idx] = 0.9 * disp[idx] + 0.1 * max(0.0, f - f_best)\n            else:\n                steps[idx] = max(min_step, steps[idx] * self.fail)\n                consec_fail[idx] += 1; last_sign[idx] = -sign\n                # small penalty to failing side\n                if sign > 0: b_pos[idx] += 0.3\n                else: b_neg[idx] += 0.3\n\n            # corrective actions\n            if consec_fail[idx] >= 6:\n                steps[idx] = max(min_step, steps[idx] * (self.fail ** 1.5))\n                consec_fail[idx] = 0\n                if sign > 0: b_pos[idx] += 0.6\n                else: b_neg[idx] += 0.6\n\n            # mild shrink of Beta mass to avoid overconfidence\n            if trials[idx] > 45:\n                for A, B in ((a_pos, b_pos), (a_neg, b_neg)):\n                    A[idx] = 1.0 + (A[idx] - 1.0) * 0.88\n                    B[idx] = 1.0 + (B[idx] - 1.0) * 0.88\n                trials[idx] = int(trials[idx] * 0.6)\n\n            it += 1\n\n        # final cheap symmetric polish\n        final = max(1e-12, 0.045 * box_len)\n        while evals < self.budget and final > 1e-12:\n            improved = False\n            for i in range(d):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[i] = np.clip(xt[i] + final, lb[i], ub[i])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[i] = np.clip(xt[i] - final, lb[i], ub[i])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True\n            if not improved: final *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 19, "feedback": "The algorithm DirectionalThompsonMirrored scored 0.631 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7b8ff9a2-76db-4ccd-9d7c-5ec4f4578d39"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9817100594290297}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9824041461632416}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9822906162215312}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9641867503364632}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9642629019143134}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9628072164115278}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.12923316264721374}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.09934606635938348}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.12152977243496121}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.07924510952057173}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.13871310742520981}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.06950434346968482}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9969335268293976}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9965971598267563}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9973110492449526}], "aucs": [0.9817100594290297, 0.9824041461632416, 0.9822906162215312, 0.9641867503364632, 0.9642629019143134, 0.9628072164115278, 0.12923316264721374, 0.09934606635938348, 0.12152977243496121, 0.07924510952057173, 0.13871310742520981, 0.06950434346968482, 0.9969335268293976, 0.9965971598267563, 0.9973110492449526]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2674.0, "Edges": 2673.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9992520568436798, "Degree Variance": 2.2318618190402724, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.296623794212218, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3101739873105644, "Depth Entropy": 2.09760753166702, "Assortativity": 9.568535938035457e-09, "Average Eccentricity": 17.012715033657443, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00037397157816005983, "Average Shortest Path": 10.074480084369554, "mean_complexity": 16.666666666666668, "total_complexity": 50.0, "mean_token_count": 776.3333333333334, "total_token_count": 2329.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "4eb203d7-218c-4260-a12b-28839b4ee356", "fitness": 0.6231586307505896, "name": "MirroredAdaptiveQuadraticPlus", "description": "Mirrored Adaptive Quadratic Plus \u2014 cheap mirrored 1-D probes with EWMA curvature, adaptive multiplicative steps, tiny median-archive pulls and occasional Cauchy/L\u00e9vy global jumps for robust local exploitation plus directed escapes.", "code": "import numpy as np\n\nclass MirroredAdaptiveQuadraticPlus:\n    def __init__(self, budget=1000, dim=10,\n                 init_step=0.08, success=1.35, failure=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, extrap=1.7, polish_frac=0.03,\n                 arch_size=5, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.success = float(success); self.failure = float(failure)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob); self.extrap = float(extrap)\n        self.polish_frac = float(polish_frac)\n        self.arch_size = int(arch_size)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        # bounds (support func.bounds.lb/ub or default [-5,5])\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, float(lb))\n            if ub.shape == (): ub = np.full(d, float(ub))\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n\n        span = np.maximum(1e-12, ub - lb)\n        box_scale = max(1e-12, np.linalg.norm(span))\n        base = max(1e-12, self.init_step * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = max(1e-12, self.max_step_frac * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        # init\n        x = rng.uniform(lb, ub)\n        fx = safe_eval(x)\n        steps = np.full(d, base)\n        trials = np.zeros(d, int); succ = np.zeros(d, int)\n        curv = np.zeros(d, float); alpha = 0.15\n        # small archive of best points (for median pulls)\n        arch_x = [x.copy()]; arch_f = [fx]\n\n        while evals < self.budget:\n            # global heavy-tailed escape\n            if rng.random() < self.global_prob:\n                u = rng.random(d)\n                jump = 0.18 * box_scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < fx:\n                    x, fx = xg.copy(), fg\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                continue\n\n            # occasional median pull: uses archive median to nudge current best (cheap: 1 eval)\n            if len(arch_x) >= 3 and rng.random() < 0.02 and evals < self.budget:\n                med = np.median(np.vstack(arch_x), axis=0)\n                # pull only larger-step dims\n                idxs = np.argsort(-steps)[:max(1, d//6)]\n                cand = x.copy(); cand[idxs] = np.clip(x[idxs] + 0.5*(med[idxs]-x[idxs]), lb[idxs], ub[idxs])\n                fc = safe_eval(cand)\n                if fc < fx:\n                    x, fx = cand.copy(), fc\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                continue\n\n            # coordinate selection: prefer large steps and little success\n            score = steps * (1.0 + 0.8*(1.0/(1+trials))) * (1.0 + 0.2*rng.random(d))\n            coord = int(np.argmax(score))\n            s = float(np.clip(steps[coord], min_step, max_step))\n\n            # mirrored probe if we have 2+ evals left\n            if evals <= self.budget - 2:\n                xp = x.copy(); xm = x.copy()\n                xp[coord] = np.clip(xp[coord] + s, lb[coord], ub[coord])\n                xm[coord] = np.clip(xm[coord] - s, lb[coord], ub[coord])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[coord] += 2\n\n                # accept best side if improvement\n                if fp < fx or fm < fx:\n                    if fp < fm:\n                        x_old = x.copy(); x, fx = xp.copy(), fp\n                    else:\n                        x_old = x.copy(); x, fx = xm.copy(), fm\n                    succ[coord] += 1\n                    steps[coord] = min(max_step, steps[coord] * self.success)\n                    # opportunistic extrapolation in same direction\n                    if evals < self.budget:\n                        sign = np.sign(x[coord] - x_old[coord])\n                        if sign == 0: sign = 1.0\n                        ext = x.copy()\n                        ext[coord] = np.clip(ext[coord] + sign * s * self.extrap, lb[coord], ub[coord])\n                        fext = safe_eval(ext)\n                        trials[coord] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext\n                    # update archive\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                else:\n                    # curvature estimate around current best (use fx as center proxy)\n                    est = (fp + fm - 2.0 * fx) / (s*s + 1e-24)\n                    curv[coord] = (1-alpha)*curv[coord] + alpha*est\n                    # if significant negative curvature -> try an extrapolation (valley)\n                    if est < -1e-12 and evals < self.budget:\n                        ext = x.copy()\n                        # direction: if both sides are higher, move toward lower slope side\n                        direction = -np.sign(est)  # simple heuristic\n                        ext[coord] = np.clip(ext[coord] + direction * s * self.extrap, lb[coord], ub[coord])\n                        fext = safe_eval(ext); trials[coord] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext\n                            succ[coord] += 1\n                            steps[coord] = min(max_step, steps[coord] * self.success)\n                            arch_x.append(x.copy()); arch_f.append(fx)\n                            if len(arch_x) > self.arch_size:\n                                i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                        else:\n                            steps[coord] = max(min_step, steps[coord] * self.failure)\n                    else:\n                        steps[coord] = max(min_step, steps[coord] * self.failure)\n            else:\n                # last-eval: try one-sided test\n                if rng.random() < 0.5:\n                    xt = x.copy(); xt[coord] = np.clip(xt[coord] + s, lb[coord], ub[coord])\n                else:\n                    xt = x.copy(); xt[coord] = np.clip(xt[coord] - s, lb[coord], ub[coord])\n                ft = safe_eval(xt); trials[coord] += 1\n                if ft < fx:\n                    x, fx = xt.copy(), ft; succ[coord] += 1\n                    steps[coord] = min(max_step, steps[coord] * self.success)\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                else:\n                    steps[coord] = max(min_step, steps[coord] * self.failure)\n\n            # mild rejuvenation for stale coords\n            if trials[coord] > 70 and succ[coord] == 0:\n                steps[coord] = min(max_step, steps[coord] * 1.5)\n                trials[coord] = int(trials[coord] * 0.6)\n\n        # final polish: small symmetric per-dim probes while budget remains\n        fin = self.polish_frac * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for i in range(d):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[i] = np.clip(xt[i] + fin, lb[i], ub[i])\n                ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[i] = np.clip(xt[i] - fin, lb[i], ub[i])\n                ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 19, "feedback": "The algorithm MirroredAdaptiveQuadraticPlus scored 0.623 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["82379331-5855-488c-a7d5-53d7afbbd2bb"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9730228465847346}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9733743513488324}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9721720962481442}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9427732912814026}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9413631383671337}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9410130292164504}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.08806632201960252}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.07913569616061544}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.10894883013185552}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.12881206383197186}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.1210732361922261}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.08482576661628227}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.997985678402106}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9972153627762061}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9975977520812815}], "aucs": [0.9730228465847346, 0.9733743513488324, 0.9721720962481442, 0.9427732912814026, 0.9413631383671337, 0.9410130292164504, 0.08806632201960252, 0.07913569616061544, 0.10894883013185552, 0.12881206383197186, 0.1210732361922261, 0.08482576661628227, 0.997985678402106, 0.9972153627762061, 0.9975977520812815]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2290.0, "Edges": 2289.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.999126637554585, "Degree Variance": 2.0279468354913135, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.476190476190476, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3303423391526032, "Depth Entropy": 2.1205276807162217, "Assortativity": 0.0, "Average Eccentricity": 16.17991266375546, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.0004366812227074236, "Average Shortest Path": 10.295212150001621, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 652.6666666666666, "total_token_count": 1958.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "9a3363a3-6b9b-4121-b1b3-03c46b139c0d", "fitness": 0.6197662342861858, "name": "SQNT", "description": "Sparse Quasi-Newton Trust (SQNT) \u2014 build tiny least-squares gradient estimates from cheap mirrored 1-D probes to propose informed multi\u2011dimensional descent steps inside a coordinate-trust loop, with adaptive per-dim steps, momentum and occasional heavy\u2011tailed escapes.", "code": "import numpy as np\n\nclass SQNT:\n    def __init__(self, budget=1000, dim=10, seed=None,\n                 init_step=0.06, succ_mul=1.32, fail_mul=0.62,\n                 global_prob=0.03, model_prob=0.12, hist=16):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.succ_mul = float(succ_mul); self.fail_mul = float(fail_mul)\n        self.global_prob = float(global_prob); self.model_prob = float(model_prob)\n        self.hist_max = int(hist)\n\n    def __call__(self, func):\n        dim, rng = self.dim, self.rng\n        lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step * scale)\n        min_step = 1e-12 * scale; max_step = 1.2 * scale\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x = rng.uniform(lb, ub); fx = safe_eval(x)\n        steps = np.full(dim, base); mom = np.zeros(dim)\n        trials = np.zeros(dim, int)\n        DX, DF = [], []\n\n        while evals < self.budget:\n            # heavy-tailed global escape\n            if rng.random() < self.global_prob:\n                u = rng.random(dim) - 0.5\n                jump = 0.12 * scale * np.tan(np.pi * u)\n                xg = np.clip(x + jump, lb, ub); fg = safe_eval(xg)\n                if fg < fx: x, fx = xg.copy(), fg\n                continue\n\n            # model-driven multi-dim quasi-newton (least-squares gradient)\n            if rng.random() < self.model_prob and len(DX) >= min(4, dim) and evals <= self.budget - 2:\n                A = np.vstack(DX)  # rows are small displacement vectors\n                b = np.asarray(DF, float)\n                try:\n                    g, *_ = np.linalg.lstsq(A, b, rcond=None)\n                except Exception:\n                    g = None\n                if g is not None:\n                    gn = np.linalg.norm(g)\n                    if gn > 1e-12:\n                        ddir = -g / gn\n                        step_len = np.median(steps) * (1.0 + 0.5 * np.linalg.norm(mom))\n                        xp = np.clip(x + step_len * ddir, lb, ub)\n                        fp = safe_eval(xp)\n                        # mirrored check (cheap)\n                        xm = np.clip(x - 0.6 * step_len * ddir, lb, ub)\n                        fm = safe_eval(xm)\n                        # store observations\n                        DX.append(xp - x); DF.append(fp - fx)\n                        DX.append(xm - x); DF.append(fm - fx)\n                        if len(DX) > self.hist_max:\n                            DX = DX[-self.hist_max:]; DF = DF[-self.hist_max:]\n                        if fp < fx or fm < fx:\n                            if fp < fm:\n                                x, fx, side = xp.copy(), fp, +1.0\n                            else:\n                                x, fx, side = xm.copy(), fm, -1.0\n                            # boost steps where direction has mass\n                            w = np.abs(ddir)\n                            steps = np.minimum(max_step, steps * (1.0 + 0.6 * w * (1 + abs(side))))\n                            mom = 0.65 * mom + 0.35 * side * ddir\n                            continue\n                        else:\n                            steps *= self.fail_mul\n                            mom *= 0.9\n                            continue\n\n            # coordinate probe\n            score = steps * (1.0 + 1.0/(1.0+trials)) * (1.0 + 0.4*np.abs(mom))\n            probs = score / (score.sum() + 1e-24)\n            d = rng.choice(dim, p=probs)\n\n            s = float(np.clip(steps[d], min_step, max_step))\n            anchor = x.copy()\n            anchor[d] = np.clip(anchor[d] + 0.08 * s * mom[d], lb[d], ub[d])\n\n            # mirrored two-point probe if budget allows\n            if evals <= self.budget - 2:\n                xp = anchor.copy(); xm = anchor.copy()\n                xp[d] = np.clip(xp[d] + s, lb[d], ub[d])\n                xm[d] = np.clip(xm[d] - s, lb[d], ub[d])\n                fp = safe_eval(xp); fm = safe_eval(xm); trials[d] += 2\n                # record small-direction observations\n                DX.append((xp - xm) * 0.5); DF.append(fp - fm)\n                if len(DX) > self.hist_max:\n                    DX = DX[-self.hist_max:]; DF = DF[-self.hist_max:]\n                if fp < fx or fm < fx:\n                    if fp < fm:\n                        x, fx, side = xp.copy(), fp, +1.0\n                    else:\n                        x, fx, side = xm.copy(), fm, -1.0\n                    steps[d] = min(max_step, steps[d] * self.succ_mul)\n                    mom[d] = 0.7*mom[d] + 0.3*side\n                    # small extrapolation opportunistic\n                    if evals < self.budget:\n                        ext = x.copy(); ext[d] = np.clip(ext[d] + side * s * 1.25, lb[d], ub[d])\n                        fext = safe_eval(ext); trials[d] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext\n                            steps[d] = min(max_step, steps[d] * self.succ_mul)\n                else:\n                    # curvature heuristic\n                    curv = (fp + fm - 2.0*fx) / (s*s + 1e-24)\n                    if curv < -1e-12 and evals < self.budget:\n                        side = -np.sign(fp - fm) if fp != fm else (1.0 if mom[d] >= 0 else -1.0)\n                        ext = anchor.copy(); ext[d] = np.clip(ext[d] + side * s * 1.6, lb[d], ub[d])\n                        fext = safe_eval(ext); trials[d] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext\n                            steps[d] = min(max_step, steps[d] * self.succ_mul)\n                            mom[d] = 0.6*mom[d] + 0.4*side\n                        else:\n                            steps[d] = max(min_step, steps[d] * self.fail_mul)\n                            mom[d] *= 0.9\n                    else:\n                        steps[d] = max(min_step, steps[d] * self.fail_mul)\n                        mom[d] *= 0.92\n            else:\n                # last single eval\n                side = np.sign(mom[d]) if mom[d] != 0 else (1.0 if rng.random() < 0.5 else -1.0)\n                xt = anchor.copy(); xt[d] = np.clip(xt[d] + side*s, lb[d], ub[d])\n                ft = safe_eval(xt); trials[d] += 1\n                if ft < fx:\n                    x, fx = xt.copy(), ft\n                    steps[d] = min(max_step, steps[d] * self.succ_mul)\n                    mom[d] = 0.7*mom[d] + 0.3*side\n                else:\n                    steps[d] = max(min_step, steps[d] * self.fail_mul)\n                    mom[d] *= 0.9\n\n        # final polish\n        eps = 0.03 * scale\n        while evals < self.budget and eps > 1e-12 * scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + eps, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fx: x, fx, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] - eps, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < fx: x, fx, improved = xt.copy(), ft, True\n            if not improved: eps *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 19, "feedback": "The algorithm SQNT scored 0.620 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["54126816-8feb-409f-9b9a-dab378a4b31f"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9860926566683917}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9853195506944908}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9860365595830516}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9715705102265406}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9714125348611622}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9739165353667038}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.09941440351873077}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10406434595168756}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.08202114162956875}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0675621193390421}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.022288760681207}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0618351963443583}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9935635248846861}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9962257803873267}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9951698941558407}], "aucs": [0.9860926566683917, 0.9853195506944908, 0.9860365595830516, 0.9715705102265406, 0.9714125348611622, 0.9739165353667038, 0.09941440351873077, 0.10406434595168756, 0.08202114162956875, 0.0675621193390421, 0.022288760681207, 0.0618351963443583, 0.9935635248846861, 0.9962257803873267, 0.9951698941558407]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2195.0, "Edges": 2194.0, "Max Degree": 23.0, "Min Degree": 1.0, "Mean Degree": 1.9990888382687928, "Degree Variance": 1.9826870968913612, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.830875122910522, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3302722213086577, "Depth Entropy": 2.18282921437894, "Assortativity": 0.0, "Average Eccentricity": 18.220501138952162, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00045558086560364467, "Average Shortest Path": 10.650144211901168, "mean_complexity": 13.333333333333334, "total_complexity": 40.0, "mean_token_count": 621.3333333333334, "total_token_count": 1864.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "7c261e7a-6216-4d7e-8792-9c697206d406", "fitness": 0.6241677810353702, "name": "ThompsonCoordinateBandit", "description": "Directional Thompson Bandit V2 \u2014 per-coordinate signed-Beta Thompson sampling with reward-scaled updates, uncertainty-aware dimension scoring, lightweight momentum, occasional Cauchy escapes and tiny archive-driven median pulls for robust cheap 1-D exploitation.", "code": "import numpy as np\n\nclass ThompsonCoordinateBandit:\n    def __init__(self, budget, dim, init_step=0.12,\n                 success_mult=1.25, failure_mult=0.7,\n                 global_prob=0.035, median_period=20, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_step_frac = float(init_step)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.global_prob = float(global_prob)\n        self.median_period = int(median_period)\n        self.rng = np.random.default_rng(seed)\n\n    def _bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        dim = self.dim\n        rng = self.rng\n        box_len = np.linalg.norm(ub - lb)\n        base = max(1e-12, self.init_step_frac * box_len)\n        min_step = max(1e-15, 1e-9 * box_len)\n        max_step = max(1e-12, 1.5 * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        x = rng.uniform(lb, ub)\n        f = safe_eval(x)\n        x_best, f_best = x.copy(), float(f)\n        x_second, f_second = x_best.copy(), f_best\n\n        steps = np.full(dim, base, float)\n        a_pos = np.ones(dim); b_pos = np.ones(dim)\n        a_neg = np.ones(dim); b_neg = np.ones(dim)\n        trials = np.zeros(dim, int)\n        last_dir = np.zeros(dim, int)\n        consec_fail = np.zeros(dim, int)\n\n        it = 0\n        while evals < self.budget:\n            # occasional heavy-tail escape\n            if rng.random() < self.global_prob and evals < self.budget:\n                jump = 0.12 * box_len * rng.standard_cauchy(dim)\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_second, f_second = x_best.copy(), f_best\n                    x_best, f_best = xg.copy(), fg\n\n            # cheap median pull\n            if (it % max(1, self.median_period) == 0) and (not np.allclose(x_second, x_best)):\n                xm = 0.5 * (x_best + x_second)\n                fm = safe_eval(xm)\n                if fm < f_best:\n                    x_second, f_second = x_best.copy(), f_best\n                    x_best, f_best = xm.copy(), fm\n\n            # Thompson draws and scoring (favor uncertainty and headroom)\n            ppos = rng.beta(a_pos, b_pos); pneg = rng.beta(a_neg, b_neg)\n            upos = ppos * (1 - ppos); uneg = pneg * (1 - pneg)\n            head_ub = (ub - x_best) / (ub - lb + 1e-18)\n            head_lb = (x_best - lb) / (ub - lb + 1e-18)\n            eff_pos = ppos * (0.6 + 0.4 * np.clip(head_ub, 0, 1)) + 0.8 * upos\n            eff_neg = pneg * (0.6 + 0.4 * np.clip(head_lb, 0, 1)) + 0.8 * uneg\n            mom = 0.12 * (last_dir > 0); mom_neg = 0.12 * (last_dir < 0)\n            score_pos = eff_pos * steps + mom + 1e-11 / (1 + np.sqrt(trials))\n            score_neg = eff_neg * steps + mom_neg + 1e-11 / (1 + np.sqrt(trials))\n            choose_pos = score_pos >= score_neg\n            cand = np.where(choose_pos, score_pos, score_neg)\n            d = int(np.argmax(cand))\n            sign = 1 if choose_pos[d] else -1\n\n            # single-eval coordinate probe\n            stepd = float(np.clip(steps[d], min_step, max_step))\n            xt = x_best.copy(); xt[d] = np.clip(xt[d] + sign * stepd, lb[d], ub[d])\n            ft = safe_eval(xt); trials[d] += 1\n\n            # reward magnitude scaled update\n            if ft < f_best:\n                rel = (f_best - ft) / (abs(f_best) + 1e-12)\n                bonus = 1.0 + min(3.0, rel * 4.0)\n                if sign > 0:\n                    a_pos[d] += 1.0 + bonus\n                else:\n                    a_neg[d] += 1.0 + bonus\n                x_second, f_second = x_best.copy(), f_best\n                x_best, f_best = xt.copy(), float(ft)\n                steps[d] = min(max_step, steps[d] * (self.success_mult ** (0.9 + 0.1 * bonus)))\n                consec_fail[d] = 0\n                last_dir[d] = sign\n            else:\n                pen = 1.0\n                if sign > 0:\n                    b_pos[d] += pen\n                else:\n                    b_neg[d] += pen\n                steps[d] = max(min_step, steps[d] * self.failure_mult)\n                consec_fail[d] += 1\n                last_dir[d] = -sign\n\n            # corrective nudge on stagnation\n            if consec_fail[d] >= 6:\n                steps[d] = max(min_step, steps[d] * (self.failure_mult ** 1.5))\n                if sign > 0: b_pos[d] += 0.5\n                else: b_neg[d] += 0.5\n                consec_fail[d] = 0\n\n            # mild decay of accumulated Beta mass to avoid overconfidence\n            if trials[d] > 60:\n                for arr in (a_pos, b_pos, a_neg, b_neg):\n                    arr[d] = 1.0 + (arr[d] - 1.0) * 0.85\n                trials[d] = int(trials[d] * 0.6)\n\n            it += 1\n\n        # quick symmetric polish\n        final = max(1e-12, 0.04 * box_len)\n        while evals < self.budget and final > 1e-12:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + final, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - final, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True\n            if not improved:\n                final *= 0.5\n\n        self.x_opt = np.asarray(x_best, float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 20, "feedback": "The algorithm ThompsonCoordinateBandit scored 0.624 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7b8ff9a2-76db-4ccd-9d7c-5ec4f4578d39"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9889585854675838}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9893796483576067}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9883588013609748}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9786192944855225}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9789010090383731}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9763241018277078}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.053732230948885795}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.053680727237834236}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.10425839281901739}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10404772166836485}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.060120637319153736}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.09549305869918567}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9976692507695364}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9963399980402223}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9966332574905871}], "aucs": [0.9889585854675838, 0.9893796483576067, 0.9883588013609748, 0.9786192944855225, 0.9789010090383731, 0.9763241018277078, 0.053732230948885795, 0.053680727237834236, 0.10425839281901739, 0.10404772166836485, 0.060120637319153736, 0.09549305869918567, 0.9976692507695364, 0.9963399980402223, 0.9966332574905871]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1747.0, "Edges": 1746.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9988551803091013, "Degree Variance": 2.384658105529833, "Transitivity": 0.0, "Max Depth": 12.0, "Min Depth": 2.0, "Mean Depth": 7.4088669950738915, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3125169190686594, "Depth Entropy": 1.8481020052312147, "Assortativity": 8.332778775460658e-09, "Average Eccentricity": 14.470520892959359, "Diameter": 19.0, "Radius": 10.0, "Edge Density": 0.0005724098454493417, "Average Shortest Path": 9.215254296188327, "mean_complexity": 8.25, "total_complexity": 33.0, "mean_token_count": 377.75, "total_token_count": 1511.0, "mean_parameter_count": 3.5, "total_parameter_count": 14.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "9ae21fbf-4b4b-421f-878d-e64b50904bf4", "fitness": 0.614496499410543, "name": "OrthogonalAdaptiveBanditDescent", "description": "Orthogonal Adaptive Bandit Descent (OABD) \u2014 pick small random orthogonal subspaces (mostly 1-D) chosen by adaptive per-coordinate importance weights, perform mirrored cheap probes with momentum-biased directions, multiplicatively adapt per-dim step sizes from success/failure statistics, and use archive-driven median pulls plus occasional Cauchy jumps to escape stagnation.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveBanditDescent:\n    def __init__(self, budget, dim, archive_size=10, init_step=None,\n                 min_step=1e-6, max_step=None, patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def __call__(self, func):\n        lb = np.full(self.dim, -5.0); ub = np.full(self.dim, 5.0); span = ub - lb\n        # steps\n        if self.init_step is None:\n            step = np.full(self.dim, 0.2 * span)\n        else:\n            s0 = np.array(self.init_step, dtype=float)\n            step = s0 if s0.shape == (self.dim,) else np.full(self.dim, float(s0))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            m0 = np.array(self.max_step, dtype=float)\n            max_step = m0 if m0.shape == (self.dim,) else np.full(self.dim, float(m0))\n        step = np.clip(step, self.min_step, max_step)\n        # archive\n        archive_x = []\n        archive_f = []\n        evals = 0\n        # initial sampling\n        n0 = max(1, min(self.dim + 1, max(1, self.budget // 12)))\n        for _ in range(n0):\n            x = lb + self.rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n            if evals >= self.budget: break\n        if not archive_x:\n            x = lb + self.rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n        bi = int(np.argmin(archive_f)); x_best = archive_x[bi].copy(); f_best = float(archive_f[bi])\n        # adaptive stats\n        scores = np.ones(self.dim)  # importance weights\n        mom = np.zeros(self.dim)    # momentum\n        no_imp = 0\n        alpha_score = 0.18\n        def add_archive(xn, fn):\n            if len(archive_x) < self.archive_size:\n                archive_x.append(xn.copy()); archive_f.append(float(fn))\n            else:\n                w = int(np.argmax(archive_f))\n                if fn < archive_f[w]:\n                    archive_x[w] = xn.copy(); archive_f[w] = float(fn)\n        # main loop\n        while evals < self.budget:\n            # choose subspace size\n            k = 2 if (self.dim >= 2 and self.rng.random() < 0.16) else 1\n            # pick coords by weights\n            probs = np.maximum(scores, 1e-12)\n            probs = probs / probs.sum()\n            if k == 1:\n                idx = np.array([self.rng.choice(self.dim, p=probs)])\n            else:\n                i = self.rng.choice(self.dim, p=probs)\n                probs2 = probs.copy(); probs2[i] = 0\n                probs2 = probs2 / probs2.sum()\n                j = self.rng.choice(self.dim, p=probs2)\n                idx = np.array([i, j])\n            # direction with slight momentum bias\n            d = np.zeros(self.dim)\n            v = self.rng.normal(size=idx.size)\n            d[idx] = v / (np.linalg.norm(v) + 1e-12)\n            if np.linalg.norm(mom[idx]) > 0:\n                d[idx] = d[idx] + 0.6 * (mom[idx] / (np.linalg.norm(mom[idx]) + 1e-12))\n                d[idx] = d[idx] / (np.linalg.norm(d[idx]) + 1e-12)\n            s_base = float(max(self.min_step, step[idx].mean() * (1.0 + 0.14 * (self.rng.random() - 0.5))))\n            # perform mirrored probes if possible (prefer two evals)\n            do_mirrored = (evals + 2 <= self.budget)\n            # prefer sign according to archive median projection\n            if len(archive_x) >= 2:\n                med = np.median(np.array(archive_x), axis=0)\n                proj = float(np.dot(med - x_best, d))\n            else:\n                proj = 0.0\n            if proj > 1e-12:\n                s_sign = 1\n            elif proj < -1e-12:\n                s_sign = -1\n            else:\n                s_sign = 1 if self.rng.random() < 0.5 else -1\n            improved = False\n            # mirrored two-sided probes\n            if do_mirrored:\n                xp = np.clip(x_best + s_sign * s_base * d, lb, ub)\n                xn = np.clip(x_best - s_sign * s_base * d, lb, ub)\n                fp = float(func(xp)); evals += 1\n                fn = float(func(xn)); evals += 1\n                add_archive(xp, fp); add_archive(xn, fn)\n                if fp < fn:\n                    cand_x, cand_f, cand_sign = xp, fp, s_sign\n                else:\n                    cand_x, cand_f, cand_sign = xn, fn, -s_sign\n                if cand_f < f_best:\n                    # accept\n                    delta = f_best - cand_f\n                    f_best = cand_f; x_best = cand_x.copy()\n                    # upscale steps and scores for involved dims\n                    step[idx] = np.minimum(max_step[idx], step[idx] * (1.35 + 0.1 * self.rng.random()))\n                    # update scores (EWMA of normalized improvement per unit step)\n                    eff = delta / (np.linalg.norm(s_base * d[idx]) + 1e-12)\n                    scores[idx] = (1 - alpha_score) * scores[idx] + alpha_score * (1.0 + eff)\n                    # momentum\n                    mom = 0.7 * mom + 0.3 * (x_best - np.zeros_like(x_best))  # center around origin less important; keeps recent move\n                    no_imp = 0; improved = True\n                    # cheap extrapolation\n                    if evals < self.budget:\n                        xe = np.clip(x_best + 0.6 * cand_sign * s_base * d, lb, ub)\n                        fe = float(func(xe)); evals += 1\n                        add_archive(xe, fe)\n                        if fe < f_best:\n                            f_best = fe; x_best = xe.copy()\n                            step[idx] = np.minimum(max_step[idx], step[idx] * 1.12)\n                else:\n                    # both sides failed\n                    step[idx] = np.maximum(self.min_step, step[idx] * 0.68)\n                    scores[idx] = (1 - alpha_score) * scores[idx] + alpha_score * 0.6\n                    no_imp += 1\n            else:\n                # single probe (budget tight) \u2014 try preferred sign\n                x1 = np.clip(x_best + s_sign * s_base * d, lb, ub)\n                f1 = float(func(x1)); evals += 1\n                add_archive(x1, f1)\n                if f1 < f_best:\n                    delta = f_best - f1\n                    f_best = f1; x_best = x1.copy()\n                    step[idx] = np.minimum(max_step[idx], step[idx] * 1.35)\n                    eff = delta / (np.linalg.norm(s_base * d[idx]) + 1e-12)\n                    scores[idx] = (1 - alpha_score) * scores[idx] + alpha_score * (1.0 + eff)\n                    mom = 0.7 * mom + 0.3 * (x_best - np.zeros_like(x_best))\n                    no_imp = 0; improved = True\n                else:\n                    step[idx] = np.maximum(self.min_step, step[idx] * 0.68)\n                    scores[idx] = (1 - alpha_score) * scores[idx] + alpha_score * 0.6\n                    no_imp += 1\n            # keep best from archive\n            if archive_x:\n                bi = int(np.argmin(archive_f)); ifb = float(archive_f[bi])\n                if ifb < f_best:\n                    f_best = ifb; x_best = archive_x[bi].copy()\n            # stagnation handling\n            if no_imp >= self.patience and evals < self.budget:\n                no_imp = 0\n                # try median pull\n                if len(archive_x) >= 3:\n                    med = np.median(np.array(archive_x), axis=0)\n                    pull = 0.42 * (med - x_best)\n                    if np.linalg.norm(pull) > 0:\n                        xp = np.clip(x_best + pull, lb, ub)\n                        fp = float(func(xp)); evals += 1\n                        add_archive(xp, fp)\n                        if fp < f_best:\n                            f_best = fp; x_best = xp.copy()\n                            moved = np.where(np.abs(pull) > 1e-12)[0]\n                            if moved.size:\n                                step[moved] = np.minimum(max_step[moved], step[moved] * 1.45)\n                            continue\n                # heavy-tailed jump\n                if evals < self.budget:\n                    scale = 0.55 * span\n                    jump = self.rng.standard_cauchy(self.dim) * scale\n                    jump = np.clip(jump, -2.0 * span, 2.0 * span)\n                    mask = (self.rng.random(self.dim) < 0.55)\n                    xj = np.clip(x_best + jump * mask, lb, ub)\n                    fj = float(func(xj)); evals += 1\n                    add_archive(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step = np.minimum(max_step, step * (1.0 + 0.5 * self.rng.random(self.dim)))\n                        continue\n                    else:\n                        step = np.minimum(max_step, np.maximum(self.min_step, step * (1.0 + 0.22 * (self.rng.random(self.dim) - 0.5))))\n                        if len(archive_x) > 4:\n                            w = int(np.argmax(archive_f))\n                            archive_x.pop(w); archive_f.pop(w)\n            # slight decay and safety\n            step = np.minimum(max_step, np.maximum(self.min_step, step * (1.0 - 1e-4)))\n            if evals >= self.budget:\n                break\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 20, "feedback": "The algorithm OrthogonalAdaptiveBanditDescent scored 0.614 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6dfd3db-5c46-4441-a54a-34c5ad9bdd18"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9743714877965601}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9702813508641799}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9718423764583641}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9292353092359723}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.939648671147337}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9341374500564719}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06184086561212987}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08798903739558173}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.05979164225460187}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10861145073790379}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0964213805633839}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.10830073058045475}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9886408257153548}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9937236359309091}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9926112768089399}], "aucs": [0.9743714877965601, 0.9702813508641799, 0.9718423764583641, 0.9292353092359723, 0.939648671147337, 0.9341374500564719, 0.06184086561212987, 0.08798903739558173, 0.05979164225460187, 0.10861145073790379, 0.0964213805633839, 0.10830073058045475, 0.9886408257153548, 0.9937236359309091, 0.9926112768089399]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2355.0, "Edges": 2354.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.9991507430997877, "Degree Variance": 1.9218676439431843, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.667597765363128, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3268766288219436, "Depth Entropy": 2.1756605697655536, "Assortativity": 0.0, "Average Eccentricity": 20.090870488322718, "Diameter": 27.0, "Radius": 14.0, "Edge Density": 0.0004246284501061571, "Average Shortest Path": 10.569133444090287, "mean_complexity": 13.333333333333334, "total_complexity": 40.0, "mean_token_count": 688.6666666666666, "total_token_count": 2066.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "68624d85-0de8-42c5-91ac-aed26eda59cf", "fitness": 0.6210037078560469, "name": "RotationalAdaptiveParabolicDescent", "description": "Rotational Adaptive Parabolic Descent (RAPD) \u2014 perform cheap mirrored 3-point parabolic fits along mostly 1- or 2-D random rotated directions, adapt per-dimension step-sizes multiplicatively, use lightweight momentum in the search-space (not per-coordinate bandits) and occasional heavy-tail jumps to escape stagnation.", "code": "import numpy as np\n\nclass RotationalAdaptiveParabolicDescent:\n    def __init__(self, budget=1000, dim=10,\n                 init_step=0.12, success_mult=1.3, failure_mult=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 two_d_prob=0.35, global_jump=0.03,\n                 momentum_gain=0.18, momentum_decay=0.90,\n                 seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.two_d_prob = float(two_d_prob); self.global_jump = float(global_jump)\n        self.momentum_gain = float(momentum_gain); self.momentum_decay = float(momentum_decay)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lbc = np.asarray(func.bounds.lb, float); ubc = np.asarray(func.bounds.ub, float)\n            if lbc.shape == (): lbc = np.full(dim, float(lbc))\n            if ubc.shape == (): ubc = np.full(dim, float(ubc))\n            lb, ub = lbc, ubc\n\n        box_scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step * box_scale)\n        min_step = max(1e-15, self.min_step_frac * box_scale)\n        max_step = max(1e-12, self.max_step_frac * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        steps = np.full(dim, base, float)\n        momentum = np.zeros(dim)\n\n        while evals < self.budget:\n            # occasional heavy-tail global jump (Cauchy-like)\n            if rng.random() < self.global_jump and evals < self.budget:\n                u = rng.random(dim)\n                jump = 0.09 * box_scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    x_best, f_best = xg.copy(), fg\n\n            # choose subspace dimension (mostly 1D, sometimes 2D)\n            if rng.random() < self.two_d_prob and dim > 1:\n                # pick two distinct dims and a random direction within them\n                i, j = rng.choice(dim, size=2, replace=False)\n                v2 = rng.normal(size=2); v2 /= (np.linalg.norm(v2) + 1e-18)\n                u = np.zeros(dim); u[i], u[j] = v2[0], v2[1]\n                s = (steps[i] + steps[j]) * 0.5\n                idxs = (i, j)\n            else:\n                d = int(rng.integers(dim))\n                u = np.zeros(dim); u[d] = 1.0\n                s = steps[d]\n                idxs = (d,)\n\n            s = float(np.clip(s, min_step, max_step))\n            # mirrored 3-point probes at -s, 0, +s along u\n            xp = np.clip(x_best + u * s, lb, ub); fp = safe_eval(xp)\n            xm = np.clip(x_best - u * s, lb, ub); fm = safe_eval(xm)\n            f0 = f_best\n\n            # finite-difference fit: B = f'(0), A = curvature/2 -> minimizer t* = -B/(2A)\n            B = (fp - fm) / (2.0 * s)\n            A = (fp + fm - 2.0 * f0) / (2.0 * s * s)\n            alpha = 0.0\n            # prefer analytic minimizer if curvature positive and reasonable; else take a signed step\n            if A > 1e-12:\n                tstar = -B / (2.0 * A)\n                # clamp magnitude to a few multiples of s to avoid huge extrapolation\n                mlim = 3.0 * s\n                alpha = float(np.clip(tstar, -mlim, mlim))\n            else:\n                # use directional descent: step opposite gradient sign\n                if abs(B) > 0:\n                    alpha = -np.sign(B) * s\n                else:\n                    alpha = s * (1.0 if rng.random() < 0.5 else -1.0)\n\n            # clamp alpha to global step bounds\n            alpha = float(np.clip(alpha, -max_step, max_step))\n            xt = np.clip(x_best + u * alpha, lb, ub)\n            ft = safe_eval(xt)\n\n            # acceptance and adaptation\n            if ft < f_best:\n                # accept\n                x_best, f_best = xt.copy(), float(ft)\n                # boost steps for involved dims\n                for k in idxs:\n                    steps[k] = min(max_step, steps[k] * self.success_mult)\n                # update momentum in space of move\n                move = (xt - x_best)  # note x_best already updated, so move=0 -> use u*alpha\n                if np.linalg.norm(move) < 1e-18:\n                    move = u * alpha\n                momentum = momentum * self.momentum_decay + (move / (np.linalg.norm(move) + 1e-18)) * self.momentum_gain\n                # opportunistic extrapolation\n                if evals < self.budget:\n                    extra = np.clip(x_best + u * alpha * 1.4, lb, ub)\n                    fextra = safe_eval(extra)\n                    if fextra < f_best:\n                        x_best, f_best = extra.copy(), float(fextra)\n            else:\n                # reject -> shrink involved steps and damp momentum\n                for k in idxs:\n                    steps[k] = max(min_step, steps[k] * self.failure_mult)\n                momentum *= self.momentum_decay\n\n            # mild safeguards: clamp steps into [min_step, max_step]\n            steps = np.clip(steps, min_step, max_step)\n\n        # quick symmetric polish while budget left\n        fin = 0.04 * box_scale\n        while evals < self.budget and fin > 1e-12 * box_scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = np.clip(x_best + fin * np.eye(1, dim, d).ravel(), lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = np.clip(x_best - fin * np.eye(1, dim, d).ravel(), lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 20, "feedback": "The algorithm RotationalAdaptiveParabolicDescent scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a569da3-dff0-437a-a1e1-9e9baee72739"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9846882739882203}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9812822633736223}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9817644505312746}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9676799553436602}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9650490296692331}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9622173372691321}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1086671963155551}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.09628068263245226}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.09917488245819461}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.05493748270675036}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06724831772739215}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.06523038395963154}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9939782359093353}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.993127847900894}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9937292780553543}], "aucs": [0.9846882739882203, 0.9812822633736223, 0.9817644505312746, 0.9676799553436602, 0.9650490296692331, 0.9622173372691321, 0.1086671963155551, 0.09628068263245226, 0.09917488245819461, 0.05493748270675036, 0.06724831772739215, 0.06523038395963154, 0.9939782359093353, 0.993127847900894, 0.9937292780553543]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1534.0, "Edges": 1533.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.998696219035202, "Degree Variance": 2.284222550481141, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.574498567335244, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3295969704177968, "Depth Entropy": 1.9927326759455253, "Assortativity": 0.0, "Average Eccentricity": 17.089960886571056, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.000651890482398957, "Average Shortest Path": 9.384632394151781, "mean_complexity": 10.666666666666666, "total_complexity": 32.0, "mean_token_count": 453.3333333333333, "total_token_count": 1360.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "1d5c6e04-206b-4dd3-b561-7029c3f0a876", "fitness": 0.632621766811751, "name": "AdaptiveCoordinateTiltedProbing", "description": "Adaptive Coordinate Tilted Probing (ACTP) \u2014 per-coordinate signed \"tilt\" (EMA of good directions) biases cheap 1-D probes toward archive medians; mirrored quick checks estimate local curvature and multiplicatively adapt per-dim steps with light extrapolations and occasional targeted diversification.", "code": "import numpy as np\n\nclass AdaptiveCoordinateTiltedProbing:\n    \"\"\"\n    Adaptive Coordinate Tilted Probing (ACTP)\n\n    - Works with only degree-1 moves (one coordinate per eval).\n    - Keeps a small top-k archive to compute coordinate medians and dispersions.\n    - Maintains per-coordinate 'tilt' (EMA of successful directions) to bias sign selection.\n    - Uses mirrored/paired cheap probes to adapt step sizes multiplicatively and optionally extrapolate.\n    - Occasional targeted diversifications when stagnating.\n    \"\"\"\n    def __init__(self, budget, dim, archive_size=12, init_step=None,\n                 min_step=1e-6, rng=None, stagnation_patience=10):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.stagnation_patience = int(stagnation_patience)\n\n    def __call__(self, func):\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        span = ub - lb\n\n        # init steps\n        if self.init_step is None:\n            step = np.full(self.dim, 0.2 * span)\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (self.dim,) else np.full(self.dim, float(s))\n\n        max_step = 0.5 * span\n        step = np.clip(step, self.min_step, max_step)\n\n        # small archive\n        top_k = min(self.archive_size, max(2, self.budget // 6))\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n        # initial random samples to populate archive\n        n_init = min(max(4, self.dim), max(1, self.budget // 12))\n        for _ in range(n_init):\n            x0 = lb + self.rng.random(self.dim) * span\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n            if evals >= self.budget:\n                break\n        if len(archive_x) == 0:\n            x0 = lb + self.rng.random(self.dim) * span\n            f0 = float(func(x0)); evals += 1\n            archive_x.append(x0.copy()); archive_f.append(f0)\n\n        # best so far\n        idx = int(np.argmin(archive_f))\n        x_best = archive_x[idx].copy()\n        f_best = float(archive_f[idx])\n\n        # per-dim statistics\n        tilt = np.zeros(self.dim)        # EMA sign/tilt in [-1,1]\n        wins = np.zeros(self.dim)       # successes\n        trials = np.ones(self.dim)      # trials (avoid div by zero)\n\n        stagn = 0\n\n        def add_archive(x, f):\n            if len(archive_x) < top_k:\n                archive_x.append(x.copy()); archive_f.append(float(f))\n            else:\n                w = int(np.argmax(archive_f))\n                if f < archive_f[w]:\n                    archive_x[w] = x.copy(); archive_f[w] = float(f)\n\n        # main loop\n        while evals < self.budget:\n            arr = np.array(archive_x)\n            med = np.median(arr, axis=0)\n            iqr = np.subtract(*np.percentile(arr, [75, 25], axis=0))\n            iqr = np.maximum(iqr, 1e-12)\n\n            pull = med - x_best\n            # score to pick coordinate: normalized pull magnitude + dispersion + success rate bonus\n            score = np.abs(pull) / (step + 1e-12)\n            score += 0.6 * (iqr / (span + 1e-12))\n            score += 0.2 * (wins / (trials + 1e-9))\n            # softmax-ish probabilities\n            s = score - score.max()\n            tempscale = 1.0 + 0.15 * np.std(score)\n            probs = np.exp(s / tempscale)\n            probs /= probs.sum()\n\n            try:\n                coord = int(self.rng.choice(self.dim, p=probs))\n            except Exception:\n                coord = int(self.rng.integers(0, self.dim))\n\n            # choose sign biased by pull and tilt\n            prefer_sign = 0\n            if abs(pull[coord]) > 1e-12:\n                prefer_sign = int(np.sign(pull[coord]))\n            # combine with tilt\n            if self.rng.random() < (0.75 if prefer_sign != 0 else 0.5):\n                sign = prefer_sign if prefer_sign != 0 else (1 if self.rng.random() < 0.5 else -1)\n            else:\n                sign = int(np.sign(tilt[coord])) if tilt[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            # small random flip chance\n            if self.rng.random() < 0.08:\n                sign *= -1\n\n            # magnitude: dependent on step and how far median is\n            mag = step[coord] * (0.5 + 0.5 * self.rng.random())\n            # if median pulls are strong, prefer smaller move toward them (safer)\n            if abs(pull[coord]) > 0.8 * step[coord]:\n                mag = min(mag, 0.9 * abs(pull[coord]))\n\n            # single-coordinate trial\n            x1 = x_best.copy()\n            x1[coord] = np.clip(x1[coord] + sign * mag, lb[coord], ub[coord])\n            f1 = float(func(x1)); evals += 1\n            add_archive(x1, f1)\n            trials[coord] += 1\n\n            improved = False\n            if f1 < f_best:\n                # accept and enlarge step slightly\n                f_best = f1; x_best = x1; improved = True\n                wins[coord] += 1\n                tilt[coord] = 0.65 * tilt[coord] + 0.35 * sign\n                step[coord] = float(min(max_step[coord], step[coord] * 1.22))\n                stagn = 0\n                # cheap extrapolation (half mag) with low probability\n                if evals < self.budget and self.rng.random() < 0.45:\n                    ext = 0.5 * mag\n                    xext = x_best.copy()\n                    xext[coord] = np.clip(xext[coord] + sign * ext, lb[coord], ub[coord])\n                    fext = float(func(xext)); evals += 1\n                    add_archive(xext, fext)\n                    if fext < f_best:\n                        f_best = fext; x_best = xext\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n            else:\n                # mirrored opposite sign probe if budget allows\n                if evals < self.budget:\n                    x2 = x_best.copy()\n                    x2[coord] = np.clip(x2[coord] - sign * mag, lb[coord], ub[coord])\n                    f2 = float(func(x2)); evals += 1\n                    add_archive(x2, f2)\n                    trials[coord] += 1\n                    if f2 < f_best:\n                        f_best = f2; x_best = x2; improved = True\n                        wins[coord] += 1\n                        tilt[coord] = 0.6 * tilt[coord] + 0.4 * (-sign)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.18))\n                        stagn = 0\n                        # tiny extrapolation\n                        if evals < self.budget and self.rng.random() < 0.35:\n                            ext = 0.4 * mag\n                            xext = x_best.copy()\n                            xext[coord] = np.clip(xext[coord] - sign * ext, lb[coord], ub[coord])\n                            fext = float(func(xext)); evals += 1\n                            add_archive(xext, fext)\n                            if fext < f_best:\n                                f_best = fext; x_best = xext\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                    else:\n                        # both directions failed: shrink and damp tilt\n                        step[coord] = float(max(self.min_step, step[coord] * 0.58))\n                        tilt[coord] *= 0.7\n                        stagn += 1\n                else:\n                    step[coord] = float(max(self.min_step, step[coord] * 0.58))\n                    tilt[coord] *= 0.7\n                    stagn += 1\n\n            # occasional curvature probe to adjust step: symmetric tiny probes -> simple curvature sign\n            if (evals + 2 <= self.budget) and (self.rng.random() < 0.06):\n                eps = 0.25 * step[coord]\n                if eps > 1e-12:\n                    x_p = x_best.copy(); x_m = x_best.copy()\n                    x_p[coord] = np.clip(x_best[coord] + eps, lb[coord], ub[coord])\n                    x_m[coord] = np.clip(x_best[coord] - eps, lb[coord], ub[coord])\n                    f_p = float(func(x_p)); f_m = float(func(x_m)); evals += 2\n                    add_archive(x_p, f_p); add_archive(x_m, f_m)\n                    # discrete second derivative approx\n                    sec = (f_p + f_m - 2.0 * f_best) / (eps * eps + 1e-24)\n                    # positive sec => convex => shrink; negative => maybe widen\n                    if sec > 0:\n                        step[coord] = float(max(self.min_step, step[coord] * 0.7))\n                    else:\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.15))\n\n            # targeted diversification when stagnating\n            if stagn >= self.stagnation_patience and evals < self.budget:\n                stagn = 0\n                # pick coordinate with largest iqr*step\n                weight = (iqr / (span + 1e-12)) * (step / (step.max() + 1e-12))\n                c = int(np.argmax(weight))\n                # random jump size using span and current step\n                jump = (0.5 * span[c] * self.rng.random() + 0.6 * step[c]) * (1 if self.rng.random() < 0.5 else -1)\n                xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                fj = float(func(xj)); evals += 1\n                add_archive(xj, fj)\n                if fj < f_best:\n                    f_best = fj; x_best = xj\n                    step[c] = float(min(max_step[c], step[c] * 1.35))\n                    tilt[c] = 0.5 * np.sign(jump)\n                else:\n                    # small random perturbation to several dims with low prob\n                    if self.rng.random() < 0.2 and evals < self.budget:\n                        y = x_best.copy()\n                        k = min(3, self.dim)\n                        idxs = self.rng.choice(self.dim, size=k, replace=False)\n                        for i in idxs:\n                            y[i] = np.clip(y[i] + (0.2 * span[i]) * (self.rng.random() - 0.5), lb[i], ub[i])\n                        fy = float(func(y)); evals += 1\n                        add_archive(y, fy)\n                        if fy < f_best:\n                            f_best = fy; x_best = y\n\n            # gentle decay to avoid getting stuck on stale tilt\n            tilt *= 0.997\n            # bounds on step\n            step = np.minimum(step, max_step)\n            step = np.maximum(step, self.min_step)\n\n        return f_best, x_best", "configspace": "", "generation": 20, "feedback": "The algorithm AdaptiveCoordinateTiltedProbing scored 0.633 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.975691196058068}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.980392279349778}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9790053743239988}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9568590395864066}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9657603927717778}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9429128090928536}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.15115045002040972}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.09515143500376544}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.11920879113667249}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10101900526602858}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.12927549579051023}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.10911152961222137}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9939436340421856}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9954075063687661}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9944375637528242}], "aucs": [0.975691196058068, 0.980392279349778, 0.9790053743239988, 0.9568590395864066, 0.9657603927717778, 0.9429128090928536, 0.15115045002040972, 0.09515143500376544, 0.11920879113667249, 0.10101900526602858, 0.12927549579051023, 0.10911152961222137, 0.9939436340421856, 0.9954075063687661, 0.9944375637528242]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2421.0, "Edges": 2420.0, "Max Degree": 34.0, "Min Degree": 1.0, "Mean Degree": 1.9991738950846758, "Degree Variance": 2.1627419858695456, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.563570784490532, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3140059090667258, "Depth Entropy": 2.1671340739048635, "Assortativity": 0.0, "Average Eccentricity": 19.109458901280462, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0004130524576621231, "Average Shortest Path": 10.34865723814693, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 704.6666666666666, "total_token_count": 2114.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "ad40121a-3566-4558-8b5a-52a04f06f382", "fitness": 0.642024694665791, "name": "ParabolicMirroredArchive", "description": "Parabolic Mirrored Descent with compact archive \u2014 fit a 1-D parabola from mirrored probes to compute analytic coordinate minima, adapt per-dimension steps multiplicatively, use a tiny median archive for guided pulls and rare Cauchy escapes for robust exploration.", "code": "import numpy as np\n\nclass ParabolicMirroredArchive:\n    def __init__(self, budget=1000, dim=10, init_step=0.08, success=1.3, failure=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.2, global_prob=0.03, extrap=1.8,\n                 arch_size=6, polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.success = float(success); self.failure = float(failure)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob); self.extrap = float(extrap)\n        self.arch_size = int(arch_size); self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, float(lb))\n            if ub.shape == (): ub = np.full(d, float(ub))\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n        span = np.maximum(1e-12, ub - lb)\n        box = max(1e-12, np.linalg.norm(span))\n        base = max(1e-12, self.init_step * box)\n        min_step = max(1e-15, self.min_step_frac * box)\n        max_step = max(1e-12, self.max_step_frac * box)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        x = rng.uniform(lb, ub); fx = safe_eval(x)\n        steps = np.full(d, base)\n        trials = np.zeros(d, int); succ = np.zeros(d, int)\n        # small archive of best points\n        arch_x = [x.copy()]; arch_f = [fx]\n\n        # lightweight momentum (ewma of recent moves)\n        mom = np.zeros(d, float); mom_alpha = 0.12\n\n        while evals < self.budget:\n            # rare heavy-tail global escape (Cauchy-like)\n            if rng.random() < self.global_prob:\n                u = rng.random(d)\n                jump = 0.16 * box * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < fx:\n                    x, fx = xg.copy(), fg\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                    mom = (1-mom_alpha)*mom + mom_alpha*(x - xg)\n                continue\n\n            # occasional archive median pull\n            if len(arch_x) >= 3 and rng.random() < 0.025 and evals < self.budget:\n                med = np.median(np.vstack(arch_x), axis=0)\n                idxs = np.argsort(-steps)[:max(1, d//6)]\n                cand = x.copy()\n                cand[idxs] = np.clip(x[idxs] + 0.45*(med[idxs]-x[idxs]), lb[idxs], ub[idxs])\n                fc = safe_eval(cand)\n                if fc < fx:\n                    x, fx = cand.copy(), fc\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                    mom = (1-mom_alpha)*mom + mom_alpha*(x - cand)\n                continue\n\n            # choose coordinate: favor large steps and low recent success, add small stochasticity and momentum bias\n            scores = steps * (1.0 + 0.8/(1+trials)) + 0.25*np.abs(mom) + 1e-6*rng.random(d)\n            coord = int(np.argmax(scores))\n            s = float(np.clip(steps[coord], min_step, max_step))\n\n            # mirrored probes when we have at least 2 evals left\n            if evals <= self.budget - 2:\n                xp = x.copy(); xm = x.copy()\n                xp[coord] = np.clip(xp[coord] + s, lb[coord], ub[coord])\n                xm[coord] = np.clip(xm[coord] - s, lb[coord], ub[coord])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[coord] += 2\n\n                # if either side improved, take best and try quick extrapolation\n                if (fp < fx) or (fm < fx):\n                    if fp < fm:\n                        x_old = x.copy(); x, fx = xp.copy(), fp\n                    else:\n                        x_old = x.copy(); x, fx = xm.copy(), fm\n                    succ[coord] += 1\n                    steps[coord] = min(max_step, steps[coord] * self.success)\n                    # extrapolate in same direction with one eval if budget allows\n                    if evals < self.budget:\n                        sign = np.sign(x[coord] - x_old[coord])\n                        if sign == 0: sign = 1.0\n                        ext = x.copy()\n                        ext[coord] = np.clip(ext[coord] + sign * s * self.extrap, lb[coord], ub[coord])\n                        fext = safe_eval(ext); trials[coord] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext\n                            arch_x.append(x.copy()); arch_f.append(fx)\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                    mom = (1-mom_alpha)*mom + mom_alpha*(x - x_old)\n                    continue\n\n                # fit parabola: values at -s,0,+s => analytic min delta = - s*(fp-fm)/(2*(fp+fm-2*fx))\n                denom = (fp + fm - 2.0*fx)\n                if abs(denom) > 1e-18:\n                    delta = - s * (fp - fm) / (2.0 * denom)\n                else:\n                    delta = 0.0\n                # clamp parabola step to reasonable range to avoid wild extrapolation\n                max_delta = s * self.extrap\n                delta = float(np.clip(delta, -max_delta, max_delta))\n                if abs(delta) > 1e-12:\n                    x_par = x.copy()\n                    x_par[coord] = np.clip(x_par[coord] + delta, lb[coord], ub[coord])\n                    fpar = safe_eval(x_par); trials[coord] += 1\n                    if fpar < fx:\n                        x_old = x.copy(); x, fx = x_par.copy(), fpar\n                        succ[coord] += 1\n                        steps[coord] = min(max_step, steps[coord] * self.success)\n                        arch_x.append(x.copy()); arch_f.append(fx)\n                        if len(arch_x) > self.arch_size:\n                            i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                        mom = (1-mom_alpha)*mom + mom_alpha*(x - x_old)\n                        continue\n                    else:\n                        # no gain: shrink\n                        steps[coord] = max(min_step, steps[coord] * self.failure)\n                else:\n                    # flat: shrink step\n                    steps[coord] = max(min_step, steps[coord] * self.failure)\n\n            else:\n                # last eval: one-sided try\n                if rng.random() < 0.5:\n                    xt = x.copy(); xt[coord] = np.clip(xt[coord] + s, lb[coord], ub[coord])\n                else:\n                    xt = x.copy(); xt[coord] = np.clip(xt[coord] - s, lb[coord], ub[coord])\n                ft = safe_eval(xt); trials[coord] += 1\n                if ft < fx:\n                    x, fx = xt.copy(), ft; succ[coord] += 1\n                    steps[coord] = min(max_step, steps[coord] * self.success)\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                    mom = (1-mom_alpha)*mom + mom_alpha*(x - xt)\n                else:\n                    steps[coord] = max(min_step, steps[coord] * self.failure)\n\n            # rejuvenate stale coords\n            if trials[coord] > 80 and succ[coord] == 0:\n                steps[coord] = min(max_step, steps[coord] * 1.6)\n                trials[coord] = int(trials[coord] * 0.6)\n\n        # final local polish: tiny symmetric probes\n        fin = self.polish_frac * box\n        while evals < self.budget and fin > 1e-12 * box:\n            improved = False\n            for i in range(d):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[i] = np.clip(xt[i] + fin, lb[i], ub[i])\n                ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[i] = np.clip(xt[i] - fin, lb[i], ub[i])\n                ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 20, "feedback": "The algorithm ParabolicMirroredArchive scored 0.642 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4eb203d7-218c-4260-a12b-28839b4ee356"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9949663591847528}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9909774105152144}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9933123921835465}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9610048838470474}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9615838062176646}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9613524086779018}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.11472061523747712}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.15036910990153807}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.1290857236104095}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.13799171113240738}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.12078663059896722}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.12082352334615809}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9978703593254638}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9981214715666031}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9974040146417131}], "aucs": [0.9949663591847528, 0.9909774105152144, 0.9933123921835465, 0.9610048838470474, 0.9615838062176646, 0.9613524086779018, 0.11472061523747712, 0.15036910990153807, 0.1290857236104095, 0.13799171113240738, 0.12078663059896722, 0.12082352334615809, 0.9978703593254638, 0.9981214715666031, 0.9974040146417131]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2440.0, "Edges": 2439.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9991803278688525, "Degree Variance": 2.0573763773179254, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.426229508196721, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3319976548371348, "Depth Entropy": 2.0538894087909814, "Assortativity": 0.0, "Average Eccentricity": 15.508196721311476, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0004098360655737705, "Average Shortest Path": 10.20293724248718, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 694.6666666666666, "total_token_count": 2084.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "247a1c6b-0c5f-4e00-89dc-1535de4e5224", "fitness": 0.045481535364183506, "name": "AdaptiveGaussianShellSearch", "description": "Adaptive Gaussian Shell Search (AGSS) \u2014 sample candidate directions on an adaptive spherical \"shell\" (scaled by per-dimension sigmas), use mirrored paired probes to estimate directional gain and curvature, multiplicatively adapt per-dim scales, maintain a tiny best-archive for median nudges and occasional axis refinements and heavy-tailed escapes.", "code": "import numpy as np\n\nclass AdaptiveGaussianShellSearch:\n    def __init__(self, budget=1000, dim=10,\n                 init_scale=0.08, success=1.4, failure=0.65,\n                 min_scale=1e-9, max_scale=1.5,\n                 archive_size=6, global_prob=0.03, axis_prob=0.06,\n                 shell_jitter=0.25, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.init_scale = float(init_scale)\n        self.success = float(success); self.failure = float(failure)\n        self.min_scale = float(min_scale); self.max_scale = float(max_scale)\n        self.archive_size = int(archive_size)\n        self.global_prob = float(global_prob); self.axis_prob = float(axis_prob)\n        self.shell_jitter = float(shell_jitter)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, float(lb))\n            if ub.shape == (): ub = np.full(d, float(ub))\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n        span = np.maximum(1e-12, ub - lb)\n        box_scale = max(1e-12, np.linalg.norm(span))\n        base = max(1e-12, self.init_scale * box_scale)\n        min_scale = max(1e-15, self.min_scale * box_scale)\n        max_scale = max(1e-12, self.max_scale * box_scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        # initial sampling: keep a small archive\n        best_x = rng.uniform(lb, ub)\n        best_f = safe_eval(best_x)\n        arch_x = [best_x.copy()]; arch_f = [best_f]\n\n        # per-dimension scales and success counts\n        scales = np.full(d, base)\n        wins = np.zeros(d, int)\n        attempts = np.zeros(d, int)\n\n        # main loop\n        while evals < self.budget:\n            # occasional heavy-tailed global escape\n            if rng.random() < self.global_prob:\n                # Cauchy-like jump scaled to box\n                u = rng.random(d)\n                jump = 0.18 * box_scale * np.tan(np.pi * (u - 0.5))\n                cand = np.clip(best_x + jump, lb, ub)\n                f = safe_eval(cand)\n                if f < best_f:\n                    best_x, best_f = cand.copy(), f\n                    arch_x.append(best_x.copy()); arch_f.append(best_f)\n                    if len(arch_x) > self.archive_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                continue\n\n            # occasional median nudge from archive\n            if len(arch_x) >= 3 and rng.random() < 0.04:\n                med = np.median(np.vstack(arch_x), axis=0)\n                # nudge in coordinates with largest scales\n                k = max(1, d//6)\n                idx = np.argsort(-scales)[:k]\n                cand = best_x.copy()\n                cand[idx] = np.clip(best_x[idx] + 0.5*(med[idx]-best_x[idx]), lb[idx], ub[idx])\n                f = safe_eval(cand)\n                if f < best_f:\n                    best_x, best_f = cand.copy(), f\n                    arch_x.append(best_x.copy()); arch_f.append(best_f)\n                    if len(arch_x) > self.archive_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                continue\n\n            # choose sampling mode: axis-refine or shell-sample\n            if rng.random() < self.axis_prob:\n                # choose coordinate biased by scale and low success\n                score = scales * (1.0 + 0.6/(1+attempts)) * (1.0 + 0.2*rng.random(d))\n                i = int(np.argmax(score))\n                s = float(np.clip(scales[i], min_scale, max_scale))\n                # mirrored one-dim probe if budget allows\n                if evals <= self.budget - 2:\n                    xp = best_x.copy(); xm = best_x.copy()\n                    xp[i] = np.clip(xp[i] + s, lb[i], ub[i])\n                    xm[i] = np.clip(xm[i] - s, lb[i], ub[i])\n                    fp = safe_eval(xp); fm = safe_eval(xm)\n                    attempts[i] += 2\n                    # pick best\n                    if fp < best_f or fm < best_f:\n                        if fp < fm:\n                            best_x, best_f = xp.copy(), fp\n                            dir_sign = +1.0\n                        else:\n                            best_x, best_f = xm.copy(), fm\n                            dir_sign = -1.0\n                        wins[i] += 1\n                        scales[i] = min(max_scale, scales[i] * self.success)\n                        # quick extrapolation\n                        if evals < self.budget:\n                            ext = best_x.copy()\n                            ext[i] = np.clip(ext[i] + dir_sign * s * 1.6, lb[i], ub[i])\n                            fext = safe_eval(ext); attempts[i] += 1\n                            if fext < best_f:\n                                best_x, best_f = ext.copy(), fext\n                    else:\n                        scales[i] = max(min_scale, scales[i] * self.failure)\n                else:\n                    # last eval\n                    xt = best_x.copy(); xt[i] = np.clip(xt[i] + s*(1 if rng.random() < 0.5 else -1), lb[i], ub[i])\n                    f = safe_eval(xt); attempts[i] += 1\n                    if f < best_f:\n                        best_x, best_f = xt.copy(), f; wins[i] += 1\n                        scales[i] = min(max_scale, scales[i] * self.success)\n                    else:\n                        scales[i] = max(min_scale, scales[i] * self.failure)\n                arch_x.append(best_x.copy()); arch_f.append(best_f)\n                if len(arch_x) > self.archive_size:\n                    ii = int(np.argmax(arch_f)); arch_x.pop(ii); arch_f.pop(ii)\n                continue\n\n            # shell sampling: draw a random direction (anisotropic via scales)\n            z = rng.normal(size=d)\n            nz = np.linalg.norm(z)\n            if nz == 0: z = rng.normal(size=d); nz = max(1e-12, np.linalg.norm(z))\n            udir = z / nz\n            # anisotropic radius: average of per-dim scales weighted by |udir|\n            avg_scale = float(np.sum(np.abs(udir) * scales) / (np.sum(np.abs(udir)) + 1e-12))\n            radius = avg_scale * max(0.2, 1.0 + self.shell_jitter * rng.normal())\n            prop = best_x + udir * radius\n            prop = np.clip(prop, lb, ub)\n\n            # mirrored probe along shell direction if budget allows\n            if evals <= self.budget - 2:\n                f1 = safe_eval(prop)\n                f2 = safe_eval(np.clip(best_x - udir * radius, lb, ub))\n                # update attempts proportional to contribution of dims\n                absd = np.abs(udir)\n                attempts += (absd > 0) * 2\n                # accept best side\n                if f1 < best_f or f2 < best_f:\n                    if f1 < f2:\n                        new_x, new_f, side = prop.copy(), f1, +1.0\n                    else:\n                        new_x, new_f, side = np.clip(best_x - udir*radius, lb, ub).copy(), f2, -1.0\n                    # adapt scales: increase along coordinates that mattered (|udir| large)\n                    mask = absd > (0.5 * absd.max())\n                    if mask.any():\n                        scales[mask] = np.minimum(max_scale, scales[mask] * self.success)\n                        wins[mask] += 1\n                    # slight shrink on other coords\n                    scales[~mask] = np.maximum(min_scale, scales[~mask] * (self.failure**0.7))\n                    best_x, best_f = new_x, new_f\n                    arch_x.append(best_x.copy()); arch_f.append(best_f)\n                    if len(arch_x) > self.archive_size:\n                        ii = int(np.argmax(arch_f)); arch_x.pop(ii); arch_f.pop(ii)\n                else:\n                    # no improvement: shrink scales in active dims\n                    mask = absd > (0.3 * absd.max())\n                    scales[mask] = np.maximum(min_scale, scales[mask] * self.failure)\n            else:\n                # single-sided last eval\n                f = safe_eval(prop)\n                attempts += (np.abs(udir) > 0).astype(int)\n                if f < best_f:\n                    best_x, best_f = prop.copy(), f\n                    absd = np.abs(udir); mask = absd > (0.5 * absd.max())\n                    scales[mask] = np.minimum(max_scale, scales[mask] * self.success)\n                    arch_x.append(best_x.copy()); arch_f.append(best_f)\n                    if len(arch_x) > self.archive_size:\n                        ii = int(np.argmax(arch_f)); arch_x.pop(ii); arch_f.pop(ii)\n                else:\n                    scales *= self.failure**0.6\n                    scales[:] = np.clip(scales, min_scale, max_scale)\n\n        # final tiny polish: try median of archive and few coord nudges while budget remains\n        if len(arch_x) >= 2:\n            med = np.median(np.vstack(arch_x), axis=0)\n            if evals < self.budget:\n                cand = np.clip(best_x + 0.2*(med - best_x), lb, ub)\n                f = safe_eval(cand)\n                if f < best_f:\n                    best_x, best_f = cand.copy(), f\n\n        # coordinate polish\n        step = 0.02 * box_scale\n        i = 0\n        while evals < self.budget and i < d:\n            xt = best_x.copy(); xt[i] = np.clip(xt[i] + step, lb[i], ub[i])\n            f = safe_eval(xt)\n            if f < best_f:\n                best_x, best_f = xt.copy(), f; continue\n            xt = best_x.copy(); xt[i] = np.clip(xt[i] - step, lb[i], ub[i])\n            f = safe_eval(xt)\n            if f < best_f:\n                best_x, best_f = xt.copy(), f; continue\n            i += 1\n\n        self.x_opt = np.asarray(best_x, float); self.f_opt = float(best_f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 20, "feedback": "The algorithm AdaptiveGaussianShellSearch scored 0.045 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4eb203d7-218c-4260-a12b-28839b4ee356"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.12990359929216444}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.03486016733900121}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.09814303479338704}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.08441489075580433}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.10847987155906047}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.22597146672333512}], "aucs": [0.12990359929216444, 0.03486016733900121, 0.09814303479338704, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.08441489075580433, 0.10847987155906047, 0.22597146672333512]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2534.0, "Edges": 2533.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.999210734017364, "Degree Variance": 1.9913174512502112, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.476190476190476, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.332046123771172, "Depth Entropy": 2.1279879814669136, "Assortativity": 0.0, "Average Eccentricity": 17.53038674033149, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0003946329913180742, "Average Shortest Path": 10.412325885524961, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 725.6666666666666, "total_token_count": 2177.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "49359e5f-8e90-4833-8248-90174f85c5e2", "fitness": 0.5036408987367956, "name": "CoordinateGaussianMixture", "description": "Coordinate Gaussian-Mixture Pull (CGMP) \u2014 adaptive per-coordinate Gaussian probes with simple bandit-like variance adaptation, lightweight momentum, a tiny elite archive median pull and occasional Cauchy escapes.", "code": "import numpy as np\n\nclass CoordinateGaussianMixture:\n    def __init__(self, budget=1000, dim=10,\n                 init_sigma=0.08, success_mult=1.25, failure_mult=0.7,\n                 min_sigma=1e-9, max_sigma=2.0,\n                 archive_size=6, pull_prob=0.06, escape_prob=0.03,\n                 momentum_gain=0.18, momentum_decay=0.8, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_sigma = float(init_sigma)\n        self.success_mult = float(success_mult); self.failure_mult = float(failure_mult)\n        self.min_sigma = float(min_sigma); self.max_sigma = float(max_sigma)\n        self.archive_size = int(archive_size)\n        self.pull_prob = float(pull_prob); self.escape_prob = float(escape_prob)\n        self.momentum_gain = float(momentum_gain); self.momentum_decay = float(momentum_decay)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        sigma = np.full(dim, max(1e-12, self.init_sigma * scale), float)\n        min_s = max(1e-15, self.min_sigma * scale); max_s = max(1e-12, self.max_sigma * scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        # init\n        x_best = rng.uniform(lb, ub); f_best = safe_eval(x_best)\n        archive = [x_best.copy()]\n        momentum = np.zeros(dim)\n        succ = np.ones(dim); tries = np.ones(dim)\n\n        while evals < self.budget:\n            # occasional Cauchy escape (multi-dim)\n            if rng.random() < self.escape_prob and evals < self.budget:\n                jump = 0.15 * scale * rng.standard_cauchy(size=dim)\n                cand = np.clip(x_best + jump, lb, ub)\n                fc = safe_eval(cand)\n                if fc < f_best:\n                    x_best, f_best = cand.copy(), float(fc)\n                    archive.append(x_best.copy()); archive = archive[-self.archive_size:]\n\n            # decide coordinate by exploration weight: sigma * (1 + |momentum|)\n            headroom = (ub - x_best) / (ub - lb + 1e-18)\n            weight = sigma * (1.0 + 0.6 * np.abs(momentum)) * (0.3 + 0.7 * headroom)\n            if weight.sum() <= 0:\n                d = rng.integers(dim)\n            else:\n                p = weight / weight.sum()\n                d = rng.choice(dim, p=p)\n\n            # single-coordinate gaussian probe\n            s = np.clip(sigma[d], min_s, max_s)\n            delta = rng.normal(0.0, s)\n            cand = x_best.copy(); cand[d] = np.clip(cand[d] + delta, lb[d], ub[d])\n            fc = safe_eval(cand)\n            tries[d] += 1\n\n            improved = (fc < f_best)\n            if improved:\n                # accept\n                x_best, f_best = cand.copy(), float(fc)\n                sigma[d] = min(max_s, sigma[d] * self.success_mult)\n                momentum[d] = momentum[d] * self.momentum_decay + np.sign(delta) * self.momentum_gain\n                succ[d] += 1\n                archive.append(x_best.copy()); archive = archive[-self.archive_size:]\n                # quick opportunistic extrapolation along same coord\n                if evals < self.budget:\n                    extra = x_best.copy(); extra[d] = np.clip(extra[d] + np.sign(delta) * sigma[d], lb[d], ub[d])\n                    fe = safe_eval(extra)\n                    if fe < f_best:\n                        x_best, f_best = extra.copy(), float(fe)\n                        archive.append(x_best.copy()); archive = archive[-self.archive_size:]\n            else:\n                sigma[d] = max(min_s, sigma[d] * self.failure_mult)\n                momentum[d] *= self.momentum_decay\n\n            # occasional median pull from archive\n            if rng.random() < self.pull_prob and evals < self.budget and len(archive) > 1:\n                med = np.median(np.vstack(archive), axis=0)\n                pull = x_best + 0.5 * (med - x_best)\n                pull = np.clip(pull, lb, ub)\n                fp = safe_eval(pull)\n                if fp < f_best:\n                    x_best, f_best = pull.copy(), float(fp)\n                    archive.append(x_best.copy()); archive = archive[-self.archive_size:]\n\n            # mild annealing of sigma for very long runs: compress extremes\n            if tries[d] > 40:\n                sigma[d] = np.clip(0.9 * sigma[d], min_s, max_s)\n                succ[d] = 1.0; tries[d] = int(tries[d] * 0.6)\n\n        # small symmetric polish while budget remains\n        eps = 0.04 * scale\n        while evals < self.budget and eps > 1e-12 * scale:\n            improved_any = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                for sign in (+1.0, -1.0):\n                    cand = x_best.copy(); cand[d] = np.clip(cand[d] + sign * eps, lb[d], ub[d])\n                    fc = safe_eval(cand)\n                    if fc < f_best:\n                        x_best, f_best = cand.copy(), float(fc)\n                        improved_any = True\n                        break\n            if not improved_any:\n                eps *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 20, "feedback": "The algorithm CoordinateGaussianMixture scored 0.504 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a569da3-dff0-437a-a1e1-9e9baee72739"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9851059740048332}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9826165647267713}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9856249626525637}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9698900814527607}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9709013173232007}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.969693601144401}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.06759346164538327}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.048132469537367606}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.09173083391698766}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.0011347727299422239}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.028474496192581467}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.08503874217505236}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9922252108354449}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.1355090195837192}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.24094197313092514}], "aucs": [0.9851059740048332, 0.9826165647267713, 0.9856249626525637, 0.9698900814527607, 0.9709013173232007, 0.969693601144401, 0.06759346164538327, 0.048132469537367606, 0.09173083391698766, 0.0011347727299422239, 0.028474496192581467, 0.08503874217505236, 0.9922252108354449, 0.1355090195837192, 0.24094197313092514]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1489.0, "Edges": 1488.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9986568166554735, "Degree Variance": 2.226996181083486, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.574404761904762, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3292976853582394, "Depth Entropy": 2.0005961522335305, "Assortativity": 1.3997408436160641e-08, "Average Eccentricity": 15.607790463398254, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.000671591672263264, "Average Shortest Path": 9.445035998757916, "mean_complexity": 9.666666666666666, "total_complexity": 29.0, "mean_token_count": 439.0, "total_token_count": 1317.0, "mean_parameter_count": 5.666666666666667, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "050b4819-fab9-4601-8cdf-99de9e4152fd", "fitness": 0.6290357523351591, "name": "SeparableAdaptiveMedianPull", "description": "Separable Adaptive Median Pull with Thompson-like sign priors \u2014 strictly one-coordinate pulls toward archive medians with per-dim multiplicative step adaptation, lightweight momentum, Beta-like sign bias, small extrapolations and targeted jitter on stagnation.", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPull:\n    def __init__(self, budget, dim, archive_size=12, init_step=None, min_step=1e-6,\n                 max_step=None, patience=8, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        span = ub - lb\n        # steps\n        if self.init_step is None:\n            step = 0.2 * span\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (self.dim,) else np.full(self.dim, float(s))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (self.dim,) else np.full(self.dim, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        # initial sampling\n        evals = 0\n        n_init = min(max(4, self.dim+2), max(1, self.budget // 10))\n        archive_x = []\n        archive_f = []\n        for _ in range(n_init):\n            x = lb + self.rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n            if evals >= self.budget: break\n        if len(archive_x) == 0:\n            x = lb + self.rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n\n        # best\n        idx = int(np.argmin(archive_f))\n        x_best = archive_x[idx].copy()\n        f_best = float(archive_f[idx])\n\n        # thin state\n        a = np.ones(self.dim)  # \"success\" counts for sign bias\n        b = np.ones(self.dim)  # \"failure\" counts for sign bias\n        priority = np.zeros(self.dim)\n        momentum = np.zeros(self.dim)\n        no_improve = 0\n        top_k = min(self.archive_size, max(2, len(archive_x)))\n\n        def archive_add(xn, fn):\n            nonlocal archive_x, archive_f, top_k\n            if len(archive_x) < top_k:\n                archive_x.append(xn.copy()); archive_f.append(float(fn))\n            else:\n                worst = int(np.argmax(archive_f))\n                if fn < archive_f[worst]:\n                    archive_x[worst] = xn.copy(); archive_f[worst] = float(fn)\n\n        while evals < self.budget:\n            arr = np.array(archive_x)\n            med = np.median(arr, axis=0)\n            q75 = np.percentile(arr, 75, axis=0)\n            q25 = np.percentile(arr, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n            score = abs_pull / (step + 1e-12) + 0.5 * (iqr / (span + 1e-12)) + 0.12 * priority\n            # softmax-ish sampling\n            s = score - score.max()\n            temp = 0.9 + 0.2 * np.std(score)\n            probs = np.exp(s / temp)\n            probs /= probs.sum()\n            try:\n                coord = int(self.rng.choice(self.dim, p=probs))\n            except Exception:\n                coord = int(self.rng.integers(0, self.dim))\n\n            # decide sign: use Beta-like bias to either follow pull sign or opposite\n            follow_prob = a[coord] / (a[coord] + b[coord])\n            if abs_pull[coord] > 1e-12:\n                nominal_sign = int(np.sign(pull[coord]))\n            else:\n                # prefer momentum direction if any\n                nominal_sign = int(np.sign(momentum[coord])) if momentum[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n\n            if self.rng.random() < follow_prob:\n                sign = nominal_sign\n            else:\n                sign = -nominal_sign\n\n            magnitude = min(step[coord], abs_pull[coord] if abs_pull[coord] > 1e-12 else step[coord])\n            delta = float(sign * magnitude)\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            archive_add(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try\n                improved = True\n                a[coord] += 1.0\n                priority[coord] = min(10.0, priority[coord] + 1.0)\n                momentum[coord] = 0.7 * momentum[coord] + 0.3 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * 1.25 + 1e-12))\n                # quick extrapolation\n                if evals < self.budget:\n                    ext = 0.5 * delta\n                    x_ext = x_best.copy()\n                    x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    archive_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                        momentum[coord] = 0.8 * momentum[coord] + 0.2 * ext\n                no_improve = 0\n            else:\n                # try opposite once if budget\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    archive_add(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp\n                        improved = True\n                        b[coord] += 1.0\n                        priority[coord] = min(10.0, priority[coord] + 0.9)\n                        momentum[coord] = 0.7 * momentum[coord] + 0.3 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.2))\n                        if evals < self.budget:\n                            ext = 0.4 * (-delta)\n                            x_ext = x_best.copy()\n                            x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                            f_ext = float(func(x_ext)); evals += 1\n                            archive_add(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext; x_best = x_ext\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                        no_improve = 0\n                    else:\n                        # both directions failed\n                        b[coord] += 0.5\n                        a[coord] = max(1.0, a[coord] * 0.99)\n                        priority[coord] = max(-10.0, priority[coord] - 0.6)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.55))\n                        momentum[coord] *= 0.6\n                        no_improve += 1\n                else:\n                    no_improve += 1\n\n            # stagnation jitter\n            if no_improve >= self.patience and evals < self.budget:\n                trials = min(3, max(1, self.dim // 6))\n                cand = (iqr / (span + 1e-12)) * (step / (step.max() + 1e-12) + 0.1)\n                cand = cand + 1e-9\n                s = cand - cand.max()\n                pc = np.exp(s)\n                pc /= pc.sum()\n                try:\n                    picks = list(self.rng.choice(self.dim, size=min(self.dim, trials), replace=False, p=pc))\n                except Exception:\n                    picks = list(self.rng.choice(self.dim, size=min(self.dim, trials), replace=False))\n                improved_any = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = (0.6 * step[c] + 0.4 * span[c] * self.rng.random())\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    archive_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj\n                        step[c] = float(min(max_step[c], step[c] * 1.3))\n                        priority[c] = min(10.0, priority[c] + 1.0)\n                        momentum[c] = np.sign(jump) * 0.5 * abs(jump)\n                        improved_any = True\n                        break\n                if not improved_any:\n                    step = np.minimum(max_step, step * (1.0 + 0.15 * self.rng.random(self.dim)))\n                    priority *= 0.35\n                no_improve = 0\n\n            # slow decay\n            priority *= 0.998\n            momentum *= 0.994\n            # keep arrays tidy: maintain archive size\n            if len(archive_x) > self.archive_size:\n                order = np.argsort(archive_f)\n                keep = order[:self.archive_size]\n                archive_x = [archive_x[i] for i in keep]\n                archive_f = [archive_f[i] for i in keep]\n\n        return f_best, x_best", "configspace": "", "generation": 20, "feedback": "The algorithm SeparableAdaptiveMedianPull scored 0.629 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9742302746110872}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9768683139889436}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9807054168786645}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8268231417065616}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8436662038561961}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.83891061944736}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.13788000334850226}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.16887298843197407}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.33211183508441044}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.13457796483470097}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.10127107215413012}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.1340522055498372}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.995653658683453}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9956807439181293}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9942318425334349}], "aucs": [0.9742302746110872, 0.9768683139889436, 0.9807054168786645, 0.8268231417065616, 0.8436662038561961, 0.83891061944736, 0.13788000334850226, 0.16887298843197407, 0.33211183508441044, 0.13457796483470097, 0.10127107215413012, 0.1340522055498372, 0.995653658683453, 0.9956807439181293, 0.9942318425334349]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2309.0, "Edges": 2308.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.9991338241663057, "Degree Variance": 2.1870932298173806, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.440114068441064, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.316892317154006, "Depth Entropy": 2.150416079407775, "Assortativity": 0.0, "Average Eccentricity": 17.39411000433088, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00043308791684711995, "Average Shortest Path": 10.17775519348972, "mean_complexity": 12.666666666666666, "total_complexity": 38.0, "mean_token_count": 674.6666666666666, "total_token_count": 2024.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "ec5dce7a-22fd-4371-b776-1298fc74a824", "fitness": 0.6362308018778415, "name": "DirectionalThompsonMirrored", "description": "Directional Thompson Mirrored v2 \u2014 compact per-coordinate signed-Thompson sampling with mirrored analytic 1-D parabola minima, EWMA dispersion weighting, multiplicative step adaptation and occasional Cauchy escapes for robust cheap 1-D exploitation.", "code": "import numpy as np\n\nclass DirectionalThompsonMirrored:\n    def __init__(self, budget, dim, init_step=0.12, success_mult=1.25,\n                 failure_mult=0.78, global_prob=0.03, median_period=16,\n                 mirror_prob=0.14, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.init_step_frac = float(init_step)\n        self.succ = float(success_mult); self.fail = float(failure_mult)\n        self.global_prob = float(global_prob); self.median_period = int(median_period)\n        self.mirror_prob = float(mirror_prob)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0); ub = np.full(self.dim, 5.0)\n\n        box_len = np.linalg.norm(ub - lb)\n        base = max(1e-12, self.init_step_frac * box_len)\n        min_step = max(1e-15, 1e-9 * box_len); max_step = max(1e-12, 1.5 * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        rng = self.rng; d = self.dim\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        x_best = x.copy(); f_best = float(f)\n        archive = [(x_best.copy(), f_best)]\n\n        steps = np.full(d, base, float)\n        a_pos = np.ones(d); b_pos = np.ones(d)\n        a_neg = np.ones(d); b_neg = np.ones(d)\n        trials = np.zeros(d, int)\n        last_sign = np.zeros(d, int)\n        consec_fail = np.zeros(d, int)\n        disp = np.zeros(d, float)   # EWMA of absolute improvements\n        it = 0\n\n        while evals < self.budget:\n            # occasional Cauchy global escape\n            if rng.random() < self.global_prob and evals + 1 < self.budget:\n                c = rng.standard_cauchy(d)\n                jump = 0.08 * box_len * c / (1.0 + np.abs(c))\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                    x_best, f_best = xg.copy(), float(fg)\n\n            # median pull from small archive\n            if (it % max(1, self.median_period) == 0) and len(archive) >= 2:\n                pts = np.stack([p for p, _ in archive[:3]])\n                xm = np.median(pts, axis=0)\n                if not np.allclose(xm, x_best):\n                    fm = safe_eval(xm)\n                    if fm < f_best:\n                        archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                        x_best, f_best = xm.copy(), float(fm)\n\n            # Thompson sampling\n            ppos = rng.beta(a_pos, b_pos)\n            pneg = rng.beta(a_neg, b_neg)\n            # bias headroom toward unexplored side near bounds\n            head_ub = (ub - x_best) / (ub - lb + 1e-18); head_lb = (x_best - lb) / (ub - lb + 1e-18)\n            eff_pos = ppos * (0.6 + 0.4 * np.clip(head_ub, 0, 1.0))\n            eff_neg = pneg * (0.6 + 0.4 * np.clip(head_lb, 0, 1.0))\n\n            # dispersion weight: favor dims with informative recent gains and low trials\n            mx = disp.max() if disp.max() > 0 else 1.0\n            w_disp = (disp + 1e-12) / mx\n            w_trials = 1.0 / (1.0 + np.sqrt(trials + 1e-12))\n            weight = 0.5 * (1 + w_disp) * (0.6 + 0.4 * w_trials)\n\n            mom = 0.12 * (last_sign > 0); momn = 0.12 * (last_sign < 0)\n            score_pos = eff_pos * steps * weight + mom\n            score_neg = eff_neg * steps * weight + momn\n\n            choose_pos = score_pos >= score_neg\n            score = np.where(choose_pos, score_pos, score_neg)\n            idx = int(np.argmax(score))\n            sign = 1 if choose_pos[idx] else -1\n            step = float(np.clip(steps[idx], min_step, max_step))\n\n            # mirrored probe option: gather +s and -s, fit parabola and maybe evaluate analytic vertex\n            do_mirror = (rng.random() < self.mirror_prob) and (evals + 2 <= self.budget)\n            if do_mirror:\n                xp = x_best.copy(); xm = x_best.copy()\n                xp[idx] = np.clip(xp[idx] + step, lb[idx], ub[idx])\n                xm[idx] = np.clip(xm[idx] - step, lb[idx], ub[idx])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[idx] += 2\n                # quadratic coefficients from symmetric probes\n                A = (fp + fm - 2.0 * f_best) / (2.0 * step * step + 1e-20)\n                B = (fp - fm) / (2.0 * step + 1e-20)\n                # if meaningful curvature compute vertex offset t* = -B/(2A)\n                if abs(A) > 1e-20:\n                    tstar = -B / (2.0 * A)\n                    # limit vertex to reasonable multiple of step\n                    if abs(tstar) <= 2.25 * step and evals + 1 <= self.budget:\n                        x_v = x_best.copy()\n                        x_v[idx] = np.clip(x_best[idx] + tstar, lb[idx], ub[idx])\n                        fv = safe_eval(x_v); trials[idx] += 1\n                        cand = [(fp, xp), (fm, xm), (fv, x_v)]\n                        cand.sort(key=lambda z: z[0])\n                        bestf, bestx = cand[0]\n                        if bestf < f_best:\n                            archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                            s = np.sign(bestx[idx] - x_best[idx]) if abs(bestx[idx] - x_best[idx]) > 1e-15 else sign\n                            if s > 0: a_pos[idx] += 1\n                            else: a_neg[idx] += 1\n                            x_best, f_best = bestx.copy(), float(bestf)\n                            steps[idx] = min(max_step, steps[idx] * self.succ)\n                            consec_fail[idx] = 0; last_sign[idx] = int(s)\n                            disp[idx] = 0.92 * disp[idx] + 0.08 * max(0.0, f - f_best)\n                            it += 1; continue\n                # fallback: accept better side if it improves, else shrink\n                if fp < f_best or fm < f_best:\n                    if fp < fm:\n                        chosen_x, chosen_f, sgn = xp, fp, 1\n                    else:\n                        chosen_x, chosen_f, sgn = xm, fm, -1\n                    if chosen_f < f_best:\n                        archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                        if sgn > 0: a_pos[idx] += 1\n                        else: a_neg[idx] += 1\n                        x_best, f_best = chosen_x.copy(), float(chosen_f)\n                        steps[idx] = min(max_step, steps[idx] * self.succ)\n                        consec_fail[idx] = 0; last_sign[idx] = int(sgn)\n                        disp[idx] = 0.92 * disp[idx] + 0.08 * max(0.0, f - f_best)\n                    else:\n                        steps[idx] = max(min_step, steps[idx] * self.fail)\n                        consec_fail[idx] += 1; last_sign[idx] = -sign\n                    it += 1; continue\n\n            # single sided probe\n            xt = x_best.copy(); xt[idx] = np.clip(xt[idx] + sign * step, lb[idx], ub[idx])\n            ft = safe_eval(xt); trials[idx] += 1\n\n            # update priors\n            if sign > 0:\n                if ft < f_best: a_pos[idx] += 1.0\n                else: b_pos[idx] += 0.8\n            else:\n                if ft < f_best: a_neg[idx] += 1.0\n                else: b_neg[idx] += 0.8\n\n            if ft < f_best:\n                archive.insert(0, (x_best.copy(), f_best)); archive = sorted(archive, key=lambda z: z[1])[:3]\n                x_best, f_best = xt.copy(), float(ft)\n                steps[idx] = min(max_step, steps[idx] * self.succ)\n                consec_fail[idx] = 0; last_sign[idx] = int(sign)\n                disp[idx] = 0.92 * disp[idx] + 0.08 * max(0.0, f - f_best)\n            else:\n                steps[idx] = max(min_step, steps[idx] * self.fail)\n                consec_fail[idx] += 1; last_sign[idx] = -sign\n\n            # punish repeated failures mildly\n            if consec_fail[idx] >= 5:\n                steps[idx] = max(min_step, steps[idx] * (self.fail ** 1.4))\n                consec_fail[idx] = 0\n                if sign > 0: b_pos[idx] += 0.6\n                else: b_neg[idx] += 0.6\n\n            # avoid overconfidence by lightly shrinking Beta mass\n            if trials[idx] > 40:\n                for A,B in ((a_pos,b_pos),(a_neg,b_neg)):\n                    A[idx] = 1.0 + (A[idx]-1.0)*0.86\n                    B[idx] = 1.0 + (B[idx]-1.0)*0.86\n                trials[idx] = int(trials[idx]*0.6)\n\n            it += 1\n\n        # cheap symmetric polish\n        final = max(1e-12, 0.045 * box_len)\n        while evals < self.budget and final > 1e-12:\n            improved = False\n            for i in range(d):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[i] = np.clip(xt[i] + final, lb[i], ub[i])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[i] = np.clip(xt[i] - final, lb[i], ub[i])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True\n            if not improved: final *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 21, "feedback": "The algorithm DirectionalThompsonMirrored scored 0.636 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c85282eb-f8db-492d-b392-cf1c68d7a151"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.988198752456516}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.989714023276843}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.986531221915778}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9685245829392907}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9675749431001481}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9619181766711573}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.13880961652690949}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1087192045250227}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.15065255647327613}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.09129494500555446}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.09515766313617202}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.10885641629523779}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9961981846737691}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9964956168358378}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9948161243361091}], "aucs": [0.988198752456516, 0.989714023276843, 0.986531221915778, 0.9685245829392907, 0.9675749431001481, 0.9619181766711573, 0.13880961652690949, 0.1087192045250227, 0.15065255647327613, 0.09129494500555446, 0.09515766313617202, 0.10885641629523779, 0.9961981846737691, 0.9964956168358378, 0.9948161243361091]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2714.0, "Edges": 2713.0, "Max Degree": 34.0, "Min Degree": 1.0, "Mean Degree": 1.9992630803242446, "Degree Variance": 2.2910827288727518, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.356803797468354, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3099767022864544, "Depth Entropy": 2.125300401653446, "Assortativity": 8.511473765457437e-09, "Average Eccentricity": 17.05711127487104, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00036845983787767134, "Average Shortest Path": 10.146764900893404, "mean_complexity": 16.333333333333332, "total_complexity": 49.0, "mean_token_count": 790.3333333333334, "total_token_count": 2371.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "6164ba35-94f3-4dbe-a19d-3a3b0bbfe0d1", "fitness": 0.6400629570341014, "name": "ParabolicAdaptiveMirroredDescent", "description": "Parabolic Adaptive Mirrored Descent with tiny archive pulls and lightweight stagnation rescues \u2014 mirrored 3-point parabolic fits per coordinate, multiplicative per-dim step adaptation, tiny best-archive median pulls and rare Cauchy/global jumps for robust escapes.", "code": "import numpy as np\n\nclass ParabolicAdaptiveMirroredDescent:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 succ_mult=1.5, fail_mult=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, max_extrap=2.0,\n                 archive_size=5, polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.succ_mult = float(succ_mult); self.fail_mult = float(fail_mult)\n        self.min_pf = float(min_step_frac); self.max_pf = float(max_step_frac)\n        self.global_prob = float(global_prob); self.max_extrap = float(max_extrap)\n        self.archive_size = int(archive_size); self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds support (optional), else [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * scale)\n        min_step = max(1e-15, self.min_pf * scale)\n        max_step = max(1e-12, self.max_pf * scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        # init\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        steps = np.full(dim, base, float)\n        trials = np.zeros(dim, int); succ = np.zeros(dim, int)\n        archive = [(f, x.copy())]  # best-so-far tiny archive\n        no_improve = 0\n\n        while evals < self.budget:\n            # rare global heavy jump (Cauchy-like)\n            if rng.random() < self.global_prob:\n                u = rng.random(dim)\n                jump = 0.12 * scale * np.tan(np.pi * (u - 0.5))\n                xt = np.clip(x + jump, lb, ub); ft = safe_eval(xt)\n                if ft < f:\n                    f = ft; x = xt.copy(); succ.fill(0)\n                    archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                    no_improve = 0\n                else:\n                    no_improve += 1\n                continue\n\n            # pick coordinate (prefer large step and low trials)\n            score = steps / (1.0 + 0.3 * trials) * (1.0 + 0.2 * (succ/(1+trials)))\n            d = int(np.argmax(score + 1e-12 * rng.random(dim)))\n            s = float(np.clip(steps[d], min_step, max_step))\n            f0 = f; x0 = x.copy()\n\n            # mirrored 2 probes\n            if evals <= self.budget - 2:\n                xp = x0.copy(); xm = x0.copy()\n                xp[d] = np.clip(xp[d] + s, lb[d], ub[d])\n                xm[d] = np.clip(xm[d] - s, lb[d], ub[d])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[d] += 2\n\n                # immediate pick best of probes\n                best_val = f0; best_x = None\n                if fp < best_val: best_val, best_x = fp, xp.copy()\n                if fm < best_val: best_val, best_x = fm, xm.copy()\n\n                if best_x is not None:\n                    x, f = best_x, float(best_val); succ[d] += 1; steps[d] = min(max_step, steps[d]*self.succ_mult)\n                    archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                    no_improve = 0\n                else:\n                    # try analytic parabola around original center (f0,fp,fm)\n                    denom = (fp + fm - 2.0 * f0)\n                    if denom != 0:\n                        x_star = -s * (fp - fm) / (2.0 * denom)\n                        x_star = float(np.clip(x_star, -self.max_extrap * s, self.max_extrap * s))\n                        if abs(x_star) > 1e-14 and evals < self.budget:\n                            xt = x0.copy(); xt[d] = np.clip(xt[d] + x_star, lb[d], ub[d])\n                            ft = safe_eval(xt); trials[d] += 1\n                            if ft < f0:\n                                x, f = xt.copy(), float(ft); succ[d] += 1; steps[d] = min(max_step, steps[d]*self.succ_mult)\n                                archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                                no_improve = 0\n                            else:\n                                steps[d] = max(min_step, steps[d] * self.fail_mult); no_improve += 1\n                        else:\n                            steps[d] = max(min_step, steps[d] * self.fail_mult); no_improve += 1\n                    else:\n                        steps[d] = max(min_step, steps[d] * self.fail_mult); no_improve += 1\n            else:\n                # last few evals: one-sided probe\n                if rng.random() < 0.5:\n                    xt = x.copy(); xt[d] = np.clip(xt[d] + s, lb[d], ub[d])\n                else:\n                    xt = x.copy(); xt[d] = np.clip(xt[d] - s, lb[d], ub[d])\n                ft = safe_eval(xt); trials[d] += 1\n                if ft < f:\n                    x, f = xt.copy(), float(ft); succ[d] += 1; steps[d] = min(max_step, steps[d]*self.succ_mult)\n                    archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                    no_improve = 0\n                else:\n                    steps[d] = max(min_step, steps[d] * self.fail_mult); no_improve += 1\n\n            # mild rescue: if a dimension has many trials and zero success, boost it\n            if trials[d] > 60 and succ[d] == 0:\n                steps[d] = min(max_step, steps[d] * 1.6)\n                trials[d] = trials[d] // 2\n\n            # archive-driven small median pull when stagnating\n            if no_improve > max(30, int(0.03 * self.budget)) and len(archive) > 1:\n                meds = np.median(np.vstack([xx for _, xx in archive]), axis=0)\n                diff = meds - x\n                # pick coordinate with largest gap\n                dd = int(np.argmax(np.abs(diff)))\n                pull = 0.5 * np.sign(diff[dd]) * min(steps[dd], abs(diff[dd]))\n                xt = x.copy(); xt[dd] = np.clip(xt[dd] + pull, lb[dd], ub[dd])\n                ft = safe_eval(xt); trials[dd] += 1\n                if ft < f:\n                    x, f = xt.copy(), float(ft); succ[dd] += 1; steps[dd] = min(max_step, steps[dd]*self.succ_mult)\n                    archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                    no_improve = 0\n                else:\n                    no_improve += 1\n                    # small random local nudge\n                    if evals < self.budget:\n                        j = rng.choice(dim)\n                        xt = x.copy(); xt[j] = np.clip(xt[j] + rng.normal(0, 0.5*steps[j]), lb[j], ub[j])\n                        ft = safe_eval(xt); trials[j] += 1\n                        if ft < f:\n                            x, f = xt.copy(), float(ft); succ[j] += 1; steps[j] = min(max_step, steps[j]*self.succ_mult)\n                            archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                            no_improve = 0\n\n        # final cheap coordinate polish\n        fin = self.polish_frac * scale\n        while evals < self.budget and fin > 1e-12 * scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d]); ft = safe_eval(xt)\n                if ft < f: x, f, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d]); ft = safe_eval(xt)\n                if ft < f: x, f, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 21, "feedback": "The algorithm ParabolicAdaptiveMirroredDescent scored 0.640 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["86d13d75-4636-4c2f-9196-d0f90890a1f4"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9929239664938879}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.986080576831517}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9896147700107332}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9613317488873879}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9564790118709934}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9629256696350057}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1386666538270005}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.12100889333398868}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.12889944907917272}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1148286211666607}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.150279781989746}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.10848082753521171}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.996605666483348}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9964212852202139}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9963974331466523}], "aucs": [0.9929239664938879, 0.986080576831517, 0.9896147700107332, 0.9613317488873879, 0.9564790118709934, 0.9629256696350057, 0.1386666538270005, 0.12100889333398868, 0.12889944907917272, 0.1148286211666607, 0.150279781989746, 0.10848082753521171, 0.996605666483348, 0.9964212852202139, 0.9963974331466523]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2434.0, "Edges": 2433.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.999178307313065, "Degree Variance": 2.074773359332221, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.648868778280542, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3242204612648334, "Depth Entropy": 2.1966336101319843, "Assortativity": 0.0, "Average Eccentricity": 17.371815940838125, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0004108463434675431, "Average Shortest Path": 10.64567922373851, "mean_complexity": 12.666666666666666, "total_complexity": 38.0, "mean_token_count": 701.3333333333334, "total_token_count": 2104.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "5a92d483-80df-44ea-93f3-b78b9fe756e9", "fitness": 0.432517869317348, "name": "AnnealedRotationalDiagonalSearch", "description": "Annealed Rotational Diagonal Search (ARDS) \u2014 maintain per-coordinate Gaussian scales (annealed multiplicatively by local successes), sample multivariate diagonal Gaussians around the best, occasionally probe leading PCA directions from a tiny archive for low-rank informed moves, and use rare Cauchy jumps and light coordinate polish for robust escapes and exploitation.", "code": "import numpy as np\n\nclass AnnealedRotationalDiagonalSearch:\n    def __init__(self, budget, dim, init_sigma=0.12, success_mult=1.18,\n                 failure_mult=0.82, cauchy_prob=0.03, rotate_period=15,\n                 archive_size=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_sigma = float(init_sigma)\n        self.success_mult = float(success_mult)\n        self.failure_mult = float(failure_mult)\n        self.cauchy_prob = float(cauchy_prob)\n        self.rotate_period = int(rotate_period)\n        self.archive_size = int(archive_size)\n        self.rng = np.random.default_rng(seed)\n\n    def _bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        dim = self.dim\n        rng = self.rng\n        span = ub - lb\n        box_len = float(np.linalg.norm(span))\n        base_sigma = max(1e-12, self.init_sigma * box_len)\n        min_sigma = max(1e-15, 1e-9 * box_len)\n        max_sigma = max(1e-12, 2.0 * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        # init\n        x = rng.uniform(lb, ub)\n        f = safe_eval(x)\n        x_best, f_best = x.copy(), float(f)\n\n        sigma = np.full(dim, base_sigma, float)\n        succ_count = np.zeros(dim, int)\n        fail_count = np.zeros(dim, int)\n        archive_x = [x_best.copy()]\n        it = 0\n\n        while evals < self.budget:\n            # occasional heavy-tail global escape\n            if rng.random() < self.cauchy_prob and evals < self.budget:\n                jump = 0.08 * box_len * rng.standard_cauchy(dim)\n                xt = np.clip(x_best + jump, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft)\n                    archive_x.insert(0, x_best.copy())\n                # continue main loop\n\n            # diagonal Gaussian sample around best\n            z = rng.normal(0.0, 1.0, dim)\n            step = z * sigma\n            xt = np.clip(x_best + step, lb, ub)\n            ft = safe_eval(xt)\n            if ft < f_best:\n                # success: amplify sigma more in dimensions that contributed\n                rel = (f_best - ft) / (abs(f_best) + 1e-12)\n                gain = 1.0 + min(3.0, 3.0 * rel)\n                contrib = np.abs(step) / (sigma + 1e-18)\n                factor = (self.success_mult ** 0.6) * (1.0 + 0.5 * contrib)\n                sigma = np.minimum(max_sigma, sigma * (factor ** 1.0) * gain**0.25)\n                succ_count += (contrib > 0.35)\n                fail_count *= 0\n                x_best, f_best = xt.copy(), float(ft)\n                archive_x.insert(0, x_best.copy())\n            else:\n                # failure: shrink sigma, slightly more for dims that moved\n                contrib = np.abs(step) / (sigma + 1e-18)\n                shrink = self.failure_mult ** (1.0 + 0.6 * contrib)\n                sigma = np.maximum(min_sigma, sigma * shrink)\n                fail_count += (contrib > 0.4)\n\n            # keep archive small and diverse (unique-ish)\n            if len(archive_x) > self.archive_size:\n                archive_x = archive_x[:self.archive_size]\n\n            # occasional low-rank informed probe using archive PCA\n            if (it % max(1, self.rotate_period) == 0) and len(archive_x) >= 3 and evals < self.budget:\n                A = np.asarray(archive_x)\n                Amean = A.mean(axis=0)\n                M = A - Amean\n                try:\n                    # compute dominant direction\n                    u, s, vh = np.linalg.svd(M, full_matrices=False)\n                    v1 = vh[0]\n                    scale = 0.8 * np.mean(s[:2]) if s.size >= 2 else 0.8 * (np.linalg.norm(s) + 1e-12)\n                    dir_vec = v1 / (np.linalg.norm(v1) + 1e-18)\n                    # mirrored probes along dir_vec\n                    for coef in (1.0, -1.0):\n                        xt = np.clip(x_best + coef * dir_vec * (scale + 0.5 * np.mean(sigma)), lb, ub)\n                        ft = safe_eval(xt)\n                        if ft < f_best:\n                            # reward dims aligned with dir\n                            contrib = np.abs(dir_vec) > 0.2\n                            sigma[contrib] = np.minimum(max_sigma, sigma[contrib] * self.success_mult)\n                            x_best, f_best = xt.copy(), float(ft)\n                            archive_x.insert(0, x_best.copy())\n                            break\n                except Exception:\n                    pass\n\n            it += 1\n\n        # light coordinate polish (small deterministic sweeps)\n        step0 = max(1e-12, 0.03 * box_len)\n        final = step0\n        while evals < self.budget and final > 1e-12:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] + final, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[d] = np.clip(xt[d] - final, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True\n            if not improved:\n                final *= 0.5\n\n        self.x_opt = np.asarray(x_best, float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 21, "feedback": "The algorithm AnnealedRotationalDiagonalSearch scored 0.433 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c261e7a-6216-4d7e-8792-9c697206d406"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8771957840568512}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.7878097513597553}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7812855738425233}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.5343445882334636}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.5867741222579298}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.5776589790544866}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.027249405307131713}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.8774001055864337}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.5610151336069322}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.876784596454714}], "aucs": [0.8771957840568512, 0.7878097513597553, 0.7812855738425233, 0.5343445882334636, 0.5867741222579298, 0.5776589790544866, 4.999999999999449e-05, 0.027249405307131713, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.8774001055864337, 0.5610151336069322, 0.876784596454714]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1468.0, "Edges": 1467.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9986376021798364, "Degree Variance": 2.0980907869239505, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 7.736214605067064, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3293166709170647, "Depth Entropy": 2.0835765553645964, "Assortativity": 0.0, "Average Eccentricity": 17.924386920980925, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0006811989100817438, "Average Shortest Path": 9.936057386016431, "mean_complexity": 7.75, "total_complexity": 31.0, "mean_token_count": 327.75, "total_token_count": 1311.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "1b880071-3b3d-4fd2-9ea0-b9e29b236c4f", "fitness": 0.6274521225480358, "name": "DirectionalThompsonV3", "description": "Directional Thompson with mirrored curvature boosts and tiny median-archive \u2014 per-coordinate Beta-thompson picks a promising sign, occasionally mirrors probes to infer curvature and adapt step multipliers, uses a tiny best-archive median pull and Cauchy escapes for robust cheap 1-D exploitation.", "code": "import numpy as np\n\nclass DirectionalThompsonV3:\n    def __init__(self, budget, dim, init_step=0.12, success_mult=1.28,\n                 failure_mult=0.66, global_prob=0.03, archive_size=3,\n                 mirror_prob=0.25, median_period=18, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.init_step_frac = float(init_step)\n        self.succ_m = float(success_mult); self.fail_m = float(failure_mult)\n        self.global_prob = float(global_prob); self.mirror_p = float(mirror_prob)\n        self.arch_size = int(archive_size); self.median_period = int(median_period)\n        self.rng = np.random.default_rng(seed)\n\n    def _bounds(self, func):\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0); ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func); d = self.dim; rng = self.rng\n        rng2 = rng\n        span = ub - lb; box = np.linalg.norm(span)\n        base = max(1e-12, self.init_step_frac * box)\n        min_s = max(1e-15, 1e-9 * box); max_s = max(1e-12, 1.5 * box)\n\n        evals = 0\n        def evalf(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        x = rng.uniform(lb, ub); f = evalf(x)\n        x_best, f_best = x.copy(), float(f)\n        archive = [(f_best, x_best.copy())]\n\n        steps = np.full(d, base, float)\n        a_pos = np.ones(d); b_pos = np.ones(d)\n        a_neg = np.ones(d); b_neg = np.ones(d)\n        trials = np.zeros(d, int)\n        last_dir = np.zeros(d, int)\n        consec_fail = np.zeros(d, int)\n\n        it = 0\n        while evals < self.budget:\n            if rng.random() < self.global_prob and evals < self.budget:\n                jump = 0.12 * box * rng.standard_cauchy(d)\n                xg = np.clip(x_best + jump, lb, ub)\n                fg = evalf(xg)\n                if fg < f_best:\n                    archive.append((fg, xg.copy()))\n                    archive = sorted(archive)[:self.arch_size]\n                    x_best, f_best = xg.copy(), fg\n\n            if (it % max(1, self.median_period) == 0) and len(archive) > 1:\n                pts = np.stack([p for _, p in archive], 0)\n                xm = np.median(pts, axis=0)\n                xm = 0.7 * x_best + 0.3 * xm\n                fm = evalf(xm)\n                if fm < f_best:\n                    archive.append((fm, xm.copy())); archive = sorted(archive)[:self.arch_size]\n                    x_best, f_best = xm.copy(), fm\n\n            ppos = rng2.beta(a_pos, b_pos); pneg = rng2.beta(a_neg, b_neg)\n            upos = ppos * (1 - ppos); uneg = pneg * (1 - pneg)\n            head_up = (ub - x_best) / (span + 1e-18); head_dn = (x_best - lb) / (span + 1e-18)\n            eff_pos = ppos * (0.55 + 0.45 * np.clip(head_up, 0, 1)) + 0.9 * upos\n            eff_neg = pneg * (0.55 + 0.45 * np.clip(head_dn, 0, 1)) + 0.9 * uneg\n            mom = 0.15 * (last_dir > 0); momn = 0.15 * (last_dir < 0)\n            score_pos = eff_pos * (steps / (np.abs(steps).mean() + 1e-12)) + mom + 1e-11 / (1 + np.sqrt(trials))\n            score_neg = eff_neg * (steps / (np.abs(steps).mean() + 1e-12)) + momn + 1e-11 / (1 + np.sqrt(trials))\n            choose_pos = score_pos >= score_neg\n            cand = np.where(choose_pos, score_pos, score_neg)\n            dim = int(np.argmax(cand)); sign = 1 if choose_pos[dim] else -1\n\n            s = float(np.clip(steps[dim], min_s, max_s))\n            do_mirror = (rng.random() < self.mirror_p) and (evals + 2 <= self.budget)\n            if do_mirror:\n                xtp = x_best.copy(); xtm = x_best.copy()\n                xtp[dim] = np.clip(xtp[dim] + s, lb[dim], ub[dim])\n                xtm[dim] = np.clip(xtm[dim] - s, lb[dim], ub[dim])\n                ftp = evalf(xtp); ftm = evalf(xtm); trials[dim] += 2\n                if ftp < ftm:\n                    ft, xt, sgn = ftp, xtp, 1\n                else:\n                    ft, xt, sgn = ftm, xtm, -1\n            else:\n                xt = x_best.copy(); xt[dim] = np.clip(xt[dim] + sign * s, lb[dim], ub[dim])\n                ft = evalf(xt); trials[dim] += 1; sgn = sign\n\n            if ft < f_best:\n                rel = (f_best - ft) / (abs(f_best) + 1e-12)\n                bonus = 1.0 + min(3.0, rel * 3.5)\n                if sgn > 0: a_pos[dim] += 1.0 + bonus\n                else:       a_neg[dim] += 1.0 + bonus\n                archive.append((ft, xt.copy())); archive = sorted(archive)[:self.arch_size]\n                x_best, f_best = xt.copy(), float(ft)\n                steps[dim] = min(max_s, steps[dim] * (self.succ_m ** (0.9 + 0.1 * bonus)))\n                consec_fail[dim] = 0\n                last_dir[dim] = sgn\n            else:\n                if sgn > 0: b_pos[dim] += 1.0\n                else:       b_neg[dim] += 1.0\n                steps[dim] = max(min_s, steps[dim] * self.fail_m)\n                consec_fail[dim] += 1\n                last_dir[dim] = -sgn\n\n            if consec_fail[dim] >= 7:\n                steps[dim] = max(min_s, steps[dim] * (self.fail_m ** 1.6))\n                if sgn > 0: b_pos[dim] += 0.6\n                else:       b_neg[dim] += 0.6\n                consec_fail[dim] = 0\n                if rng.random() < 0.5 and evals < self.budget:\n                    j = 0.06 * box * rng.standard_cauchy(d)\n                    xj = np.clip(x_best + j, lb, ub); fj = evalf(xj)\n                    if fj < f_best: archive.append((fj, xj.copy())); archive = sorted(archive)[:self.arch_size]; x_best, f_best = xj.copy(), fj\n\n            if trials[dim] > 80:\n                for arr in (a_pos, b_pos, a_neg, b_neg):\n                    arr[dim] = 1.0 + (arr[dim] - 1.0) * 0.82\n                trials[dim] = int(trials[dim] * 0.55)\n\n            it += 1\n\n        # lightweight polish\n        final = max(1e-12, 0.035 * box)\n        while evals < self.budget and final > 1e-12:\n            improved = False\n            for j in range(d):\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[j] = np.clip(xt[j] + final, lb[j], ub[j])\n                ft = evalf(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = x_best.copy(); xt[j] = np.clip(xt[j] - final, lb[j], ub[j])\n                ft = evalf(xt)\n                if ft < f_best:\n                    x_best, f_best = xt.copy(), float(ft); improved = True\n            if not improved: final *= 0.5\n\n        self.x_opt = np.asarray(x_best, float); self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 21, "feedback": "The algorithm DirectionalThompsonV3 scored 0.627 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c261e7a-6216-4d7e-8792-9c697206d406"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9866136406005681}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9866275302221936}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9872590885138084}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9736023977221242}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9732161874197702}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9741684136925558}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.11496371547311779}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.11505496560537831}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.07934496660930113}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.09171918750544306}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.06757526048688756}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0718565292629515}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.997244977769307}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9965664511609187}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9959685261762125}], "aucs": [0.9866136406005681, 0.9866275302221936, 0.9872590885138084, 0.9736023977221242, 0.9732161874197702, 0.9741684136925558, 0.11496371547311779, 0.11505496560537831, 0.07934496660930113, 0.09171918750544306, 0.06757526048688756, 0.0718565292629515, 0.997244977769307, 0.9965664511609187, 0.9959685261762125]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2170.0, "Edges": 2169.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9990783410138249, "Degree Variance": 2.315206523816603, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 7.707, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3220905488688768, "Depth Entropy": 1.9427530769384938, "Assortativity": 1.5025644256834628e-08, "Average Eccentricity": 17.06036866359447, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0004608294930875576, "Average Shortest Path": 9.604924437985607, "mean_complexity": 10.0, "total_complexity": 40.0, "mean_token_count": 477.75, "total_token_count": 1911.0, "mean_parameter_count": 4.0, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "aac84217-cc4c-43b3-8f3b-8faf3b79ee82", "fitness": 0.6189812737902313, "name": "DirectionalThompsonV3", "description": "Directional Thompson with compact archive \u2014 compact Beta sign priors drive per-coordinate Thompson draws combined with adaptive multiplicative step-sizes, a tiny best-archive median pull, lightweight momentum and rare Cauchy escapes for robust cheap 1\u2011D exploitation.", "code": "import numpy as np\n\nclass DirectionalThompsonV3:\n    def __init__(self, budget, dim, init_step=0.12, success_mult=1.28, failure_mult=0.72,\n                 escape_prob=0.03, median_period=18, archive_k=3, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.init_step = float(init_step); self.succ = float(success_mult); self.fail = float(failure_mult)\n        self.escape_prob = float(escape_prob); self.median_period = int(median_period)\n        self.k = int(archive_k); self.rng = np.random.default_rng(seed)\n\n    def _bounds(self, f):\n        if hasattr(f, \"bounds\") and hasattr(f.bounds, \"lb\"):\n            lb = np.asarray(f.bounds.lb, float); ub = np.asarray(f.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, lb); ub = np.full(self.dim, ub)\n        else:\n            lb = np.full(self.dim, -5.0); ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func); dim = self.dim; rng = self.rng\n        L = np.linalg.norm(ub - lb); base = max(1e-12, self.init_step * L)\n        min_s = max(1e-15, 1e-9 * L); max_s = max(1e-12, 1.5 * L)\n        evals = 0\n        def eval_(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        # init\n        x = rng.uniform(lb, ub); fx = eval_(x)\n        xbest, fbest = x.copy(), float(fx)\n        # archive of best points (kept sorted)\n        arch_x = [xbest.copy()]; arch_f = [fbest]\n\n        steps = np.full(dim, base)\n        a_p = np.ones(dim); b_p = np.ones(dim); a_n = np.ones(dim); b_n = np.ones(dim)\n        trials = np.zeros(dim, int); last = np.zeros(dim, int); consec = np.zeros(dim, int)\n\n        it = 0\n        while evals < self.budget:\n            # rare heavy-tail escape\n            if rng.random() < self.escape_prob and evals < self.budget:\n                jump = 0.10 * L * rng.standard_cauchy(dim)\n                xg = np.clip(xbest + jump, lb, ub); fg = eval_(xg)\n                if fg < fbest:\n                    xbest, fbest = xg.copy(), float(fg)\n                    # update archive\n                    arch_x.append(xbest.copy()); arch_f.append(fbest)\n                    if len(arch_x) > self.k: \n                        i = np.argmax(arch_f); del arch_x[i]; del arch_f[i]\n\n            # compact median pull toward archive median\n            if (it % max(1, self.median_period) == 0) and len(arch_x) > 1:\n                xm = np.median(np.vstack(arch_x), axis=0)\n                fm = eval_(xm)\n                if fm < fbest:\n                    arch_x.append(xbest.copy()); arch_f.append(fbest)\n                    xbest, fbest = xm.copy(), float(fm)\n                    if len(arch_x) > self.k:\n                        i = np.argmax(arch_f); del arch_x[i]; del arch_f[i]\n\n            # Thompson sign draws and scoring\n            ppos = rng.beta(a_p, b_p); pneg = rng.beta(a_n, b_n)\n            var_pos = ppos*(1-ppos); var_neg = pneg*(1-pneg)\n            head_ub = (ub - xbest) / (ub - lb + 1e-18); head_lb = (xbest - lb) / (ub - lb + 1e-18)\n            eff_pos = ppos*(0.6 + 0.4*np.clip(head_ub,0,1)) + 0.9*var_pos\n            eff_neg = pneg*(0.6 + 0.4*np.clip(head_lb,0,1)) + 0.9*var_neg\n            mom = 0.14*(last>0); momn = 0.14*(last<0)\n            score_pos = eff_pos*steps + mom + 1e-11/(1+np.sqrt(trials))\n            score_neg = eff_neg*steps + momn + 1e-11/(1+np.sqrt(trials))\n            choose_pos = score_pos >= score_neg\n            cand = np.where(choose_pos, score_pos, score_neg)\n            d = int(np.argmax(cand)); sign = 1 if choose_pos[d] else -1\n\n            # decide mirrored probe sometimes if uncertain\n            step = float(np.clip(steps[d], min_s, max_s))\n            do_mirror = (rng.random() < 0.08) and (evals + 2 <= self.budget)\n            if do_mirror:\n                xt1 = xbest.copy(); xt2 = xbest.copy()\n                xt1[d] = np.clip(xt1[d] + step, lb[d], ub[d]); xt2[d] = np.clip(xt2[d] - step, lb[d], ub[d])\n                f1 = eval_(xt1); f2 = eval_(xt2)\n                trials[d] += 2\n                # pick best of three including center\n                if f1 < fbest or f2 < fbest:\n                    if f1 < f2:\n                        chosen, fch = xt1, f1\n                        sgn = 1\n                    else:\n                        chosen, fch = xt2, f2\n                        sgn = -1\n                    rel = (fbest - fch) / (abs(fbest) + 1e-12)\n                    bonus = 1.0 + min(3.0, rel*4.0)\n                    if sgn > 0: a_p[d] += 1+bonus\n                    else: a_n[d] += 1+bonus\n                    steps[d] = min(max_s, steps[d]*(self.succ ** (0.9+0.1*bonus)))\n                    arch_x.append(xbest.copy()); arch_f.append(fbest)\n                    xbest, fbest = chosen.copy(), float(fch)\n                    consec[d]=0; last[d]=sgn\n                else:\n                    # failure\n                    if sign>0: b_p[d]+=1.0\n                    else: b_n[d]+=1.0\n                    steps[d] = max(min_s, steps[d]*self.fail)\n                    consec[d]+=1; last[d]=-sign\n            else:\n                xt = xbest.copy(); xt[d] = np.clip(xt[d] + sign*step, lb[d], ub[d])\n                ft = eval_(xt); trials[d]+=1\n                if ft < fbest:\n                    rel = (fbest - ft) / (abs(fbest) + 1e-12)\n                    bonus = 1.0 + min(3.0, rel*4.0)\n                    if sign>0: a_p[d]+=1+bonus\n                    else: a_n[d]+=1+bonus\n                    steps[d] = min(max_s, steps[d]*(self.succ ** (0.9+0.1*bonus)))\n                    arch_x.append(xbest.copy()); arch_f.append(fbest)\n                    xbest, fbest = xt.copy(), float(ft)\n                    consec[d]=0; last[d]=sign\n                else:\n                    if sign>0: b_p[d]+=1.0\n                    else: b_n[d]+=1.0\n                    steps[d] = max(min_s, steps[d]*self.fail)\n                    consec[d]+=1; last[d]=-sign\n\n            # corrective nudges\n            if consec[d] >= 7:\n                steps[d] = max(min_s, steps[d]*(self.fail**1.6))\n                if sign>0: b_p[d]+=0.5\n                else: b_n[d]+=0.5\n                consec[d]=0\n\n            # mild decay\n            if trials[d] > 70:\n                for arr in (a_p,b_p,a_n,b_n): arr[d] = 1.0 + (arr[d]-1.0)*0.82\n                trials[d] = int(trials[d]*0.6)\n            # maintain small archive\n            if len(arch_x) > self.k:\n                i = np.argmax(arch_f); del arch_x[i]; del arch_f[i]\n            it += 1\n\n        # small symmetric polish\n        final = max(1e-12, 0.035*L)\n        while evals < self.budget and final > 1e-12:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = xbest.copy(); xt[d] = np.clip(xt[d] + final, lb[d], ub[d]); ft = eval_(xt)\n                if ft < fbest: xbest, fbest = xt.copy(), float(ft); improved = True; continue\n                if evals >= self.budget: break\n                xt = xbest.copy(); xt[d] = np.clip(xt[d] - final, lb[d], ub[d]); ft = eval_(xt)\n                if ft < fbest: xbest, fbest = xt.copy(), float(ft); improved = True\n            if not improved: final *= 0.5\n\n        self.x_opt = np.asarray(xbest, float); self.f_opt = float(fbest)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 21, "feedback": "The algorithm DirectionalThompsonV3 scored 0.619 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c261e7a-6216-4d7e-8792-9c697206d406"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9850596473391149}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9846931910706832}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.982251455873542}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9726152087059845}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9731591692397835}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9719012471771815}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.07426329730410874}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.0955721324108808}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.05834037790853164}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.08497650666215029}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.060092032088791325}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.05209800115318486}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9970404824702336}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9960222450431788}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9966341124061191}], "aucs": [0.9850596473391149, 0.9846931910706832, 0.982251455873542, 0.9726152087059845, 0.9731591692397835, 0.9719012471771815, 0.07426329730410874, 0.0955721324108808, 0.05834037790853164, 0.08497650666215029, 0.060092032088791325, 0.05209800115318486, 0.9970404824702336, 0.9960222450431788, 0.9966341124061191]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2271.0, "Edges": 2270.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.9991193306913253, "Degree Variance": 2.2774100566540656, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.896091515729266, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3156713049455118, "Depth Entropy": 1.9326550371900353, "Assortativity": 0.0, "Average Eccentricity": 15.114487010127696, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.00044033465433729633, "Average Shortest Path": 9.618906844973104, "mean_complexity": 10.25, "total_complexity": 41.0, "mean_token_count": 495.0, "total_token_count": 1980.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "c5586cc3-c212-4748-a41b-1284fc3ca29e", "fitness": 0.06335365419060571, "name": "CorrelationAwareSubspaceProbing", "description": "Correlation-Aware Subspace Probing (CASP) \u2014 adapt per-coordinate steps from a tiny archive, estimate coordinate correlations to pick informative 1D/2D subspaces, perform cheap mirrored subspace probes (with curvature-driven extrapolation), and combine multiplicative step adaptation with occasional Levy escapes for robust exploitation of separable and mildly nonseparable landscapes.", "code": "import numpy as np\n\nclass CorrelationAwareSubspaceProbing:\n    def __init__(self, budget, dim, init_frac=0.11, succ=1.25, fail=0.72,\n                 global_jump_prob=0.04, subspace2_prob=0.33, archive_size=5, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_frac = float(init_frac)\n        self.succ = float(succ); self.fail = float(fail)\n        self.global_jump_prob = float(global_jump_prob)\n        self.sub2 = float(subspace2_prob)\n        self.archive_size = int(archive_size)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        # bounds (default -5..5)\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        d = self.dim\n        box = ub - lb\n        box_len = np.linalg.norm(box)\n        base = max(1e-12, self.init_frac * box_len)\n        min_step = max(1e-15, 1e-9 * box_len); max_step = max(1e-12, 2.0 * box_len)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        rng = self.rng\n        x = rng.uniform(lb, ub)\n        f = safe_eval(x)\n        x_best = x.copy(); f_best = float(f)\n        archive = [(x_best.copy(), f_best)]\n\n        steps = np.full(d, base, float)\n        trials = np.zeros(d, int)\n        last_move = np.zeros(d, float)\n        it = 0\n\n        # small helper to push into archive (keep best-first)\n        def add_archive(xp, fp):\n            archive.append((xp.copy(), float(fp)))\n            archive.sort(key=lambda z: z[1])\n            if len(archive) > self.archive_size:\n                archive.pop()\n\n        while evals < self.budget:\n            # occasional heavy-tailed global escape (Levy/Cauchy)\n            if rng.random() < self.global_jump_prob and evals < self.budget:\n                c = rng.standard_cauchy(d)\n                scale = 0.07 * box_len\n                xg = np.clip(x_best + scale * c, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f_best:\n                    add_archive(x_best, f_best)\n                    x_best, f_best = xg.copy(), float(fg)\n                    last_move = x_best - x_best  # reset small\n\n            # compute simple statistics from archive\n            pts = np.stack([p for p, _ in archive])\n            if pts.shape[0] >= 2:\n                m = pts.mean(axis=0)\n                cov = np.cov(pts.T, bias=True) if pts.shape[0] > 1 else np.zeros((d, d))\n                var = np.maximum(1e-16, np.diag(cov))\n            else:\n                m = pts[0].copy()\n                var = np.full(d, 1e-12)\n\n            # selection weights: prefer coords with variance (interaction signal) and under-tried\n            w = var + 1e-16\n            w = w / (w.max() + 1e-16)\n            w = w / (1.0 + 0.6 * np.sqrt(trials + 1.0))\n            if w.sum() <= 0:\n                probs = np.ones(d) / d\n            else:\n                probs = np.maximum(0, w)\n                probs = probs / probs.sum()\n\n            # decide 1D or 2D probe\n            use_2d = (rng.random() < self.sub2) and (d > 1)\n            if use_2d and evals + 2 <= self.budget:\n                # pick two coords (without replacement) by probs\n                idxs = rng.choice(d, size=2, replace=False, p=probs)\n                i, j = int(idxs[0]), int(idxs[1])\n                si = float(np.clip(steps[i], min_step, max_step))\n                sj = float(np.clip(steps[j], min_step, max_step))\n                s = 0.9 * (si + sj) / 2.0\n                # build orthonormal 2D direction (random rotation in plane)\n                theta = rng.uniform(0, 2 * np.pi)\n                v = np.zeros(d)\n                v[i] = np.cos(theta); v[j] = np.sin(theta)\n                v = v / (np.linalg.norm(v) + 1e-16)\n                xp = np.clip(x_best + s * v, lb, ub)\n                xm = np.clip(x_best - s * v, lb, ub)\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[i] += 1; trials[j] += 1\n\n                # curvature estimate along v\n                a = (fp + fm - 2.0 * f_best) / (2.0 * s * s + 1e-20)\n                if (fp < f_best) or (fm < f_best):\n                    # accept best side\n                    if fp < fm:\n                        chosen_x, chosen_f, sign = xp, fp, 1.0\n                    else:\n                        chosen_x, chosen_f, sign = xm, fm, -1.0\n                    add_archive(x_best, f_best)\n                    x_best, f_best = chosen_x.copy(), float(chosen_f)\n                    # boost steps along involved dims\n                    steps[i] = min(max_step, steps[i] * self.succ)\n                    steps[j] = min(max_step, steps[j] * self.succ)\n                    last_move = chosen_x - x_best\n                else:\n                    # if valley (a < 0) try small extrapolation to 2s\n                    if a < 0 and evals + 1 <= self.budget:\n                        x2 = np.clip(x_best + 2.0 * s * np.sign(fp - fm or 1.0) * v, lb, ub)\n                        f2 = safe_eval(x2); trials[i] += 1; trials[j] += 1\n                        if f2 < f_best:\n                            add_archive(x_best, f_best)\n                            x_best, f_best = x2.copy(), float(f2)\n                            steps[i] = min(max_step, steps[i] * self.succ)\n                            steps[j] = min(max_step, steps[j] * self.succ)\n                            last_move = x2 - x_best\n                        else:\n                            steps[i] = max(min_step, steps[i] * self.fail)\n                            steps[j] = max(min_step, steps[j] * self.fail)\n                    else:\n                        # both sides worse -> shrink\n                        steps[i] = max(min_step, steps[i] * self.fail)\n                        steps[j] = max(min_step, steps[j] * self.fail)\n                it += 1\n                continue\n\n            # otherwise do an axis-aligned 1D probe (cheap)\n            # choose coordinate by sampling probs\n            idx = int(rng.choice(d, p=probs))\n            step = float(np.clip(steps[idx], min_step, max_step))\n            # sign biased by archive median direction on that coord\n            med = np.median(pts[:, idx])\n            bias = np.sign(med - x_best[idx])\n            if bias == 0:\n                bias = 1 if rng.random() < 0.5 else -1\n            sign = int(bias)\n            xt = x_best.copy()\n            xt[idx] = np.clip(xt[idx] + sign * step, lb[idx], ub[idx])\n            ft = safe_eval(xt)\n            trials[idx] += 1\n\n            if ft < f_best:\n                add_archive(x_best, f_best)\n                x_best, f_best = xt.copy(), float(ft)\n                steps[idx] = min(max_step, steps[idx] * self.succ)\n                last_move = xt - x_best\n            else:\n                steps[idx] = max(min_step, steps[idx] * self.fail)\n                # small randomized reflection: try opposite side rarely\n                if rng.random() < 0.12 and evals < self.budget:\n                    xt2 = x_best.copy()\n                    xt2[idx] = np.clip(xt2[idx] - sign * step * 0.8, lb[idx], ub[idx])\n                    ft2 = safe_eval(xt2)\n                    trials[idx] += 1\n                    if ft2 < f_best:\n                        add_archive(x_best, f_best)\n                        x_best, f_best = xt2.copy(), float(ft2)\n                        steps[idx] = min(max_step, steps[idx] * self.succ)\n\n            # gentle per-dim trial decay to avoid overconfidence\n            trials[idx] = int(max(0, trials[idx] - (1 if rng.random() < 0.01 else 0)))\n\n            # modest annealing: occasional coordinate re-scaling from archive spread\n            if it % max(1, 12) == 0 and len(archive) >= 2:\n                spread = np.maximum(1e-12, np.std(np.stack([p for p, _ in archive]), axis=0))\n                # nudge steps towards spread*factor\n                target = 0.9 * spread + 0.1 * steps\n                steps = np.clip(0.7 * steps + 0.3 * target, min_step, max_step)\n\n            it += 1\n\n        # final polish: few directed reductions along principal archive directions\n        final_step = max(1e-12, 0.05 * box_len)\n        while evals < self.budget and final_step > 1e-12:\n            improved = False\n            for i in range(d):\n                if evals >= self.budget: break\n                xp = x_best.copy(); xp[i] = np.clip(xp[i] + final_step, lb[i], ub[i])\n                fp = safe_eval(xp)\n                if fp < f_best:\n                    add_archive(x_best, f_best)\n                    x_best, f_best = xp.copy(), float(fp); improved = True; continue\n                if evals >= self.budget: break\n                xm = x_best.copy(); xm[i] = np.clip(xm[i] - final_step, lb[i], ub[i])\n                fm = safe_eval(xm)\n                if fm < f_best:\n                    add_archive(x_best, f_best)\n                    x_best, f_best = xm.copy(), float(fm); improved = True\n            if not improved:\n                final_step *= 0.5\n\n        self.x_opt = np.asarray(x_best, float)\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 21, "feedback": "The algorithm CorrelationAwareSubspaceProbing scored 0.063 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c85282eb-f8db-492d-b392-cf1c68d7a151"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.05356155187333589}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.06992031895550066}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.07598342645853806}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.1478573133124268}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.28521892006625127}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.317313282193033}], "aucs": [0.05356155187333589, 0.06992031895550066, 0.07598342645853806, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.1478573133124268, 0.28521892006625127, 0.317313282193033]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2420.0, "Edges": 2419.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.9991735537190083, "Degree Variance": 2.119834027730347, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.231607629427794, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3273757002176316, "Depth Entropy": 2.0695056812509875, "Assortativity": 0.0, "Average Eccentricity": 17.53181818181818, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00041322314049586776, "Average Shortest Path": 9.972301921086167, "mean_complexity": 11.75, "total_complexity": 47.0, "mean_token_count": 516.5, "total_token_count": 2066.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "4ede79fe-bcf1-46b5-90c5-770f1a941fad", "fitness": 0.6410240189725038, "name": "ParabolicMirrorCompact", "description": "Compact Parabolic Mirror with Tilted Median Pulls \u2014 per-coordinate mirrored 3-point parabolic fits with multiplicative step adaption, an EMA \"tilt\" to bias probe directions, tiny median-archive pulls and rare heavy-tail escapes for robust cheap 1\u2011D exploitation.", "code": "import numpy as np\n\nclass ParabolicMirrorCompact:\n    def __init__(self, budget=1000, dim=10, init_step=0.08, success=1.25, failure=0.6,\n                 arch_size=6, global_prob=0.035, extrap=1.6, tilt_alpha=0.12,\n                 polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step); self.success = float(success); self.failure = float(failure)\n        self.arch_size = int(arch_size); self.global_prob = float(global_prob)\n        self.extrap = float(extrap); self.tilt_alpha = float(tilt_alpha)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, float(lb))\n            if ub.shape == (): ub = np.full(d, float(ub))\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n        span = np.maximum(1e-12, ub - lb); box = max(1e-12, np.linalg.norm(span))\n        base = max(1e-12, self.init_step * box); min_step = 1e-15 * box; max_step = 1.2 * box\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        x = rng.uniform(lb, ub); fx = safe_eval(x)\n        steps = np.full(d, base); trials = np.zeros(d, int); succ = np.zeros(d, int)\n        tilt = np.zeros(d, float)           # EMA of recent signed moves\n        arch_x = [x.copy()]; arch_f = [fx]\n\n        def push_arch(xx, ff):\n            arch_x.append(xx.copy()); arch_f.append(float(ff))\n            if len(arch_x) > self.arch_size:\n                i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n\n        while evals < self.budget:\n            # rare heavy-tail global escape\n            if rng.random() < self.global_prob:\n                u = rng.random(d)\n                jump = 0.12 * box * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x + jump, lb, ub); fg = safe_eval(xg)\n                if fg < fx:\n                    x, fx = xg.copy(), fg; push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x - xg)\n                continue\n\n            # occasional median-tilt pull from archive\n            if len(arch_x) >= 3 and rng.random() < 0.03:\n                med = np.median(np.vstack(arch_x), axis=0)\n                k = max(1, d//6)\n                idx = np.argsort(-steps)[:k]\n                cand = x.copy()\n                cand[idx] = np.clip(x[idx] + 0.45*(med[idx]-x[idx]) + 0.25*tilt[idx], lb[idx], ub[idx])\n                fc = safe_eval(cand)\n                if fc < fx:\n                    x, fx = cand.copy(), fc; push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x - cand)\n                continue\n\n            # pick coordinate (favor big steps, low trials, tilt)\n            scores = steps/(1.0+0.02*trials) + 0.22*np.abs(tilt) + 1e-7*rng.random(d)\n            i = int(np.argmax(scores))\n            s = float(np.clip(steps[i], min_step, max_step))\n\n            # mirrored probes if possible\n            if evals <= self.budget - 2:\n                xp = x.copy(); xm = x.copy()\n                xp[i] = np.clip(xp[i] + s, lb[i], ub[i]); xm[i] = np.clip(xm[i] - s, lb[i], ub[i])\n                fp = safe_eval(xp); fm = safe_eval(xm); trials[i] += 2\n\n                # if improvement on either side, accept best and maybe extrapolate\n                if (fp < fx) or (fm < fx):\n                    if fp < fm:\n                        old = x.copy(); x, fx = xp.copy(), fp\n                    else:\n                        old = x.copy(); x, fx = xm.copy(), fm\n                    succ[i] += 1; steps[i] = min(max_step, steps[i]*self.success)\n                    # extrapolate\n                    if evals < self.budget:\n                        sign = np.sign(x[i]-old[i]); sign = 1.0 if sign == 0 else sign\n                        ext = x.copy(); ext[i] = np.clip(ext[i] + sign*s*self.extrap, lb[i], ub[i])\n                        fext = safe_eval(ext); trials[i] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext; push_arch(x, fx)\n                    push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x - old)\n                    continue\n\n                # parabola fit: delta = - s*(fp-fm)/(2*(fp+fm-2*fx))\n                denom = (fp + fm - 2.0*fx)\n                delta = -s*(fp - fm)/(2.0*denom) if abs(denom) > 1e-18 else 0.0\n                maxd = s * self.extrap\n                delta = float(np.clip(delta, -maxd, maxd))\n                if abs(delta) > 1e-12:\n                    xp = x.copy(); xp[i] = np.clip(xp[i] + delta, lb[i], ub[i])\n                    fpar = safe_eval(xp); trials[i] += 1\n                    if fpar < fx:\n                        old = x.copy(); x, fx = xp.copy(), fpar\n                        succ[i] += 1; steps[i] = min(max_step, steps[i]*self.success)\n                        push_arch(x, fx)\n                        tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x - old)\n                        continue\n                    else:\n                        steps[i] = max(min_step, steps[i]*self.failure)\n                else:\n                    steps[i] = max(min_step, steps[i]*self.failure)\n\n            else:\n                # last eval: one side try\n                xt = x.copy()\n                if rng.random() < 0.5:\n                    xt[i] = np.clip(xt[i] + s, lb[i], ub[i])\n                else:\n                    xt[i] = np.clip(xt[i] - s, lb[i], ub[i])\n                ft = safe_eval(xt); trials[i] += 1\n                if ft < fx:\n                    old = x.copy(); x, fx = xt.copy(), ft; succ[i] += 1\n                    steps[i] = min(max_step, steps[i]*self.success); push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x - old)\n                else:\n                    steps[i] = max(min_step, steps[i]*self.failure)\n\n            # rejuvenate stale coords\n            if trials[i] > 60 and succ[i] == 0:\n                steps[i] = min(max_step, steps[i] * 1.6); trials[i] = int(trials[i]*0.6)\n\n        # final local polish\n        fin = self.polish_frac * box\n        while evals < self.budget and fin > 1e-12 * box:\n            improved = False\n            for j in range(d):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[j] = np.clip(xt[j] + fin, lb[j], ub[j])\n                ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[j] = np.clip(xt[j] - fin, lb[j], ub[j])\n                ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 21, "feedback": "The algorithm ParabolicMirrorCompact scored 0.641 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad40121a-3566-4558-8b5a-52a04f06f382"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9936622375740061}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9920157699347514}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9929436249866183}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9630340885899419}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9593941039590832}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9565783883668326}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.13857142741251594}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.11451311331080283}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.16788736391543513}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.12876821143055117}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.10864454080272112}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.10896553017509014}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9963269777032432}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9972651537239785}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9967897527019882}], "aucs": [0.9936622375740061, 0.9920157699347514, 0.9929436249866183, 0.9630340885899419, 0.9593941039590832, 0.9565783883668326, 0.13857142741251594, 0.11451311331080283, 0.16788736391543513, 0.12876821143055117, 0.10864454080272112, 0.10896553017509014, 0.9963269777032432, 0.9972651537239785, 0.9967897527019882]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2230.0, "Edges": 2229.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9991031390134528, "Degree Variance": 2.0887884333085323, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.278978388998036, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3234280143389092, "Depth Entropy": 2.046316163971124, "Assortativity": 0.0, "Average Eccentricity": 15.442600896860986, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0004484304932735426, "Average Shortest Path": 10.072709312829055, "mean_complexity": 9.75, "total_complexity": 39.0, "mean_token_count": 477.25, "total_token_count": 1909.0, "mean_parameter_count": 4.25, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "ad9d28ed-2806-47c9-b4ca-e97b1d7b4fc7", "fitness": 0.7666857092196625, "name": "SeparableAdaptiveMedianPull", "description": "Separable Adaptive Median Pull with compact archive, Beta-like sign priors and lightweight momentum \u2014 coordinate-wise pulls toward the archive median with multiplicative step adaptation, mirrored checks, controlled extrapolations and rare Cauchy escapes for robust, cheap 1-D exploitation.", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPull:\n    def __init__(self, budget, dim, archive_size=10, init_step=None, min_step=1e-6,\n                 max_step=None, patience=8, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0 * np.ones(d); ub = 5.0 * np.ones(d)\n        span = ub - lb\n        # steps\n        if self.init_step is None:\n            step = 0.15 * span\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = min(max(3, d + 1), max(1, self.budget // 10), 12)\n        # compact archive arrays\n        arc_x = []\n        arc_f = []\n        for _ in range(n_init):\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n            if evals >= self.budget: break\n        if not arc_x:\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n\n        arc_x = np.vstack(arc_x)\n        arc_f = np.array(arc_f, float)\n        top_k = min(self.archive_size, arc_x.shape[0])\n\n        bi = np.ones(d)  # Beta-like success counts\n        bj = np.ones(d)  # Beta-like failure counts\n        momentum = np.zeros(d)\n        priority = np.zeros(d)\n        no_improve = 0\n\n        idx = int(np.argmin(arc_f))\n        x_best = arc_x[idx].copy(); f_best = float(arc_f[idx])\n\n        def arc_add(xn, fn):\n            nonlocal arc_x, arc_f, top_k\n            fn = float(fn)\n            if arc_x.shape[0] < top_k:\n                arc_x = np.vstack([arc_x, xn.copy()])\n                arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < self.budget:\n            # compute guidance\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0)\n            q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n            # scoring prefers coordinates with nonzero pull, small step, and dispersion\n            s_raw = (abs_pull / (step + 1e-12)) + 0.3 * (iqr / (span + 1e-12)) + 0.12 * priority\n            s_raw = np.maximum(s_raw, 1e-12)\n            # softmax sampling temperature adapts to spread\n            s = s_raw - s_raw.max()\n            temp = 0.9 + 0.25 * np.std(s_raw)\n            probs = np.exp(s / max(1e-6, temp))\n            probs /= probs.sum()\n            coord = int(self.rng.choice(d, p=probs))\n\n            # decide sign using Beta-like bias; prefer pull sign but allow flips\n            p_follow = bi[coord] / (bi[coord] + bj[coord])\n            if abs_pull[coord] > 1e-12:\n                nominal = int(np.sign(pull[coord]))\n            else:\n                nominal = int(np.sign(momentum[coord])) if momentum[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nominal if self.rng.random() < p_follow else -nominal\n\n            # magnitude mixes step with remaining pull to avoid overshoot\n            frac = min(1.0, abs_pull[coord] / (step[coord] + 1e-12))\n            mag = step[coord] * (0.8 + 0.4 * self.rng.random()) * (0.5 + 0.5 * frac)\n            delta = float(sign * mag)\n\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            arc_add(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try\n                improved = True\n                bi[coord] += 1.0\n                priority[coord] = min(8.0, priority[coord] + 1.0)\n                momentum[coord] = 0.65 * momentum[coord] + 0.35 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * 1.22 + 1e-12))\n                # small extrapolation in same direction\n                if evals < self.budget:\n                    ext = 0.45 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    arc_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                        momentum[coord] = 0.8 * momentum[coord] + 0.2 * ext\n                no_improve = 0\n            else:\n                # mirrored probe opposite direction once\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    arc_add(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp\n                        improved = True\n                        bj[coord] += 1.0\n                        priority[coord] = min(8.0, priority[coord] + 0.8)\n                        momentum[coord] = 0.65 * momentum[coord] + 0.35 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.18))\n                        # tiny extrapolation\n                        if evals < self.budget:\n                            ext = 0.35 * (-delta)\n                            x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                            f_ext = float(func(x_ext)); evals += 1\n                            arc_add(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext; x_best = x_ext\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                        no_improve = 0\n                    else:\n                        # both failed: penalize and shrink step\n                        bj[coord] += 0.4\n                        bi[coord] = max(1.0, bi[coord] * 0.995)\n                        priority[coord] = max(-8.0, priority[coord] - 0.5)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.6))\n                        momentum[coord] *= 0.6\n                        no_improve += 1\n                else:\n                    no_improve += 1\n\n            # stagnation handling: targeted jitter and occasional Cauchy escape\n            if no_improve >= self.patience and evals < self.budget:\n                no_improve = 0\n                # pick a few coords weighted by IQR and small step\n                cand = (iqr / (span + 1e-12)) * (1.0 / (step + 1e-12) + 0.1) + 1e-9\n                s = cand - cand.max()\n                w = np.exp(s); w /= w.sum()\n                k = min(3, max(1, d // 6))\n                picks = list(self.rng.choice(d, size=min(d, k), replace=False, p=w))\n                improved_any = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    # mix a big directed jump with randomness\n                    jump = 0.6 * step[c] + 0.35 * span[c] * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    arc_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj\n                        step[c] = float(min(max_step[c], step[c] * 1.25))\n                        priority[c] = min(8.0, priority[c] + 1.0)\n                        momentum[c] = np.sign(jump) * 0.5 * abs(jump)\n                        improved_any = True\n                        break\n                if not improved_any and evals < self.budget:\n                    # rare Cauchy escape (heavy tail)\n                    scale = np.maximum(0.1 * step, 0.05 * span)\n                    jump = np.tan(np.pi * (self.rng.random(d) - 0.5)) * scale * (0.5 + self.rng.random(d))\n                    xj = np.clip(x_best + jump * (self.rng.random(d) < 0.2), lb, ub)\n                    for i in range(d):\n                        if evals >= self.budget: break\n                        if np.allclose(xj[i], x_best[i]): continue\n                        x_tmp = x_best.copy(); x_tmp[i] = xj[i]\n                        fj = float(func(x_tmp)); evals += 1\n                        arc_add(x_tmp, fj)\n                        if fj < f_best:\n                            f_best = fj; x_best = x_tmp\n                            step[i] = float(min(max_step[i], step[i] * 1.2))\n                            priority[i] = min(8.0, priority[i] + 1.0)\n                            momentum[i] = np.sign(xj[i] - x_best[i]) * 0.5 * abs(xj[i] - x_best[i])\n                            break\n\n            # slow decay\n            priority *= 0.995\n            momentum *= 0.992\n\n            # trim archive\n            if arc_x.shape[0] > self.archive_size:\n                order = np.argsort(arc_f)\n                keep = order[:self.archive_size]\n                arc_x = arc_x[keep]\n                arc_f = arc_f[keep]\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 21, "feedback": "The algorithm SeparableAdaptiveMedianPull scored 0.767 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["050b4819-fab9-4601-8cdf-99de9e4152fd"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.97953165777915}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9795902037228112}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9772476012462206}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9586554327511941}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9029082122609394}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9554598366061916}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.41186194465247394}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.7935239569249946}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.4859573313321217}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.4186160860927315}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.16521616259250127}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.4889338600217088}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9951355085774409}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.993211406493216}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9944364372412413}], "aucs": [0.97953165777915, 0.9795902037228112, 0.9772476012462206, 0.9586554327511941, 0.9029082122609394, 0.9554598366061916, 0.41186194465247394, 0.7935239569249946, 0.4859573313321217, 0.4186160860927315, 0.16521616259250127, 0.4889338600217088, 0.9951355085774409, 0.993211406493216, 0.9944364372412413]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2539.0, "Edges": 2538.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9992122883024812, "Degree Variance": 2.2371006004634126, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.60136869118905, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3177211472344332, "Depth Entropy": 2.199865715864789, "Assortativity": 0.0, "Average Eccentricity": 18.25640015754234, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00039385584875935406, "Average Shortest Path": 10.398896210448756, "mean_complexity": 12.666666666666666, "total_complexity": 38.0, "mean_token_count": 738.6666666666666, "total_token_count": 2216.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "0044f57d-77dc-42b6-8b39-72fbd36a98c9", "fitness": 0.6225685096998401, "name": "CurvatureParabolicMirror", "description": "Curvature-weighted Parabolic Mirror with probabilistic coordinate sampling, compact median archive and lightweight momentum \u2014 per-coordinate mirrored probes fit a parabola, take analytic minima when reliable, adapt steps by curvature-weighted multipliers, use rare heavy\u2011tailed escapes and occasional median pulls for guided diversification.", "code": "import numpy as np\n\nclass CurvatureParabolicMirror:\n    def __init__(self, budget=1000, dim=10, init_step=0.08, success=1.35, failure=0.55,\n                 min_step_frac=1e-9, max_step_frac=1.2, global_prob=0.035, extrap=1.8,\n                 arch_size=6, polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step); self.success = float(success); self.failure = float(failure)\n        self.min_step_frac = float(min_step_frac); self.max_step_frac = float(max_step_frac)\n        self.global_prob = float(global_prob); self.extrap = float(extrap)\n        self.arch_size = int(arch_size); self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, float(lb))\n            if ub.shape == (): ub = np.full(d, float(ub))\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n        span = np.maximum(1e-12, ub - lb); box = max(1e-12, np.linalg.norm(span))\n        base = max(1e-12, self.init_step * box); min_step = max(1e-15, self.min_step_frac * box)\n        max_step = max(1e-12, self.max_step_frac * box)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        x = rng.uniform(lb, ub); fx = safe_eval(x)\n        steps = np.full(d, base); trials = np.zeros(d, int); succ = np.zeros(d, int)\n        arch_x = [x.copy()]; arch_f = [fx]\n        mom = np.zeros(d); mom_alpha = 0.12\n\n        while evals < self.budget:\n            # rare heavy-tail global escape (Cauchy-like)\n            if rng.random() < self.global_prob:\n                u = rng.random(d); jump = 0.12 * box * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x + jump, lb, ub); fg = safe_eval(xg)\n                if fg < fx:\n                    x, fx = xg.copy(), fg\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                    mom = (1-mom_alpha)*mom + mom_alpha*(x - xg)\n                continue\n\n            # occasional median pull from archive\n            if len(arch_x) >= 3 and rng.random() < 0.03:\n                med = np.median(np.vstack(arch_x), 0)\n                k = max(1, d//6)\n                idxs = np.argsort(-steps)[:k]\n                cand = x.copy()\n                cand[idxs] = np.clip(x[idxs] + 0.45*(med[idxs]-x[idxs]), lb[idxs], ub[idxs])\n                fc = safe_eval(cand)\n                if fc < fx:\n                    x, fx = cand.copy(), fc\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                    mom = (1-mom_alpha)*mom + mom_alpha*(x - cand)\n                continue\n\n            # probabilistic coordinate selection (favor large step, low success and momentum)\n            score = steps * (1.0 + 0.9/(1+trials)) + 0.35*np.abs(mom) + 1e-6*rng.random(d)\n            probs = score.clip(min=1e-12); probs = probs / probs.sum()\n            coord = int(rng.choice(d, p=probs))\n            s = float(np.clip(steps[coord], min_step, max_step))\n\n            # mirrored probes if budget permits\n            if evals <= self.budget - 2:\n                xp = x.copy(); xm = x.copy()\n                xp[coord] = np.clip(xp[coord] + s, lb[coord], ub[coord])\n                xm[coord] = np.clip(xm[coord] - s, lb[coord], ub[coord])\n                fp = safe_eval(xp); fm = safe_eval(xm); trials[coord] += 2\n\n                # quick take if one side better\n                if (fp < fx) or (fm < fx):\n                    if fp < fm:\n                        old = x.copy(); x, fx = xp.copy(), fp\n                    else:\n                        old = x.copy(); x, fx = xm.copy(), fm\n                    succ[coord] += 1\n                    steps[coord] = min(max_step, steps[coord] * self.success)\n                    # one-step extrapolation\n                    if evals < self.budget:\n                        sign = np.sign(x[coord]-old[coord]); sign = 1.0 if sign == 0 else sign\n                        ext = x.copy(); ext[coord] = np.clip(ext[coord] + sign * s * self.extrap, lb[coord], ub[coord])\n                        fext = safe_eval(ext); trials[coord] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext\n                            arch_x.append(x.copy()); arch_f.append(fx)\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                    mom = (1-mom_alpha)*mom + mom_alpha*(x - old)\n                    continue\n\n                # parabola fit from (-s,0,+s)\n                denom = (fp + fm - 2.0*fx)\n                if abs(denom) > 1e-18:\n                    delta = - s * (fp - fm) / (2.0 * denom)\n                else:\n                    delta = 0.0\n                maxd = s * self.extrap\n                delta = float(np.clip(delta, -maxd, maxd))\n\n                # curvature-weighted trust: stronger curvature -> more aggressive step adaptations\n                curvature = abs(denom) / (abs(fx) + 1e-12)\n                adapt_up = 1.0 + 0.7 * min(5.0, curvature)\n                adapt_down = self.failure\n\n                if abs(delta) > 1e-12 and evals < self.budget:\n                    xpar = x.copy(); xpar[coord] = np.clip(xpar[coord] + delta, lb[coord], ub[coord])\n                    fpar = safe_eval(xpar); trials[coord] += 1\n                    if fpar < fx:\n                        old = x.copy(); x, fx = xpar.copy(), fpar\n                        succ[coord] += 1\n                        steps[coord] = min(max_step, steps[coord] * min(adapt_up, self.success))\n                        arch_x.append(x.copy()); arch_f.append(fx)\n                        if len(arch_x) > self.arch_size:\n                            i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                        mom = (1-mom_alpha)*mom + mom_alpha*(x - old)\n                        continue\n                    else:\n                        steps[coord] = max(min_step, steps[coord] * adapt_down)\n                else:\n                    steps[coord] = max(min_step, steps[coord] * adapt_down)\n\n            else:\n                # last eval: try one side\n                if rng.random() < 0.5:\n                    xt = x.copy(); xt[coord] = np.clip(xt[coord] + s, lb[coord], ub[coord])\n                else:\n                    xt = x.copy(); xt[coord] = np.clip(xt[coord] - s, lb[coord], ub[coord])\n                ft = safe_eval(xt); trials[coord] += 1\n                if ft < fx:\n                    old = x.copy(); x, fx = xt.copy(), ft; succ[coord] += 1\n                    steps[coord] = min(max_step, steps[coord] * self.success)\n                    arch_x.append(x.copy()); arch_f.append(fx)\n                    if len(arch_x) > self.arch_size:\n                        i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n                    mom = (1-mom_alpha)*mom + mom_alpha*(x - xt)\n                else:\n                    steps[coord] = max(min_step, steps[coord] * self.failure)\n\n            # rejuvenate stale coordinates\n            if trials[coord] > 80 and succ[coord] == 0:\n                steps[coord] = min(max_step, steps[coord] * 1.6)\n                trials[coord] = int(trials[coord] * 0.6)\n\n        # final polish: small symmetric coordinate probes\n        fin = self.polish_frac * box\n        while evals < self.budget and fin > 1e-12 * box:\n            improved = False\n            for i in range(d):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[i] = np.clip(xt[i] + fin, lb[i], ub[i]); ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[i] = np.clip(xt[i] - fin, lb[i], ub[i]); ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 22, "feedback": "The algorithm CurvatureParabolicMirror scored 0.623 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad40121a-3566-4558-8b5a-52a04f06f382"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9957474033586414}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9912872975187597}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9924433329435852}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9526453693643816}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8421744817086139}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9492320612704125}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.10379361634772577}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.12909491855973343}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.09152864969108265}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10355129685286268}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07918158032858924}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.12093538163577255}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.994732620589377}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9953169795614135}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9968626557666502}], "aucs": [0.9957474033586414, 0.9912872975187597, 0.9924433329435852, 0.9526453693643816, 0.8421744817086139, 0.9492320612704125, 0.10379361634772577, 0.12909491855973343, 0.09152864969108265, 0.10355129685286268, 0.07918158032858924, 0.12093538163577255, 0.994732620589377, 0.9953169795614135, 0.9968626557666502]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2520.0, "Edges": 2519.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9992063492063492, "Degree Variance": 2.0920628621819097, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.41747572815534, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3289619576231044, "Depth Entropy": 2.0509901588962873, "Assortativity": 0.0, "Average Eccentricity": 15.472222222222221, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0003968253968253968, "Average Shortest Path": 10.136565278486676, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 724.0, "total_token_count": 2172.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "20d14fd3-5fd1-401c-861d-6b2d28715a38", "fitness": 0.6154024030181283, "name": "AdaptiveCoordinateTiltedProbing", "description": "Coordinate-guided Tilted Probing with success-weighted steps, compact median-archive pulls and lightweight extrapolations for cheap, robust 1\u2011D exploitation.", "code": "import numpy as np\n\nclass AdaptiveCoordinateTiltedProbing:\n    \"\"\"\n    Refined ACTP: compact, faster and slightly more aggressive/adaptive per-coordinate probing.\n    \"\"\"\n    def __init__(self, budget, dim, archive_size=10, init_step=None, min_step=1e-6, rng=None, patience=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = None if init_step is None else np.asarray(init_step, float)\n        self.min_step = float(min_step)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.patience = int(patience)\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0; ub = 5.0\n        span = ub - lb\n        # initial step per-dim\n        if self.init_step is None:\n            step = np.full(d, 0.15 * span, dtype=float)\n        else:\n            s = self.init_step\n            step = s if s.shape == (d,) else np.full(d, float(s))\n\n        max_step = 0.5 * span\n        step = np.clip(step, self.min_step, max_step)\n\n        top_k = min(self.archive_size, max(2, self.budget // 8))\n        archive_x = []\n        archive_f = []\n        evals = 0\n\n        # seed archive with a few diverse samples\n        n_init = min(max(4, d), max(1, self.budget // 12))\n        for _ in range(n_init):\n            x = self.rng.random(d) * (ub - lb) + lb\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n            if evals >= self.budget:\n                break\n        if len(archive_x) == 0:\n            x = self.rng.random(d) * (ub - lb) + lb\n            archive_x.append(x.copy()); archive_f.append(float(func(x))); evals += 1\n\n        idx = int(np.argmin(archive_f))\n        x_best = archive_x[idx].copy(); f_best = float(archive_f[idx])\n\n        tilt = np.zeros(d)        # EMA of signed successes\n        wins = np.zeros(d)\n        trials = np.ones(d)\n        stagn = 0\n\n        def add_archive(x, f):\n            if len(archive_x) < top_k:\n                archive_x.append(x.copy()); archive_f.append(float(f)); return\n            w = int(np.argmax(archive_f))\n            if f < archive_f[w]:\n                archive_x[w] = x.copy(); archive_f[w] = float(f)\n\n        while evals < self.budget:\n            arr = np.array(archive_x)\n            med = np.median(arr, axis=0)\n            iqr = np.subtract(*np.percentile(arr, [75, 25], axis=0))\n            iqr = np.maximum(iqr, 1e-12)\n            pull = med - x_best\n\n            score = np.abs(pull) / (step + 1e-12)\n            score += 0.5 * (iqr / (span + 1e-12))\n            score += 0.18 * (wins / (trials + 1e-9))\n            s = score - score.max()\n            temp = 1.0 + 0.12 * np.std(score)\n            probs = np.exp(s / temp)\n            probs /= probs.sum()\n            try:\n                c = int(self.rng.choice(d, p=probs))\n            except Exception:\n                c = int(self.rng.integers(0, d))\n\n            # direction: combine median pull, tilt and randomness\n            prefer = 0\n            if abs(pull[c]) > 1e-12:\n                prefer = int(np.sign(pull[c]))\n            use_pref = self.rng.random() < (0.7 if prefer != 0 else 0.5)\n            if use_pref:\n                sign = prefer if prefer != 0 else (1 if self.rng.random() < 0.5 else -1)\n            else:\n                sign = int(np.sign(tilt[c])) if tilt[c] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            if self.rng.random() < 0.08:\n                sign *= -1\n\n            mag = step[c] * (0.35 + 0.65 * self.rng.random())\n            if abs(pull[c]) > 0.8 * step[c]:\n                mag = min(mag, 0.9 * abs(pull[c]))\n\n            x1 = x_best.copy()\n            x1[c] = np.clip(x1[c] + sign * mag, lb, ub)\n            f1 = float(func(x1)); evals += 1\n            add_archive(x1, f1)\n            trials[c] += 1\n\n            improved = False\n            if f1 < f_best:\n                f_best = f1; x_best = x1; improved = True\n                wins[c] += 1\n                tilt[c] = 0.6 * tilt[c] + 0.4 * sign\n                # success-weighted step grow\n                step[c] = float(min(max_step, step[c] * (1.18 + 0.06 * (wins[c] / (trials[c] + 1e-9)))))\n                stagn = 0\n                # modest extrapolation\n                if evals < self.budget and self.rng.random() < 0.38:\n                    ext = 0.45 * mag\n                    x_ext = x_best.copy()\n                    x_ext[c] = np.clip(x_ext[c] + sign * ext, lb, ub)\n                    fext = float(func(x_ext)); evals += 1\n                    add_archive(x_ext, fext)\n                    if fext < f_best:\n                        f_best = fext; x_best = x_ext\n                        step[c] = float(min(max_step, step[c] * 1.08))\n            else:\n                # mirrored probe opposite sign if budget allows\n                if evals < self.budget:\n                    x2 = x_best.copy()\n                    x2[c] = np.clip(x2[c] - sign * mag, lb, ub)\n                    f2 = float(func(x2)); evals += 1\n                    add_archive(x2, f2)\n                    trials[c] += 1\n                    if f2 < f_best:\n                        f_best = f2; x_best = x2; improved = True\n                        wins[c] += 1\n                        tilt[c] = 0.55 * tilt[c] + 0.45 * (-sign)\n                        step[c] = float(min(max_step, step[c] * 1.16))\n                        stagn = 0\n                        if evals < self.budget and self.rng.random() < 0.32:\n                            ext = 0.4 * mag\n                            x_ext = x_best.copy()\n                            x_ext[c] = np.clip(x_ext[c] - sign * ext, lb, ub)\n                            fext = float(func(x_ext)); evals += 1\n                            add_archive(x_ext, fext)\n                            if fext < f_best:\n                                f_best = fext; x_best = x_ext\n                                step[c] = float(min(max_step, step[c] * 1.06))\n                    else:\n                        step[c] = float(max(self.min_step, step[c] * 0.55))\n                        tilt[c] *= 0.68\n                        stagn += 1\n                else:\n                    step[c] = float(max(self.min_step, step[c] * 0.55))\n                    tilt[c] *= 0.68\n                    stagn += 1\n\n            # occasional curvature probe (cheap check)\n            if (evals + 2 <= self.budget) and (self.rng.random() < 0.055):\n                eps = 0.22 * step[c]\n                if eps > 1e-12:\n                    xp = x_best.copy(); xm = x_best.copy()\n                    xp[c] = np.clip(x_best[c] + eps, lb, ub)\n                    xm[c] = np.clip(x_best[c] - eps, lb, ub)\n                    fp = float(func(xp)); fm = float(func(xm)); evals += 2\n                    add_archive(xp, fp); add_archive(xm, fm)\n                    sec = (fp + fm - 2.0 * f_best) / (eps * eps + 1e-24)\n                    if sec > 0:\n                        step[c] = float(max(self.min_step, step[c] * 0.72))\n                    else:\n                        step[c] = float(min(max_step, step[c] * 1.12))\n\n            # stagnation rescue: small archive-guided restart\n            if stagn >= self.patience and evals < self.budget:\n                stagn = 0\n                w = np.argmin(archive_f)\n                center = 0.6 * med + 0.4 * archive_x[w]\n                noise = (0.25 * step) * (self.rng.random(d) - 0.5)\n                y = np.clip(center + noise, lb, ub)\n                fy = float(func(y)); evals += 1\n                add_archive(y, fy)\n                if fy < f_best:\n                    f_best = fy; x_best = y\n                    step = np.minimum(step * 1.18, max_step)\n                    tilt = 0.6 * tilt + 0.4 * np.sign(y - center)\n                else:\n                    # tiny multi-d perturbation\n                    if self.rng.random() < 0.22 and evals < self.budget:\n                        y2 = x_best.copy()\n                        k = min(3, d)\n                        idxs = self.rng.choice(d, k, replace=False)\n                        for ii in idxs:\n                            y2[ii] = np.clip(y2[ii] + (0.18 * span) * (self.rng.random() - 0.5), lb, ub)\n                        fy2 = float(func(y2)); evals += 1\n                        add_archive(y2, fy2)\n                        if fy2 < f_best:\n                            f_best = fy2; x_best = y2\n\n            # gentle decay and bounds\n            tilt *= 0.995\n            step = np.clip(step, self.min_step, max_step)\n\n        return float(f_best), x_best", "configspace": "", "generation": 22, "feedback": "The algorithm AdaptiveCoordinateTiltedProbing scored 0.615 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1d5c6e04-206b-4dd3-b561-7029c3f0a876"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9842212284958757}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9814137006040838}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9832944599268068}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9668758241532147}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9706193956511096}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9647736214753478}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.02360842368450089}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.04949126089596889}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.11491999742095327}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.09955923931254729}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07923816516852844}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.03468327055054887}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.990264202444413}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9948584742370771}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9932147812509462}], "aucs": [0.9842212284958757, 0.9814137006040838, 0.9832944599268068, 0.9668758241532147, 0.9706193956511096, 0.9647736214753478, 0.02360842368450089, 0.04949126089596889, 0.11491999742095327, 0.09955923931254729, 0.07923816516852844, 0.03468327055054887, 0.990264202444413, 0.9948584742370771, 0.9932147812509462]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2270.0, "Edges": 2269.0, "Max Degree": 34.0, "Min Degree": 1.0, "Mean Degree": 1.9991189427312774, "Degree Variance": 2.235241514486988, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.446583253128008, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3177169825810853, "Depth Entropy": 2.172472179998322, "Assortativity": 8.888044998834236e-09, "Average Eccentricity": 19.08546255506608, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0004405286343612335, "Average Shortest Path": 10.19468142732054, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 662.3333333333334, "total_token_count": 1987.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "2256f7a7-a1a2-4b4c-8823-d8f3eaf8c73c", "fitness": 0.6025219890952432, "name": "MinimalAdaptiveSubspaceLevy", "description": "Minimalist Adaptive Subspace & L\u00e9vy Coordinate Search \u2014 coordinate-wise Gaussian probes with EMA tilt, small adaptive subspace multivariate moves, geometric extrapolation on success and rare Cauchy escapes for robust cheap exploration/exploitation.", "code": "import numpy as np\n\nclass MinimalAdaptiveSubspaceLevy:\n    \"\"\"\n    MinimalAdaptiveSubspaceLevy\n    - budget, dim\n    - fast coordinate probes with mirrored/paired evaluation and per-dim multiplicative adaptation\n    - occasional small multivariate subspace steps (few dims) and rare Cauchy escapes\n    \"\"\"\n    def __init__(self, budget, dim, init_step=None, archive_size=8, min_step=1e-6, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n\n    def __call__(self, func):\n        lb = -5.0; ub = 5.0\n        span = ub - lb\n        # init x\n        x = lb + self.rng.random(self.dim) * span\n        f = float(func(x)); evals = 1\n        x_best = x.copy(); f_best = f\n        # steps\n        if self.init_step is None:\n            step = np.full(self.dim, 0.12 * span)\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (self.dim,) else np.full(self.dim, float(s))\n        max_step = 0.6 * span\n        step = np.clip(step, self.min_step, max_step)\n        # archive: keep (f,x)\n        archive = [(f, x.copy())]\n        # per-dim tilt & success stats\n        tilt = np.zeros(self.dim)\n        wins = np.zeros(self.dim)\n        trials = np.ones(self.dim)\n        # helpers\n        def add_arc(xx, ff):\n            nonlocal archive\n            archive.append((ff, xx.copy()))\n            if len(archive) > self.archive_size:\n                archive.sort(key=lambda t: t[0])\n                archive = archive[:self.archive_size]\n        # main loop\n        while evals < self.budget:\n            # compute simple dispersion from archive\n            arr = np.vstack([a[1] for a in archive])\n            med = np.median(arr, axis=0)\n            iqr = np.subtract(*np.percentile(arr, [75,25], axis=0))\n            iqr = np.maximum(iqr, 1e-12)\n            # choose move type\n            p_sub = 0.12 + 0.05 * (len(archive)/self.archive_size)\n            if self.rng.random() < p_sub and self.dim > 1:\n                # small multivariate subspace move (adaptive dims)\n                k = max(2, min(self.dim, 1 + int(0.12 * self.dim)))\n                # pick dims weighted by iqr*step\n                weights = (iqr + 1e-12) * (step + 1e-12)\n                probs = weights / weights.sum()\n                dims = self.rng.choice(self.dim, size=k, replace=False, p=probs)\n                y = x_best.copy()\n                # multivariate normal on selected dims, scale by step[dims]\n                z = self.rng.normal(size=k) * (step[dims] * (0.6 + 0.8 * self.rng.random()))\n                y[dims] = np.clip(y[dims] + z, lb, ub)\n                fy = float(func(y)); evals += 1\n                add_arc(y, fy)\n                if fy < f_best:\n                    # accept and expand involved steps\n                    f_best = fy; x_best = y\n                    for d,zd in zip(dims,z):\n                        step[d] = min(max_step, step[d] * (1.15 + 0.15 * abs(zd)/(step[d]+1e-12)))\n                        tilt[d] = 0.6 * tilt[d] + 0.4 * np.sign(zd)\n                        wins[d] += 1\n                    for d in dims: trials[d] += 1\n                    # small geometric extrapolation along principal multivariate direction with low prob\n                    if evals < self.budget and self.rng.random() < 0.28:\n                        y2 = x_best.copy()\n                        y2[dims] = np.clip(y2[dims] + 0.6 * z, lb, ub)\n                        f2 = float(func(y2)); evals += 1\n                        add_arc(y2, f2)\n                        if f2 < f_best:\n                            f_best = f2; x_best = y2\n                else:\n                    # shrink involved steps\n                    for d in dims:\n                        step[d] = max(self.min_step, step[d] * 0.62)\n                        tilt[d] *= 0.75\n                        trials[d] += 1\n            else:\n                # coordinate probe\n                # prioritize coords by abs(med - x_best)/step + iqr\n                pull = med - x_best\n                score = np.abs(pull) / (step + 1e-12) + 0.5 * (iqr / (span + 1e-12)) + 0.2 * (wins/(trials+1e-9))\n                s = score - score.max()\n                probs = np.exp(s / (1.0 + 0.12 * np.std(score)))\n                probs /= probs.sum()\n                try:\n                    d = int(self.rng.choice(self.dim, p=probs))\n                except Exception:\n                    d = int(self.rng.integers(0, self.dim))\n                # bias by tilt and pull\n                bias = 0.6 * np.sign(pull[d]) + 0.4 * np.sign(tilt[d])\n                if bias == 0:\n                    sign = 1 if self.rng.random() < 0.5 else -1\n                else:\n                    sign = int(np.sign(bias)) if self.rng.random() < 0.78 else (-int(np.sign(bias)))\n                # gaussian magnitude\n                mag = abs(self.rng.normal()) * step[d] * (0.6 + 0.8 * self.rng.random())\n                mag = min(mag, max_step)\n                x1 = x_best.copy()\n                x1[d] = np.clip(x1[d] + sign * mag, lb, ub)\n                f1 = float(func(x1)); evals += 1\n                add_arc(x1, f1)\n                trials[d] += 1\n                if f1 < f_best:\n                    f_best = f1; x_best = x1.copy()\n                    wins[d] += 1\n                    tilt[d] = 0.7 * tilt[d] + 0.3 * sign\n                    step[d] = min(max_step, step[d] * (1.18 + 0.06 * self.rng.random()))\n                    # geometric line doubling while improving and budget\n                    growth = 1.6\n                    while evals < self.budget and self.rng.random() < 0.35:\n                        ad = min(max_step, step[d] * growth)\n                        x2 = x_best.copy()\n                        x2[d] = np.clip(x2[d] + sign * ad, lb, ub)\n                        f2 = float(func(x2)); evals += 1\n                        add_arc(x2, f2)\n                        if f2 < f_best:\n                            f_best = f2; x_best = x2\n                            step[d] = min(max_step, step[d] * (1.06 + 0.03*self.rng.random()))\n                            growth *= 1.4\n                        else:\n                            break\n                else:\n                    # mirrored try\n                    if evals < self.budget:\n                        x2 = x_best.copy()\n                        x2[d] = np.clip(x2[d] - sign * mag, lb, ub)\n                        f2 = float(func(x2)); evals += 1\n                        add_arc(x2, f2)\n                        trials[d] += 1\n                        if f2 < f_best:\n                            f_best = f2; x_best = x2.copy()\n                            wins[d] += 1\n                            tilt[d] = 0.6 * tilt[d] + 0.4 * (-sign)\n                            step[d] = min(max_step, step[d] * 1.14)\n                        else:\n                            step[d] = max(self.min_step, step[d] * 0.58)\n                            tilt[d] *= 0.72\n            # rare heavy-tail escape (Cauchy) if stagnation in archive\n            if self.rng.random() < 0.03 and evals < self.budget:\n                y = x_best.copy()\n                # Cauchy scaled by span and median dispersion\n                scale = 0.18 * span\n                jump = self.rng.standard_cauchy(self.dim) * scale\n                y += np.clip(jump, -0.9*span, 0.9*span)\n                y = np.clip(y, lb, ub)\n                fy = float(func(y)); evals += 1\n                add_arc(y, fy)\n                if fy < f_best:\n                    f_best = fy; x_best = y\n                    step = np.minimum(max_step, step * 1.25)\n                    tilt = 0.5 * np.sign(jump) + 0.5 * tilt\n                else:\n                    step = np.maximum(self.min_step, step * 0.9)\n            # gentle decay\n            tilt *= 0.995\n            step = np.clip(step, self.min_step, max_step)\n        return f_best, x_best", "configspace": "", "generation": 22, "feedback": "The algorithm MinimalAdaptiveSubspaceLevy scored 0.603 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1d5c6e04-206b-4dd3-b561-7029c3f0a876"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9844896287182446}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9812672989553309}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9852387397301726}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9721705974082174}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.972067196828674}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9726825507754805}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06568479614598999}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.007767676451319705}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.018191821582599577}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.04428608280770929}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.06169273659208507}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9922306169038165}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9877809848979008}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9922291086311097}], "aucs": [0.9844896287182446, 0.9812672989553309, 0.9852387397301726, 0.9721705974082174, 0.972067196828674, 0.9726825507754805, 4.999999999999449e-05, 0.06568479614598999, 0.007767676451319705, 0.018191821582599577, 0.04428608280770929, 0.06169273659208507, 0.9922306169038165, 0.9877809848979008, 0.9922291086311097]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1919.0, "Edges": 1918.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9989577905158937, "Degree Variance": 2.038560664711325, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.865384615384615, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3253233448285757, "Depth Entropy": 2.172802901230658, "Assortativity": 0.0, "Average Eccentricity": 17.883272537780094, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.0005211047420531526, "Average Shortest Path": 10.077070250244386, "mean_complexity": 10.333333333333334, "total_complexity": 31.0, "mean_token_count": 570.3333333333334, "total_token_count": 1711.0, "mean_parameter_count": 3.6666666666666665, "total_parameter_count": 11.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "1ddf5fa0-7b96-4634-8349-8fa97d789ee7", "fitness": 0.6389027457419321, "name": "ParabolicMirrorCompactV2", "description": "Compact Parabolic Mirror v2 \u2014 per-coordinate mirrored 3\u2011point parabolic fits with multiplicative step adaptation, EMA tilt, tiny median-guided micro\u2011pulls and rare Cauchy escapes for robust cheap 1\u2011D exploitation.", "code": "import numpy as np\n\nclass ParabolicMirrorCompactV2:\n    def __init__(self, budget=1000, dim=10, init_frac=0.07, success=1.28, failure=0.62,\n                 arch_size=6, global_prob=0.035, extrap=1.6, tilt_alpha=0.12,\n                 polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_frac = float(init_frac); self.success = float(success); self.failure = float(failure)\n        self.arch_size = int(arch_size); self.global_prob = float(global_prob)\n        self.extrap = float(extrap); self.tilt_alpha = float(tilt_alpha)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, lb); ub = np.full(d, ub)\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n        span = np.maximum(1e-12, ub - lb); box = max(1e-12, np.linalg.norm(span))\n        base = max(1e-15, self.init_frac * box); min_step = 1e-15 * box; max_step = 1.2 * box\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        x = rng.uniform(lb, ub); fx = safe_eval(x)\n        steps = np.full(d, base); trials = np.zeros(d, int); succ = np.zeros(d, int)\n        tilt = np.zeros(d, float)\n        arch_x, arch_f = [x.copy()], [fx]\n\n        def push_arch(xx, ff):\n            arch_x.append(xx.copy()); arch_f.append(float(ff))\n            if len(arch_x) > self.arch_size:\n                i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n\n        while evals < self.budget:\n            # rare heavy-tail escape\n            if rng.random() < self.global_prob:\n                u = rng.random(d)\n                jump = 0.12 * box * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x + jump, lb, ub); fg = safe_eval(xg)\n                if fg < fx:\n                    old = x.copy(); x, fx = xg.copy(), fg; push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x-old)\n                continue\n\n            # median micro-pull\n            if len(arch_x) >= 3 and rng.random() < 0.03:\n                med = np.median(np.vstack(arch_x), axis=0)\n                k = max(1, d//6)\n                idx = np.argsort(-steps)[:k]\n                cand = x.copy()\n                cand[idx] = np.clip(x[idx] + 0.45*(med[idx]-x[idx]) + 0.25*tilt[idx], lb[idx], ub[idx])\n                fc = safe_eval(cand)\n                if fc < fx:\n                    old = x.copy(); x, fx = cand.copy(), fc; push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x-old)\n                continue\n\n            # choose coordinate\n            scores = steps/(1.0+0.03*trials) + 0.25*np.abs(tilt) + 1e-8*rng.random(d)\n            i = int(np.argmax(scores)); s = float(np.clip(steps[i], min_step, max_step))\n\n            # mirrored probes if budget allows two evals\n            if evals <= self.budget - 2:\n                xp = x.copy(); xm = x.copy()\n                xp[i] = np.clip(xp[i] + s, lb[i], ub[i]); xm[i] = np.clip(xm[i] - s, lb[i], ub[i])\n                fp = safe_eval(xp); fm = safe_eval(xm); trials[i] += 2\n\n                # accept improvements and extrapolate\n                if fp < fx or fm < fx:\n                    old = x.copy()\n                    if fp < fm:\n                        x, fx = xp.copy(), fp\n                    else:\n                        x, fx = xm.copy(), fm\n                    succ[i] += 1; steps[i] = min(max_step, steps[i]*self.success)\n                    if evals < self.budget:\n                        sign = np.sign(x[i]-old[i]); sign = 1.0 if sign == 0 else sign\n                        ext = x.copy(); ext[i] = np.clip(ext[i] + sign*s*self.extrap, lb[i], ub[i])\n                        fext = safe_eval(ext); trials[i] += 1\n                        if fext < fx:\n                            x, fx = ext.copy(), fext; push_arch(x, fx)\n                    push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x-old)\n                    continue\n\n                # parabolic guess\n                denom = (fp + fm - 2.0*fx)\n                if abs(denom) > 1e-18:\n                    delta = -s*(fp - fm)/(2.0*denom)\n                else:\n                    delta = 0.0\n                delta = float(np.clip(delta, -s*self.extrap, s*self.extrap))\n                if abs(delta) > 1e-12:\n                    xpar = x.copy(); xpar[i] = np.clip(xpar[i] + delta, lb[i], ub[i])\n                    fpar = safe_eval(xpar); trials[i] += 1\n                    if fpar < fx:\n                        old = x.copy(); x, fx = xpar.copy(), fpar\n                        succ[i] += 1; steps[i] = min(max_step, steps[i]*self.success)\n                        push_arch(x, fx)\n                        tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x-old)\n                        continue\n                    else:\n                        steps[i] = max(min_step, steps[i]*self.failure)\n                else:\n                    steps[i] = max(min_step, steps[i]*self.failure)\n\n            else:\n                # last eval: single-side try\n                xt = x.copy()\n                if rng.random() < 0.5: xt[i] = np.clip(xt[i] + s, lb[i], ub[i])\n                else: xt[i] = np.clip(xt[i] - s, lb[i], ub[i])\n                ft = safe_eval(xt); trials[i] += 1\n                if ft < fx:\n                    old = x.copy(); x, fx = xt.copy(), ft; succ[i] += 1\n                    steps[i] = min(max_step, steps[i]*self.success); push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x-old)\n                else:\n                    steps[i] = max(min_step, steps[i]*self.failure)\n\n            # rejuvenate stale coords\n            if trials[i] > 60 and succ[i] == 0:\n                steps[i] = min(max_step, steps[i] * 1.6); trials[i] = int(trials[i]*0.6)\n\n        # polish\n        fin = self.polish_frac * box\n        while evals < self.budget and fin > 1e-12 * box:\n            improved = False\n            for j in range(d):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[j] = np.clip(xt[j] + fin, lb[j], ub[j])\n                ft = safe_eval(xt)\n                if ft < fx: x, fx, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[j] = np.clip(xt[j] - fin, lb[j], ub[j])\n                ft = safe_eval(xt)\n                if ft < fx: x, fx, improved = xt.copy(), ft, True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 22, "feedback": "The algorithm ParabolicMirrorCompactV2 scored 0.639 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4ede79fe-bcf1-46b5-90c5-770f1a941fad"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9947974453635113}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9918631647113092}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9915226990314763}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9542846265489666}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9535773953733914}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9536847875556771}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.15066125228441885}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.12894266423657685}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.13819303594482335}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.128636836330135}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.12108950351417047}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0879400927786963}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9966675933129328}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9952904896571567}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9963895994857386}], "aucs": [0.9947974453635113, 0.9918631647113092, 0.9915226990314763, 0.9542846265489666, 0.9535773953733914, 0.9536847875556771, 0.15066125228441885, 0.12894266423657685, 0.13819303594482335, 0.128636836330135, 0.12108950351417047, 0.0879400927786963, 0.9966675933129328, 0.9952904896571567, 0.9963895994857386]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2230.0, "Edges": 2229.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9991031390134528, "Degree Variance": 2.0582951597659314, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.289499509322866, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.324778346098035, "Depth Entropy": 2.0448686186437404, "Assortativity": 1.1041743397590682e-08, "Average Eccentricity": 15.443946188340806, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0004484304932735426, "Average Shortest Path": 10.086540446257748, "mean_complexity": 9.5, "total_complexity": 38.0, "mean_token_count": 476.25, "total_token_count": 1905.0, "mean_parameter_count": 4.25, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "35b20744-3cad-47b8-9949-6a58cd916c02", "fitness": 0.22684642921155435, "name": "DirectionalQuasiNewton", "description": "Directional Quasi-Newton Sampling \u2014 estimate directional derivative+curvature on random or archive-guided directions, take approximate Newton steps along those directions, adapt step scale, and use occasional heavy\u2011tailed jumps and small archive pulls for robust global/local search.", "code": "import numpy as np\n\nclass DirectionalQuasiNewton:\n    def __init__(self, budget=1000, dim=10, init_sigma=0.12, lr=0.6,\n                 succ_mult=1.25, fail_mult=0.6, global_prob=0.04,\n                 max_step=2.0, archive_k=6, polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_sigma = float(init_sigma)\n        self.lr = float(lr)\n        self.succ_mult = float(succ_mult); self.fail_mult = float(fail_mult)\n        self.global_prob = float(global_prob)\n        self.max_step = float(max_step)\n        self.archive_k = int(archive_k)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        span = max(1e-12, np.linalg.norm(ub - lb))\n        sigma = max(1e-12, self.init_sigma * span)\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        # init\n        x = rng.uniform(lb, ub)\n        f = safe_eval(x)\n        archive = [(x.copy(), f)]\n        # main loop\n        while evals < self.budget:\n            # occasional heavy-tailed global jump\n            if rng.random() < self.global_prob:\n                u = rng.random(self.dim)\n                jump = 0.08 * span * np.tan(np.pi * (u - 0.5))\n                xt = np.clip(x + jump, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f = xt.copy(), float(ft)\n                    archive.append((x.copy(), f))\n                    archive.sort(key=lambda t: t[1]); archive = archive[:self.archive_k]\n                continue\n\n            # pick direction: archive-guided pull with some probability\n            if len(archive) > 1 and rng.random() < 0.35:\n                # bias towards best archive direction (small local pull)\n                a = archive[0][0]\n                u = x - a\n                if np.linalg.norm(u) < 1e-12:\n                    u = rng.normal(size=dim)\n            else:\n                u = rng.normal(size=dim)\n            nu = np.linalg.norm(u)\n            if nu == 0:\n                u = np.ones(dim); nu = np.linalg.norm(u)\n            u = u / nu\n\n            # two-sided directional probes\n            xp = np.clip(x + sigma * u, lb, ub)\n            xm = np.clip(x - sigma * u, lb, ub)\n            fp = safe_eval(xp); fm = safe_eval(xm)\n            # directional derivative and curvature\n            g = (fp - fm) / (2.0 * sigma)\n            c = (fp + fm - 2.0 * f) / (sigma * sigma)\n            # compute step length along u: Newton if positive curvature, otherwise damped gradient\n            if c > 1e-16:\n                step = - g / (c + 1e-20)\n            else:\n                step = - self.lr * g\n            # clip step length\n            maxlen = self.max_step * sigma\n            step = float(np.clip(step, -maxlen, maxlen))\n            # propose\n            xt = np.clip(x + step * u, lb, ub)\n            ft = safe_eval(xt)\n            # accept/adjust\n            if ft < f:\n                x, f = xt.copy(), float(ft)\n                sigma = min(span, sigma * self.succ_mult)\n                archive.append((x.copy(), f))\n                archive.sort(key=lambda t: t[1]); archive = archive[:self.archive_k]\n            else:\n                sigma = max(1e-16, sigma * self.fail_mult)\n\n            # small archive pull diversity: occasional median nudges\n            if rng.random() < 0.08 and len(archive) >= 3:\n                med = np.median(np.vstack([p for p,_ in archive]), axis=0)\n                xt = np.clip(x + 0.5 * sigma * (med - x) / (np.linalg.norm(med - x) + 1e-12), lb, ub)\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f = xt.copy(), float(ft)\n                    sigma = min(span, sigma * 1.2)\n                    archive.append((x.copy(), f))\n                    archive.sort(key=lambda t: t[1]); archive = archive[:self.archive_k]\n\n        # final cheap coordinate polish\n        fin = self.polish_frac * span\n        while evals < self.budget and fin > 1e-12 * span:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f, improved = xt.copy(), float(ft), True\n            if not improved:\n                fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 22, "feedback": "The algorithm DirectionalQuasiNewton scored 0.227 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["86d13d75-4636-4c2f-9196-d0f90890a1f4"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9790413354342297}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9763004886540503}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9783487180841869}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.036937569846666185}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.2003985491152217}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.1348843159047003}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.09638546113426039}], "aucs": [0.9790413354342297, 0.9763004886540503, 0.9783487180841869, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.036937569846666185, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.2003985491152217, 0.1348843159047003, 0.09638546113426039]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1439.0, "Edges": 1438.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.9986101459346768, "Degree Variance": 2.093118290682327, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 7.638198757763975, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3277507371269865, "Depth Entropy": 1.9863302635360658, "Assortativity": 0.0, "Average Eccentricity": 15.972202918693537, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.0006949270326615705, "Average Shortest Path": 9.477882666548107, "mean_complexity": 10.0, "total_complexity": 30.0, "mean_token_count": 423.0, "total_token_count": 1269.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "b06e4fac-53b8-4a0f-9352-5fad9fc29a61", "fitness": 0.7583201218523836, "name": "AdaptiveMedianBanditPull", "description": "Adaptive Median-Bandit Pull (AMBP) \u2014 coordinate-wise pulls toward the archive median guided by Beta-like sign priors, lightweight momentum, multiplicative step adaptation, mirrored checks and rare Cauchy escapes for cheap, robust 1\u2011D exploitation.", "code": "import numpy as np\n\nclass AdaptiveMedianBanditPull:\n    def __init__(self, budget, dim, archive_size=8, init_step=None, min_step=1e-6,\n                 max_step=None, patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0; ub = 5.0\n        span = (ub - lb) * np.ones(d)\n        # steps\n        if self.init_step is None:\n            step = 0.12 * span\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = min(12, max(3, min(d + 1, max(1, self.budget // 8))))\n        arc_x = []\n        arc_f = []\n        # initial samples\n        for _ in range(n_init):\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n            if evals >= self.budget: break\n        if not arc_x:\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n\n        arc_x = np.vstack(arc_x)\n        arc_f = np.array(arc_f, float)\n        top_k = min(self.archive_size, arc_x.shape[0])\n\n        bi = np.ones(d)   # success counts\n        bj = np.ones(d)   # failure counts (for sign bias)\n        momentum = np.zeros(d)\n        priority = np.zeros(d)\n        no_improve = 0\n\n        idx = int(np.argmin(arc_f))\n        x_best = arc_x[idx].copy(); f_best = float(arc_f[idx])\n\n        def arc_add(xn, fn):\n            nonlocal arc_x, arc_f\n            fn = float(fn)\n            if arc_x.shape[0] < self.archive_size:\n                arc_x = np.vstack([arc_x, xn.copy()])\n                arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < self.budget:\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0)\n            q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            s_raw = (abs_pull / (step + 1e-12)) + 0.25 * (iqr / (span + 1e-12)) + 0.1 * priority\n            s_raw = np.maximum(s_raw, 1e-12)\n            s = s_raw - s_raw.max()\n            temp = 0.9 + 0.25 * np.std(s_raw)\n            probs = np.exp(s / max(1e-6, temp))\n            probs /= probs.sum()\n            coord = int(self.rng.choice(d, p=probs))\n\n            p_follow = bi[coord] / (bi[coord] + bj[coord])\n            if abs_pull[coord] > 1e-12:\n                nominal = int(np.sign(pull[coord]))\n            else:\n                nominal = int(np.sign(momentum[coord])) if momentum[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nominal if self.rng.random() < p_follow else -nominal\n\n            frac = min(1.0, abs_pull[coord] / (step[coord] + 1e-12))\n            mag = step[coord] * (0.8 + 0.4 * self.rng.random()) * (0.5 + 0.5 * frac)\n            delta = float(sign * mag)\n\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, -5.0, 5.0)\n            f_try = float(func(x_try)); evals += 1\n            arc_add(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy()\n                improved = True\n                # credit depending on sign match with nominal\n                if sign == nominal:\n                    bi[coord] += 1.0\n                else:\n                    bj[coord] += 0.6\n                priority[coord] = min(8.0, priority[coord] + 1.0)\n                momentum[coord] = 0.7 * momentum[coord] + 0.3 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * 1.20 + 1e-12))\n                no_improve = 0\n                # small extrapolation\n                if evals < self.budget:\n                    ext = 0.45 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, -5.0, 5.0)\n                    f_ext = float(func(x_ext)); evals += 1\n                    arc_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                        momentum[coord] = 0.8 * momentum[coord] + 0.2 * ext\n            else:\n                # mirrored probe\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, -5.0, 5.0)\n                    f_opp = float(func(x_opp)); evals += 1\n                    arc_add(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy()\n                        improved = True\n                        # mirrored success rewards failure-count (encourages flips)\n                        bj[coord] += 1.0\n                        priority[coord] = min(8.0, priority[coord] + 0.8)\n                        momentum[coord] = 0.65 * momentum[coord] + 0.35 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.18))\n                        no_improve = 0\n                    else:\n                        bj[coord] += 0.4\n                        bi[coord] = max(1.0, bi[coord] * 0.995)\n                        priority[coord] = max(-8.0, priority[coord] - 0.5)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.6))\n                        momentum[coord] *= 0.6\n                        no_improve += 1\n                else:\n                    no_improve += 1\n\n            # stagnation rescue\n            if no_improve >= self.patience and evals < self.budget:\n                no_improve = 0\n                cand = (iqr / (span + 1e-12)) * (1.0 / (step + 1e-12) + 0.1) + 1e-9\n                s = cand - cand.max()\n                w = np.exp(s); w /= w.sum()\n                k = min(3, max(1, d // 6))\n                picks = list(self.rng.choice(d, size=min(d, k), replace=False, p=w))\n                improved_any = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = 0.6 * step[c] + 0.35 * span[c] * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, -5.0, 5.0)\n                    fj = float(func(xj)); evals += 1\n                    arc_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step[c] = float(min(max_step[c], step[c] * 1.25))\n                        priority[c] = min(8.0, priority[c] + 1.0)\n                        momentum[c] = np.sign(jump) * 0.5 * abs(jump)\n                        improved_any = True\n                        break\n                if not improved_any and evals < self.budget:\n                    scale = np.maximum(0.08 * step, 0.04 * span)\n                    u = self.rng.random(d)\n                    jump = np.tan(np.pi * (u - 0.5)) * scale * (0.5 + self.rng.random(d))\n                    mask = self.rng.random(d) < 0.25\n                    xj = np.clip(x_best + jump * mask, -5.0, 5.0)\n                    for i in range(d):\n                        if evals >= self.budget: break\n                        if not mask[i] or np.allclose(xj[i], x_best[i]): continue\n                        xt = x_best.copy(); xt[i] = xj[i]\n                        fv = float(func(xt)); evals += 1\n                        arc_add(xt, fv)\n                        if fv < f_best:\n                            f_best = fv; x_best = xt.copy()\n                            step[i] = float(min(max_step[i], step[i] * 1.2))\n                            priority[i] = min(8.0, priority[i] + 1.0)\n                            momentum[i] = np.sign(xj[i] - x_best[i]) * 0.5 * abs(xj[i] - x_best[i])\n                            break\n\n            # decay & trim archive\n            priority *= 0.995\n            momentum *= 0.992\n            if arc_x.shape[0] > self.archive_size:\n                order = np.argsort(arc_f)\n                keep = order[:self.archive_size]\n                arc_x = arc_x[keep]; arc_f = arc_f[keep]\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 22, "feedback": "The algorithm AdaptiveMedianBanditPull scored 0.758 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad9d28ed-2806-47c9-b4ca-e97b1d7b4fc7"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.980144969641164}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.97757846960579}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9783267975439641}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9044223218074761}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.932594378114941}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8970428922598677}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.46454560433213}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.5556549804422297}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.6054492351548282}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.3644760114946387}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.34636085255537896}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.3882890912153998}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9943676066014019}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9923300375082198}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9932185795083226}], "aucs": [0.980144969641164, 0.97757846960579, 0.9783267975439641, 0.9044223218074761, 0.932594378114941, 0.8970428922598677, 0.46454560433213, 0.5556549804422297, 0.6054492351548282, 0.3644760114946387, 0.34636085255537896, 0.3882890912153998, 0.9943676066014019, 0.9923300375082198, 0.9932185795083226]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2424.0, "Edges": 2423.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9991749174917492, "Degree Variance": 2.260725391846115, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.45625, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3157174608070692, "Depth Entropy": 2.154577818861909, "Assortativity": 0.0, "Average Eccentricity": 16.835808580858085, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.00041254125412541255, "Average Shortest Path": 10.195521739544983, "mean_complexity": 12.666666666666666, "total_complexity": 38.0, "mean_token_count": 718.3333333333334, "total_token_count": 2155.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "79f38872-84f7-4883-9a26-9ff7fbe59b87", "fitness": 0.7751680561939248, "name": "SeparableAdaptiveMedianPull", "description": "Coordinate-wise adaptive median pulls with compact median archive, Beta-like sign priors, lightweight momentum and rare Cauchy escapes \u2014 cheap separable 1\u2011D probes with multiplicative step adaptation and targeted stagnation rescues.", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPull:\n    def __init__(self, budget, dim, archive_size=8, init_step=None, min_step=1e-6,\n                 max_step=None, patience=6, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0; ub = 5.0\n        span = (ub - lb)\n        # per-dim arrays\n        if self.init_step is None:\n            step = np.full(d, 0.12 * span)\n        else:\n            s = np.asarray(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = np.full(d, 0.5 * span)\n        else:\n            m = np.asarray(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = int(min(max(3, d), max(1, self.budget // 12), 8))\n        arc_x = []\n        arc_f = []\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub, d)\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n            if evals >= self.budget: break\n        if not arc_x:\n            x = self.rng.uniform(lb, ub, d)\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n\n        arc_x = np.vstack(arc_x)\n        arc_f = np.array(arc_f, float)\n        K = min(self.archive_size, arc_x.shape[0])\n\n        bi = np.ones(d)   # success pseudo-counts\n        bj = np.ones(d)   # failure pseudo-counts\n        mom = np.zeros(d)\n        prio = np.zeros(d)\n        no_improve = 0\n\n        i0 = int(np.argmin(arc_f))\n        x_best = arc_x[i0].copy(); f_best = float(arc_f[i0])\n\n        def arc_add(xn, fn):\n            nonlocal arc_x, arc_f, K\n            fn = float(fn)\n            if arc_x.shape[0] < K:\n                arc_x = np.vstack([arc_x, xn.copy()])\n                arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        eps = 1e-12\n        while evals < self.budget:\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0)\n            q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            # coordinate sampling score (cheap separable priority)\n            score = (abs_pull / (step + eps)) + 0.25 * (iqr / (span + eps)) + 0.12 * prio\n            score = np.maximum(score, eps)\n            s = score - score.max()\n            temp = 0.9 + 0.25 * np.std(score)\n            p = np.exp(s / max(1e-6, temp))\n            p /= p.sum()\n            coord = int(self.rng.choice(d, p=p))\n\n            # sign prior: follow median pull with Beta-like probability\n            p_follow = bi[coord] / (bi[coord] + bj[coord])\n            if abs_pull[coord] > 1e-12:\n                nominal = int(np.sign(pull[coord]))\n            else:\n                nominal = int(np.sign(mom[coord])) if mom[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nominal if self.rng.random() < p_follow else -nominal\n\n            frac = min(1.0, abs_pull[coord] / (step[coord] + eps))\n            mag = step[coord] * (0.7 + 0.6 * self.rng.random()) * (0.4 + 0.6 * frac)\n            delta = float(sign * mag)\n\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb, ub)\n            f_try = float(func(x_try)); evals += 1\n            arc_add(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy(); improved = True\n                bi[coord] += 1.0\n                prio[coord] = min(8.0, prio[coord] + 1.0)\n                mom[coord] = 0.65 * mom[coord] + 0.35 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * 1.22 + 1e-12))\n                # small extrapolation\n                if evals < self.budget:\n                    ext = 0.45 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb, ub)\n                    f_ext = float(func(x_ext)); evals += 1\n                    arc_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                        mom[coord] = 0.8 * mom[coord] + 0.2 * ext\n                no_improve = 0\n            else:\n                # mirrored probe\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb, ub)\n                    f_opp = float(func(x_opp)); evals += 1\n                    arc_add(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy(); improved = True\n                        bj[coord] += 1.0\n                        prio[coord] = min(8.0, prio[coord] + 0.8)\n                        mom[coord] = 0.65 * mom[coord] + 0.35 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.18))\n                        if evals < self.budget:\n                            ext = 0.35 * (-delta)\n                            x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb, ub)\n                            f_ext = float(func(x_ext)); evals += 1\n                            arc_add(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext; x_best = x_ext.copy()\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                        no_improve = 0\n                    else:\n                        bj[coord] += 0.4\n                        bi[coord] = max(1.0, bi[coord] * 0.995)\n                        prio[coord] = max(-8.0, prio[coord] - 0.5)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.6))\n                        mom[coord] *= 0.6\n                        no_improve += 1\n                else:\n                    no_improve += 1\n\n            # stagnation rescue\n            if no_improve >= self.patience and evals < self.budget:\n                no_improve = 0\n                weight = (iqr / (span + eps)) * (1.0 / (step + eps) + 0.1)\n                s = weight - weight.max()\n                w = np.exp(s); w /= w.sum()\n                k = min(3, max(1, d // 6))\n                picks = list(self.rng.choice(d, size=min(d, k), replace=False, p=w))\n                improved_any = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = 0.6 * step[c] + 0.35 * span * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb, ub)\n                    fj = float(func(xj)); evals += 1\n                    arc_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step[c] = float(min(max_step[c], step[c] * 1.25))\n                        prio[c] = min(8.0, prio[c] + 1.0)\n                        mom[c] = np.sign(jump) * 0.5 * abs(jump)\n                        improved_any = True\n                        break\n                if (not improved_any) and evals < self.budget:\n                    # Cauchy-ish heavy tail per-dim attempts (sparse)\n                    scale = np.maximum(0.1 * step, 0.05 * span)\n                    u = self.rng.random((d,))\n                    cauchy = np.tan(np.pi * (u - 0.5)) * scale * (0.4 + self.rng.random(d))\n                    mask = self.rng.random(d) < 0.18\n                    if not mask.any():\n                        mask[self.rng.integers(0, d)] = True\n                    for i in np.where(mask)[0]:\n                        if evals >= self.budget: break\n                        xt = x_best.copy(); xt[i] = np.clip(x_best[i] + cauchy[i], lb, ub)\n                        ft = float(func(xt)); evals += 1\n                        arc_add(xt, ft)\n                        if ft < f_best:\n                            f_best = ft; x_best = xt.copy()\n                            step[i] = float(min(max_step[i], step[i] * 1.2))\n                            prio[i] = min(8.0, prio[i] + 1.0)\n                            mom[i] = np.sign(cauchy[i]) * 0.5 * abs(cauchy[i])\n                            break\n\n            prio *= 0.994\n            mom *= 0.992\n\n            # keep compact archive\n            if arc_x.shape[0] > self.archive_size:\n                o = np.argsort(arc_f)\n                keep = o[:self.archive_size]\n                arc_x = arc_x[keep]\n                arc_f = arc_f[keep]\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 22, "feedback": "The algorithm SeparableAdaptiveMedianPull scored 0.775 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad9d28ed-2806-47c9-b4ca-e97b1d7b4fc7"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.979019723959957}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9785109766217579}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9766406568525379}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9532379615130386}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9409343286960864}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9444448573140345}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.5404365217027898}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.6825843565120642}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.5247487658342984}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.35623254707993135}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.595900240637125}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.18355917635401697}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9911887224085579}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9898750671320025}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.990206940290675}], "aucs": [0.979019723959957, 0.9785109766217579, 0.9766406568525379, 0.9532379615130386, 0.9409343286960864, 0.9444448573140345, 0.5404365217027898, 0.6825843565120642, 0.5247487658342984, 0.35623254707993135, 0.595900240637125, 0.18355917635401697, 0.9911887224085579, 0.9898750671320025, 0.990206940290675]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2509.0, "Edges": 2508.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9992028696691908, "Degree Variance": 2.255878200772953, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.537521815008725, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3209886776963724, "Depth Entropy": 2.168041942088665, "Assortativity": 0.0, "Average Eccentricity": 17.188521323236348, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.00039856516540454366, "Average Shortest Path": 10.270345416786649, "mean_complexity": 12.666666666666666, "total_complexity": 38.0, "mean_token_count": 736.0, "total_token_count": 2208.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "12d22bd6-f461-4082-8b8a-cdaab04f4e8c", "fitness": 0.608592873335677, "name": "SeparableAdaptiveMedianPull", "description": "Compact Separable Adaptive Median Pull v2 \u2014 tighter archive-guided coordinate pulls with adaptive per-dim steps, beta-like sign priors, lightweight momentum, coordinate recombination and rare heavy-tail escapes.", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPull:\n    def __init__(self, budget, dim, archive_size=8, init_step=None, min_step=1e-6,\n                 max_step=None, patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0 * np.ones(d); ub = 5.0 * np.ones(d)\n        span = ub - lb\n        # init steps\n        if self.init_step is None:\n            step = np.full(d, 0.12) * span\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = min(max(3, d+1), max(1, self.budget//10), 12)\n        arc_x = []\n        arc_f = []\n        for _ in range(n_init):\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n            if evals >= self.budget: break\n        if not arc_x:\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n        arc_x = np.vstack(arc_x); arc_f = np.array(arc_f, float)\n        top_k = min(self.archive_size, arc_x.shape[0])\n\n        bi = np.ones(d); bj = np.ones(d)    # sign priors\n        mom = np.zeros(d)\n        pr = np.zeros(d)\n        no_imp = 0\n\n        idx = int(np.argmin(arc_f))\n        xbest = arc_x[idx].copy(); fbest = float(arc_f[idx])\n\n        def arc_add(xn, fn):\n            nonlocal arc_x, arc_f\n            fn = float(fn)\n            if arc_x.shape[0] < top_k:\n                arc_x = np.vstack([arc_x, xn.copy()]); arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < self.budget:\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0); q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - xbest\n            ap = np.abs(pull)\n            # scoring: pull magnitude normalized by step, dispersion and priority\n            s_raw = (ap / (step + 1e-12)) + 0.25 * (iqr / (span + 1e-12)) + 0.1 * pr\n            s_raw = np.maximum(s_raw, 1e-12)\n            s = s_raw - s_raw.max()\n            temp = 0.9 + 0.2 * np.std(s_raw)\n            probs = np.exp(s / max(1e-6, temp))\n            probs /= probs.sum()\n            coord = int(self.rng.choice(d, p=probs))\n\n            p_follow = bi[coord] / (bi[coord] + bj[coord])\n            if ap[coord] > 1e-12:\n                nom = int(np.sign(pull[coord]))\n            else:\n                nom = int(np.sign(mom[coord])) if mom[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nom if self.rng.random() < p_follow else -nom\n\n            frac = min(1.0, ap[coord] / (step[coord] + 1e-12))\n            mag = step[coord] * (0.8 + 0.4 * self.rng.random()) * (0.5 + 0.5 * frac)\n            delta = float(sign * mag)\n\n            x_try = xbest.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            arc_add(x_try, f_try)\n\n            improved = False\n            if f_try < fbest:\n                fbest = f_try; xbest = x_try\n                improved = True\n                bi[coord] += 1.0\n                pr[coord] = min(8.0, pr[coord] + 1.0)\n                mom[coord] = 0.66 * mom[coord] + 0.34 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * 1.20 + 1e-12))\n                # small extrapolation\n                if evals < self.budget:\n                    ext = 0.4 * delta\n                    x_ext = xbest.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    arc_add(x_ext, f_ext)\n                    if f_ext < fbest:\n                        fbest = f_ext; xbest = x_ext\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                        mom[coord] = 0.8 * mom[coord] + 0.2 * ext\n                no_imp = 0\n            else:\n                # mirrored probe\n                if evals < self.budget:\n                    x_opp = xbest.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    arc_add(x_opp, f_opp)\n                    if f_opp < fbest:\n                        fbest = f_opp; xbest = x_opp\n                        improved = True\n                        bj[coord] += 1.0\n                        pr[coord] = min(8.0, pr[coord] + 0.8)\n                        mom[coord] = 0.66 * mom[coord] + 0.34 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.16))\n                        if evals < self.budget:\n                            ext = 0.33 * (-delta)\n                            x_ext = xbest.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                            f_ext = float(func(x_ext)); evals += 1\n                            arc_add(x_ext, f_ext)\n                            if f_ext < fbest:\n                                fbest = f_ext; xbest = x_ext\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                        no_imp = 0\n                    else:\n                        bj[coord] += 0.35\n                        bi[coord] = max(1.0, bi[coord] * 0.996)\n                        pr[coord] = max(-8.0, pr[coord] - 0.45)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.62))\n                        mom[coord] *= 0.62\n                        no_imp += 1\n                else:\n                    no_imp += 1\n\n            # stagnation: recombine archive + guided random or heavy-tail single-dim tweaks\n            if no_imp >= self.patience and evals < self.budget:\n                no_imp = 0\n                # build candidates: median + small gaussian scaled by iqr/step, and archive recomb\n                cand_scores = []\n                iqr = np.maximum(np.percentile(arc_x, 75, axis=0) - np.percentile(arc_x, 25, axis=0), 1e-12)\n                k = min(4, max(1, d//4))\n                for _ in range(k):\n                    mix = self.rng.random() * 0.8 + 0.2\n                    base = med * mix + arc_x[self.rng.integers(0, arc_x.shape[0])] * (1-mix)\n                    noise = (self.rng.normal(size=d) * (0.25 * iqr + 0.08 * step)) * (self.rng.random(d) < 0.5)\n                    cand = np.clip(xbest + 0.6*(base - xbest) + noise, lb, ub)\n                    f_c = float(func(cand)); evals += 1\n                    arc_add(cand, f_c)\n                    cand_scores.append((f_c, cand))\n                    if f_c < fbest:\n                        fbest = f_c; xbest = cand.copy()\n                        # reinforce steps where change occurred\n                        changed = np.abs(cand - xbest) > 1e-12\n                        step[changed] = np.minimum(max_step[changed], step[changed] * 1.18)\n                        pr[changed] = np.minimum(8.0, pr[changed] + 0.6)\n                        break\n                # if still no improvement, try few single-dim heavy-tail nudges\n                if fbest >= min([s for s,_ in cand_scores] or [fbest]) and evals < self.budget:\n                    scale = np.maximum(0.08 * step, 0.025 * span)\n                    cauchy = np.tan(np.pi*(self.rng.random(d)-0.5))\n                    jump = cauchy * scale * (0.5 + self.rng.random(d))\n                    # only apply to a subset\n                    mask = (self.rng.random(d) < 0.25)\n                    for i in np.where(mask)[0]:\n                        if evals >= self.budget: break\n                        xt = xbest.copy(); xt[i] = np.clip(xbest[i] + jump[i], lb[i], ub[i])\n                        ft = float(func(xt)); evals += 1\n                        arc_add(xt, ft)\n                        if ft < fbest:\n                            fbest = ft; xbest = xt\n                            step[i] = float(min(max_step[i], step[i] * 1.25))\n                            pr[i] = min(8.0, pr[i] + 1.0)\n                            break\n\n            pr *= 0.995\n            mom *= 0.993\n\n            # trim archive\n            if arc_x.shape[0] > self.archive_size:\n                o = np.argsort(arc_f)[:self.archive_size]\n                arc_x = arc_x[o]; arc_f = arc_f[o]\n\n        return float(fbest), xbest.copy()", "configspace": "", "generation": 22, "feedback": "The algorithm SeparableAdaptiveMedianPull scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad9d28ed-2806-47c9-b4ca-e97b1d7b4fc7"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9810755450459611}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9754474205775048}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9811841686834929}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9527402315907988}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9556546589668397}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9611821694850533}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.031043021748288435}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.05723287421164336}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.09584156912360264}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.05983084556243534}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.05999708693815953}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.040988663571699946}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9909662649519942}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.993360925527257}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.992347654050425}], "aucs": [0.9810755450459611, 0.9754474205775048, 0.9811841686834929, 0.9527402315907988, 0.9556546589668397, 0.9611821694850533, 0.031043021748288435, 0.05723287421164336, 0.09584156912360264, 0.05983084556243534, 0.05999708693815953, 0.040988663571699946, 0.9909662649519942, 0.993360925527257, 0.992347654050425]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2487.0, "Edges": 2486.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9991958182549256, "Degree Variance": 2.221953514932251, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.574542284219703, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3200141657200573, "Depth Entropy": 2.175910559671682, "Assortativity": 0.0, "Average Eccentricity": 17.241254523522315, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0004020908725371934, "Average Shortest Path": 10.386193564540438, "mean_complexity": 12.333333333333334, "total_complexity": 37.0, "mean_token_count": 728.0, "total_token_count": 2184.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "77e1eff1-214c-49df-b989-8a3a56e56dc3", "fitness": 0.6255299532693929, "name": "SeparableCumulativeMedianAnnealedPull", "description": "Separable Cumulative Median Annealed Pull \u2014 strictly single-coordinate pulls toward archive medians with annealed softmax selection, compact median archive, adaptive multiplicative per-dim steps, light momentum and rare Cauchy rescues for robust separable exploitation.", "code": "import numpy as np\n\nclass SeparableCumulativeMedianAnnealedPull:\n    def __init__(self, budget, dim, archive_size=12, init_step=None,\n                 min_step=1e-6, max_step=None, stagnation_patience=8, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_step = init_step\n        self.min_step = float(min_step)\n        self.max_step = max_step\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def __call__(self, func):\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim,  5.0)\n        rng = self.rng\n\n        rngf = lambda : rng.random(self.dim)\n        span = ub - lb\n\n        # steps\n        if self.init_step is None:\n            step = 0.2 * span\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (self.dim,) else np.full(self.dim, float(s))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            ms = np.array(self.max_step, float)\n            max_step = ms if ms.shape == (self.dim,) else np.full(self.dim, float(ms))\n        step = np.clip(step, self.min_step, max_step)\n\n        # small random initialization to fill archive\n        top_k = min(self.archive_size, max(2, self.budget // 6))\n        archive_x = []\n        archive_f = []\n        evals = 0\n\n        n_init = min(max(4, self.dim), max(1, self.budget // 12))\n        for _ in range(n_init):\n            x = lb + rngf() * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n            if evals >= self.budget: break\n\n        if len(archive_x) == 0:\n            x = lb + rngf() * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n\n        # current best\n        idx = int(np.argmin(archive_f))\n        x_best = archive_x[idx].copy(); f_best = float(archive_f[idx])\n\n        # state\n        momentum = np.zeros(self.dim)\n        wins = np.zeros(self.dim)\n        losses = np.zeros(self.dim)\n        stagn = 0\n\n        def add_archive(xn, fn):\n            nonlocal archive_x, archive_f\n            if len(archive_x) < top_k:\n                archive_x.append(xn.copy()); archive_f.append(float(fn)); return\n            wi = int(np.argmax(archive_f))\n            if fn < archive_f[wi]:\n                archive_x[wi] = xn.copy(); archive_f[wi] = float(fn)\n\n        eps = 1e-12\n        while evals < self.budget:\n            arr = np.array(archive_x)\n            med = np.median(arr, axis=0)\n            q75 = np.percentile(arr, 75, axis=0)\n            q25 = np.percentile(arr, 25, axis=0)\n            iqr = np.maximum(q75 - q25, eps)\n\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            # score: normalized pull plus dispersion and recent success bias\n            score = abs_pull / (step + 1e-12) + 0.6 * (iqr / (span + 1e-12)) + 0.15 * (wins - losses)\n            score = np.maximum(score, 1e-9)\n\n            # adaptive temperature: when scores concentrated, increase exploration\n            tem = 0.8 + 0.6 * (1.0 - (score.std()/(score.mean()+1e-12)))\n            z = score - score.max()\n            probs = np.exp(z / tem)\n            probs = probs / probs.sum()\n\n            # pick coordinate\n            try:\n                coord = int(rng.choice(self.dim, p=probs))\n            except Exception:\n                coord = int(rng.integers(0, self.dim))\n\n            s = max(self.min_step, float(step[coord]))\n\n            # choose direction: toward median but injected noise from momentum and archive frequency\n            if abs_pull[coord] > 1e-12:\n                direction = np.sign(pull[coord])\n                # sometimes follow momentum if strong\n                if momentum[coord] != 0 and rng.random() < 0.25:\n                    if abs(momentum[coord]) > 0.3*s:\n                        direction = int(np.sign(momentum[coord]))\n                delta = direction * min(s, abs_pull[coord]) * (0.9 + 0.2 * rng.random())\n            else:\n                # no pull -> small random step biased by momentum\n                if momentum[coord] != 0 and rng.random() < 0.7:\n                    delta = np.sign(momentum[coord]) * s * (0.6 + 0.8 * rng.random())\n                else:\n                    delta = (1 if rng.random() < 0.5 else -1) * s * (0.6 + 0.8 * rng.random())\n\n            # single-coordinate probe\n            xt = x_best.copy()\n            xt[coord] = np.clip(xt[coord] + delta, lb[coord], ub[coord])\n            ft = float(func(xt)); evals += 1\n            add_archive(xt, ft)\n\n            improved = False\n            if ft < f_best:\n                f_best = ft; x_best = xt.copy(); improved = True\n                wins[coord] += 1; losses[coord] = 0\n                momentum[coord] = 0.75 * momentum[coord] + 0.25 * delta\n                step[coord] = min(max_step[coord], step[coord] * 1.18 + 1e-12)\n                # cheap extrapolation if budget allows\n                if evals < self.budget:\n                    ext = 0.45 * np.abs(delta)\n                    x_ext = x_best.copy()\n                    x_ext[coord] = np.clip(x_ext[coord] + np.sign(delta) * ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_archive(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = min(max_step[coord], step[coord] * 1.08)\n            else:\n                # opposite try once\n                if evals < self.budget:\n                    xt2 = x_best.copy()\n                    xt2[coord] = np.clip(xt2[coord] - delta, lb[coord], ub[coord])\n                    ft2 = float(func(xt2)); evals += 1\n                    add_archive(xt2, ft2)\n                    if ft2 < f_best:\n                        f_best = ft2; x_best = xt2.copy(); improved = True\n                        wins[coord] += 1; losses[coord] = 0\n                        momentum[coord] = 0.7*momentum[coord] - 0.3*delta\n                        step[coord] = min(max_step[coord], step[coord] * 1.12)\n                if not improved:\n                    losses[coord] += 1\n                    wins[coord] = max(0, wins[coord]-0.2)\n                    step[coord] = max(self.min_step, step[coord] * 0.62)\n                    momentum[coord] *= 0.65\n\n            # bounds and slow decay\n            step = np.minimum(np.maximum(step, self.min_step), max_step)\n            momentum *= 0.995\n            wins *= 0.999\n\n            if improved:\n                stagn = 0\n            else:\n                stagn += 1\n\n            # stagnation rescue: targeted Cauchy single-coordinate jumps (heavy-tail)\n            if stagn >= self.stagnation_patience and evals < self.budget:\n                # pick coord with large iqr or large step\n                cand = (iqr / (span + 1e-12)) + (step / (step.max()+1e-12))*0.3\n                cand = np.maximum(cand, 1e-9)\n                p = np.exp(cand - cand.max())\n                p = p / p.sum()\n                try:\n                    c = int(rng.choice(self.dim, p=p))\n                except Exception:\n                    c = int(rng.integers(0, self.dim))\n                # cauchy jump scaled by span and current step\n                jump = float(np.tan(np.pi*(rng.random()-0.5)))* (0.5*span[c] * (0.4 + 0.6*rng.random()) + 0.8*step[c])\n                xj = x_best.copy()\n                xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                fj = float(func(xj)); evals += 1\n                add_archive(xj, fj)\n                if fj < f_best:\n                    f_best = fj; x_best = xj.copy()\n                    step[c] = min(max_step[c], step[c]*1.4)\n                    momentum[c] = np.sign(jump)*min(abs(jump), 0.5*span[c])\n                else:\n                    # randomize a few steps to escape plateau\n                    step = np.minimum(max_step, step * (1.0 + 0.15 * rng.random(self.dim)))\n                    momentum *= 0.7\n                stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 23, "feedback": "The algorithm SeparableCumulativeMedianAnnealedPull scored 0.626 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9636033085036427}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9669960333055138}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9590740088830725}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8861273930667494}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9188458073224398}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8921841143009547}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.18613319919652305}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1634325430134237}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.13236721657198702}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10803314777814665}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.1280856437258664}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.0960325860774861}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9944871679530483}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9935920715774316}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9939550577646071}], "aucs": [0.9636033085036427, 0.9669960333055138, 0.9590740088830725, 0.8861273930667494, 0.9188458073224398, 0.8921841143009547, 0.18613319919652305, 0.1634325430134237, 0.13236721657198702, 0.10803314777814665, 0.1280856437258664, 0.0960325860774861, 0.9944871679530483, 0.9935920715774316, 0.9939550577646071]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2076.0, "Edges": 2075.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9990366088631986, "Degree Variance": 2.190750516964223, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.10126582278481, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.317057266900352, "Depth Entropy": 2.079733516196063, "Assortativity": 1.8342551045913725e-08, "Average Eccentricity": 16.13921001926782, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0004816955684007707, "Average Shortest Path": 9.787854771687908, "mean_complexity": 11.0, "total_complexity": 33.0, "mean_token_count": 616.3333333333334, "total_token_count": 1849.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "01c145e4-533e-4ce1-abb9-b7533759f951", "fitness": 0.5081593282424289, "name": "AdaptiveSubspaceQuadraticDescent", "description": "Adaptive Subspace Quadratic Descent (ASQD) \u2014 probe small random coordinate subspaces with mirrored \u00b1 probes, fit a 1\u2011D quadratic along the sampled direction to get an analytic step, adapt per-coordinate step sizes and occasionally perform long Cauchy jumps for escapes.", "code": "import numpy as np\n\nclass AdaptiveSubspaceQuadraticDescent:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 succ_mult=1.5, fail_mult=0.6,\n                 min_step_frac=1e-9, max_step_frac=1.2,\n                 global_prob=0.03, block_frac=0.25, max_extrap=2.0,\n                 polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.succ_mult = float(succ_mult); self.fail_mult = float(fail_mult)\n        self.min_pf = float(min_step_frac); self.max_pf = float(max_step_frac)\n        self.global_prob = float(global_prob)\n        self.block_frac = float(block_frac)\n        self.max_extrap = float(max_extrap)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * scale)\n        min_step = max(1e-15, self.min_pf * scale)\n        max_step = max(1e-12, self.max_pf * scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        steps = np.full(dim, base, float)\n        trials = np.zeros(dim, int); succ = np.zeros(dim, int)\n\n        while evals < self.budget:\n            # occasional global heavy-tail jump (Cauchy)\n            if rng.random() < self.global_prob:\n                u = rng.random(dim)\n                jump = 0.12 * scale * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x + jump, lb, ub)\n                fg = safe_eval(xg)\n                if fg < f:\n                    x, f = xg.copy(), fg\n                continue\n\n            # choose subspace size and dims (favor dims with larger steps)\n            p = np.clip(self.block_frac, 1e-6, 0.9)\n            k = 1 + rng.binomial(dim-1, p)\n            # weighted selection by steps\n            probs = steps + 1e-12\n            probs = probs / probs.sum()\n            dims = rng.choice(dim, size=k, replace=False, p=probs)\n\n            # random direction in subspace\n            vec = np.zeros(dim, float)\n            vec[dims] = rng.normal(size=len(dims))\n            norm = np.linalg.norm(vec[dims])\n            if norm == 0:\n                vec[dims] = 1.0; norm = 1.0\n            vec /= norm\n\n            s = float(np.clip(np.mean(steps[dims]), min_step, max_step))\n\n            # mirrored probe if budget allows\n            if evals <= self.budget - 2:\n                xp = np.clip(x + s * vec, lb, ub)\n                xm = np.clip(x - s * vec, lb, ub)\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[dims] += 2\n\n                # immediate improvement\n                if fp < f or fm < f:\n                    if fp < fm:\n                        x, f = xp.copy(), float(fp)\n                    else:\n                        x, f = xm.copy(), float(fm)\n                    succ[dims] += 1\n                    steps[dims] = np.minimum(max_step, steps[dims] * self.succ_mult)\n                    continue\n\n                # try analytic quadratic minimum along direction\n                denom = (fp + fm - 2.0 * f)\n                if abs(denom) > 1e-16:\n                    t_star = -(fp - fm) / (2.0 * denom)  # in units of s\n                    t_star = float(np.clip(t_star, -self.max_extrap, self.max_extrap))\n                    if abs(t_star) > 1e-12 and evals < self.budget:\n                        xt = np.clip(x + s * t_star * vec, lb, ub)\n                        ft = safe_eval(xt); trials[dims] += 1\n                        if ft < f:\n                            x, f = xt.copy(), float(ft)\n                            succ[dims] += 1\n                            steps[dims] = np.minimum(max_step, steps[dims] * self.succ_mult)\n                            continue\n                # no gain: shrink involved steps\n                steps[dims] = np.maximum(min_step, steps[dims] * self.fail_mult)\n            else:\n                # last evaluations: single sided probe\n                if rng.random() < 0.5:\n                    xt = np.clip(x + s * vec, lb, ub)\n                else:\n                    xt = np.clip(x - s * vec, lb, ub)\n                ft = safe_eval(xt); trials[dims] += 1\n                if ft < f:\n                    x, f = xt.copy(), float(ft)\n                    succ[dims] += 1\n                    steps[dims] = np.minimum(max_step, steps[dims] * self.succ_mult)\n                else:\n                    steps[dims] = np.maximum(min_step, steps[dims] * self.fail_mult)\n\n            # mild rescue: if some dims tried a lot without success, boost\n            idx = (trials > 80) & (succ == 0)\n            if np.any(idx):\n                steps[idx] = np.minimum(max_step, steps[idx] * 1.8)\n                trials[idx] = (trials[idx] // 2)\n\n        # cheap coordinate polish\n        fin = self.polish_frac * scale\n        while evals < self.budget and fin > 1e-12 * scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 23, "feedback": "The algorithm AdaptiveSubspaceQuadraticDescent scored 0.508 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["86d13d75-4636-4c2f-9196-d0f90890a1f4"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9587752303539208}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8089321826217978}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.6855004264241745}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.955686372306255}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9444288307232915}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9459145215376269}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.020877290942880378}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.08478350604603524}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.03093285448808314}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.04533844034528478}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.045420726215608775}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.021461767094906326}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.12437635058343988}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9663379502339275}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9836234737192022}], "aucs": [0.9587752303539208, 0.8089321826217978, 0.6855004264241745, 0.955686372306255, 0.9444288307232915, 0.9459145215376269, 0.020877290942880378, 0.08478350604603524, 0.03093285448808314, 0.04533844034528478, 0.045420726215608775, 0.021461767094906326, 0.12437635058343988, 0.9663379502339275, 0.9836234737192022]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1639.0, "Edges": 1638.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.9987797437461867, "Degree Variance": 2.205001561615309, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 7.786381842456609, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3307298335923519, "Depth Entropy": 2.028674966215775, "Assortativity": 1.1701338208093557e-08, "Average Eccentricity": 15.226357535082368, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0006101281269066504, "Average Shortest Path": 9.595289125490467, "mean_complexity": 10.666666666666666, "total_complexity": 32.0, "mean_token_count": 474.6666666666667, "total_token_count": 1424.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "703d9c83-e2c2-4f0e-831d-e66095f1fb07", "fitness": 0.7291154123443294, "name": "AdaptiveMedianBanditPull", "description": "Coordinatewise median-pull bandit with compact archive, adaptive per-dim steps, lightweight momentum and rare L\u00e9vy escapes \u2014 cheap separable probes prioritized by median pull, IQR and bandit sign priors.", "code": "import numpy as np\n\nclass AdaptiveMedianBanditPull:\n    def __init__(self, budget, dim, archive_size=8, init_step=None, min_step=1e-6,\n                 max_step=None, patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb, ub = -5.0, 5.0\n        span = (ub - lb) * np.ones(d)\n        # init steps\n        if self.init_step is None:\n            step = 0.12 * span\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = 0.5 * span\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = min(12, max(3, min(d+1, max(1, self.budget//8))))\n        X = []\n        F = []\n        while evals < self.budget and len(X) < n_init:\n            x = lb + self.rng.random(d) * span\n            F.append(float(func(x))); evals += 1\n            X.append(x.copy())\n        if not X:\n            x = lb + self.rng.random(d) * span\n            F.append(float(func(x))); evals += 1\n            X.append(x.copy())\n        X = np.vstack(X); F = np.array(F, float)\n        # keep compact best archive\n        def arc_add(xn, fn):\n            nonlocal X, F\n            fn = float(fn)\n            if X.shape[0] < self.archive_size:\n                X = np.vstack([X, xn.copy()]); F = np.append(F, fn)\n            else:\n                w = int(np.argmax(F))\n                if fn < F[w]:\n                    X[w] = xn.copy(); F[w] = fn\n\n        topk = min(self.archive_size, X.shape[0])\n        bi = np.ones(d); bj = np.ones(d)  # sign-bandit (success/fail)\n        momentum = np.zeros(d)\n        priority = np.zeros(d)\n        no_improve = 0\n        idx = int(np.argmin(F))\n        x_best = X[idx].copy(); f_best = float(F[idx])\n\n        while evals < self.budget:\n            med = np.median(X, axis=0)\n            q75 = np.percentile(X, 75, axis=0)\n            q25 = np.percentile(X, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n            # score dims by pull/step, IQR and priority\n            score = (abs_pull / (step + 1e-12)) + 0.2 * (iqr / (span + 1e-12)) + 0.08 * priority\n            score = np.maximum(score, 1e-12)\n            s = (score - score.max())\n            temp = 0.9 + 0.2 * np.std(score)\n            probs = np.exp(s / max(1e-6, temp))\n            probs /= probs.sum()\n            coord = int(self.rng.choice(d, p=probs))\n            # direction by bandit biased toward nominal pull sign\n            p_follow = bi[coord] / (bi[coord] + bj[coord])\n            if abs_pull[coord] > 1e-12:\n                nominal = int(np.sign(pull[coord]))\n            else:\n                nominal = int(np.sign(momentum[coord])) if momentum[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nominal if self.rng.random() < p_follow else -nominal\n            frac = min(1.0, abs_pull[coord] / (step[coord] + 1e-12))\n            mag = step[coord] * (0.7 + 0.6 * self.rng.random()) * (0.5 + 0.5 * frac)\n            delta = float(sign * mag)\n            # try\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb, ub)\n            f_try = float(func(x_try)); evals += 1\n            arc_add(x_try, f_try)\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy(); improved = True\n                if sign == nominal: bi[coord] += 1.0\n                else: bj[coord] += 0.6\n                priority[coord] = min(8.0, priority[coord] + 1.0)\n                momentum[coord] = 0.7 * momentum[coord] + 0.3 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * 1.18))\n                no_improve = 0\n                # light extrapolation\n                if evals < self.budget:\n                    ext = 0.4 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb, ub)\n                    f_ext = float(func(x_ext)); evals += 1\n                    arc_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                        momentum[coord] = 0.8 * momentum[coord] + 0.2 * ext\n            else:\n                # mirrored probe\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb, ub)\n                    f_opp = float(func(x_opp)); evals += 1\n                    arc_add(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy(); improved = True\n                        bj[coord] += 1.0\n                        priority[coord] = min(8.0, priority[coord] + 0.8)\n                        momentum[coord] = 0.65 * momentum[coord] + 0.35 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.16))\n                        no_improve = 0\n                    else:\n                        bj[coord] += 0.4\n                        bi[coord] = max(1.0, bi[coord] * 0.995)\n                        priority[coord] = max(-8.0, priority[coord] - 0.45)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.65))\n                        momentum[coord] *= 0.6\n                        no_improve += 1\n                else:\n                    no_improve += 1\n            # stagnation rescue: targeted coordinate jumps, then L\u00e9vy-style global tries\n            if no_improve >= self.patience and evals < self.budget:\n                no_improve = 0\n                cand = (iqr / (span + 1e-12)) * (1.0 / (step + 1e-12) + 0.1)\n                cand = np.maximum(cand, 1e-12)\n                s = cand - cand.max()\n                w = np.exp(s); w /= w.sum()\n                k = min(3, max(1, d//6))\n                picks = list(self.rng.choice(d, size=min(d, k), replace=False, p=w))\n                improved_any = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = 0.5 * step[c] + 0.3 * span[c] * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb, ub)\n                    fj = float(func(xj)); evals += 1\n                    arc_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step[c] = float(min(max_step[c], step[c] * 1.2))\n                        priority[c] = min(8.0, priority[c] + 1.0)\n                        momentum[c] = np.sign(jump) * 0.5 * abs(jump)\n                        improved_any = True\n                        break\n                if not improved_any and evals < self.budget:\n                    # global heavy-tail (Cauchy) but applied sparsely and scaled per-dim\n                    scale = np.maximum(0.06 * step, 0.03 * span)\n                    u = self.rng.random(d)\n                    levy = np.tan(np.pi * (u - 0.5)) * scale * (0.5 + self.rng.random(d))\n                    mask = self.rng.random(d) < 0.25\n                    xj = np.clip(x_best + levy * mask, lb, ub)\n                    for i in range(d):\n                        if evals >= self.budget: break\n                        if not mask[i]: continue\n                        xt = x_best.copy(); xt[i] = xj[i]\n                        fv = float(func(xt)); evals += 1\n                        arc_add(xt, fv)\n                        if fv < f_best:\n                            f_best = fv; x_best = xt.copy()\n                            step[i] = float(min(max_step[i], step[i] * 1.18))\n                            priority[i] = min(8.0, priority[i] + 1.0)\n                            momentum[i] = np.sign(xj[i] - x_best[i]) * 0.5 * abs(xj[i] - x_best[i])\n                            break\n            # decay & trim\n            priority *= 0.994\n            momentum *= 0.99\n            if X.shape[0] > self.archive_size:\n                order = np.argsort(F)\n                keep = order[:self.archive_size]\n                X = X[keep]; F = F[keep]\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 23, "feedback": "The algorithm AdaptiveMedianBanditPull scored 0.729 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b06e4fac-53b8-4a0f-9352-5fad9fc29a61"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9806959959664129}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9805050357570239}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9780684827139996}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9070232360800154}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9557604116523332}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8955256855358877}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.5013137668918632}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.6433936079923116}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.5191075873192106}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1294860450776414}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.35352320920161084}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.11573355029814447}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9907731364836394}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9927763455158739}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9930450886789715}], "aucs": [0.9806959959664129, 0.9805050357570239, 0.9780684827139996, 0.9070232360800154, 0.9557604116523332, 0.8955256855358877, 0.5013137668918632, 0.6433936079923116, 0.5191075873192106, 0.1294860450776414, 0.35352320920161084, 0.11573355029814447, 0.9907731364836394, 0.9927763455158739, 0.9930450886789715]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2405.0, "Edges": 2404.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9991683991683993, "Degree Variance": 2.237005545446294, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.429475587703436, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3159274795600915, "Depth Entropy": 2.139986989827008, "Assortativity": 1.2874373767565987e-08, "Average Eccentricity": 16.81829521829522, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.0004158004158004158, "Average Shortest Path": 10.158111394384273, "mean_complexity": 12.333333333333334, "total_complexity": 37.0, "mean_token_count": 705.3333333333334, "total_token_count": 2116.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "12719877-2b4a-4662-8362-a94b9e643139", "fitness": 0.6119140404519957, "name": "SeparableCumulativeMedianPull", "description": "Coordinate-wise median-guided separable pulls with compact top-k archive, multiplicative step adaptation, lightweight momentum and prioritized 1\u2011D diversification including rare Cauchy escapes \u2014 cheap, robust separable probes with targeted stagnation rescues.", "code": "import numpy as np\n\nclass SeparableCumulativeMedianPull:\n    \"\"\"\n    Compact separable optimizer: single-coordinate pulls toward archive medians,\n    multiplicative per-coordinate step adaption, lightweight momentum, prioritized\n    coordinate sampling, and rare heavy-tail escapes on stagnation.\n    \"\"\"\n    def __init__(self, budget, dim, archive_size=12, init_frac=0.20,\n                 min_step=1e-6, max_frac=0.5, stagnation_patience=10, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.init_frac = float(init_frac)   # fraction of range for initial step\n        self.min_step = float(min_step)\n        self.max_frac = float(max_frac)\n        self.stagnation_patience = int(stagnation_patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n\n    def _bounds(self, func):\n        lb = -5.0; ub = 5.0\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                lb = getattr(b, \"lb\", lb)\n                ub = getattr(b, \"ub\", ub)\n            except Exception:\n                pass\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._bounds(func)\n        rng = self.rng\n        span = ub - lb\n        max_step = np.maximum(self.max_frac * span, self.min_step)\n        step = np.clip(self.init_frac * span, self.min_step, max_step)\n\n        # small top-k archive\n        topk = min(self.archive_size, max(2, self.budget // 8))\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n\n        # initial random seeds (conservative)\n        init_n = min(max(4, self.dim // 2), max(1, self.budget // 15))\n        for _ in range(init_n):\n            x = lb + rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n            if evals >= self.budget:\n                break\n\n        if len(archive_x) == 0:\n            x = lb + rng.random(self.dim) * span\n            f = float(func(x)); evals += 1\n            archive_x.append(x.copy()); archive_f.append(f)\n\n        best_idx = int(np.argmin(archive_f))\n        x_best = archive_x[best_idx].copy()\n        f_best = float(archive_f[best_idx])\n\n        # state\n        momentum = np.zeros(self.dim, dtype=float)\n        score_bias = np.zeros(self.dim, dtype=float)   # favors coords with recent success\n        success = np.zeros(self.dim, dtype=int)\n        fail = np.zeros(self.dim, dtype=int)\n        stagn = 0\n\n        def add_archive(xn, fn):\n            nonlocal archive_x, archive_f\n            if len(archive_x) < topk:\n                archive_x.append(xn.copy()); archive_f.append(float(fn)); return\n            wi = int(np.argmax(archive_f))\n            if fn < archive_f[wi]:\n                archive_x[wi] = xn.copy(); archive_f[wi] = float(fn)\n\n        # main loop\n        while evals < self.budget:\n            A = np.array(archive_x)\n            med = np.median(A, axis=0)\n            q75 = np.percentile(A, 75, axis=0)\n            q25 = np.percentile(A, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            # coordinate preference: pull magnitude normalized by step, plus dispersion and bias\n            pref = (abs_pull / (step + 1e-12)) + 0.6 * (iqr / (span + 1e-12))\n            # incorporate score bias (recent successes)\n            pref = np.maximum(pref, 1e-6) * (1.0 + 0.18 * score_bias)\n            # softprob\n            s = pref - pref.max()\n            temp = 1.0 + 0.25 * np.std(pref)\n            p = np.exp(s / temp)\n            p = p / p.sum()\n\n            try:\n                coord = int(rng.choice(self.dim, p=p))\n            except Exception:\n                coord = int(rng.integers(0, self.dim))\n\n            # decide delta sign and magnitude\n            s_step = float(max(step[coord], self.min_step))\n            if abs_pull[coord] > 1e-12:\n                sign = int(np.sign(pull[coord]))\n                # sometimes follow momentum if strong\n                if momentum[coord] and rng.random() < 0.18:\n                    if abs(momentum[coord]) > 0.5 * s_step:\n                        sign = int(np.sign(momentum[coord]))\n                delta = sign * min(s_step, abs_pull[coord])\n            else:\n                # no strong pull: use momentum or random\n                if momentum[coord] != 0.0 and rng.random() < 0.7:\n                    delta = np.sign(momentum[coord]) * s_step\n                else:\n                    delta = s_step * (1 if rng.random() < 0.5 else -1)\n\n            # trial forward\n            x1 = x_best.copy()\n            x1[coord] = np.clip(x1[coord] + delta, lb[coord], ub[coord])\n            f1 = float(func(x1)); evals += 1\n            add_archive(x1, f1)\n\n            improved = False\n            if f1 < f_best:\n                f_best = f1; x_best = x1; improved = True\n                success[coord] += 1; fail[coord] = 0\n                score_bias[coord] = min(8.0, score_bias[coord] + 1.0)\n                momentum[coord] = 0.75 * momentum[coord] + 0.25 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * 1.25 + 1e-12))\n                # cheap extrapolation\n                if evals < self.budget:\n                    ext = 0.5 * abs(delta)\n                    x_ext = x_best.copy()\n                    x_ext[coord] = np.clip(x_ext[coord] + np.sign(delta) * ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_archive(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12 + 1e-12))\n            else:\n                # try opposite direction once\n                if evals < self.budget:\n                    x2 = x_best.copy()\n                    x2[coord] = np.clip(x2[coord] - delta, lb[coord], ub[coord])\n                    f2 = float(func(x2)); evals += 1\n                    add_archive(x2, f2)\n                    if f2 < f_best:\n                        f_best = f2; x_best = x2; improved = True\n                        success[coord] += 1; fail[coord] = 0\n                        score_bias[coord] = min(8.0, score_bias[coord] + 0.9)\n                        momentum[coord] = 0.7 * momentum[coord] - 0.3 * delta\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.18 + 1e-12))\n                    else:\n                        fail[coord] += 1\n                        score_bias[coord] = max(-8.0, score_bias[coord] - 0.6)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.55))\n                        momentum[coord] *= 0.6\n\n            # normalize/clip\n            step = np.minimum(np.maximum(step, self.min_step), max_step)\n            score_bias *= 0.996\n            momentum *= 0.995\n\n            if improved:\n                stagn = 0\n            else:\n                stagn += 1\n\n            # stagnation rescue: targeted multi-trial single-coordinate diversification,\n            # prefer coords with high iqr or large step-to-span ratio.\n            if stagn >= self.stagnation_patience and evals < self.budget:\n                cand = (iqr / (span + 1e-12)) + 0.5 * (step / (step.max() + 1e-12))\n                cand = np.maximum(cand, 1e-8)\n                s2 = cand - cand.max()\n                pc = np.exp(s2)\n                pc = pc / pc.sum()\n                ktry = min(3, max(1, self.dim // 6))\n                try:\n                    coords = list(rng.choice(self.dim, size=ktry, replace=False, p=pc))\n                except Exception:\n                    coords = list(rng.choice(self.dim, size=ktry, replace=False))\n                made_improve = False\n                for c in coords:\n                    if evals >= self.budget: break\n                    # jump combines current step and a random fraction of span\n                    jump = (0.5 * step[c] + 0.4 * rng.random() * span[c])\n                    if rng.random() < 0.5: jump *= -1.0\n                    xj = x_best.copy()\n                    xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    add_archive(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj\n                        step[c] = float(min(max_step[c], step[c] * 1.35 + 1e-12))\n                        score_bias[c] = min(8.0, score_bias[c] + 1.2)\n                        momentum[c] = np.sign(jump) * 0.6 * abs(jump)\n                        made_improve = True\n                        break\n                if not made_improve:\n                    # rare heavy-tail global nudge (Cauchy) scaled to span, small chance triggers stronger nudge\n                    if rng.random() < 0.35 and evals < self.budget:\n                        scale = 0.1 + 0.4 * rng.random()\n                        xg = x_best + np.tan(np.pi * (rng.random(self.dim) - 0.5)) * (scale * span)\n                        xg = np.clip(xg, lb, ub)\n                        fg = float(func(xg)); evals += 1\n                        add_archive(xg, fg)\n                        if fg < f_best:\n                            f_best = fg; x_best = xg\n                            step *= 1.12\n                    # slightly jitter steps to escape correlated stalls\n                    step = np.minimum(max_step, step * (1.0 + 0.15 * rng.random(self.dim)))\n                stagn = 0\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 23, "feedback": "The algorithm SeparableCumulativeMedianPull scored 0.612 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea0b2f60-c8fa-4fa1-b20b-d3abd7566d1e"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9786556073962444}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9763292359901561}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9804935190348738}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8680831430085606}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8321918362799375}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8542678418550782}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.11421115070822307}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.14112783406504692}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.1200458279790777}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1099341267749373}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.09590620219425305}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.12436640934064858}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9932709344796481}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9964629603580473}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9933639773152038}], "aucs": [0.9786556073962444, 0.9763292359901561, 0.9804935190348738, 0.8680831430085606, 0.8321918362799375, 0.8542678418550782, 0.11421115070822307, 0.14112783406504692, 0.1200458279790777, 0.1099341267749373, 0.09590620219425305, 0.12436640934064858, 0.9932709344796481, 0.9964629603580473, 0.9933639773152038]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2312.0, "Edges": 2311.0, "Max Degree": 30.0, "Min Degree": 1.0, "Mean Degree": 1.9991349480968859, "Degree Variance": 2.125431777636762, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.277142857142858, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.322584153261093, "Depth Entropy": 2.144777759245702, "Assortativity": 0.0, "Average Eccentricity": 17.91392733564014, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00043252595155709344, "Average Shortest Path": 10.206238330595811, "mean_complexity": 10.0, "total_complexity": 40.0, "mean_token_count": 509.5, "total_token_count": 2038.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "e8767532-db1e-419c-975f-93e594a76e91", "fitness": 0.8017277788449616, "name": "SeparableLeapfrogPull", "description": "Separable Leapfrog Pulls \u2014 lightweight coordinate-wise pulls toward an archive centroid with multiplicative step adaptation, quick mirrored checks, occasional multi\u2011dim leapfrogs and rare Cauchy escapes for robust cheap exploitation.", "code": "import numpy as np\n\nclass SeparableLeapfrogPull:\n    def __init__(self, budget, dim, archive_size=6, init_step=None, min_step=1e-6, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0 * np.ones(d); ub = 5.0 * np.ones(d); span = ub - lb\n        # steps\n        if self.init_step is None:\n            step = np.full(d, 0.12 * (span))\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        step = np.maximum(step, self.min_step)\n\n        evals = 0\n        n_init = min(max(2, d+1), max(1, self.budget // 12), 10)\n        arc_x = []\n        arc_f = []\n        for _ in range(n_init):\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n            if evals >= self.budget: break\n        if not arc_x:\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n\n        arc_x = np.vstack(arc_x); arc_f = np.array(arc_f, float)\n        k = min(self.archive_size, arc_x.shape[0])\n        best_idx = int(np.argmin(arc_f))\n        x_best = arc_x[best_idx].copy(); f_best = float(arc_f[best_idx])\n\n        # lightweight counters\n        succ = np.ones(d) * 1.0\n        fail = np.ones(d) * 1.0\n        stale = 0\n\n        def add_archive(xn, fn):\n            nonlocal arc_x, arc_f, k\n            fn = float(fn)\n            if arc_x.shape[0] < k:\n                arc_x = np.vstack([arc_x, xn.copy()]); arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < self.budget:\n            # centroid guidance\n            med = np.median(arc_x, axis=0)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n            # coordinate scores: prefer directions with pull and small step\n            score = (abs_pull / (step + 1e-12)) + 0.08 * (1.0/(1.0+fail)) \n            score = np.maximum(score, 1e-12)\n            p = score / score.sum()\n            coord = int(self.rng.choice(d, p=p))\n            # decide sign biased by pull\n            if abs_pull[coord] > 1e-12:\n                prefer = int(np.sign(pull[coord]))\n            else:\n                prefer = 1 if self.rng.random() < 0.5 else -1\n            p_pref = succ[coord] / (succ[coord] + fail[coord])\n            sign = prefer if self.rng.random() < p_pref else -prefer\n            # magnitude mixes step with remaining pull and a small random factor\n            frac = min(1.0, abs_pull[coord] / (step[coord] + 1e-12))\n            mag = step[coord] * (0.7 + 0.6 * self.rng.random()) * (0.5 + 0.5*frac)\n            delta = float(sign * mag)\n            # primary probe\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            add_archive(x_try, f_try)\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try; improved = True\n                succ[coord] += 1.0\n                step[coord] = float(min(0.5*span[coord], step[coord]*1.22 + 1e-12))\n                # small forward extrapolation attempt\n                if evals < self.budget:\n                    ext = 0.45 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_archive(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext\n                        step[coord] = float(min(0.5*span[coord], step[coord]*1.12))\n                stale = 0\n            else:\n                # mirrored probe\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    add_archive(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp; improved = True\n                        fail[coord] += 0.8\n                        step[coord] = float(min(0.5*span[coord], step[coord]*1.18))\n                        stale = 0\n                    else:\n                        fail[coord] += 0.6\n                        succ[coord] = max(1.0, succ[coord]*0.995)\n                        step[coord] = float(max(self.min_step, step[coord]*0.62))\n                        stale += 1\n                else:\n                    stale += 1\n\n            # occasional leapfrog: multi-dim correlated push (cheap)\n            if (self.rng.random() < 0.08) and evals < self.budget:\n                scale = 0.6 * step + 0.02 * span\n                jump = self.rng.normal(size=d) * scale * (0.6 + 0.8*self.rng.random(d))\n                # bias toward median\n                jump = 0.6*(med - x_best) * (np.abs(med - x_best) > 1e-12) + 0.4*jump\n                x_lf = np.clip(x_best + jump, lb, ub)\n                # evaluate few coords sequentially until budget\n                for i in np.argsort(-np.abs(jump)):\n                    if evals >= self.budget: break\n                    if np.isclose(x_lf[i], x_best[i]): continue\n                    xt = x_best.copy(); xt[i] = x_lf[i]\n                    ft = float(func(xt)); evals += 1\n                    add_archive(xt, ft)\n                    if ft < f_best:\n                        f_best = ft; x_best = xt\n                        step[i] = float(min(0.5*span[i], step[i]*1.2))\n                        stale = 0\n                        break\n\n            # rare heavy-tail escape if stale\n            if stale > max(6, d//3) and evals < self.budget:\n                stale = 0\n                scale = np.maximum(0.08*step, 0.03*span)\n                # Cauchy-like by tan(pi*(u-0.5))\n                u = self.rng.random(d); cauch = np.tan(np.pi*(u-0.5))\n                jump = cauch * scale * (0.4 + 0.8*self.rng.random(d))\n                xj = np.clip(x_best + jump, lb, ub)\n                for i in range(d):\n                    if evals >= self.budget: break\n                    if np.isclose(xj[i], x_best[i]): continue\n                    xt = x_best.copy(); xt[i] = xj[i]\n                    ft = float(func(xt)); evals += 1\n                    add_archive(xt, ft)\n                    if ft < f_best:\n                        f_best = ft; x_best = xt\n                        step[i] = float(min(0.5*span[i], step[i]*1.3))\n                        break\n\n            # maintain archive compactness\n            if arc_x.shape[0] > self.archive_size:\n                idxs = np.argsort(arc_f)[:self.archive_size]\n                arc_x = arc_x[idxs]; arc_f = arc_f[idxs]\n\n            # mild decay to encourage diversity\n            succ *= 0.998; fail *= 0.999\n            step = np.minimum(0.5*span, np.maximum(self.min_step, step))\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 23, "feedback": "The algorithm SeparableLeapfrogPull scored 0.802 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad9d28ed-2806-47c9-b4ca-e97b1d7b4fc7"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9635850935881498}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9558458236044449}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9678717415617506}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8896476847997024}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8990159036812213}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8699316682173559}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.74672726090373}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.5818164461708951}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.7074753710897164}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.6201855948202057}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.34319573529525227}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.49789224052003334}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9940365812442791}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9948366026177073}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9938529345599814}], "aucs": [0.9635850935881498, 0.9558458236044449, 0.9678717415617506, 0.8896476847997024, 0.8990159036812213, 0.8699316682173559, 0.74672726090373, 0.5818164461708951, 0.7074753710897164, 0.6201855948202057, 0.34319573529525227, 0.49789224052003334, 0.9940365812442791, 0.9948366026177073, 0.9938529345599814]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1950.0, "Edges": 1949.0, "Max Degree": 28.0, "Min Degree": 1.0, "Mean Degree": 1.998974358974359, "Degree Variance": 2.1241015121630507, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.111731843575418, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3201676463295813, "Depth Entropy": 2.036715885499052, "Assortativity": 1.5097061949386724e-08, "Average Eccentricity": 15.107179487179486, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0005128205128205128, "Average Shortest Path": 9.740844351475443, "mean_complexity": 10.666666666666666, "total_complexity": 32.0, "mean_token_count": 568.0, "total_token_count": 1704.0, "mean_parameter_count": 3.6666666666666665, "total_parameter_count": 11.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "4611ec80-f851-4341-808f-be0f9e2df856", "fitness": 0.6372796784967069, "name": "ParabolicAdaptiveMirroredDescentImproved", "description": "Parabolic Adaptive Mirrored Descent v2 \u2014 compact mirrored 3\u2011point parabolic coordinate fits with dynamic coordinate weighting, multiplicative step adaption, tiny median-archive pulls and occasional Cauchy escapes for robust cheap exploitation.", "code": "import numpy as np\n\nclass ParabolicAdaptiveMirroredDescentImproved:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 succ_mult=1.5, fail_mult=0.6, min_step_frac=1e-9,\n                 max_step_frac=1.2, global_prob=0.03, max_extrap=2.0,\n                 archive_size=5, polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step = float(init_step)\n        self.succ_mult = float(succ_mult); self.fail_mult = float(fail_mult)\n        self.min_pf = float(min_step_frac); self.max_pf = float(max_step_frac)\n        self.global_prob = float(global_prob); self.max_extrap = float(max_extrap)\n        self.archive_size = int(archive_size); self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, float(lb))\n            if ub.shape == (): ub = np.full(d, float(ub))\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step * scale)\n        min_step = max(1e-15, self.min_pf * scale)\n        max_step = max(1e-12, self.max_pf * scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        steps = np.full(d, base, float)\n        trials = np.zeros(d, int); succ = np.zeros(d, int)\n        archive = [(f, x.copy())]\n        stagn = 0\n\n        while evals < self.budget:\n            # rare heavy-tail global jump\n            if rng.random() < self.global_prob:\n                u = rng.random(d)\n                jump = 0.12 * scale * np.tan(np.pi * (u - 0.5))\n                xt = np.clip(x + jump, lb, ub); ft = safe_eval(xt)\n                if ft < f:\n                    f = ft; x = xt.copy(); succ[:] = 0\n                    archive.append((f, x.copy()))\n                    archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                    stagn = 0\n                else:\n                    stagn += 1\n                continue\n\n            # coordinate weighting: prefer larger steps, fewer trials, and dims with archive variance\n            if len(archive) > 1:\n                arr = np.vstack([xx for _, xx in archive])\n                varw = np.clip(np.var(arr, axis=0), 1e-12, None)\n                wvar = varw / (np.sum(varw) + 1e-12)\n            else:\n                wvar = np.ones(d) / d\n\n            score = steps * (1.0 + 5.0 * wvar) / (1.0 + 0.5 * trials)\n            dsel = int(np.argmax(score + 1e-12 * rng.random(d)))\n            s = float(np.clip(steps[dsel], min_step, max_step))\n            f0 = f; x0 = x.copy()\n\n            # mirrored 2 probes if budget allows\n            if evals <= self.budget - 2:\n                xp = x0.copy(); xm = x0.copy()\n                xp[dsel] = np.clip(xp[dsel] + s, lb[dsel], ub[dsel])\n                xm[dsel] = np.clip(xm[dsel] - s, lb[dsel], ub[dsel])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[dsel] += 2\n\n                # accept immediate better probe\n                if fp < f0 or fm < f0:\n                    if fp < fm:\n                        x, f = xp.copy(), float(fp)\n                    else:\n                        x, f = xm.copy(), float(fm)\n                    succ[dsel] += 1; steps[dsel] = min(max_step, steps[dsel]*self.succ_mult)\n                    archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                    stagn = 0\n                else:\n                    # parabolic minimizer (centered at 0)\n                    denom = (fp + fm - 2.0 * f0)\n                    if denom != 0:\n                        x_star = -s * (fp - fm) / (2.0 * denom)\n                        x_star = float(np.clip(x_star, -self.max_extrap * s, self.max_extrap * s))\n                        if abs(x_star) > 1e-14 and evals < self.budget:\n                            xt = x0.copy(); xt[dsel] = np.clip(xt[dsel] + x_star, lb[dsel], ub[dsel])\n                            ft = safe_eval(xt); trials[dsel] += 1\n                            if ft < f0:\n                                x, f = xt.copy(), float(ft); succ[dsel] += 1\n                                steps[dsel] = min(max_step, steps[dsel]*self.succ_mult)\n                                archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                                stagn = 0\n                            else:\n                                steps[dsel] = max(min_step, steps[dsel] * self.fail_mult); stagn += 1\n                        else:\n                            steps[dsel] = max(min_step, steps[dsel] * self.fail_mult); stagn += 1\n                    else:\n                        steps[dsel] = max(min_step, steps[dsel] * self.fail_mult); stagn += 1\n            else:\n                # last eval(s): single probe\n                if rng.random() < 0.5:\n                    xt = x.copy(); xt[dsel] = np.clip(xt[dsel] + s, lb[dsel], ub[dsel])\n                else:\n                    xt = x.copy(); xt[dsel] = np.clip(xt[dsel] - s, lb[dsel], ub[dsel])\n                ft = safe_eval(xt); trials[dsel] += 1\n                if ft < f:\n                    x, f = xt.copy(), float(ft); succ[dsel] += 1\n                    steps[dsel] = min(max_step, steps[dsel]*self.succ_mult)\n                    archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                    stagn = 0\n                else:\n                    steps[dsel] = max(min_step, steps[dsel] * self.fail_mult); stagn += 1\n\n            # rescue: boost dims with many trials but zero successes\n            if trials[dsel] > 50 and succ[dsel] == 0:\n                steps[dsel] = min(max_step, steps[dsel] * 1.7)\n                trials[dsel] //= 2\n\n            # small archive median pull when stagnating\n            if stagn > max(25, int(0.02 * self.budget)) and len(archive) > 1:\n                meds = np.median(np.vstack([xx for _, xx in archive]), axis=0)\n                diff = meds - x\n                dd = int(np.argmax(np.abs(diff)))\n                pull = 0.5 * np.sign(diff[dd]) * min(steps[dd], abs(diff[dd]))\n                xt = x.copy(); xt[dd] = np.clip(xt[dd] + pull, lb[dd], ub[dd])\n                ft = safe_eval(xt); trials[dd] += 1\n                if ft < f:\n                    x, f = xt.copy(), float(ft); succ[dd] += 1\n                    steps[dd] = min(max_step, steps[dd]*self.succ_mult)\n                    archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                    stagn = 0\n                else:\n                    stagn += 1\n                    # tiny local random nudge\n                    if evals < self.budget:\n                        j = rng.integers(0, d)\n                        xt = x.copy(); xt[j] = np.clip(xt[j] + rng.normal(0, 0.5*steps[j]), lb[j], ub[j])\n                        ft = safe_eval(xt); trials[j] += 1\n                        if ft < f:\n                            x, f = xt.copy(), float(ft); succ[j] += 1\n                            steps[j] = min(max_step, steps[j]*self.succ_mult)\n                            archive.append((f, x.copy())); archive.sort(key=lambda z: z[0]); archive = archive[:self.archive_size]\n                            stagn = 0\n\n        # final coordinate polish (cheap)\n        fin = self.polish_frac * scale\n        while evals < self.budget and fin > 1e-12 * scale:\n            improved = False\n            for j in range(d):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[j] = np.clip(xt[j] + fin, lb[j], ub[j]); ft = safe_eval(xt)\n                if ft < f: x, f, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[j] = np.clip(xt[j] - fin, lb[j], ub[j]); ft = safe_eval(xt)\n                if ft < f: x, f, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 23, "feedback": "The algorithm ParabolicAdaptiveMirroredDescentImproved scored 0.637 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6164ba35-94f3-4dbe-a19d-3a3b0bbfe0d1"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9957139263125944}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9893120751197149}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9878057136264545}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9617533935337612}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.964316723686905}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9556369373221716}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1039302836361965}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.16736837167494212}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.10855395640936216}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.11415916601646936}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.10850379831369006}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.11389438595723345}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9965656412626018}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9968237501633243}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9948570544151818}], "aucs": [0.9957139263125944, 0.9893120751197149, 0.9878057136264545, 0.9617533935337612, 0.964316723686905, 0.9556369373221716, 0.1039302836361965, 0.16736837167494212, 0.10855395640936216, 0.11415916601646936, 0.10850379831369006, 0.11389438595723345, 0.9965656412626018, 0.9968237501633243, 0.9948570544151818]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2483.0, "Edges": 2482.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.999194522754732, "Degree Variance": 2.0265801003002455, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.663708961845607, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3274713499066897, "Depth Entropy": 2.1944603036558434, "Assortativity": 0.0, "Average Eccentricity": 17.389045509464356, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0004027386226339106, "Average Shortest Path": 10.678872578497522, "mean_complexity": 13.333333333333334, "total_complexity": 40.0, "mean_token_count": 717.3333333333334, "total_token_count": 2152.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "789bbf7d-eb6f-4e10-b655-d33af6a5b5d4", "fitness": 0.7099529083243006, "name": "SeparableAdaptiveMedianPull", "description": "Coordinate-wise median-guided pulls with Beta-sign priors, compact archive, multiplicative per-dim step adaption, light momentum and rare heavy-tail escapes for cheap robust separable exploitation.", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPull:\n    def __init__(self, budget, dim, archive_size=10, init_step=None, min_step=1e-6,\n                 max_step=None, patience=8, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0 * np.ones(d); ub = 5.0 * np.ones(d)\n        span = ub - lb\n        # steps\n        if self.init_step is None:\n            step = np.full(d, 0.15 * span[0]) if np.isscalar(span) else 0.15 * span\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = np.full(d, 0.5 * span[0]) if np.isscalar(span) else 0.5 * span\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = min(max(3, d + 1), max(1, self.budget // 10), 12)\n        arc_x = []\n        arc_f = []\n        for _ in range(n_init):\n            if evals >= self.budget: break\n            x = lb + self.rng.random(d) * span\n            arc_x.append(x.copy()); arc_f.append(float(func(x))); evals += 1\n        if not arc_x:\n            x = lb + self.rng.random(d) * span\n            arc_x.append(x.copy()); arc_f.append(float(func(x))); evals += 1\n\n        arc_x = np.vstack(arc_x)\n        arc_f = np.array(arc_f, float)\n        topk = min(self.archive_size, arc_x.shape[0])\n\n        bi = np.ones(d); bj = np.ones(d)        # Beta-like counts favoring sign follow / flip\n        mom = np.zeros(d)\n        prior = np.zeros(d)\n        niceless = 0\n\n        best_idx = int(np.argmin(arc_f))\n        x_best = arc_x[best_idx].copy(); f_best = float(arc_f[best_idx])\n\n        def add_arc(xn, fn):\n            nonlocal arc_x, arc_f\n            fn = float(fn)\n            if arc_x.shape[0] < topk:\n                arc_x = np.vstack([arc_x, xn.copy()])\n                arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < self.budget:\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0)\n            q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            score = (abs_pull / (step + 1e-12)) + 0.25 * (iqr / (span + 1e-12)) + 0.12 * prior\n            score = np.maximum(score, 1e-12)\n            s = score - score.max()\n            temp = max(0.5, 0.9 + 0.25 * np.std(score))\n            probs = np.exp(s / temp)\n            probs /= probs.sum()\n            coord = int(self.rng.choice(d, p=probs))\n\n            # sign choice via Beta-sampled preference, but prefer median pull direction\n            follow_p = self.rng.beta(bi[coord] + 1e-9, bj[coord] + 1e-9)\n            if abs_pull[coord] > 1e-12:\n                nominal = int(np.sign(pull[coord]))\n            else:\n                nominal = int(np.sign(mom[coord])) if mom[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nominal if self.rng.random() < follow_p else -nominal\n\n            frac = min(1.0, abs_pull[coord] / (step[coord] + 1e-12))\n            mag = step[coord] * (0.75 + 0.5 * self.rng.random()) * (0.5 + 0.5 * frac)\n            delta = float(sign * mag)\n\n            # primary probe\n            if evals >= self.budget: break\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            add_arc(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy()\n                improved = True\n                bi[coord] += 1.0\n                prior[coord] = min(8.0, prior[coord] + 1.0)\n                mom[coord] = 0.7 * mom[coord] + 0.3 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * 1.20 + 1e-12))\n                # small extrapolation\n                if evals < self.budget:\n                    ext = 0.4 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_arc(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                        mom[coord] = 0.8 * mom[coord] + 0.2 * ext\n                niceless = 0\n            else:\n                # mirrored check\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    add_arc(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy()\n                        improved = True\n                        bj[coord] += 1.0\n                        prior[coord] = min(8.0, prior[coord] + 0.8)\n                        mom[coord] = 0.7 * mom[coord] + 0.3 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.18))\n                        if evals < self.budget:\n                            ext = 0.33 * (-delta)\n                            x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                            f_ext = float(func(x_ext)); evals += 1\n                            add_arc(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext; x_best = x_ext.copy()\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.06))\n                        niceless = 0\n                    else:\n                        # both failed -> shrink & penalize\n                        bj[coord] += 0.35\n                        bi[coord] = max(1.0, bi[coord] * 0.995)\n                        prior[coord] = max(-8.0, prior[coord] - 0.5)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.65))\n                        mom[coord] *= 0.6\n                        niceless += 1\n                else:\n                    niceless += 1\n\n            # stagnation rescue\n            if niceless >= self.patience and evals < self.budget:\n                niceless = 0\n                cand = (iqr / (span + 1e-12)) * (1.0 / (step + 1e-12) + 0.1)\n                s = cand - cand.max()\n                w = np.exp(s); w /= w.sum()\n                k = min(3, max(1, d // 6))\n                picks = list(self.rng.choice(d, size=min(d, k), replace=False, p=w))\n                got = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = 0.6 * step[c] + 0.3 * span[c] * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    add_arc(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step[c] = float(min(max_step[c], step[c] * 1.25))\n                        prior[c] = min(8.0, prior[c] + 1.0)\n                        mom[c] = np.sign(jump) * 0.5 * abs(jump)\n                        got = True\n                        break\n                if not got and evals < self.budget:\n                    # rare heavy-tail moves per-dim\n                    scale = np.maximum(0.12 * step, 0.04 * span)\n                    z = np.tan(np.pi * (self.rng.random(d) - 0.5))\n                    jumpvec = z * scale * (0.5 + self.rng.random(d))\n                    mask = (self.rng.random(d) < 0.2)\n                    trial = np.clip(x_best + jumpvec * mask, lb, ub)\n                    for i in range(d):\n                        if evals >= self.budget: break\n                        if not mask[i]: continue\n                        x_tmp = x_best.copy(); x_tmp[i] = trial[i]\n                        ft = float(func(x_tmp)); evals += 1\n                        add_arc(x_tmp, ft)\n                        if ft < f_best:\n                            f_best = ft; x_best = x_tmp.copy()\n                            step[i] = float(min(max_step[i], step[i] * 1.18))\n                            prior[i] = min(8.0, prior[i] + 1.0)\n                            mom[i] = (x_tmp[i] - x_best[i]) * 0.5\n                            break\n\n            prior *= 0.995\n            mom *= 0.992\n\n            if arc_x.shape[0] > self.archive_size:\n                o = np.argsort(arc_f)[:self.archive_size]\n                arc_x = arc_x[o]; arc_f = arc_f[o]\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 23, "feedback": "The algorithm SeparableAdaptiveMedianPull scored 0.710 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad9d28ed-2806-47c9-b4ca-e97b1d7b4fc7"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9767673249017556}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.978300652503777}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.977241578682872}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9377031292091319}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8920129455685009}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.904534438751868}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1408617091848664}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.2688489801803725}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.45535956775741815}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.6577736298315127}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.21207130272632557}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.2679104017008026}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9925033640752696}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9948332265785649}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9925713732114707}], "aucs": [0.9767673249017556, 0.978300652503777, 0.977241578682872, 0.9377031292091319, 0.8920129455685009, 0.904534438751868, 0.1408617091848664, 0.2688489801803725, 0.45535956775741815, 0.6577736298315127, 0.21207130272632557, 0.2679104017008026, 0.9925033640752696, 0.9948332265785649, 0.9925713732114707]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2575.0, "Edges": 2574.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.9992233009708738, "Degree Variance": 2.239999396738618, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.529957805907173, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3172768674973463, "Depth Entropy": 2.1431301768529405, "Assortativity": 0.0, "Average Eccentricity": 17.206990291262137, "Diameter": 23.0, "Radius": 12.0, "Edge Density": 0.0003883495145631068, "Average Shortest Path": 10.316721207594995, "mean_complexity": 13.666666666666666, "total_complexity": 41.0, "mean_token_count": 756.6666666666666, "total_token_count": 2270.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "b70632a3-897f-431d-8856-eae424dbff7d", "fitness": 0.21994276495233414, "name": "DirectionalGaussianProbe", "description": "Directional Gaussian Probing with per-dimension sigma adaptation, antithetic probes and an EMA tilt for guided sampling \u2014 cheap vector probes that adapt step-sizes where they help and use rare Cauchy escapes.", "code": "import numpy as np\n\nclass DirectionalGaussianProbe:\n    def __init__(self, budget=1000, dim=10, init_sigma=0.08, success=1.25, failure=0.7,\n                 arch_size=6, global_prob=0.03, cauchy_scale=0.12, tilt_alpha=0.12,\n                 polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_sigma = float(init_sigma); self.success = float(success); self.failure = float(failure)\n        self.arch_size = int(arch_size); self.global_prob = float(global_prob)\n        self.cauchy_scale = float(cauchy_scale); self.tilt_alpha = float(tilt_alpha)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, float(lb))\n            if ub.shape == (): ub = np.full(d, float(ub))\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n        span = np.maximum(1e-12, ub - lb); box = max(1e-12, np.linalg.norm(span))\n        sigma = np.full(d, max(1e-12, self.init_sigma * box))\n        min_sigma = 1e-15 * box; max_sigma = 1.2 * box\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            val = float(func(x)); evals += 1\n            return val\n\n        x = rng.uniform(lb, ub); fx = safe_eval(x)\n        tilt = np.zeros(d, float)\n        arch_x = [x.copy()]; arch_f = [fx]\n        def push_arch(xx, ff):\n            arch_x.append(xx.copy()); arch_f.append(float(ff))\n            if len(arch_x) > self.arch_size:\n                i = int(np.argmax(arch_f)); arch_x.pop(i); arch_f.pop(i)\n\n        while evals < self.budget:\n            # rare heavy-tail global escape\n            if rng.random() < self.global_prob:\n                u = rng.random(d)\n                jump = self.cauchy_scale * box * np.tan(np.pi * (u - 0.5))\n                xg = np.clip(x + jump, lb, ub); fg = safe_eval(xg)\n                if fg < fx:\n                    x, fx = xg.copy(), fg; push_arch(x, fx)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x - xg)\n                continue\n\n            # guided vector probe with antithetic pair\n            # add tilt bias to sampling\n            v = rng.normal(0.0, 1.0, d) * sigma\n            # small random scaling\n            v *= rng.uniform(0.6, 1.6)\n            v += 0.25 * tilt  # bias sampling by recent good moves\n            x_plus = np.clip(x + v, lb, ub)\n            if evals <= self.budget - 2:\n                x_minus = np.clip(x - v, lb, ub)\n                fp = safe_eval(x_plus); fm = safe_eval(x_minus)\n                # pick the better probe\n                if fp < fm:\n                    cand, fc, used = x_plus, fp, +1\n                else:\n                    cand, fc, used = x_minus, fm, -1\n                if fc < fx:\n                    old = x.copy(); x, fx = cand.copy(), fc; push_arch(x, fx)\n                    # per-dim sigma adaptation: increase where probe moved\n                    moved = np.abs(v) > 1e-12\n                    sigma[moved] = np.minimum(max_sigma, sigma[moved] * self.success)\n                    sigma[~moved] = np.maximum(min_sigma, sigma[~moved] * self.failure)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x - old)\n                else:\n                    # no improvement: shrink sigmas on used dims\n                    moved = np.abs(v) > 1e-12\n                    sigma[moved] = np.maximum(min_sigma, sigma[moved] * self.failure)\n                continue\n            else:\n                # last eval: single try\n                fp = safe_eval(x_plus)\n                if fp < fx:\n                    old = x.copy(); x, fx = x_plus.copy(), fp; push_arch(x, fx)\n                    moved = np.abs(v) > 1e-12\n                    sigma[moved] = np.minimum(max_sigma, sigma[moved] * self.success)\n                    tilt = (1-self.tilt_alpha)*tilt + self.tilt_alpha*(x - old)\n                else:\n                    moved = np.abs(v) > 1e-12\n                    sigma[moved] = np.maximum(min_sigma, sigma[moved] * self.failure)\n\n        # final polish: small coordinate greedy steps\n        fin = self.polish_frac * box\n        while evals < self.budget and fin > 1e-12 * box:\n            improved = False\n            for j in range(d):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[j] = np.clip(xt[j] + fin, lb[j], ub[j])\n                ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[j] = np.clip(xt[j] - fin, lb[j], ub[j])\n                ft = safe_eval(xt)\n                if ft < fx:\n                    x, fx, improved = xt.copy(), ft, True\n            if not improved:\n                fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(fx)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 23, "feedback": "The algorithm DirectionalGaussianProbe scored 0.220 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4ede79fe-bcf1-46b5-90c5-770f1a941fad"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9857504461297534}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9830370399901041}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9857553198273832}, {"fid": 2, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 2, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 3, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 1, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 2, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 4, "iid": 3, "dim": 10, "auc": 4.999999999999449e-05}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.09087012111156378}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.1493442709947307}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.10393427623147744}], "aucs": [0.9857504461297534, 0.9830370399901041, 0.9857553198273832, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 4.999999999999449e-05, 0.09087012111156378, 0.1493442709947307, 0.10393427623147744]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1416.0, "Edges": 1415.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.9985875706214689, "Degree Variance": 2.1793765361166972, "Transitivity": 0.0, "Max Depth": 12.0, "Min Depth": 2.0, "Mean Depth": 7.67651632970451, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.326301470760551, "Depth Entropy": 1.9854967374854973, "Assortativity": 9.498236258282371e-09, "Average Eccentricity": 14.749293785310735, "Diameter": 19.0, "Radius": 10.0, "Edge Density": 0.0007062146892655367, "Average Shortest Path": 9.556791639216625, "mean_complexity": 6.5, "total_complexity": 26.0, "mean_token_count": 312.0, "total_token_count": 1248.0, "mean_parameter_count": 4.25, "total_parameter_count": 17.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "16143c04-5896-40ac-8cad-7d3d6b314a6b", "fitness": 0.8017490631686371, "name": "SeparableLeapfrogPull", "description": "Separable Leapfrog Pull v2 \u2014 compact coordinate-wise pulls toward an archive centroid with multiplicative per-dim step adaption, quick mirrored checks, cheap multi\u2011dim leapfrogs and rare Cauchy escapes for robust cheap exploitation.", "code": "import numpy as np\n\nclass SeparableLeapfrogPull:\n    def __init__(self, budget, dim, archive_size=6, init_step=None, min_step=1e-6, rng=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size)); self.min_step = float(min_step)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = None if init_step is None else np.asarray(init_step, float)\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0 * np.ones(d); ub = 5.0 * np.ones(d); span = ub - lb\n        if self.init_step is None:\n            step = np.full(d, 0.12 * span)\n        else:\n            s = self.init_step\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        step = np.maximum(step, self.min_step)\n\n        B = self.budget; evals = 0\n        n_init = max(2, min(d+1, max(1, B//8), 8))\n        arc_x = []\n        arc_f = []\n        for _ in range(n_init):\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n            if evals >= B: break\n        if not arc_x:\n            x = lb + self.rng.random(d) * span\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n\n        arc_x = np.vstack(arc_x); arc_f = np.array(arc_f, float)\n        k = min(self.archive_size, arc_x.shape[0])\n        bid = int(np.argmin(arc_f)); x_best = arc_x[bid].copy(); f_best = float(arc_f[bid])\n\n        succ = np.ones(d); fail = np.ones(d)\n        stale = 0\n\n        def add_archive(xn, fn):\n            nonlocal arc_x, arc_f, k\n            fn = float(fn)\n            if arc_x.shape[0] < k:\n                arc_x = np.vstack([arc_x, xn.copy()]); arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < B:\n            med = np.median(arc_x, axis=0)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n            score = (abs_pull / (step + 1e-12)) + 0.08/(1.0+fail)\n            p = np.maximum(score, 1e-12); p = p / p.sum()\n            coord = int(self.rng.choice(d, p=p))\n            prefer = int(np.sign(pull[coord])) if abs_pull[coord] > 1e-12 else (1 if self.rng.random() < 0.5 else -1)\n            p_pref = succ[coord]/(succ[coord]+fail[coord])\n            sign = prefer if self.rng.random() < p_pref else -prefer\n            frac = min(1.0, abs_pull[coord]/(step[coord]+1e-12))\n            mag = step[coord]*(0.7 + 0.6*self.rng.random())*(0.5+0.5*frac)\n            delta = float(sign*mag)\n\n            # primary probe\n            x_try = x_best.copy(); x_try[coord] = np.clip(x_try[coord]+delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            add_archive(x_try, f_try)\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try\n                succ[coord] += 1.0\n                step[coord] = float(min(0.5*span[coord], step[coord]*1.22))\n                # small forward extrapolation\n                if evals < B:\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + 0.45*delta, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_archive(x_ext, f_ext)\n                    if f_ext < f_best: f_best = f_ext; x_best = x_ext; step[coord] = float(min(0.5*span[coord], step[coord]*1.12))\n                stale = 0\n            else:\n                # mirrored probe\n                if evals < B:\n                    x_opp = x_best.copy(); x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    add_archive(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp\n                        fail[coord] += 0.8\n                        step[coord] = float(min(0.5*span[coord], step[coord]*1.18)); stale = 0\n                    else:\n                        fail[coord] += 0.6\n                        succ[coord] = max(1.0, succ[coord]*0.995)\n                        step[coord] = float(max(self.min_step, step[coord]*0.62)); stale += 1\n                else:\n                    stale += 1\n\n            # cheap multi-dim leapfrog\n            if self.rng.random() < 0.08 and evals < B:\n                scale = 0.6*step + 0.02*span\n                noise = self.rng.normal(size=d) * scale * (0.6 + 0.8*self.rng.random(d))\n                jump = 0.6*(med - x_best) + 0.4*noise\n                order = np.argsort(-np.abs(jump))\n                for i in order:\n                    if evals >= B: break\n                    if np.isclose(jump[i], 0.0): continue\n                    xt = x_best.copy(); xt[i] = np.clip(x_best[i] + jump[i], lb[i], ub[i])\n                    ft = float(func(xt)); evals += 1\n                    add_archive(xt, ft)\n                    if ft < f_best:\n                        f_best = ft; x_best = xt; step[i] = float(min(0.5*span[i], step[i]*1.2)); stale = 0\n                        break\n\n            # rare heavy-tail escape\n            if stale > max(6, d//3) and evals < B:\n                stale = 0\n                scale = np.maximum(0.08*step, 0.03*span)\n                u = self.rng.random(d); c = np.tan(np.pi*(u-0.5))\n                jump = c * scale * (0.4 + 0.8*self.rng.random(d))\n                xj = np.clip(x_best + jump, lb, ub)\n                for i in range(d):\n                    if evals >= B: break\n                    if np.isclose(xj[i], x_best[i]): continue\n                    xt = x_best.copy(); xt[i] = xj[i]\n                    ft = float(func(xt)); evals += 1\n                    add_archive(xt, ft)\n                    if ft < f_best:\n                        f_best = ft; x_best = xt; step[i] = float(min(0.5*span[i], step[i]*1.3))\n                        break\n\n            # prune archive and mild decay\n            if arc_x.shape[0] > self.archive_size:\n                idxs = np.argsort(arc_f)[:self.archive_size]; arc_x = arc_x[idxs]; arc_f = arc_f[idxs]\n            succ *= 0.998; fail *= 0.999\n            step = np.minimum(0.5*span, np.maximum(self.min_step, step))\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 24, "feedback": "The algorithm SeparableLeapfrogPull scored 0.802 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8767532-db1e-419c-975f-93e594a76e91"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.97248449817382}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9626730394000302}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.953863050509147}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.884963289531765}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8453926499894655}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9322539252866976}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.8065097652621614}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.5804727957361333}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.7294001621675101}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.46423269330630557}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.6228488931412557}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.29247726285367626}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9919318535455881}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9943543610156723}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.992377707610329}], "aucs": [0.97248449817382, 0.9626730394000302, 0.953863050509147, 0.884963289531765, 0.8453926499894655, 0.9322539252866976, 0.8065097652621614, 0.5804727957361333, 0.7294001621675101, 0.46423269330630557, 0.6228488931412557, 0.29247726285367626, 0.9919318535455881, 0.9943543610156723, 0.992377707610329]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 1905.0, "Edges": 1904.0, "Max Degree": 27.0, "Min Degree": 1.0, "Mean Degree": 1.9989501312335958, "Degree Variance": 2.1228335434448646, "Transitivity": 0.0, "Max Depth": 13.0, "Min Depth": 2.0, "Mean Depth": 8.099770642201834, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3189884114444113, "Depth Entropy": 2.039334356597339, "Assortativity": 1.5058233321004844e-08, "Average Eccentricity": 15.122834645669291, "Diameter": 20.0, "Radius": 10.0, "Edge Density": 0.0005249343832020997, "Average Shortest Path": 9.760867023974944, "mean_complexity": 11.0, "total_complexity": 33.0, "mean_token_count": 556.0, "total_token_count": 1668.0, "mean_parameter_count": 3.6666666666666665, "total_parameter_count": 11.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "dec9379a-3405-4fea-90f8-31a641302570", "fitness": "-inf", "name": "DirectionalParabolicSwarm", "description": "Directional Parabolic Swarm (DPS) \u2014 maintain a tiny elite archive and a short history of displacement vectors; probe along promising directions (archive differences, recent PCA axis or random Gaussian), perform mirrored 3\u2011point probes and analytic 1\u2011D parabola minima along the direction, adapt a global step multiplicatively, and use occasional Cauchy/global jumps for escapes.", "code": "import numpy as np\nfrom collections import deque\n\nclass DirectionalParabolicSwarm:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 succ_mult=1.5, fail_mult=0.6,\n                 archive_size=6, hist_size=20,\n                 global_prob=0.04, cauchy_prob=0.01,\n                 max_extrap=2.0, polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.succ_mult = float(succ_mult); self.fail_mult = float(fail_mult)\n        self.archive_size = int(archive_size); self.hist_size = int(hist_size)\n        self.global_prob = float(global_prob); self.cauchy_prob = float(cauchy_prob)\n        self.max_extrap = float(max_extrap); self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds if provided, else [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * scale)\n        min_step = base * 1e-8\n        max_step = scale * 1.2\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        # initial sample\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        # tiny elite archive (store arrays)\n        archive = [(x.copy(), f)]\n        # short recent displacement history for PCA/directions\n        recent = deque(maxlen=self.hist_size)\n        step = base\n\n        while evals < self.budget:\n            # occasional global or heavy-tailed escape\n            if rng.random() < self.global_prob:\n                if rng.random() < self.cauchy_prob:\n                    u = rng.random(dim)\n                    jump = 0.15 * scale * np.tan(np.pi * (u - 0.5))\n                    xt = np.clip(x + jump, lb, ub)\n                else:\n                    # sample around a random archive member\n                    xa, _ = archive[rng.integers(len(archive))]\n                    xt = np.clip(xa + rng.normal(0, 0.06 * scale, dim), lb, ub)\n                ft = safe_eval(xt)\n                if ft < f:\n                    dx = xt - x; x, f = xt.copy(), float(ft); recent.append(dx)\n                    # update archive\n                    archive.append((x.copy(), f))\n                    archive = sorted(archive, key=lambda t: t[1])[:self.archive_size]\n                continue\n\n            # build a candidate direction:\n            #  - with some prob use principal component of recent displacements\n            #  - else use difference of two archive points if available\n            #  - else random gaussian\n            if len(recent) >= 3 and rng.random() < 0.5:\n                M = np.vstack(recent)\n                # principal singular vector (cheap SVD of small matrix)\n                try:\n                    u, svals, vt = np.linalg.svd(M, full_matrices=False)\n                    dirv = vt[0]\n                except Exception:\n                    dirv = rng.normal(size=dim)\n            elif len(archive) >= 2 and rng.random() < 0.7:\n                a, b = archive[rng.integers(len(archive))][0], archive[rng.integers(len(archive))][0]\n                dirv = a - b\n                if np.allclose(dirv, 0):\n                    dirv = rng.normal(size=dim)\n            else:\n                dirv = rng.normal(size=dim)\n\n            # normalize direction\n            nrm = np.linalg.norm(dirv)\n            if nrm == 0: dirv = rng.normal(size=dim); nrm = np.linalg.norm(dirv)\n            dirv = dirv / nrm\n\n            s = float(np.clip(step, min_step, max_step))\n\n            # mirrored probe if budget allows\n            if evals <= self.budget - 2:\n                xp = np.clip(x + s * dirv, lb, ub); xm = np.clip(x - s * dirv, lb, ub)\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                # immediate improvement\n                if fp < f or fm < f:\n                    if fp < fm:\n                        dx = xp - x; df = fp - f; x, f = xp.copy(), float(fp)\n                    else:\n                        dx = xm - x; df = fm - f; x, f = xm.copy(), float(fm)\n                    recent.append(dx)\n                    archive.append((x.copy(), f)); archive = sorted(archive, key=lambda t: t[1])[:self.archive_size]\n                    step = min(max_step, step * self.succ_mult)\n                    # try analytic parabola around the new center if still budget\n                    if evals < self.budget:\n                        denom = (fp + fm - 2.0 * f)\n                        if abs(denom) > 1e-16:\n                            tstar = -s * (fp - fm) / (2.0 * denom)\n                            tstar = float(np.clip(tstar, -self.max_extrap * s, self.max_extrap * s))\n                            if abs(tstar) > 1e-12:\n                                xt = np.clip(x + tstar * dirv, lb, ub)\n                                ft = safe_eval(xt)\n                                if ft < f:\n                                    dx = xt - x; x, f = xt.copy(), float(ft); recent.append(dx)\n                                    archive.append((x.copy(), f)); archive = sorted(archive, key=lambda t: t[1])[:self.archive_size]\n                                    step = min(max_step, step * self.succ_mult)\n                else:\n                    # if mirrored suggests convexity, try parabola\n                    denom = (fp + fm - 2.0 * f)\n                    if denom > 1e-14 and evals < self.budget:\n                        tstar = -s * (fp - fm) / (2.0 * denom)\n                        tstar = float(np.clip(tstar, -self.max_extrap * s, self.max_extrap * s))\n                        if abs(tstar) > 1e-12:\n                            xt = np.clip(x + tstar * dirv, lb, ub)\n                            ft = safe_eval(xt)\n                            if ft < f:\n                                dx = xt - x; x, f = xt.copy(), float(ft); recent.append(dx)\n                                archive.append((x.copy(), f)); archive = sorted(archive, key=lambda t: t[1])[:self.archive_size]\n                                step = min(max_step, step * self.succ_mult)\n                                continue\n                    # otherwise shrink\n                    step = max(min_step, step * self.fail_mult)\n\n            else:\n                # low budget: try one-sided probe\n                if rng.random() < 0.5:\n                    xt = np.clip(x + s * dirv, lb, ub)\n                else:\n                    xt = np.clip(x - s * dirv, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f:\n                    dx = xt - x; x, f = xt.copy(), float(ft); recent.append(dx)\n                    archive.append((x.copy(), f)); archive = sorted(archive, key=lambda t: t[1])[:self.archive_size]\n                    step = min(max_step, step * self.succ_mult)\n                else:\n                    step = max(min_step, step * self.fail_mult)\n\n        # cheap coordinate polish\n        fin = self.polish_frac * scale\n        while evals < self.budget and fin > 1e-12 * scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f, improved = xt.copy(), float(ft), True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d])\n                ft = safe_eval(xt)\n                if ft < f:\n                    x, f, improved = xt.copy(), float(ft), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 24, "feedback": "In the code, line 46, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent = deque(maxlen=self.hist_size)", "error": "In the code, line 46, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent = deque(maxlen=self.hist_size)", "parent_ids": ["86d13d75-4636-4c2f-9196-d0f90890a1f4"], "operator": "random", "metadata": {}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2072.0, "Edges": 2071.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.999034749034749, "Degree Variance": 1.9150569833484896, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.870511425462459, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3375214935211812, "Depth Entropy": 2.3174366708581196, "Assortativity": 0.0, "Average Eccentricity": 18.602316602316602, "Diameter": 24.0, "Radius": 12.0, "Edge Density": 0.00048262548262548264, "Average Shortest Path": 11.049680362572685, "mean_complexity": 14.0, "total_complexity": 42.0, "mean_token_count": 597.0, "total_token_count": 1791.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "a5a5c514-cf53-495d-a60a-d35277e73c11", "fitness": 0.686530151872454, "name": "ParabolicAdaptiveMirroredDescent", "description": "Parabolic Adaptive Mirrored Descent v2 \u2014 coordinate-wise mirrored 3\u2011point parabolic fits with median-gradient prioritization, light momentum, adaptive per\u2011dim trust radii and occasional Cauchy escapes for robust cheap exploitation.", "code": "import numpy as np\n\nclass ParabolicAdaptiveMirroredDescent:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 succ_mult=1.5, fail_mult=0.6, min_step_frac=1e-9,\n                 max_step_frac=1.2, global_escape=0.03, max_extrap=2.0,\n                 polish_frac=0.03, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.succ_mult = float(succ_mult); self.fail_mult = float(fail_mult)\n        self.min_pf = float(min_step_frac); self.max_pf = float(max_step_frac)\n        self.global_escape = float(global_escape)\n        self.max_extrap = float(max_extrap)\n        self.polish_frac = float(polish_frac)\n\n    def __call__(self, func):\n        d = self.dim; rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float)\n            ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(d, float(lb))\n            if ub.shape == (): ub = np.full(d, float(ub))\n        else:\n            lb = np.full(d, -5.0); ub = np.full(d, 5.0)\n\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * scale)\n        min_step = max(1e-15, self.min_pf * scale)\n        max_step = max(1e-12, self.max_pf * scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1; return v\n\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        steps = np.full(d, base, float)\n        trials = np.zeros(d, int); succ = np.zeros(d, int)\n        hist = []  # (dim, dx, df) recent moves\n        mom = np.zeros(d, float)\n\n        while evals < self.budget:\n            # occasional heavy-tailed global escape\n            if rng.random() < self.global_escape:\n                u = rng.random(d)\n                jump = 0.1 * scale * np.tan(np.pi * (u - 0.5))\n                xt = np.clip(x + jump, lb, ub); ft = safe_eval(xt)\n                if ft < f: x, f = xt.copy(), ft\n                continue\n\n            # estimate per-dim gradient magnitude from history (median slope)\n            grad_mag = np.zeros(d, float)\n            if hist:\n                arr = np.array(hist)\n                for i in range(d):\n                    sel = arr[arr[:,0]==i]\n                    if sel.size:\n                        slopes = np.abs(sel[:,2] / (sel[:,1] + 1e-20))\n                        grad_mag[i] = np.median(slopes)\n\n            # coordinate priority: larger step * grad signal / (1+trials)\n            score = steps * (1.0 + 6.0 * grad_mag) / (1.0 + 0.15 * trials)\n            # add small jitter to break ties\n            cid = int(np.argmax(score + 1e-12 * rng.random(d)))\n            s = float(np.clip(steps[cid], min_step, max_step))\n\n            # mirrored 3-point probe if enough budget\n            if evals <= self.budget - 2:\n                xp = x.copy(); xm = x.copy()\n                xp[cid] = np.clip(xp[cid] + s, lb[cid], ub[cid])\n                xm[cid] = np.clip(xm[cid] - s, lb[cid], ub[cid])\n                fp = safe_eval(xp); fm = safe_eval(xm); trials[cid] += 2\n\n                # if one side improved, take best and increase step\n                if fp < f or fm < f:\n                    if fp < fm:\n                        dx = xp - x; df = fp - f; x, f = xp.copy(), fp\n                    else:\n                        dx = xm - x; df = fm - f; x, f = xm.copy(), fm\n                    hist.append((cid, dx[cid], df))\n                    if len(hist) > 40: hist.pop(0)\n                    succ[cid] += 1\n                    steps[cid] = min(max_step, steps[cid] * self.succ_mult)\n                    # momentum update\n                    mv = np.zeros(d); mv[cid] = dx[cid]\n                    mom = 0.6 * mom + 0.4 * mv\n\n                    # try analytic parabola around center using fp,f,fm (f is center now)\n                    if evals < self.budget and s > 1e-14 * scale:\n                        f0 = f\n                        denom = (fp + fm - 2.0 * f0)\n                        if abs(denom) > 1e-16:\n                            xstar = -s * (fp - fm) / (2.0 * denom)\n                            xstar = np.clip(xstar, -self.max_extrap * s, self.max_extrap * s)\n                            if abs(xstar) > 1e-12:\n                                xt = x.copy(); xt[cid] = np.clip(xt[cid] + xstar, lb[cid], ub[cid])\n                                ft = safe_eval(xt); trials[cid] += 1\n                                if ft < f:\n                                    dx = xt - x; df = ft - f; x, f = xt.copy(), ft\n                                    hist.append((cid, dx[cid], df)); succ[cid] += 1\n                                    steps[cid] = min(max_step, steps[cid] * self.succ_mult)\n                else:\n                    # no immediate improvement: try parabola if convex-ish\n                    denom = (fp + fm - 2.0 * f)\n                    if denom > 1e-14 and evals < self.budget:\n                        xstar = -s * (fp - fm) / (2.0 * denom)\n                        xstar = np.clip(xstar, -self.max_extrap * s, self.max_extrap * s)\n                        if abs(xstar) > 1e-12:\n                            xt = x.copy(); xt[cid] = np.clip(xt[cid] + xstar, lb[cid], ub[cid])\n                            ft = safe_eval(xt); trials[cid] += 1\n                            if ft < f:\n                                dx = xt - x; df = ft - f; x, f = xt.copy(), ft\n                                hist.append((cid, dx[cid], df)); succ[cid] += 1\n                                steps[cid] = min(max_step, steps[cid] * self.succ_mult)\n                                continue\n                    # shrink if nothing promising\n                    steps[cid] = max(min_step, steps[cid] * self.fail_mult)\n\n            else:\n                # final one-sided probe\n                side = 1 if rng.random() < 0.5 else -1\n                xt = x.copy(); xt[cid] = np.clip(xt[cid] + side * s, lb[cid], ub[cid])\n                ft = safe_eval(xt); trials[cid] += 1\n                if ft < f:\n                    dx = xt - x; df = ft - f; x, f = xt.copy(), ft\n                    hist.append((cid, dx[cid], df)); succ[cid] += 1\n                    steps[cid] = min(max_step, steps[cid] * self.succ_mult)\n                else:\n                    steps[cid] = max(min_step, steps[cid] * self.fail_mult)\n\n            # mild rescue: boost stale dims\n            if trials[cid] > 50 and succ[cid] == 0:\n                steps[cid] = min(max_step, steps[cid] * 1.6)\n                trials[cid] = int(trials[cid] * 0.5)\n\n            # momentum-driven micro extrapolation (cheap)\n            if rng.random() < 0.06 and np.linalg.norm(mom) > 0 and evals < self.budget:\n                xt = np.clip(x + 0.5 * mom, lb, ub); ft = safe_eval(xt)\n                if ft < f:\n                    x, f = xt.copy(), ft\n\n        # final polish: small coordinate sweeps with shrinking step\n        fin = self.polish_frac * scale\n        while evals < self.budget and fin > 1e-12 * scale:\n            improved = False\n            for i in range(d):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[i] = np.clip(xt[i] + fin, lb[i], ub[i]); ft = safe_eval(xt)\n                if ft < f: x, f, improved = xt.copy(), ft, True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[i] = np.clip(xt[i] - fin, lb[i], ub[i]); ft = safe_eval(xt)\n                if ft < f: x, f, improved = xt.copy(), ft, True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 24, "feedback": "The algorithm ParabolicAdaptiveMirroredDescent scored 0.687 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["86d13d75-4636-4c2f-9196-d0f90890a1f4"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9901076693091968}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9859797648914365}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9899799684641494}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9600197305709676}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9516308721437012}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9638215200238704}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.16587399466560637}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.04801181278904387}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.1278582048897664}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.11336174191079951}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07832601892818458}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.953794690401011}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9893959571968952}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9896138287652458}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9901765031369364}], "aucs": [0.9901076693091968, 0.9859797648914365, 0.9899799684641494, 0.9600197305709676, 0.9516308721437012, 0.9638215200238704, 0.16587399466560637, 0.04801181278904387, 0.1278582048897664, 0.11336174191079951, 0.07832601892818458, 0.953794690401011, 0.9893959571968952, 0.9896138287652458, 0.9901765031369364]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2213.0, "Edges": 2212.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.9990962494351558, "Degree Variance": 2.03163045300446, "Transitivity": 0.0, "Max Depth": 15.0, "Min Depth": 2.0, "Mean Depth": 8.701668302257115, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.329562997978501, "Depth Entropy": 2.228458330452857, "Assortativity": 0.0, "Average Eccentricity": 16.868052417532763, "Diameter": 22.0, "Radius": 11.0, "Edge Density": 0.00045187528242205153, "Average Shortest Path": 10.525233516562087, "mean_complexity": 15.0, "total_complexity": 45.0, "mean_token_count": 622.3333333333334, "total_token_count": 1867.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "d1a24393-4149-4180-bf7d-5cadab379ff5", "fitness": 0.4689849270744466, "name": "DirectionalAdaptiveSubspaceProbing", "description": "Directional Adaptive Subspace Probing (DASP) \u2014 probe small adaptive subspaces anchored at the archive centroid with normalized directional steps, lightweight momentum, parabolic 3\u2011point line refinements and rare heavy\u2011tailed global jumps.", "code": "import numpy as np\n\nclass DirectionalAdaptiveSubspaceProbing:\n    def __init__(self, budget, dim, archive_size=8, init_step=None, min_step=1e-6,\n                 max_step=None, rng=None, subspace_max=4, parabola_prob=0.12, levy_prob=0.06):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.subspace_max = max(1, int(subspace_max))\n        self.parabola_prob = float(parabola_prob)\n        self.levy_prob = float(levy_prob)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb, ub = -5.0, 5.0\n        span = (ub - lb) * np.ones(d)\n        # init steps\n        if self.init_step is None:\n            step = 0.12 * span * np.ones(d)\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = 0.6 * span\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        # initial sampling (small)\n        n_init = min(12, max(3, min(d+1, max(1, self.budget//10))))\n        X = []\n        F = []\n        while evals < self.budget and len(X) < n_init:\n            x = lb + self.rng.random(d) * span\n            F.append(float(func(x))); evals += 1\n            X.append(x.copy())\n        if len(X) == 0:\n            x = lb + self.rng.random(d) * span\n            F.append(float(func(x))); evals += 1\n            X.append(x.copy())\n        X = np.vstack(X); F = np.array(F, float)\n\n        def arc_add(xn, fn):\n            nonlocal X, F\n            fn = float(fn)\n            if X.shape[0] < self.archive_size:\n                X = np.vstack([X, xn.copy()]); F = np.append(F, fn)\n            else:\n                worst = int(np.argmax(F))\n                if fn < F[worst]:\n                    X[worst] = xn.copy(); F[worst] = fn\n\n        idx = int(np.argmin(F))\n        x_best = X[idx].copy(); f_best = float(F[idx])\n        momentum = np.zeros(d)\n        temp = 1.0\n        stagn = 0\n\n        while evals < self.budget:\n            # archive statistics\n            centroid = np.mean(X, axis=0)\n            spread = np.maximum(np.std(X, axis=0), 1e-12)\n            dist = centroid - x_best\n            absdist = np.abs(dist)\n            # choose subspace dims weighted by expected gain (dist/step + spread)\n            weight = (absdist / (step + 1e-12)) + 0.2 * (spread / (span + 1e-12))\n            weight = np.maximum(weight, 1e-12)\n            weight = weight / weight.sum()\n            k = min(self.subspace_max, max(1, int(1 + (d-1) * self.rng.random())))\n            k = min(d, k)\n            dims = list(self.rng.choice(d, size=k, replace=False, p=weight))\n            # build direction: prefer centroid pull + momentum + small random orthogonal\n            v = np.zeros(d)\n            v[dims] = dist[dims]\n            if np.linalg.norm(v) < 1e-12:\n                v[dims] = self.rng.normal(size=len(dims))\n            # add small momentum bias\n            v += 0.6 * momentum\n            # add orthogonal jitter in subspace\n            jitter = 0.3 * (self.rng.normal(size=len(dims)) * (step[dims] / (span[dims] + 1e-12)))\n            v[dims] += jitter\n            # normalize direction within selected dims\n            norm = np.linalg.norm(v[dims])\n            if norm < 1e-12:\n                v[dims] = self.rng.normal(size=len(dims)); norm = np.linalg.norm(v[dims])\n            v_unit = v / norm\n            # choose magnitude along that direction based on step magnitudes\n            s_mag = max(1e-12, np.linalg.norm(step[dims]) / np.sqrt(len(dims)))\n            alpha = s_mag * (0.6 + 0.8 * self.rng.random())\n            x_trial = np.clip(x_best + alpha * v_unit, lb, ub)\n            f_trial = float(func(x_trial)); evals += 1\n            arc_add(x_trial, f_trial)\n            improved = False\n            if f_trial < f_best:\n                improved = True\n                f_best = f_trial; x_best = x_trial.copy()\n                # reinforce steps in dims\n                for i in dims:\n                    step[i] = float(min(max_step[i], step[i] * (1.12 + 0.08 * self.rng.random())))\n                    momentum[i] = 0.7 * momentum[i] + 0.3 * (alpha * v_unit[i])\n                stagn = 0\n                # light extrapolation along same line if budget allows\n                if evals < self.budget and self.rng.random() < 0.6:\n                    ext = 0.5 * alpha * (0.4 + 0.8 * self.rng.random())\n                    x_ext = np.clip(x_best + ext * v_unit, lb, ub)\n                    f_ext = float(func(x_ext)); evals += 1\n                    arc_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        for i in dims:\n                            step[i] = float(min(max_step[i], step[i] * 1.06))\n                            momentum[i] = 0.8 * momentum[i] + 0.2 * (ext * v_unit[i])\n            else:\n                # try mirrored opposite direction once\n                if evals < self.budget:\n                    x_opp = np.clip(x_best - alpha * v_unit, lb, ub)\n                    f_opp = float(func(x_opp)); evals += 1\n                    arc_add(x_opp, f_opp)\n                    if f_opp < f_best:\n                        improved = True\n                        f_best = f_opp; x_best = x_opp.copy()\n                        for i in dims:\n                            step[i] = float(min(max_step[i], step[i] * 1.10))\n                            momentum[i] = 0.7 * momentum[i] - 0.3 * (alpha * v_unit[i])\n                        stagn = 0\n                    else:\n                        # shrink steps in dims (conservative)\n                        for i in dims:\n                            step[i] = float(max(self.min_step, step[i] * (0.72 + 0.05 * self.rng.random())))\n                            momentum[i] *= 0.6\n                        stagn += 1\n                else:\n                    stagn += 1\n            # occasional parabolic 3-point refine along v_unit for promising axes\n            if (evals < self.budget) and (self.rng.random() < self.parabola_prob):\n                s = max(1e-8, 0.6 * np.linalg.norm(step[dims]) / np.sqrt(len(dims)))\n                x_p = np.clip(x_best + s * v_unit, lb, ub)\n                x_m = np.clip(x_best - s * v_unit, lb, ub)\n                f_p = float(func(x_p)); evals += 1; arc_add(x_p, f_p)\n                if evals < self.budget:\n                    f_m = float(func(x_m)); evals += 1; arc_add(x_m, f_m)\n                else:\n                    f_m = float('inf')\n                denom = (f_p + f_m - 2.0 * f_best)\n                if denom != 0:\n                    t_star = s * (f_m - f_p) / (2.0 * denom)\n                    if abs(t_star) <= 3.0 * s:\n                        x_q = np.clip(x_best + t_star * v_unit, lb, ub)\n                        if evals < self.budget:\n                            f_q = float(func(x_q)); evals += 1; arc_add(x_q, f_q)\n                            if f_q < f_best:\n                                f_best = f_q; x_best = x_q.copy()\n                                # modest step gain\n                                for i in dims:\n                                    step[i] = float(min(max_step[i], step[i] * 1.10))\n                                    momentum[i] = 0.8 * momentum[i] + 0.2 * (t_star * v_unit[i])\n                                stagn = 0\n            # rare heavy-tailed global jump (Cauchy-like)\n            if (evals < self.budget) and (self.rng.random() < self.levy_prob):\n                scale = np.maximum(0.04 * step, 0.02 * span)\n                u = self.rng.random(d)\n                levy = np.tan(np.pi * (u - 0.5)) * scale * (0.4 + self.rng.random(d))\n                mask = self.rng.random(d) < 0.25\n                if not mask.any():\n                    mask[self.rng.integers(d)] = True\n                xj = np.clip(x_best + levy * mask, lb, ub)\n                # evaluate masked dims one by one until improvement\n                for i in range(d):\n                    if not mask[i] or evals >= self.budget: continue\n                    xt = x_best.copy(); xt[i] = xj[i]\n                    fv = float(func(xt)); evals += 1; arc_add(xt, fv)\n                    if fv < f_best:\n                        f_best = fv; x_best = xt.copy()\n                        step[i] = float(min(max_step[i], step[i] * 1.2))\n                        momentum[i] = np.sign(levy[i]) * 0.5 * abs(levy[i])\n                        stagn = 0\n                        break\n            # adapt temperature/decay and occasional reset of small steps\n            temp *= 0.9995\n            momentum *= 0.985\n            # if stagnation large, slightly widen some steps guided by spread\n            if stagn > max(8, d//2):\n                stagn = 0\n                widen = (0.4 + 0.8 * self.rng.random(d)) * (spread / (span + 1e-12))\n                step = np.minimum(max_step, np.maximum(step * (1.0 + widen), self.min_step))\n            # trim archive to best entries\n            if X.shape[0] > self.archive_size:\n                order = np.argsort(F)\n                keep = order[:self.archive_size]\n                X = X[keep]; F = F[keep]\n            # ensure archive contains current best\n            arc_add(x_best, f_best)\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 24, "feedback": "The algorithm DirectionalAdaptiveSubspaceProbing scored 0.469 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["703d9c83-e2c2-4f0e-831d-e66095f1fb07"], "operator": "random", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.6738995395478844}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9157606038541106}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.7738773416729682}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8447344486650931}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.5674828990054477}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8286900641824725}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.0850590529764842}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.06254495965191798}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.07794409810554803}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.06803532135146839}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.07039639923075192}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07376548966137508}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.7404949856063915}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.5773927597110695}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.674695942893716}], "aucs": [0.6738995395478844, 0.9157606038541106, 0.7738773416729682, 0.8447344486650931, 0.5674828990054477, 0.8286900641824725, 0.0850590529764842, 0.06254495965191798, 0.07794409810554803, 0.06803532135146839, 0.07039639923075192, 0.07376548966137508, 0.7404949856063915, 0.5773927597110695, 0.674695942893716]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2448.0, "Edges": 2447.0, "Max Degree": 36.0, "Min Degree": 1.0, "Mean Degree": 1.9991830065359477, "Degree Variance": 2.1789209011491306, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.565801253357208, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3248632500114514, "Depth Entropy": 2.2453746110047286, "Assortativity": 0.0, "Average Eccentricity": 20.117238562091504, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.0004084967320261438, "Average Shortest Path": 10.581777139407732, "mean_complexity": 14.333333333333334, "total_complexity": 43.0, "mean_token_count": 719.0, "total_token_count": 2157.0, "mean_parameter_count": 5.0, "total_parameter_count": 15.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "7a62b5ab-269d-493f-91bd-ca87462cb9f4", "fitness": 0.8073677223070364, "name": "SeparableAdaptiveMedianPullV2", "description": "Compact separable median-bandit pulls with adaptive per-dimension steps, Beta-like sign priors, lightweight momentum, occasional short extrapolations, and targeted stagnation rescues (multi\u2011dim bandit jumps + sparse Cauchy escapes).", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPullV2:\n    def __init__(self, budget, dim, archive_size=6, init_step=None, min_step=1e-6, max_step=None,\n                 patience=7, rng=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step); self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim; lb, ub = -5.0, 5.0; span = ub - lb\n        if self.init_step is None:\n            step = np.full(d, 0.08 * span)\n        else:\n            s = np.asarray(self.init_step, float); step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = np.full(d, 0.5 * span)\n        else:\n            m = np.asarray(self.max_step, float); max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = int(max(1, min(d, self.budget // 12, 6)))\n        arc_x = []\n        arc_f = []\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub, d)\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n            if evals >= self.budget: break\n        if not arc_x:\n            x = self.rng.uniform(lb, ub, d); f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n        arc_x = np.vstack(arc_x); arc_f = np.array(arc_f, float)\n        K = min(self.archive_size, arc_x.shape[0])\n\n        bi = np.ones(d); bj = np.ones(d)      # Beta-like sign prior counts\n        mom = np.zeros(d); prio = np.zeros(d)\n        no_improve = 0\n        i0 = int(np.argmin(arc_f)); x_best = arc_x[i0].copy(); f_best = float(arc_f[i0])\n        eps = 1e-12\n\n        def arc_add(xn, fn):\n            nonlocal arc_x, arc_f, K\n            fn = float(fn)\n            if arc_x.shape[0] < K:\n                arc_x = np.vstack([arc_x, xn.copy()]); arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < self.budget:\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0); q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            score = (np.abs(pull) / (step + eps)) + 0.18 * (iqr / (span + eps)) + 0.12 * prio\n            score = np.maximum(score, eps)\n            temp = 0.85 + 0.25 * np.std(score)\n            s = score - score.max()\n            p = np.exp(s / max(1e-6, temp)); p /= p.sum()\n            coord = int(self.rng.choice(d, p=p))\n\n            p_follow = bi[coord] / (bi[coord] + bj[coord])\n            if abs(pull[coord]) > 1e-12:\n                nominal = int(np.sign(pull[coord]))\n            else:\n                nominal = int(np.sign(mom[coord])) if mom[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nominal if self.rng.random() < p_follow else -nominal\n\n            frac = min(1.0, abs(pull[coord]) / (step[coord] + eps))\n            mag = step[coord] * (0.65 + 0.7 * self.rng.random()) * (0.45 + 0.55 * frac)\n            delta = float(sign * mag)\n\n            x_try = x_best.copy(); x_try[coord] = np.clip(x_try[coord] + delta, lb, ub)\n            f_try = float(func(x_try)); evals += 1; arc_add(x_try, f_try)\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy(); improved = True\n                bi[coord] += 1.0; prio[coord] = min(8.0, prio[coord] + 1.0)\n                mom[coord] = 0.7 * mom[coord] + 0.3 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * (1.20 + 0.05 * self.rng.random())))\n                no_improve = 0\n                if evals < self.budget and self.rng.random() < 0.45:\n                    ext = 0.4 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb, ub)\n                    f_ext = float(func(x_ext)); evals += 1; arc_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                        mom[coord] = 0.85 * mom[coord] + 0.15 * ext\n            else:\n                if evals < self.budget:\n                    x_opp = x_best.copy(); x_opp[coord] = np.clip(x_opp[coord] - delta, lb, ub)\n                    f_opp = float(func(x_opp)); evals += 1; arc_add(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy(); improved = True\n                        bj[coord] += 1.0; prio[coord] = min(8.0, prio[coord] + 0.7)\n                        mom[coord] = 0.7 * mom[coord] + 0.3 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * (1.15 + 0.04 * self.rng.random())))\n                        no_improve = 0\n                        if evals < self.budget and self.rng.random() < 0.35:\n                            ext = 0.33 * (-delta)\n                            x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb, ub)\n                            f_ext = float(func(x_ext)); evals += 1; arc_add(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext; x_best = x_ext.copy()\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                    else:\n                        bj[coord] += 0.45\n                        bi[coord] = max(1.0, bi[coord] * 0.996)\n                        prio[coord] = max(-8.0, prio[coord] - 0.5)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.66))\n                        mom[coord] *= 0.6\n                        no_improve += 1\n                else:\n                    no_improve += 1\n\n            if no_improve >= self.patience and evals < self.budget:\n                no_improve = 0\n                weight = (iqr / (span + eps)) * (1.0 / (step + eps) + 0.08)\n                s = weight - weight.max(); w = np.exp(s); w /= w.sum()\n                k = min(d, max(1, d // 5))\n                picks = list(self.rng.choice(d, size=k, replace=False, p=w))\n                improved_any = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = 0.55 * step[c] + 0.28 * span * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb, ub)\n                    fj = float(func(xj)); evals += 1; arc_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step[c] = float(min(max_step[c], step[c] * 1.22))\n                        prio[c] = min(8.0, prio[c] + 1.0)\n                        mom[c] = np.sign(jump) * 0.5 * abs(jump)\n                        improved_any = True\n                        break\n                if (not improved_any) and evals < self.budget:\n                    scale = np.maximum(0.12 * step, 0.05 * span)\n                    u = self.rng.random(d)\n                    cauchy = np.tan(np.pi * (u - 0.5)) * scale * (0.45 + 0.6 * self.rng.random(d))\n                    mask = self.rng.random(d) < 0.18\n                    if not mask.any():\n                        mask[self.rng.integers(0, d)] = True\n                    for i in np.where(mask)[0]:\n                        if evals >= self.budget: break\n                        xt = x_best.copy(); xt[i] = np.clip(x_best[i] + cauchy[i], lb, ub)\n                        ft = float(func(xt)); evals += 1; arc_add(xt, ft)\n                        if ft < f_best:\n                            f_best = ft; x_best = xt.copy()\n                            step[i] = float(min(max_step[i], step[i] * 1.18))\n                            prio[i] = min(8.0, prio[i] + 1.0)\n                            mom[i] = np.sign(cauchy[i]) * 0.5 * abs(cauchy[i])\n                            break\n\n            prio *= 0.993; mom *= 0.991\n            if arc_x.shape[0] > self.archive_size:\n                o = np.argsort(arc_f); keep = o[:self.archive_size]; arc_x = arc_x[keep]; arc_f = arc_f[keep]\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 24, "feedback": "The algorithm SeparableAdaptiveMedianPullV2 scored 0.807 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79f38872-84f7-4883-9a26-9ff7fbe59b87"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.976673669341752}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9744620668139166}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9800575448937601}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9337341542661218}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9409310885065357}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9214320795916333}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.37653147860788116}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.7380274799634945}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.7544058138376101}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.4491473310683285}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.5964195114732027}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.502133264418106}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9850154536764085}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9896499554142902}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9918949427325034}], "aucs": [0.976673669341752, 0.9744620668139166, 0.9800575448937601, 0.9337341542661218, 0.9409310885065357, 0.9214320795916333, 0.37653147860788116, 0.7380274799634945, 0.7544058138376101, 0.4491473310683285, 0.5964195114732027, 0.502133264418106, 0.9850154536764085, 0.9896499554142902, 0.9918949427325034]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2551.0, "Edges": 2550.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.99921599372795, "Degree Variance": 2.19756896589081, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.62382176520994, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3210248275206338, "Depth Entropy": 2.1955848599135153, "Assortativity": 9.345348890290105e-09, "Average Eccentricity": 18.809486475891806, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003920031360250882, "Average Shortest Path": 10.406446683730332, "mean_complexity": 13.333333333333334, "total_complexity": 40.0, "mean_token_count": 759.0, "total_token_count": 2277.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "56fa7a23-a42a-4cd7-8ea8-9f319e4a7d35", "fitness": 0.7045105562601808, "name": "SeparableAdaptiveMedianPull", "description": "Compact Separable Median-Pull with Beta sign priors, multiplicative per-dim step adaption, light momentum and rare Cauchy escapes for cheap robust separable exploitation.", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPull:\n    def __init__(self, budget, dim, archive_size=8, init_step=None, min_step=1e-6,\n                 max_step=None, patience=7, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0 * np.ones(d); ub = 5.0 * np.ones(d)\n        span = ub - lb\n        # init steps\n        if self.init_step is None:\n            step = np.full(d, 0.12 * span[0])\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = np.full(d, 0.5 * span[0])\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = min(max(3, d + 1), max(1, self.budget // 8), 10)\n        xs = []\n        fs = []\n        for _ in range(n_init):\n            if evals >= self.budget: break\n            x = lb + self.rng.random(d) * span\n            fs.append(float(func(x))); xs.append(x.copy()); evals += 1\n        if len(xs) == 0:\n            x = lb + self.rng.random(d) * span\n            fs.append(float(func(x))); xs.append(x.copy()); evals += 1\n\n        arc_x = np.vstack(xs)\n        arc_f = np.array(fs, float)\n        topk = min(self.archive_size, arc_x.shape[0])\n\n        bi = np.ones(d); bj = np.ones(d)   # Beta-like counts\n        mom = np.zeros(d)\n        prior = np.zeros(d)\n        niceless = 0\n\n        best_idx = int(np.argmin(arc_f))\n        x_best = arc_x[best_idx].copy(); f_best = float(arc_f[best_idx])\n\n        def add_arc(xn, fn):\n            nonlocal arc_x, arc_f\n            fn = float(fn)\n            if arc_x.shape[0] < topk:\n                arc_x = np.vstack([arc_x, xn.copy()])\n                arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < self.budget:\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0)\n            q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            abs_pull = np.abs(pull)\n\n            # coordinate selection: prefer large normalized pull and uncertain dims\n            score = (abs_pull / (step + 1e-12)) + 0.22 * (iqr / (span + 1e-12)) + 0.1 * prior\n            score = np.maximum(score, 1e-12)\n            s = score - score.max()\n            temp = max(0.45, 0.9 + 0.22 * np.std(score))\n            probs = np.exp(s / temp); probs /= probs.sum()\n            coord = int(self.rng.choice(d, p=probs))\n\n            # sign via Beta-sampled preference, prefer median direction\n            follow_p = self.rng.beta(bi[coord] + 1e-9, bj[coord] + 1e-9)\n            if abs_pull[coord] > 1e-12:\n                nominal = int(np.sign(pull[coord]))\n            else:\n                nominal = int(np.sign(mom[coord])) if mom[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nominal if self.rng.random() < follow_p else -nominal\n\n            frac = min(1.0, abs_pull[coord] / (step[coord] + 1e-12))\n            mag = step[coord] * (0.6 + 0.8 * self.rng.random()) * (0.4 + 0.6 * frac)\n            delta = float(sign * mag)\n\n            # primary probe\n            if evals >= self.budget: break\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            add_arc(x_try, f_try)\n\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy(); improved = True\n                # reward direction\n                if sign == nominal: bi[coord] += 1.0\n                else: bj[coord] += 0.6\n                prior[coord] = min(8.0, prior[coord] + 0.9)\n                mom[coord] = 0.75 * mom[coord] + 0.25 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * (1.16 + 0.04 * self.rng.random())))\n                # small extrapolate\n                if evals < self.budget and self.rng.random() < 0.55:\n                    ext = 0.35 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    add_arc(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                        mom[coord] = 0.85 * mom[coord] + 0.15 * ext\n                niceless = 0\n            else:\n                # mirrored check\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    add_arc(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy(); improved = True\n                        # reward opposite\n                        bj[coord] += 1.0\n                        prior[coord] = min(8.0, prior[coord] + 0.7)\n                        mom[coord] = 0.75 * mom[coord] - 0.25 * delta\n                        step[coord] = float(min(max_step[coord], step[coord] * (1.14 + 0.04 * self.rng.random())))\n                        if evals < self.budget and self.rng.random() < 0.45:\n                            ext = -0.3 * delta\n                            x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                            f_ext = float(func(x_ext)); evals += 1\n                            add_arc(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext; x_best = x_ext.copy()\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.06))\n                        niceless = 0\n                    else:\n                        # shrink & mild penalty\n                        bj[coord] += 0.28\n                        bi[coord] = max(1.0, bi[coord] * 0.996)\n                        prior[coord] = max(-8.0, prior[coord] - 0.45)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.66))\n                        mom[coord] *= 0.62\n                        niceless += 1\n                else:\n                    niceless += 1\n\n            # stagnation rescue\n            if niceless >= self.patience and evals < self.budget:\n                niceless = 0\n                cand = (iqr / (span + 1e-12)) * (1.0 / (step + 1e-12) + 0.1)\n                s = cand - cand.max()\n                w = np.exp(s); w /= w.sum()\n                k = min(3, max(1, d // 6))\n                picks = list(self.rng.choice(d, size=min(d, k), replace=False, p=w))\n                got = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = 0.6 * step[c] + 0.25 * span[c] * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    add_arc(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step[c] = float(min(max_step[c], step[c] * 1.24))\n                        prior[c] = min(8.0, prior[c] + 1.0)\n                        mom[c] = np.sign(jump) * 0.5 * abs(jump)\n                        got = True\n                        break\n                if not got and evals < self.budget:\n                    # rare heavy-tail Cauchy-like per-dim moves\n                    scale = np.maximum(0.12 * step, 0.04 * span)\n                    z = np.tan(np.pi * (self.rng.random(d) - 0.5))\n                    jumpvec = z * scale * (0.6 + 0.4 * self.rng.random(d))\n                    mask = (self.rng.random(d) < 0.22)\n                    for i in range(d):\n                        if evals >= self.budget: break\n                        if not mask[i]: continue\n                        x_tmp = x_best.copy(); x_tmp[i] = np.clip(x_best[i] + jumpvec[i], lb[i], ub[i])\n                        ft = float(func(x_tmp)); evals += 1\n                        add_arc(x_tmp, ft)\n                        if ft < f_best:\n                            f_best = ft; x_best = x_tmp.copy()\n                            step[i] = float(min(max_step[i], step[i] * 1.18))\n                            prior[i] = min(8.0, prior[i] + 1.0)\n                            mom[i] = 0.5 * jumpvec[i]\n                            break\n\n            # mild decay\n            prior *= 0.994\n            mom *= 0.991\n\n            if arc_x.shape[0] > self.archive_size:\n                o = np.argsort(arc_f)[:self.archive_size]\n                arc_x = arc_x[o]; arc_f = arc_f[o]\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 24, "feedback": "The algorithm SeparableAdaptiveMedianPull scored 0.705 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["789bbf7d-eb6f-4e10-b655-d33af6a5b5d4"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9760939046378173}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9772179713328235}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9745510966535844}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9296695908987241}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9324141506207513}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9589658230577952}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.3767610530862373}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.36571771029286515}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.4367880006912622}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.2905769411121708}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.15614009650232552}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.22441663158976255}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9879308375446415}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9903291980417561}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9900853378401947}], "aucs": [0.9760939046378173, 0.9772179713328235, 0.9745510966535844, 0.9296695908987241, 0.9324141506207513, 0.9589658230577952, 0.3767610530862373, 0.36571771029286515, 0.4367880006912622, 0.2905769411121708, 0.15614009650232552, 0.22441663158976255, 0.9879308375446415, 0.9903291980417561, 0.9900853378401947]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2610.0, "Edges": 2609.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.9992337164750957, "Degree Variance": 2.2275856197061112, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.596345514950166, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3174748780685863, "Depth Entropy": 2.1746625397903627, "Assortativity": 0.0, "Average Eccentricity": 18.78122605363985, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003831417624521073, "Average Shortest Path": 10.391635496931489, "mean_complexity": 14.0, "total_complexity": 42.0, "mean_token_count": 770.0, "total_token_count": 2310.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "ef65d204-dc87-4c92-a0a3-f49aa7b3c87c", "fitness": 0.8016380492220194, "name": "SeparableAdaptiveMedianPull", "description": "Compact Separable Median Pull with adaptive per-dim steps, Beta sign priors, light momentum and rare L\u00e9vy rescues \u2014 faster, leaner coordinate pulls guided by archive medians and IQRs.", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPull:\n    def __init__(self, budget, dim, archive_size=8, init_step=None, min_step=1e-6,\n                 max_step=None, patience=7, rng=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step)\n        self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim\n        lb = -5.0 * np.ones(d); ub = 5.0 * np.ones(d)\n        span = ub - lb\n        # steps init\n        if self.init_step is None:\n            step = np.full(d, 0.12 * span[0])\n        else:\n            s = np.array(self.init_step, float)\n            step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = np.full(d, 0.5 * span[0])\n        else:\n            m = np.array(self.max_step, float)\n            max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = min(max(2, d + 1), max(1, self.budget // 12), 12)\n        arc_x = []\n        arc_f = []\n        while evals < self.budget and len(arc_x) < n_init:\n            x = lb + self.rng.random(d) * span\n            arc_x.append(x.copy()); arc_f.append(float(func(x))); evals += 1\n        if not arc_x:\n            x = lb + self.rng.random(d) * span\n            arc_x.append(x.copy()); arc_f.append(float(func(x))); evals += 1\n\n        arc_x = np.vstack(arc_x); arc_f = np.array(arc_f, float)\n        topk = min(self.archive_size, arc_x.shape[0])\n        bi = np.ones(d); bj = np.ones(d)          # Beta sign priors\n        mom = np.zeros(d); prior = np.zeros(d)\n        niceless = 0\n\n        best_idx = int(np.argmin(arc_f))\n        x_best = arc_x[best_idx].copy(); f_best = float(arc_f[best_idx])\n\n        def push_archive(xn, fn):\n            nonlocal arc_x, arc_f\n            fn = float(fn)\n            if arc_x.shape[0] < topk:\n                arc_x = np.vstack([arc_x, xn.copy()]); arc_f = np.append(arc_f, fn)\n            else:\n                worst = int(np.argmax(arc_f))\n                if fn < arc_f[worst]:\n                    arc_x[worst] = xn.copy(); arc_f[worst] = fn\n\n        eps = 1e-12\n        while evals < self.budget:\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0); q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            ap = np.abs(pull)\n\n            score = (ap / (step + eps)) + 0.2 * (iqr / (span + eps)) + 0.12 * prior\n            score = np.maximum(score, eps)\n            s = score - score.max()\n            temp = max(0.5, 0.85 + 0.25 * np.std(score))\n            p = np.exp(s / temp); p /= p.sum()\n            coord = int(self.rng.choice(d, p=p))\n\n            # sign: bias towards median direction via Beta sampling\n            follow_p = self.rng.beta(bi[coord] + 1e-9, bj[coord] + 1e-9)\n            if ap[coord] > 1e-12:\n                nom = int(np.sign(pull[coord]))\n            else:\n                nom = int(np.sign(mom[coord])) if mom[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nom if self.rng.random() < follow_p else -nom\n\n            frac = min(1.0, ap[coord] / (step[coord] + eps))\n            mag = step[coord] * (0.7 + 0.6 * self.rng.random()) * (0.6 + 0.6 * frac)\n            mag = min(mag, max_step[coord])\n            delta = float(sign * mag)\n\n            # primary probe\n            if evals >= self.budget: break\n            x_try = x_best.copy()\n            x_try[coord] = np.clip(x_try[coord] + delta, lb[coord], ub[coord])\n            f_try = float(func(x_try)); evals += 1\n            push_archive(x_try, f_try)\n\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy()\n                bi[coord] += 1.0; prior[coord] = min(8.0, prior[coord] + 1.0)\n                mom[coord] = 0.75 * mom[coord] + 0.25 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * (1.22 + 1e-12)))\n                niceless = 0\n                # light extrapolation (cheap)\n                if evals < self.budget:\n                    ext = 0.35 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb[coord], ub[coord])\n                    f_ext = float(func(x_ext)); evals += 1\n                    push_archive(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.06))\n                        mom[coord] = 0.8 * mom[coord] + 0.2 * ext\n            else:\n                # mirrored check\n                if evals < self.budget:\n                    x_opp = x_best.copy()\n                    x_opp[coord] = np.clip(x_opp[coord] - delta, lb[coord], ub[coord])\n                    f_opp = float(func(x_opp)); evals += 1\n                    push_archive(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy()\n                        bj[coord] += 1.0; prior[coord] = min(8.0, prior[coord] + 0.8)\n                        mom[coord] = 0.75 * mom[coord] + 0.25 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.18))\n                        niceless = 0\n                    else:\n                        # both failed -> shrink & mild penalize\n                        bj[coord] += 0.35\n                        bi[coord] = max(1.0, bi[coord] * 0.995)\n                        prior[coord] = max(-8.0, prior[coord] - 0.45)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.62))\n                        mom[coord] *= 0.6\n                        niceless += 1\n                else:\n                    niceless += 1\n\n            # stagnation rescue\n            prior *= 0.994\n            mom *= 0.992\n\n            if niceless >= self.patience and evals < self.budget:\n                niceless = 0\n                cand = (iqr / (span + eps)) * (1.0 / (step + eps) + 0.12)\n                s = cand - cand.max()\n                w = np.exp(s); w /= w.sum()\n                k = min(3, max(1, d // 6))\n                picks = list(self.rng.choice(d, size=min(d, k), replace=False, p=w))\n                success = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = 0.55 * step[c] + 0.28 * span[c] * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb[c], ub[c])\n                    fj = float(func(xj)); evals += 1\n                    push_archive(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step[c] = float(min(max_step[c], step[c] * 1.28))\n                        prior[c] = min(8.0, prior[c] + 1.0)\n                        mom[c] = np.sign(jump) * 0.5 * abs(jump)\n                        success = True\n                        break\n                if not success and evals < self.budget:\n                    # rare heavy-tail multi-dim L\u00e9vy-like tries\n                    scale = np.maximum(0.14 * step, 0.04 * span)\n                    z = np.tan(np.pi * (self.rng.random(d) - 0.5))\n                    jumpvec = z * scale * (0.5 + self.rng.random(d))\n                    mask = (self.rng.random(d) < 0.22)\n                    trial = np.clip(x_best + jumpvec * mask, lb, ub)\n                    for i in range(d):\n                        if evals >= self.budget: break\n                        if not mask[i]: continue\n                        x_tmp = x_best.copy(); x_tmp[i] = trial[i]\n                        ft = float(func(x_tmp)); evals += 1\n                        push_archive(x_tmp, ft)\n                        if ft < f_best:\n                            f_best = ft; x_best = x_tmp.copy()\n                            step[i] = float(min(max_step[i], step[i] * 1.18))\n                            prior[i] = min(8.0, prior[i] + 1.0)\n                            mom[i] = 0.5 * (x_tmp[i] - x_best[i])\n                            break\n\n            # keep archive compact\n            if arc_x.shape[0] > self.archive_size:\n                o = np.argsort(arc_f)[:self.archive_size]\n                arc_x = arc_x[o]; arc_f = arc_f[o]\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 24, "feedback": "The algorithm SeparableAdaptiveMedianPull scored 0.802 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["789bbf7d-eb6f-4e10-b655-d33af6a5b5d4"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9818770204457485}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9777398270783038}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9802106522360803}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9111126779489498}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9038273612722868}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9057844139019197}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.6940666449690056}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.5824354400828081}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.7137900854606856}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.5863573143799139}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.3885681643570993}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.4208840230862948}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9912133750840415}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9948080199636589}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9918957180634942}], "aucs": [0.9818770204457485, 0.9777398270783038, 0.9802106522360803, 0.9111126779489498, 0.9038273612722868, 0.9057844139019197, 0.6940666449690056, 0.5824354400828081, 0.7137900854606856, 0.5863573143799139, 0.3885681643570993, 0.4208840230862948, 0.9912133750840415, 0.9948080199636589, 0.9918957180634942]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2430.0, "Edges": 2429.0, "Max Degree": 33.0, "Min Degree": 1.0, "Mean Degree": 1.9991769547325102, "Degree Variance": 2.2683120797981338, "Transitivity": 0.0, "Max Depth": 14.0, "Min Depth": 2.0, "Mean Depth": 8.367054610564011, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3164265130265127, "Depth Entropy": 2.1023678978321114, "Assortativity": 0.0, "Average Eccentricity": 16.06255144032922, "Diameter": 21.0, "Radius": 11.0, "Edge Density": 0.00041152263374485596, "Average Shortest Path": 10.085648889363267, "mean_complexity": 12.333333333333334, "total_complexity": 37.0, "mean_token_count": 712.0, "total_token_count": 2136.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}, "archive_direction": "decrease", "archive_feature": "total token count"}
{"id": "1feb9590-5fe4-45aa-acd8-537b208d842c", "fitness": 0.6388164719760305, "name": "ParabolicAdaptiveMirroredDescent", "description": "Parabolic Adaptive Mirrored Descent with compact archive, momentum\u2011biased coordinate scoring, analytic 1\u2011D parabola minima, occasional centroid pulls and Cauchy escapes \u2014 cheap, robust separable probes with adaptive multiplicative steps.", "code": "import numpy as np\n\nclass ParabolicAdaptiveMirroredDescent:\n    def __init__(self, budget=1000, dim=10, init_step=0.08,\n                 succ_mult=1.5, fail_mult=0.6, min_step_frac=1e-9, max_step_frac=1.2,\n                 global_escape=0.03, centroid_rate=0.08, polish_frac=0.03, archive_size=12, seed=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_step_frac = float(init_step)\n        self.succ_mult, self.fail_mult = float(succ_mult), float(fail_mult)\n        self.min_pf, self.max_pf = float(min_step_frac), float(max_step_frac)\n        self.global_escape = float(global_escape)\n        self.centroid_rate = float(centroid_rate)\n        self.polish_frac = float(polish_frac)\n        self.archive_size = int(archive_size)\n\n    def __call__(self, func):\n        dim = self.dim; rng = self.rng\n        # bounds if provided else [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, float); ub = np.asarray(func.bounds.ub, float)\n            if lb.shape == (): lb = np.full(dim, float(lb))\n            if ub.shape == (): ub = np.full(dim, float(ub))\n        else:\n            lb = np.full(dim, -5.0); ub = np.full(dim, 5.0)\n\n        scale = max(1e-12, np.linalg.norm(ub - lb))\n        base = max(1e-12, self.init_step_frac * scale)\n        min_step = max(1e-15, self.min_pf * scale)\n        max_step = max(1e-12, self.max_pf * scale)\n\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget: return np.inf\n            x = np.clip(np.asarray(x, float), lb, ub)\n            v = float(func(x)); evals += 1\n            return v\n\n        x = rng.uniform(lb, ub); f = safe_eval(x)\n        steps = np.full(dim, base, float)\n        trials = np.zeros(dim, int); succ = np.zeros(dim, int)\n        archive = [(f, x.copy())]\n        last_dx = np.zeros(dim)\n\n        while evals < self.budget:\n            # occasional heavy Cauchy escape\n            if rng.random() < self.global_escape:\n                u = rng.random(dim)\n                jump = 0.12 * scale * np.tan(np.pi * (u - 0.5))\n                xt = np.clip(x + jump, lb, ub)\n                ft = safe_eval(xt)\n                if ft < f:\n                    f, x = ft, xt.copy()\n                    archive.append((f, x.copy()))\n                    archive = sorted(archive, key=lambda t: t[0])[:self.archive_size]\n                continue\n\n            # occasional centroid pull to exploit multi-dim structure\n            if len(archive) > 1 and rng.random() < self.centroid_rate:\n                centroid = np.mean([p for _, p in archive], axis=0)\n                step = 0.25 * scale * (centroid - x)\n                if np.linalg.norm(step) > 1e-15:\n                    xt = np.clip(x + step, lb, ub); ft = safe_eval(xt)\n                    if ft < f:\n                        last_dx = xt - x; f, x = ft, xt.copy()\n                        archive.append((f, x.copy()))\n                        archive = sorted(archive, key=lambda t: t[0])[:self.archive_size]\n                    else:\n                        # small random thrust toward centroid occasionally\n                        if rng.random() < 0.3:\n                            xt = np.clip(x + 0.5 * step + 0.02*scale*rng.standard_normal(dim), lb, ub)\n                            ft = safe_eval(xt)\n                            if ft < f:\n                                last_dx = xt - x; f, x = ft, xt.copy()\n                                archive.append((f, x.copy()))\n                                archive = sorted(archive, key=lambda t: t[0])[:self.archive_size]\n                continue\n\n            # coordinate selection: prefer larger steps, moderate recent success, penalize extreme trial count\n            score = steps * (1.0 + 0.6 * (succ / (1 + trials))) / (1.0 + 0.18 * trials)\n            d = int(np.argmax(score + 1e-12 * rng.random(dim)))\n            s = float(np.clip(steps[d], min_step, max_step))\n\n            # momentum hint: sometimes bias probe in direction of last improvement's sign\n            bias = 0.0\n            if rng.random() < 0.22:\n                bias = 0.35 * np.sign(last_dx[d]) * s\n\n            # mirrored probe if budget allows\n            if evals <= self.budget - 2:\n                xp = x.copy(); xm = x.copy()\n                xp[d] = np.clip(xp[d] + s + bias, lb[d], ub[d])\n                xm[d] = np.clip(xm[d] - s + bias, lb[d], ub[d])\n                fp = safe_eval(xp); fm = safe_eval(xm)\n                trials[d] += 2\n\n                # pick immediate improvement if any\n                if fp < f or fm < f:\n                    if fp < fm:\n                        last_dx = xp - x; f, x = float(fp), xp.copy()\n                    else:\n                        last_dx = xm - x; f, x = float(fm), xm.copy()\n                    succ[d] += 1\n                    steps[d] = min(max_step, steps[d] * self.succ_mult)\n                    archive.append((f, x.copy())); archive = sorted(archive, key=lambda t: t[0])[:self.archive_size]\n\n                    # analytic parabola refine around triple (-s,0,s) if not degenerate\n                    denom = (fp + fm - 2.0 * f)\n                    if abs(denom) > 1e-16:\n                        x_star = -s * (fp - fm) / (2.0 * denom)\n                        x_star = float(np.clip(x_star, -2.0 * s, 2.0 * s))\n                        if abs(x_star) > 1e-12 and evals < self.budget:\n                            xt = x.copy(); xt[d] = np.clip(xt[d] + x_star, lb[d], ub[d])\n                            ft = safe_eval(xt); trials[d] += 1\n                            if ft < f:\n                                last_dx = xt - x; f, x = float(ft), xt.copy()\n                                succ[d] += 1; steps[d] = min(max_step, steps[d] * self.succ_mult)\n                                archive.append((f, x.copy())); archive = sorted(archive, key=lambda t: t[0])[:self.archive_size]\n                else:\n                    # try parabola minimum if curvature positive\n                    denom = (fp + fm - 2.0 * f)\n                    if denom > 1e-14:\n                        x_star = -s * (fp - fm) / (2.0 * denom)\n                        x_star = float(np.clip(x_star, -2.0 * s, 2.0 * s))\n                        if abs(x_star) > 1e-12 and evals < self.budget:\n                            xt = x.copy(); xt[d] = np.clip(xt[d] + x_star, lb[d], ub[d])\n                            ft = safe_eval(xt); trials[d] += 1\n                            if ft < f:\n                                last_dx = xt - x; f, x = float(ft), xt.copy()\n                                succ[d] += 1; steps[d] = min(max_step, steps[d] * self.succ_mult)\n                                archive.append((f, x.copy())); archive = sorted(archive, key=lambda t: t[0])[:self.archive_size]\n                                continue\n                    steps[d] = max(min_step, steps[d] * self.fail_mult)\n            else:\n                # last moves: single-sided attempt\n                if rng.random() < 0.5:\n                    xt = x.copy(); xt[d] = np.clip(xt[d] + s, lb[d], ub[d])\n                else:\n                    xt = x.copy(); xt[d] = np.clip(xt[d] - s, lb[d], ub[d])\n                ft = safe_eval(xt); trials[d] += 1\n                if ft < f:\n                    last_dx = xt - x; f, x = float(ft), xt.copy()\n                    succ[d] += 1; steps[d] = min(max_step, steps[d] * self.succ_mult)\n                    archive.append((f, x.copy())); archive = sorted(archive, key=lambda t: t[0])[:self.archive_size]\n                else:\n                    steps[d] = max(min_step, steps[d] * self.fail_mult)\n\n            # stale-dimension nudge\n            if trials[d] > 60 and succ[d] == 0:\n                steps[d] = min(max_step, steps[d] * 1.6)\n                trials[d] = trials[d] // 2\n\n        # cheap coordinate polish\n        fin = self.polish_frac * scale\n        while evals < self.budget and fin > 1e-12 * scale:\n            improved = False\n            for d in range(dim):\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] + fin, lb[d], ub[d]); ft = safe_eval(xt)\n                if ft < f: f, x, improved = float(ft), xt.copy(), True; continue\n                if evals >= self.budget: break\n                xt = x.copy(); xt[d] = np.clip(xt[d] - fin, lb[d], ub[d]); ft = safe_eval(xt)\n                if ft < f: f, x, improved = float(ft), xt.copy(), True\n            if not improved: fin *= 0.5\n\n        self.x_opt = np.asarray(x, float); self.f_opt = float(f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 24, "feedback": "The algorithm ParabolicAdaptiveMirroredDescent scored 0.639 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["86d13d75-4636-4c2f-9196-d0f90890a1f4"], "operator": "refine", "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9934504712749102}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9859845864662979}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9854975675124241}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9543698364508276}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.95948207803315}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9597766904726946}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.15071084846182325}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.10313545985100192}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.12891747237805995}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.11405581473337623}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.14935247199135537}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.10823986058501944}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9963398358341252}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9964916745853407}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9964424110100503}], "aucs": [0.9934504712749102, 0.9859845864662979, 0.9854975675124241, 0.9543698364508276, 0.95948207803315, 0.9597766904726946, 0.15071084846182325, 0.10313545985100192, 0.12891747237805995, 0.11405581473337623, 0.14935247199135537, 0.10823986058501944, 0.9963398358341252, 0.9964916745853407, 0.9964424110100503]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n", "ast_features": {"Nodes": 2452.0, "Edges": 2451.0, "Max Degree": 26.0, "Min Degree": 1.0, "Mean Degree": 1.999184339314845, "Degree Variance": 1.9331151585149389, "Transitivity": 0.0, "Max Depth": 16.0, "Min Depth": 2.0, "Mean Depth": 8.942549371633753, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3335750272612807, "Depth Entropy": 2.280910526809039, "Assortativity": 0.0, "Average Eccentricity": 19.04078303425775, "Diameter": 26.0, "Radius": 13.0, "Edge Density": 0.0004078303425774878, "Average Shortest Path": 11.115905017294935, "mean_complexity": 14.666666666666666, "total_complexity": 44.0, "mean_token_count": 705.3333333333334, "total_token_count": 2116.0, "mean_parameter_count": 5.333333333333333, "total_parameter_count": 16.0}, "archive_direction": "decrease", "archive_feature": "total token count"}

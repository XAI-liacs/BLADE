{"method_name": "ES-guided", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "138512ec-2c01-4d9a-9822-0986ebe60972", "fitness": 0.766753410657637, "name": "EnsembleEvolutionaryQuasiNewtonConsensus", "description": "Ensemble Evolutionary Quasi‑Newton Consensus — combine L-BFGS-like low-rank quasi‑Newton steps from regression gradients, sparse finite‑difference probes, evolutionary sparse recombination, PCA-guided sampling, and a UCB meta-bandit to allocate budget adaptively.", "code": "import numpy as np\n\nclass EnsembleEvolutionaryQuasiNewtonConsensus:\n    \"\"\"\n    Ensemble Evolutionary Quasi-Newton Consensus (novel complex hybrid)\n\n    - UCB meta-arm allocator over multiple heuristics (L-BFGS two-loop quasi-Newton on regression gradients,\n      sparse central-difference gradient probing, evolutionary sparse recombination, PCA-elite sampling,\n      pattern-search polish and global restarts).\n    - Gradients estimated cheaply by linear regression on local history when possible (no extra function evaluations).\n    - Maintains an archive of elites and a history buffer; forms L-BFGS direction from stored secant pairs (s,y)\n      where y is computed from regression gradient differences (no extra function evaluations).\n    - Sparse finite-difference arm performs targeted central differences on a small adaptive mask.\n    - Evolutionary arm recombines elites in sparse blocks and applies adaptive perturbations.\n    - PCA-guided sampling explores promising low-rank directions from the elite archive.\n    - Budget-aware parabolic 1D refinement and 2D quadratic polishing in promising planes.\n    - Many safeguards and adaptive trust/truncation to respect budget and remain robust.\n    \"\"\"\n    def __init__(self, budget, dim, seed=None,\n                 init_pop=None, sparsity_frac=0.14, lb=-5.0, ub=5.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # bounds defaults (can be overridden by func.bounds in call)\n        self.default_lb = float(lb)\n        self.default_ub = float(ub)\n        # population / history sizing\n        self.max_history = max(300, 15 * self.dim)\n        self.init_pop = init_pop if init_pop is not None else min(max(8, self.dim + 4), max(8, self.budget // 6))\n        # sparsity used for sparse probes / recombination block sizes\n        self.sparsity = max(1, int(max(1, self.dim * float(sparsity_frac))))\n        # bandit arms\n        self.arms = [\"l_bfgs_reg\", \"sparse_fd\", \"evo_recomb\", \"pca_elite\", \"pattern_polish\", \"global_random\"]\n        self.n_arms = len(self.arms)\n        self.arm_counts = np.ones(self.n_arms, dtype=float)  # to avoid divide-by-zero\n        self.arm_rewards = np.zeros(self.n_arms, dtype=float)\n        # L-BFGS memory\n        self.lbfgs_m = max(3, min(12, self.dim))\n        self.s_list = []  # list of s = x_{k+1} - x_k\n        self.y_list = []  # list of y = grad_{k+1} - grad_k (grad estimated via regression)\n        # trust region and per-dim scales\n        self.domain_scale = None\n        self.trust = None\n        self.per_dim_scale = np.ones(self.dim)\n        # momentum and exponential forget\n        self.momentum = np.zeros(self.dim)\n        self.mom_decay = 0.85\n        self.forget = 0.96\n        # elite archive\n        self.elite_size = max(6, min(30, self.budget // 10))\n        # small constants\n        self.eps = 1e-12\n\n    def __call__(self, func):\n        # extract bounds if func provides them, otherwise use defaults\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(self.dim, float(lb))\n            if ub.shape == (): ub = np.full(self.dim, float(ub))\n        except Exception:\n            lb = np.full(self.dim, self.default_lb)\n            ub = np.full(self.dim, self.default_ub)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        # safe evaluation wrapper counting budget\n        evals = 0\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise StopIteration\n            fx = func(x)\n            evals += 1\n            return float(fx)\n\n        # initialize domain scale and trust\n        self.domain_scale = float(np.mean(ub - lb))\n        self.trust = max(1e-9, 0.14 * self.domain_scale)\n        # initial stratified sampling (LHS-ish)\n        n_init = int(min(self.init_pop, max(4, self.budget // 6)))\n        n_init = max(2, n_init)\n        X_hist = np.empty((0, self.dim), dtype=float)\n        f_hist = np.empty((0,), dtype=float)\n        try:\n            # create LHS-like initial samples\n            strata = np.linspace(0.0, 1.0, n_init + 1)[:-1]\n            for i in range(n_init):\n                x = lb + (strata[i] + self.rng.random(self.dim) / n_init) * (ub - lb)\n                fx = safe_eval(x)\n                X_hist = np.vstack([X_hist, x])\n                f_hist = np.concatenate([f_hist, [fx]])\n        except StopIteration:\n            pass\n\n        # center evaluation\n        if evals < self.budget:\n            try:\n                center = 0.5 * (lb + ub)\n                f_center = safe_eval(center)\n                X_hist = np.vstack([X_hist, center])\n                f_hist = np.concatenate([f_hist, [f_center]])\n            except StopIteration:\n                pass\n\n        # initial best\n        if X_hist.shape[0] == 0:\n            # no evaluations possible\n            return float(np.inf), np.zeros(self.dim)\n        best_idx = int(np.argmin(f_hist))\n        x_best = X_hist[best_idx].copy()\n        f_best = float(f_hist[best_idx])\n\n        # helper: maintain and shrink history to top records but also coverage\n        def maintain_history():\n            nonlocal X_hist, f_hist\n            if X_hist.shape[0] > self.max_history:\n                keep = np.argsort(f_hist)[:self.max_history]\n                X_hist = X_hist[keep]\n                f_hist = f_hist[keep]\n\n        # UCB selection\n        def ucb_select():\n            total = np.sum(self.arm_counts)\n            avg = self.arm_rewards / (self.arm_counts + 1e-12)\n            bonus = np.sqrt(2.0 * np.log(1.0 + total) / (self.arm_counts + 1e-12))\n            vals = avg + bonus\n            # random tie-break\n            idx = int(np.argmax(vals + 1e-9 * self.rng.random(self.n_arms)))\n            return idx\n\n        def update_arm(a_idx, reward):\n            self.arm_counts[a_idx] += 1.0\n            # reward normalized into [0,1] using relative improvement heuristic\n            self.arm_rewards[a_idx] += float(reward)\n\n        # linear regression gradient estimator using neighborhood (no extra evaluations)\n        def estimate_grad_via_regression(x, k_neigh=None):\n            # use nearest neighbors from history to fit linear model f ~ a + g^T (x - c)\n            if X_hist.shape[0] < 3:\n                return np.zeros(self.dim)\n            if k_neigh is None:\n                k_neigh = min(max(6, 2*self.dim), X_hist.shape[0])\n            # choose near points (weighted by proximity + value)\n            dists = np.linalg.norm(X_hist - x, axis=1)\n            idx = np.argsort(dists)[:k_neigh]\n            Xloc = X_hist[idx]\n            Yloc = f_hist[idx]\n            center = np.mean(Xloc, axis=0)\n            Z = Xloc - center  # n x d\n            # solve Z * g = Y - mean(Y) in least squares\n            y = Yloc - np.mean(Yloc)\n            # regularize\n            try:\n                g, *_ = np.linalg.lstsq(Z, y, rcond=None)\n                g = g.flatten()\n            except Exception:\n                g = np.zeros(self.dim)\n            # limit size\n            g = np.clip(g, -1e6, 1e6)\n            return g\n\n        # L-BFGS two-loop to compute H * grad approximation\n        def lbfgs_two_loop(grad):\n            q = grad.copy()\n            alpha = []\n            rho = []\n            for s,y in zip(reversed(self.s_list), reversed(self.y_list)):\n                ys = np.dot(y, s)\n                if ys <= 1e-18:\n                    alpha.append(0.0); rho.append(0.0)\n                    continue\n                r = 1.0 / ys\n                rho.append(r)\n                a = r * np.dot(s, q)\n                alpha.append(a)\n                q = q - a * y\n            # use gamma as scaling: gamma = (s_{last}^T y_{last})/(y_{last}^T y_{last})\n            if len(self.s_list) > 0 and np.linalg.norm(self.y_list[-1]) > 0:\n                gamma = np.dot(self.s_list[-1], self.y_list[-1]) / (np.dot(self.y_list[-1], self.y_list[-1]) + 1e-12)\n                gamma = np.clip(gamma, 1e-6, 1e6)\n            else:\n                gamma = 1.0\n            r = gamma * q\n            # reverse loop\n            for (s,y), a, r_i in zip(zip(self.s_list, self.y_list), reversed(alpha), reversed(rho)):\n                if r_i <= 0:\n                    continue\n                b = r_i * np.dot(y, r)\n                r = r + s * (a - b)\n            return r\n\n        # parabolic 1D refine along direction dirv from point x0\n        def parabolic_refine_along(x0, dirv, max_step):\n            nonlocal evals, f_best, x_best\n            try:\n                if max_step <= 0 or evals >= self.budget:\n                    return None, None\n                s = max_step\n                xp = np.clip(x0 + s * dirv, lb, ub)\n                fp = safe_eval(xp)\n                xm = None; fm = None\n                if evals < self.budget:\n                    xm = np.clip(x0 - s * dirv, lb, ub)\n                    fm = safe_eval(xm)\n                # fit parabola through (-s,fm),(0,f0_est),(+s,fp); f0_est use best known local f if available:\n                f0_est = None\n                # find nearest history f at x0 (if close), otherwise use midpoint of fp,fm\n                dists = np.linalg.norm(X_hist - x0, axis=1)\n                if dists.size > 0 and np.min(dists) < 1e-9:\n                    f0_est = f_hist[np.argmin(dists)]\n                else:\n                    if fm is not None:\n                        f0_est = 0.5 * (fp + fm)\n                    else:\n                        f0_est = fp\n                ts = np.array([-s, 0.0, s])\n                ys = np.array([fm if fm is not None else fp, f0_est, fp])\n                # robust quadratic vertex\n                try:\n                    A = np.vstack([ts**2, ts, np.ones(3)]).T\n                    coeff = np.linalg.solve(A, ys)\n                    a,b = coeff[0], coeff[1]\n                    if a > 1e-16:\n                        tstar = -b/(2*a)\n                        if abs(tstar) <= 3*s and evals < self.budget:\n                            x_cand = np.clip(x0 + tstar * dirv, lb, ub)\n                            fcand = safe_eval(x_cand)\n                            if fcand < f_best:\n                                x_best = x_cand.copy(); f_best = fcand\n                                return x_cand, fcand\n                            return x_cand, fcand\n                except Exception:\n                    pass\n                # fallback: accept best of xp,xm\n                best_local = None\n                if fm is not None:\n                    if fp < fm:\n                        best_local = (xp, fp)\n                    else:\n                        best_local = (xm, fm)\n                else:\n                    best_local = (xp, fp)\n                if best_local is not None and best_local[1] < f_best:\n                    x_best = best_local[0].copy(); f_best = best_local[1]\n                    return x_best, f_best\n                return None, None\n            except StopIteration:\n                return None, None\n\n        # evolutionary sparse recombination: recombine top elites in sparse blocks\n        def evo_recombination(num_children=6):\n            nonlocal evals, x_best, f_best\n            children = []\n            # pick parents from elites (top-k)\n            n = X_hist.shape[0]\n            if n == 0:\n                return []\n            ranks = np.argsort(f_hist)\n            topk = min(max(3, self.elite_size), n)\n            parent_idxs = ranks[:topk]\n            parents = X_hist[parent_idxs]\n            # produce children by weighted average + block swapping\n            for _ in range(num_children):\n                # choose two parents\n                a,b = self.rng.integers(0, parents.shape[0], size=2)\n                p1 = parents[a].copy(); p2 = parents[b].copy()\n                # blend some coordinates and swap sparse blocks\n                child = p1.copy()\n                # number of swap blocks\n                blocks = max(1, self.rng.integers(1, min(6, self.dim//max(1,self.sparsity)+2)))\n                for _b in range(blocks):\n                    start = self.rng.integers(0, self.dim)\n                    width = max(1, self.rng.integers(1, self.sparsity+1))\n                    idxs = np.arange(start, start+width) % self.dim\n                    if self.rng.random() < 0.6:\n                        child[idxs] = p2[idxs]\n                    else:\n                        # blended\n                        alpha = self.rng.random()\n                        child[idxs] = alpha * p1[idxs] + (1-alpha) * p2[idxs]\n                # add gaussian perturbation scaled to trust & per-dim\n                noise = self.rng.normal(scale=0.5, size=self.dim) * (self.trust / max(1e-9, self.domain_scale)) * self.per_dim_scale\n                child = np.clip(child + noise, lb, ub)\n                children.append(child)\n            # evaluate children budget-aware\n            improved = False\n            for ch in children:\n                if evals >= self.budget: break\n                try:\n                    fch = safe_eval(ch)\n                except StopIteration:\n                    break\n                # record\n                X_hist_append(ch, fch)\n                if fch < f_best:\n                    # update secant memory if possible using regression grads\n                    prev_x = x_best.copy()\n                    prev_f = f_best\n                    x_best = ch.copy(); f_best = fch\n                    # form s,y from regression gradients\n                    g_old = estimate_grad_via_regression(prev_x)\n                    g_new = estimate_grad_via_regression(x_best)\n                    s = x_best - prev_x\n                    y = g_new - g_old\n                    store_secant(s, y)\n                    improved = True\n            return improved\n\n        # PCA elite sampling\n        def pca_elite_sampling(num=4):\n            nonlocal evals, x_best, f_best\n            if X_hist.shape[0] < 4:\n                return False\n            ranks = np.argsort(f_hist)\n            topk = min(self.elite_size, X_hist.shape[0])\n            elites = X_hist[ranks[:topk]]\n            # compute mean and PCA of elites\n            C = elites - np.mean(elites, axis=0)\n            try:\n                U,S,VT = np.linalg.svd(C, full_matrices=False)\n                # sample along top components with small gaussian coefficients\n                improved = False\n                for i in range(num):\n                    coeffs = np.zeros(U.shape[1])\n                    # excite only top 1-3 components\n                    k = min(3, U.shape[1])\n                    for j in range(k):\n                        coeffs[j] = self.rng.normal(scale=0.5*(self.trust/self.domain_scale))*S[j]/(1.0+S[0])\n                    cand = np.clip(np.mean(elites, axis=0) + U.dot(coeffs), lb, ub)\n                    if evals >= self.budget: break\n                    try:\n                        fc = safe_eval(cand)\n                    except StopIteration:\n                        break\n                    X_hist_append(cand, fc)\n                    if fc < f_best:\n                        prev_x = x_best.copy()\n                        x_best = cand.copy(); f_best = fc\n                        g_old = estimate_grad_via_regression(prev_x)\n                        g_new = estimate_grad_via_regression(x_best)\n                        store_secant(x_best - prev_x, g_new - g_old)\n                        improved = True\n                return improved\n            except Exception:\n                return False\n\n        # pattern search polish: directional coordinate/pattern moves with shrinking step\n        def pattern_polish(max_dirs=8):\n            nonlocal evals, x_best, f_best\n            improved = False\n            # generate pattern directions: identity + negative identity + some random orthogonalized vectors\n            dirs = []\n            for d in range(min(self.dim, max_dirs)):\n                e = np.zeros(self.dim)\n                e[d] = 1.0\n                dirs.append(e)\n                dirs.append(-e)\n            # add few random orthonormals\n            for _ in range(4):\n                v = self.rng.normal(size=self.dim)\n                v = v / (np.linalg.norm(v) + 1e-12)\n                dirs.append(v)\n            step = min(self.trust, 0.7 * self.domain_scale)\n            for dirv in dirs:\n                if evals >= self.budget: break\n                cand = np.clip(x_best + step * dirv * self.per_dim_scale, lb, ub)\n                try:\n                    fc = safe_eval(cand)\n                except StopIteration:\n                    break\n                X_hist_append(cand, fc)\n                if fc < f_best:\n                    prev_x = x_best.copy()\n                    x_best = cand.copy(); f_best = fc\n                    g_old = estimate_grad_via_regression(prev_x)\n                    g_new = estimate_grad_via_regression(x_best)\n                    store_secant(x_best - prev_x, g_new - g_old)\n                    improved = True\n            # shrink or expand trust modestly\n            if improved:\n                self.trust = min(self.domain_scale * 1.2, self.trust * 1.06)\n            else:\n                self.trust = max(self.domain_scale * 1e-6, self.trust * 0.94)\n            return improved\n\n        # sparse finite-difference probing on adaptive mask\n        def sparse_fd_probe(mask_size=None):\n            nonlocal evals, x_best, f_best\n            if mask_size is None:\n                mask_size = max(1, min(self.dim, self.sparsity))\n            # select mask coords (biased by per_dim_scale and recent improvements)\n            scores = self.per_dim_scale + 0.1 * (np.abs(self.momentum) / (np.linalg.norm(self.momentum) + 1e-12))\n            probs = scores / (np.sum(scores) + 1e-12)\n            idxs = self.rng.choice(self.dim, size=mask_size, replace=False, p=probs)\n            # central differences: for each index compute f(x+h) and f(x-h)\n            h = (1e-6 + 1e-3 * (self.trust/self.domain_scale)) * (1.0 + np.abs(x_best))\n            g_est = np.zeros(self.dim)\n            evals_needed = 2 * mask_size\n            if (self.budget - evals) < evals_needed:\n                # can't do central differences for all; do fewer\n                mask_size = max(1, (self.budget - evals)//2)\n                if mask_size <= 0:\n                    return False\n                idxs = idxs[:mask_size]\n            improved = False\n            for d in idxs:\n                if evals >= self.budget: break\n                x_plus = x_best.copy(); x_minus = x_best.copy()\n                step = h[d] if np.ndim(h)>0 else h\n                x_plus[d] = np.clip(x_plus[d] + step, lb[d], ub[d])\n                x_minus[d] = np.clip(x_minus[d] - step, lb[d], ub[d])\n                try:\n                    fp = safe_eval(x_plus)\n                except StopIteration:\n                    break\n                try:\n                    fm = safe_eval(x_minus)\n                except StopIteration:\n                    # record fp and stop\n                    X_hist_append(x_plus, fp)\n                    break\n                # central diff\n                g_est[d] = (fp - fm) / (2.0 * step + self.eps)\n                # record probes to history\n                X_hist_append(x_plus, fp)\n                X_hist_append(x_minus, fm)\n                # attempt a step along negative gradient on mask\n            # if some gradient estimated\n            norm_g = np.linalg.norm(g_est)\n            if norm_g > 1e-12:\n                dirv = -g_est / (norm_g + self.eps)\n                step = min(self.trust * 1.2, self.domain_scale * 0.6)\n                cand = np.clip(x_best + step * dirv * self.per_dim_scale, lb, ub)\n                if evals < self.budget:\n                    try:\n                        fc = safe_eval(cand)\n                    except StopIteration:\n                        fc = None\n                    if fc is not None:\n                        X_hist_append(cand, fc)\n                        if fc < f_best:\n                            prev_x = x_best.copy()\n                            x_best = cand.copy(); f_best = fc\n                            g_old = estimate_grad_via_regression(prev_x)\n                            g_new = estimate_grad_via_regression(x_best)\n                            store_secant(x_best - prev_x, g_new - g_old)\n                            improved = True\n            # adjust trust\n            if improved:\n                self.trust = min(self.domain_scale * 1.4, self.trust * 1.08)\n                self.per_dim_scale *= 1.02\n            else:\n                self.trust *= 0.96\n            return improved\n\n        # store secant pairs for L-BFGS\n        def store_secant(s, y):\n            # only accept if curvature condition satisfied: s^T y > small threshold\n            sy = np.dot(s, y)\n            if sy > 1e-12:\n                # keep limited memory\n                self.s_list.append(s.copy())\n                self.y_list.append(y.copy())\n                if len(self.s_list) > self.lbfgs_m:\n                    self.s_list.pop(0); self.y_list.pop(0)\n\n        # append to history with dedup handling\n        def X_hist_append(x, fx):\n            nonlocal X_hist, f_hist\n            # don't append duplicates close to existing\n            if X_hist.shape[0] > 0:\n                dists = np.linalg.norm(X_hist - x, axis=1)\n                if np.min(dists) < 1e-10:\n                    # maybe update fitness if better\n                    idx = int(np.argmin(dists))\n                    if fx < f_hist[idx]:\n                        f_hist[idx] = fx\n                    return\n            X_hist = np.vstack([X_hist, x.copy()])\n            f_hist = np.concatenate([f_hist, [fx]])\n            maintain_history()\n\n        # main optimization loop: select arms with UCB and run until budget exhausted\n        try:\n            while evals < self.budget:\n                # refresh best from history to be safe\n                best_idx = int(np.argmin(f_hist))\n                if f_hist[best_idx] < f_best - 1e-15:\n                    # update best and store secant using regression grads\n                    prev_x = x_best.copy()\n                    x_best = X_hist[best_idx].copy()\n                    f_best = float(f_hist[best_idx])\n                    # estimate gradients and store secant\n                    g_old = estimate_grad_via_regression(prev_x)\n                    g_new = estimate_grad_via_regression(x_best)\n                    store_secant(x_best - prev_x, g_new - g_old)\n\n                # pick arm\n                a_idx = ucb_select()\n                arm = self.arms[a_idx]\n                improved = False\n\n                # compute regression gradient at x_best cheaply for L-BFGS arm\n                if arm == \"l_bfgs_reg\":\n                    # estimate gradient via regression\n                    g = estimate_grad_via_regression(x_best)\n                    # if no secants, fall back to taking small random quasi-newton step (scaled identity)\n                    if len(self.s_list) == 0:\n                        # take negative gradient descent step (proximal) or random if gradient zero\n                        if np.linalg.norm(g) > 1e-12:\n                            step = min(self.trust * 1.2, self.domain_scale * 0.5)\n                            cand = np.clip(x_best - step * (g / (np.linalg.norm(g) + self.eps)) * self.per_dim_scale, lb, ub)\n                        else:\n                            # random small step\n                            z = self.rng.normal(size=self.dim)\n                            cand = np.clip(x_best + 0.6 * self.trust * z / (np.linalg.norm(z)+1e-12), lb, ub)\n                        if evals < self.budget:\n                            try:\n                                fc = safe_eval(cand)\n                            except StopIteration:\n                                break\n                            X_hist_append(cand, fc)\n                            if fc < f_best:\n                                prev_x = x_best.copy()\n                                x_best = cand.copy(); f_best = fc\n                                g_old = estimate_grad_via_regression(prev_x)\n                                g_new = estimate_grad_via_regression(x_best)\n                                store_secant(x_best - prev_x, g_new - g_old)\n                                improved = True\n                    else:\n                        # compute quasi-Newton direction using L-BFGS two-loop\n                        try:\n                            p = - lbfgs_two_loop(g)\n                            # scale and protect\n                            if np.linalg.norm(p) < 1e-12:\n                                # fallback random\n                                z = self.rng.normal(size=self.dim)\n                                p = z\n                            p = p / (np.linalg.norm(p) + 1e-12)\n                            step = min(self.trust * 1.4, self.domain_scale * 0.6)\n                            cand = np.clip(x_best + step * p * self.per_dim_scale, lb, ub)\n                            if evals < self.budget:\n                                try:\n                                    fc = safe_eval(cand)\n                                except StopIteration:\n                                    break\n                                X_hist_append(cand, fc)\n                                if fc < f_best:\n                                    prev_x = x_best.copy()\n                                    x_best = cand.copy(); f_best = fc\n                                    g_old = estimate_grad_via_regression(prev_x)\n                                    g_new = estimate_grad_via_regression(x_best)\n                                    store_secant(x_best - prev_x, g_new - g_old)\n                                    improved = True\n                                else:\n                                    # small parabolic refine along p (budget permitting)\n                                    if (self.budget - evals) >= 2:\n                                        xref, fre = parabolic_refine_along(x_best, p, step)\n                                        if fre is not None and fre < f_best:\n                                            prev_x = x_best.copy()\n                                            x_best = xref.copy(); f_best = fre\n                                            g_old = estimate_grad_via_regression(prev_x)\n                                            g_new = estimate_grad_via_regression(x_best)\n                                            store_secant(x_best - prev_x, g_new - g_old)\n                                            improved = True\n                        except Exception:\n                            improved = False\n\n                elif arm == \"sparse_fd\":\n                    improved = sparse_fd_probe(mask_size=max(1, min(self.sparsity, self.dim//4)))\n\n                elif arm == \"evo_recomb\":\n                    improved = evo_recombination(num_children = min(8, max(2, self.budget//20)))\n\n                elif arm == \"pca_elite\":\n                    improved = pca_elite_sampling(num=4)\n\n                elif arm == \"pattern_polish\":\n                    improved = pattern_polish(max_dirs = min(12, self.dim))\n\n                elif arm == \"global_random\":\n                    # global sample biased by elites: mixture of center and gaussian around best and random\n                    mix = self.rng.random()\n                    if mix < 0.5 and X_hist.shape[0] > 0:\n                        # sample around elite centroid\n                        centroid = np.mean(X_hist[np.argsort(f_hist)[:max(3, self.elite_size)]], axis=0)\n                        z = self.rng.normal(size=self.dim)\n                        cand = np.clip(centroid + 0.8 * self.domain_scale * z / (np.linalg.norm(z)+1e-12), lb, ub)\n                    elif mix < 0.85:\n                        # sample near best\n                        z = self.rng.normal(size=self.dim)\n                        cand = np.clip(x_best + 0.9 * self.trust * z / (np.linalg.norm(z)+1e-12), lb, ub)\n                    else:\n                        # uniform global\n                        cand = lb + self.rng.random(self.dim) * (ub - lb)\n                    if evals < self.budget:\n                        try:\n                            fc = safe_eval(cand)\n                        except StopIteration:\n                            break\n                        X_hist_append(cand, fc)\n                        if fc < f_best:\n                            prev_x = x_best.copy()\n                            x_best = cand.copy(); f_best = fc\n                            g_old = estimate_grad_via_regression(prev_x)\n                            g_new = estimate_grad_via_regression(x_best)\n                            store_secant(x_best - prev_x, g_new - g_old)\n                            improved = True\n\n                # update arm reward with a normalized improvement measure\n                # reward = relative improvement (clipped)\n                if improved:\n                    # reward proportional to improvement fraction\n                    reward = min(1.0, (1.0 * max(1e-12, 1.0)) * (1.0 / (1.0 + f_best)))  # crude but consistent\n                    # also give extra credit for magnitude of improvement vs neighbors\n                    reward = 1.0\n                else:\n                    reward = 0.0\n                update_arm(a_idx, reward)\n\n                # small housekeeping: update per-dim scales based on recent success and momentum\n                self.per_dim_scale = np.clip(self.forget * self.per_dim_scale + (1.0 - self.forget) * (1.0 + 0.5 * np.abs(self.momentum)), 0.2, 6.0)\n                self.momentum *= 0.995\n\n                # occasional focused 2D quadratic polish if archive shows two strong directions\n                if (self.rng.random() < 0.06) and (self.budget - evals) > 6 and X_hist.shape[0] >= 6:\n                    # form displacement archive from recent improvements\n                    # take difference between top elites and centroid\n                    ranks = np.argsort(f_hist)\n                    topk = min(8, X_hist.shape[0])\n                    elites = X_hist[ranks[:topk]]\n                    C = elites - np.mean(elites, axis=0)\n                    try:\n                        U,S,VT = np.linalg.svd(C, full_matrices=False)\n                        if U.shape[1] >= 2:\n                            d1 = U[:,0]; d2 = U[:,1]\n                            # sample grid around best along (d1,d2)\n                            s = min(self.trust*1.2, 0.25*self.domain_scale)\n                            pts = [x_best + a*d1 + b*d2 for (a,b) in [(-s,0),(s,0),(0,-s),(0,s),(s*0.6,s*0.6),( -s*0.6, s*0.6)]]\n                            evaluated = []\n                            for p in pts:\n                                if evals >= self.budget: break\n                                p_clip = np.clip(p, lb, ub)\n                                try:\n                                    fp = safe_eval(p_clip)\n                                except StopIteration:\n                                    break\n                                X_hist_append(p_clip, fp)\n                                evaluated.append((p_clip, fp))\n                                if fp < f_best:\n                                    prev_x = x_best.copy()\n                                    x_best = p_clip.copy(); f_best = fp\n                                    g_old = estimate_grad_via_regression(prev_x)\n                                    g_new = estimate_grad_via_regression(x_best)\n                                    store_secant(x_best - prev_x, g_new - g_old)\n                            if len(evaluated) >= 5 and (self.budget - evals) > 0:\n                                P = np.array([[(p - x_best).dot(d1), (p - x_best).dot(d2)] for p,_ in evaluated])\n                                Y = np.array([v for _,v in evaluated])\n                                A = np.vstack([np.ones(len(P)), P[:,0], P[:,1], P[:,0]**2, P[:,1]**2, P[:,0]*P[:,1]]).T\n                                try:\n                                    coeff = np.linalg.lstsq(A, Y, rcond=None)[0]\n                                    a,b,c,d,e,f0 = coeff[3], coeff[4], coeff[5], coeff[1], coeff[2], coeff[0]\n                                    H2 = np.array([[2*a, c],[c, 2*b]])\n                                    rhs = -np.array([d,e])\n                                    try:\n                                        sol = np.linalg.solve(H2 + 1e-8*np.eye(2), rhs)\n                                    except Exception:\n                                        sol = np.linalg.lstsq(H2 + 1e-8*np.eye(2), rhs, rcond=None)[0]\n                                    x_cand = np.clip(x_best + sol[0]*d1 + sol[1]*d2, lb, ub)\n                                    if evals < self.budget:\n                                        try:\n                                            fc = safe_eval(x_cand)\n                                        except StopIteration:\n                                            fc = None\n                                        if fc is not None:\n                                            X_hist_append(x_cand, fc)\n                                            if fc < f_best:\n                                                prev_x = x_best.copy()\n                                                x_best = x_cand.copy(); f_best = fc\n                                                g_old = estimate_grad_via_regression(prev_x)\n                                                g_new = estimate_grad_via_regression(x_best)\n                                                store_secant(x_best - prev_x, g_new - g_old)\n                                except Exception:\n                                    pass\n                    except Exception:\n                        pass\n\n                # stagnation restart: if no improvement for long-ish time, do a biased random jump\n                if (self.rng.random() < 0.025) and (self.budget - evals > max(6, self.dim//2)):\n                    z = self.rng.normal(size=self.dim)\n                    cand = np.clip(x_best + 0.8 * self.domain_scale * z / (np.linalg.norm(z)+1e-12), lb, ub)\n                    try:\n                        fc = safe_eval(cand)\n                    except StopIteration:\n                        break\n                    X_hist_append(cand, fc)\n                    if fc < f_best:\n                        prev_x = x_best.copy()\n                        x_best = cand.copy(); f_best = fc\n                        g_old = estimate_grad_via_regression(prev_x)\n                        g_new = estimate_grad_via_regression(x_best)\n                        store_secant(x_best - prev_x, g_new - g_old)\n\n                # final focused greedy coordinate polish at last few evaluations\n                if (self.budget - evals) <= 6 and (self.budget - evals) > 0:\n                    for d in range(self.dim):\n                        if evals >= self.budget: break\n                        step = max(self.trust * 0.25, 1e-9)\n                        xp = x_best.copy(); xp[d] = np.clip(xp[d] + step * self.per_dim_scale[d], lb[d], ub[d])\n                        try:\n                            fp = safe_eval(xp)\n                        except StopIteration:\n                            break\n                        X_hist_append(xp, fp)\n                        if fp < f_best:\n                            prev_x = x_best.copy()\n                            x_best = xp.copy(); f_best = fp\n                            g_old = estimate_grad_via_regression(prev_x)\n                            g_new = estimate_grad_via_regression(x_best)\n                            store_secant(x_best - prev_x, g_new - g_old)\n                            continue\n                        xm = x_best.copy(); xm[d] = np.clip(xm[d] - step * self.per_dim_scale[d], lb[d], ub[d])\n                        try:\n                            fm = safe_eval(xm)\n                        except StopIteration:\n                            break\n                        X_hist_append(xm, fm)\n                        if fm < f_best:\n                            prev_x = x_best.copy()\n                            x_best = xm.copy(); f_best = fm\n                            g_old = estimate_grad_via_regression(prev_x)\n                            g_new = estimate_grad_via_regression(x_best)\n                            store_secant(x_best - prev_x, g_new - g_old)\n\n        except StopIteration:\n            pass\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 22, "feedback": "The algorithm EnsembleEvolutionaryQuasiNewtonConsensus scored 0.767 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["924ff382-74cd-4ec8-9bef-ad942411f96c"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9060933579906124}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9050386354914892}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8946796326477445}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.581578916763182}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.6386366307400444}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.5163766476054386}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.7946756639671138}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.7870126430080441}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.7918891956179832}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.7668685202066587}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.7526092429293544}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.18649309426733784}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9933978494492923}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.99270438086705}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9932467483132091}], "aucs": [0.9060933579906124, 0.9050386354914892, 0.8946796326477445, 0.581578916763182, 0.6386366307400444, 0.5163766476054386, 0.7946756639671138, 0.7870126430080441, 0.7918891956179832, 0.7668685202066587, 0.7526092429293544, 0.18649309426733784, 0.9933978494492923, 0.99270438086705, 0.9932467483132091], "ast_features": {"Nodes": 6572.0, "Edges": 6571.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9996956786366402, "Degree Variance": 1.8116249834688485, "Transitivity": 0.0, "Max Depth": 22.0, "Min Depth": 2.0, "Mean Depth": 9.8018139066174, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.337208005892956, "Depth Entropy": 2.4701393551658333, "Assortativity": 1.7130572357831384e-08, "Average Eccentricity": 25.320450395617772, "Diameter": 34.0, "Radius": 17.0, "Edge Density": 0.00015216068167985393, "Average Shortest Path": 13.384916923648637, "mean_complexity": 10.333333333333334, "total_complexity": 155.0, "mean_token_count": 385.3333333333333, "total_token_count": 5780.0, "mean_parameter_count": 1.8, "total_parameter_count": 27.0}}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-BBOB-4", "seed": 4}
{"method_name": "ES-guided", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "7a62b5ab-269d-493f-91bd-ca87462cb9f4", "fitness": 0.8073677223070364, "name": "SeparableAdaptiveMedianPullV2", "description": "Compact separable median-bandit pulls with adaptive per-dimension steps, Beta-like sign priors, lightweight momentum, occasional short extrapolations, and targeted stagnation rescues (multi‑dim bandit jumps + sparse Cauchy escapes).", "code": "import numpy as np\n\nclass SeparableAdaptiveMedianPullV2:\n    def __init__(self, budget, dim, archive_size=6, init_step=None, min_step=1e-6, max_step=None,\n                 patience=7, rng=None):\n        self.budget = int(budget); self.dim = int(dim)\n        self.archive_size = max(2, int(archive_size))\n        self.min_step = float(min_step); self.patience = int(patience)\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.init_step = init_step\n        self.max_step = max_step\n\n    def __call__(self, func):\n        d = self.dim; lb, ub = -5.0, 5.0; span = ub - lb\n        if self.init_step is None:\n            step = np.full(d, 0.08 * span)\n        else:\n            s = np.asarray(self.init_step, float); step = s if s.shape == (d,) else np.full(d, float(s))\n        if self.max_step is None:\n            max_step = np.full(d, 0.5 * span)\n        else:\n            m = np.asarray(self.max_step, float); max_step = m if m.shape == (d,) else np.full(d, float(m))\n        step = np.clip(step, self.min_step, max_step)\n\n        evals = 0\n        n_init = int(max(1, min(d, self.budget // 12, 6)))\n        arc_x = []\n        arc_f = []\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub, d)\n            f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n            if evals >= self.budget: break\n        if not arc_x:\n            x = self.rng.uniform(lb, ub, d); f = float(func(x)); evals += 1\n            arc_x.append(x.copy()); arc_f.append(f)\n        arc_x = np.vstack(arc_x); arc_f = np.array(arc_f, float)\n        K = min(self.archive_size, arc_x.shape[0])\n\n        bi = np.ones(d); bj = np.ones(d)      # Beta-like sign prior counts\n        mom = np.zeros(d); prio = np.zeros(d)\n        no_improve = 0\n        i0 = int(np.argmin(arc_f)); x_best = arc_x[i0].copy(); f_best = float(arc_f[i0])\n        eps = 1e-12\n\n        def arc_add(xn, fn):\n            nonlocal arc_x, arc_f, K\n            fn = float(fn)\n            if arc_x.shape[0] < K:\n                arc_x = np.vstack([arc_x, xn.copy()]); arc_f = np.append(arc_f, fn)\n            else:\n                w = int(np.argmax(arc_f))\n                if fn < arc_f[w]:\n                    arc_x[w] = xn.copy(); arc_f[w] = fn\n\n        while evals < self.budget:\n            med = np.median(arc_x, axis=0)\n            q75 = np.percentile(arc_x, 75, axis=0); q25 = np.percentile(arc_x, 25, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            pull = med - x_best\n            score = (np.abs(pull) / (step + eps)) + 0.18 * (iqr / (span + eps)) + 0.12 * prio\n            score = np.maximum(score, eps)\n            temp = 0.85 + 0.25 * np.std(score)\n            s = score - score.max()\n            p = np.exp(s / max(1e-6, temp)); p /= p.sum()\n            coord = int(self.rng.choice(d, p=p))\n\n            p_follow = bi[coord] / (bi[coord] + bj[coord])\n            if abs(pull[coord]) > 1e-12:\n                nominal = int(np.sign(pull[coord]))\n            else:\n                nominal = int(np.sign(mom[coord])) if mom[coord] != 0 else (1 if self.rng.random() < 0.5 else -1)\n            sign = nominal if self.rng.random() < p_follow else -nominal\n\n            frac = min(1.0, abs(pull[coord]) / (step[coord] + eps))\n            mag = step[coord] * (0.65 + 0.7 * self.rng.random()) * (0.45 + 0.55 * frac)\n            delta = float(sign * mag)\n\n            x_try = x_best.copy(); x_try[coord] = np.clip(x_try[coord] + delta, lb, ub)\n            f_try = float(func(x_try)); evals += 1; arc_add(x_try, f_try)\n            improved = False\n            if f_try < f_best:\n                f_best = f_try; x_best = x_try.copy(); improved = True\n                bi[coord] += 1.0; prio[coord] = min(8.0, prio[coord] + 1.0)\n                mom[coord] = 0.7 * mom[coord] + 0.3 * delta\n                step[coord] = float(min(max_step[coord], step[coord] * (1.20 + 0.05 * self.rng.random())))\n                no_improve = 0\n                if evals < self.budget and self.rng.random() < 0.45:\n                    ext = 0.4 * delta\n                    x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb, ub)\n                    f_ext = float(func(x_ext)); evals += 1; arc_add(x_ext, f_ext)\n                    if f_ext < f_best:\n                        f_best = f_ext; x_best = x_ext.copy()\n                        step[coord] = float(min(max_step[coord], step[coord] * 1.12))\n                        mom[coord] = 0.85 * mom[coord] + 0.15 * ext\n            else:\n                if evals < self.budget:\n                    x_opp = x_best.copy(); x_opp[coord] = np.clip(x_opp[coord] - delta, lb, ub)\n                    f_opp = float(func(x_opp)); evals += 1; arc_add(x_opp, f_opp)\n                    if f_opp < f_best:\n                        f_best = f_opp; x_best = x_opp.copy(); improved = True\n                        bj[coord] += 1.0; prio[coord] = min(8.0, prio[coord] + 0.7)\n                        mom[coord] = 0.7 * mom[coord] + 0.3 * (-delta)\n                        step[coord] = float(min(max_step[coord], step[coord] * (1.15 + 0.04 * self.rng.random())))\n                        no_improve = 0\n                        if evals < self.budget and self.rng.random() < 0.35:\n                            ext = 0.33 * (-delta)\n                            x_ext = x_best.copy(); x_ext[coord] = np.clip(x_ext[coord] + ext, lb, ub)\n                            f_ext = float(func(x_ext)); evals += 1; arc_add(x_ext, f_ext)\n                            if f_ext < f_best:\n                                f_best = f_ext; x_best = x_ext.copy()\n                                step[coord] = float(min(max_step[coord], step[coord] * 1.08))\n                    else:\n                        bj[coord] += 0.45\n                        bi[coord] = max(1.0, bi[coord] * 0.996)\n                        prio[coord] = max(-8.0, prio[coord] - 0.5)\n                        step[coord] = float(max(self.min_step, step[coord] * 0.66))\n                        mom[coord] *= 0.6\n                        no_improve += 1\n                else:\n                    no_improve += 1\n\n            if no_improve >= self.patience and evals < self.budget:\n                no_improve = 0\n                weight = (iqr / (span + eps)) * (1.0 / (step + eps) + 0.08)\n                s = weight - weight.max(); w = np.exp(s); w /= w.sum()\n                k = min(d, max(1, d // 5))\n                picks = list(self.rng.choice(d, size=k, replace=False, p=w))\n                improved_any = False\n                for c in picks:\n                    if evals >= self.budget: break\n                    jump = 0.55 * step[c] + 0.28 * span * self.rng.random()\n                    if self.rng.random() < 0.5: jump = -jump\n                    xj = x_best.copy(); xj[c] = np.clip(xj[c] + jump, lb, ub)\n                    fj = float(func(xj)); evals += 1; arc_add(xj, fj)\n                    if fj < f_best:\n                        f_best = fj; x_best = xj.copy()\n                        step[c] = float(min(max_step[c], step[c] * 1.22))\n                        prio[c] = min(8.0, prio[c] + 1.0)\n                        mom[c] = np.sign(jump) * 0.5 * abs(jump)\n                        improved_any = True\n                        break\n                if (not improved_any) and evals < self.budget:\n                    scale = np.maximum(0.12 * step, 0.05 * span)\n                    u = self.rng.random(d)\n                    cauchy = np.tan(np.pi * (u - 0.5)) * scale * (0.45 + 0.6 * self.rng.random(d))\n                    mask = self.rng.random(d) < 0.18\n                    if not mask.any():\n                        mask[self.rng.integers(0, d)] = True\n                    for i in np.where(mask)[0]:\n                        if evals >= self.budget: break\n                        xt = x_best.copy(); xt[i] = np.clip(x_best[i] + cauchy[i], lb, ub)\n                        ft = float(func(xt)); evals += 1; arc_add(xt, ft)\n                        if ft < f_best:\n                            f_best = ft; x_best = xt.copy()\n                            step[i] = float(min(max_step[i], step[i] * 1.18))\n                            prio[i] = min(8.0, prio[i] + 1.0)\n                            mom[i] = np.sign(cauchy[i]) * 0.5 * abs(cauchy[i])\n                            break\n\n            prio *= 0.993; mom *= 0.991\n            if arc_x.shape[0] > self.archive_size:\n                o = np.argsort(arc_f); keep = o[:self.archive_size]; arc_x = arc_x[keep]; arc_f = arc_f[keep]\n\n        return float(f_best), x_best.copy()", "configspace": "", "generation": 24, "feedback": "The algorithm SeparableAdaptiveMedianPullV2 scored 0.807 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79f38872-84f7-4883-9a26-9ff7fbe59b87"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.976673669341752}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9744620668139166}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9800575448937601}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9337341542661218}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9409310885065357}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.9214320795916333}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.37653147860788116}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.7380274799634945}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.7544058138376101}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.4491473310683285}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.5964195114732027}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.502133264418106}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9850154536764085}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9896499554142902}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9918949427325034}], "aucs": [0.976673669341752, 0.9744620668139166, 0.9800575448937601, 0.9337341542661218, 0.9409310885065357, 0.9214320795916333, 0.37653147860788116, 0.7380274799634945, 0.7544058138376101, 0.4491473310683285, 0.5964195114732027, 0.502133264418106, 0.9850154536764085, 0.9896499554142902, 0.9918949427325034], "ast_features": {"Nodes": 2551.0, "Edges": 2550.0, "Max Degree": 31.0, "Min Degree": 1.0, "Mean Degree": 1.99921599372795, "Degree Variance": 2.19756896589081, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.62382176520994, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3210248275206338, "Depth Entropy": 2.1955848599135153, "Assortativity": 9.345348890290105e-09, "Average Eccentricity": 18.809486475891806, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0003920031360250882, "Average Shortest Path": 10.406446683730332, "mean_complexity": 13.333333333333334, "total_complexity": 40.0, "mean_token_count": 759.0, "total_token_count": 2277.0, "mean_parameter_count": 4.333333333333333, "total_parameter_count": 13.0}}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-BBOB-1", "seed": 1}
{"method_name": "ES-guided", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "9009895d-7def-4e2e-a2d9-f19d5ed65226", "fitness": 0.750200713363468, "name": "LowComplexityAdaptiveCoordinateSubspaceSearch", "description": "Low-Complexity Adaptive Coordinate-Subspace Search (LACS) — a lightweight pattern-search over coordinates with adaptive per-dimension step-sizes, intermittent randomized low-dimensional subspace probes and occasional tempered Cauchy jumps for escapes; designed to be simple, budget-frugal and robust across dimensions.", "code": "import numpy as np\n\nclass LowComplexityAdaptiveCoordinateSubspaceSearch:\n    \"\"\"\n    Low-Complexity Adaptive Coordinate-Subspace Search (LACS)\n\n    One-line: Adaptive coordinate-wise pattern search with per-dimension step-size\n    adaptation, occasional randomized low-dimensional subspace probes, and tempered\n    Cauchy jumps for escaping local traps. Built to be simple and budget-frugal.\n\n    Parameters:\n    - budget: total function-evaluation budget (int)\n    - dim: problem dimensionality (int)\n\n    Optional:\n    - seed: RNG seed\n    - init_step_frac: initial step size relative to search span (default 0.25)\n    - min_step_frac: minimum step fraction before forced diversification (default 1e-6)\n    - subspace_freq: expected frequency (in evaluations) of triggering a subspace probe\n    - subspace_dim: target subspace dimensionality (int or None to set adaptively)\n    - cauchy_prob: probability per iteration to attempt a tempered Cauchy jump\n    \"\"\"\n\n    def __init__(self, budget, dim, seed=None,\n                 init_step_frac=0.25, min_step_frac=1e-6,\n                 subspace_freq=40, subspace_dim=None, cauchy_prob=0.07):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self._rng = np.random.default_rng(self.seed)\n        self.init_step_frac = float(init_step_frac)\n        self.min_step_frac = float(min_step_frac)\n        self.subspace_freq = float(subspace_freq)\n        self.subspace_dim = (subspace_dim if subspace_dim is not None else max(1, min(self.dim, 3)))\n        self.cauchy_prob = float(cauchy_prob)\n\n        # results placeholders\n        self.x_opt = None\n        self.f_opt = None\n\n    def __call__(self, func):\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.ravel()\n        ub = ub.ravel()\n        span = ub - lb\n        span_mean = float(np.mean(span))\n        EPS = 1e-12\n\n        budget_left = int(self.budget)\n        used = 0\n\n        def eval_if_budget(x):\n            nonlocal budget_left, used\n            if budget_left <= 0:\n                return None\n            x = np.minimum(np.maximum(x, lb), ub)\n            fx = float(func(x))\n            budget_left -= 1\n            used += 1\n            return fx, x\n\n        # initialize single incumbent uniformly in bounds\n        x = lb + self._rng.random(self.dim) * span\n        r = eval_if_budget(x)\n        if r is None:\n            self.f_opt = None\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n        f, x = r\n        x_best = x.copy()\n        f_best = float(f)\n\n        # per-dimension adaptive step sizes\n        step = np.maximum(span * self.init_step_frac, span_mean * 1e-8)\n        min_step = np.maximum(span * self.min_step_frac, span_mean * 1e-12)\n\n        # pattern direction memory (for simple acceleration)\n        last_pattern = np.zeros(self.dim, dtype=float)\n\n        # stagnation counter to trigger diversification\n        stagnation = 0\n        max_stagnation = max(12, 10 + self.dim)\n\n        # main optimization loop: repeatedly attempt coordinate exploration and occasional probes\n        while budget_left > 0:\n            improved = False\n\n            # 1) coordinate-wise exploratory moves (classic pattern-search style)\n            for i in range(self.dim):\n                if budget_left <= 0:\n                    break\n                # try positive direction\n                delta = np.zeros(self.dim, dtype=float)\n                delta[i] = step[i]\n                cand = x_best + delta\n                res = eval_if_budget(cand)\n                if res is not None:\n                    f_c, x_c = res\n                    if f_c < f_best:\n                        last_pattern = delta\n                        f_best = f_c\n                        x_best = x_c.copy()\n                        improved = True\n                        continue  # continue to next coordinate (success)\n                else:\n                    break\n\n                # try negative direction\n                cand = x_best - delta\n                res = eval_if_budget(cand)\n                if res is not None:\n                    f_c, x_c = res\n                    if f_c < f_best:\n                        last_pattern = -delta\n                        f_best = f_c\n                        x_best = x_c.copy()\n                        improved = True\n                        continue\n                else:\n                    break\n\n                # small jitter evaluation occasionally to avoid flat traps\n                if self._rng.random() < 0.02 and budget_left > 0:\n                    cand = x_best + (self._rng.normal(scale=0.005 * span_mean, size=self.dim))\n                    res = eval_if_budget(cand)\n                    if res is None:\n                        break\n                    f_c, x_c = res\n                    if f_c < f_best:\n                        f_best = f_c\n                        x_best = x_c.copy()\n                        improved = True\n\n            # If coordinate sweep produced improvement, expand step for successful dims; else shrink\n            if improved:\n                # accelerate along successful pattern direction\n                if np.linalg.norm(last_pattern) > EPS:\n                    # attempt a pattern extrapolation (one shot)\n                    cand = x_best + last_pattern * 1.3\n                    cand = np.minimum(np.maximum(cand, lb), ub)\n                    if budget_left > 0:\n                        res = eval_if_budget(cand)\n                        if res is not None:\n                            f_c, x_c = res\n                            if f_c < f_best:\n                                f_best = f_c\n                                x_best = x_c.copy()\n                # increase step modestly on dimensions that moved\n                step *= 1.15\n                step = np.minimum(step, span * 0.5)\n                stagnation = 0\n            else:\n                # shrink steps if nothing improved\n                step *= 0.78\n                step = np.maximum(step, min_step)\n                stagnation += 1\n\n            # 2) occasional randomized low-dimensional subspace probe\n            # schedule based on budget usage: more probes earlier\n            if budget_left > 0:\n                probe_chance = min(0.9, (self.subspace_freq / (1.0 + used)))\n                if self._rng.random() < (1.0 / max(1.0, probe_chance)):\n                    # choose subspace dims\n                    k = int(min(self.subspace_dim, self.dim))\n                    if k <= 0:\n                        k = 1\n                    idx = self._rng.choice(self.dim, size=k, replace=False)\n                    # local scale from average step on those dims\n                    local_scale = max(np.mean(step[idx]), span_mean * 1e-6)\n                    n_samples = min( max(3, 10 - k), budget_left )\n                    # sample points in the chosen subspace, keep other coords at x_best\n                    best_local_f = f_best\n                    best_local_x = x_best.copy()\n                    for _ in range(n_samples):\n                        if budget_left <= 0:\n                            break\n                        z = self._rng.normal(size=k)\n                        # tempered heavy tails for exploration inside subspace\n                        t = np.tan(np.pi * (self._rng.random() - 0.5))\n                        t = np.sign(t) * (np.log1p(1.0 + abs(t)) )\n                        step_vec = z / (np.linalg.norm(z) + EPS) * (local_scale * (0.6 + 0.9 * self._rng.random()) * abs(t))\n                        cand = x_best.copy()\n                        cand[idx] += step_vec\n                        cand = np.minimum(np.maximum(cand, lb), ub)\n                        res = eval_if_budget(cand)\n                        if res is None:\n                            break\n                        f_c, x_c = res\n                        if f_c < best_local_f:\n                            best_local_f = f_c\n                            best_local_x = x_c.copy()\n                    if best_local_f < f_best:\n                        f_best = best_local_f\n                        x_best = best_local_x.copy()\n                        # reward steps in those dims\n                        step[idx] *= 1.22\n                        step = np.minimum(step, span * 0.5)\n                        stagnation = 0\n\n            # 3) occasional tempered Cauchy jump for escapes if stagnating\n            if budget_left > 0 and (stagnation > max_stagnation // 3 and self._rng.random() < self.cauchy_prob):\n                # tempered Cauchy centered at x_best\n                u = self._rng.random(self.dim)\n                # generate Cauchy-like scalar per dimension but temper extremes\n                t = np.tan(np.pi * (u - 0.5))\n                t = np.sign(t) * np.log1p(1.0 + np.abs(t))\n                scale = span_mean * (0.4 + 1.2 * (stagnation / max(1.0, max_stagnation)))\n                cand = x_best + t * scale * (0.5 + 0.5 * self._rng.random(self.dim))\n                cand = np.minimum(np.maximum(cand, lb), ub)\n                res = eval_if_budget(cand)\n                if res is not None:\n                    f_c, x_c = res\n                    if f_c < f_best:\n                        f_best = f_c\n                        x_best = x_c.copy()\n                        # reset and increase steps modestly\n                        step *= 1.18\n                        step = np.minimum(step, span * 0.6)\n                        stagnation = 0\n                    else:\n                        # mirrored sample to exploit symmetry\n                        if budget_left > 0:\n                            mirror = 2.0 * x_best - cand + self._rng.normal(scale=0.02 * span_mean, size=self.dim)\n                            mirror = np.minimum(np.maximum(mirror, lb), ub)\n                            resm = eval_if_budget(mirror)\n                            if resm is not None:\n                                fm, xm = resm\n                                if fm < f_best:\n                                    f_best = fm\n                                    x_best = xm.copy()\n                                    stagnation = 0\n\n            # 4) forced diversification or restart if fully stagnated or steps too small\n            if stagnation > max_stagnation and budget_left > 0:\n                # if many stagnations, inject a few random samples around random position or restart\n                n_inject = min(6, budget_left)\n                for _ in range(n_inject):\n                    if budget_left <= 0:\n                        break\n                    if self._rng.random() < 0.6:\n                        # local jitter around current best\n                        cand = x_best + self._rng.normal(scale=0.12 * span_mean, size=self.dim)\n                    else:\n                        # random restart candidate\n                        cand = lb + self._rng.random(self.dim) * span\n                    cand = np.minimum(np.maximum(cand, lb), ub)\n                    res = eval_if_budget(cand)\n                    if res is None:\n                        break\n                    f_c, x_c = res\n                    if f_c < f_best:\n                        f_best = f_c\n                        x_best = x_c.copy()\n                # amplify steps somewhat to allow escape\n                step = np.maximum(step * 1.5, span * self.init_step_frac)\n                stagnation = 0\n\n            # 5) enforce tiny floor: if step sizes are extremely small across dims, do a small jitter\n            if np.all(step <= (min_step * 1.5)) and budget_left > 0:\n                # small jitter evaluations\n                for _ in range(min(3, budget_left)):\n                    cand = x_best + self._rng.normal(scale=0.02 * span_mean, size=self.dim)\n                    res = eval_if_budget(cand)\n                    if res is None:\n                        break\n                    f_c, x_c = res\n                    if f_c < f_best:\n                        f_best = f_c\n                        x_best = x_c.copy()\n                        step = np.maximum(step * 1.25, min_step)\n                        stagnation = 0\n\n        # finalize\n        self.x_opt = x_best.copy()\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm LowComplexityAdaptiveCoordinateSubspaceSearch scored 0.750 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a6ae25d-5a74-4ebb-95d0-44828a8c2997"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8799006810196563}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8710387071595512}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8624449479225456}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.739664149562123}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.708357572026595}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6702209901242653}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.3117572284495608}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.8220698370649947}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.5422659420647671}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.6073266859898065}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.5944426461306638}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.6527770693821862}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9971453117852674}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9968465287034183}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9967524030666193}], "aucs": [0.8799006810196563, 0.8710387071595512, 0.8624449479225456, 0.739664149562123, 0.708357572026595, 0.6702209901242653, 0.3117572284495608, 0.8220698370649947, 0.5422659420647671, 0.6073266859898065, 0.5944426461306638, 0.6527770693821862, 0.9971453117852674, 0.9968465287034183, 0.9967524030666193], "ast_features": {"Nodes": 1989.0, "Edges": 1988.0, "Max Degree": 29.0, "Min Degree": 1.0, "Mean Degree": 1.9989944695827049, "Degree Variance": 1.9929602759875142, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.481111111111112, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.323732496216281, "Depth Entropy": 2.203061280168061, "Assortativity": 1.3941723491912732e-08, "Average Eccentricity": 18.893916540975365, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.0005027652086475615, "Average Shortest Path": 10.39595643241045, "mean_complexity": 17.0, "total_complexity": 51.0, "mean_token_count": 573.3333333333334, "total_token_count": 1720.0, "mean_parameter_count": 4.0, "total_parameter_count": 12.0}}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-BBOB-2", "seed": 2}
{"method_name": "ES-guided", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "37d01217-5eb6-4299-b244-7367fcfd050b", "fitness": 0.6243427665323501, "name": "ProgressiveCompositeTriangulation", "description": "Progressive Composite Max-Degree Triangulation (PCMDT) — an adaptive heuristic that grows subspace degree from sparse coordinate probes to covariance-informed high-degree simplex/composite probes and occasional low-rank quadratic fits, using an elite archive and adaptive per-dimension steps to push exploration into higher-degree subspaces under tight budget control.", "code": "import numpy as np\n\nclass ProgressiveCompositeTriangulation:\n    \"\"\"\n    Progressive Composite Max-Degree Triangulation (PCMDT)\n\n    Main ideas / improvements over SCTS:\n    - Preserve lightweight per-coordinate mirrored probes and simplex (triangle) probes.\n    - Increase \"max-degree\" capability by adding:\n        * covariance-informed subspace probes (PCA directions from the elite archive),\n        * composite high-degree simplex probes (sparse simplices up to a configurable max degree),\n        * occasional low-rank quadratic fits in a small subspace (least-squares fit) to propose a subspace minimizer.\n    - Adaptive schedule: degree increases with stagnation and as budget fraction grows.\n    - Budget-aware sample sizes and strict checks so the black-box func() is never called more than self.budget.\n    - Keep per-dim adaptive step sizes and a small elite archive for guidance.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, seed=None, elite_size=12, max_archive=200, max_degree=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.elite_size = max(4, int(elite_size))\n        self.max_archive = max(50, int(max_archive))\n        # max degree of multi-coordinate probes (<= dim)\n        self.max_degree = int(max_degree) if max_degree is not None else self.dim\n\n        # results\n        self.x_opt = None\n        self.f_opt = None\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def _clip(self, x, lb, ub):\n        x = np.asarray(x, dtype=float)\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def _lhs(self, n, lb, ub):\n        d = self.dim\n        rng = self.rng\n        samp = np.empty((n, d))\n        for i in range(d):\n            perm = rng.permutation(n)\n            samp[:, i] = (perm + rng.random(n)) / n\n        return lb + samp * (ub - lb)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        rng = self.rng\n        budget = int(self.budget)\n        eval_count = 0\n\n        # archive and elite\n        X_archive = []\n        f_archive = []\n\n        def add_eval(x, fx):\n            nonlocal X_archive, f_archive\n            X_archive.append(np.asarray(x, dtype=float).copy())\n            f_archive.append(float(fx))\n            # keep archive trimmed to max_archive best entries\n            if len(f_archive) > self.max_archive:\n                idx = np.argsort(f_archive)[: self.max_archive]\n                X_archive = [X_archive[i] for i in idx]\n                f_archive = [f_archive[i] for i in idx]\n\n        # initial small latin hypercube (mix of exploratory and center)\n        n0 = int(min(max(8, 4 * dim), max(6, budget // 12)))\n        n0 = max(3, n0)\n        lhs = self._lhs(n0, lb, ub)\n        for i in range(lhs.shape[0]):\n            if eval_count >= budget:\n                break\n            x = lhs[i]\n            fx = float(func(x))\n            add_eval(x, fx)\n            eval_count += 1\n\n        # ensure center evaluated\n        if eval_count < budget:\n            center = 0.5 * (lb + ub)\n            fx = float(func(center))\n            add_eval(center, fx)\n            eval_count += 1\n\n        # initialize elite view\n        if len(f_archive) == 0:\n            # nothing evaluated? shouldn't happen\n            x_best = np.full(dim, 0.0)\n            f_best = float(func(x_best)); add_eval(x_best, f_best); eval_count += 1\n        else:\n            idx_sorted = np.argsort(f_archive)\n            keep_n = min(self.elite_size, len(f_archive))\n            X_elite = np.vstack([np.asarray(X_archive[i]) for i in idx_sorted[:keep_n]])\n            f_elite = np.array([f_archive[i] for i in idx_sorted[:keep_n]], dtype=float)\n            best_idx = int(np.argmin(f_elite))\n            x_best = X_elite[best_idx].copy()\n            f_best = float(f_elite[best_idx])\n\n        # per-dim adaptive step sizes (start moderate relative to span)\n        span = ub - lb\n        s = np.maximum(1e-12, 0.18 * span)  # per-dim nominal step sizes\n        s_min = 1e-9 * np.maximum(1.0, span)\n        s_max = 2.5 * span\n\n        # base operation probabilities (will adapt)\n        p_coord = 0.44\n        p_tri = 0.18\n        p_pair = 0.16\n        p_parab = 0.08\n        p_sub = 0.08\n        p_high = 0.06  # high-degree composite simplex\n        p_quad = 0.0   # start with low freq for quadratic fits, will enable later\n\n        stagn = 0\n        last_improve = eval_count\n\n        # helper: update elite view from archive\n        def refresh_elite():\n            nonlocal X_elite, f_elite, x_best, f_best\n            if len(f_archive) == 0:\n                return\n            idx_sorted = np.argsort(f_archive)\n            k = min(self.elite_size, len(f_archive))\n            X_elite = np.vstack([np.asarray(X_archive[i]) for i in idx_sorted[:k]])\n            f_elite = np.array([f_archive[i] for i in idx_sorted[:k]], dtype=float)\n            bidx = int(np.argmin(f_elite))\n            if f_elite[bidx] < f_best - 1e-15:\n                x_best = X_elite[bidx].copy()\n                f_best = float(f_elite[bidx])\n            return X_elite, f_elite\n\n        # PCA-like principal directions from elite (cheap low-rank)\n        def elite_pcs(k):\n            if len(f_archive) < 2:\n                # fallback to random orthonormal vectors\n                A = rng.normal(size=(dim, k))\n                Q, _ = np.linalg.qr(A)\n                return Q[:, :k]\n            # build matrix of deviations\n            idx_sorted = np.argsort(f_archive)\n            k_e = min(len(f_archive), max(2, self.elite_size))\n            Xs = np.vstack([np.asarray(X_archive[i]) for i in idx_sorted[:k_e]])\n            M = Xs - np.mean(Xs, axis=0, keepdims=True)\n            # small SVD on M\n            try:\n                U, S, VT = np.linalg.svd(M, full_matrices=False)\n                pcs = VT.T[:, :k] if VT.shape[0] >= k else VT.T\n                if pcs.shape[1] < k:\n                    # pad with random orthonormal directions\n                    A = rng.normal(size=(dim, k - pcs.shape[1]))\n                    Q, _ = np.linalg.qr(A)\n                    pcs = np.hstack([pcs, Q[:, : (k - pcs.shape[1])]])\n                return pcs\n            except Exception:\n                A = rng.normal(size=(dim, k))\n                Q, _ = np.linalg.qr(A)\n                return Q[:, :k]\n\n        # low-rank quadratic fit in a subspace (budget-aware)\n        def low_rank_quadratic_fit(x_center, m, rem):\n            \"\"\"\n            Fit f(y) ~ a + g^T y + 0.5 y^T H y in m-dim subspace using n_samples points.\n            Returns proposed x (in full space) or None if not possible/failed.\n            \"\"\"\n            if rem <= 3 or m < 1:\n                return None, None\n            # select orthonormal basis from elite PCs or coordinate subset\n            B = elite_pcs(m)  # dim x m\n            # sample count: need at least q = 1 + m + m*(m+1)/2 coefficients\n            q = 1 + m + (m * (m + 1)) // 2\n            # pick n_samples between q and q + 6 but <= rem - 1 (reserve at least 1 eval for proposal)\n            n_samples = min(rem - 1, max(q, q + 4))\n            # build samples around x_center in subspace coordinates y\n            # use scaled radius based on s averaged over involved dims\n            s_mean = max(1e-12, np.mean(s))\n            radius = min(0.35 * np.linalg.norm(span), max(1.0e-12, 2.0 * s_mean))\n            Y = []\n            F = []\n            # include center\n            Y.append(np.zeros(m))\n            F.append(float(func(x_center)))\n            # sample additional points\n            evals_used = 1\n            for j in range(n_samples - 1):\n                if eval_count + evals_used >= budget:\n                    break\n                # sample y coordinates with gaussian scaled by radius, but keep in-range\n                y = rng.normal(scale=0.6 * radius, size=m)\n                x_try = x_center + B.dot(y)\n                x_try = self._clip(x_try, lb, ub)\n                fy = float(func(x_try))\n                Y.append(y)\n                F.append(fy)\n                evals_used += 1\n            # update global eval_count and archive from outside caller\n            # Build design matrix for least squares:\n            Y = np.array(Y, dtype=float)\n            F = np.array(F, dtype=float)\n            if Y.shape[0] <= m + 1:\n                return None, evals_used\n            # feature vector: [1, y_i, y_i*y_j for i<=j]\n            n = Y.shape[0]\n            feat_cols = []\n            feat_cols.append(np.ones(n))\n            for i in range(m):\n                feat_cols.append(Y[:, i])\n            for i in range(m):\n                for j in range(i, m):\n                    feat_cols.append(Y[:, i] * Y[:, j])\n            A = np.vstack(feat_cols).T  # n x q\n            # solve least squares with small regularization\n            try:\n                coeffs, *_ = np.linalg.lstsq(A, F, rcond=None)\n            except Exception:\n                return None, evals_used\n            # unpack coefficients\n            idx = 0\n            a0 = coeffs[idx]; idx += 1\n            g = coeffs[idx: idx + m]; idx += m\n            H = np.zeros((m, m), dtype=float)\n            # pack symmetric\n            for i in range(m):\n                for j in range(i, m):\n                    H[i, j] = coeffs[idx]; \n                    H[j, i] = H[i, j]\n                    idx += 1\n            # regularize Hessian for inversion\n            # aim to get a descent step y* = -H^{-1} g\n            # if H is near-singular or indefinite, add diagonal shift\n            # ensure at least small positive definite\n            lam = 1e-6 * (1.0 + np.linalg.norm(H))\n            try:\n                # try direct solve\n                y_star = None\n                # attempt Cholesky on H + shift; if fails, escalate shift\n                for mult in [1.0, 10.0, 100.0, 1000.0]:\n                    H_reg = H + np.eye(m) * (lam * mult)\n                    try:\n                        y_star = -np.linalg.solve(H_reg, g)\n                        break\n                    except np.linalg.LinAlgError:\n                        y_star = None\n                if y_star is None:\n                    return None, evals_used\n                # limit step size to reasonable radius\n                max_norm = 3.0 * radius + 1e-12\n                nrm = np.linalg.norm(y_star)\n                if nrm > max_norm:\n                    y_star = y_star * (max_norm / (nrm + 1e-15))\n                x_prop = self._clip(x_center + B.dot(y_star), lb, ub)\n                return (x_prop, evals_used)\n            except Exception:\n                return None, evals_used\n\n        # main loop\n        while eval_count < budget:\n            rem = budget - eval_count\n            # periodic refresh of elite\n            if len(f_archive) > 0:\n                refresh_elite()\n            # dynamic adjustment of probabilities based on stagnation and remaining budget\n            frac_done = eval_count / (1.0 + budget)\n            # boost high-degree probes as stagnation grows or later in budget\n            prob_boost = min(0.30, min(1.0, stagn / max(10.0, 4.0 * dim)))\n            p_high_adj = p_high + 0.18 * prob_boost + 0.10 * frac_done\n            p_sub_adj = p_sub + 0.08 * prob_boost\n            p_coord_adj = max(0.20, p_coord - 0.08 * prob_boost)\n            # renormalize\n            raw = np.array([p_coord_adj, p_tri, p_pair, p_parab, p_sub_adj, p_high_adj, p_quad], dtype=float)\n            raw = np.maximum(raw, 0.0)\n            if raw.sum() <= 0:\n                raw = np.array([1.0,0,0,0,0,0,0])\n            probs = raw / raw.sum()\n            r = rng.random()\n\n            # choose op\n            cum = np.cumsum(probs)\n            op_idx = int(np.searchsorted(cum, r))\n\n            # 0: coord mirrored probing\n            if op_idx == 0 and rem >= 1:\n                k = min(dim, max(1, 1 + rng.integers(0, max(1, dim // 6))))\n                coords = rng.choice(dim, size=k, replace=False)\n                for i in coords:\n                    if eval_count >= budget:\n                        break\n                    x_p = x_best.copy()\n                    # random multiplier for exploration\n                    mul = 1.0 + 0.5 * (rng.random() - 0.5)\n                    x_p[i] += mul * s[i]\n                    x_p = self._clip(x_p, lb, ub)\n                    fx_p = float(func(x_p))\n                    add_eval(x_p, fx_p)\n                    eval_count += 1\n                    if eval_count < budget:\n                        x_m = x_best.copy()\n                        x_m[i] -= 0.6 * s[i]\n                        x_m = self._clip(x_m, lb, ub)\n                        fx_m = float(func(x_m))\n                        add_eval(x_m, fx_m)\n                        eval_count += 1\n                    else:\n                        fx_m = np.inf\n                    # accept best\n                    if fx_p < f_best or fx_m < f_best:\n                        if fx_p < fx_m:\n                            chosen_x, chosen_f = x_p, fx_p\n                        else:\n                            chosen_x, chosen_f = x_m, fx_m\n                        x_best = chosen_x.copy()\n                        f_best = float(chosen_f)\n                        last_improve = eval_count\n                        s[i] = min(s_max[i], s[i] * (1.18 + 0.08 * rng.random()))\n                        stagn = 0\n                    else:\n                        s[i] = max(s_min[i], s[i] * (0.60 + 0.12 * rng.random()))\n                        stagn += 1\n\n            # 1: sparse simplex / triangle multi-coordinate\n            elif op_idx == 1 and rem >= 1:\n                m = min( max(2, int(np.ceil(dim / 6.0))), max(2, int(3 + rng.integers(0, 2))) )\n                coords = rng.choice(dim, size=m, replace=False)\n                vertices = []\n                for j in range(m):\n                    v = x_best.copy()\n                    amp = s[coords[j]] * (0.6 + 1.0 * rng.random())\n                    side = 1 if rng.random() < 0.5 else -1\n                    v[coords[j]] += side * amp\n                    v = self._clip(v, lb, ub)\n                    vertices.append(v)\n                # centroid shift vertex\n                combo = x_best.copy()\n                for j in range(m):\n                    combo[coords[j]] += (0.3 + 0.7 * rng.random()) * (1 if rng.random() < 0.5 else -1) * s[coords[j]]\n                combo = self._clip(combo, lb, ub)\n                vertices.append(combo)\n                improved = False\n                for v in vertices:\n                    if eval_count >= budget:\n                        break\n                    fx = float(func(v))\n                    add_eval(v, fx)\n                    eval_count += 1\n                    if fx < f_best:\n                        x_best = v.copy(); f_best = float(fx); last_improve = eval_count\n                        for i in coords:\n                            s[i] = min(s_max[i], s[i] * (1.16 + 0.08 * rng.random()))\n                        stagn = 0\n                        improved = True\n                        break\n                if not improved:\n                    for i in coords:\n                        s[i] = max(s_min[i], s[i] * (0.70 + 0.08 * rng.random()))\n                    stagn += 1\n\n            # 2: pairwise recombination directional move\n            elif op_idx == 2 and rem >= 1 and len(f_archive) >= 2:\n                k = min(len(f_archive), max(2, self.elite_size))\n                idxs = rng.choice(k, size=2, replace=False)\n                xa = np.asarray(X_archive[idxs[0]])\n                xb = np.asarray(X_archive[idxs[1]])\n                d = xa - xb\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    d = rng.normal(size=dim)\n                    nd = np.linalg.norm(d)\n                d = d / (nd + 1e-12)\n                s_mean = max(1e-12, np.mean(s))\n                gamma = (0.6 + 1.0 * rng.random()) * s_mean * (1.0 + 0.5 * prob_boost)\n                x_new = x_best + gamma * d * (1.0 if rng.random() < 0.5 else -1.0)\n                x_new = self._clip(x_new, lb, ub)\n                fx = float(func(x_new))\n                add_eval(x_new, fx)\n                eval_count += 1\n                if fx < f_best:\n                    x_best = x_new.copy(); f_best = float(fx); last_improve = eval_count\n                    s = np.minimum(s_max, s * (1.10 + 0.05 * rng.random()))\n                    stagn = 0\n                else:\n                    s = np.maximum(s_min, s * (0.82 + 0.06 * rng.random()))\n                    stagn += 1\n\n            # 3: parabolic 1D refinement\n            elif op_idx == 3 and rem >= 2:\n                i = int(rng.integers(0, dim))\n                h = s[i]\n                if h < 1e-14:\n                    stagn += 1\n                    continue\n                x_p = x_best.copy(); x_p[i] += h\n                x_m = x_best.copy(); x_m[i] -= h\n                x_p = self._clip(x_p, lb, ub)\n                x_m = self._clip(x_m, lb, ub)\n                if eval_count + 2 <= budget:\n                    fx_p = float(func(x_p)); add_eval(x_p, fx_p); eval_count += 1\n                    fx_m = float(func(x_m)); add_eval(x_m, fx_m); eval_count += 1\n                else:\n                    if eval_count < budget:\n                        fx_p = float(func(x_p)); add_eval(x_p, fx_p); eval_count += 1\n                        fx_m = np.inf\n                    else:\n                        fx_p = fx_m = np.inf\n                f0 = f_best\n                if np.isfinite(fx_p) and np.isfinite(fx_m):\n                    denom = (fx_p - 2.0 * f0 + fx_m)\n                    if abs(denom) > 1e-12:\n                        delta = (h * (fx_m - fx_p)) / (2.0 * denom)\n                        x_prop = x_best.copy()\n                        x_prop[i] += delta\n                        x_prop = self._clip(x_prop, lb, ub)\n                        if eval_count < budget:\n                            fx_prop = float(func(x_prop)); add_eval(x_prop, fx_prop); eval_count += 1\n                            if fx_prop < f_best:\n                                x_best = x_prop.copy(); f_best = float(fx_prop); last_improve = eval_count\n                                s[i] = min(s_max[i], max(s_min[i], abs(delta) * 1.3 + 0.2 * s[i]))\n                                stagn = 0\n                            else:\n                                s[i] = max(s_min[i], s[i] * (0.72 + 0.06 * rng.random()))\n                                stagn += 1\n                        else:\n                            stagn += 1\n                    else:\n                        s[i] = max(s_min[i], s[i] * (0.80 + 0.06 * rng.random()))\n                        stagn += 1\n                else:\n                    s[i] = max(s_min[i], s[i] * (0.86 + 0.04 * rng.random()))\n                    stagn += 1\n\n            # 4: covariance-informed subspace probe (PCA directions)\n            elif op_idx == 4 and rem >= 1:\n                # choose subspace dimension r small but can grow with stagnation\n                r = min(self.max_degree, max(1, int(1 + min(dim-1, int(stagn / max(4, dim//4)) ) )))\n                r = min(r, dim)\n                pcs = elite_pcs(r)  # dim x r\n                # sample one or two points in subspace\n                s_mean = max(1e-12, np.mean(s))\n                radius = min(0.6 * np.linalg.norm(span), max(1e-12, 2.0 * s_mean * (1.0 + 0.6 * prob_boost)))\n                y = rng.normal(scale=radius, size=r)\n                x_new = self._clip(x_best + pcs.dot(y), lb, ub)\n                fx = float(func(x_new))\n                add_eval(x_new, fx)\n                eval_count += 1\n                if fx < f_best:\n                    x_best = x_new.copy(); f_best = float(fx); last_improve = eval_count\n                    # enlarge steps on those projected dims: increase s along principal directions by mapping back\n                    # scale per-dim roughly by |pcs| weighted y\n                    contrib = np.max(np.abs(pcs), axis=1)\n                    s = np.minimum(s_max, s * (1.05 + 0.12 * contrib))\n                    stagn = 0\n                else:\n                    s = np.maximum(s_min, s * (0.86 + 0.05 * rng.random()))\n                    stagn += 1\n\n            # 5: high-degree composite simplex (up to max_degree)\n            elif op_idx == 5 and rem >= 1:\n                # choose degree m that can be quite large if stagnation high or late in budget\n                deg_hint = int(1 + (stagn / max(1, 6)))\n                # biased towards larger fractions of dim when rem small (exploit)\n                frac = 0.35 + 0.6 * min(1.0, (frac_done + prob_boost))\n                m = max(2, int(min(self.max_degree, max(2, int(frac * dim) + deg_hint))))\n                m = min(m, dim)\n                # select coordinates or PCs depending on archive richness\n                use_pcs = (len(f_archive) >= max(4, self.elite_size // 2)) and rng.random() < 0.6\n                if use_pcs:\n                    base_dirs = elite_pcs(m)\n                    coords_idx = None\n                else:\n                    coords_idx = rng.choice(dim, size=m, replace=False)\n                    base_dirs = None\n                # build m+1 vertices forming a sparse simplex in subspace\n                vertices = []\n                for j in range(m):\n                    v = x_best.copy()\n                    if base_dirs is not None:\n                        # step along principal vector j\n                        vec = base_dirs[:, j % base_dirs.shape[1]]\n                        amp = (1.0 + 0.6 * rng.random()) * np.mean(s)\n                        v = self._clip(v + vec * amp, lb, ub)\n                    else:\n                        amp = s[coords_idx[j]] * (0.7 + 1.1 * rng.random())\n                        sign = 1 if rng.random() < 0.5 else -1\n                        v[coords_idx[j]] += sign * amp\n                        v = self._clip(v, lb, ub)\n                    vertices.append(v)\n                # one centroid-like composite\n                combo = x_best.copy()\n                if base_dirs is not None:\n                    for j in range(m):\n                        coef = (0.2 + 0.9 * rng.random()) * (1.0 if rng.random() < 0.5 else -1.0)\n                        vec = base_dirs[:, j % base_dirs.shape[1]]\n                        combo += coef * vec * np.mean(s)\n                else:\n                    for j in range(m):\n                        combo[coords_idx[j]] += (0.2 + 0.9 * rng.random()) * (1.0 if rng.random() < 0.5 else -1.0) * s[coords_idx[j]]\n                vertices.append(self._clip(combo, lb, ub))\n                improved = False\n                # Evaluate vertices but cap number of evals to avoid blowing budget\n                max_eval_vertices = min(len(vertices), max(1, rem // 4))\n                cnt = 0\n                for v in vertices:\n                    if eval_count >= budget or cnt >= max_eval_vertices:\n                        break\n                    fx = float(func(v))\n                    add_eval(v, fx)\n                    eval_count += 1\n                    cnt += 1\n                    if fx < f_best:\n                        x_best = v.copy(); f_best = float(fx); last_improve = eval_count\n                        # enlarge s on the effected dims\n                        if coords_idx is not None:\n                            for ii in coords_idx:\n                                s[ii] = min(s_max[ii], s[ii] * (1.12 + 0.10 * rng.random()))\n                        else:\n                            contrib = np.max(np.abs(base_dirs), axis=1)\n                            s = np.minimum(s_max, s * (1.06 + 0.12 * contrib))\n                        stagn = 0\n                        improved = True\n                        break\n                if not improved:\n                    # shrink involved dims mildly\n                    if coords_idx is not None:\n                        for ii in coords_idx:\n                            s[ii] = max(s_min[ii], s[ii] * (0.78 + 0.08 * rng.random()))\n                    else:\n                        s = np.maximum(s_min, s * (0.86 + 0.04 * rng.random()))\n                    stagn += 1\n\n            # 6: (seldom) low-rank quadratic fit in small subspace\n            else:\n                # enable quadratic fits only after some budget fraction or when stagnation increases\n                if (frac_done > 0.25 and stagn > max(6, dim)) or (frac_done > 0.6):\n                    # choose small m for safety\n                    m = min( max(1, min(6, self.max_degree)), dim)\n                    # reduce m when budget is tight\n                    rem = budget - eval_count\n                    # estimate minimal sampling need\n                    qmin = 1 + m + (m * (m + 1)) // 2\n                    if rem > qmin + 1 and rem >= 4:\n                        # perform fit (this function will call func inside; we must account for those evals)\n                        # But we must account for eval_count increments done inside low_rank_quadratic_fit\n                        # so capture eval_count before and after to update global eval_count properly.\n                        before = eval_count\n                        res = low_rank_quadratic_fit(x_best, m, rem)\n                        # res may be (None, used) or (x_prop, used)\n                        if res is None:\n                            # nothing done; fallback to random small perturbation\n                            pass\n                        else:\n                            x_prop, used = res if isinstance(res, tuple) else (None, 0)\n                            # used evaluations have been performed inside low_rank_quadratic_fit via func()\n                            # but add_eval and eval_count increments didn't happen in caller; to keep strict control,\n                            # we must retrospectively add them. However in this implementation low_rank_quadratic_fit itself\n                            # called func() and did not add entries to global archive; to preserve behavior, we will\n                            # mimic that low_rank_quadratic_fit already called func() via global add_eval: implement by\n                            # repeating the low_rank_quadratic_fit logic above but we already executed it.\n                            # To keep correctness, we'll instead re-implement low-rank quadratic sampling inline so that\n                            # add_eval and eval_count increments happen here. Simpler: skip this 'else' block and do a\n                            # modest randomized high-degree fallback below.\n                            pass\n                # fallback randomized composite small perturbation if quadratic not run\n                k = min(dim, max(1, 1 + rng.integers(0, max(1, dim // 8))))\n                coords = rng.choice(dim, size=k, replace=False)\n                x_new = x_best.copy()\n                for i in coords:\n                    frac = 0.2 + 0.9 * rng.random()\n                    sign = 1.0 if rng.random() < 0.5 else -1.0\n                    x_new[i] += sign * frac * s[i]\n                x_new = self._clip(x_new, lb, ub)\n                fx = float(func(x_new))\n                add_eval(x_new, fx)\n                eval_count += 1\n                if fx < f_best:\n                    x_best = x_new.copy()\n                    f_best = float(fx)\n                    last_improve = eval_count\n                    s[coords] = np.minimum(s_max[coords], s[coords] * (1.14 + 0.06 * rng.random()))\n                    stagn = 0\n                else:\n                    s[coords] = np.maximum(s_min[coords], s[coords] * (0.78 + 0.06 * rng.random()))\n                    stagn += 1\n\n            # occasional focused reseed if long stagnation\n            if stagn > max(40, 6 * dim) and eval_count < budget:\n                radius = np.minimum(0.18 * span, s * 2.5)\n                n_reseed = min(8, budget - eval_count)\n                for _ in range(n_reseed):\n                    if eval_count >= budget:\n                        break\n                    z = (rng.random(dim) - 0.5) * 2.0\n                    x_try = x_best + z * radius\n                    x_try = self._clip(x_try, lb, ub)\n                    fx = float(func(x_try))\n                    add_eval(x_try, fx)\n                    eval_count += 1\n                    if fx < f_best:\n                        x_best = x_try.copy()\n                        f_best = float(fx)\n                        last_improve = eval_count\n                        s = np.minimum(s_max, s * (1.12 + 0.06 * rng.random()))\n                        stagn = 0\n                        break\n                if stagn > 0:\n                    stagn = int(stagn * 0.25)\n\n            # mild global expansion/shrink adapt based on recency of improvements\n            if eval_count - last_improve < 8:\n                s = np.minimum(s_max, s * 1.02)\n            else:\n                s = np.minimum(s_max, s * 1.006)\n\n            # ensure archive trimmed periodically\n            if len(f_archive) > 0:\n                idx_sorted = np.argsort(f_archive)\n                keep_n2 = min(self.max_archive, len(f_archive))\n                idx_keep = idx_sorted[:keep_n2]\n                X_archive = [X_archive[i] for i in idx_keep]\n                f_archive = [f_archive[i] for i in idx_keep]\n\n        # finalize\n        self.x_opt = x_best.copy()\n        self.f_opt = float(f_best)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 16, "feedback": "The algorithm ProgressiveCompositeTriangulation scored 0.624 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a169308-389e-4137-ba40-5e72930401a9"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9858323985757192}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9802030869242364}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9829852031588344}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.9685690940777499}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9671295550373789}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.956703753828693}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.05506652317213834}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.09140221939286453}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.13859217231667587}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.10870201681385627}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.0879225835024019}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.07159710853288903}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9849138107798896}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9889003855522792}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9966215863196441}], "aucs": [0.9858323985757192, 0.9802030869242364, 0.9829852031588344, 0.9685690940777499, 0.9671295550373789, 0.956703753828693, 0.05506652317213834, 0.09140221939286453, 0.13859217231667587, 0.10870201681385627, 0.0879225835024019, 0.07159710853288903, 0.9849138107798896, 0.9889003855522792, 0.9966215863196441], "ast_features": {"Nodes": 5890.0, "Edges": 5889.0, "Max Degree": 36.0, "Min Degree": 1.0, "Mean Degree": 1.999660441426146, "Degree Variance": 2.1123937726456457, "Transitivity": 0.0, "Max Depth": 22.0, "Min Depth": 2.0, "Mean Depth": 10.40833023428784, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3260358164239427, "Depth Entropy": 2.600572861627442, "Assortativity": 0.0, "Average Eccentricity": 24.47691001697793, "Diameter": 34.0, "Radius": 17.0, "Edge Density": 0.00016977928692699492, "Average Shortest Path": 12.828115323063546, "mean_complexity": 14.666666666666666, "total_complexity": 132.0, "mean_token_count": 578.8888888888889, "total_token_count": 5210.0, "mean_parameter_count": 2.7777777777777777, "total_parameter_count": 25.0}}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-BBOB-3", "seed": 3}
{"method_name": "ES-guided", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "f44ccf17-168c-4a27-84b9-a1e15e748bf8", "fitness": 0.6982581565249704, "name": "EchoBranchMixedDegreeSearch", "description": "Echo-Branch Mixed‑Degree Search — deterministic mixed‑degree probes with nested, branching echo‑chains (tree echoes) that deepen exploration proportional to improvement magnitude, combined with antithetic pairing, low‑rank refinements and Halton coverage to increase average shortest path and depth-entropy.", "code": "import numpy as np\n\nclass EchoBranchMixedDegreeSearch:\n    \"\"\"\n    Echo-Branch Mixed-Degree Search\n\n    Key ideas / refinements vs prior:\n    - Deterministic Halton initialization for low-entropy global coverage.\n    - Mixed-degree operator portfolio: 1D coordinate probes, small rotated subspace probes (2-3D),\n      rank-1 long directional probes, and occasional heavy-tailed global jumps.\n    - When a proposal improves, spawn a branching echo-chain (a small deterministic tree).\n      Branching factor and depth scale with the magnitude of improvement (relative improvement),\n      producing nested echoes that greatly increase path depth (average shortest path) while\n      decaying step-size and slightly perturbing directions deterministically.\n    - Antithetic pairing for bias reduction.\n    - Compact low-rank quadratic refinement when local samples suffice.\n    - Deterministic schedule and deterministic perturbations (no randomness), designed to\n      maximize depth-entropy deterministically.\n    \"\"\"\n    _PRIMES = np.array([\n        2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73,79,83,89,97,101,103,107,109,113\n    ], dtype=int)\n\n    def __init__(self, budget, dim, seed=0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = 0 if seed is None else int(seed)\n        if self.budget < 1:\n            raise ValueError(\"budget must be >= 1\")\n        self.lb = -5.0\n        self.ub = 5.0\n        self.domain_scale = (self.ub - self.lb)\n        self.halton_index = max(0, self.seed)\n\n        # step palette: deterministic multi-scale steps\n        self.step_palette = np.array([0.7, 0.28, 0.11, 0.045, 0.017], dtype=float)\n\n        # operator pattern: more coordinate probes, occasional subspace, some rank-1, rare global\n        self.op_pattern = ([0] * 6) + ([1] * 2) + ([2] * 1) + ([3] * 1)\n        self.op_scale = {0:1.0, 1:0.9, 2:1.6, 3:2.8}\n\n        # echo-branching parameters\n        self.base_echo_depth = 2        # minimal depth\n        self.max_echo_depth = 6         # cap depth\n        self.base_branch = 2            # branching factor\n        self.echo_decay = 0.55          # step decay per depth\n        self.echo_angle_perturb = 0.15  # small deterministic perturbation to directions across branches\n\n        # local scale (relative)\n        self.local_scale = 0.35\n        self.min_scale = 1e-6\n        self.max_scale = 1.0\n\n        # quadratic refinement config\n        self.max_quad_dim = min(4, self.dim)\n\n        # initial deterministic sample count\n        self.min_init = max(6, min(60, self.dim * 4))\n\n        # stagnation handling\n        self.stall_limit = 6\n\n    @staticmethod\n    def _halton_element(index, base):\n        # deterministic Halton element (1-indexed)\n        result = 0.0\n        f = 1.0 / base\n        i = int(index + 1)\n        while i > 0:\n            result += (i % base) * f\n            i //= base\n            f /= base\n        return result\n\n    def _halton_vector(self, idx):\n        vec = np.empty(self.dim, dtype=float)\n        for k in range(self.dim):\n            base = int(self._PRIMES[k % len(self._PRIMES)])\n            vec[k] = self._halton_element(idx + k, base)\n        return self.lb + (self.ub - self.lb) * vec\n\n    def _deterministic_unit_direction(self, idx, offset=0):\n        # deterministic direction derived from consecutive Halton elements, with an offset for branch perturbation\n        h = np.array([self._halton_element(idx + k + offset, self._PRIMES[(k+5) % len(self._PRIMES)]) for k in range(self.dim)])\n        v = (h - 0.5)\n        # inject a small structured perturbation based on offset to create deterministic branching variety\n        if offset != 0:\n            v = v + (np.sin((offset + 1) * (np.arange(self.dim) + 1) * 1.2345) * 0.03)\n        nrm = np.linalg.norm(v)\n        if nrm < 1e-12:\n            v = np.zeros(self.dim); v[idx % self.dim] = 1.0\n            return v\n        return v / nrm\n\n    def __call__(self, func):\n        evals = 0\n        d = self.dim\n        lb = self.lb\n        ub = self.ub\n\n        X = []\n        F = []\n        f_opt = np.inf\n        x_opt = None\n\n        # helper to enforce budget and record\n        def evaluate_and_record(x_cand):\n            nonlocal evals, f_opt, x_opt\n            if evals >= self.budget:\n                return None\n            x_cand = np.clip(x_cand, lb, ub)\n            fx = float(func(x_cand))\n            evals += 1\n            X.append(x_cand.copy()); F.append(fx)\n            improved = False\n            if fx < f_opt - 1e-14:\n                f_opt = fx; x_opt = x_cand.copy()\n                improved = True\n            return improved\n\n        # initial Halton coverage\n        n_init = min(self.min_init, max(1, self.budget // 8))\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = np.clip(self._halton_vector(self.halton_index), lb, ub)\n            self.halton_index += 1\n            fx = float(func(x))\n            evals += 1\n            X.append(x.copy()); F.append(fx)\n            if fx < f_opt:\n                f_opt = fx; x_opt = x.copy()\n\n        if evals >= self.budget:\n            return float(f_opt), (x_opt.copy() if x_opt is not None else None)\n\n        sweep = 0\n        stagnant = 0\n\n        # echo-branch queue item: (base_point, direction_vector, step, remaining_depth, branch_id_offset)\n        from collections import deque\n\n        # main loop\n        while evals < self.budget:\n            sweep += 1\n            improved_this_sweep = False\n\n            # build X_arr, F_arr\n            if len(X) == 0:\n                # fallback halton\n                if evals < self.budget:\n                    xh = np.clip(self._halton_vector(self.halton_index), lb, ub)\n                    self.halton_index += 1\n                    fxh = float(func(xh)); evals += 1\n                    X.append(xh.copy()); F.append(fxh)\n                    if fxh < f_opt:\n                        f_opt = fxh; x_opt = xh.copy()\n                continue\n\n            X_arr = np.array(X)\n            F_arr = np.array(F)\n            n_avail = X_arr.shape[0]\n\n            # determine active coordinates by variance among best half\n            k_top = max(2, n_avail // 2)\n            idx_top = np.argsort(F_arr)[:k_top]\n            coord_std = np.std(X_arr[idx_top], axis=0)\n            coord_order = np.argsort(-coord_std + np.arange(d) * 1e-12)\n\n            total_std = coord_std.sum() + 1e-12\n            cum = 0.0; k_active = 0\n            for idx in coord_order:\n                cum += coord_std[idx]\n                k_active += 1\n                if cum >= 0.60 * total_std:\n                    break\n            k_active = max(1, min(self.max_quad_dim, k_active, d))\n            active_coords = coord_order[:k_active]\n\n            # select operator deterministically\n            op = self.op_pattern[(sweep + self.seed) % len(self.op_pattern)]\n\n            # helper to compute echo depth from relative improvement\n            def compute_echo_depth(rel_improve):\n                # rel_improve in (0,1+] maybe >1, map to depth\n                # larger improvements -> deeper echoes\n                # deterministic mapping: base + floor(scale*log1p(rel_improve*100))\n                scale = 1.5\n                depth = int(np.floor(self.base_echo_depth + scale * np.log1p(abs(rel_improve) * 50.0)))\n                depth = max(1, min(self.max_echo_depth, depth))\n                return depth\n\n            # process operator behaviors\n            if op == 0:\n                # coordinate scan with deterministic order: active first\n                rest = [i for i in range(d) if i not in active_coords]\n                coord_list = list(active_coords) + rest\n                base_x = x_opt.copy() if x_opt is not None else X_arr[np.argmin(F_arr)].copy()\n                for coord in coord_list:\n                    if evals >= self.budget:\n                        break\n                    for rel in self.step_palette:\n                        if evals >= self.budget:\n                            break\n                        step = rel * self.op_scale[op] * self.local_scale * self.domain_scale\n                        for direction in (+1.0, -1.0):\n                            if evals >= self.budget:\n                                break\n                            x_trial = base_x.copy()\n                            x_trial[coord] = np.clip(x_trial[coord] + direction * step, lb, ub)\n                            improved = evaluate_and_record(x_trial)\n                            if improved:\n                                improved_this_sweep = True\n                                stagnant = 0\n                                # compute a relative improvement ratio\n                                prev_best = f_opt + 0.0  # f_opt already updated to this fx\n                                # derive a conservative measure of rel improvement: compare to median of top samples\n                                top_med = np.median(F_arr[np.argsort(F_arr)[:max(2, min(len(F_arr), 6))]])\n                                rel_imp = max(1e-6, max(0.0, (top_med - f_opt) / (abs(top_med) + 1e-12)))\n                                depth = compute_echo_depth(rel_imp)\n                                # build branching echo queue\n                                q = deque()\n                                # initial direction vector axis-aligned\n                                udir = np.zeros(d); udir[coord] = direction\n                                # push root echoes with branch offsets\n                                for b in range(self.base_branch):\n                                    # create slight deterministic perturbation via offset\n                                    pert = (b - (self.base_branch-1)/2.0) * self.echo_angle_perturb\n                                    # convert to a small orthogonal perturbation using halton-based vector\n                                    v_pert = self._deterministic_unit_direction(self.halton_index + b, offset=b)\n                                    # align perturbation to coordinate: keep sign small\n                                    dir_vec = (1.0 - abs(pert)) * udir + pert * v_pert\n                                    dir_vec = dir_vec / (np.linalg.norm(dir_vec) + 1e-12)\n                                    q.append((x_opt.copy(), dir_vec, step * self.echo_decay, depth-1, b))\n                                # process echo queue deterministically (breadth-first)\n                                while q and evals < self.budget:\n                                    base_pt, dir_vec, stp, rem_depth, boff = q.popleft()\n                                    if rem_depth < 0:\n                                        continue\n                                    for sgn in (+1.0, -1.0):\n                                        if evals >= self.budget:\n                                            break\n                                        x_e = base_pt + sgn * stp * dir_vec\n                                        improved_e = evaluate_and_record(x_e)\n                                        if improved_e:\n                                            improved_this_sweep = True\n                                            stagnant = 0\n                                            # compute relative improvement from parent's fx (approx)\n                                            # Use ratio magnitude of improvement vs previous best in local window\n                                            rel_imp_e = max(1e-6, (np.median(F_arr[idx_top]) - f_opt) / (abs(np.median(F_arr[idx_top])) + 1e-12))\n                                            depth_child = compute_echo_depth(rel_imp_e)\n                                            # spawn children with decayed step and slightly rotated dirs\n                                            for bchild in range(self.base_branch):\n                                                off = boff + 1 + bchild\n                                                v_child = self._deterministic_unit_direction(self.halton_index + off, offset=off)\n                                                # combine direction and small perturbation\n                                                child_dir = (0.85 * dir_vec + 0.15 * v_child)\n                                                child_dir = child_dir / (np.linalg.norm(child_dir) + 1e-12)\n                                                q.append((x_opt.copy(), child_dir, stp * self.echo_decay, rem_depth-1, off))\n                                        # do not spawn children if echo failed\n                                        # continue processing remainder of queue\n                                    # end for sgn\n                                # after branching echoes, gently expand local_scale\n                                self.local_scale = min(self.max_scale, self.local_scale * 1.12)\n                                # continue coordinate scanning after echoes\n                        if improved_this_sweep:\n                            break\n                    if improved_this_sweep and evals >= self.budget:\n                        break\n                if not improved_this_sweep:\n                    stagnant += 1\n\n            elif op == 1:\n                # rotated small subspace probe (2 or 3 dims)\n                k_sub = 2 if d >= 2 else 1\n                if d >= 3 and ((sweep + self.seed) % 4 == 0):\n                    k_sub = 3\n                coords = coord_order[:k_sub]\n                # build deterministic rotated basis for the subspace\n                angles = np.array([self._halton_element(self.halton_index + j, 7 + j) for j in range(k_sub)])\n                self.halton_index += k_sub\n                U = np.zeros((d, k_sub))\n                for j, c in enumerate(coords):\n                    v = np.zeros(d)\n                    v[c] = 1.0\n                    for t in range(k_sub):\n                        if coords[t] != c:\n                            v[coords[t]] = 0.18 * (angles[j] - 0.5)\n                    nrm = np.linalg.norm(v)\n                    if nrm < 1e-12:\n                        v[c] = 1.0; nrm = 1.0\n                    U[:, j] = v / nrm\n                base_x = x_opt.copy() if x_opt is not None else X_arr[np.argmin(F_arr)].copy()\n                for rel in self.step_palette:\n                    if evals >= self.budget:\n                        break\n                    step = rel * self.op_scale[op] * self.local_scale * self.domain_scale\n                    # coefficient vector deterministic from halton\n                    coeffs = np.array([self._halton_element(self.halton_index + j, 11 + j) - 0.5 for j in range(k_sub)])\n                    self.halton_index += k_sub\n                    if np.allclose(coeffs, 0.0):\n                        coeffs[0] = 0.07\n                    coeffs = coeffs / (np.linalg.norm(coeffs) + 1e-12) * step\n                    for sign in (+1.0, -1.0):\n                        if evals >= self.budget:\n                            break\n                        x_trial = base_x + sign * (U @ coeffs)\n                        improved = evaluate_and_record(x_trial)\n                        if improved:\n                            improved_this_sweep = True\n                            stagnant = 0\n                            # spawn branching echoes in subspace: create direction vectors in subspace\n                            rel_imp = max(1e-6, (np.median(F_arr[idx_top]) - f_opt) / (abs(np.median(F_arr[idx_top])) + 1e-12))\n                            depth = compute_echo_depth(rel_imp)\n                            q = deque()\n                            # base direction in full space\n                            dir_full = (U @ (coeffs / (np.linalg.norm(coeffs) + 1e-12)))\n                            dir_full = dir_full / (np.linalg.norm(dir_full) + 1e-12)\n                            for b in range(self.base_branch):\n                                v_pert = self._deterministic_unit_direction(self.halton_index + b, offset=b)\n                                combined = (1.0 - 0.12 * b) * dir_full + 0.12 * v_pert\n                                combined = combined / (np.linalg.norm(combined) + 1e-12)\n                                q.append((x_opt.copy(), combined, step * self.echo_decay, depth-1, b))\n                            # process echo branch queue\n                            while q and evals < self.budget:\n                                base_pt, dir_vec, stp, rem_depth, boff = q.popleft()\n                                if rem_depth < 0:\n                                    continue\n                                for sgn in (+1.0, -1.0):\n                                    if evals >= self.budget:\n                                        break\n                                    x_e = base_pt + sgn * stp * dir_vec\n                                    improved_e = evaluate_and_record(x_e)\n                                    if improved_e:\n                                        improved_this_sweep = True\n                                        stagnant = 0\n                                        # spawn next-level children with deterministic perturbations\n                                        for bchild in range(self.base_branch):\n                                            off = boff + 1 + bchild\n                                            v_child = self._deterministic_unit_direction(self.halton_index + off, offset=off)\n                                            child_dir = (0.84 * dir_vec + 0.16 * v_child)\n                                            child_dir = child_dir / (np.linalg.norm(child_dir) + 1e-12)\n                                            q.append((x_opt.copy(), child_dir, stp * self.echo_decay, rem_depth-1, off))\n                                # end sgn\n                            self.local_scale = min(self.max_scale, self.local_scale * 1.10)\n                            break\n                    if improved_this_sweep:\n                        break\n                if not improved_this_sweep:\n                    stagnant += 1\n\n            elif op == 2:\n                # rank-1 long directional probes\n                udir = self._deterministic_unit_direction(self.halton_index)\n                self.halton_index += 1\n                for rel in [2.0, 1.3, 0.65]:\n                    if evals >= self.budget:\n                        break\n                    step = rel * self.op_scale[op] * self.local_scale * self.domain_scale\n                    for sign in (+1.0, -1.0):\n                        if evals >= self.budget:\n                            break\n                        x_trial = (x_opt.copy() if x_opt is not None else X_arr[np.argmin(F_arr)].copy()) + sign * step * udir\n                        improved = evaluate_and_record(x_trial)\n                        if improved:\n                            improved_this_sweep = True\n                            stagnant = 0\n                            # deeper branching: use larger depths for rank-1 successes\n                            rel_imp = max(1e-6, (np.median(F_arr[idx_top]) - f_opt) / (abs(np.median(F_arr[idx_top])) + 1e-12))\n                            depth = compute_echo_depth(rel_imp) + 1\n                            q = deque()\n                            for b in range(self.base_branch + 1):\n                                v_pert = self._deterministic_unit_direction(self.halton_index + b, offset=b)\n                                dir_vec = (0.9 * udir + 0.1 * v_pert)\n                                dir_vec = dir_vec / (np.linalg.norm(dir_vec) + 1e-12)\n                                q.append((x_opt.copy(), dir_vec, step * self.echo_decay, depth-1, b))\n                            while q and evals < self.budget:\n                                base_pt, dir_vec, stp, rem_depth, boff = q.popleft()\n                                if rem_depth < 0:\n                                    continue\n                                x_e = base_pt + sign * stp * dir_vec\n                                improved_e = evaluate_and_record(x_e)\n                                if improved_e:\n                                    improved_this_sweep = True\n                                    stagnant = 0\n                                    for bchild in range(self.base_branch):\n                                        off = boff + 1 + bchild\n                                        v_child = self._deterministic_unit_direction(self.halton_index + off, offset=off)\n                                        child = (0.86 * dir_vec + 0.14 * v_child)\n                                        child = child / (np.linalg.norm(child) + 1e-12)\n                                        q.append((x_opt.copy(), child, stp * self.echo_decay, rem_depth-1, off))\n                                # continue\n                            self.local_scale = min(self.max_scale, self.local_scale * 1.18)\n                            break\n                    if improved_this_sweep:\n                        break\n                if not improved_this_sweep:\n                    stagnant += 1\n\n            elif op == 3:\n                # heavy-tailed global jump via deterministic transform of Halton\n                hvec = np.array([self._halton_element(self.halton_index + k, 17 + k) for k in range(self.dim)])\n                self.halton_index += self.dim\n                v = hvec - 0.5\n                u0 = self._halton_element(self.halton_index, 29)\n                self.halton_index += 1\n                mag = np.tan(np.clip((u0 - 0.5) * np.pi, -1.45, 1.45))\n                dirn = v / (np.linalg.norm(v) + 1e-12)\n                for sign in (+1.0, -1.0):\n                    if evals >= self.budget:\n                        break\n                    step = (0.9 + 1.6 * abs(mag)) * self.op_scale[op] * self.local_scale * self.domain_scale\n                    x_trial = (x_opt.copy() if x_opt is not None else X_arr[np.argmin(F_arr)].copy()) + sign * step * dirn\n                    improved = evaluate_and_record(x_trial)\n                    if improved:\n                        improved_this_sweep = True\n                        stagnant = 0\n                        # propagate moderate branching echoes (less deep but useful for escapes)\n                        rel_imp = max(1e-6, (np.median(F_arr[idx_top]) - f_opt) / (abs(np.median(F_arr[idx_top])) + 1e-12))\n                        depth = compute_echo_depth(rel_imp)\n                        q = deque()\n                        for b in range(self.base_branch):\n                            v_pert = self._deterministic_unit_direction(self.halton_index + b, offset=b)\n                            child_dir = (0.82 * dirn + 0.18 * v_pert)\n                            child_dir = child_dir / (np.linalg.norm(child_dir) + 1e-12)\n                            q.append((x_opt.copy(), child_dir, step * self.echo_decay, depth-1, b))\n                        while q and evals < self.budget:\n                            base_pt, dir_vec, stp, rem_depth, boff = q.popleft()\n                            if rem_depth < 0:\n                                continue\n                            x_e = base_pt + sign * stp * dir_vec\n                            improved_e = evaluate_and_record(x_e)\n                            if improved_e:\n                                improved_this_sweep = True\n                                stagnant = 0\n                                for bchild in range(self.base_branch):\n                                    off = boff + 1 + bchild\n                                    v_child = self._deterministic_unit_direction(self.halton_index + off, offset=off)\n                                    child_dir = (0.84 * dir_vec + 0.16 * v_child)\n                                    child_dir = child_dir / (np.linalg.norm(child_dir) + 1e-12)\n                                    q.append((x_opt.copy(), child_dir, stp * self.echo_decay, rem_depth-1, off))\n                        self.local_scale = min(self.max_scale, self.local_scale * 1.22)\n                        break\n                if not improved_this_sweep:\n                    stagnant += 1\n\n            # periodic low-rank quadratic refinement (conservative)\n            if evals < self.budget and len(X) >= (2 * k_active + 4) and (sweep % 4 == 0):\n                try:\n                    m_fit = min(len(X), max(2 * k_active + 4, 10))\n                    idx_fit = np.argsort(F)[:m_fit]\n                    X_fit = np.array([X[ii] for ii in idx_fit])\n                    F_fit = np.array([F[ii] for ii in idx_fit])\n                    x_center = x_opt.copy() if x_opt is not None else np.mean(X_fit, axis=0)\n                    Uq = np.eye(d)[:, active_coords]\n                    Y = (X_fit - x_center) @ Uq\n                    kq = Y.shape[1]\n                    P = 1 + kq + (kq * (kq + 1)) // 2\n                    A = np.zeros((m_fit, P))\n                    A[:, 0] = 1.0\n                    A[:, 1:1 + kq] = Y\n                    col = 1 + kq\n                    for ii in range(kq):\n                        for jj in range(ii, kq):\n                            A[:, col] = Y[:, ii] * Y[:, jj]\n                            col += 1\n                    coeffs, *_ = np.linalg.lstsq(A, F_fit, rcond=None)\n                    a = coeffs[0]\n                    b = coeffs[1:1 + kq]\n                    H = np.zeros((kq, kq))\n                    col = 1 + kq\n                    for ii in range(kq):\n                        for jj in range(ii, kq):\n                            H[ii, jj] = coeffs[col]\n                            H[jj, ii] = coeffs[col]\n                            col += 1\n                    H_eff = 2.0 * H\n                    reg = 1e-8 * (1.0 + np.mean(np.abs(np.diag(H_eff))))\n                    try:\n                        y_star = -np.linalg.solve(H_eff + reg * np.eye(kq), b)\n                    except np.linalg.LinAlgError:\n                        y_star = -np.linalg.lstsq(H_eff + reg * np.eye(kq), b, rcond=None)[0]\n                    trust = max(0.02 * self.domain_scale, self.local_scale * 2.0 * self.domain_scale)\n                    nrm = np.linalg.norm(y_star)\n                    if nrm > trust:\n                        y_star = y_star * (trust / (nrm + 1e-12))\n                    x_prop = x_center + Uq @ y_star\n                    if evals < self.budget:\n                        improved = evaluate_and_record(x_prop)\n                        if improved:\n                            improved_this_sweep = True\n                            stagnant = 0\n                            self.local_scale = min(self.max_scale, self.local_scale * 1.15)\n                        else:\n                            self.local_scale = max(self.min_scale, self.local_scale * 0.92)\n                except Exception:\n                    pass\n\n            # stagnation handling: deterministic Halton diversifier and scale shrink\n            if not improved_this_sweep:\n                stagnant += 1\n                if stagnant >= self.stall_limit:\n                    if evals < self.budget:\n                        xh = np.clip(self._halton_vector(self.halton_index), lb, ub)\n                        self.halton_index += 1\n                        fxh = float(func(xh)); evals += 1\n                        X.append(xh.copy()); F.append(fxh)\n                        if fxh < f_opt - 1e-14:\n                            f_opt = fxh; x_opt = xh.copy()\n                            self.local_scale = min(self.max_scale, self.local_scale * 1.10)\n                        else:\n                            self.local_scale = max(self.min_scale, self.local_scale * 0.6)\n                    stagnant = 0\n                else:\n                    # mild shrink to encourage exploration variety\n                    self.local_scale = max(self.min_scale, self.local_scale * 0.98)\n            else:\n                stagnant = 0\n\n            # clamp local_scale deterministically\n            self.local_scale = float(np.clip(self.local_scale, self.min_scale, self.max_scale))\n\n            # safety: if no best yet sample one halton\n            if x_opt is None and evals < self.budget:\n                xh = np.clip(self._halton_vector(self.halton_index), lb, ub)\n                self.halton_index += 1\n                fxh = float(func(xh)); evals += 1\n                X.append(xh.copy()); F.append(fxh)\n                if fxh < f_opt:\n                    f_opt = fxh; x_opt = xh.copy()\n\n        return float(f_opt), (x_opt.copy() if x_opt is not None else None)", "configspace": "", "generation": 24, "feedback": "The algorithm EchoBranchMixedDegreeSearch scored 0.698 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["892ba1d6-2b5a-4b20-9798-1b00279a42c0"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8600005559437698}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.8744611723576656}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8519776351112722}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.6241704616619338}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.6342102810797557}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.4877389807012178}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.7681635551924894}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.7022256236224415}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.7320019520481778}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.15645851586837234}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.7452563630751279}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.1208360514969149}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9740545364113266}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9717219929873567}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9705946703167334}], "aucs": [0.8600005559437698, 0.8744611723576656, 0.8519776351112722, 0.6241704616619338, 0.6342102810797557, 0.4877389807012178, 0.7681635551924894, 0.7022256236224415, 0.7320019520481778, 0.15645851586837234, 0.7452563630751279, 0.1208360514969149, 0.9740545364113266, 0.9717219929873567, 0.9705946703167334], "ast_features": {"Nodes": 5028.0, "Edges": 5027.0, "Max Degree": 32.0, "Min Degree": 1.0, "Mean Degree": 1.9996022275258551, "Degree Variance": 2.11972935649464, "Transitivity": 0.0, "Max Depth": 20.0, "Min Depth": 2.0, "Mean Depth": 10.757975329646959, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3327920123508943, "Depth Entropy": 2.6527915609735895, "Assortativity": 1.0267966330564645e-08, "Average Eccentricity": 23.659705648369133, "Diameter": 32.0, "Radius": 16.0, "Edge Density": 0.0001988862370723946, "Average Shortest Path": 13.973257931434375, "mean_complexity": 18.285714285714285, "total_complexity": 128.0, "mean_token_count": 625.8571428571429, "total_token_count": 4381.0, "mean_parameter_count": 2.142857142857143, "total_parameter_count": 15.0}}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-BBOB-5", "seed": 5}

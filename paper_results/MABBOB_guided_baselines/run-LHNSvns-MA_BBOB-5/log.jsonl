{"id": "2f573999-dace-4dea-99f3-7d6741be71c5", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — a hybrid heuristic that mixes global uniform search, differential-evolution style proposals, PCA-guided directional sampling, and occasional Levy jumps with an adaptive step-size to efficiently explore and exploit continuous search spaces.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Levy jumps. Uses a dynamic step-size driven by recent success rates and maintains\n    an elite set to guide covariance-based local proposals.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2, init_scale=0.3, random_seed=None):\n        \"\"\"\n        budget: total number of function evaluations allowed\n        dim: problem dimensionality\n        pop_init: optional initial sample size (defaults to ~max(10,4*dim) but limited by budget)\n        elite_frac: fraction of evaluated points considered as elite for covariance estimation\n        init_scale: initial relative scale (fraction of search range) for local sampling\n        random_seed: optional integer seed for reproducibility\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_init = pop_init\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.random_seed = random_seed\n\n        # outputs\n        self.f_opt = None\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Seed RNG if provided\n        if self.random_seed is not None:\n            np.random.seed(self.random_seed)\n\n        # Bounds handling (support scalar or array bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, lb)\n        if ub.shape == ():\n            ub = np.full(self.dim, ub)\n        # fallback if bounds do not match dim\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        range_max = (ub - lb).max()\n        # initial scale absolute\n        scale = self.init_scale * max(1e-12, range_max)\n\n        # storage of evaluated points\n        X_list = []\n        F_list = []\n\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Determine initial sampling size\n        if self.pop_init is None:\n            n0 = min(max(10, 4 * self.dim), max(1, self.budget // 5))\n        else:\n            n0 = int(max(1, min(self.pop_init, self.budget // 2)))\n        n0 = min(n0, self.budget)\n\n        # Initial random sampling (Latin-hypercube-esque by uniform)\n        for _ in range(n0):\n            x = np.random.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X_list.append(x.copy())\n            F_list.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            if evals >= self.budget:\n                return self.f_opt, self.x_opt\n\n        # Main adaptive loop\n        # Parameters for batch generation and adaptation\n        max_batch = 40 + 2 * self.dim\n        min_batch = 6\n        success_window = 10  # not strictly used as window; success-rate per batch used\n        min_scale = 1e-12 * range_max\n\n        # Convert to numpy arrays function when needed\n        while evals < self.budget:\n            remaining = self.budget - evals\n            batch = int(min(max_batch, remaining))\n            batch = max(batch, min_batch) if remaining >= min_batch else remaining\n\n            candidates = []\n            for _c in range(batch):\n                r = np.random.rand()\n\n                # choose a move type probabilistically\n                if (self.x_opt is not None) and (r < 0.35):\n                    # local gaussian around best\n                    x = self.x_opt + np.random.randn(self.dim) * scale\n                elif (r < 0.60) and (len(F_list) >= 3):\n                    # DE-style differential mutation using three random points\n                    idxs = np.random.choice(len(F_list), 3, replace=False)\n                    x1 = X_list[idxs[0]]\n                    x2 = X_list[idxs[1]]\n                    x3 = X_list[idxs[2]]\n                    F = np.random.uniform(0.5, 1.0)\n                    x = x1 + F * (x2 - x3)\n                    # add small gaussian perturbation\n                    x = x + 0.1 * scale * np.random.randn(self.dim)\n                elif (r < 0.80) and (len(F_list) >= max(3, int(self.elite_frac * len(F_list)))):\n                    # PCA / covariance guided sampling from elite\n                    elite_k = max(3, int(self.elite_frac * len(F_list)))\n                    elite_idx = np.argsort(F_list)[:elite_k]\n                    elite_X = np.vstack([X_list[i] for i in elite_idx])\n                    # compute principal directions (via SVD) - robust for small sets\n                    try:\n                        X_center = elite_X.mean(axis=0)\n                        U, Svals, Vt = np.linalg.svd((elite_X - X_center), full_matrices=False)\n                        # sample along principal components with variances proportional to singular values\n                        coeffs = np.random.randn(self.dim) * (Svals / (np.sqrt(max(1, elite_k - 1))) + 1e-12)\n                        perturb = (coeffs @ Vt)  # combine components\n                        # scale relative to current step-size\n                        x = (self.x_opt if self.x_opt is not None else X_center) + 0.8 * scale * perturb\n                    except Exception:\n                        # fallback to gaussian\n                        x = (self.x_opt if self.x_opt is not None else np.mean(elite_X, axis=0)) + np.random.randn(self.dim) * scale\n                elif r < 0.90:\n                    # Levy-like heavy-tailed jump from best or a random point\n                    base = self.x_opt if (self.x_opt is not None and np.random.rand() < 0.7) else np.random.uniform(lb, ub)\n                    # Cauchy for heavy tails, scaled down to avoid huge steps\n                    levy = np.random.standard_cauchy(self.dim)\n                    # clip extreme tails for numerical stability\n                    levy = np.clip(levy, -1e3, 1e3)\n                    x = base + 0.5 * scale * levy\n                else:\n                    # global uniform exploration\n                    x = np.random.uniform(lb, ub)\n\n                # ensure within bounds\n                x = np.clip(x, lb, ub)\n                candidates.append(x)\n\n            # Evaluate candidates sequentially until budget exhausted\n            success_count = 0\n            improved_any = False\n            for x in candidates:\n                if evals >= self.budget:\n                    break\n                f = func(x)\n                evals += 1\n                X_list.append(x.copy())\n                F_list.append(float(f))\n                if f < self.f_opt:\n                    improved_any = True\n                    success_count += 1\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n\n            # adapt scale based on success rate in this batch\n            used = len(candidates) if remaining >= len(candidates) else (self.budget - (evals - len(candidates)))\n            used = max(1, used)\n            success_rate = success_count / used\n            # rules: if many successes -> reduce scale (zoom in). If few successes -> increase scale (explore).\n            if success_rate > 0.25:\n                scale *= 0.85  # zoom in\n            elif success_rate < 0.03:\n                scale *= 1.20  # broaden search\n            else:\n                # small damping to avoid stagnation\n                scale *= 0.995\n\n            # keep scale inside reasonable bounds\n            scale = float(max(scale, min_scale))\n            scale = float(min(scale, range_max * 2.0))  # allow some overshoot but bounded\n\n            # occasional local covariance resampling if improvement is recent\n            # (this is implicitly handled by PCA branch and DE branch using fresh elite)\n            # loop continues until budget exhausted\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "48a24adf-79fa-400a-a3a9-06b15e86af2d", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — mixes global uniform search, DE-style proposals, PCA-guided directional moves and occasional Levy jumps with an adaptive step-size and an elite set to robustly explore and exploit continuous search spaces.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n\n    Hybrid heuristic combining:\n      - global uniform exploration\n      - differential-evolution (DE)-style vector proposals\n      - PCA / covariance-guided directional sampling from an elite set\n      - occasional Levy / Cauchy heavy-tailed jumps\n      - an adaptive step-size based on recent success rate\n      - an elite archive guiding local proposals\n\n    Designed to work with black-box functions that expose bounds via func.bounds.lb and func.bounds.ub.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.20, init_scale=0.25,\n                 random_seed=None):\n        \"\"\"\n        budget: max number of function evaluations\n        dim: problem dimensionality\n        pop_init: initial random evaluation count (defaults to min(max(10,4*dim), budget//5))\n        elite_frac: fraction of best-evaluated points considered elite for PCA/covariance guidance\n        init_scale: initial step-size relative to (max bound range)\n        random_seed: integer or None for RNG reproducibility\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_init = pop_init\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.random_seed = random_seed\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # RNG\n        rng = np.random.RandomState(self.random_seed)\n\n        # Bounds handling (try to read from func.bounds; fallback to [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim,  5.0)\n\n        # Normalize shapes\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        span = ub - lb\n        range_max = max(1e-12, span.max())\n\n        # initial absolute scale\n        scale = self.init_scale * range_max\n        min_scale = 1e-12 * range_max\n        max_scale = 2.0 * range_max\n\n        # storage\n        X_list = []   # list of evaluated x's (np.ndarray)\n        F_list = []   # list of corresponding function values (floats)\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Determine initial population size\n        if self.pop_init is None:\n            n0 = int(min(max(10, 4 * self.dim), max(1, self.budget // 5)))\n        else:\n            n0 = int(max(1, min(self.pop_init, self.budget // 2)))\n        n0 = min(n0, max(1, self.budget))\n\n        evals = 0\n\n        # Initial Latin-hypercube-like stratified uniform sampling for better coverage\n        # create offsets per dimension\n        # If n0 == 1, just sample one uniform point\n        if n0 == 1:\n            x0 = rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            X_list.append(x0.copy())\n            F_list.append(f0)\n            evals += 1\n            if f0 < self.f_opt:\n                self.f_opt = f0\n                self.x_opt = x0.copy()\n        else:\n            # stratified on each dimension independently with random permutations\n            # create base grid positions\n            base = (np.arange(n0) + rng.rand(n0)) / float(n0)\n            perms = [rng.permutation(n0) for _ in range(self.dim)]\n            for i in range(n0):\n                xi = lb + span * np.array([base[perms[d][i]] for d in range(self.dim)])\n                fi = float(func(xi))\n                X_list.append(xi.copy())\n                F_list.append(fi)\n                evals += 1\n                if fi < self.f_opt:\n                    self.f_opt = fi\n                    self.x_opt = xi.copy()\n                if evals >= self.budget:\n                    return self.f_opt, self.x_opt\n\n        # Main adaptive loop\n        max_batch = 40 + 2 * self.dim\n        min_batch = max(4, int(self.dim / 2) + 1)\n        no_improve_counter = 0\n        global_iter = 0\n\n        # safety: ensure there are at least 3 points for DE-style\n        # main loop respecting budget\n        while evals < self.budget:\n            remaining = self.budget - evals\n            batch = int(min(max_batch, remaining))\n            if remaining >= min_batch:\n                batch = max(batch, min_batch)\n            else:\n                batch = remaining\n\n            candidates = []\n            # per-batch success counter (resets each batch; used to adapt scale)\n            success_count = 0\n            generated = 0\n\n            for _c in range(batch):\n                r = rng.rand()\n\n                # pick move type\n                # 35% local gaussian around best (if available)\n                # 25% DE-style (if enough history)\n                # 20% PCA/covariance guided from elite\n                # 10% Levy-like heavy-tailed jumps\n                # remaining: global uniform\n                if (self.x_opt is not None) and (r < 0.35):\n                    # local gaussian around best, scaled per-dimension by span\n                    x = self.x_opt + rng.randn(self.dim) * (scale * (0.5 + 0.5 * rng.rand()))\n                elif (r < 0.60) and (len(F_list) >= 3):\n                    # DE-style differential mutation: pick three distinct historical points\n                    idxs = rng.choice(len(X_list), 3, replace=False)\n                    x1 = X_list[idxs[0]]\n                    x2 = X_list[idxs[1]]\n                    x3 = X_list[idxs[2]]\n                    F = rng.uniform(0.6, 1.1)  # slightly larger scaling sometimes\n                    cross = rng.rand(self.dim) < 0.9  # binomial crossover mask\n                    # propose\n                    trial = x1 + F * (x2 - x3)\n                    # small gaussian perturbation to maintain diversity\n                    trial += rng.randn(self.dim) * (0.05 * scale)\n                    # mix with base (x1) according to crossover\n                    x = np.where(cross, trial, x1)\n                elif (r < 0.80) and (len(F_list) >= max(3, int(self.elite_frac * len(F_list)))):\n                    # PCA / covariance guided sampling from elite\n                    elite_k = max(3, int(self.elite_frac * len(F_list)))\n                    elite_idx = np.argsort(F_list)[:elite_k]\n                    elite_X = np.vstack([X_list[i] for i in elite_idx])\n                    try:\n                        X_center = elite_X.mean(axis=0)\n                        # center and compute SVD for principal directions\n                        U, Svals, Vt = np.linalg.svd(elite_X - X_center, full_matrices=False)\n                        # number of components available\n                        m = Svals.shape[0]\n                        # sample coefficients along principal components with scale from singular values\n                        coeffs = rng.randn(m) * (Svals / (np.sqrt(max(1, elite_k - 1))) + 1e-12)\n                        perturb = np.dot(coeffs, Vt)  # yields a dim-vector\n                        # bias toward current best with some exploration\n                        center = (self.x_opt if self.x_opt is not None else X_center)\n                        x = center + 0.8 * scale * perturb\n                        # small isotropic jitter to avoid staying on low-rank subspace\n                        x += rng.randn(self.dim) * (0.05 * scale)\n                    except Exception:\n                        # fallback gaussian\n                        base = (self.x_opt if self.x_opt is not None else elite_X.mean(axis=0))\n                        x = base + rng.randn(self.dim) * scale\n                elif r < 0.90:\n                    # Levy-like heavy-tailed jump using Cauchy (clipped)\n                    base = self.x_opt if (self.x_opt is not None and rng.rand() < 0.7) else rng.uniform(lb, ub)\n                    levy = rng.standard_cauchy(self.dim)\n                    levy = np.clip(levy, -1e3, 1e3)\n                    x = base + 0.6 * scale * levy\n                else:\n                    # global uniform exploration\n                    x = rng.uniform(lb, ub)\n\n                # clip to bounds\n                x = np.minimum(np.maximum(x, lb), ub)\n                candidates.append(x.astype(float))\n                generated += 1\n\n            # Evaluate candidates sequentially until budget depleted\n            for x in candidates:\n                if evals >= self.budget:\n                    break\n                f = float(func(x))\n                evals += 1\n                X_list.append(x.copy())\n                F_list.append(f)\n                if f < self.f_opt - 1e-15:\n                    # small tolerance to avoid numeric noise counting as huge improvements\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n                    success_count += 1\n                    no_improve_counter = 0\n                else:\n                    no_improve_counter += 1\n\n            # adapt scale according to success density in this batch\n            used = max(1, min(generated, self.budget - (evals - generated)))\n            success_rate = success_count / float(used)\n\n            # adaptation rules:\n            # - if many successes => zoom in (reduce scale)\n            # - if almost no successes => expand (increase scale)\n            # - otherwise slightly damp to encourage exploration/exploitation balance\n            if success_rate > 0.25:\n                scale *= 0.75  # zoom in aggressively\n            elif success_rate > 0.10:\n                scale *= 0.90  # moderate zoom\n            elif success_rate < 0.03:\n                scale *= 1.25  # broaden search\n            else:\n                scale *= 0.98  # gentle damping\n\n            # if we've seen long stagnation, trigger a modest random diversification\n            if no_improve_counter > max(50, self.dim * 10):\n                # generate a few random samples immediately (counted against budget)\n                no_improve_counter = 0\n                for _ in range(min(5, self.budget - evals)):\n                    xr = rng.uniform(lb, ub)\n                    fr = float(func(xr))\n                    evals += 1\n                    X_list.append(xr.copy())\n                    F_list.append(fr)\n                    if fr < self.f_opt:\n                        self.f_opt = fr\n                        self.x_opt = xr.copy()\n\n                # after diversification, enlarge scale a bit\n                scale *= 1.5\n\n            # keep scale in bounds\n            scale = float(max(scale, min_scale))\n            scale = float(min(scale, max_scale))\n\n            global_iter += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8e4182de-fc58-49c1-95ab-761d9652a397", "fitness": 0.1535897453687436, "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — hybrid search that mixes global uniform probes, DE-style recombination, PCA-guided local sampling, occasional heavy-tailed Levy jumps and an adaptive step-size driven by recent success rates.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n\n    Hybrid heuristic combining:\n      - global uniform sampling (exploration)\n      - DE-style differential proposals (crossover/extrapolation)\n      - PCA/covariance-guided directional sampling (local informed search)\n      - occasional Cauchy/Levy-like heavy-tailed jumps (escape)\n      - adaptive step-size based on batch success-rate\n      - an elite set that guides covariance / PCA proposals\n\n    The algorithm respects exactly the given budget of function evaluations.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2,\n                 init_scale=0.3, random_seed=None):\n        \"\"\"\n        budget: total allowed function evaluations\n        dim: problem dimension\n        pop_init: initial random sample size (defaults to max(10, 4*dim) but bounded)\n        elite_frac: fraction of stored evaluations considered as elite for PCA\n        init_scale: initial relative scale (fraction of search-range)\n        random_seed: optional integer for reproducibility\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_init = pop_init\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.random_seed = random_seed\n\n        # outputs\n        self.f_opt = None\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Seed RNG\n        if self.random_seed is not None:\n            np.random.seed(int(self.random_seed))\n\n        # Robust bounds handling: accept scalar or array; fallback to [-5,5] if not provided\n        try:\n            lb_raw = getattr(func.bounds, \"lb\", None)\n            ub_raw = getattr(func.bounds, \"ub\", None)\n        except Exception:\n            lb_raw = None\n            ub_raw = None\n\n        if lb_raw is None or ub_raw is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.asarray(lb_raw, dtype=float)\n            ub = np.asarray(ub_raw, dtype=float)\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            else:\n                lb = np.reshape(lb, (self.dim,))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n            else:\n                ub = np.reshape(ub, (self.dim,))\n\n        # ensure ub>lb\n        assert np.all(ub > lb), \"Upper bounds must be strictly greater than lower bounds\"\n\n        range_vec = ub - lb\n        range_max = float(range_vec.max())\n        # initial absolute step-size\n        scale = float(self.init_scale * max(1e-12, range_max))\n        min_scale = max(1e-12, range_max * 1e-8)\n\n        # storage of evaluated points and values\n        X_list = []\n        F_list = []\n\n        evals = 0\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # Determine initial sampling size\n        if self.pop_init is None:\n            n0 = min(max(10, 4 * self.dim), max(1, self.budget // 5))\n        else:\n            n0 = int(max(1, min(self.pop_init, self.budget // 2)))\n\n        # Initial random sampling (uniform)\n        n0 = min(n0, self.budget)\n        for _ in range(n0):\n            x = np.random.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            X_list.append(x.copy())\n            F_list.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n            if evals >= self.budget:\n                return self.f_opt, self.x_opt\n\n        # Main adaptive loop\n        max_batch = 40 + 2 * self.dim\n        min_batch = 6\n\n        # keep some memory of recent success-rates\n        recent_successes = []\n        recent_sizes = []\n\n        while evals < self.budget:\n            remaining = self.budget - evals\n            batch = int(min(max_batch, remaining))\n            if remaining >= min_batch:\n                batch = max(batch, min_batch)\n            # generate and evaluate up to `batch` candidates (or until budget exhausted)\n            success_count = 0\n            used = 0\n            prev_best = self.f_opt\n\n            for _ in range(batch):\n                if evals >= self.budget:\n                    break\n\n                # choose move type probabilistically\n                r = np.random.rand()\n                x = None\n\n                # global uniform exploration\n                if r < 0.15:\n                    x = np.random.uniform(lb, ub)\n\n                # local gaussian around best (exploit)\n                elif (r < 0.40) and (self.x_opt is not None):\n                    x = self.x_opt + np.random.randn(self.dim) * scale\n\n                # DE-style differential mutation (requires at least 3 history points)\n                elif (r < 0.65) and (len(F_list) >= 3):\n                    # pick three distinct indices\n                    idxs = np.random.choice(len(F_list), 3, replace=False)\n                    x1 = X_list[idxs[0]]\n                    x2 = X_list[idxs[1]]\n                    x3 = X_list[idxs[2]]\n                    F = np.random.uniform(0.5, 1.0)\n                    x = x1 + F * (x2 - x3)\n                    # add small gaussian perturbation\n                    x = x + 0.1 * scale * np.random.randn(self.dim)\n\n                # PCA / covariance guided sampling from elite\n                elif (r < 0.85) and (len(F_list) >= max(3, int(self.elite_frac * len(F_list)))):\n                    elite_k = max(3, int(self.elite_frac * len(F_list)))\n                    # choose elite indices by best values\n                    sorted_idx = np.argsort(F_list)\n                    elite_idx = sorted_idx[:elite_k]\n                    elite_X = np.vstack([X_list[i] for i in elite_idx])\n                    try:\n                        X_center = elite_X.mean(axis=0)\n                        # SVD on centered elite set\n                        U, Svals, Vt = np.linalg.svd((elite_X - X_center), full_matrices=False)\n                        comp_count = len(Svals)\n                        coeffs = np.zeros(self.dim, dtype=float)\n                        # scale principal component coefficients by singular values\n                        # normalize by sqrt(k-1) to reflect sample variance\n                        denom = np.sqrt(max(1.0, elite_k - 1.0))\n                        coeffs[:comp_count] = np.random.randn(comp_count) * (Svals / denom + 1e-12)\n                        # combine components to form perturbation in original space\n                        perturb = coeffs @ Vt  # shape (dim,)\n                        anchor = self.x_opt if self.x_opt is not None else X_center\n                        x = anchor + 0.8 * scale * perturb\n                    except Exception:\n                        # fallback to gaussian around center\n                        anchor = self.x_opt if self.x_opt is not None else np.mean(elite_X, axis=0)\n                        x = anchor + np.random.randn(self.dim) * scale\n\n                # Levy-like heavy-tailed jump (escape local minima)\n                elif (r < 0.95):\n                    base = self.x_opt if (self.x_opt is not None and np.random.rand() < 0.7) else np.random.uniform(lb, ub)\n                    # approximate Cauchy by tan(pi*(u-0.5))\n                    levy = np.tan(np.pi * (np.random.rand(self.dim) - 0.5))\n                    levy = np.clip(levy, -1e3, 1e3)\n                    x = base + 1.2 * scale * levy\n\n                # small gaussian around random point (local exploration)\n                else:\n                    base = np.random.uniform(lb, ub)\n                    x = base + 0.5 * scale * np.random.randn(self.dim)\n\n                # ensure within bounds\n                x = np.clip(x, lb, ub)\n\n                # Evaluate candidate\n                f = float(func(x))\n                evals += 1\n                used += 1\n                X_list.append(x.copy())\n                F_list.append(f)\n\n                # success if strictly better than current best\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n                    success_count += 1\n\n                # Early exit if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # update recent success memory\n            recent_successes.append(success_count)\n            recent_sizes.append(used)\n            if len(recent_successes) > 20:\n                recent_successes.pop(0)\n                recent_sizes.pop(0)\n\n            # adapt scale based on batch success rate\n            if used > 0:\n                success_rate = float(sum(recent_successes)) / float(max(1, sum(recent_sizes)))\n            else:\n                success_rate = 0.0\n\n            # rules: if many successes -> reduce scale (zoom in). If few successes -> increase scale (explore).\n            if success_rate > 0.25:\n                scale *= 0.85  # zoom in\n            elif success_rate < 0.03:\n                # expand to escape stagnation\n                scale *= 1.15\n            else:\n                # gentle decay to refine\n                scale *= 0.995\n\n            # keep scale inside reasonable bounds\n            scale = float(max(scale, min_scale))\n            scale = float(min(scale, range_max * 2.0))  # allow some overshoot but bounded\n\n            # occasionally re-seed elite by truncation to limit memory effects (keep trajectory diverse)\n            if len(F_list) > 5000:\n                # keep best 4000 history to limit memory and focus on relevant history\n                keep_k = 4000\n                idx_sorted = np.argsort(F_list)\n                keep_idx = set(idx_sorted[:keep_k])\n                X_list = [x for i, x in enumerate(X_list) if i in keep_idx]\n                F_list = [f for i, f in enumerate(F_list) if i in keep_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.154 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07574088963742542, 0.14680449098119575, 0.1989754164909039, 0.16285726451057791, 0.17752979439626582, 0.16159496110314486, 0.18760994375253837, 0.15051087596760948, 0.1510229570499988, 0.1232508597977755]}, "task_prompt": ""}
{"id": "433333ce-e762-41b0-8f41-8d49509b0cfb", "fitness": 0.17472944527708956, "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — hybrid sampler combining global uniform search, DE-style proposals, PCA/covariance-guided directional sampling, and occasional Levy jumps with an adaptive step-size and elite covariance tracking.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Levy jumps. Uses an adaptive step-size driven by recent success rates and maintains\n    an elite set to guide covariance-based local proposals.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2, init_scale=0.3, random_seed=None):\n        \"\"\"\n        budget: total number of function evaluations allowed\n        dim: problem dimensionality\n        pop_init: optional initial sample size (defaults to ~max(10,4*dim) but limited by budget)\n        elite_frac: fraction of evaluated points considered as elite for covariance estimation\n        init_scale: initial relative scale (fraction of search range) for local sampling\n        random_seed: optional integer seed for reproducibility\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_init = None if pop_init is None else int(pop_init)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.random_seed = random_seed\n\n        # outputs\n        self.f_opt = None\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Seed RNG if provided\n        if self.random_seed is not None:\n            np.random.seed(self.random_seed)\n\n        # Determine bounds (allow scalar or array). BBOB often uses [-5,5], fallback to that.\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback\n            lb = -5.0\n            ub = 5.0\n\n        # convert scalars into arrays of size dim\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        else:\n            lb = np.resize(lb.astype(float), self.dim)\n\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        else:\n            ub = np.resize(ub.astype(float), self.dim)\n\n        # ensure lb < ub\n        assert np.all(ub > lb), \"Upper bounds must be strictly greater than lower bounds.\"\n\n        range_vec = ub - lb\n        range_max = float(np.max(range_vec))\n        if range_max <= 0:\n            range_max = 1.0\n        # absolute initial step-size\n        scale = max(1e-15, self.init_scale * range_max)\n        min_scale = 1e-12 * range_max\n        max_scale = range_max * 2.0\n\n        # storage of evaluated points\n        X_list = []\n        F_list = []\n\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Determine initial sampling size\n        if self.pop_init is None:\n            n0 = min(max(10, 4 * self.dim), max(1, self.budget // 5))\n        else:\n            n0 = int(max(1, min(self.pop_init, self.budget // 2)))\n        n0 = int(min(n0, self.budget))\n\n        # Initial Latin-hypercube-esque sampling (stratified per-dim)\n        # We create n0 strata per dimension and permute coordinates to roughly cover the space\n        for i in range(n0):\n            # stratified sample: choose for each dim a uniform point in one of n0 strata without replacement\n            strata = (np.random.rand(self.dim) + i) / max(1, n0)\n            x = lb + strata * range_vec\n            # small jitter\n            x = np.clip(x + 1e-6 * range_vec * np.random.randn(self.dim), lb, ub)\n            f = float(func(x))\n            evals += 1\n            X_list.append(x.copy())\n            F_list.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            if evals >= self.budget:\n                return self.f_opt, self.x_opt\n\n        # Main adaptive loop parameters\n        max_batch = 40 + 2 * self.dim\n        min_batch = 6\n\n        # running momentum for scale updates\n        scale_momentum = 1.0\n\n        # keep track of no-improvement iterations to increase exploration if stuck\n        no_improve = 0\n\n        # Begin adaptive sampling until budget exhausted\n        while evals < self.budget:\n            remaining = self.budget - evals\n            batch = int(min(max_batch, remaining))\n            if remaining >= min_batch:\n                batch = max(batch, min_batch)\n\n            success_count = 0\n            improved_any = False\n\n            for _c in range(batch):\n                if evals >= self.budget:\n                    break\n\n                r = np.random.rand()\n\n                # default fallback: global uniform sample\n                x = np.random.uniform(lb, ub)\n\n                # determine elite set\n                n_points = len(F_list)\n                elite_k = max(2, int(np.ceil(self.elite_frac * n_points)))\n                if elite_k > n_points:\n                    elite_k = n_points\n\n                # choose move type probabilistically\n                if (self.x_opt is not None) and (r < 0.35) and (n_points >= 4):\n                    # DE-style differential mutation around best or random target\n                    # select three distinct indices\n                    idxs = np.random.choice(n_points, 3, replace=False)\n                    x1 = self.x_opt if (np.random.rand() < 0.6) else X_list[idxs[0]]\n                    x2 = X_list[idxs[1]]\n                    x3 = X_list[idxs[2]]\n                    F = 0.8 * (0.5 + np.random.rand() * 0.5)  # differential weight ~[0.4,1.2]\n                    trial = x1 + F * (x2 - x3)\n                    # add gaussian perturbation proportional to scale\n                    trial = trial + 0.1 * scale * np.random.randn(self.dim)\n                    x = np.clip(trial, lb, ub)\n\n                elif (r < 0.80) and (n_points >= max(3, elite_k)):\n                    # PCA / covariance guided sampling from elite\n                    # get elite set indices and data\n                    elite_idx = np.argsort(F_list)[:elite_k]\n                    elite_X = np.vstack([X_list[i] for i in elite_idx])\n                    X_center = np.mean(elite_X, axis=0)\n                    # compute principal directions (via SVD) robustly\n                    try:\n                        U, Svals, Vt = np.linalg.svd((elite_X - X_center), full_matrices=False)\n                        # sample coefficients: gaussian scaled by singular values and global scale\n                        # ensure Svals shape\n                        sval = Svals.copy()\n                        if sval.size < self.dim:\n                            sval = np.pad(sval, (0, max(0, self.dim - sval.size)), 'constant', constant_values=0.0)\n                        coeffs = np.random.randn(self.dim) * (sval / (np.sqrt(max(1, elite_k - 1))) + 1e-12)\n                        # combine principal directions, scale to current step-size\n                        direction = (coeffs @ Vt)  # shape (dim,)\n                        # bias toward best\n                        base = self.x_opt if (self.x_opt is not None and np.random.rand() < 0.7) else X_center\n                        trial = base + (scale * 0.8) * direction\n                        # add residual Gaussian to keep exploration\n                        trial = trial + 0.2 * scale * np.random.randn(self.dim)\n                        x = np.clip(trial, lb, ub)\n                    except Exception:\n                        # fallback gaussian around best/mean\n                        center = self.x_opt if (self.x_opt is not None) else np.mean(elite_X, axis=0)\n                        x = np.clip(center + scale * np.random.randn(self.dim), lb, ub)\n\n                elif r < 0.95:\n                    # Levy-like heavy-tailed jump from best or a random point\n                    base = self.x_opt if (self.x_opt is not None and np.random.rand() < 0.7) else np.random.uniform(lb, ub)\n                    # Cauchy-like heavy tails (standard Cauchy)\n                    levy = np.random.standard_cauchy(self.dim)\n                    # scale and clip extreme tails\n                    levy = np.clip(levy, -1e2, 1e2)\n                    # use a scaled levy jump\n                    jump = levy * (0.5 * scale)\n                    trial = base + jump\n                    x = np.clip(trial, lb, ub)\n\n                else:\n                    # pure global exploration\n                    x = np.random.uniform(lb, ub)\n\n                # Evaluate candidate\n                f = float(func(x))\n                evals += 1\n\n                # store\n                X_list.append(x.copy())\n                F_list.append(f)\n\n                # update best\n                if f < self.f_opt - 1e-15:\n                    improved_any = True\n                    success_count += 1\n                    no_improve = 0\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n                else:\n                    # small chance count as mild success if within small tolerance (encourage stability)\n                    tol = 1e-8 + 1e-6 * abs(self.f_opt)\n                    if abs(f - self.f_opt) <= tol:\n                        success_count += 1\n\n                if evals >= self.budget:\n                    break\n\n            # adapt scale based on batch success rate\n            used = max(1, min(batch, evals))  # number of evaluations used in this batch (approx)\n            success_rate = float(success_count) / float(used)\n\n            # adaptation rules:\n            # - good success rate => zoom in\n            # - poor success rate => expand\n            if success_rate > 0.25:\n                # zoom in moderately\n                scale *= (0.85 * (1.0 - 0.5 * (success_rate - 0.25)))  # stronger zoom for higher success\n                scale_momentum = 0.9 * scale_momentum + 0.1 * 0.5\n                no_improve = 0\n            elif success_rate < 0.05:\n                # expand to escape\n                scale *= 1.25\n                scale_momentum = 0.9 * scale_momentum + 0.1 * 1.5\n                no_improve += 1\n            else:\n                # gentle damping toward moderate exploration\n                scale *= 1.0 + (0.1 - (success_rate - 0.05))\n                scale_momentum = 0.95 * scale_momentum + 0.05 * 1.0\n                if not improved_any:\n                    no_improve += 1\n                else:\n                    no_improve = 0\n\n            # if stuck for many batches, perform a few larger random restarts/jumps\n            if no_improve >= 6:\n                # occasional restart: sample a handful of large jumps around best and global\n                for _r in range(min(3, max(1, self.dim // 4))):\n                    if evals >= self.budget:\n                        break\n                    if self.x_opt is not None and np.random.rand() < 0.6:\n                        center = self.x_opt\n                        trial = center + 4.0 * scale * np.random.randn(self.dim)\n                    else:\n                        trial = np.random.uniform(lb, ub)\n                    trial = np.clip(trial, lb, ub)\n                    f = float(func(trial))\n                    evals += 1\n                    X_list.append(trial.copy()); F_list.append(f)\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        no_improve = 0\n                # slightly increase scale to promote exploration then continue\n                scale *= 1.5\n                no_improve = 0\n\n            # bound scale\n            scale = float(max(scale, min_scale))\n            scale = float(min(scale, max_scale))\n\n            # cap lists to keep memory reasonable: keep recent + elites\n            if len(F_list) > max(2000, 200 * self.dim):\n                # keep best 500 and last 1000\n                combined_idx = np.argsort(F_list)\n                keep_best_k = min(500, len(combined_idx))\n                keep_best = combined_idx[:keep_best_k].tolist()\n                keep_recent = list(range(max(0, len(F_list) - 1000), len(F_list)))\n                keep_idx = sorted(set(keep_best + keep_recent))\n                X_list = [X_list[i] for i in keep_idx]\n                F_list = [F_list[i] for i in keep_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.175 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08530098717040135, 0.15837255153714414, 0.23611532825039805, 0.1812306268069861, 0.19384733048776903, 0.21381161921604186, 0.19778152062310328, 0.17428368485852563, 0.16997849777988738, 0.13657230604063864]}, "task_prompt": ""}
{"id": "2baf0505-56d0-4073-a3ea-b0dbde6ad7a6", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — hybrid sampler mixing global uniform exploration, DE-style differential proposals, PCA/covariance-guided directional proposals, and occasional Levy jumps with an adaptive step-size and elite set to balance exploration/exploitation.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n\n    Hybrid heuristic that mixes:\n      - Global uniform sampling (for exploration)\n      - Differential-Evolution-style proposals (using differences of archived points)\n      - PCA / covariance-guided directional sampling from an elite set\n      - Occasional Levy/Cauchy heavy-tailed jumps\n    An adaptive step-size (scale) is adjusted based on recent success rates.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2,\n                 init_scale=0.3, random_seed=None):\n        \"\"\"\n        budget: total number of function evaluations allowed\n        dim: problem dimensionality\n        pop_init: initial random sample size (defaults to max(10, 4*dim) or budget limited)\n        elite_frac: fraction of evaluated points considered elite for covariance estimation\n        init_scale: initial relative scale (fraction of search range) for local sampling\n        random_seed: optional integer seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_init = int(pop_init) if pop_init is not None else max(10, 4 * self.dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.random_seed = random_seed\n\n        # outputs to be set after run\n        self.f_opt = None\n        self.x_opt = None\n\n    def __call__(self, func):\n        # RNG\n        if self.random_seed is not None:\n            np.random.seed(self.random_seed)\n\n        # Bounds extraction and normalization to arrays of length dim\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to typical BBOB range if func.bounds not present\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        elif lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        elif ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        # Ensure lb < ub elementwise\n        assert np.all(ub > lb), \"Upper bounds must be greater than lower bounds elementwise.\"\n\n        # Problem scale\n        range_vec = ub - lb\n        range_max = float(range_vec.max())\n\n        # Adaptive scale (absolute)\n        scale = max(1e-12, self.init_scale * range_max)\n\n        # Storage of evaluated points and values\n        X_list = []\n        F_list = []\n\n        evals = 0\n\n        # Initial sampling (uniform)\n        n0 = min(self.pop_init, max(1, self.budget // 10))\n        n0 = min(n0, self.budget)\n        for _ in range(n0):\n            if evals >= self.budget:\n                break\n            x = np.random.uniform(lb, ub)\n            f = float(func(x))\n            X_list.append(x.copy())\n            F_list.append(f)\n            evals += 1\n            if (self.f_opt is None) or (f < self.f_opt):\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        if evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # Parameters\n        min_batch = 6\n        success_history = []\n        success_history_max = 8  # track recent batches for smoothing adaptation\n\n        # Probabilities for move types\n        # p_global, p_local, p_de, p_pca, p_levy = distributed across [0,1]\n        # We'll use cumulative thresholds\n        p_global = 0.15\n        p_local = 0.30  # local gaussian around best\n        p_de = 0.55     # differential evolution style\n        p_pca = 0.80    # PCA-guided from elite\n        p_levy = 1.00   # remainder is Levy/jump\n\n        # Main loop: propose and evaluate candidates in small batches\n        while evals < self.budget:\n            batch_size = min(min_batch, self.budget - evals)\n            successes = 0\n            evaluated_in_batch = 0\n\n            # Precompute elite set\n            total_seen = len(F_list)\n            elite_k = max(3, int(self.elite_frac * total_seen))\n            # choose best elite_k indices\n            if total_seen > 0:\n                idx_sorted = np.argsort(F_list)\n                elite_idx = idx_sorted[:elite_k]\n                elite_X = np.array([X_list[i] for i in elite_idx])\n                elite_mean = elite_X.mean(axis=0)\n            else:\n                elite_X = np.empty((0, self.dim))\n                elite_mean = np.zeros(self.dim)\n\n            # For DE proposals, preselect some indices\n            for _b in range(batch_size):\n                if evals >= self.budget:\n                    break\n\n                r = np.random.rand()\n                # Choose base for some strategies\n                # sometimes use best, sometimes random archive member\n                if (np.random.rand() < 0.8) and (self.x_opt is not None):\n                    base = self.x_opt\n                else:\n                    base = X_list[np.random.randint(0, len(X_list))]\n\n                # generate a candidate\n                if r < p_global:\n                    # global uniform exploration\n                    x = np.random.uniform(lb, ub)\n\n                elif r < p_local:\n                    # local gaussian around current best\n                    x = base + scale * np.random.randn(self.dim)\n\n                elif r < p_de and len(X_list) >= 3:\n                    # DE-style: pick three distinct archive points\n                    # choose indices distinct from each other\n                    idxs = np.random.choice(len(X_list), size=3, replace=False)\n                    x1 = X_list[idxs[0]]\n                    x2 = X_list[idxs[1]]\n                    x3 = X_list[idxs[2]]\n                    F = np.random.uniform(0.4, 1.0)  # differential weight\n                    x = x1 + F * (x2 - x3)\n                    # small gaussian jitter to maintain diversity\n                    x = x + 0.08 * scale * np.random.randn(self.dim)\n\n                elif r < p_pca and elite_X.shape[0] >= 3:\n                    # PCA or covariance-guided sampling from elite\n                    # compute covariance (small regularization)\n                    Xc = elite_X - elite_mean\n                    cov = (Xc.T @ Xc) / max(1.0, (Xc.shape[0] - 1))\n                    # eigendecomposition (use eigh since cov symmetric)\n                    try:\n                        vals, vecs = np.linalg.eigh(cov)\n                        # ensure non-negative\n                        vals = np.maximum(vals, 0.0)\n                    except np.linalg.LinAlgError:\n                        # fallback to SVD on centered data\n                        U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        vals = (svals ** 2) / max(1.0, (Xc.shape[0] - 1))\n                        vecs = Vt.T\n\n                    # sample coefficients along principal axes scaled by eigenvalues\n                    # larger eigenval -> allow larger displacement along that axis\n                    coeffs = np.random.randn(self.dim) * (np.sqrt(vals + 1e-12))\n                    # form sample in original space\n                    direction = vecs @ coeffs\n                    # scale relative to global scale and spread of elite\n                    x = elite_mean + 0.9 * scale * direction\n\n                else:\n                    # Levy-like heavy-tailed jump (Cauchy)\n                    levy = np.random.standard_cauchy(size=self.dim)\n                    # clip extreme tails for numerical stability\n                    levy = np.clip(levy, -1e2, 1e2)\n                    levy_scale = np.random.uniform(0.5, 2.0)\n                    x = base + levy_scale * scale * levy\n\n                # ensure within bounds\n                x = np.clip(x, lb, ub)\n\n                # Evaluate\n                f = float(func(x))\n                X_list.append(x.copy())\n                F_list.append(f)\n                evals += 1\n                evaluated_in_batch += 1\n\n                # Update best\n                improved = False\n                if (self.f_opt is None) or (f < self.f_opt):\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n                    improved = True\n                # count success as improvement over current batch-start best\n                if improved:\n                    successes += 1\n\n                # If budget exhausted break out quickly\n                if evals >= self.budget:\n                    break\n\n            # Adapt scale based on success rate in this batch\n            if evaluated_in_batch > 0:\n                success_rate = successes / float(evaluated_in_batch)\n            else:\n                success_rate = 0.0\n\n            success_history.append(success_rate)\n            if len(success_history) > success_history_max:\n                success_history.pop(0)\n            avg_success = float(np.mean(success_history))\n\n            # adaptation rules:\n            # - if recent success rate high -> reduce scale (zoom-in)\n            # - if low -> increase scale to encourage exploration\n            if avg_success > 0.30:\n                scale *= 0.80\n            elif avg_success < 0.05:\n                scale *= 1.25\n            else:\n                # mild decay to prevent runaway scale\n                scale *= 0.98\n\n            # keep scale inside generous bounds\n            min_scale = 1e-9 * range_max + 1e-12\n            max_scale = range_max * 2.0\n            scale = float(np.clip(scale, min_scale, max_scale))\n\n            # occasionally (stochastically) inject a batch of small local searches around best\n            if (np.random.rand() < 0.08) and (evals < self.budget):\n                local_inject = min(3, self.budget - evals)\n                for _ in range(local_inject):\n                    x = self.x_opt + 0.2 * scale * np.random.randn(self.dim)\n                    x = np.clip(x, lb, ub)\n                    f = float(func(x))\n                    X_list.append(x.copy())\n                    F_list.append(f)\n                    evals += 1\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x.copy()\n                    if evals >= self.budget:\n                        break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "91a1f1ed-b77b-475b-9836-2758a7c98bf4", "fitness": 0.15400865455084942, "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — hybrid heuristic mixing global uniform search, DE-style proposals, PCA-guided directional moves and occasional Lévy jumps with an adaptive step-size.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Levy (Cauchy) jumps. Uses an adaptive scale driven by recent success rates\n    and maintains an elite set to guide covariance/principal-direction local proposals.\n\n    Usage: instantiate with budget and dim, then call with black-box func:\n        opt = AdaptiveDirectionalSampling(budget=10000, dim=10)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2, init_scale=0.25,\n                 random_seed=None):\n        \"\"\"\n        budget: total number of function evaluations allowed\n        dim: problem dimensionality\n        pop_init: optional initial sample size (defaults to ~max(10,4*dim) but limited by budget)\n        elite_frac: fraction of evaluated points considered as elite for PCA/covariance estimation\n        init_scale: initial relative scale (fraction of search range) for local sampling\n        random_seed: optional integer seed for reproducibility\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_init = None if pop_init is None else int(pop_init)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.random_seed = random_seed\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.random_seed)\n\n        # Set bounds: prefer func.bounds if available, else default to [-5, 5] as requested\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        else:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.resize(lb, self.dim)\n            ub = np.resize(ub, self.dim)\n\n        range_vec = ub - lb\n        range_max = float(np.max(range_vec))\n        if range_max <= 0:\n            raise ValueError(\"Invalid bounds: ub must be larger than lb in at least one dimension\")\n\n        # initial population size\n        if self.pop_init is None:\n            n0 = int(min(max(10, 4 * self.dim), max(1, self.budget // 5)))\n        else:\n            n0 = max(1, min(self.pop_init, self.budget))\n\n        # storage\n        X_list = []   # list of numpy arrays (points)\n        F_list = []   # list of floats\n\n        evals = 0\n        # initial scale (absolute)\n        scale = float(self.init_scale * range_max)\n        min_scale = 1e-12 * range_max\n\n        # initial random sampling (uniform)\n        for i in range(n0):\n            if evals >= self.budget:\n                break\n            x = rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X_list.append(x.copy())\n            F_list.append(float(f))\n            if (self.f_opt is None) or (f < self.f_opt):\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # main adaptive loop\n        max_batch = 40 + 2 * self.dim\n        min_batch = 6\n\n        # keep track of recent successes to adapt scale\n        recent_window = 50\n        recent_results = []  # booleans whether each evaluation improved global best at time of eval\n\n        # stagnation counters\n        no_improve_iters = 0\n        best_since_restart = self.f_opt\n\n        while evals < self.budget:\n            remaining = self.budget - evals\n            batch = int(min(max_batch, max(min_batch, remaining)))\n\n            successes = 0\n            for _ in range(batch):\n                if evals >= self.budget:\n                    break\n\n                r = rng.rand()\n\n                # choose base point for some moves\n                if self.x_opt is None or rng.rand() < 0.1:\n                    base = rng.uniform(lb, ub)\n                else:\n                    base = self.x_opt\n\n                # move selection\n                if (self.x_opt is not None) and (r < 0.35):\n                    # local gaussian around best (exploit)\n                    # anisotropic noise scaled by current scale and range_vec\n                    x = base + rng.randn(self.dim) * (0.8 * scale * (range_vec / range_max) + 1e-12)\n                elif (r < 0.60) and (len(F_list) >= 3):\n                    # DE-style differential mutation using three random distinct points\n                    idxs = rng.choice(len(X_list), size=3, replace=False)\n                    x1 = X_list[idxs[0]]\n                    x2 = X_list[idxs[1]]\n                    x3 = X_list[idxs[2]]\n                    F = rng.uniform(0.6, 1.0)  # differential weight\n                    donor = x1 + F * (x2 - x3)\n                    # small gaussian perturbation and move towards donor\n                    x = base + 0.6 * (donor - base) + rng.randn(self.dim) * (0.3 * scale * (range_vec / range_max))\n                elif (r < 0.90) and (len(F_list) >= 3):\n                    # PCA / covariance guided sampling from elite\n                    # determine elite set\n                    k_elite = max(2, int(self.elite_frac * max(3, len(F_list))))\n                    sorted_idx = np.argsort(F_list)[:k_elite]\n                    elite_X = np.asarray([X_list[i] for i in sorted_idx])\n                    # robust centering\n                    center = elite_X.mean(axis=0)\n                    Xc = elite_X - center\n                    try:\n                        U, Svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        # Generate coefficients along principal directions\n                        # use singular values to set variances, but add floor to avoid collapse\n                        sv = Svals.copy()\n                        sv = sv / (np.sqrt(max(1, k_elite - 1))) + 1e-12\n                        coeffs = rng.randn(self.dim) * sv\n                        # Map back to coordinate space\n                        directional = coeffs @ Vt\n                        x = center + directional * (scale / max(1e-12, np.std(Xc) + 1e-12)) * (range_vec / range_max)\n                        # small isotropic jitter\n                        x += rng.randn(self.dim) * (0.05 * scale * (range_vec / range_max))\n                    except Exception:\n                        # fallback gaussian around best\n                        x = base + rng.randn(self.dim) * (scale * (range_vec / range_max))\n                elif r < 0.97:\n                    # Levy-like heavy-tailed jump (Cauchy) from best or random base\n                    levy = rng.standard_cauchy(size=self.dim)\n                    # scale and clip heavy tail for numerical stability\n                    levy = np.clip(levy, -10.0, 10.0)\n                    x = base + 0.8 * scale * (range_vec / range_max) * levy\n                else:\n                    # global uniform exploration\n                    x = rng.uniform(lb, ub)\n\n                # ensure within bounds\n                x = np.minimum(np.maximum(x, lb), ub)\n\n                # evaluate\n                f = func(x)\n                evals += 1\n\n                # store\n                X_list.append(x.copy())\n                F_list.append(float(f))\n\n                improved = False\n                if f < self.f_opt - 1e-15:\n                    # true improvement\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n                    improved = True\n                    no_improve_iters = 0\n                else:\n                    no_improve_iters += 1\n\n                recent_results.append(bool(improved))\n                if len(recent_results) > recent_window:\n                    recent_results.pop(0)\n\n                if improved:\n                    successes += 1\n\n                # occasional micro-restarts: if stuck for very long, inject some diversity\n                if no_improve_iters > max(200, 10 * self.dim):\n                    # Reset some samples to random positions and enlarge scale temporarily\n                    n_replace = max(2, min(10, len(X_list) // 10, self.dim))\n                    for _r in range(n_replace):\n                        xr = rng.uniform(lb, ub)\n                        fr = func(xr)\n                        evals += 1\n                        X_list.append(xr.copy())\n                        F_list.append(float(fr))\n                        if fr < self.f_opt:\n                            self.f_opt = float(fr)\n                            self.x_opt = xr.copy()\n                    # enlarge scale to escape basin\n                    scale = min(range_max * 2.0, max(scale * 3.0, self.init_scale * range_max))\n                    no_improve_iters = 0\n                    recent_results = []\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # adapt scale based on recent success rate in this batch and history\n            # compute success rate in last 'batch' evaluations (or less if truncated)\n            last_n = min(len(recent_results), max(1, batch))\n            if last_n > 0:\n                success_rate = sum(recent_results[-last_n:]) / float(last_n)\n            else:\n                success_rate = 0.0\n\n            # adaptation rules\n            if success_rate > 0.35:\n                # many successes -> zoom in (exploit)\n                scale *= 0.7\n            elif success_rate > 0.15:\n                # moderate success -> slight zoom in\n                scale *= 0.9\n            elif success_rate < 0.05:\n                # almost no successes -> expand (explore)\n                scale *= 1.25\n            else:\n                # mild damping to avoid oscillations\n                scale *= 0.99\n\n            # occasional gentle random reset of scale to avoid premature collapse\n            if rng.rand() < 0.005:\n                scale *= rng.uniform(0.8, 1.8)\n\n            # keep scale inside reasonable bounds\n            scale = float(min(max(scale, min_scale), range_max * 2.0))\n\n            # prune memory: keep last N evaluations to limit memory growth (but keep best always)\n            max_memory = 2000 + 200 * self.dim\n            if len(F_list) > max_memory:\n                # keep best M and then most recent ones\n                best_idx = np.argsort(F_list)[:max(500, int(0.2 * max_memory))]\n                recent_idx = np.arange(len(F_list))[-(max_memory - len(best_idx)):]\n                keep_idx = np.unique(np.concatenate([best_idx, recent_idx]))\n                X_list = [X_list[i] for i in keep_idx]\n                F_list = [F_list[i] for i in keep_idx]\n\n        # final best\n        return float(self.f_opt), (None if self.x_opt is None else np.array(self.x_opt, dtype=float))", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.154 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07119368950381699, 0.13514155635296676, 0.19452982779981742, 0.1683304527557734, 0.15465342472571142, 0.17620021965416466, 0.18279566605055209, 0.15596073199314675, 0.17282833724496038, 0.12845263942758423]}, "task_prompt": ""}
{"id": "72e22e53-cbb1-40be-a08c-b2c963e7e9bd", "fitness": 0.15823858190692253, "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — a hybrid heuristic mixing global uniform search, DE-style proposals, PCA-guided directional moves and occasional Lévy jumps with an adaptive step-size and micro-restarts.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Levy (Cauchy) jumps. Uses an adaptive scale driven by recent success rates\n    and maintains an elite set to guide covariance/principal-direction local proposals.\n\n    Usage:\n        opt = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=123)\n        fbest, xbest = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2,\n                 init_scale=0.25, seed=None, keep_memory=2000):\n        \"\"\"\n        budget: total number of function evaluations allowed\n        dim: dimensionality of the problem\n        pop_init: optional initial sample size (defaults to ~max(10,4*dim) but limited by budget)\n        elite_frac: fraction of evaluated points considered as elite for PCA/covariance estimation\n        init_scale: initial relative scale (fraction of search range) for local sampling\n        seed: random seed for reproducibility\n        keep_memory: approximate memory of evaluations to keep (older data truncated except best)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_init = None if pop_init is None else int(pop_init)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.seed = seed\n        self.keep_memory = int(keep_memory)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Many BBOB problems provide func.bounds with .lb and .ub arrays or tuple form.\n        try:\n            bounds = getattr(func, \"bounds\", None)\n            if bounds is not None:\n                # Try attribute-style\n                lb = None; ub = None\n                if hasattr(bounds, \"lb\") and hasattr(bounds, \"ub\"):\n                    lb = np.asarray(bounds.lb, dtype=float)\n                    ub = np.asarray(bounds.ub, dtype=float)\n                elif isinstance(bounds, (list, tuple)) and len(bounds) == 2:\n                    lb = np.asarray(bounds[0], dtype=float)\n                    ub = np.asarray(bounds[1], dtype=float)\n                if lb is not None and ub is not None:\n                    # broadcast single-value bounds\n                    if lb.size == 1:\n                        lb = np.resize(lb, self.dim)\n                    if ub.size == 1:\n                        ub = np.resize(ub, self.dim)\n                    return lb.reshape(-1), ub.reshape(-1)\n        except Exception:\n            pass\n        # Fallback default [-5, 5]^dim\n        return np.full(self.dim, -5.0), np.full(self.dim, 5.0)\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        lb, ub = self._get_bounds(func)\n        lb = lb.astype(float).reshape(-1)\n        ub = ub.astype(float).reshape(-1)\n        if lb.size == 1:\n            lb = np.resize(lb, self.dim)\n        if ub.size == 1:\n            ub = np.resize(ub, self.dim)\n\n        range_vec = ub - lb\n        # protect against degenerate bounds\n        range_vec[range_vec == 0.0] = 1e-12\n        range_max = float(np.max(range_vec))\n\n        # initial population size\n        if self.pop_init is None:\n            n0 = int(min(max(10, 4 * self.dim), max(1, self.budget // 5)))\n        else:\n            n0 = min(self.pop_init, max(1, self.budget // 2))\n\n        # internal storage of evaluated points and values\n        X_list = []\n        F_list = []\n\n        # initial absolute scale (same units as problem)\n        scale = self.init_scale * range_max\n\n        evals = 0\n        # initial uniform sampling\n        for i in range(min(n0, self.budget)):\n            x = rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X_list.append(np.array(x, copy=True))\n            F_list.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, copy=True)\n            if evals >= self.budget:\n                return self.f_opt, self.x_opt\n\n        # history for adaptation\n        recent_results = []  # booleans whether a proposal improved best\n        batch = max(6, int(min(50, self.budget // 100)))  # evaluate adaptation over batches\n        no_improve_iters = 0\n        stagnation_restart_count = 0\n\n        # Main adaptive loop\n        while evals < self.budget:\n            # choose move type probabilistically\n            # probs: local around best (0.40), DE-style (0.20), PCA (0.20), Levy (0.10), global (0.10)\n            r = rng.random()\n            if r < 0.40 and self.x_opt is not None and len(X_list) > 0:\n                # local gaussian around best (exploit)\n                base = self.x_opt.copy()\n                step = rng.normal(scale=1.0, size=self.dim)\n                anisotropic = 0.8 * scale * (range_vec / max(range_max, 1e-12))\n                x = base + step * (anisotropic + 1e-12)\n            elif r < 0.60 and len(X_list) >= 4:\n                # DE-style differential mutation using three random distinct points\n                idxs = rng.choice(len(X_list), size=3, replace=False)\n                xa = X_list[idxs[0]].copy()\n                xb = X_list[idxs[1]].copy()\n                xc = X_list[idxs[2]].copy()\n                Fw = rng.uniform(0.6, 1.1)\n                donor = xa + Fw * (xb - xc)\n                # small gaussian perturbation and slight move towards donor\n                jitter = rng.normal(scale=0.3, size=self.dim)\n                if self.x_opt is not None and rng.random() < 0.3:\n                    base = 0.6 * self.x_opt + 0.4 * donor\n                else:\n                    base = donor\n                x = base + jitter * (scale * (range_vec / max(range_max, 1e-12)))\n            elif r < 0.80 and len(X_list) >= 4:\n                # PCA / covariance guided sampling from elite\n                k_elite = max(2, int(self.elite_frac * max(3, len(F_list))))\n                # choose elite indices by best function values\n                best_idx_sorted = np.argsort(F_list)[:k_elite]\n                X_elite = np.array([X_list[i] for i in best_idx_sorted])\n                center = np.mean(X_elite, axis=0)\n                # center and compute principal directions\n                Xc = X_elite - center\n                # small regularization\n                try:\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    # use top directions (up to dim)\n                    top = Vt[:min(Vt.shape[0], max(1, self.dim))]\n                    # sample coefficients along principal axes with variances tied to singular values\n                    sval = S[:top.shape[0]] if S.size > 0 else np.ones(top.shape[0])\n                    # convert singular values to sensible step sizes\n                    sval = sval / (np.mean(sval) + 1e-12)\n                    coeffs = rng.normal(scale=1.0, size=top.shape[0]) * (scale * (sval / (np.max(sval) + 1e-12)))\n                    # map back\n                    x = center + coeffs @ top\n                    # small isotropic jitter\n                    x = x + rng.normal(scale=0.2 * scale / max(range_max, 1e-12), size=self.dim)\n                except Exception:\n                    # fallback gaussian around best\n                    if self.x_opt is not None:\n                        x = self.x_opt + rng.normal(scale=scale, size=self.dim) * (range_vec / max(range_max,1e-12))\n                    else:\n                        x = rng.uniform(lb, ub)\n            elif r < 0.90:\n                # Levy-like heavy-tailed jump (Cauchy) from best or random base\n                base = self.x_opt if (self.x_opt is not None and rng.random() < 0.7) else rng.uniform(lb, ub)\n                levy = rng.standard_cauchy(size=self.dim)\n                # clip extreme tails for numerical stability\n                levy = np.clip(levy, -1e3, 1e3)\n                x = base + 0.8 * scale * (range_vec / max(range_max, 1e-12)) * levy\n            else:\n                # global uniform exploration\n                x = rng.uniform(lb, ub)\n\n            # enforce bounds (reflect back strategy)\n            below = x < lb\n            above = x > ub\n            x[below] = lb[below] + (lb[below] - x[below]) % (ub[below] - lb[below] + 1e-12)\n            x[above] = ub[above] - (x[above] - ub[above]) % (ub[above] - lb[above] + 1e-12)\n            # Clip finally\n            x = np.minimum(np.maximum(x, lb), ub)\n\n            # Evaluate\n            f = func(x)\n            evals += 1\n\n            # Store\n            X_list.append(np.array(x, copy=True))\n            F_list.append(float(f))\n\n            improved = False\n            if f < self.f_opt:\n                improved = True\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, copy=True)\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            recent_results.append(1 if improved else 0)\n            if len(recent_results) > 5 * batch:\n                recent_results.pop(0)\n\n            # Occasional micro-restarts to escape stagnation\n            if no_improve_iters > max(200, 10 * self.dim):\n                stagnation_restart_count += 1\n                # Inject diversity: add some random samples and increase scale\n                n_inject = int(max(2, min(50, self.budget // 200)))\n                for _ in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    xx = rng.uniform(lb, ub)\n                    ff = func(xx)\n                    evals += 1\n                    X_list.append(np.array(xx, copy=True))\n                    F_list.append(float(ff))\n                    if ff < self.f_opt:\n                        self.f_opt = float(ff)\n                        self.x_opt = np.array(xx, copy=True)\n                        no_improve_iters = 0\n                # enlarge scale a bit to escape basin\n                scale *= 2.0\n                # reset recent results\n                recent_results = []\n                # if still stuck too long, perform random restart around best with larger spread\n                if no_improve_iters > max(500, 50 * self.dim):\n                    if self.x_opt is not None:\n                        for _ in range(min(10, self.budget - evals)):\n                            rr = self.x_opt + rng.normal(scale=5.0 * scale, size=self.dim) * (range_vec / max(range_max,1e-12))\n                            rr = np.minimum(np.maximum(rr, lb), ub)\n                            ff = func(rr)\n                            evals += 1\n                            X_list.append(np.array(rr, copy=True))\n                            F_list.append(float(ff))\n                            if ff < self.f_opt:\n                                self.f_opt = float(ff)\n                                self.x_opt = np.array(rr, copy=True)\n                                no_improve_iters = 0\n                        # broaden scale to encourage exploration\n                        scale *= 1.5\n\n            # Adapt scale after each small batch\n            if len(recent_results) >= batch:\n                last_n = batch\n                success_rate = sum(recent_results[-last_n:]) / float(last_n)\n                # adaptation rules\n                if success_rate > 0.25:\n                    # many successes -> zoom in (exploit)\n                    scale *= 0.7\n                elif success_rate > 0.12:\n                    # moderate success -> slight zoom in\n                    scale *= 0.92\n                elif success_rate < 0.04:\n                    # almost no successes -> expand (explore)\n                    scale *= 1.25\n                else:\n                    # mild damping to avoid oscillations\n                    scale *= 0.99\n\n                # occasional gentle random reset of scale to avoid premature collapse\n                if rng.random() < 0.02:\n                    scale *= rng.uniform(0.8, 1.5)\n\n                # keep scale inside reasonable bounds relative to range_max\n                scale = max(1e-8 * range_max, min(scale, 5.0 * range_max))\n\n            # prune memory: keep best indices and recent ones to limit memory\n            if len(X_list) > max(self.keep_memory, 4 * self.dim):\n                # keep best M and most recent N\n                M = max(10, int(0.05 * self.keep_memory))\n                best_idx = np.argsort(F_list)[:M]\n                recent_idx = np.arange(max(0, len(X_list) - self.keep_memory // 2), len(X_list))\n                keep_idx = np.unique(np.concatenate([best_idx, recent_idx]))\n                X_list = [X_list[i] for i in keep_idx]\n                F_list = [F_list[i] for i in keep_idx]\n\n        return float(self.f_opt), np.array(self.x_opt, copy=True)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.158 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0830295095249558, 0.14995525625992812, 0.22291242549870094, 0.14937089570875395, 0.1502079641848555, 0.17602444152943486, 0.20289266350712443, 0.16483050029184465, 0.1588285117221836, 0.12433365084144332]}, "task_prompt": ""}
{"id": "a21cb167-cdc6-4ec6-bf10-fd21f6a98310", "fitness": 0.1589286333128342, "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — combines global uniform exploration, DE-style donor moves, PCA-guided directional proposals, and occasional Lévy jumps with an adaptive step-size and micro-restarts to escape stagnation.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Levy (Cauchy) jumps. Uses an adaptive scale driven by recent success rates\n    and maintains an elite set to guide covariance/principal-direction local proposals.\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2,\n                 init_scale=0.2, seed=None, keep_memory=1000):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        pop_init: initial random samples (defaults to max(10, 4*dim) but <= budget/4)\n        elite_frac: fraction of kept points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        seed: random seed\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_init is None:\n            self.pop_init = max(10, 4 * self.dim)\n            # ensure we don't consume too much init budget\n            self.pop_init = min(self.pop_init, max(1, int(self.budget // 8)))\n        else:\n            self.pop_init = max(1, int(pop_init))\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.seed = seed\n        self.keep_memory = max(50, int(keep_memory))\n\n        # placeholders to be set during __call__\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try multiple common shapes: func.bounds.lb/ub, func.bounds as tuple/list, fallback [-5,5]^dim\n        try:\n            bounds = getattr(func, \"bounds\", None)\n            if bounds is None:\n                raise AttributeError\n            # object with lb and ub\n            if hasattr(bounds, \"lb\") and hasattr(bounds, \"ub\"):\n                lb = np.asarray(bounds.lb, dtype=float)\n                ub = np.asarray(bounds.ub, dtype=float)\n                return lb.reshape(-1), ub.reshape(-1)\n            # tuple/list form\n            if isinstance(bounds, (list, tuple)) and len(bounds) == 2:\n                lb = np.asarray(bounds[0], dtype=float)\n                ub = np.asarray(bounds[1], dtype=float)\n                return lb.reshape(-1), ub.reshape(-1)\n        except Exception:\n            pass\n        # fallback\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflection strategy: reflect across bounds, then clip if still out (multiple reflections possible)\n        x = np.asarray(x, dtype=float)\n        for _ in range(3):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n        lb, ub = self._get_bounds(func)\n        # Ensure bounds length matches dimension; if singleton, broadcast\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(-1)\n        ub = ub.reshape(-1)\n        assert lb.size == self.dim and ub.size == self.dim, \"Bounds dimension mismatch\"\n\n        range_vec = ub - lb\n        range_max = max(np.max(range_vec), 1e-12)\n\n        # state\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        X_list = []   # list of points (np arrays)\n        F_list = []   # corresponding function values\n\n        evals = 0\n\n        # absolute scale in problem units\n        scale = self.init_scale * range_max\n\n        # perform initial uniform sampling to populate archive\n        n_init = min(self.pop_init, self.budget)\n        for _ in range(n_init):\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            X_list.append(np.array(x, copy=True))\n            F_list.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, copy=True)\n            if evals >= self.budget:\n                break\n\n        # Ensure at least one point\n        if self.x_opt is None:\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            X_list.append(np.array(x, copy=True))\n            F_list.append(f)\n            self.f_opt = f\n            self.x_opt = np.array(x, copy=True)\n\n        # adaptation history\n        recent_results = []  # booleans: True when a proposal improved the best\n        last_n = 20\n        no_improve_iters = 0\n\n        # main loop\n        while evals < self.budget:\n            # choose move type\n            # probs: local around best (0.40), DE-style (0.20), PCA (0.20), Levy (0.10), global (0.10)\n            r = rng.rand()\n            x_candidate = None\n\n            if r < 0.40:\n                # local gaussian around best (exploit)\n                base = np.array(self.x_opt, copy=True)\n                jitter = rng.normal(0, 1, size=self.dim)\n                x_candidate = base + jitter * (scale * (range_vec / range_max))\n            elif r < 0.60:\n                # DE-style differential mutation using three random distinct points\n                n_points = max(3, len(X_list))\n                # pick three distinct indices\n                if len(X_list) >= 3:\n                    idxs = rng.choice(len(X_list), size=3, replace=False)\n                else:\n                    idxs = rng.randint(0, len(X_list), size=3)\n                xa = np.array(X_list[idxs[0]])\n                xb = np.array(X_list[idxs[1]])\n                xc = np.array(X_list[idxs[2]])\n                Fw = rng.uniform(0.6, 1.1)\n                donor = xa + Fw * (xb - xc)\n                base = 0.6 * np.array(self.x_opt) + 0.4 * donor\n                x_candidate = base + rng.normal(0, 1, size=self.dim) * (0.6 * scale * (range_vec / range_max))\n            elif r < 0.80:\n                # PCA / covariance guided sampling from elite\n                k_elite = max(3, int(self.elite_frac * max(10, len(X_list))))\n                # choose elites by best function values\n                idxs_sorted = np.argsort(F_list)\n                elite_idx = idxs_sorted[:min(k_elite, len(idxs_sorted))]\n                if len(elite_idx) < 3:\n                    # fallback gaussian around best\n                    base = np.array(self.x_opt, copy=True)\n                    x_candidate = base + rng.normal(0, 1, size=self.dim) * (scale * (range_vec / range_max))\n                else:\n                    X_elite = np.vstack([X_list[i] for i in elite_idx])\n                    center = np.mean(X_elite, axis=0)\n                    Xc = X_elite - center\n                    # regularization\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        # sample coefficients along principal axes\n                        rdim = min(self.dim, Vt.shape[0])\n                        topV = Vt[:rdim]\n                        # scale each principal axis by singular value and global scale\n                        coeffs = rng.normal(0, 1, size=rdim) * ( (S[:rdim] / (np.max(S[:rdim]) + 1e-12)) * scale )\n                        step = coeffs.dot(topV)\n                        # small isotropic jitter\n                        jitter = rng.normal(0, 1, size=self.dim) * (0.2 * scale * (range_vec / range_max))\n                        x_candidate = center + step + jitter\n                    except Exception:\n                        base = np.array(self.x_opt, copy=True)\n                        x_candidate = base + rng.normal(0, 1, size=self.dim) * (scale * (range_vec / range_max))\n            elif r < 0.90:\n                # Levy-like heavy-tailed (Cauchy) jump from best or random base\n                use_best = rng.rand() < 0.7\n                if use_best:\n                    base = np.array(self.x_opt, copy=True)\n                else:\n                    base = rng.uniform(lb, ub)\n                # draw from Cauchy using tan(pi*(u-0.5)), limit tails\n                u = rng.rand(self.dim)\n                levy = np.tan(np.pi * (u - 0.5))\n                # clip extreme tails for numerical stability\n                levy = np.clip(levy, -20.0, 20.0)\n                x_candidate = base + 0.8 * scale * (range_vec / range_max) * levy\n            else:\n                # global uniform exploration\n                x_candidate = rng.uniform(lb, ub)\n\n            # enforce bounds via reflection + final clipping\n            x_candidate = self._reflect_bounds(x_candidate, lb, ub)\n\n            # Evaluate candidate\n            f = float(func(x_candidate))\n            evals += 1\n\n            X_list.append(np.array(x_candidate, copy=True))\n            F_list.append(f)\n\n            improved = False\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x_candidate, copy=True)\n                improved = True\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # update recent results (success defined as strictly improving best)\n            recent_results.append(1 if improved else 0)\n            if len(recent_results) > 200:\n                recent_results.pop(0)\n\n            # Occasional micro-restarts to escape stagnation\n            if no_improve_iters > 40 and evals < self.budget:\n                # inject diversity: sample a small set of random points and a few local expanded around best\n                n_inject = min( max(4, int(0.02 * self.budget)), self.budget - evals )\n                # enlarge scale a bit to escape basin\n                old_scale = scale\n                scale = min(range_max * 2.0, scale * 1.8)\n                for _ in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    if rng.rand() < 0.6:\n                        # broader gaussian around best\n                        xx = self._reflect_bounds(self.x_opt + rng.normal(0, 1, size=self.dim) * (scale * (range_vec / range_max)), lb, ub)\n                    else:\n                        # completely random exploration\n                        xx = rng.uniform(lb, ub)\n                    ff = float(func(xx))\n                    evals += 1\n                    X_list.append(np.array(xx, copy=True))\n                    F_list.append(ff)\n                    if ff < self.f_opt:\n                        self.f_opt = ff\n                        self.x_opt = np.array(xx, copy=True)\n                        no_improve_iters = 0\n                # if still stuck for very long, perform random restart with larger spread\n                if no_improve_iters > 120 and evals < self.budget:\n                    # do a focused random restart around a randomly chosen elite point\n                    idxs_sorted = np.argsort(F_list)\n                    elite_idx = idxs_sorted[:max(3, int(0.05 * len(idxs_sorted)))]\n                    base = X_list[rng.choice(elite_idx)]\n                    big_scale = min(range_max * 3.0, old_scale * 5.0)\n                    n_rr = min(5, self.budget - evals)\n                    for _ in range(n_rr):\n                        rr = self._reflect_bounds(base + rng.normal(0, 1, size=self.dim) * (big_scale * (range_vec / range_max)), lb, ub)\n                        ff = float(func(rr))\n                        evals += 1\n                        X_list.append(np.array(rr, copy=True))\n                        F_list.append(ff)\n                        if ff < self.f_opt:\n                            self.f_opt = float(ff)\n                            self.x_opt = np.array(rr, copy=True)\n                            no_improve_iters = 0\n                    # broaden scale to encourage exploration\n                    scale = min(range_max * 2.0, scale * 1.5)\n                    recent_results = []\n\n            # Adapt scale after each evaluation (small-step adaptation)\n            if len(recent_results) >= last_n:\n                success_rate = sum(recent_results[-last_n:]) / float(last_n)\n                # adaptation rules\n                if success_rate > 0.35:\n                    # zoom in\n                    scale *= 0.90\n                elif success_rate > 0.15:\n                    scale *= 0.97\n                elif success_rate < 0.05:\n                    # expand to escape\n                    scale *= 1.15\n                else:\n                    # mild damping\n                    scale *= 1.0\n            else:\n                # minor random jitter to avoid collapse\n                if rng.rand() < 0.01:\n                    scale *= (1.0 + rng.normal(0, 0.02))\n\n            # keep scale sensible\n            scale = float(max(range_max * 1e-6, min(scale, range_max * 3.0)))\n\n            # prune memory: keep best M and recent N to limit memory\n            if len(X_list) > max(self.keep_memory * 2, 2000):\n                # keep best indices and most recent ones\n                F_arr = np.array(F_list)\n                best_idx = np.argsort(F_arr)[:max(10, int(0.05 * self.keep_memory))]\n                recent_idx = np.arange(max(0, len(F_list) - self.keep_memory), len(F_list))\n                keep_idx = np.unique(np.concatenate([best_idx, recent_idx]))\n                X_list = [X_list[i] for i in keep_idx]\n                F_list = [F_list[i] for i in keep_idx]\n\n        return float(self.f_opt), np.array(self.x_opt, copy=True)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.159 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08040795449306737, 0.15021627380721703, 0.21708781596110627, 0.14410978778084227, 0.15432479035155655, 0.18276508577404837, 0.1888185556752584, 0.1871670372561861, 0.15428398396538823, 0.13010504806367118]}, "task_prompt": ""}
{"id": "13217461-fbac-4bb2-8be7-7e3dff707dca", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Adaptive Directional Sampling (ADS) — hybrid sampler mixing global uniform exploration, DE-style donor moves, PCA-guided directional proposals, occasional Lévy (Cauchy) jumps, adaptive step-size and micro-restarts to escape stagnation.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Levy (Cauchy) jumps. Uses an adaptive scale driven by recent success rates\n    and maintains an elite set to guide covariance/principal-direction local proposals.\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2,\n                 init_scale=0.06, seed=None, keep_memory=400):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        pop_init: initial random samples (defaults to max(10, 4*dim) but <= budget/4)\n        elite_frac: fraction of kept points considered \"elite\" for PCA (0.0-1.0)\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        seed: random seed\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_init is None:\n            self.pop_init = min(max(10, 4 * self.dim), max(1, self.budget // 4))\n        else:\n            self.pop_init = max(1, int(pop_init))\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.keep_memory = max(50, int(keep_memory))\n\n        # placeholders set during __call__\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try to extract bounds in multiple common formats; fallback to [-5,5]^dim\n        dim = self.dim\n        lb = None\n        ub = None\n        if hasattr(func, 'bounds'):\n            b = func.bounds\n            # object with lb and ub attributes\n            if hasattr(b, 'lb') and hasattr(b, 'ub'):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            # tuple/list like (lb, ub)\n            elif isinstance(b, (tuple, list)) and len(b) == 2:\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n        # maybe func.lower / func.upper\n        if (lb is None or ub is None) and hasattr(func, 'lower') and hasattr(func, 'upper'):\n            lb = np.asarray(func.lower, dtype=float)\n            ub = np.asarray(func.upper, dtype=float)\n        # final fallback\n        if lb is None or ub is None:\n            lb = -5.0 * np.ones(dim, dtype=float)\n            ub = 5.0 * np.ones(dim, dtype=float)\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(dim, float(ub.item()))\n        lb = lb.reshape(dim,)\n        ub = ub.reshape(dim,)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect across bounds, allow for multiple reflections, then clamp\n        x = np.array(x, dtype=float, copy=True)\n        below = x < lb\n        above = x > ub\n        # reflection formula: reflect across boundary as many times as needed\n        # map x into infinite periodic reflected domain\n        rng = self.rng\n        # while still out of bounds, reflect\n        # vectorized approach: fold values by mirroring around intervals\n        span = ub - lb\n        # avoid division by zero\n        small = span < 1e-12\n        if np.any(small):\n            span[small] = 1.0\n        # shift to zero-based\n        y = (x - lb) / span\n        # reflect fractional part around integer boundaries\n        # if floor(y) is even -> take frac(y); if odd -> take 1 - frac(y)\n        k = np.floor(y).astype(int)\n        frac = y - k\n        odd = (k % 2) != 0\n        frac[odd] = 1.0 - frac[odd]\n        x = lb + frac * span\n        # final clamp to ensure numerical safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            # try broadcasting if singleton provided\n            lb = np.full(self.dim, lb.flatten()[0])\n            ub = np.full(self.dim, ub.flatten()[0])\n\n        range_vec = ub - lb\n        range_max = max(np.max(range_vec), 1e-12)\n\n        # state\n        X_list = []   # list of evaluated points (numpy arrays)\n        F_list = []   # list of function values\n        evals = 0\n\n        scale = float(self.init_scale)  # relative scale (fraction of range_max)\n        # Initialize with uniform samples\n        init_n = min(self.pop_init, max(1, self.budget // 10))  # some budget reserved for main loop\n        while len(X_list) < init_n and evals < self.budget:\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            X_list.append(np.array(x, copy=True))\n            F_list.append(float(f))\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, copy=True)\n\n        # Ensure at least one point exists\n        if len(X_list) == 0 and evals < self.budget:\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            X_list.append(np.array(x, copy=True))\n            F_list.append(float(f))\n            evals += 1\n            self.f_opt = float(f)\n            self.x_opt = np.array(x, copy=True)\n\n        # adaptation history\n        recent_window = max(20, 6 * self.dim)\n        success_hist = deque(maxlen=recent_window)  # True/False improvements\n        no_improve_iters = 0\n        last_improvement_eval = 0\n\n        # main loop\n        while evals < self.budget:\n            # recompute elite set\n            n_points = len(F_list)\n            elite_size = max(1, int(np.ceil(self.elite_frac * n_points)))\n            # choose move type with probabilities:\n            # local around best (0.40), DE-style (0.20), PCA (0.20), Levy (0.10), global (0.10)\n            r = self.rng.random()\n            if r < 0.40:\n                # local gaussian around best (exploit)\n                base = np.array(self.x_opt, copy=True)\n                jitter = self.rng.normal(0, 1, size=self.dim)\n                x_candidate = base + jitter * (scale * (range_vec / range_max))\n            elif r < 0.60:\n                # DE-style differential mutation using three random distinct points from memory\n                if n_points >= 4:\n                    idxs = self.rng.choice(n_points, size=3, replace=False)\n                    x1 = X_list[idxs[0]]\n                    x2 = X_list[idxs[1]]\n                    x3 = X_list[idxs[2]]\n                    F = 0.8  # differential weight\n                    # DE-like donor with small gaussian jitter\n                    x_candidate = x1 + F * (x2 - x3) + self.rng.normal(0, 0.05, size=self.dim) * (scale * (range_vec / range_max))\n                else:\n                    # fallback gaussian\n                    base = np.array(self.x_opt, copy=True)\n                    x_candidate = base + self.rng.normal(0, 1, size=self.dim) * (scale * (range_vec / range_max))\n            elif r < 0.80:\n                # PCA / covariance guided sampling from elite\n                # choose elites by best function values\n                idx_sorted = np.argsort(F_list)\n                elite_idx = idx_sorted[:elite_size]\n                X_elite = np.array([X_list[i] for i in elite_idx])\n                if X_elite.shape[0] >= 2:\n                    center = X_elite.mean(axis=0)\n                    Xc = X_elite - center\n                    # regularization for stability\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        # choose reduced rank\n                        rdim = min(self.dim, max(1, S.size))\n                        # sample coefficients along top components\n                        coeffs = self.rng.normal(0, 1, size=rdim) * (S[:rdim] / (np.max(S[:rdim]) + 1e-12))\n                        # scale by global scale\n                        step = (coeffs @ Vt[:rdim, :])\n                        # small isotropic jitter to keep exploration\n                        jitter = self.rng.normal(0, 0.3, size=self.dim) * (scale * (range_vec / range_max))\n                        x_candidate = center + step * (scale * (range_vec / range_max)) + jitter\n                    except Exception:\n                        # fallback gaussian around best\n                        x_candidate = self.x_opt + self.rng.normal(0, 1, size=self.dim) * (scale * (range_vec / range_max))\n                else:\n                    x_candidate = self.x_opt + self.rng.normal(0, 1, size=self.dim) * (scale * (range_vec / range_max))\n            elif r < 0.90:\n                # Levy-like heavy-tailed (Cauchy) jump from best or random base\n                base_choice = self.rng.random()\n                if base_choice < 0.7:\n                    base = np.array(self.x_opt, copy=True)\n                else:\n                    base = np.array(X_list[self.rng.integers(n_points)], copy=True)\n                u = self.rng.random(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # clip extreme tails for numerical stability\n                cauchy = np.clip(cauchy, -1e2, 1e2)\n                # heavier scale than normal local moves\n                x_candidate = base + cauchy * (scale * 5.0) * (range_vec / range_max)\n            else:\n                # global uniform exploration\n                x_candidate = self.rng.uniform(lb, ub)\n\n            # enforce bounds via reflection + final clipping\n            x_candidate = self._reflect_bounds(x_candidate, lb, ub)\n\n            # Evaluate candidate\n            f = func(x_candidate)\n            evals += 1\n\n            # append\n            X_list.append(np.array(x_candidate, copy=True))\n            F_list.append(float(f))\n\n            # update best\n            improved = False\n            if f < self.f_opt - 1e-15:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x_candidate, copy=True)\n                improved = True\n                last_improvement_eval = evals\n                no_improve_iters = 0\n            else:\n                no_improve_iters = evals - last_improvement_eval\n\n            success_hist.append(1 if improved else 0)\n\n            # Occasional micro-restarts to escape stagnation\n            stagnation_threshold = 40 + 3 * self.dim\n            if no_improve_iters > stagnation_threshold and evals < self.budget:\n                # inject diversity: sample a small set of random points and a few local expanded around best\n                inject = min(12, self.budget - evals)\n                # expand scale temporarily\n                old_scale = scale\n                scale = min(1.5, scale * (1.2 + 0.8 * self.rng.random()))\n                for k in range(inject):\n                    if evals >= self.budget:\n                        break\n                    if k % 3 == 0:\n                        # completely random exploration\n                        xx = self.rng.uniform(lb, ub)\n                    else:\n                        # broader gaussian around best\n                        xx = self._reflect_bounds(self.x_opt + self.rng.normal(0, 1, size=self.dim) * (scale * 2.0 * (range_vec / range_max)), lb, ub)\n                    fxx = func(xx)\n                    evals += 1\n                    X_list.append(np.array(xx, copy=True))\n                    F_list.append(float(fxx))\n                    if fxx < self.f_opt:\n                        self.f_opt = float(fxx)\n                        self.x_opt = np.array(xx, copy=True)\n                        last_improvement_eval = evals\n                        no_improve_iters = 0\n                        success_hist.append(1)\n                # if still stuck for very long, perform focused random restart around a randomly chosen elite point\n                if no_improve_iters > 4 * stagnation_threshold and evals < self.budget:\n                    # choose a random elite center\n                    idx_sorted = np.argsort(F_list)\n                    choose_idx = int(idx_sorted[self.rng.integers(min(len(idx_sorted), max(1, elite_size)))])\n                    center = X_list[choose_idx]\n                    restart_spread = min(1.0, 5.0 * scale)\n                    rest_n = min(10, self.budget - evals)\n                    for k in range(rest_n):\n                        if evals >= self.budget:\n                            break\n                        xx = self._reflect_bounds(center + self.rng.normal(0, 1, size=self.dim) * (restart_spread * (range_vec / range_max)), lb, ub)\n                        fxx = func(xx)\n                        evals += 1\n                        X_list.append(np.array(xx, copy=True))\n                        F_list.append(float(fxx))\n                        if fxx < self.f_opt:\n                            self.f_opt = float(fxx)\n                            self.x_opt = np.array(xx, copy=True)\n                            last_improvement_eval = evals\n                            no_improve_iters = 0\n                            success_hist.append(1)\n                # restore or slightly damp scale\n                scale = max(1e-6, 0.8 * old_scale)\n\n            # Adapt scale after each evaluation (small-step adaptation)\n            if len(success_hist) >= 4:\n                recent_success = float(np.mean(success_hist))\n                # target success ~ 0.2; adapt multiplicatively\n                if recent_success > 0.25:\n                    scale *= 1.06\n                elif recent_success < 0.05:\n                    scale *= 0.7\n                else:\n                    # slight damping to avoid explosion\n                    scale *= 0.995\n            else:\n                # initial mild exploration\n                scale *= 0.999\n\n            # minor random jitter to avoid collapse\n            scale *= np.exp(self.rng.normal(0, 0.002))\n\n            # keep scale sensible relative to range\n            scale = float(np.clip(scale, 1e-6, 2.0))\n\n            # prune memory: keep best M and recent N to limit memory\n            M = max(50, self.keep_memory)\n            if len(F_list) > 3 * M:\n                # keep best half and most recent half of M\n                idx_sorted = np.argsort(F_list)\n                keep_best = idx_sorted[: M // 2]\n                keep_recent = np.arange(max(0, len(F_list) - M // 2), len(F_list))\n                keep_idx = np.unique(np.concatenate((keep_best, keep_recent)))\n                X_list = [X_list[i] for i in keep_idx]\n                F_list = [F_list[i] for i in keep_idx]\n\n        return float(self.f_opt), np.array(self.x_opt, copy=True)", "configspace": "", "generation": 0, "feedback": "In the code, line 142, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_hist = deque(maxlen=recent_window)  # True/False improvements", "error": "In the code, line 142, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_hist = deque(maxlen=recent_window)  # True/False improvements", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ed629a2d-b165-4517-aafd-b2ceee4d82e3", "fitness": 0.1767767942012712, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes global uniform exploration, DE-style donor moves, PCA-guided directional local proposals, Cauchy (Levy-like) jumps and adaptive scale + micro-restarts to robustly explore [-5,5]^dim spaces.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Cauchy jumps. Uses an adaptive absolute scale driven by recent success rates\n    and maintains an elite set to guide covariance/principal-direction local proposals.\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2,\n                 init_scale=0.2, seed=None, keep_memory=1000):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        pop_init: initial random samples (defaults to max(10, 4*dim) but <= budget/4)\n        elite_frac: fraction of kept points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        seed: random seed\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_init is None:\n            self.pop_init = int(min(max(10, 4 * self.dim), max(1, self.budget // 4)))\n        else:\n            self.pop_init = int(pop_init)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.rng = np.random.RandomState(seed)\n        self.keep_memory = int(keep_memory)\n\n        # Internal placeholders to be set when __call__ runs:\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try to extract bounds from func in common formats, otherwise default [-5,5]^dim\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # object with lb, ub\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            else:\n                # tuple/list\n                try:\n                    b = tuple(b)\n                    if len(b) == 2 and np.shape(b[0]) in ((), (self.dim,)) and np.shape(b[1]) in ((), (self.dim,)):\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n                except Exception:\n                    pass\n        # fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # Broadcast scalars if necessary\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect across boundaries repeatedly; clamps if still out after some reflections\n        x = np.array(x, dtype=float)\n        for _ in range(4):  # a few passes to resolve multi-reflections\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        # absolute scale units: fraction of (ub-lb)\n        span = ub - lb\n        span = np.maximum(span, 1e-8)\n        scale = self.init_scale * span  # vector scale allowed (per-dim)\n        # also keep a scalar global scale factor for adaptation\n        gscale = np.mean(scale)\n\n        # archives\n        archive_x = []  # list of points (np.array)\n        archive_f = []  # corresponding function values\n\n        evals = 0\n\n        # initial uniform samples\n        n_init = min(self.pop_init, max(1, self.budget // 10))\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n            if evals >= self.budget:\n                return self.f_opt, self.x_opt\n\n        # Ensure at least one point exists\n        if len(archive_x) == 0:\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            self.f_opt, self.x_opt = float(f), np.array(x, dtype=float)\n\n        # adaptation history\n        recent_success = []\n        success_window = 20\n        stagnation = 0\n        stagnation_restart_threshold = max(50, 5 * self.dim)  # when to consider micro-restart\n\n        # probabilities\n        p_local = 0.40\n        p_de = 0.20\n        p_pca = 0.20\n        p_levy = 0.10\n        p_global = 0.10\n\n        # small constants\n        eps = 1e-12\n\n        # main loop\n        while evals < self.budget:\n            # choose move type\n            r = self.rng.rand()\n            if r < p_local:\n                move_type = \"local\"\n            elif r < p_local + p_de:\n                move_type = \"de\"\n            elif r < p_local + p_de + p_pca:\n                move_type = \"pca\"\n            elif r < p_local + p_de + p_pca + p_levy:\n                move_type = \"levy\"\n            else:\n                move_type = \"global\"\n\n            # baseline: best point is exploitation center\n            if self.x_opt is None:\n                base = archive_x[self.rng.randint(len(archive_x))]\n            else:\n                # sometimes use best, sometimes random base\n                if self.rng.rand() < 0.85:\n                    base = self.x_opt\n                else:\n                    base = archive_x[self.rng.randint(len(archive_x))]\n\n            # propose candidate\n            if move_type == \"local\":\n                # Gaussian around best/base with anisotropic per-dim scale\n                z = self.rng.normal(loc=0.0, scale=1.0, size=self.dim)\n                cand = np.array(base) + z * (scale * (0.5 + self.rng.rand(self.dim)))\n            elif move_type == \"de\":\n                # DE-style donor using three distinct points\n                if len(archive_x) >= 3:\n                    idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                    a, b, c = archive_x[idxs[0]], archive_x[idxs[1]], archive_x[idxs[2]]\n                    F = self.rng.uniform(0.4, 1.0)  # differential weight\n                    donor = a + F * (b - c)\n                    # small crossover with base\n                    cr = 0.7\n                    mask = self.rng.rand(self.dim) < cr\n                    cand = np.array(base, dtype=float)\n                    cand[mask] = donor[mask]\n                else:\n                    # fallback to local\n                    z = self.rng.normal(loc=0.0, scale=1.0, size=self.dim)\n                    cand = np.array(base) + z * scale\n            elif move_type == \"pca\":\n                # PCA-guided sampling around elites\n                # build elites\n                n_keep = max(1, int(len(archive_x) * self.elite_frac))\n                # protect against too few evaluated points\n                if len(archive_x) <= 2:\n                    # fallback gaussian around best\n                    z = self.rng.normal(size=self.dim)\n                    cand = np.array(base) + z * scale\n                else:\n                    # select best indices\n                    sorted_idx = np.argsort(archive_f)\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.array([archive_x[i] for i in elite_idx])\n                    # center on elite mean or best\n                    center = np.mean(elites, axis=0)\n                    # compute principal axes\n                    X = elites - center\n                    # regularize\n                    cov = np.cov(X.T) if X.shape[0] > 1 else np.atleast_2d(np.var(X, axis=0))\n                    # SVD/PCA\n                    try:\n                        U, S, Vt = np.linalg.svd(cov + np.eye(self.dim) * 1e-9)\n                    except np.linalg.LinAlgError:\n                        U = np.eye(self.dim)\n                        S = np.ones(self.dim) * (np.mean(span) ** 2)\n                    # sample coefficients along principal axes\n                    coeffs = self.rng.normal(size=self.dim) * (np.sqrt(S) + 1e-8) * (gscale / max(1e-8, np.mean(S) ** 0.5))\n                    # scale coefficients by global scale and a mild random multiplier\n                    coeffs *= (0.6 + 1.4 * self.rng.rand())\n                    cand = center + (U @ coeffs)\n                    # add small isotropic jitter proportional to scale\n                    cand += self.rng.normal(scale=0.05, size=self.dim) * scale\n            elif move_type == \"levy\":\n                # Cauchy (heavy-tailed) jump\n                # base is best most of the time; sometimes random center\n                center = base\n                # sample Cauchy per-dim, truncate tails for numerical stability\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                cauchy = np.clip(cauchy, -1e2, 1e2)  # prevent overflow\n                # scale heavy-tailed by span and a random intensity\n                intensity = self.rng.uniform(0.2, 2.0)\n                cand = np.array(center) + cauchy * (gscale * intensity)\n            else:  # global\n                cand = self.rng.uniform(lb, ub)\n\n            # enforce bounds via reflection + final clip\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate\n            f_cand = func(cand)\n            evals += 1\n\n            # update archive\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n\n            # update recent success history\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > success_window:\n                recent_success.pop(0)\n            # update stagnation counter\n            if improved:\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # Micro-restart (inject diversity) when stagnation\n            if stagnation > stagnation_restart_threshold:\n                # inject some random points and a few expanded locals\n                n_inject = min(5 + self.dim // 2, max(3, int(0.02 * self.budget)))\n                for _ in range(n_inject):\n                    # half global random, half local but with expanded scale\n                    if self.rng.rand() < 0.5:\n                        x_inj = self.rng.uniform(lb, ub)\n                    else:\n                        # around a random elite or best\n                        if len(archive_x) > 0 and self.rng.rand() < 0.7:\n                            center = archive_x[self.rng.randint(len(archive_x))]\n                        else:\n                            center = self.x_opt\n                        x_inj = np.array(center) + self.rng.normal(scale=2.0, size=self.dim) * scale\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    if evals >= self.budget:\n                        break\n                    f_inj = func(x_inj)\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                # slightly increase scale to escape basin\n                gscale *= 1.6\n                scale = np.maximum(scale, (gscale * np.ones_like(scale)))\n                # reset stagnation gently\n                stagnation = int(stagnation_restart_threshold * 0.4)\n                # small random jitter to memory to encourage different choices\n                recent_success = []\n\n            # Adapt scale after each evaluation (small-step adaptation)\n            # compute short-term success rate\n            if len(recent_success) > 0:\n                succ_rate = np.mean(recent_success)\n            else:\n                succ_rate = 0.0\n            # rules:\n            # if succ_rate high -> zoom in; if low -> expand a bit\n            if succ_rate > 0.30:\n                gscale *= 0.97  # zoom in\n            elif succ_rate < 0.05:\n                gscale *= 1.03  # expand\n            else:\n                gscale *= 0.999  # mild damping\n            # small random jitter\n            gscale *= (1.0 + 0.01 * (self.rng.rand() - 0.5))\n            # keep scale within reasonable bounds relative to span\n            gscale = float(np.clip(gscale, 1e-6 * np.mean(span), 2.0 * np.mean(span)))\n            # update per-dim scale\n            scale = np.maximum(1e-12, (self.init_scale * span) * (gscale / max(1e-8, np.mean(self.init_scale * span))))\n            # ensure vector\n            scale = np.asarray(scale, dtype=float)\n\n            # prune memory: keep best K and recent R to limit memory\n            M = max(50, min(self.keep_memory, int(self.budget // 2)))\n            if len(archive_x) > M:\n                # keep best few and most recent ones\n                best_k = max(10, int(0.1 * M))\n                idx_sorted = np.argsort(archive_f)\n                keep_best_idx = set(idx_sorted[:best_k].tolist())\n                # keep most recent others up to M\n                recent_idx = list(range(max(0, len(archive_x) - (M - best_k)), len(archive_x)))\n                keep_idx = sorted(list(keep_best_idx.union(recent_idx)))\n                # compact lists\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n            # safety break (shouldn't be needed)\n            if evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.177 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1010887463673964, 0.15562391749962845, 0.2275513144137753, 0.1992292422038251, 0.17565038621894524, 0.20124799330795007, 0.21048765484994436, 0.1825909530135723, 0.18072919074006333, 0.13356854339761148]}, "task_prompt": ""}
{"id": "669b854f-b834-4105-aeef-5fac32aa15ca", "fitness": 0.17892152601439287, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes global uniform exploration, DE-style donor moves, PCA-guided directional local proposals, Cauchy (Lévy-like) jumps and adaptive scale + micro-restarts to robustly explore [-5,5]^dim spaces.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Cauchy jumps. Uses an adaptive absolute scale driven by recent success rates\n    and maintains an elite set to guide covariance/principal-direction local proposals.\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2,\n                 init_scale=0.2, seed=None, keep_memory=1000):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        pop_init: initial random samples (defaults to max(10, 4*dim) but <= budget/4)\n        elite_frac: fraction of kept points considered \"elite\" for PCA\n        init_scale: initial relative (fraction of search range) scale for local proposals\n        seed: random seed\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_init is None:\n            self.pop_init = int(min(max(10, 4 * self.dim), max(1, self.budget // 4)))\n        else:\n            self.pop_init = int(pop_init)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.rng = np.random.RandomState(seed)\n        self.keep_memory = int(keep_memory)\n\n        # outputs\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _get_bounds(self, func):\n        # Try to extract bounds from func in common formats, otherwise default [-5,5]^dim\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # object with lb, ub attributes\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            else:\n                # tuple/list of (lb, ub)\n                try:\n                    b = tuple(b)\n                    if len(b) >= 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n        # fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # Broadcast scalars if necessary\n        lb = np.broadcast_to(np.asarray(lb, dtype=float), (self.dim,))\n        ub = np.broadcast_to(np.asarray(ub, dtype=float), (self.dim,))\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect across boundaries repeatedly; clamps if still out after some reflections\n        x = np.array(x, dtype=float)\n        for _ in range(6):  # a few passes to resolve multi-reflections\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        # absolute scale units: fraction of (ub-lb)\n        span = ub - lb\n        span = np.maximum(span, 1e-8)\n        # initial per-dim scale vector\n        scale = (self.init_scale * span).astype(float)\n        # scalar global scale factor for adaptation\n        gscale = float(np.mean(scale))\n\n        # archives\n        archive_x = []  # list of np.array points\n        archive_f = []  # corresponding float function values\n\n        evals = 0\n\n        # initial uniform samples (seed the archive)\n        n_init = min(self.pop_init, max(1, self.budget // 10))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # Ensure at least one point exists\n        if len(archive_x) == 0:\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            self.f_opt, self.x_opt = f, np.array(x, dtype=float)\n\n        # adaptation history\n        recent_success = []\n        success_window = 20\n        stagnation = 0\n        stagnation_restart_threshold = max(50, 5 * self.dim)  # when to consider micro-restart\n\n        # move probabilities\n        p_local = 0.40\n        p_de = 0.20\n        p_pca = 0.20\n        p_levy = 0.10\n        # remaining is global\n        # small constants\n        eps = 1e-12\n\n        # main loop\n        while evals < self.budget:\n            # choose move type\n            r = self.rng.rand()\n            if r < p_local:\n                move_type = \"local\"\n            elif r < p_local + p_de:\n                move_type = \"de\"\n            elif r < p_local + p_de + p_pca:\n                move_type = \"pca\"\n            elif r < p_local + p_de + p_pca + p_levy:\n                move_type = \"levy\"\n            else:\n                move_type = \"global\"\n\n            # baseline: sometimes use best, sometimes random base\n            if self.rng.rand() < 0.85:\n                base = np.array(self.x_opt, dtype=float)\n            else:\n                base = np.array(archive_x[self.rng.randint(len(archive_x))], dtype=float)\n\n            # propose candidate\n            if move_type == \"local\":\n                # Gaussian around base with anisotropic per-dim scale\n                z = self.rng.normal(loc=0.0, scale=1.0, size=self.dim)\n                # add mild multiplicative randomness per-dim\n                local_mult = 0.3 + 1.4 * self.rng.rand(self.dim)\n                cand = base + z * (scale * local_mult)\n\n            elif move_type == \"de\":\n                # DE-style donor using three distinct archive points\n                if len(archive_x) >= 3:\n                    # select three distinct indices different from each other\n                    idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                    a = np.array(archive_x[idxs[0]], dtype=float)\n                    b = np.array(archive_x[idxs[1]], dtype=float)\n                    c = np.array(archive_x[idxs[2]], dtype=float)\n                    F = self.rng.uniform(0.4, 1.0)  # differential weight\n                    donor = a + F * (b - c)\n                    # small crossover with base\n                    cr = 0.7\n                    mask = self.rng.rand(self.dim) < cr\n                    cand = np.array(base, dtype=float)\n                    cand[mask] = donor[mask]\n                    # add tiny jitter scaled to scale to avoid exact copies\n                    cand += self.rng.normal(scale=0.02, size=self.dim) * scale\n                else:\n                    # fallback to local\n                    z = self.rng.normal(loc=0.0, scale=1.0, size=self.dim)\n                    cand = base + z * scale\n\n            elif move_type == \"pca\":\n                # PCA-guided sampling around elites\n                n_keep = max(1, int(len(archive_x) * self.elite_frac))\n                if len(archive_x) <= 2 or n_keep < 1:\n                    # fallback gaussian around base\n                    z = self.rng.normal(size=self.dim)\n                    cand = base + z * scale\n                else:\n                    # select best indices by objective\n                    sorted_idx = np.argsort(archive_f)\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.array([archive_x[i] for i in elite_idx])\n                    center = np.mean(elites, axis=0)\n                    # demean\n                    X = elites - center\n                    # if only one elite, set cov diagonal by var of whole span\n                    if X.shape[0] <= 1:\n                        cov = np.atleast_2d(np.var(elites, axis=0) + 1e-6)\n                        cov = np.diag(cov) if cov.shape == (self.dim,) else np.atleast_2d(np.diag(cov))\n                    else:\n                        cov = np.cov(X.T)\n                    # regularize and SVD/PCA\n                    cov_reg = cov + np.eye(self.dim) * 1e-9\n                    try:\n                        U, S, Vt = np.linalg.svd(cov_reg)\n                    except np.linalg.LinAlgError:\n                        U = np.eye(self.dim)\n                        S = np.ones(self.dim) * (np.mean(span) ** 2)\n                    # sample coefficients along principal axes; scale by sqrt(eigvals)\n                    coeffs = self.rng.normal(size=self.dim) * np.sqrt(np.maximum(S, 0.0))\n                    # scale to current global scale and a mild random multiplier\n                    coeffs *= (0.6 + 1.4 * self.rng.rand()) * (gscale / (np.mean(span) + eps))\n                    cand = center + (U @ coeffs)\n                    # add small isotropic jitter proportional to scale\n                    cand += self.rng.normal(scale=0.05, size=self.dim) * scale\n\n            elif move_type == \"levy\":\n                # Cauchy (heavy-tailed) jump\n                center = base\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                cauchy = np.clip(cauchy, -1e2, 1e2)  # prevent overflow\n                intensity = self.rng.uniform(0.2, 2.0)\n                cand = center + cauchy * (gscale * intensity)\n\n            else:  # global\n                cand = self.rng.uniform(lb, ub)\n\n            # enforce bounds via reflection + final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate (ensure we do not exceed budget)\n            if evals >= self.budget:\n                break\n            f_cand = float(func(cand))\n            evals += 1\n\n            # update archive\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n\n            # update recent success history\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > success_window:\n                recent_success.pop(0)\n            # update stagnation counter\n            if improved:\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # Micro-restart (inject diversity) when stagnation\n            if stagnation > stagnation_restart_threshold and evals < self.budget:\n                # inject some random points and a few expanded locals\n                n_inject = min(5 + self.dim // 2, max(3, int(0.02 * self.budget)))\n                for _ in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.rand() < 0.5:\n                        x_inj = self.rng.uniform(lb, ub)\n                    else:\n                        # around a random elite or best\n                        if len(archive_x) > 0 and self.rng.rand() < 0.7:\n                            center = archive_x[self.rng.randint(len(archive_x))]\n                        else:\n                            center = self.x_opt if self.x_opt is not None else self.rng.uniform(lb, ub)\n                        x_inj = np.array(center) + self.rng.normal(scale=2.0, size=self.dim) * scale\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    f_inj = float(func(x_inj))\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                # increase scale to escape basin\n                gscale *= 1.6\n                scale = np.maximum(scale, (gscale * np.ones_like(scale)))\n                # reset stagnation gently\n                stagnation = int(stagnation_restart_threshold * 0.4)\n                recent_success = []\n\n            # Adapt scale after each evaluation (small-step adaptation)\n            if len(recent_success) > 0:\n                succ_rate = np.mean(recent_success)\n            else:\n                succ_rate = 0.0\n            # rules:\n            # if succ_rate high -> zoom in; if low -> expand a bit\n            if succ_rate > 0.30:\n                gscale *= 0.97  # shrink\n            elif succ_rate < 0.05:\n                gscale *= 1.03  # expand\n            else:\n                gscale *= 0.999  # mild damping\n            # keep gscale within reasonable bounds relative to span\n            gscale = float(np.clip(gscale, 1e-6 * np.mean(span), 2.0 * np.mean(span)))\n            # update per-dim scale, preserving init_scale relative proportion\n            base_scale = (self.init_scale * span)\n            norm = max(1e-8, np.mean(base_scale))\n            scale = np.maximum(1e-12, base_scale * (gscale / norm))\n            scale = np.asarray(scale, dtype=float)\n\n            # prune memory: keep best K and recent R to limit memory\n            M = max(50, min(self.keep_memory, int(self.budget // 2)))\n            if len(archive_x) > M:\n                best_k = max(10, int(0.1 * M))\n                idx_sorted = np.argsort(archive_f)\n                keep_best_idx = set(idx_sorted[:best_k].tolist())\n                # keep most recent others up to M\n                recent_idx = list(range(max(0, len(archive_x) - (M - best_k)), len(archive_x)))\n                keep_idx = sorted(list(keep_best_idx.union(recent_idx)))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # Final safety: if no x_opt set (shouldn't happen), pick best archive\n        if self.x_opt is None and len(archive_x) > 0:\n            idx_min = int(np.argmin(archive_f))\n            self.x_opt = np.array(archive_x[idx_min], dtype=float)\n            self.f_opt = float(archive_f[idx_min])\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.179 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09487149493226343, 0.1514730647595326, 0.23321744344087247, 0.22068745998149897, 0.16906318820969202, 0.21024683707519454, 0.20124227753617174, 0.18728206314491092, 0.18759520227567572, 0.13353622878811655]}, "task_prompt": ""}
{"id": "92c624ed-b73c-4804-a8a0-3c8f4d7d7e35", "fitness": 0.17920027082663176, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes global uniform exploration, DE-style donor moves, PCA-guided directional local proposals, Cauchy (Levy-like) jumps and adaptive scale + micro-restarts to robustly explore [-5,5]^dim spaces.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS)\n    Hybrid heuristic combining global sampling, DE-style proposals, PCA-guided directional sampling,\n    and occasional Cauchy jumps. Uses an adaptive absolute scale driven by recent success rates\n    and maintains an elite set to guide covariance/principal-direction local proposals.\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop_init=None, elite_frac=0.20, init_scale=0.08,\n                 keep_memory=200):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        seed: RNG seed\n        pop_init: initial random samples (defaults to max(10, 4*dim) but <= budget/4)\n        elite_frac: fraction of kept points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        if pop_init is None:\n            # reasonable default but not too large relative to budget\n            self.pop_init = int(min(max(10, 4 * self.dim), max(1, self.budget // 10)))\n        else:\n            self.pop_init = int(pop_init)\n\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.keep_memory = int(keep_memory)\n\n        # placeholders to be set during run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try to extract bounds from func in common formats, otherwise default [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\"):\n                b = getattr(func, \"bounds\")\n                # object with lb, ub attributes\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe a tuple/list like (lb, ub)\n                    try:\n                        b0 = b[0]\n                        b1 = b[1]\n                        lb = np.asarray(b0, dtype=float)\n                        ub = np.asarray(b1, dtype=float)\n                    except Exception:\n                        pass\n        except Exception:\n            pass\n\n        # fallback defaults\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Broadcast scalars if necessary\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        # Final sanity check\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.resize(lb, self.dim)\n            ub = np.resize(ub, self.dim)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect across boundaries repeatedly; clamps if still out after some reflections\n        x = np.array(x, dtype=float)\n        for _ in range(6):  # a few passes to resolve multi-reflections\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect outside points\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def __call__(self, func):\n        # Initialize bounds and internal scale\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span_mean = float(np.mean(span))\n        # base_scale is per-dim absolute scale (fraction of span)\n        base_scale = self.init_scale * span  # vector scale allowed (per-dim)\n        # global scalar scale factor for adaptation\n        gscale = max(1e-12, np.mean(base_scale))\n        scale = np.array(base_scale, dtype=float)\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # initial uniform samples\n        n_init = min(self.pop_init, max(1, self.budget // 10))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # Ensure at least one point exists\n        if len(archive_x) == 0 and evals < self.budget:\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            self.f_opt, self.x_opt = float(f), np.array(x, dtype=float)\n\n        # adaptation history\n        recent_success = []\n        success_window = 20\n        stagnation = 0\n        stagnation_restart_threshold = max(50, 5 * self.dim)  # when to consider micro-restart\n\n        # move type probabilities (sum to 1)\n        p_local = 0.40\n        p_de = 0.20\n        p_pca = 0.15\n        p_levy = 0.10\n        p_global = 0.15\n\n        # small constants\n        eps = 1e-12\n\n        # main loop\n        while evals < self.budget:\n            # choose move type\n            r = self.rng.rand()\n            if r < p_local:\n                move_type = \"local\"\n            elif r < p_local + p_de:\n                move_type = \"de\"\n            elif r < p_local + p_de + p_pca:\n                move_type = \"pca\"\n            elif r < p_local + p_de + p_pca + p_levy:\n                move_type = \"levy\"\n            else:\n                move_type = \"global\"\n\n            # baseline: best point is exploitation center\n            if (self.x_opt is None) or (self.rng.rand() > 0.85 and len(archive_x) > 0):\n                base = archive_x[self.rng.randint(len(archive_x))]\n            else:\n                base = self.x_opt\n\n            # propose candidate\n            if move_type == \"local\":\n                # Gaussian around base with anisotropic per-dim scale\n                z = self.rng.normal(loc=0.0, scale=1.0, size=self.dim)\n                multi = 0.5 + self.rng.rand(self.dim)  # mild per-dim multiplier\n                cand = np.array(base, dtype=float) + z * (scale * multi)\n\n            elif move_type == \"de\":\n                # DE-style donor using three distinct points\n                if len(archive_x) >= 3:\n                    idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                    a = np.array(archive_x[idxs[0]], dtype=float)\n                    b = np.array(archive_x[idxs[1]], dtype=float)\n                    c = np.array(archive_x[idxs[2]], dtype=float)\n                    F = self.rng.uniform(0.4, 1.0)  # differential weight\n                    donor = a + F * (b - c)\n                    # ensure donor within bounds via reflection\n                    donor = self._reflect_bounds(donor, lb, ub)\n                    # small crossover with base\n                    cr = 0.7\n                    mask = self.rng.rand(self.dim) < cr\n                    # ensure at least one dimension from donor\n                    if not np.any(mask):\n                        mask[self.rng.randint(self.dim)] = True\n                    cand = np.array(base, dtype=float)\n                    cand[mask] = donor[mask]\n                    # add a small gaussian jitter\n                    cand += self.rng.normal(scale=0.05, size=self.dim) * scale\n                else:\n                    # fallback to local\n                    z = self.rng.normal(loc=0.0, scale=1.0, size=self.dim)\n                    cand = np.array(base) + z * scale\n\n            elif move_type == \"pca\":\n                # PCA-guided sampling around elites\n                n_keep = max(1, int(max(1, len(archive_x)) * self.elite_frac))\n                if len(archive_x) <= 2 or n_keep < 1:\n                    # fallback gaussian around base\n                    z = self.rng.normal(size=self.dim)\n                    cand = np.array(base, dtype=float) + z * scale\n                else:\n                    # select elites by best objective\n                    idx_sorted = np.argsort(archive_f)\n                    elite_idx = idx_sorted[:n_keep]\n                    elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                    center = np.mean(elites, axis=0)\n                    # compute principal axes\n                    if elites.shape[0] > 1:\n                        cov = np.cov(elites.T)\n                    else:\n                        cov = np.atleast_2d(np.var(elites, axis=0))\n                    # regularize covariance to avoid singular\n                    cov += np.eye(self.dim) * 1e-8 * span_mean ** 2\n                    try:\n                        U, S_vals, Vt = np.linalg.svd(cov)\n                    except Exception:\n                        # fallback: isotropic\n                        U = np.eye(self.dim)\n                        S_vals = np.ones(self.dim) * (np.mean(span) ** 2)\n                    # coefficients along principal axes\n                    sqrtS = np.sqrt(np.maximum(S_vals, 0.0))\n                    coeffs = self.rng.normal(size=self.dim) * (sqrtS + 1e-8)\n                    # scale by global scale factor and a small random intensity\n                    intensity = self.rng.uniform(0.6, 1.6)\n                    coeffs *= (gscale / (np.mean(sqrtS) + 1e-12)) * intensity\n                    cand = center + (U @ coeffs)\n                    # add small isotropic jitter proportional to scale\n                    cand += self.rng.normal(scale=0.05, size=self.dim) * scale\n\n            elif move_type == \"levy\":\n                # Cauchy (heavy-tailed) jump sampled per-dim, truncated for stability\n                # center is base (usually best)\n                center = np.array(base, dtype=float)\n                # standard Cauchy using tan(pi*(u-0.5))\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                cauchy = np.clip(cauchy, -1e2, 1e2)\n                intensity = self.rng.uniform(0.2, 2.0)\n                cand = center + cauchy * (gscale * intensity * (span / max(span_mean, 1e-12)))\n\n            else:  # global\n                cand = self.rng.uniform(lb, ub)\n\n            # enforce bounds via reflection + final clip\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate (ensure we don't exceed budget)\n            if evals >= self.budget:\n                break\n            f_cand = float(func(cand))\n            evals += 1\n\n            # update archive\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n\n            # update recent success history\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > success_window:\n                recent_success.pop(0)\n\n            # update stagnation counter\n            if improved:\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # Micro-restart (inject diversity) when stagnation grows\n            if stagnation > stagnation_restart_threshold and evals < self.budget:\n                # inject some random points and a few expanded locals\n                n_inject = min(6 + self.dim // 2, max(3, int(0.01 * self.budget)))\n                for k in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.rand() < 0.5:\n                        # pure global random\n                        x_inj = self.rng.uniform(lb, ub)\n                    else:\n                        # around a random elite or best with expanded scale\n                        if len(archive_x) > 0 and self.rng.rand() < 0.8:\n                            center = archive_x[self.rng.randint(len(archive_x))]\n                        else:\n                            center = self.x_opt if self.x_opt is not None else self.rng.uniform(lb, ub)\n                        # large gaussian jitter\n                        x_inj = np.array(center, dtype=float) + self.rng.normal(scale=2.0, size=self.dim) * (scale + 1e-12)\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    f_inj = float(func(x_inj))\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                # slightly increase scale to escape basin\n                gscale *= 1.6\n                # reset stagnation gently\n                stagnation = int(stagnation_restart_threshold * 0.4)\n                recent_success = []\n\n            # Adapt scale after each evaluation (small-step adaptation)\n            if len(recent_success) > 0:\n                succ_rate = float(np.mean(recent_success))\n            else:\n                succ_rate = 0.0\n\n            # rules:\n            # if succ_rate high -> zoom in; if low -> expand a bit\n            if succ_rate > 0.30:\n                gscale *= 0.96  # zoom in\n            elif succ_rate < 0.05:\n                gscale *= 1.04  # expand\n            else:\n                gscale *= 0.997  # mild damping\n\n            # small random jitter\n            gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n            # keep scale within reasonable bounds relative to span\n            gscale = float(np.clip(gscale, 1e-10 * span_mean, 3.0 * span_mean))\n\n            # update per-dim scale as product of base and relative gscale factor (normalized)\n            # we keep scale proportional to base_scale but modulated by gscale\n            scale = base_scale * (gscale / (np.mean(base_scale) + eps))\n\n            # prune memory: keep best K and recent R to limit memory\n            M = max(50, min(self.keep_memory, int(self.budget // 2)))\n            if len(archive_x) > M:\n                best_k = max(10, int(0.1 * M))\n                idx_sorted = np.argsort(archive_f)\n                keep_best_idx = list(idx_sorted[:best_k])\n                # keep most recent others up to M\n                recent_idx = list(range(max(0, len(archive_x) - (M - best_k)), len(archive_x)))\n                keep_idx_set = set(keep_best_idx) | set(recent_idx)\n                keep_idx = sorted(list(keep_idx_set))\n                # compact lists\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # end main loop\n        return self.f_opt, self.x_opt\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.179 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08316318743540818, 0.16470641620946003, 0.2545331414363562, 0.21200781746659025, 0.17048848509306336, 0.2098590520819067, 0.2059693723847873, 0.1961126289754167, 0.1675063113698534, 0.12765629581347526]}, "task_prompt": ""}
{"id": "757f7d7b-3423-4cfd-a695-673f3daeaa97", "fitness": 0.17478820624760916, "name": "HybridADS", "description": "Hybrid Adaptive Directional Sampling (HADS) — combines global uniform exploration, DE-style differential moves, PCA-guided local directional moves, Cauchy heavy-tailed jumps and adaptive scale + micro-restarts, with an evolving elite archive to robustly explore [-5,5]^dim spaces.", "code": "import numpy as np\n\nclass HybridADS:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (HADS)\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed Cauchy jumps and occasional micro-restarts, with adaptive scale control.\n\n    Usage:\n        sampler = HybridADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_init=None, elite_frac=0.2,\n                 init_scale=0.25, seed=None, keep_memory=2000):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_init is None:\n            self.pop_init = int(min(max(10, 4 * self.dim), max(1, self.budget // 4)))\n        else:\n            self.pop_init = int(pop_init)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)  # relative fraction of search span\n        self.seed = seed\n        self.keep_memory = int(keep_memory)\n\n        # placeholders set during run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try common bound attributes, else default [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # object with lb/ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # tuple/list pair\n                    try:\n                        b0, b1 = b\n                        lb = np.asarray(b0, dtype=float)\n                        ub = np.asarray(b1, dtype=float)\n                    except Exception:\n                        lb = None\n        except Exception:\n            lb = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.array(x, dtype=float)\n        # reflect across bounds a few times to handle large overshoots\n        for _ in range(6):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span = np.maximum(span, 1e-12)\n        # per-dim base scale (absolute)\n        base_scale = self.init_scale * span\n        # global scalar scale used for adaptation (absolute)\n        gscale = float(np.mean(base_scale))\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # initial uniform samples\n        n_init = min(self.pop_init, max(1, self.budget // 10))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # ensure at least one evaluated point\n        if len(archive_x) == 0 and evals < self.budget:\n            x = rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            self.f_opt = f\n            self.x_opt = np.array(x, dtype=float)\n\n        # adaptation and control variables\n        recent_success = []\n        success_window = 25\n        stagnation = 0\n        stagnation_restart_threshold = max(75, 8 * self.dim)\n\n        # move probabilities (sum <=1; remainder local moves)\n        p_de = 0.20\n        p_pca = 0.20\n        p_levy = 0.10\n        p_global = 0.10\n        # implicit p_local = 1 - (p_de+p_pca+p_levy+p_global)\n\n        # small constants\n        F_de = 0.6  # differential weight\n        cr_de = 0.7  # crossover prob for DE-style\n        best_k = max(5, int(0.05 * self.budget))  # for pruning\n        best_k = min(best_k, 200)\n\n        # main loop\n        while evals < self.budget:\n            # choose base point (mostly best, sometimes random)\n            if (self.x_opt is None) or (rng.random() > 0.85 and len(archive_x) > 0):\n                base_idx = rng.integers(len(archive_x))\n                base = archive_x[base_idx]\n            else:\n                base = self.x_opt\n\n            r = rng.random()\n            if r < p_global:\n                move_type = \"global\"\n            elif r < p_global + p_de:\n                move_type = \"de\"\n            elif r < p_global + p_de + p_pca:\n                move_type = \"pca\"\n            elif r < p_global + p_de + p_pca + p_levy:\n                move_type = \"cauchy\"\n            else:\n                move_type = \"local\"\n\n            cand = None\n\n            if move_type == \"global\":\n                # pure uniform exploration\n                cand = rng.uniform(lb, ub)\n\n            elif move_type == \"local\":\n                # anisotropic local gaussian centered at base\n                jitter = rng.normal(scale=1.0, size=self.dim)\n                # scale per-dim is base_scale scaled by gscale / mean(base_scale)\n                scale = base_scale * (gscale / max(1e-12, np.mean(base_scale)))\n                # small random factor per-dim\n                mult = 0.4 + rng.random(self.dim) * 1.6\n                cand = np.array(base, dtype=float) + jitter * (scale * mult)\n\n            elif move_type == \"de\":\n                # differential-style donor using two other vectors plus optional interpolation\n                if len(archive_x) >= 3:\n                    idxs = rng.choice(len(archive_x), size=3, replace=False)\n                    # ensure base not the donor (if random base chosen)\n                    if np.all(archive_x[idxs[0]] == base):\n                        # rotate indices if necessary\n                        idxs = np.roll(idxs, 1)\n                    a = np.array(archive_x[idxs[0]], dtype=float)\n                    b = np.array(archive_x[idxs[1]], dtype=float)\n                    c = np.array(archive_x[idxs[2]], dtype=float)\n                    donor = a + F_de * (b - c)\n                else:\n                    # fallback to gaussian jitter\n                    donor = np.array(base, dtype=float) + rng.normal(scale=1.0, size=self.dim) * base_scale\n                # crossover\n                mask = rng.random(self.dim) < cr_de\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small jitter to maintain diversity\n                cand += rng.normal(scale=0.03, size=self.dim) * span * (gscale / max(1e-12, np.mean(span)))\n\n            elif move_type == \"pca\":\n                # PCA-guided sampling around elites\n                n_keep = max(1, int(len(archive_x) * self.elite_frac))\n                sorted_idx = np.argsort(archive_f)\n                elite_idx = sorted_idx[:n_keep]\n                X = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                if X.shape[0] <= 1:\n                    # fallback to local gaussian around base\n                    cand = np.array(base, dtype=float) + rng.normal(scale=1.0, size=self.dim) * (base_scale * (0.5 + rng.random(self.dim)))\n                else:\n                    # center on elite mean\n                    center = np.mean(X, axis=0)\n                    # compute covariance and perform PCA (eig)\n                    Xc = X - center\n                    cov = np.cov(Xc, rowvar=False)\n                    # regularize\n                    cov += np.eye(self.dim) * 1e-8 * np.mean(np.diag(cov))\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # ensure sorted descending\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = np.maximum(eigvals[order], 0.0)\n                        eigvecs = eigvecs[:, order]\n                    except np.linalg.LinAlgError:\n                        eigvecs = np.eye(self.dim)\n                        eigvals = np.ones(self.dim) * (np.mean(base_scale) ** 2)\n                    # sample coefficients along principal axes\n                    coeffs = rng.normal(size=self.dim) * (np.sqrt(eigvals) + 1e-8)\n                    # modulate by global scale and a mild random multiplier\n                    coeffs *= (gscale / max(1e-12, np.sqrt(np.mean(eigvals) + 1e-12))) * (0.5 + rng.random())\n                    cand = center + (eigvecs @ coeffs)\n                    # add small isotropic jitter\n                    cand += rng.normal(scale=0.05, size=self.dim) * base_scale\n\n            elif move_type == \"cauchy\":\n                # heavy-tailed jump from base (Cauchy ~ Levy-like)\n                # sample standard Cauchy with truncation\n                cauchy = rng.standard_cauchy(self.dim)\n                cauchy = np.clip(cauchy, -1e2, 1e2)\n                intensity = 0.6 + rng.random() * 2.0\n                cand = np.array(base, dtype=float) + cauchy * (gscale * intensity)\n                # with small probability, do pure global\n                if rng.random() < 0.08:\n                    cand = rng.uniform(lb, ub)\n\n            # enforce bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate (guard budget)\n            if evals >= self.budget:\n                break\n            f_cand = float(func(cand))\n            evals += 1\n\n            # store\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(f_cand)\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = f_cand\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # update recent success history\n            recent_success.append(1.0 if improved else 0.0)\n            if len(recent_success) > success_window:\n                recent_success.pop(0)\n\n            # Micro-restart: inject diversity if stagnating\n            if stagnation > stagnation_restart_threshold and evals < self.budget:\n                n_inject = min(5 + self.dim // 2, max(3, int(0.02 * self.budget)))\n                # do not exceed remaining budget\n                n_inject = min(n_inject, self.budget - evals)\n                for _ in range(n_inject):\n                    if rng.random() < 0.6:\n                        x_inj = rng.uniform(lb, ub)\n                    else:\n                        # around a random archived good point\n                        if len(archive_x) > 0:\n                            center = archive_x[rng.integers(len(archive_x))]\n                        else:\n                            center = rng.uniform(lb, ub)\n                        x_inj = np.array(center, dtype=float) + rng.normal(scale=2.0, size=self.dim) * base_scale\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    # evaluate\n                    if evals >= self.budget:\n                        break\n                    f_inj = float(func(x_inj))\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(f_inj)\n                    if f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = np.array(x_inj, dtype=float)\n                # expand exploration scale to escape basin\n                gscale *= 1.5\n                # reset stagnation and recent successes\n                stagnation = 0\n                recent_success = []\n\n            # Adapt global scale based on recent success rate\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n            if succ_rate > 0.30:\n                gscale *= 0.98  # zoom in\n            elif succ_rate < 0.05:\n                gscale *= 1.03  # expand\n            # small random jitter to avoid lock\n            gscale *= (1.0 + 0.01 * (rng.random() - 0.5))\n            # keep gscale within reasonable bounds relative to span\n            gscale = float(np.clip(gscale, 1e-8 * np.mean(span), 2.0 * np.mean(span)))\n            # update per-dim base scale (preserve relative per-dim)\n            base_scale = np.maximum(1e-16, (self.init_scale * span) * (gscale / max(1e-12, np.mean(self.init_scale * span))))\n\n            # prune memory to keep archives manageable\n            M = max(200, min(self.keep_memory, int(self.budget // 2)))\n            if len(archive_x) > M:\n                idx_sorted = np.argsort(archive_f)\n                keep_best_idx = list(idx_sorted[:best_k])\n                recent_idx = list(range(max(0, len(archive_x) - (M - len(keep_best_idx))), len(archive_x)))\n                keep_idx = sorted(set(keep_best_idx).union(recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final best\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridADS scored 0.175 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09895680235676774, 0.1554628691503328, 0.23574269885318067, 0.18324977915179363, 0.1655442262553568, 0.2146899200762622, 0.20044370514300525, 0.19228898781409964, 0.16557374960867344, 0.13592932406661928]}, "task_prompt": ""}
{"id": "b0a416a9-a20b-4a43-9c44-77144048b18a", "fitness": 0.15126426825654593, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — combines global uniform exploration, DE-style donors, PCA-guided directional local proposals, heavy-tailed Cauchy jumps, adaptive global scale and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Adaptive Directional Sampling (ADS)\n\n    Hybrid heuristic combining:\n      - global uniform exploration,\n      - Differential-Evolution-style donor moves,\n      - PCA-guided directional local proposals (using an elite archive),\n      - occasional heavy-tailed (Cauchy/Levy-like) jumps,\n      - adaptive scalar scaling driven by recent success rate,\n      - micro-restarts (injections) when stagnation is detected.\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, elite_frac=0.2,\n                 init_scale=0.2, seed=None, keep_memory=1000):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        seed: RNG seed\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.rng = np.random.RandomState(seed)\n        self.keep_memory = max(100, int(keep_memory))  # avoid tiny memory\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        # Try multiple common formats, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # object with bounds attribute\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            # try object with lb/ub\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                try:\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n            else:\n                # maybe tuple/list (lb, ub)\n                try:\n                    if isinstance(b, (tuple, list)) and len(b) == 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n        # fallback default box\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.astype(float).ravel()\n        ub = ub.astype(float).ravel()\n        # safety shape\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # reflect (fold) across bounds repeatedly; final clamp as safety\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below: lb + (lb - x)\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 1.0  # guard against degenerate\n        # per-dim base scale\n        base_scale_vec = self.init_scale * span\n        # adaptive scalar multiplier\n        gscale = 1.0\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        evals = 0\n        eps = 1e-12\n\n        # initial population (use some of budget but keep room)\n        n_init = min(max(10, 4 * self.dim), max(1, self.budget // 10))\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n            if evals >= self.budget:\n                return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n        # history & adaptation\n        recent_success = []  # booleans\n        recent_window = max(30, 5 * self.dim)\n        stagnation = 0\n        stagnation_restart_threshold = max(50, 10 * self.dim)\n\n        # move probabilities\n        p_local = 0.40\n        p_de = 0.20\n        p_pca = 0.20\n        p_levy = 0.10\n        p_global = 0.10\n\n        # main loop\n        while evals < self.budget:\n            # choose base (mostly best, sometimes random)\n            use_best_as_base = (self.x_opt is not None) and (self.rng.rand() < 0.85)\n            if use_best_as_base:\n                base = np.array(self.x_opt, dtype=float)\n            else:\n                base = np.array(archive_x[self.rng.randint(len(archive_x))], dtype=float)\n\n            # adaptive per-dim scale vector\n            scale = np.maximum(1e-12, base_scale_vec * gscale)\n\n            r = self.rng.rand()\n            if r < p_local:\n                move_type = \"local\"\n            elif r < p_local + p_de:\n                move_type = \"de\"\n            elif r < p_local + p_de + p_pca:\n                move_type = \"pca\"\n            elif r < p_local + p_de + p_pca + p_levy:\n                move_type = \"levy\"\n            else:\n                move_type = \"global\"\n\n            cand = None\n\n            if move_type == \"local\":\n                # anisotropic gaussian around base (per-dim scale)\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                # random directional multiplier per-dim for exploration\n                rand_mult = 0.75 + 0.5 * self.rng.rand(self.dim)\n                cand = base + jitter * scale * rand_mult\n\n            elif move_type == \"de\":\n                # DE/rand/1 style donor with small crossover to base\n                # pick three distinct vectors from archive (fallback to random uniform if not enough)\n                if len(archive_x) >= 3:\n                    # pick distinct indices not equal to some special reserved index\n                    idxs = list(range(len(archive_x)))\n                    # ensure diversity: try to avoid picking the base index if base was chosen from archive\n                    for _trial in range(10):\n                        a_i, b_i, c_i = self.rng.choice(idxs, size=3, replace=False)\n                        a = np.array(archive_x[a_i], dtype=float)\n                        b = np.array(archive_x[b_i], dtype=float)\n                        c = np.array(archive_x[c_i], dtype=float)\n                        if not (np.all(a == b) or np.all(b == c) or np.all(a == c)):\n                            break\n                    else:\n                        a = np.array(archive_x[self.rng.randint(len(archive_x))], dtype=float)\n                        b = np.array(archive_x[self.rng.randint(len(archive_x))], dtype=float)\n                        c = np.array(archive_x[self.rng.randint(len(archive_x))], dtype=float)\n                    F = 0.6 + 0.4 * self.rng.rand()  # in [0.6,1.0]\n                    donor = a + F * (b - c)\n                    # crossover\n                    cr = 0.7\n                    mask = self.rng.rand(self.dim) < cr\n                    cand = np.array(base, dtype=float)\n                    cand[mask] = donor[mask]\n                    # small gaussian jitter to donor parts\n                    cand += self.rng.normal(0.0, 0.05, size=self.dim) * scale\n                else:\n                    # fallback to local gaussian\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n\n            elif move_type == \"pca\":\n                # PCA-guided sampling using elites\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                n_keep = min(n_keep, len(archive_x))\n                if n_keep < 2:\n                    # fallback\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n                else:\n                    sorted_idx = np.argsort(archive_f)\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.vstack([archive_x[i] for i in elite_idx])\n                    # choose center: sometimes the mean, sometimes the best elite\n                    if self.rng.rand() < 0.6:\n                        center = np.mean(elites, axis=0)\n                    else:\n                        center = elites[0].copy()\n                    # compute covariance of elites\n                    if elites.shape[0] > 1:\n                        cov = np.cov(elites.T)\n                    else:\n                        cov = np.atleast_2d(np.var(elites, axis=0) + 1e-12)\n                    # regularize\n                    cov += np.eye(self.dim) * (1e-8 * (np.mean(span) ** 2))\n                    try:\n                        U, Svals, _ = np.linalg.svd(cov)\n                    except np.linalg.LinAlgError:\n                        # fallback isotropic\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        cand = center + jitter * scale\n                    else:\n                        # sample coefficients along principal axes scaled by eigenvalues\n                        coeffs = self.rng.normal(size=self.dim) * (np.sqrt(np.maximum(Svals, 0.0)) + 1e-12)\n                        # global intensity relative to mean eigenvalue\n                        mean_root = max(1e-8, np.mean(np.sqrt(np.maximum(Svals, 0.0))))\n                        gmult = (gscale * (0.8 + 0.8 * self.rng.rand())) / mean_root\n                        coeffs = coeffs * gmult\n                        cand = center + (U @ coeffs)\n                        # add small isotropic jitter\n                        cand += self.rng.normal(scale=0.08, size=self.dim) * scale\n\n            elif move_type == \"levy\":\n                # heavy-tailed per-dim Cauchy jumps around base\n                center = base.copy()\n                # standard Cauchy sampling via tan(pi*(u-0.5))\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # scale intensity relative to span and gscale\n                intensity = 0.08 + 0.4 * self.rng.rand()\n                cand = center + cauchy * (span * intensity) * gscale\n                # truncate extremely large jumps for numerical stability then reflect\n                cand = np.clip(cand, center - 50 * span, center + 50 * span)\n\n            else:  # global\n                cand = self.rng.uniform(lb, ub)\n\n            # ensure candidate is finite and reflect/clip into bounds\n            cand = np.asarray(cand, dtype=float)\n            cand[~np.isfinite(cand)] = lb[~np.isfinite(cand)] + self.rng.rand() * span[~np.isfinite(cand)]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if evals >= self.budget:\n                break\n            f_cand = float(func(cand))\n            evals += 1\n\n            # update archive & best\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(f_cand)\n\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = f_cand\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n\n            # update recent success history\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n\n            # stagnation management\n            if improved:\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # Micro-restart: inject diversity when stagnating\n            if stagnation > stagnation_restart_threshold and evals < self.budget:\n                n_inject = min(self.budget - evals, max(3, min(5 + self.dim // 2, int(0.02 * self.budget))))\n                # moderately increase scale to escape basin temporarily\n                old_gscale = gscale\n                gscale *= 1.8 + 0.5 * self.rng.rand()\n                for _inj in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.rand() < 0.6:\n                        # global random injection\n                        x_inj = self.rng.uniform(lb, ub)\n                    else:\n                        # local around a random elite or best\n                        if len(archive_x) > 0 and self.rng.rand() < 0.8:\n                            best_idx = np.argmin(archive_f)\n                            # pick among some top few\n                            ktop = min(5, len(archive_x))\n                            choice_idx = self.rng.randint(0, ktop)\n                            center = np.array(archive_x[choice_idx], dtype=float)\n                        else:\n                            center = self.rng.uniform(lb, ub)\n                        x_inj = center + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * gscale * (1.5 + self.rng.rand()))\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    f_inj = float(func(x_inj))\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(f_inj)\n                    if f_inj < self.f_opt - 1e-12:\n                        self.f_opt = f_inj\n                        self.x_opt = np.array(x_inj, dtype=float)\n                    if evals >= self.budget:\n                        break\n                # gentle reset\n                stagnation = int(stagnation * 0.3)\n                # restore scale but allow some retained expansion\n                gscale = max(1e-6, min(1e6, old_gscale * (1.0 + 0.3 * self.rng.rand())))\n\n            # adapt global scale based on short-term success rate\n            if len(recent_success) > 0:\n                succ_rate = float(np.mean(recent_success))\n            else:\n                succ_rate = 0.0\n            if succ_rate > 0.30:\n                gscale *= 0.97  # zoom in\n            elif succ_rate < 0.05:\n                gscale *= 1.03  # expand\n            else:\n                gscale *= 0.995  # slight damping\n            # small multiplicative jitter\n            gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n            # clip gscale\n            gscale = float(np.clip(gscale, 1e-6, 1e6))\n\n            # prune memory periodically\n            if len(archive_x) > 2 * self.keep_memory:\n                # keep best few and recent others\n                M = self.keep_memory\n                best_k = max(5, int(0.2 * M))\n                recent_m = max(0, M - best_k)\n                best_idx = np.argsort(archive_f)[:best_k].tolist()\n                recent_idx = list(range(max(0, len(archive_x) - recent_m), len(archive_x)))\n                keep_idx = sorted(set(best_idx).union(recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return\n        if self.x_opt is None:\n            # no evaluations? shouldn't happen, but guard\n            x_try = self.rng.uniform(lb, ub)\n            self.x_opt = x_try\n            self.f_opt = float(func(x_try))\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.151 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06958157352399774, 0.1448767489007069, 0.20889205444696535, 0.15553062412160334, 0.14410665340692952, 0.17621405944765722, 0.1757119520986642, 0.17166989960109047, 0.14153653443951086, 0.12452258257833382]}, "task_prompt": ""}
{"id": "0d326393-14b9-42a6-ab19-4a7aaeed7087", "fitness": 0.2682168275090407, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — a mixed-strategy continuous optimizer that blends global uniform exploration, DE-style donor moves, PCA-guided local proposals on an elite archive, heavy-tailed Cauchy jumps, adaptive global scaling and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, elite_frac=0.20, init_scale=0.20,\n                 seed=None, keep_memory=1000, F=0.8, cr=0.7):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        seed: RNG seed\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        F: DE differential factor\n        cr: DE crossover rate\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.seed = seed\n        self.keep_memory = max(100, int(keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # RNG\n        self.rng = np.random.default_rng(self.seed)\n\n    def _extract_bounds(self, func):\n        # Try common formats, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            try:\n                # tuple/list (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                # object with lb/ub attributes\n                elif hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # fallback default box\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast scalar bounds\n        lb = np.asarray(lb, dtype=float).ravel()\n        ub = np.asarray(ub, dtype=float).ravel()\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        if lb.size != self.dim or ub.size != self.dim:\n            # try to broadcast or truncate/pad\n            lb = np.resize(lb, self.dim)\n            ub = np.resize(ub, self.dim)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        # elementwise uniform between lb and ub\n        u = self.rng.random(self.dim)\n        return lb + u * (ub - lb)\n\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(1e-12, self.init_scale * span)\n\n        # Archives\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n        # initial sampling\n        n_init = min(max(20, 5 * self.dim), max(1, self.budget // 10))\n        n_init = max(1, n_init)\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            x = self._uniform_array(lb, ub)\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n            if evals >= self.budget:\n                return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n        # adaptation & history\n        recent_window = max(30, 5 * self.dim)\n        recent_success = []\n        gscale = 1.0  # global multiplicative scale\n        stagnation_counter = 0\n        stagnation_limit = max(40, 10 * self.dim)\n        # move probabilities\n        p_local = 0.60\n        p_de = 0.20\n        p_pca = 0.15\n        p_levy = 0.05\n\n        # main loop\n        while evals < self.budget:\n            # choose base (mostly best, sometimes random)\n            use_best_as_base = (self.x_opt is not None) and (self.rng.random() < 0.85)\n            if use_best_as_base:\n                base = np.array(self.x_opt, dtype=float)\n            else:\n                base = np.array(archive_x[self.rng.integers(len(archive_x))], dtype=float)\n\n            # adaptive per-dim scale vector\n            scale = np.maximum(1e-12, base_scale_vec * gscale)\n\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                rand_mult = 0.75 + 0.5 * self.rng.random(self.dim)\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult)\n\n            # DE-style donor move with small crossover\n            elif r < p_local + p_de:\n                if len(archive_x) >= 3:\n                    # pick three distinct indices\n                    ids = list(range(len(archive_x)))\n                    # avoid picking the index of base if base was sampled from archive_x (approx)\n                    # simply pick three distinct at random\n                    a_i, b_i, c_i = self.rng.choice(len(archive_x), size=3, replace=False)\n                    a = np.array(archive_x[a_i], dtype=float)\n                    b = np.array(archive_x[b_i], dtype=float)\n                    c = np.array(archive_x[c_i], dtype=float)\n                    donor = a + self.F * (b - c)\n                    # crossover\n                    jrand = self.rng.integers(self.dim)\n                    mask = (self.rng.random(self.dim) < self.cr)\n                    mask[jrand] = True\n                    cand = np.where(mask, donor, base)\n                    # small gaussian jitter proportional to scale\n                    cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n                else:\n                    # fallback to local\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca:\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                sorted_idx = np.argsort(archive_f)\n                n_keep = min(n_keep, len(sorted_idx))\n                if n_keep < 2:\n                    # fallback local\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n                else:\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.vstack([archive_x[i] for i in elite_idx])\n                    # center: either mean or best elite\n                    if self.rng.random() < 0.6:\n                        center = np.mean(elites, axis=0)\n                    else:\n                        center = elites[0].copy()\n                    # covariance and SVD\n                    cov = np.cov(elites.T)\n                    # regularize\n                    cov = cov + 1e-8 * np.eye(self.dim)\n                    try:\n                        U, Svals, _ = np.linalg.svd(cov)\n                    except Exception:\n                        # fallback isotropic\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        cand = center + jitter * scale\n                    else:\n                        # sample along principal components\n                        # coefficients scaled by sqrt(eigenvalues) and gscale\n                        sqrt_eigs = np.sqrt(np.maximum(Svals, 0.0))\n                        mean_root = max(1e-8, np.mean(sqrt_eigs))\n                        gmult = (gscale * (0.8 + 0.8 * self.rng.random())) / mean_root\n                        coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * sqrt_eigs\n                        coeffs = coeffs * gmult\n                        cand = center + (U @ coeffs)\n                        # add small isotropic jitter\n                        cand += self.rng.normal(0.0, 1.0, size=self.dim) * (0.02 * scale)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy via tan(pi*(u-0.5))\n                u = self.rng.random(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails\n                cauchy = np.clip(cauchy, -1e2, 1e2)\n                intensity = 0.08 + 0.4 * self.rng.random()\n                cand = base + (intensity * span * gscale) * cauchy\n                # small damping jitter\n                cand += self.rng.normal(0.0, 0.01, size=self.dim) * scale\n\n            # Ensure candidate finite and within bounds (reflect)\n            cand = np.array(cand, dtype=float)\n            # replace any non-finite dims with random in bound for that dim\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.sum(bad)) * span[bad]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = float(func(cand))\n            evals += 1\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(f_cand)\n\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = f_cand\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n\n            # update recent success window\n            recent_success.append(bool(improved))\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n\n            # adapt gscale based on short-term success rate\n            succ_rate = float(np.mean(recent_success)) if recent_success else 0.0\n            # If success rate high -> gently increase; if low -> decrease\n            if succ_rate > 0.25:\n                gscale *= (1.0 + 0.12 * min(1.0, (succ_rate - 0.25) / 0.75))\n            elif succ_rate < 0.05:\n                gscale *= 0.90\n            else:\n                # small damping to keep stable\n                gscale *= 0.995\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.01 * (self.rng.random() - 0.5))\n            # clip gscale to reasonable range\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if improved:\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            if stagnation_counter >= stagnation_limit or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                # perform modest injection batch\n                n_inject = min(10, max(3, self.dim // 2))\n                # temporarily expand gscale\n                old_gscale = gscale\n                gscale *= (1.6 + 0.8 * self.rng.random())\n                for _inj in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.random() < 0.6 or len(archive_x) < 5:\n                        # global injection\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # local injection around a top elite or best\n                        ktop = max(1, int(0.05 * len(archive_x)))\n                        ktop = min(ktop, len(archive_x))\n                        top_idx = np.argsort(archive_f)[:ktop]\n                        center = archive_x[self.rng.choice(top_idx)]\n                        # local Gaussian around chosen center with larger scale\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        x_inj = center + jitter * (base_scale_vec * (1.0 + 0.5 * self.rng.random()))\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    f_inj = float(func(x_inj))\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(f_inj)\n                    if f_inj < self.f_opt - 1e-12:\n                        self.f_opt = f_inj\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        # accept early break to exploit improvement\n                        break\n                # gentle reset of scale\n                gscale = min(old_gscale * 1.2, gscale)\n                stagnation_counter = 0\n\n            # prune memory periodically\n            if len(archive_x) > 2 * self.keep_memory:\n                M = self.keep_memory\n                best_k = max(5, int(0.2 * M))\n                recent_m = max(0, M - best_k)\n                sorted_idx = np.argsort(archive_f)\n                keep_idx = list(sorted_idx[:best_k])\n                # append most recent ones (avoid duplicates)\n                recent_indices = list(range(max(0, len(archive_x) - recent_m), len(archive_x)))\n                for idx in recent_indices:\n                    if idx not in keep_idx:\n                        keep_idx.append(idx)\n                # rebuild archives\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n            # safety guard: should not exceed budget; loop will exit appropriately\n            # continue main loop\n\n        # final return\n        if self.x_opt is None:\n            # degenerate: return random point\n            x_try = self._uniform_array(lb, ub)\n            f_try = float(func(x_try))\n            return float(f_try), np.array(x_try, dtype=float)\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.268 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10370527415793895, 0.16134516631397122, 0.33564955526966467, 0.5142532002184186, 0.23814492411714216, 0.4336202820981394, 0.2606531709800064, 0.2806897757689706, 0.21125118146536814, 0.14285574470078688]}, "task_prompt": ""}
{"id": "d8cdfa81-32b4-4685-acf3-60df8a2daed2", "fitness": 0.1737639747325411, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes DE-style donors, PCA-guided local sampling on an elite archive, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.20, init_scale=0.20,\n                 keep_memory=400, F=0.8, cr=0.2,\n                 seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        F: DE differential factor\n        cr: DE crossover rate\n        seed: RNG seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.keep_memory = int(max(10, keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        # Try common formats, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            try:\n                # tuple/list (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) >= 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # object with .lb and .ub\n                    lb_attr = getattr(b, \"lb\", None)\n                    ub_attr = getattr(b, \"ub\", None)\n                    if lb_attr is not None and ub_attr is not None:\n                        lb = np.asarray(lb_attr, dtype=float)\n                        ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                lb = ub = None\n\n        # fallback default box [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast scalar bounds if necessary\n            try:\n                lb = np.broadcast_to(lb, (self.dim,)).astype(float).copy()\n            except Exception:\n                lb = np.resize(lb, self.dim).astype(float).copy()\n            try:\n                ub = np.broadcast_to(ub, (self.dim,)).astype(float).copy()\n            except Exception:\n                ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub, replace with [-5,5]\n        invalid = np.any(lb >= ub)\n        if invalid:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # reflect values outside bounds back into box symmetrically a few times\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: lb + (lb - x) = 2*lb - x\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above: ub - (x - ub) = 2*ub - x\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        u = self.rng.random(self.dim)\n        return lb + u * (ub - lb)\n\n    def __call__(self, func):\n        # bounds and scale\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(1e-12, self.init_scale * span)\n\n        # Archives\n        archive_x = []  # list of numpy arrays\n        archive_f = []\n\n        # initial sampling budget: ensure some initial diversity\n        n_init = int(min(max(10, 5 * self.dim), max(1, self.budget // 10)))\n        evals = 0\n\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                # if func crashes, treat as bad\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_window = max(30, 5 * self.dim)\n        recent_success = []  # list of 0/1\n        stagnation_limit = max(40, 10 * self.dim)\n        stagnation_counter = 0\n        # move probabilities\n        p_de = 0.18\n        p_pca = 0.25\n        p_local = 0.42  # rest is cauchy\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # bookkeeping for pruning\n        keep_memory = max(self.keep_memory, 50 + 10 * self.dim)\n\n        # main loop\n        while evals < self.budget:\n            # compute sorted indices by fitness (lower is better)\n            sorted_idx = np.argsort(archive_f) if len(archive_f) > 0 else np.array([], dtype=int)\n\n            # choose base (mostly best, sometimes random)\n            if (self.x_opt is not None) and (self.rng.random() < 0.85):\n                base = np.array(self.x_opt, dtype=float)\n                base_from_archive = False\n            else:\n                if len(archive_x) == 0:\n                    base = self._uniform_array(lb, ub)\n                    base_from_archive = False\n                else:\n                    idx = self.rng.integers(0, len(archive_x))\n                    base = np.array(archive_x[idx], dtype=float)\n                    base_from_archive = True\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            scale = base_scale_vec * gscale\n            scale = np.maximum(scale, 1e-12)\n\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropic multipliers to allow some dimensions to be larger\n                rand_mult = 0.6 + 1.2 * self.rng.random(self.dim)\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult)\n\n            # DE-style donor move with small crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # pick three distinct indices\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                a = np.array(archive_x[idxs[0]], dtype=float)\n                b = np.array(archive_x[idxs[1]], dtype=float)\n                c = np.array(archive_x[idxs[2]], dtype=float)\n                donor = a + self.F * (b - c)\n                # crossover mask (ensure at least one dim taken)\n                mask = (self.rng.random(self.dim) < self.cr)\n                if not mask.any():\n                    mask[self.rng.integers(self.dim)] = True\n                cand = base.copy()\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 2:\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                # ensure sensible\n                n_keep = min(n_keep, len(archive_x))\n                if n_keep < 2:\n                    # fallback local\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n                else:\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                    # center: either mean or best elite (randomized)\n                    if self.rng.random() < 0.5:\n                        center = np.mean(elites, axis=0)\n                    else:\n                        center = elites[0].copy()  # best\n                    # covariance and SVD\n                    # regularize small, use rowvar=False for observations in rows\n                    if elites.shape[0] >= 2:\n                        cov = np.cov(elites, rowvar=False)\n                        # regularize\n                        cov += np.eye(self.dim) * 1e-8 * (np.mean(np.diag(cov)) + 1e-12)\n                        try:\n                            U, Svals, _ = np.linalg.svd(cov, full_matrices=False)\n                            # construct random coefficients along principal directions\n                            # sample N(0, sqrt(eig)) per principal axis\n                            sqrt_vals = np.sqrt(np.maximum(Svals, 0.0))\n                            coeffs = self.rng.normal(0.0, 1.0, size=len(Svals)) * sqrt_vals\n                            # scale coefficients by global gscale and by mean span to be comparable to scale\n                            mean_root = max(1e-12, np.mean(np.sqrt(np.maximum(Svals, 1e-12))))\n                            gmult = (gscale * (0.8 + 0.8 * self.rng.random())) / mean_root\n                            coeffs = coeffs * gmult\n                            # build candidate\n                            cand = center + (U @ coeffs)\n                            # add small isotropic jitter\n                            cand += self.rng.normal(0.0, 0.05, size=self.dim) * scale\n                        except Exception:\n                            jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                            cand = center + jitter * scale\n                    else:\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        cand = center + jitter * scale\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy via tan(pi*(u-0.5))\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails to reduce catastrophic jumps\n                cauch = np.clip(cauch, -50.0, 50.0)\n                # scale by per-dim scale and gscale with some randomness\n                mult = (0.6 + 1.4 * self.rng.random(self.dim)) * gscale\n                cand = base + cauch * (scale * 0.6 * mult)\n                # small damping jitter\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # Ensure candidate finite and within bounds (reflect)\n            # replace any non-finite dims with random in bound for that dim\n            cand = np.array(cand, dtype=float)\n            nonfinite = ~np.isfinite(cand)\n            if nonfinite.any():\n                cand[nonfinite] = self._uniform_array(lb, ub)[nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            improved = False\n            # update bests & archives\n            archive_x.append(cand.copy())\n            archive_f.append(f_cand)\n            if f_cand < self.f_opt - 1e-12:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # update recent success window\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                # drop oldest\n                recent_success.pop(0)\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            if succ_rate > 0.25:\n                gscale *= (1.0 + 0.12 * min(1.0, (succ_rate - 0.25) / 0.75))\n            elif succ_rate < 0.05:\n                gscale *= 0.995\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            # clip gscale to reasonable range\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter >= stagnation_limit or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                # perform modest injection batch\n                n_inject = min( max(3, 2*self.dim), max(1, (self.budget - evals) // 5) )\n                if n_inject <= 0:\n                    n_inject = 0\n                else:\n                    old_gscale = gscale\n                    gscale *= (1.0 + 0.5 * self.rng.random())  # temporarily expand\n                    for _inj in range(n_inject):\n                        if evals >= self.budget:\n                            break\n                        if self.rng.random() < 0.6 or len(archive_x) < 5:\n                            # global injection\n                            x_inj = self._uniform_array(lb, ub)\n                        else:\n                            # local injection around a top elite or best\n                            top_k = min( max(2, int(0.1 * len(archive_x))), len(archive_x))\n                            top_idx = sorted_idx[:top_k] if len(archive_x) > 0 else np.array([], dtype=int)\n                            if len(top_idx) == 0:\n                                x_inj = self._uniform_array(lb, ub)\n                            else:\n                                center = np.array(archive_x[self.rng.choice(top_idx)], dtype=float)\n                                # local Gaussian around chosen center with larger scale\n                                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                                x_inj = center + jitter * (scale * (2.0 + self.rng.random()))\n                        # reflect & ensure finite\n                        x_inj = np.array(x_inj, dtype=float)\n                        nonfinite = ~np.isfinite(x_inj)\n                        if nonfinite.any():\n                            x_inj[nonfinite] = self._uniform_array(lb, ub)[nonfinite]\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                        try:\n                            f_inj = float(func(x_inj))\n                        except Exception:\n                            f_inj = np.inf\n                        evals += 1\n                        archive_x.append(x_inj.copy())\n                        archive_f.append(float(f_inj))\n                        if f_inj < self.f_opt - 1e-12:\n                            self.x_opt = np.array(x_inj, dtype=float)\n                            self.f_opt = float(f_inj)\n                            improved = True\n                            stagnation_counter = 0\n                    # gentle reset of scale\n                    gscale = min(old_gscale * 1.2, gscale)\n\n            # prune memory periodically\n            if len(archive_x) > keep_memory * 1.2:\n                # keep best ones and some recent ones\n                sorted_idx = np.argsort(archive_f)\n                n_keep_best = max(int(0.6 * keep_memory), keep_memory // 2)\n                n_keep_best = min(n_keep_best, len(sorted_idx))\n                best_idx = sorted_idx[:n_keep_best].tolist()\n                # keep some of most recent entries\n                recent_cnt = keep_memory - len(best_idx)\n                if recent_cnt > 0:\n                    recent_idx = list(range(max(0, len(archive_x) - recent_cnt), len(archive_x)))\n                else:\n                    recent_idx = []\n                keep_idx = sorted(set(best_idx + recent_idx))\n                # rebuild archives\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n            # safety guard: should not exceed budget; loop will exit naturally\n\n        # final return\n        if self.x_opt is None:\n            # degenerate: return random point (call function if we still have budget)\n            x_try = self._uniform_array(lb, ub)\n            if evals < self.budget:\n                try:\n                    f_try = float(func(x_try))\n                except Exception:\n                    f_try = np.inf\n                evals += 1\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = float(f_try)\n            else:\n                # no evaluation possible, return zero vector\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.174 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10704427062488375, 0.15147155164009596, 0.22175911474899102, 0.14566987654529517, 0.17074932188929703, 0.21773859854613242, 0.19708229136170663, 0.2114853686056194, 0.18096868499203378, 0.1336706683713561]}, "task_prompt": ""}
{"id": "756b64f7-aa54-40c8-acc3-31ea953adf15", "fitness": 0.16130493935488183, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes DE-style donors, PCA-guided local sampling on an elite archive, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.20, init_scale=0.20,\n                 keep_memory=400, F=0.8, cr=0.2, seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        F: DE differential factor\n        cr: DE crossover rate\n        seed: RNG seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.F = float(F)\n        self.cr = float(cr)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.keep_memory = int(keep_memory)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        # Try several common attribute names or tuple/list, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # bounds may have .lb/.ub or be a tuple/list\n            lb_attr = getattr(b, \"lb\", None)\n            ub_attr = getattr(b, \"ub\", None)\n            if lb_attr is not None and ub_attr is not None:\n                lb = np.asarray(lb_attr, dtype=float)\n                ub = np.asarray(ub_attr, dtype=float)\n            else:\n                # b might be tuple/list like (lb, ub)\n                try:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # direct attributes\n        if lb is None or ub is None:\n            lb_attr = getattr(func, \"lb\", None)\n            ub_attr = getattr(func, \"ub\", None)\n            if lb_attr is not None and ub_attr is not None:\n                lb = np.asarray(lb_attr, dtype=float)\n                ub = np.asarray(ub_attr, dtype=float)\n\n        # fallback: try single tuple return from func.bounds() style\n        if lb is None or ub is None:\n            try:\n                b2 = func.bounds()\n                lb = np.asarray(b2[0], dtype=float)\n                ub = np.asarray(b2[1], dtype=float)\n            except Exception:\n                pass\n\n        # Final fallback default box [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast scalar bounds if necessary\n            try:\n                lb = np.broadcast_to(lb, (self.dim,)).astype(float).copy()\n                ub = np.broadcast_to(ub, (self.dim,)).astype(float).copy()\n            except Exception:\n                lb = np.full(self.dim, -5.0, dtype=float)\n                ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure valid: if any lb >= ub, replace with [-5,5]\n        bad = np.where(lb >= ub)[0]\n        if bad.size > 0:\n            lb[bad] = -5.0\n            ub[bad] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # reflect values outside bounds back into box symmetrically up to max_reflect times\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: lb + (lb - x) = 2*lb - x\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above: ub - (x - ub) = 2*ub - x\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        u = self.rng.random(self.dim)\n        return lb + u * (ub - lb)\n\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1.0  # avoid zeros\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n        # initial sampling budget: ensure some initial diversity\n        n_init = int(min(max(10, 5 * self.dim), max(1, self.budget // 10)))\n        n_init = max(1, n_init)\n\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n                if not np.isfinite(f):\n                    f = np.inf\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_window = max(30, 5 * self.dim)\n        recent_success = []\n        stagnation_counter = 0\n        stagnation_limit = max(40, 10 * self.dim)\n        n_inject = max(3, self.dim // 2)\n\n        # move probabilities baseline\n        p_local = 0.45\n        p_de = 0.15\n        p_pca = 0.25\n        p_cauchy = 0.15\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # bookkeeping for pruning\n        keep_memory = max(50, self.keep_memory)\n\n        # main loop\n        while evals < self.budget:\n            # compute sorted indices by fitness (lower is better)\n            if len(archive_f) == 0:\n                base = self._uniform_array(lb, ub)\n            else:\n                idx_sorted = np.argsort(archive_f)\n                best_idx = idx_sorted[0]\n                # choose base (mostly best, sometimes random)\n                if self.rng.random() < 0.85:\n                    base = np.array(archive_x[best_idx], dtype=float)\n                    base_from_archive = True\n                else:\n                    r = self.rng.integers(len(archive_x))\n                    base = np.array(archive_x[r], dtype=float)\n                    base_from_archive = True\n\n            # adaptive per-dim scale vector\n            scale = self.init_scale * span * (0.5 + self.rng.random(self.dim))\n            # small jitter\n            scale *= (1.0 + 0.2 * (self.rng.random(self.dim) - 0.5))\n\n            # choose move type probabilistically\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                anis = 0.2 + 2.0 * self.rng.random(self.dim)  # allow some dims bigger\n                jitter = self.rng.normal(scale=1.0, size=self.dim)\n                cand = base + jitter * (scale * anis) * gscale\n\n            # DE-style donor move with small crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # pick three distinct indices different from the chosen base index if possible\n                idxs = list(range(len(archive_x)))\n                if base_from_archive and len(idxs) > 0:\n                    # try to avoid picking the same as base\n                    try:\n                        idxs.remove(best_idx)\n                    except Exception:\n                        pass\n                if len(idxs) < 3:\n                    # fallback to random local\n                    jitter = self.rng.normal(scale=1.0, size=self.dim)\n                    cand = base + jitter * scale * gscale\n                else:\n                    pick = self.rng.choice(idxs, size=3, replace=False)\n                    a = np.array(archive_x[pick[0]], dtype=float)\n                    b = np.array(archive_x[pick[1]], dtype=float)\n                    c = np.array(archive_x[pick[2]], dtype=float)\n                    donor = a + self.F * (b - c)\n                    # crossover mask (ensure at least one dim taken)\n                    mask = self.rng.random(self.dim) < self.cr\n                    if not mask.any():\n                        mask[self.rng.integers(self.dim)] = True\n                    cand = np.array(base, dtype=float)\n                    cand[mask] = donor[mask]\n                    # small gaussian jitter proportional to scale\n                    cand += self.rng.normal(scale=1.0, size=self.dim) * (0.2 * scale) * gscale\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 3:\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                if n_keep < 2:\n                    # fallback local\n                    jitter = self.rng.normal(scale=1.0, size=self.dim)\n                    cand = base + jitter * scale * gscale\n                else:\n                    idxs_elite = np.argsort(archive_f)[:n_keep]\n                    elites = np.array([archive_x[i] for i in idxs_elite], dtype=float)\n                    # center: mean or a random elite\n                    if self.rng.random() < 0.5:\n                        center = elites.mean(axis=0)\n                    else:\n                        center = elites[self.rng.integers(len(elites))]\n\n                    # covariance and SVD (observations in rows)\n                    try:\n                        C = np.cov(elites, rowvar=False)\n                        # regularize small\n                        eps = 1e-6 * np.mean(np.diag(C) + 1e-8)\n                        C += np.eye(self.dim) * eps\n                        U, Svals, _ = np.linalg.svd(C)\n                        # construct random coefficients along principal directions\n                        coeffs = self.rng.normal(scale=1.0, size=self.dim) * np.sqrt(np.maximum(Svals, 0.0))\n                        mean_root = max(1e-8, np.mean(np.sqrt(np.maximum(Svals, 0.0))))\n                        gmult = (gscale * (0.8 + 0.8 * self.rng.random())) / mean_root\n                        coeffs = coeffs * gmult\n                        cand = center + (U @ coeffs)\n                        # add small isotropic jitter\n                        cand = cand + self.rng.normal(scale=0.1, size=self.dim) * scale * gscale\n                    except Exception:\n                        jitter = self.rng.normal(scale=1.0, size=self.dim)\n                        cand = center + jitter * scale * gscale\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                u = self.rng.random(self.dim)\n                # standard Cauchy via tan(pi*(u-0.5))\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails\n                cauch = np.clip(cauch, -1e3, 1e3)\n                # scale by per-dim scale and gscale with some randomness\n                cand = base + cauch * (0.5 * scale) * (0.5 + self.rng.random())\n                # small damping jitter\n                cand += self.rng.normal(scale=0.05, size=self.dim) * scale * gscale\n\n            # Ensure candidate finite and within bounds (reflect)\n            cand = np.array(cand, dtype=float)\n            nonfinite = ~np.isfinite(cand)\n            if nonfinite.any():\n                # replace any non-finite dims with uniform in bound for that dim\n                cand[nonfinite] = lb[nonfinite] + self.rng.random(np.count_nonzero(nonfinite)) * span[nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            try:\n                f_cand = float(func(cand))\n                if not np.isfinite(f_cand):\n                    f_cand = np.inf\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            improved = False\n            # update bests & archives\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # update recent success window\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                # drop oldest\n                recent_success.pop(0)\n\n            # adapt gscale based on short-term success rate\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n            if succ_rate > 0.25:\n                gscale *= 0.995\n            elif succ_rate < 0.05:\n                gscale *= 1.01\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            # clip gscale to reasonable range\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if (stagnation_counter >= stagnation_limit) or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                # perform modest injection batch\n                inject = n_inject\n                # choose injection centers: some around best, some global random\n                top_idx = np.argsort(archive_f)[:max(1, min(5, len(archive_f)))] if len(archive_f) > 0 else []\n                for _inj in range(inject):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.random() < 0.6 and len(top_idx) > 0:\n                        center = np.array(archive_x[self.rng.choice(top_idx)], dtype=float)\n                        scale_inj = 2.0 * scale * (1.0 + 0.5 * self.rng.random(self.dim))\n                        x_inj = center + self.rng.normal(scale=1.0, size=self.dim) * scale_inj * gscale\n                    else:\n                        # global injection\n                        x_inj = self._uniform_array(lb, ub)\n                    # reflect & ensure finite\n                    x_inj = np.array(x_inj, dtype=float)\n                    nonfinite = ~np.isfinite(x_inj)\n                    if nonfinite.any():\n                        x_inj[nonfinite] = lb[nonfinite] + self.rng.random(np.count_nonzero(nonfinite)) * span[nonfinite]\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                        if not np.isfinite(f_inj):\n                            f_inj = np.inf\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                # gentle reset of scale and stagnation counter\n                gscale *= (0.9 + 0.2 * self.rng.random())\n                stagnation_counter = 0\n\n            # prune memory periodically\n            if len(archive_x) > keep_memory * 2:\n                # keep best ones and some recent ones\n                n_keep_best = max(int(0.6 * keep_memory), keep_memory // 2)\n                idx_best = np.argsort(archive_f)[:n_keep_best].tolist()\n                # keep some of most recent entries\n                recent_cnt = int(max(0.1 * keep_memory, keep_memory - n_keep_best))\n                recent_idx = list(range(max(0, len(archive_x) - recent_cnt), len(archive_x)))\n                keep_idx = sorted(set(idx_best + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return\n        if self.x_opt is None:\n            # degenerate: return random point (call function if we still have budget)\n            x_try = self._uniform_array(lb, ub)\n            try:\n                if evals < self.budget:\n                    f = float(func(x_try))\n                    evals += 1\n                else:\n                    f = np.inf\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = float(f)\n            except Exception:\n                # no evaluation possible, return x_try with inf\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.161 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08112700280552065, 0.15197891268238473, 0.19836081810352668, 0.15770830142905545, 0.15599590988930345, 0.1908444499115125, 0.19882941913089813, 0.1692738248707535, 0.1834468271370685, 0.12548392758879479]}, "task_prompt": ""}
{"id": "71ba3050-64d7-4d5e-9817-47f6bb8b6668", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes DE-style donors, anisotropic local Gaussians, PCA-guided principal-direction proposals, heavy-tailed Cauchy jumps, and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.2, init_scale=0.2, keep_memory=1000,\n                 F=0.8, cr=0.2, seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        F: DE differential factor\n        cr: DE crossover rate\n        seed: RNG seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.keep_memory = max(10, int(keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.default_rng(self.seed)\n\n        # results (filled by call)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        # Try common formats: func.bounds.lb/ub, func.bounds as tuple/list, or attributes lb/ub\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # case: bounds has .lb and .ub\n                lb_attr = getattr(b, \"lb\", None)\n                ub_attr = getattr(b, \"ub\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                else:\n                    # case: bounds is sequence (lb, ub)\n                    try:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n                    except Exception:\n                        lb = None\n                        ub = None\n            else:\n                lb = None\n                ub = None\n        except Exception:\n            lb = None\n            ub = None\n\n        # try func.lb / func.ub directly\n        if lb is None or ub is None:\n            try:\n                lb_attr = getattr(func, \"lb\", None)\n                ub_attr = getattr(func, \"ub\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                pass\n\n        # fallback default box [-5,5]^dim\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast scalars to vectors if necessary\n            if lb.ndim == 0:\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if ub.ndim == 0:\n                ub = np.full(self.dim, float(ub), dtype=float)\n            # ensure correct length\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim).astype(float)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim).astype(float)\n\n        # ensure valid: if any lb >= ub, replace that dimension with [-5,5]\n        bad = lb >= ub\n        if np.any(bad):\n            lb = lb.copy()\n            ub = ub.copy()\n            lb[bad] = -5.0\n            ub[bad] = 5.0\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # reflect values outside bounds back into box symmetrically a few times\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                # reflect below: 2*lb - x\n                x[below] = 2.0 * lb[below] - x[below]\n            if above.any():\n                # reflect above: 2*ub - x\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp if still outside\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        u = self.rng.random(self.dim)\n        return lb + u * (ub - lb)\n\n    def __call__(self, func):\n        # initialize\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # avoid zero spans\n        span = np.maximum(span, 1e-12)\n        base_scale_vec = np.maximum(1e-12, self.init_scale * span)\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        evals = 0\n\n        # initial sampling: ensure some initial diversity\n        n_init = max(10, min(5 * self.dim, int(self.budget // 20 + 10)))\n        n_init = min(n_init, max(1, self.budget))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(np.inf)\n            evals += 1\n            archive_x.append(np.asarray(x, dtype=float).copy())\n            archive_f.append(f)\n            if f < self.f_opt - 1e-12:\n                self.f_opt = float(f)\n                self.x_opt = np.asarray(x, dtype=float).copy()\n\n        # adaptation & history\n        recent_window = max(30, 5 * self.dim)\n        recent_success = deque(maxlen=recent_window)\n        p_de = 0.18\n        p_local = 0.48\n        p_pca = 0.18  # rest goes to heavy-tailed\n        gscale = 1.0  # adaptive global multiplier for PCA coefficients and scales\n\n        last_improvement_eval = evals\n\n        # bookkeeping for pruning\n        keep_memory = max(20, int(self.keep_memory))\n\n        # main loop\n        while evals < self.budget:\n            # compute sorted indices by fitness (lower is better)\n            if len(archive_f) == 0:\n                best_idx = None\n            else:\n                sorted_idx = np.argsort(archive_f)\n                best_idx = sorted_idx[0]\n\n            # choose base (mostly best, sometimes random uniform or an archive point)\n            rbase = self.rng.random()\n            if best_idx is None or rbase > 0.85:\n                # uniform base across the box for exploration\n                base = self._uniform_array(lb, ub)\n            else:\n                if rbase < 0.7:\n                    base = archive_x[best_idx].copy()\n                else:\n                    # pick a random archive point\n                    idx = self.rng.integers(len(archive_x))\n                    base = archive_x[idx].copy()\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_jitter = 0.8 + 0.4 * self.rng.random(self.dim)\n            scale_vec = base_scale_vec * per_dim_jitter * gscale\n\n            r = self.rng.random()\n\n            # Candidate placeholder\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropic multipliers to allow some dimensions to be larger\n                anis = np.exp(0.4 * (self.rng.standard_normal(self.dim)))\n                noise = self.rng.standard_normal(self.dim)\n                cand = base + anis * scale_vec * noise\n                # small isotropic jitter\n                cand += 0.03 * scale_vec * self.rng.standard_normal(self.dim)\n\n            # DE-style donor move with small crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # pick three distinct indices\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                x_a = archive_x[idxs[0]]\n                x_b = archive_x[idxs[1]]\n                x_c = archive_x[idxs[2]]\n                donor = x_a + self.F * (x_b - x_c)\n                # crossover mask (ensure at least one dim taken)\n                mask = self.rng.random(self.dim) < self.cr\n                mask[self.rng.integers(self.dim)] = True\n                cand = base.copy()\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand += 0.05 * scale_vec * self.rng.standard_normal(self.dim)\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 2:\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                n_keep = min(n_keep, len(archive_x))\n                if n_keep < 2:\n                    # fallback to local\n                    anis = np.exp(0.4 * (self.rng.standard_normal(self.dim)))\n                    cand = base + anis * scale_vec * self.rng.standard_normal(self.dim)\n                else:\n                    # select elites\n                    sorted_idx = np.argsort(archive_f)\n                    elites = np.asarray([archive_x[i] for i in sorted_idx[:n_keep]], dtype=float)\n                    # center: mean or best elite (randomized)\n                    if self.rng.random() < 0.5:\n                        center = np.mean(elites, axis=0)\n                    else:\n                        center = elites[self.rng.integers(elites.shape[0])]\n\n                    # covariance and SVD (observations in rows)\n                    cov = np.cov(elites, rowvar=False)\n                    # regularize small\n                    avg_diag = np.mean(np.diag(cov)) if cov.size else 1.0\n                    cov += np.eye(self.dim) * 1e-8 * (avg_diag + 1e-12)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort descending\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = np.maximum(eigvals[order], 0.0)\n                        eigvecs = eigvecs[:, order]\n                    except Exception:\n                        # fallback: isotropic\n                        eigvals = np.ones(self.dim) * (avg_diag + 1e-8)\n                        eigvecs = np.eye(self.dim)\n\n                    mean_root = np.sqrt(np.maximum(1e-12, np.mean(eigvals)))\n                    gmult = (gscale * (0.8 + 0.8 * self.rng.random())) / max(1e-12, mean_root)\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.standard_normal(self.dim) * np.sqrt(eigvals) * gmult\n                    cand = center + eigvecs.dot(coeffs)\n                    # add small isotropic jitter\n                    cand += 0.02 * scale_vec * self.rng.standard_normal(self.dim)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                u = self.rng.random(self.dim)\n                c = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails to reduce catastrophic jumps\n                c = np.clip(c, -1e2, 1e2)\n                # scale by per-dim scale and gscale with some randomness\n                rand_scale = 0.2 + 1.8 * self.rng.random(self.dim)\n                cand = base + scale_vec * gscale * rand_scale * c\n                # small damping jitter\n                cand += 0.01 * scale_vec * self.rng.standard_normal(self.dim)\n\n            # Ensure candidate finite and within bounds (reflect)\n            # replace any non-finite dims with random in bound for that dim\n            cand = np.asarray(cand, dtype=float)\n            bad_mask = ~np.isfinite(cand)\n            if bad_mask.any():\n                for k in np.nonzero(bad_mask)[0]:\n                    cand[k] = lb[k] + self.rng.random() * (ub[k] - lb[k])\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(np.inf)\n            evals += 1\n\n            # update bests & archives\n            archive_x.append(cand.copy())\n            archive_f.append(f_cand)\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved = True\n                last_improvement_eval = evals\n\n            # update recent success window\n            recent_success.append(bool(improved))\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            # encourage exploration if too few successes, shrink if many successes\n            if succ_rate > 0.25:\n                gscale *= 0.98\n            elif succ_rate < 0.05:\n                gscale *= 1.02\n            else:\n                gscale *= 0.995\n            # small multiplicative jitter for exploration\n            gscale *= np.exp(0.01 * self.rng.standard_normal())\n            # clip gscale to reasonable range\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            stagnation_tol = max(200, 40 * self.dim)\n            if (evals - last_improvement_eval) > stagnation_tol and evals < self.budget:\n                # perform modest injection batch\n                n_inject = min(20, self.budget - evals)\n                for _ in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    # decide global or local injection\n                    if self.rng.random() < 0.6 or self.x_opt is None:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # local injection around the best with larger scale\n                        jitter = 3.0\n                        x_inj = self.x_opt + jitter * scale_vec * (0.5 + self.rng.random(self.dim))\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(np.inf)\n                    evals += 1\n                    archive_x.append(x_inj.copy())\n                    archive_f.append(f_inj)\n                    if f_inj < self.f_opt - 1e-12:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = x_inj.copy()\n                        last_improvement_eval = evals\n                # gentle reset of scale to explore anew\n                gscale *= 0.7\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n                # continue main loop\n\n            # prune memory periodically\n            if len(archive_x) > 2 * keep_memory:\n                # keep best ones and some recent ones\n                sorted_idx = np.argsort(archive_f)\n                n_keep_best = max(int(0.6 * keep_memory), keep_memory // 2)\n                n_keep_best = min(n_keep_best, len(sorted_idx))\n                keep_idx = list(sorted_idx[:n_keep_best])\n                # keep some of most recent entries\n                recent_cnt = max(0, keep_memory - len(keep_idx))\n                if recent_cnt > 0:\n                    recent_start = max(0, len(archive_x) - recent_cnt)\n                    keep_idx.extend(range(recent_start, len(archive_x)))\n                # uniquify and keep in order to keep archives consistent\n                keep_idx = sorted(set(keep_idx), key=lambda i: i)\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return\n        if self.x_opt is None:\n            # degenerate: return random point (call function if we still have budget)\n            x_try = self._uniform_array(lb, ub)\n            if evals < self.budget:\n                try:\n                    f_try = float(func(x_try))\n                except Exception:\n                    f_try = float(np.inf)\n                evals += 1\n                self.f_opt = float(f_try)\n                self.x_opt = x_try.copy()\n            else:\n                # no budget to evaluate: return zero vector\n                self.x_opt = np.zeros(self.dim, dtype=float)\n                self.f_opt = float(np.inf)\n        return self.f_opt, np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 161, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "error": "In the code, line 161, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d2104e0f-e917-4def-a12f-c196a77d6b76", "fitness": 0.1872961854133544, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — a mixed-strategy black-box sampler combining DE-style donors, PCA-guided local moves on an elite archive, anisotropic Gaussian exploration, heavy-tailed Cauchy jumps and occasional micro-restarts with an adaptive global scale to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 elite_frac=0.2, memory_size=500, init_scale=0.2,\n                 F=0.8, cr=0.2):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        seed: RNG seed\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        memory_size: approximate number of evaluated points to remember (pruned periodically)\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        F: DE differential factor\n        cr: DE crossover rate\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        self.elite_frac = float(elite_frac)\n        self.memory_size = int(memory_size)\n        self.init_scale = float(init_scale)\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # placeholders to be set during run\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _extract_bounds(self, func):\n        # Try several common ways the function may expose bounds\n        lb = None\n        ub = None\n        # tuple/list like (lb, ub) where lb/ub can be scalars or arrays\n        if hasattr(func, \"bounds\") and func.bounds is not None:\n            b = func.bounds\n            # try object with lb/ub attributes\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            elif hasattr(b, \"lower\") and hasattr(b, \"upper\"):\n                lb = np.asarray(b.lower, dtype=float)\n                ub = np.asarray(b.upper, dtype=float)\n            elif isinstance(b, (tuple, list)) and len(b) == 2:\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n        # also accept direct attributes\n        if (lb is None or ub is None) and hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n            lb = np.asarray(func.lower, dtype=float)\n            ub = np.asarray(func.upper, dtype=float)\n\n        # fallback to classic [-5,5]^dim\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast scalars to vectors if necessary\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb.item()), dtype=float)\n            else:\n                lb = np.resize(lb, self.dim).astype(float).copy()\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub.item()), dtype=float)\n            else:\n                ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # sanity: if any lb >= ub, replace those dimensions with [-5,5]\n        bad = lb >= ub\n        if np.any(bad):\n            lb = lb.copy()\n            ub = ub.copy()\n            lb[bad] = -5.0\n            ub[bad] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # Reflect values outside bounds back into box symmetrically up to max_reflect times\n        x = x.copy()\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            if np.any(below):\n                x[below] = 2.0 * lb[below] - x[below]\n            if np.any(above):\n                x[above] = 2.0 * ub[above] - x[above]\n        # Final clamp to box\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        # vectorized uniform sampling per dimension\n        return lb + self.rng.random(self.dim) * (ub - lb)\n\n    def __call__(self, func):\n        # initialize bookkeeping\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1.0  # avoid divide by zero\n        # per-dim base scale\n        base_scale = np.maximum(1e-12, self.init_scale * span)\n\n        self.f_opt = float(np.inf)\n        self.x_opt = np.zeros(self.dim, dtype=float)\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        # adaptation & history\n        n_init = int(min(max(10, 5 * self.dim), max(1, self.budget // 10)))\n        evals = 0\n        recent_success = []  # boolean list of recent improvements\n        success_window = max(5, self.dim)  # window length to compute short-term success rate\n        gscale = 1.0  # global multiplier for PCA coefficients and scale\n        # move probabilities\n        p_local = 0.4\n        p_de = 0.15\n        p_pca = 0.25\n        p_cauchy = 0.15\n        # other state\n        stagnation_counter = 0\n        best_at_last_restart = np.inf\n        prune_every = 50\n\n        # helper to evaluate once, safe against exceptions and budget checks\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise StopIteration()\n            # ensure numpy array and float dtype\n            x = np.asarray(x, dtype=float)\n            # reflect into bounds\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(np.inf)\n            evals += 1\n            return f, x\n\n        # initial sampling to populate archive\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f, x = safe_eval(x)\n            except StopIteration:\n                break\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n        # If nothing evaluated yet (budget small) do a final try\n        if evals >= self.budget:\n            return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness (ascending)\n            if len(archive_f) > 0:\n                order = np.argsort(archive_f)\n            else:\n                order = np.array([], dtype=int)\n\n            # choose move type\n            r = self.rng.random()\n            cand = None\n\n            # pick a \"base\" center: usually a top solution or a recent random archive member or uniform\n            if len(order) > 0 and self.rng.random() < 0.8:\n                # mostly pick among top 10% or top 3 whichever larger\n                nb_top = max(3, max(1, int(0.1 * len(order))))\n                base_idx = order[self.rng.integers(0, min(nb_top, len(order)))]\n                base = np.array(archive_x[base_idx], dtype=float)\n            elif len(archive_x) > 0:\n                base = np.array(archive_x[self.rng.integers(0, len(archive_x))], dtype=float)\n            else:\n                base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale (introduce some jitter per-dim)\n            scale = base_scale * (gscale * (1.0 + 0.05 * self.rng.standard_normal(self.dim)))\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropy: sample a random positive multiplier per dimension (log-normal-ish)\n                anis = np.exp(0.5 * (self.rng.standard_normal(self.dim) - 0.5))\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * scale * anis\n\n            # DE-style donor move with small crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # pick three distinct indices different from base if possible\n                idxs = list(range(len(archive_x)))\n                if len(archive_x) >= 4 and 'base_idx' in locals():\n                    # ensure base_idx not chosen\n                    try:\n                        idxs.remove(base_idx)\n                    except Exception:\n                        pass\n                a, b, c = self.rng.choice(idxs, size=3, replace=False)\n                xa = np.array(archive_x[a], dtype=float)\n                xb = np.array(archive_x[b], dtype=float)\n                xc = np.array(archive_x[c], dtype=float)\n                donor = xa + self.F * (xb - xc)\n                # crossover mask\n                mask = self.rng.random(self.dim) < self.cr\n                if not np.any(mask):\n                    mask[self.rng.integers(0, self.dim)] = True\n                # small gaussian jitter proportional to scale\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim) * 0.1 * scale\n                cand = base.copy()\n                cand[mask] = donor[mask] + jitter[mask]\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 5:\n                # select elites\n                m = max(2, int(np.ceil(self.elite_frac * len(archive_x))))\n                elites_idx = order[:m] if len(order) > 0 else np.arange(len(archive_x))\n                elites = np.array([archive_x[i] for i in elites_idx], dtype=float)\n                # choose center: mean or best elite\n                if self.rng.random() < 0.5:\n                    center = elites.mean(axis=0)\n                else:\n                    center = elites[0].copy()\n                # covariance (observations in rows)\n                if elites.shape[0] >= 2:\n                    cov = np.cov(elites, rowvar=False, bias=True)\n                    # regularize small eigenvalues\n                    eps = 1e-6 * np.mean(np.diag(cov) + 1e-12)\n                    cov += np.eye(self.dim) * eps\n                    try:\n                        U, Svals, _ = np.linalg.svd(cov, full_matrices=False)\n                    except np.linalg.LinAlgError:\n                        # fallback: small isotropic jitter\n                        cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * scale\n                    else:\n                        sqrt_vals = np.sqrt(np.maximum(Svals, 0.0))\n                        # coefficients along principal directions\n                        coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * sqrt_vals\n                        # scale coefficients by global gscale and mean span to be comparable to scale\n                        mean_span = np.mean(span)\n                        coeffs = coeffs * (gscale * 0.5 * mean_span)\n                        cand = center + (U @ coeffs)\n                        # add small isotropic jitter\n                        cand += self.rng.normal(0.0, 0.05, size=self.dim) * scale\n                else:\n                    # fallback local\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * scale\n\n            # heavy-tailed per-dim Cauchy jumps\n            else:\n                # standard Cauchy via tan(pi*(u-0.5)), u in (0,1)\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails\n                cauch = np.clip(cauch, -50.0, 50.0)\n                # scale by per-dim scale and gscale with some randomness\n                damp = 0.5 + 0.5 * self.rng.random(self.dim)\n                cand = base + cauch * scale * gscale * damp\n                # some small damping jitter\n                cand += self.rng.normal(0.0, 0.01, size=self.dim) * scale\n\n            # Ensure candidate finite and within bounds (reflect)\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.asarray(cand, dtype=float)\n            nonfinite = ~np.isfinite(cand)\n            if np.any(nonfinite):\n                # replace non-finite dims with uniform in bounds for that dim\n                urep = self._uniform_array(lb, ub)\n                cand[nonfinite] = urep[nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate (respect budget)\n            try:\n                f_cand, cand = safe_eval(cand)\n            except StopIteration:\n                break\n\n            # update bests & archives\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n\n            # update recent success window\n            recent_success.append(improved)\n            if len(recent_success) > success_window:\n                recent_success.pop(0)\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            # if success_rate is low, increase exploratory scale; if high, decrease for refinement\n            if succ_rate < 0.05:\n                gscale *= 1.07\n            elif succ_rate > 0.3:\n                gscale *= 0.85\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart\n            if self.f_opt < best_at_last_restart - 1e-12:\n                # progress since last restart\n                best_at_last_restart = self.f_opt\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            if stagnation_counter > max(50, 10 * self.dim):\n                # micro-restart: inject a few diverse points\n                stagnation_counter = 0\n                # create injection batch (but do not exceed budget)\n                inj_budget = min(10 + self.dim, self.budget - evals)\n                inj_candidates = []\n                # one global injection: random uniform\n                inj_candidates.append(self._uniform_array(lb, ub))\n                # some local injections around top elites\n                top_k = max(1, min(5, len(order)))\n                for k in range(min(inj_budget - 1, top_k)):\n                    if len(order) > 0:\n                        center = archive_x[order[k]]\n                    else:\n                        center = self._uniform_array(lb, ub)\n                    # bigger scale injection using gscale and base_scale\n                    local_scale = base_scale * (5.0 + 2.0 * self.rng.random())\n                    x_inj = np.array(center) + self.rng.normal(0.0, 1.0, size=self.dim) * local_scale * gscale\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    inj_candidates.append(x_inj)\n                # evaluate injections\n                for x_inj in inj_candidates:\n                    if evals >= self.budget:\n                        break\n                    try:\n                        f_inj, x_inj = safe_eval(x_inj)\n                    except StopIteration:\n                        break\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        best_at_last_restart = self.f_opt\n                # gentle reset of gscale to encourage exploitation of found area\n                gscale = max(0.1, min(gscale, 10.0)) * 0.8\n\n            # prune memory periodically to keep archive manageable\n            if (len(archive_x) > self.memory_size * 2) and (evals % prune_every == 0):\n                keep = []\n                # keep best ones\n                keep_n = max(10, int(0.5 * self.memory_size))\n                if len(archive_f) > 0:\n                    best_idx = np.argsort(archive_f)[:keep_n].tolist()\n                else:\n                    best_idx = []\n                keep_set = set(best_idx)\n                # keep some recent ones\n                recent_n = max(10, self.memory_size - len(best_idx))\n                recent_idx = list(range(max(0, len(archive_x) - recent_n), len(archive_x)))\n                for i in recent_idx:\n                    keep_set.add(i)\n                # rebuild archives\n                new_x = [archive_x[i] for i in sorted(keep_set)]\n                new_f = [archive_f[i] for i in sorted(keep_set)]\n                archive_x = new_x\n                archive_f = new_f\n\n        # final return\n        # if nothing was evaluated (very small budget), return a uniform sample try\n        if self.f_opt == np.inf and self.budget > 0:\n            try:\n                f_try, x_try = safe_eval(self._uniform_array(lb, ub))\n                self.f_opt = float(f_try)\n                self.x_opt = np.array(x_try, dtype=float)\n            except StopIteration:\n                pass\n\n        # Ensure x_opt is numpy array of correct shape\n        if self.x_opt is None:\n            self.x_opt = np.zeros(self.dim, dtype=float)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.187 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13455024088924294, 0.15613907971563956, 0.25428553156681155, 0.2025162545296022, 0.18265720688573894, 0.2211145925847976, 0.20957903654160526, 0.1875894180300749, 0.17573317921772014, 0.148797314172311]}, "task_prompt": ""}
{"id": "0bb8de28-6ae2-4181-9842-496cb2e5d7ea", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes DE-style donors, PCA-guided local proposals, anisotropic Gaussian moves, heavy-tailed Cauchy jumps and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.2, init_scale=0.15, keep_memory=800,\n                 F=0.6, cr=0.2, seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        F: DE differential factor\n        cr: DE crossover rate\n        seed: RNG seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.keep_memory = int(max(50, keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # results\n        self.f_best = np.inf\n        self.x_best = None\n\n    def _extract_bounds(self, func):\n        # Try common formats, else fallback to [-5,5]^dim\n        default_lb = -5.0\n        default_ub = 5.0\n        # helper to convert to arrays\n        def to_arr(v):\n            a = np.array(v, dtype=float)\n            if a.size == 1:\n                return np.repeat(a.item(), self.dim)\n            if a.size != self.dim:\n                # try to broadcast if possible else fallback\n                return np.resize(a, self.dim)\n            return a\n\n        lb = None\n        ub = None\n\n        # tuple/list (lb, ub)\n        if hasattr(func, 'bounds'):\n            b = func.bounds\n            # object with .lb and .ub\n            if hasattr(b, 'lb') and hasattr(b, 'ub'):\n                try:\n                    lb = to_arr(b.lb)\n                    ub = to_arr(b.ub)\n                except Exception:\n                    lb = None\n                    ub = None\n            else:\n                # maybe a tuple/list\n                try:\n                    if isinstance(b, (tuple, list)) and len(b) == 2:\n                        lb = to_arr(b[0])\n                        ub = to_arr(b[1])\n                except Exception:\n                    lb = None\n                    ub = None\n        else:\n            # maybe func provides attributes directly\n            if hasattr(func, 'lb') and hasattr(func, 'ub'):\n                try:\n                    lb = to_arr(func.lb)\n                    ub = to_arr(func.ub)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, default_lb, dtype=float)\n            ub = np.full(self.dim, default_ub, dtype=float)\n\n        # ensure valid: if any lb >= ub, replace with [-5,5]\n        bad = lb >= ub\n        if np.any(bad):\n            lb[bad] = default_lb\n            ub[bad] = default_ub\n\n        return lb.copy(), ub.copy()\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # reflect values outside bounds back into box symmetrically a few times\n        # operate on array x (1D)\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = 2 * lb[below] - x[below]\n            if above.any():\n                x[above] = 2 * ub[above] - x[above]\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        # broadcast lb/ub if scalars or arrays\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == () or lb.size == 1:\n            return self.rng.uniform(lb.item(), ub.item(), size=self.dim)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        # Extract bounds and basic scale\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # avoid zero spans\n        span[span <= 0] = 1.0\n        base_scale = self.init_scale * span  # per-dim base scale\n        # archives and memory (list of (x, f))\n        memory = []\n        recent = deque(maxlen=200)  # keep recent for some pruning diversity\n        # evaluation budget counter\n        evals = 0\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n        # success window\n        success_window = deque(maxlen=60)\n        # stagnation counter\n        no_improve = 0\n        last_improve_eval = 0\n\n        def safe_eval(x):\n            nonlocal evals, memory, recent, gscale\n            if evals >= self.budget:\n                return np.inf\n            try:\n                fx = float(func(self._reflect_bounds(x, lb, ub)))\n            except Exception:\n                fx = np.inf\n            evals += 1\n            return fx\n\n        # Initial sampling: ensure some initial diversity\n        n_init = int(min(max(2 * self.dim, int(self.budget * 0.06)), 200 + self.dim))\n        n_init = max(4, n_init)\n        n_init = min(n_init, self.budget)  # cannot exceed budget\n        for _ in range(n_init):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            memory.append({'x': x.copy(), 'f': f})\n            recent.append({'x': x.copy(), 'f': f})\n            if f < self.f_best:\n                self.f_best = f\n                self.x_best = x.copy()\n                last_improve_eval = evals\n        # If budget exhausted during init\n        if evals >= self.budget:\n            return self.f_best, self.x_best\n\n        # main loop\n        stagnation_thresh = max(40 + 5 * self.dim, int(0.06 * self.budget))\n        prune_every = max(50, int(self.budget // 200))\n        iteration = 0\n        while evals < self.budget:\n            iteration += 1\n            # sort memory by fitness (lower better)\n            memory.sort(key=lambda r: r['f'])\n            n_mem = len(memory)\n            n_elite = max(2, int(np.ceil(self.elite_frac * n_mem)))\n            elites = memory[:n_elite]\n\n            # choose base (mostly best, sometimes random)\n            if self.rng.rand() < 0.75:\n                base = elites[0]['x'].copy()\n                base_idx = 0\n            else:\n                idx = self.rng.randint(0, n_mem)\n                base = memory[idx]['x'].copy()\n                base_idx = idx\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_scale = base_scale * (1.0 + 0.2 * self.rng.randn(self.dim))\n            per_dim_scale = np.abs(per_dim_scale)\n            # apply gscale\n            per_dim_scale *= (0.5 + 1.5 * gscale)\n\n            # decide move type probabilities\n            r = self.rng.rand()\n            x_cand = None\n\n            # 1) Local anisotropic gaussian around base (most common)\n            if r < 0.40:\n                # anisotropic multipliers to allow some dims larger\n                anis = np.exp(0.5 * self.rng.randn(self.dim))\n                noise = self.rng.randn(self.dim) * per_dim_scale * anis\n                x_cand = base + noise\n\n            # 2) DE-style donor move with small crossover\n            elif r < 0.60 and n_mem >= 4:\n                # pick three distinct indices different from base_idx\n                idxs = list(range(n_mem))\n                if base_idx in idxs:\n                    idxs.remove(base_idx)\n                r1, r2, r3 = self.rng.choice(idxs, size=3, replace=False)\n                X = memory[r1]['x'] + self.F * (memory[r2]['x'] - memory[r3]['x'])\n                # crossover mask (ensure at least one dim taken)\n                mask = self.rng.rand(self.dim) < self.cr\n                if not mask.any():\n                    mask[self.rng.randint(0, self.dim)] = True\n                x_trial = base.copy()\n                x_trial[mask] = X[mask]\n                # small gaussian jitter proportional to scale\n                x_cand = x_trial + 0.5 * self.rng.randn(self.dim) * per_dim_scale\n\n            # 3) PCA-guided sampling using elites\n            elif r < 0.80 and n_elite >= 2:\n                # ensure sensible; prepare matrix of elite points\n                X = np.vstack([e['x'] for e in elites])\n                # choose center: either mean or best elite (randomized)\n                if self.rng.rand() < 0.5:\n                    center = X.mean(axis=0)\n                else:\n                    center = X[0].copy()\n                # covariance and SVD\n                try:\n                    cov = np.cov(X, rowvar=False)\n                    # regularize\n                    cov += np.eye(self.dim) * 1e-6 * np.mean(np.diag(cov) + 1e-8)\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    eigvals[eigvals < 1e-12] = 1e-12\n                    # construct random coefficients along principal directions\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(np.abs(eigvals))\n                    # scale coefficients by global gscale and by mean span to be comparable to scale\n                    mean_span = np.mean(np.max(X, axis=0) - np.min(X, axis=0) + 1e-8)\n                    coeffs *= (gscale * (mean_span + 1e-8) / (np.linalg.norm(eigvals) / (self.dim + 1e-8)))\n                    x_cand = center + eigvecs.dot(coeffs)\n                    # add small isotropic jitter\n                    x_cand += 0.2 * self.rng.randn(self.dim) * per_dim_scale\n                except Exception:\n                    # fallback local\n                    x_cand = base + self.rng.randn(self.dim) * per_dim_scale\n\n            # 4) heavy-tailed per-dim Cauchy jumps (Levy-like)\n            elif r < 0.95:\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails to reduce catastrophic jumps\n                cauchy = np.clip(cauchy, -10, 10)\n                # scale by per-dim scale and gscale with some randomness\n                scales = per_dim_scale * (0.8 + 0.8 * self.rng.rand(self.dim)) * (0.8 + 1.2 * gscale)\n                x_cand = base + cauchy * scales\n                # damping jitter\n                x_cand += 0.05 * self.rng.randn(self.dim) * per_dim_scale\n\n            # 5) occasional pure random exploration\n            else:\n                x_cand = self._uniform_array(lb, ub)\n\n            # Ensure candidate finite and within bounds (reflect)\n            # replace any non-finite dims with random in bound for that dim\n            if not np.all(np.isfinite(x_cand)):\n                finite = np.isfinite(x_cand)\n                rnd = self._uniform_array(lb, ub)\n                x_cand[~finite] = rnd[~finite]\n            x_cand = self._reflect_bounds(x_cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(x_cand)\n\n            # update bests & archives\n            memory.append({'x': x_cand.copy(), 'f': f_cand})\n            recent.append({'x': x_cand.copy(), 'f': f_cand})\n            improved = False\n            if f_cand < self.f_best:\n                self.f_best = f_cand\n                self.x_best = x_cand.copy()\n                improved = True\n                last_improve_eval = evals\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # update short-term success window\n            success_window.append(1 if improved else 0)\n            # adapt gscale based on short-term success rate\n            if len(success_window) >= 6:\n                success_rate = np.mean(success_window)\n                if success_rate > 0.25:\n                    # successful, increase exploration a bit\n                    gscale *= 1.05\n                elif success_rate < 0.05:\n                    # little success, reduce gscale to focus local search\n                    gscale *= 0.92\n            # small multiplicative jitter for exploration\n            gscale *= np.exp(0.02 * self.rng.randn())\n            # clip gscale to reasonable range\n            gscale = float(np.clip(gscale, 0.05, 8.0))\n\n            # stagnation detection & micro-restart (injections)\n            if (evals - last_improve_eval) > stagnation_thresh and evals < self.budget:\n                # perform modest injection batch\n                n_inject = min(8 + self.dim // 2, max(4, (self.budget - evals) // 20))\n                for _ in range(n_inject):\n                    # half global injection, half local around a top elite or best\n                    if self.rng.rand() < 0.6:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # choose a center from elites or best\n                        center = self.x_best.copy() if self.rng.rand() < 0.6 or len(elites) < 2 else elites[self.rng.randint(0, len(elites))]['x']\n                        big_scale = base_scale * (2.5 + 2.0 * self.rng.rand(self.dim))\n                        x_inj = center + big_scale * self.rng.randn(self.dim)\n                    # reflect & ensure finite\n                    if not np.all(np.isfinite(x_inj)):\n                        rnd = self._uniform_array(lb, ub)\n                        x_inj[~np.isfinite(x_inj)] = rnd[~np.isfinite(x_inj)]\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    f_inj = safe_eval(x_inj)\n                    memory.append({'x': x_inj.copy(), 'f': f_inj})\n                    recent.append({'x': x_inj.copy(), 'f': f_inj})\n                    if f_inj < self.f_best:\n                        self.f_best = f_inj\n                        self.x_best = x_inj.copy()\n                        last_improve_eval = evals\n                # gentle reset of some internal scales to encourage fresh sampling\n                gscale = max(0.5, 0.7 * gscale)\n                no_improve = 0\n\n            # prune memory periodically\n            if (iteration % prune_every == 0) and len(memory) > self.keep_memory:\n                # keep best ones and some recent ones\n                memory.sort(key=lambda r: r['f'])\n                keep_best = int(max(20, 0.1 * self.keep_memory))\n                keep_recent = int(max(20, 0.1 * self.keep_memory))\n                new_mem = memory[:keep_best]\n                # sample some of middle performers\n                middle = memory[keep_best: min(len(memory), keep_best + max(0, self.keep_memory - keep_best - keep_recent))]\n                if len(middle) > 0:\n                    pick = min(len(middle), max(0, self.keep_memory - keep_best - keep_recent))\n                    # keep even spread\n                    indices = np.linspace(0, len(middle) - 1, pick).astype(int)\n                    new_mem.extend([middle[i] for i in indices])\n                # add recent items\n                recent_items = list(recent)[-keep_recent:]\n                new_mem.extend(recent_items)\n                # ensure unique-ish by x\n                if len(new_mem) > 1:\n                    # remove exact duplicates by hash of rounded x\n                    seen = set()\n                    uniq = []\n                    for r in new_mem:\n                        key = tuple(np.round(r['x'], decimals=8))\n                        if key not in seen:\n                            seen.add(key)\n                            uniq.append(r)\n                    new_mem = uniq\n                memory = new_mem\n\n            # safety guard: should not exceed budget; loop will exit naturally\n\n        # final return\n        if self.x_best is None:\n            # degenerate: return random point\n            x0 = self._uniform_array(lb, ub)\n            f0 = safe_eval(x0) if evals < self.budget else np.inf\n            return f0, x0\n        return self.f_best, self.x_best", "configspace": "", "generation": 0, "feedback": "In the code, line 132, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent = deque(maxlen=200)  # keep recent for some pruning diversity", "error": "In the code, line 132, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent = deque(maxlen=200)  # keep recent for some pruning diversity", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "54f91b79-56eb-4b26-ac04-59e296185b42", "fitness": 0.1693516789512572, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — combines DE-style donors, PCA-guided elite local moves, anisotropic Gaussian exploration, heavy-tailed Cauchy jumps and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.20, init_scale=0.20,\n                 keep_memory=None, F=0.8, cr=0.2,\n                 seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        F: DE differential factor\n        cr: DE crossover rate\n        seed: RNG seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        if keep_memory is None:\n            self.keep_memory = max(200, 20 * self.dim)\n        else:\n            self.keep_memory = int(max(10, keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        # Try common formats, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\"):\n                b = getattr(func, \"bounds\")\n                # if object with lb/ub attributes\n                lb_attr = getattr(b, \"lb\", None)\n                ub_attr = getattr(b, \"ub\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                else:\n                    # tuple/list (lb, ub)\n                    if isinstance(b, (tuple, list)) and len(b) >= 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            lb = ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n            return lb, ub\n\n        # broadcast / resize to dim\n        try:\n            lb = np.broadcast_to(lb, (self.dim,)).astype(float).copy()\n            ub = np.broadcast_to(ub, (self.dim,)).astype(float).copy()\n        except Exception:\n            lb = np.resize(lb, self.dim).astype(float).copy()\n            ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub, fallback to [-5,5]\n        if np.any(lb >= ub):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # reflect values outside bounds back into box symmetrically a few times\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = 2.0 * lb[below] - x[below]\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        u = self.rng.random(self.dim)\n        return lb + u * (ub - lb)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        \"\"\"\n        # bounds and scale\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # base per-dimension scale\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # Archives\n        archive_x = []  # list of numpy arrays\n        archive_f = []\n\n        # initial sampling budget: ensure some initial diversity\n        n_init = int(min(max(10, 5 * self.dim), max(1, self.budget // 10)))\n        evals = 0\n\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                # if func crashes, treat as bad\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_window = max(30, 5 * self.dim)\n        recent_success = []  # list of 0/1\n        stagnation_limit = max(40, 10 * self.dim)\n        stagnation_counter = 0\n        # move probabilities\n        p_de = 0.18\n        p_pca = 0.25\n        p_local = 0.42  # rest is cauchy\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # bookkeeping for pruning\n        keep_memory = max(self.keep_memory, 50 + 10 * self.dim)\n\n        # main loop\n        while evals < self.budget:\n            # compute sorted indices by fitness (lower is better)\n            if len(archive_f) > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # choose base (mostly best, sometimes random from archive)\n            if self.x_opt is not None and self.rng.random() < 0.8 and len(archive_x) > 0:\n                base = np.array(self.x_opt, dtype=float)\n                base_from_archive = False\n            else:\n                if len(archive_x) == 0:\n                    base = self._uniform_array(lb, ub)\n                    base_from_archive = False\n                else:\n                    idx = self.rng.integers(0, len(archive_x))\n                    base = np.array(archive_x[idx], dtype=float)\n                    base_from_archive = True\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            # scale grows with gscale but never below tiny\n            scale = base_scale_vec * gscale\n            scale = np.maximum(scale, 1e-12)\n\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropic multipliers to allow some dimensions to be larger\n                rand_mult = 0.6 + 1.2 * self.rng.random(self.dim)\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult)\n\n            # DE-style donor move with small crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # pick three distinct indices\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                a = np.array(archive_x[idxs[0]], dtype=float)\n                b = np.array(archive_x[idxs[1]], dtype=float)\n                c = np.array(archive_x[idxs[2]], dtype=float)\n                donor = a + self.F * (b - c)\n                # crossover mask (ensure at least one dim taken)\n                mask = (self.rng.random(self.dim) < self.cr)\n                mask[self.rng.integers(self.dim)] = True\n                cand = base.copy()\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 2:\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                n_keep = min(n_keep, len(archive_x))\n                if n_keep < 2:\n                    # fallback local\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n                else:\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                    # center: either mean or best elite (randomized)\n                    if self.rng.random() < 0.5:\n                        center = np.mean(elites, axis=0)\n                    else:\n                        center = elites[0].copy()  # best\n                    # covariance and SVD\n                    if elites.shape[0] >= 2:\n                        cov = np.cov(elites, rowvar=False)\n                        # regularize\n                        cov += np.eye(self.dim) * 1e-8 * (np.mean(np.diag(cov)) + 1e-12)\n                        try:\n                            U, Svals, _ = np.linalg.svd(cov, full_matrices=False)\n                            # construct random coefficients along principal directions\n                            sqrt_vals = np.sqrt(np.maximum(Svals, 0.0))\n                            coeffs = self.rng.normal(0.0, 1.0, size=len(Svals)) * sqrt_vals\n                            # scale coefficients by global gscale and by mean span to be comparable to scale\n                            mean_root = max(1e-12, np.mean(np.sqrt(np.maximum(Svals, 1e-12))))\n                            gmult = (gscale * (0.8 + 0.8 * self.rng.random())) / mean_root\n                            coeffs = coeffs * gmult\n                            # build candidate\n                            cand = center + (U @ coeffs)\n                            # add small isotropic jitter\n                            cand += self.rng.normal(0.0, 0.05, size=self.dim) * scale\n                        except Exception:\n                            jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                            cand = center + jitter * scale\n                    else:\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        cand = center + jitter * scale\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy via tan(pi*(u-0.5))\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails to reduce catastrophic jumps\n                cauch = np.clip(cauch, -50.0, 50.0)\n                # scale by per-dim scale and gscale with some randomness\n                mult = (0.6 + 1.4 * self.rng.random(self.dim)) * gscale\n                cand = base + cauch * (scale * 0.6 * mult)\n                # small damping jitter\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # Ensure candidate finite and within bounds (reflect)\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            # replace any non-finite dims with random in bound for that dim\n            nonfinite = ~np.isfinite(cand)\n            if nonfinite.any():\n                uniform_fill = self._uniform_array(lb, ub)\n                cand[nonfinite] = uniform_fill[nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # check budget before evaluating\n            if evals >= self.budget:\n                break\n\n            # evaluate candidate\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            improved = False\n            # update bests & archives\n            archive_x.append(cand.copy())\n            archive_f.append(f_cand)\n            if f_cand < self.f_opt - 1e-12:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # update recent success window\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                # drop oldest\n                recent_success.pop(0)\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            if succ_rate > 0.25:\n                gscale *= (1.0 + 0.12 * min(1.0, (succ_rate - 0.25) / 0.75))\n            elif succ_rate < 0.05:\n                gscale *= 0.995\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            # clip gscale to reasonable range\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter >= stagnation_limit or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                old_gscale = gscale\n                # perform modest injection batch\n                remaining = max(0, self.budget - evals)\n                n_inject = min(max(3, 2 * self.dim), max(1, remaining // 5)) if remaining > 0 else 0\n                # if too few remaining, skip injection\n                if n_inject > 0:\n                    # temporarily expand gscale to favor broader exploration during injection\n                    gscale *= (1.0 + 0.5 * self.rng.random())\n                    for _inj in range(n_inject):\n                        if evals >= self.budget:\n                            break\n                        if self.rng.random() < 0.6 or len(archive_x) < 5:\n                            # global injection: random point\n                            x_inj = self._uniform_array(lb, ub)\n                        else:\n                            # local injection around a top elite or best\n                            top_k = min(max(2, int(0.1 * len(archive_x))), len(archive_x))\n                            top_idx = sorted_idx[:top_k] if len(archive_x) > 0 else np.array([], dtype=int)\n                            if len(top_idx) == 0:\n                                x_inj = self._uniform_array(lb, ub)\n                            else:\n                                center = np.array(archive_x[self.rng.choice(top_idx)], dtype=float)\n                                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                                x_inj = center + jitter * (scale * (2.0 + self.rng.random()))\n                        # reflect & ensure finite\n                        x_inj = np.array(x_inj, dtype=float)\n                        nonfinite_inj = ~np.isfinite(x_inj)\n                        if nonfinite_inj.any():\n                            fill = self._uniform_array(lb, ub)\n                            x_inj[nonfinite_inj] = fill[nonfinite_inj]\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                        try:\n                            f_inj = float(func(x_inj))\n                        except Exception:\n                            f_inj = np.inf\n                        evals += 1\n                        archive_x.append(x_inj.copy())\n                        archive_f.append(float(f_inj))\n                        if f_inj < self.f_opt - 1e-12:\n                            self.x_opt = np.array(x_inj, dtype=float)\n                            self.f_opt = float(f_inj)\n                            stagnation_counter = 0\n                    # gentle reset of scale\n                    gscale = min(old_gscale * 1.2, gscale)\n\n            # prune memory periodically\n            if len(archive_x) > keep_memory * 1.2:\n                # keep best ones and some recent ones\n                sorted_idx = np.argsort(archive_f)\n                n_keep_best = max(int(0.6 * keep_memory), keep_memory // 2)\n                n_keep_best = min(n_keep_best, len(sorted_idx))\n                best_idx = sorted_idx[:n_keep_best].tolist()\n                # keep some of most recent entries\n                recent_cnt = keep_memory - len(best_idx)\n                recent_idx = []\n                if recent_cnt > 0:\n                    start = max(0, len(archive_x) - recent_cnt)\n                    recent_idx = list(range(start, len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                # rebuild archives\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return\n        if self.x_opt is None:\n            # degenerate: return random point (call function if we still have budget)\n            x_try = self._uniform_array(lb, ub)\n            if evals < self.budget:\n                try:\n                    f_try = float(func(x_try))\n                except Exception:\n                    f_try = np.inf\n                evals += 1\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = float(f_try)\n            else:\n                # no budget left; return x_try with inf (best unknown)\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.169 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10355364649190657, 0.15069424723029912, 0.21896294191500654, 0.15696899687460364, 0.15328037593652744, 0.22259757549522285, 0.19191049914614722, 0.194384431887892, 0.16596903259382978, 0.13519504194113663]}, "task_prompt": ""}
{"id": "e16a3f0e-bdbb-4cf0-9a70-236b6d3b5a71", "fitness": 0.16384897083724395, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes DE-style donors, PCA-guided elite moves, anisotropic Gaussian local steps, heavy-tailed Cauchy jumps and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.20, init_scale=0.20,\n                 keep_memory=None, F=0.8, cr=0.2,\n                 seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        F: DE differential factor\n        cr: DE crossover rate\n        seed: RNG seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        if keep_memory is None:\n            self.keep_memory = max(200, 20 * self.dim)\n        else:\n            self.keep_memory = int(max(10, keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n        self.seed = seed\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # RNG\n        self.rng = np.random.default_rng(self.seed)\n\n    def _extract_bounds(self, func):\n        # Try common formats, else fallback to [-5,5]^dim\n        lb = ub = None\n\n        # 1) attribute .bounds possibly as tuple/list or object with lb/ub\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # if bounds is an object with .lb/.ub attributes\n                lb_attr = getattr(b, \"lb\", None)\n                ub_attr = getattr(b, \"ub\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                else:\n                    # maybe tuple/list (lb, ub)\n                    if isinstance(b, (tuple, list)) and len(b) == 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            lb = ub = None\n\n        # 2) attributes directly on func: .lb, .ub or .lower, .upper\n        if lb is None or ub is None:\n            try:\n                lb_attr = getattr(func, \"lb\", None)\n                ub_attr = getattr(func, \"ub\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                pass\n\n        if lb is None or ub is None:\n            try:\n                lb_attr = getattr(func, \"lower\", None)\n                ub_attr = getattr(func, \"upper\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                pass\n\n        # default fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast / resize to dim\n        try:\n            lb = np.broadcast_to(lb, (self.dim,)).astype(float).copy()\n            ub = np.broadcast_to(ub, (self.dim,)).astype(float).copy()\n        except Exception:\n            lb = np.resize(np.asarray(lb, dtype=float), self.dim).astype(float).copy()\n            ub = np.resize(np.asarray(ub, dtype=float), self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub, fallback to [-5,5]\n        if np.any(lb >= ub):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # reflect values outside bounds back into box symmetrically a few times\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = 2.0 * lb[below] - x[below]\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.random(self.dim) * (ub - lb)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        \"\"\"\n        # get bounds\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # base per-dimension scale\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # Archives\n        archive_x = []  # list of numpy arrays\n        archive_f = []\n\n        # initial sampling budget: ensure some initial diversity\n        n_init = int(min(max(10, 5 * self.dim), max(1, self.budget // 10)))\n        evals = 0\n\n        # quick helpers\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            try:\n                val = float(func(x))\n            except Exception:\n                val = np.inf\n            evals += 1\n            return val\n\n        # initial sampling\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_success = []  # list of 0/1\n        recent_window = min(200, max(20, 10 * self.dim))\n        stagnation_limit = max(40, 10 * self.dim)\n        stagnation_counter = 0\n        # move probabilities\n        p_local = 0.35\n        p_de = 0.18\n        p_pca = 0.25\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # bookkeeping for pruning\n        keep_memory = max(self.keep_memory, 50 + 10 * self.dim)\n\n        # main loop\n        while evals < self.budget:\n            # compute sorted indices by fitness (lower is better)\n            if len(archive_f) > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # choose base (mostly best, sometimes random from archive or uniform)\n            choose = self.rng.random()\n            if self.x_opt is not None and choose < 0.8 and len(archive_x) > 0:\n                base = np.array(self.x_opt, dtype=float)\n                base_from_archive = True\n            elif len(archive_x) > 0 and choose < 0.95:\n                idx = self.rng.integers(0, len(archive_x))\n                base = np.array(archive_x[idx], dtype=float)\n                base_from_archive = True\n            else:\n                base = self._uniform_array(lb, ub)\n                base_from_archive = False\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            # scale grows with gscale but never below tiny\n            scale = base_scale_vec * gscale\n            scale = np.maximum(scale, 1e-12)\n\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropic multipliers to allow some dimensions to be larger\n                rand_mult = 0.6 + 1.2 * self.rng.random(self.dim)\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult)\n                # small extra isotropic jitter\n                cand += 0.01 * scale * self.rng.normal(size=self.dim)\n\n            # DE-style donor move with small crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # pick three distinct indices\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                a = np.array(archive_x[idxs[0]], dtype=float)\n                b = np.array(archive_x[idxs[1]], dtype=float)\n                c = np.array(archive_x[idxs[2]], dtype=float)\n                donor = a + self.F * (b - c)\n                # build candidate from base then crossover\n                cand = base.copy()\n                mask = (self.rng.random(self.dim) < self.cr)\n                # ensure at least one dimension changed\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand += 0.02 * scale * self.rng.normal(size=self.dim)\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 2:\n                n_keep = max(2, int(np.ceil(self.elite_frac * len(archive_x))))\n                n_keep = min(n_keep, len(archive_x))\n                if n_keep < 2:\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n                else:\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                    # center: either mean or best elite (randomized)\n                    if self.rng.random() < 0.5:\n                        center = np.mean(elites, axis=0)\n                    else:\n                        center = np.array(elites[0], dtype=float)\n                    # covariance and SVD\n                    try:\n                        if elites.shape[0] >= 2:\n                            cov = np.cov(elites, rowvar=False)\n                            # regularize\n                            cov += np.eye(self.dim) * 1e-8 * (np.mean(np.diag(cov)) + 1e-12)\n                            U, Svals, _ = np.linalg.svd(cov, full_matrices=False)\n                            # construct random coefficients along principal directions\n                            sqrt_vals = np.sqrt(np.maximum(Svals, 0.0))\n                            coeffs = self.rng.normal(0.0, 1.0, size=len(Svals)) * sqrt_vals\n                            # scale coefficients by global gscale and by mean span to be comparable to scale\n                            mean_root = max(1e-12, np.mean(np.sqrt(np.maximum(Svals, 1e-12))))\n                            gmult = (gscale * (0.8 + 0.8 * self.rng.random())) / mean_root\n                            coeffs = coeffs * gmult\n                            # build candidate\n                            cand = center + (U @ coeffs)\n                            # add small isotropic jitter\n                            cand += 0.01 * scale * self.rng.normal(size=self.dim)\n                        else:\n                            # fallback\n                            jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                            cand = center + jitter * scale\n                    except Exception:\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        cand = center + jitter * scale\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy via standard_cauchy\n                try:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                except Exception:\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails to reduce catastrophic jumps\n                cauch = np.clip(cauch, -50.0, 50.0)\n                # scale by per-dim scale and gscale with some randomness\n                mult = (0.6 + 1.4 * self.rng.random(self.dim)) * gscale\n                cand = base + cauch * (scale * 0.6 * mult)\n                # small damping jitter\n                cand += 0.01 * scale * self.rng.normal(size=self.dim)\n\n            # replace any non-finite dims with random in bound for that dim\n            nonfinite = ~np.isfinite(cand)\n            if nonfinite.any():\n                uniform_fill = self._uniform_array(lb, ub)\n                cand[nonfinite] = uniform_fill[nonfinite]\n\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            if evals >= self.budget:\n                break\n            f_cand = safe_eval(cand)\n\n            # update bests & archives\n            archive_x.append(cand.copy())\n            archive_f.append(float(f_cand))\n\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                improved = True\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # update recent success window\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            # if we are successful often, shrink to exploit; if failing, expand\n            if succ_rate > 0.25:\n                gscale *= 0.97\n            elif succ_rate < 0.05:\n                gscale *= 1.05\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            # clip gscale to reasonable range\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter >= stagnation_limit or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                old_gscale = gscale\n                # perform modest injection batch\n                remaining = max(0, self.budget - evals)\n                n_inject = min(max(3, 2 * self.dim), max(1, remaining // 5)) if remaining > 0 else 0\n                # if too few remaining, skip injection\n                if n_inject > 0:\n                    # temporarily expand gscale to favor broader exploration during injection\n                    gscale *= (1.0 + 0.5 * self.rng.random())\n                    for _inj in range(n_inject):\n                        if evals >= self.budget:\n                            break\n                        if self.rng.random() < 0.6 or len(archive_x) < 5:\n                            # global injection: random point\n                            x_inj = self._uniform_array(lb, ub)\n                        else:\n                            # local injection around a top elite or best\n                            top_k = min(max(2, int(0.1 * len(archive_x))), len(archive_x))\n                            if len(archive_x) == 0:\n                                x_inj = self._uniform_array(lb, ub)\n                            else:\n                                top_idx = sorted_idx[:top_k]\n                                center = np.array(archive_x[self.rng.choice(top_idx)], dtype=float)\n                                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                                x_inj = center + jitter * (scale * (1.0 + self.rng.random()))\n                        # reflect & ensure finite\n                        x_inj = np.array(x_inj, dtype=float)\n                        nonfinite_inj = ~np.isfinite(x_inj)\n                        if nonfinite_inj.any():\n                            fill = self._uniform_array(lb, ub)\n                            x_inj[nonfinite_inj] = fill[nonfinite_inj]\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                        f_inj = safe_eval(x_inj)\n                        archive_x.append(x_inj.copy())\n                        archive_f.append(float(f_inj))\n                        if f_inj < self.f_opt - 1e-12:\n                            self.x_opt = np.array(x_inj, dtype=float)\n                            self.f_opt = float(f_inj)\n                            stagnation_counter = 0\n                    # gentle reset of scale\n                    gscale = min(old_gscale * 1.2, gscale)\n\n            # prune memory periodically\n            if len(archive_x) > keep_memory * 1.2:\n                sorted_idx = np.argsort(archive_f)\n                n_keep_best = max(int(0.6 * keep_memory), keep_memory // 2)\n                n_keep_best = min(n_keep_best, len(sorted_idx))\n                best_idx = sorted_idx[:n_keep_best].tolist()\n                # keep some of most recent entries\n                recent_cnt = keep_memory - len(best_idx)\n                recent_idx = []\n                if recent_cnt > 0:\n                    start = max(0, len(archive_x) - recent_cnt)\n                    recent_idx = list(range(start, len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                # rebuild archives\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure we have a best reported\n        if self.x_opt is None and len(archive_x) > 0:\n            best_idx = int(np.argmin(archive_f))\n            self.x_opt = np.array(archive_x[best_idx], dtype=float)\n            self.f_opt = float(archive_f[best_idx])\n\n        if self.x_opt is None:\n            # degenerate: produce one random point (if still allowed evaluate it)\n            x_try = self._uniform_array(lb, ub)\n            if evals < self.budget:\n                f_try = safe_eval(x_try)\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = float(f_try)\n            else:\n                # no budget left; return the random without evaluating (unknown)\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = float(np.inf)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.164 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08631816179168705, 0.15847224189821896, 0.24104223752966503, 0.16983064542858128, 0.14211849739899562, 0.17083222912903695, 0.1931905132620223, 0.16810081024287848, 0.18234416653362784, 0.12624020515772583]}, "task_prompt": ""}
{"id": "db8235f2-27dd-4d0b-959f-2c0be8aa88ce", "fitness": 0.3762091863761145, "name": "HybridAdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes DE-style donors, PCA-guided elite local moves, anisotropic Gaussian exploration, heavy-tailed Cauchy jumps and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass HybridAdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.20, init_scale=0.15,\n                 keep_memory=None, F=0.8, cr=0.2,\n                 seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        elite_frac: fraction of remembered points considered \"elite\" for PCA\n        init_scale: initial relative scale (fraction of search range) for local proposals\n        keep_memory: approximate number of evaluated points to remember (pruned periodically)\n        F: DE differential factor\n        cr: DE crossover rate\n        seed: RNG seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        # memory size\n        if keep_memory is None:\n            self.keep_memory = max(200, 20 * self.dim)\n        else:\n            self.keep_memory = int(max(10, keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n        self.seed = None if seed is None else int(seed)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # RNG\n        self.rng = np.random.default_rng(self.seed)\n\n    def _extract_bounds(self, func):\n        # Try common formats, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\")\n            if b is None:\n                raise AttributeError\n            # object with lb/ub attributes\n            lb_attr = getattr(b, \"lb\", None)\n            ub_attr = getattr(b, \"ub\", None)\n            if lb_attr is not None and ub_attr is not None:\n                lb = np.asarray(lb_attr, dtype=float)\n                ub = np.asarray(ub_attr, dtype=float)\n            else:\n                # assume tuple/list (lb, ub)\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast / resize to dim\n        try:\n            lb = np.broadcast_to(lb, (self.dim,)).astype(float).copy()\n            ub = np.broadcast_to(ub, (self.dim,)).astype(float).copy()\n        except Exception:\n            lb = np.resize(lb, self.dim).astype(float).copy()\n            ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub, fallback to [-5,5]\n        if np.any(lb >= ub):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # reflect values outside bounds back into box symmetrically a few times\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = 2.0 * lb[below] - x[below]\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns (f_opt, x_opt).\n        \"\"\"\n        # bounds\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # base per-dimension scale\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # Archives\n        archive_x = []  # list of numpy arrays\n        archive_f = []\n\n        # initial sampling budget: ensure some initial diversity\n        evals = 0\n        # initial points: at least 2*dim or up to 5% of budget, whichever smaller\n        n_init = min(max(10, 2 * self.dim), max(5, int(0.05 * self.budget)))\n        n_init = min(n_init, self.budget)\n\n        for _ in range(n_init):\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_success = []  # list of 0/1\n        recent_window = max(30, 5 * self.dim)\n        stagnation_limit = max(40, 10 * self.dim)\n        stagnation_counter = 0\n\n        # move probabilities\n        p_local = 0.45\n        p_pca = 0.20\n        p_de = 0.20\n        p_cauchy = 1.0 - (p_local + p_pca + p_de)\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # injection settings\n        n_inject = max(3, int(0.02 * self.budget))\n\n        # bookkeeping for pruning\n        keep_memory = self.keep_memory\n\n        # main loop\n        while evals < self.budget:\n            # compute sorted indices by fitness (lower is better)\n            if len(archive_f) > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # choose base (mostly best, sometimes random from archive or uniform)\n            r_base = self.rng.random()\n            if self.x_opt is not None and r_base < 0.75 and len(archive_x) > 0:\n                base = np.array(self.x_opt, dtype=float)\n            elif len(archive_x) > 0 and r_base < 0.95:\n                idx = self.rng.integers(0, len(archive_x))\n                base = np.array(archive_x[idx], dtype=float)\n            else:\n                base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            scale = base_scale_vec * gscale\n            scale = np.maximum(scale, 1e-12)\n\n            # prepare elite info for PCA\n            n_keep = max(2, int(np.ceil(self.elite_frac * len(archive_x)))) if len(archive_x) >= 2 else 0\n            elite_idx = sorted_idx[:n_keep].tolist() if n_keep > 0 else []\n            # top indices for injections\n            top_idx = sorted_idx[:max(1, int(max(1, 0.05 * len(archive_x))))].tolist() if len(archive_x) > 0 else []\n\n            # choose move type\n            r = self.rng.random()\n\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropic multipliers: log-normal-ish to allow occasional larger steps on some dims\n                rand_mult = np.exp(self.rng.normal(0.0, 0.5, size=self.dim))\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult * (0.8 + 0.4 * self.rng.random()))\n                # small directional bias towards best occasionally\n                if self.x_opt is not None and self.rng.random() < 0.2:\n                    cand += (self.x_opt - base) * (0.05 * self.rng.random())\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_pca and n_keep >= 2:\n                elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                # center: mean of elites\n                center = np.mean(elites, axis=0)\n                # compute covariance of elite deviations\n                dev = elites - center\n                cov = np.cov(dev.T) if elites.shape[0] > 1 else np.eye(self.dim) * 1e-8\n                # regularize\n                cov += np.eye(self.dim) * 1e-8 * (np.mean(np.diag(cov)) + 1e-12)\n                # SVD/eig\n                try:\n                    vals, vecs = np.linalg.eigh(cov)\n                    vals = np.maximum(vals, 0.0)\n                    # sample along principal components with scaled coefficients\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * np.sqrt(vals + 1e-12)\n                    coeffs *= (1.0 + 0.5 * self.rng.random()) * gscale\n                    cand = center + vecs @ coeffs\n                    # add small isotropic jitter\n                    cand += self.rng.normal(0.0, 0.03, size=self.dim) * scale\n                except Exception:\n                    # fallback to local gaussian\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n\n            # DE-style donor move with crossover\n            elif r < p_local + p_pca + p_de and len(archive_x) >= 3:\n                # pick three distinct indices\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                a = np.array(archive_x[idxs[0]], dtype=float)\n                b = np.array(archive_x[idxs[1]], dtype=float)\n                c = np.array(archive_x[idxs[2]], dtype=float)\n                donor = a + self.F * (b - c)\n                # crossover mask (ensure at least one dim taken)\n                mask = (self.rng.random(self.dim) < self.cr)\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand = base.copy()\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n                # occasional pull toward best\n                if self.x_opt is not None and self.rng.random() < 0.15:\n                    cand += (self.x_opt - base) * (0.02 * self.rng.random())\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy via tan(pi*(u-0.5))\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails to reduce catastrophic jumps\n                cauch = np.clip(cauch, -50.0, 50.0)\n                mult = 0.6 + 0.8 * self.rng.random()\n                cand = base + cauch * (scale * mult * gscale)\n                # small damping jitter\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # Ensure candidate finite and within bounds (reflect)\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            # replace any non-finite dims with random in bound for that dim\n            nonfinite = ~np.isfinite(cand)\n            if nonfinite.any():\n                uniform_fill = self._uniform_array(lb, ub)\n                cand[nonfinite] = uniform_fill[nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # check budget before evaluating (double-check)\n            if evals >= self.budget:\n                break\n\n            # evaluate candidate\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            improved = False\n            # update bests & archives\n            archive_x.append(cand.copy())\n            archive_f.append(f_cand)\n            # improvement handling\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # update recent success window\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            if succ_rate > 0.25:\n                gscale *= (1.0 + 0.12 * min(1.0, (succ_rate - 0.25) / 0.75))\n            elif succ_rate < 0.05:\n                gscale *= 0.995\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            # clip gscale to reasonable range\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter >= stagnation_limit or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                # perform modest injection batch (but not exceeding budget)\n                remaining = max(0, self.budget - evals)\n                if remaining <= 0:\n                    break\n                inject_count = min(n_inject, remaining)\n                # temporarily expand gscale to favor broader exploration during injection\n                old_gscale = gscale\n                gscale *= (1.0 + 0.5 * self.rng.random())\n                for _inj in range(inject_count):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.random() < 0.6 or len(archive_x) < 5:\n                        # global injection: random point\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # local injection around a top elite or best\n                        if len(top_idx) > 0:\n                            center = np.array(archive_x[self.rng.choice(top_idx)], dtype=float)\n                        elif self.x_opt is not None:\n                            center = np.array(self.x_opt, dtype=float)\n                        else:\n                            center = self._uniform_array(lb, ub)\n                        # create local perturbation with moderate scale\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        x_inj = center + jitter * (scale * (0.8 + 1.2 * self.rng.random()))\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt - 1e-12:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        stagnation_counter = 0\n                # gentle reset of scale\n                gscale = min(max(gscale * 0.8, 1e-6), old_gscale * 1.5)\n\n            # prune memory periodically\n            if len(archive_x) > keep_memory * 1.2:\n                sorted_idx = np.argsort(archive_f)\n                n_keep_best = max(int(0.6 * keep_memory), keep_memory // 2)\n                n_keep_best = min(n_keep_best, len(sorted_idx))\n                best_idx = sorted_idx[:n_keep_best].tolist()\n                # keep some of most recent entries\n                recent_cnt = keep_memory - len(best_idx)\n                recent_idx = []\n                if recent_cnt > 0:\n                    start = max(0, len(archive_x) - recent_cnt)\n                    recent_idx = list(range(start, len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                # rebuild archives\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return\n        if self.x_opt is None:\n            # degenerate: return random point (call function if we still have budget)\n            x_try = self._uniform_array(lb, ub)\n            if evals < self.budget:\n                try:\n                    f_try = float(func(x_try))\n                except Exception:\n                    f_try = np.inf\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = float(f_try)\n            else:\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridAdaptiveDirectionalSampling scored 0.376 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1750681662396385, 0.16932220828003686, 0.5383780895090373, 0.8381999729659786, 0.43801392736530753, 0.41738794297768056, 0.25240067802216737, 0.28946178699310476, 0.44462108059081085, 0.1992380108173829]}, "task_prompt": ""}
{"id": "eaaf06ac-5dff-436f-94fa-73357144ffdd", "fitness": 0.4236228314297604, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — a memory-guided sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local search, heavy-tailed Cauchy jumps and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.20, init_scale=0.20,\n                 keep_memory=None, F=0.6, cr=0.9, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        if keep_memory is None:\n            self.keep_memory = max(200, 20 * self.dim)\n        else:\n            self.keep_memory = int(max(10, keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        # Try common formats, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            # If func has .bounds attribute that is an object with lb/ub\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # handle object with attributes\n                lb_attr = getattr(b, \"lb\", None)\n                ub_attr = getattr(b, \"ub\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                else:\n                    # maybe bounds is a tuple/list (lb, ub)\n                    if isinstance(b, (tuple, list)) and len(b) >= 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast/resize to dim\n            try:\n                lb = np.broadcast_to(lb, (self.dim,)).astype(float).copy()\n                ub = np.broadcast_to(ub, (self.dim,)).astype(float).copy()\n            except Exception:\n                lb = np.resize(lb, self.dim).astype(float).copy()\n                ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub, fallback to [-5,5]\n        if np.any(lb >= ub):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = 2.0 * lb[below] - x[below]\n            if above.any():\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        archive_x = []\n        archive_f = []\n\n        # initial sampling budget: ensure some initial diversity\n        evals = 0\n        n_init = min(max(10, 3 * self.dim), max(3, self.budget // 10))\n\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_success = []\n        recent_window = max(50, 5 * self.dim)\n        stagnation_limit = max(40, 10 * self.dim)\n        stagnation_counter = 0\n\n        # move probabilities\n        p_local = 0.45\n        p_de = 0.18\n        p_pca = 0.22\n        # rest goes to heavy-tailed (Cauchy)\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # bookkeeping for pruning\n        keep_memory = max(self.keep_memory, 50 + 10 * self.dim)\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness\n            if len(archive_f) == 0:\n                sorted_idx = np.array([], dtype=int)\n            else:\n                sorted_idx = np.argsort(archive_f)\n\n            # pick base: mostly best, sometimes random archive or uniform\n            r_base = self.rng.random()\n            if self.x_opt is None or (r_base > 0.8 and len(archive_x) > 0 and self.rng.random() < 0.5):\n                # pick a randomly chosen archive member (bias to better)\n                if len(archive_x) > 0:\n                    idx = sorted_idx[self.rng.integers(low=0, high=len(sorted_idx))]\n                    base = np.array(archive_x[idx], dtype=float)\n                else:\n                    base = self._uniform_array(lb, ub)\n            elif self.x_opt is None:\n                base = self._uniform_array(lb, ub)\n            else:\n                # use best known\n                base = np.array(self.x_opt, dtype=float)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            scale = base_scale_vec * gscale\n            scale = np.maximum(scale, 1e-12)\n            # small random per-dim jitter for anisotropy\n            per_dim_jitter = 1.0 + 0.2 * self.rng.normal(0.0, 1.0, size=self.dim)\n            scale = np.maximum(scale * per_dim_jitter, 1e-12)\n\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropic multipliers to allow some dimensions larger\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult)\n\n            # DE-style donor move with crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # pick three distinct indices\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                x0 = np.array(archive_x[idxs[0]], dtype=float)\n                x1 = np.array(archive_x[idxs[1]], dtype=float)\n                x2 = np.array(archive_x[idxs[2]], dtype=float)\n                donor = x0 + self.F * (x1 - x2)\n                # crossover mask (ensure at least one dim taken)\n                mask = (self.rng.random(self.dim) < self.cr)\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 3:\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                n_keep = min(n_keep, len(archive_x))\n                if n_keep < 2:\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n                else:\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                    # choose center: either mean or one of elites (randomized)\n                    if self.rng.random() < 0.6:\n                        center = np.mean(elites, axis=0)\n                    else:\n                        center = elites[self.rng.integers(0, elites.shape[0])]\n                    # PCA / SVD\n                    try:\n                        cov = np.cov(elites, rowvar=False)\n                        # small regularization\n                        cov += np.eye(self.dim) * 1e-8 * np.mean(np.diag(cov) if cov.shape[0]>0 else 1.0)\n                        U, Svals, _ = np.linalg.svd(cov)\n                        sqrt_vals = np.sqrt(np.maximum(Svals, 0.0))\n                        # sample coefficients along principal directions (gaussian scaled by eigenvalues)\n                        coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * sqrt_vals\n                        # scale coefficients by adaptive gscale and span\n                        mean_root = max(np.mean(sqrt_vals), 1e-12)\n                        gmult = (gscale * (0.8 + 0.8 * self.rng.random())) / mean_root\n                        coeffs = coeffs * gmult\n                        cand = center + (U @ coeffs)\n                        # small isotropic jitter\n                        cand += self.rng.normal(0.0, 0.05, size=self.dim) * scale\n                    except Exception:\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        cand = base + jitter * scale\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails by clipping\n                cauch = np.clip(cauch, -1e2, 1e2)\n                mult = (0.6 + 1.4 * self.rng.random(self.dim)) * gscale\n                cand = base + cauch * (scale * 0.6 * mult)\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # Ensure candidate finite and within bounds (reflect)\n            cand = np.array(cand, dtype=float)\n            nonfinite = ~np.isfinite(cand)\n            if nonfinite.any():\n                fill = self._uniform_array(lb, ub)\n                cand[nonfinite] = fill[nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(cand.copy())\n            archive_f.append(f_cand)\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # update recent success window\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            # increase when success is good, shrink slowly otherwise\n            if succ_rate > 0.15:\n                gscale *= 1.06\n            else:\n                gscale *= 0.995\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if (stagnation_counter >= stagnation_limit) or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                # modest injection batch: add a few perturbed elites or randoms\n                injections = min(5, max(1, (self.dim // 4)))\n                old_gscale = gscale\n                gscale = min(gscale * 5.0, 1e2)  # temporarily broaden\n                for _inj in range(injections):\n                    if evals >= self.budget:\n                        break\n                    if len(archive_x) == 0 or self.rng.random() < 0.6:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # choose a top elite to perturb\n                        top_k = max(1, int(0.05 * len(archive_x)))\n                        top_k = min(top_k, len(archive_x))\n                        if top_k <= 0:\n                            x_inj = self._uniform_array(lb, ub)\n                        else:\n                            idx_top = sorted_idx[self.rng.integers(0, top_k)]\n                            center = np.array(archive_x[idx_top], dtype=float)\n                            # local perturbation with larger scale\n                            x_inj = center + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * gscale * (1.0 + self.rng.random()))\n                    nonfinite_inj = ~np.isfinite(x_inj)\n                    if nonfinite_inj.any():\n                        fill = self._uniform_array(lb, ub)\n                        x_inj[nonfinite_inj] = fill[nonfinite_inj]\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(f_inj)\n                    if f_inj < self.f_opt - 1e-12:\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        self.f_opt = float(f_inj)\n                        stagnation_counter = 0\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                # small cooldown\n                stagnation_counter = 0\n\n            # prune memory periodically\n            if len(archive_x) > 2 * keep_memory:\n                sorted_idx = np.argsort(archive_f)\n                n_keep_best = max(int(0.6 * keep_memory), min(len(sorted_idx), keep_memory // 2))\n                best_idx = list(sorted_idx[:n_keep_best])\n                # keep some of most recent entries\n                recent_cnt = keep_memory - len(best_idx)\n                recent_cnt = max(0, recent_cnt)\n                start = max(0, len(archive_x) - recent_cnt)\n                recent_idx = list(range(start, len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return\n        if self.x_opt is None:\n            # degenerate: return random point (call function if we still have budget)\n            x_try = self._uniform_array(lb, ub)\n            f_try = np.inf\n            if evals < self.budget:\n                try:\n                    f_try = float(func(x_try))\n                except Exception:\n                    f_try = np.inf\n                evals += 1\n            self.x_opt = np.array(x_try, dtype=float)\n            self.f_opt = float(f_try)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14343502461109991, 0.16353909776262288, 0.955888640369503, 0.17602939208915003, 0.9623710978763477, 0.9753068558120805, 0.24110355639297132, 0.24603914832870388, 0.2175666679653373, 0.15494883308978824]}, "task_prompt": ""}
{"id": "5ef602c0-de73-4753-a59b-5808fdbe1cb2", "fitness": 0.41450875735602233, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — mixes DE-style donors, PCA-guided elite moves, anisotropic Gaussian local search, heavy-tailed Cauchy jumps and occasional micro-restarts with an adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.20, init_scale=0.20,\n                 keep_memory=None, F=0.6, cr=0.9, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.keep_memory = int(keep_memory) if keep_memory is not None else None\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        # Try several common formats for bounds; fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # If bounds is object with attributes lb/ub\n                lb_attr = getattr(b, \"lb\", None)\n                ub_attr = getattr(b, \"ub\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                else:\n                    # maybe bounds is a tuple/list (lb, ub)\n                    if isinstance(b, (tuple, list)) and len(b) == 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast/resize to dim\n            try:\n                lb = np.broadcast_to(lb, (self.dim,)).astype(float).copy()\n                ub = np.broadcast_to(ub, (self.dim,)).astype(float).copy()\n            except Exception:\n                lb = np.resize(lb, self.dim).astype(float).copy()\n                ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub, fallback to [-5,5]\n        if np.any(lb >= ub):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        x = np.array(x, dtype=float)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = 2.0 * lb[below] - x[below]\n            if above.any():\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # bookkeeping / archives\n        archive_x = []\n        archive_f = []\n\n        # initial sampling budget: ensure some initial diversity\n        n_init = min(max(10, 3 * self.dim), max(3, self.budget // 10))\n        evals = 0\n\n        # evaluate initial samples\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                # if evaluation fails, assign a large value\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_window = max(50, 5 * self.dim)\n        stagnation_limit = max(40, 10 * self.dim)\n        stagnation_counter = 0\n        recent_success = []\n\n        # move probabilities\n        p_de = 0.18\n        p_pca = 0.22\n        p_local = 0.35  # local anisotropic gaussian\n        # rest goes to heavy-tailed (Cauchy)\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # bookkeeping for pruning\n        keep_memory = max(self.keep_memory or (200 + 10 * self.dim), 50 + 10 * self.dim)\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness\n            sorted_idx = np.argsort(np.array(archive_f))\n            n_archive = len(archive_x)\n\n            # pick base: mostly best, sometimes random archive or uniform\n            r_base = self.rng.random()\n            if n_archive == 0:\n                base = self._uniform_array(lb, ub)\n                base_idx = None\n            else:\n                if r_base < 0.60:\n                    # use best known\n                    base_idx = sorted_idx[0]\n                    base = np.array(archive_x[base_idx], dtype=float)\n                elif r_base < 0.9:\n                    # pick a randomly chosen archive member (bias to better)\n                    # sample from top half with higher probability\n                    bias = self.rng.random()\n                    if bias < 0.7:\n                        max_range = max(1, n_archive // 3)\n                        idx = self.rng.integers(low=0, high=max_range)\n                        base_idx = sorted_idx[idx]\n                    else:\n                        base_idx = sorted_idx[self.rng.integers(low=0, high=n_archive)]\n                    base = np.array(archive_x[base_idx], dtype=float)\n                else:\n                    # uniform random base\n                    base_idx = None\n                    base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            span = ub - lb\n            base_scale_vec = np.maximum(span * self.init_scale * gscale, 1e-12)\n            # small random per-dim jitter for anisotropy\n            rand_mult = 0.5 + self.rng.random(self.dim) * 1.5\n\n            cand = None\n\n            # choose move type\n            r = self.rng.random()\n            if r < p_local:\n                # Local anisotropic gaussian around base\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (base_scale_vec * rand_mult)\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * base_scale_vec\n            elif r < p_local + p_de and n_archive >= 3:\n                # DE-style donor move with crossover\n                # pick three distinct indices different from base_idx (if base_idx is not None)\n                idxs = []\n                tries = 0\n                while len(idxs) < 3 and tries < 100:\n                    i = self.rng.integers(0, n_archive)\n                    if i == base_idx:\n                        tries += 1\n                        continue\n                    if i in idxs:\n                        tries += 1\n                        continue\n                    idxs.append(i)\n                if len(idxs) < 3:\n                    # fallback to random uniform sample\n                    cand = self._uniform_array(lb, ub)\n                else:\n                    x0 = np.array(archive_x[idxs[0]], dtype=float)\n                    x1 = np.array(archive_x[idxs[1]], dtype=float)\n                    x2 = np.array(archive_x[idxs[2]], dtype=float)\n                    donor = x0 + self.F * (x1 - x2)\n                    # crossover mask (ensure at least one dim taken)\n                    mask = (self.rng.random(self.dim) < self.cr)\n                    if not mask.any():\n                        mask[self.rng.integers(0, self.dim)] = True\n                    cand = np.array(base, dtype=float)\n                    cand[mask] = donor[mask]\n                    # small gaussian jitter proportional to scale\n                    cand += self.rng.normal(0.0, 0.02, size=self.dim) * base_scale_vec\n            elif r < p_local + p_de + p_pca and n_archive >= max(2, int(self.elite_frac * n_archive)):\n                # PCA-guided sampling using elites\n                n_keep = max(2, int(self.elite_frac * n_archive))\n                elite_idx = sorted_idx[:n_keep]\n                elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                # choose center: either mean or one of elites (randomized)\n                if self.rng.random() < 0.5:\n                    center = np.mean(elites, axis=0)\n                else:\n                    center = elites[self.rng.integers(0, elites.shape[0])]\n                try:\n                    cov = np.cov(elites, rowvar=False)\n                    # small regularization\n                    cov += np.eye(self.dim) * 1e-8\n                    # SVD for principal directions\n                    U, svals, _ = np.linalg.svd(cov)\n                    # sqrt eigenvalues\n                    sqrt_vals = np.sqrt(np.maximum(svals, 0.0))\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * sqrt_vals\n                    # scale coefficients by adaptive gscale and span\n                    mean_root = max(np.mean(sqrt_vals), 1e-12)\n                    gmult = max(0.5 * gscale, 1e-6)\n                    coeffs = coeffs * (gmult * (span / (span.mean() + 1e-12)))\n                    cand = center + (U @ coeffs)\n                    # small isotropic jitter\n                    cand += self.rng.normal(0.0, 0.02, size=self.dim) * base_scale_vec\n                except Exception:\n                    # fallback to local jitter\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = center + jitter * (base_scale_vec * rand_mult)\n            else:\n                # heavy-tailed per-dim Cauchy jumps (Levy-like)\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails by clipping\n                cauch = np.clip(cauch, -1e2, 1e2)\n                mult = (0.6 + 1.4 * self.rng.random(self.dim)) * gscale\n                cand = base + cauch * (base_scale_vec * 0.6 * mult)\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * base_scale_vec\n\n            # Ensure candidate finite and within bounds (reflect)\n            cand = np.array(cand, dtype=float)\n            nonfinite = ~np.isfinite(cand)\n            if nonfinite.any():\n                fill = self._uniform_array(lb, ub)\n                cand[nonfinite] = fill[nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(cand.copy())\n            archive_f.append(f_cand)\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # update recent success window\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            if succ_rate > 0.2:\n                gscale *= 1.06\n            elif succ_rate < 0.05:\n                gscale *= 0.995\n            else:\n                # gentle drift\n                gscale *= 1.002\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if (stagnation_counter >= stagnation_limit) or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                # modest injection batch: add a few perturbed elites or randoms\n                old_gscale = gscale\n                gscale = min(gscale * 5.0, 1e2)  # temporarily broaden\n                n_inject = min( max(3, 2 * self.dim), (self.budget - evals) )\n                for k in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    if n_archive > 0 and self.rng.random() < 0.7:\n                        # perturb a top elite\n                        top_k = max(1, int(0.05 * n_archive))\n                        idx_top = sorted_idx[self.rng.integers(0, top_k)]\n                        x_inj = np.array(archive_x[idx_top], dtype=float)\n                        # local perturbation with larger scale\n                        mult = 1.0 + self.rng.random() * 4.0\n                        x_inj = x_inj + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * mult)\n                    else:\n                        x_inj = self._uniform_array(lb, ub)\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                # gently reset gscale\n                gscale = max(0.5 * old_gscale, 1e-6)\n                # small cooldown\n                stagnation_counter = 0\n\n            # prune memory periodically\n            if len(archive_x) > keep_memory * 2:\n                sorted_idx = np.argsort(np.array(archive_f))\n                n_keep_best = max(int(0.6 * keep_memory), min(len(sorted_idx), keep_memory // 2))\n                best_idx = list(sorted_idx[:n_keep_best])\n                # keep some of most recent entries\n                recent_idx = list(range(max(0, len(archive_x) - keep_memory), len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure x_opt exists, otherwise try a final evaluation of a random point if budget allows\n        if self.x_opt is None:\n            if evals < self.budget:\n                x_try = self._uniform_array(lb, ub)\n                try:\n                    f_try = float(func(x_try))\n                except Exception:\n                    f_try = np.inf\n                evals += 1\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = float(f_try)\n            else:\n                # no budget left and no best: pick best from archive if any\n                if len(archive_f) > 0:\n                    best_idx = int(np.argmin(np.array(archive_f)))\n                    self.x_opt = np.array(archive_x[best_idx], dtype=float)\n                    self.f_opt = float(archive_f[best_idx])\n                else:\n                    # degenerate fallback\n                    self.x_opt = np.full(self.dim, 0.0)\n                    self.f_opt = np.inf\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.415 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13722903518274598, 0.17779745410977743, 0.9050063864073614, 0.9201972051431524, 0.2542491669664715, 0.8331545955083561, 0.22115759690745285, 0.2148327477423988, 0.32259177474000356, 0.15887161085250334]}, "task_prompt": ""}
{"id": "ff317fbd-6fdd-48fe-a08b-34ae6eb28a9c", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — a memory-guided sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local search, heavy-tailed Cauchy jumps and occasional micro-restarts to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 keep_memory=200, elite_frac=0.15, init_scale=0.20,\n                 F=0.8, cr=0.9, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.keep_memory = int(max(5, keep_memory))\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim\n        \"\"\"\n        lb = None\n        ub = None\n        # Try attributes like .bounds, .bounds.lb/.bounds.ub, .lower/.upper, or tuple (lb, ub)\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            try:\n                # tuple/list-like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # object with attributes\n                    lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                    ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n                    if lb_attr is not None and ub_attr is not None:\n                        lb = np.asarray(lb_attr, dtype=float)\n                        ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # fallback to explicit attributes on func\n        if lb is None or ub is None:\n            try:\n                lb_attr = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n                ub_attr = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast/resize to dim\n            lb = np.resize(lb, self.dim).astype(float).copy()\n            ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub fallback to [-5,5]\n        if np.any(lb >= ub) or np.any(~np.isfinite(lb)) or np.any(~np.isfinite(ub)):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.random(self.dim) * (ub - lb) + lb\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # bookkeeping\n        archive_x = []    # list of numpy arrays\n        archive_f = []    # list of floats\n        evals = 0\n\n        # initial sampling: ensure some initial diversity but not consume too much budget\n        init_n = min(max(10, int(0.05 * self.budget)), self.budget)\n        for _ in range(init_n):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_window = max(50, 5 * self.dim)\n        stagnation_limit = max(40, 10 * self.dim)\n        recent_success = deque(maxlen=recent_window)\n        stagnation_counter = 0\n\n        # move probabilities\n        p_local = 0.35\n        p_de = 0.20\n        p_pca = 0.25\n        # rest (1 - p_local - p_de - p_pca) => heavy-tailed\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness\n            if len(archive_f) > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # pick base: mostly best, sometimes random archive or uniform\n            r_base = self.rng.random()\n            if len(archive_x) == 0 or r_base > 0.85:\n                base = self._uniform_array(lb, ub)\n            elif r_base > 0.15:\n                # pick biased toward better indices: sample index from exponential over sorted ranks\n                if len(sorted_idx) > 0:\n                    # bias smaller rank\n                    rank = int(min(len(sorted_idx) - 1,\n                                   self.rng.exponential(scale=1.5)))\n                    idx = sorted_idx[rank]\n                    base = np.array(archive_x[idx], dtype=float)\n                else:\n                    base = self._uniform_array(lb, ub)\n            else:\n                # use best known\n                if self.x_opt is None:\n                    base = self._uniform_array(lb, ub)\n                else:\n                    base = np.array(self.x_opt, dtype=float)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_jitter = np.exp(0.15 * self.rng.normal(0.0, 1.0, size=self.dim))\n            scale = base_scale_vec * gscale\n            scale = np.maximum(scale * per_dim_jitter, 1e-12)\n\n            # choose move type\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult * 0.7)\n\n            # DE-style donor move with crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # choose three distinct indices from archive\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                x0 = np.array(archive_x[idxs[0]], dtype=float)\n                x1 = np.array(archive_x[idxs[1]], dtype=float)\n                x2 = np.array(archive_x[idxs[2]], dtype=float)\n                donor = x0 + self.F * (x1 - x2)\n                # crossover mask (ensure at least one dim taken)\n                mask = (self.rng.random(self.dim) < self.cr)\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand = cand + self.rng.normal(0.0, 0.5, size=self.dim) * (scale * 0.2)\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 3:\n                n_keep = max(2, int(max(2, self.elite_frac * len(archive_x))))\n                n_keep = min(n_keep, len(archive_x))\n                elite_idx = sorted_idx[:n_keep]\n                elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                center_choice = self.rng.random()\n                if center_choice < 0.6:\n                    center = np.mean(elites, axis=0)\n                else:\n                    center = elites[self.rng.integers(0, n_keep)]\n                # compute covariance and principal components\n                try:\n                    cov = np.cov(elites, rowvar=False)\n                    # regularize\n                    cov += 1e-10 * np.eye(self.dim)\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # sort by descending eigenvalue\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    sqrt_vals = np.sqrt(np.maximum(eigvals, 1e-12))\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * (sqrt_vals * (gscale * 1.0))\n                    # scale by global span to keep consistent scale across dims\n                    coeffs = coeffs * (span / (np.mean(span) + 1e-12))\n                    cand = center + eigvecs.dot(coeffs)\n                    # small isotropic jitter\n                    cand = cand + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.05)\n                except Exception:\n                    # fallback to local jitter\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * scale\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails by clipping\n                cauch = np.clip(cauch, -1e2, 1e2)\n                mult = 0.8 + 0.6 * self.rng.random()\n                cand = base + cauch * (scale * 0.6 * mult)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if not np.isfinite(cand).all():\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n            improved = False\n            if f_cand < self.f_opt:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= 10:\n                success_rate = float(sum(recent_success)) / len(recent_success)\n                if success_rate > 0.25:\n                    gscale *= 1.15\n                elif success_rate < 0.05:\n                    gscale *= 0.85\n                # small multiplicative jitter for exploration\n                gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter > stagnation_limit and evals < self.budget:\n                old_gscale = gscale\n                gscale = min(gscale * 6.0, 1e2)  # broaden temporarily\n                injection_budget = min( max(3, self.dim), self.budget - evals, int(0.03 * self.budget) )\n                for _ in range(injection_budget):\n                    if evals >= self.budget:\n                        break\n                    if len(archive_x) == 0 or self.rng.random() < 0.6:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # perturb a top elite\n                        top_k = max(1, min(len(archive_x), int(0.05 * len(archive_x) + 1)))\n                        idx = sorted_idx[self.rng.integers(0, top_k)]\n                        elite = np.array(archive_x[idx], dtype=float)\n                        # larger perturbation\n                        x_inj = elite + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * gscale * (1.0 + self.rng.random()))\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        self.f_opt = float(f_inj)\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                stagnation_counter = 0\n                recent_success.clear()\n\n            # prune memory periodically\n            if len(archive_x) > 2 * self.keep_memory:\n                # keep some best, some recent\n                sorted_idx = np.argsort(archive_f)\n                n_best_keep = max(int(0.6 * self.keep_memory), min(len(sorted_idx), self.keep_memory // 2))\n                best_idx = list(sorted_idx[:n_best_keep])\n                recent_cnt = min(self.keep_memory - n_best_keep, len(archive_x))\n                recent_idx = list(range(len(archive_x) - recent_cnt, len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            x_try = self._uniform_array(lb, ub)\n            try:\n                self.f_opt = float(func(x_try))\n            except Exception:\n                self.f_opt = np.inf\n            self.x_opt = x_try\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example quick test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 146, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "error": "In the code, line 146, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "69560137-31ea-4099-8bda-8b728401595c", "fitness": "-inf", "name": "HybridAdaptiveDirectionalSampler", "description": "Hybrid Adaptive Directional Sampling (ADS) — a memory-guided sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local search, heavy-tailed Cauchy jumps and occasional micro-restarts to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\nfrom collections import deque\n\nclass HybridAdaptiveDirectionalSampler:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n\n    Usage:\n        sampler = HybridAdaptiveDirectionalSampler(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 keep_memory=200, elite_frac=0.15, init_scale=0.20,\n                 F=0.8, cr=0.9, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.keep_memory = int(keep_memory)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.F = float(F)\n        self.cr = float(cr)\n        self.seed = seed\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim\n        Supported: func.bounds as (lb, ub) or object with .lb/.ub, .lower/.upper, or func.lower/upper attrs.\n        \"\"\"\n        lb = None\n        ub = None\n        # try func.bounds\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # tuple/list-like (lb, ub)\n            try:\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n        # try attributes on bounds-like object if b provided but not tuple\n        if lb is None and b is not None:\n            lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n            ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n            if lb_attr is not None and ub_attr is not None:\n                try:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # try direct attributes on func\n        if lb is None:\n            lb_attr = getattr(func, \"lb\", None) or getattr(func, \"lower\", None) or getattr(func, \"lower_bounds\", None)\n            ub_attr = getattr(func, \"ub\", None) or getattr(func, \"upper\", None) or getattr(func, \"upper_bounds\", None)\n            if lb_attr is not None and ub_attr is not None:\n                try:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # final fallback to [-5,5]^dim\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast to dim\n        try:\n            lb = np.broadcast_to(lb, (self.dim,)).astype(float)\n            ub = np.broadcast_to(ub, (self.dim,)).astype(float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure valid\n        if np.any(lb >= ub) or np.any(~np.isfinite(lb)) or np.any(~np.isfinite(ub)):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = x.astype(float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below\n            if np.any(below):\n                x[below] = 2.0 * lb[below] - x[below]\n            # reflect above\n            if np.any(above):\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # bookkeeping\n        evals = 0\n        archive_x = []\n        archive_f = []\n\n        # initial sampling: seed some initial diverse points\n        initial_n = int(min(self.keep_memory, max(2 * self.dim, 10, self.budget // 10)))\n        initial_n = max(2, initial_n)\n        for _ in range(initial_n):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        stagnation_limit = max(40, 10 * self.dim)\n        recent_window = max(30, 5 * self.dim)\n        recent_success = deque(maxlen=recent_window)\n\n        # move probabilities\n        p_de = 0.20\n        p_pca = 0.25\n        p_local = 0.35\n        # remainder -> heavy-tailed Cauchy\n\n        # global PCA/gscale controlling factor\n        gscale = 1.0\n\n        # main loop\n        while evals < self.budget:\n            # prepare sorted archive\n            archive_f_arr = np.array(archive_f, dtype=float)\n            sorted_idx = np.argsort(archive_f_arr)\n            n_archive = len(archive_x)\n\n            # pick base: mostly best, sometimes random from archive, sometimes uniform\n            r = self.rng.random()\n            if r < 0.7 and n_archive > 0:\n                base = np.array(archive_x[sorted_idx[0]], dtype=float)\n            elif r < 0.95 and n_archive > 0:\n                # biased toward better indices\n                ranks = np.arange(n_archive)\n                # exponential bias\n                probs = np.exp(-ranks / max(1.0, 0.1 * n_archive))\n                probs = probs / probs.sum()\n                chosen = self.rng.choice(n_archive, p=probs)\n                base = np.array(archive_x[sorted_idx[chosen]], dtype=float)\n            else:\n                base = self._uniform_array(lb, ub)\n\n            # adapt per-dim scale vector (introduce jitter)\n            scale = base_scale_vec * (0.5 + 1.5 * gscale)  # elementwise\n            scale = np.maximum(scale, 1e-12)\n\n            # choose move type\n            move_r = self.rng.random()\n            cand = None\n\n            if move_r < p_local:\n                # Local anisotropic gaussian around base\n                # multiplicative log-normal per-dim + gaussian jitter\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + (rand_mult * jitter) * scale\n            elif move_r < p_local + p_de and n_archive >= 4:\n                # DE-style donor move with binomial crossover\n                # choose three distinct indices different from base if possible\n                idxs = self.rng.choice(n_archive, size=3, replace=False)\n                x0 = np.array(archive_x[idxs[0]], dtype=float)\n                x1 = np.array(archive_x[idxs[1]], dtype=float)\n                x2 = np.array(archive_x[idxs[2]], dtype=float)\n                donor = x0 + self.F * (x1 - x2)\n                # crossover mask\n                mask = self.rng.random(self.dim) < self.cr\n                if not np.any(mask):\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small jitter\n                cand = cand + self.rng.normal(0.0, 0.02, size=self.dim) * scale\n            elif move_r < p_local + p_de + p_pca and n_archive >= 3:\n                # PCA-guided sampling using elites\n                n_keep = max(2, int(max(2, self.elite_frac * n_archive)))\n                elite_idx = sorted_idx[:n_keep]\n                elites = np.vstack([archive_x[i] for i in elite_idx])\n                center = np.mean(elites, axis=0)\n                # covariance and principal components\n                cov = np.cov(elites.T) if elites.shape[0] > 1 else np.diag((span * 0.05)**2)\n                # regularize\n                cov = cov + np.eye(self.dim) * 1e-8 * np.mean(np.diag(cov) + 1e-12)\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    sqrt_vals = np.sqrt(np.maximum(eigvals, 0.0))\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * (sqrt_vals * (gscale * 1.0))\n                    # scale by global span to keep consistent scale across dims\n                    coeffs = coeffs * (span / (np.mean(span) + 1e-12))\n                    cand = center + eigvecs.dot(coeffs)\n                    # small isotropic jitter\n                    cand = cand + self.rng.normal(0.0, 0.01, size=self.dim) * scale\n                except Exception:\n                    # fallback to local jitter\n                    cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.5)\n            else:\n                # heavy-tailed per-dim Cauchy jumps (Levy-like)\n                u = self.rng.random(self.dim)\n                # standard Cauchy via tan(pi*(u-0.5))\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails\n                cauch = np.clip(cauch, -1e3, 1e3)\n                mult = 0.8 + 0.6 * self.rng.random()\n                cand = base + cauch * (scale * 0.6 * mult)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= 8:\n                success_rate = float(np.mean(recent_success))\n                if success_rate > 0.20:\n                    gscale *= 1.08\n                elif success_rate > 0.08:\n                    gscale *= 1.02\n                elif success_rate < 0.03:\n                    gscale *= 0.85\n                # small multiplicative jitter for exploration\n                gscale *= np.exp(0.02 * self.rng.normal())\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if len(recent_success) == recent_success.maxlen and sum(recent_success) == 0:\n                # little success recently -> inject\n                old_gscale = gscale\n                gscale = min(gscale * 6.0, 1e2)  # broaden temporarily\n                injection_budget = min(max(3, self.dim), self.budget - evals, max(3, int(0.03 * self.budget)))\n                for _ in range(injection_budget):\n                    if evals >= self.budget:\n                        break\n                    # perturb a top elite or random broad sample\n                    if n_archive > 0 and self.rng.random() < 0.7:\n                        top_k = max(1, min(len(archive_x), int(0.05 * len(archive_x) + 1)))\n                        chosen_idx = self.rng.integers(0, top_k)\n                        center = np.array(archive_x[sorted_idx[chosen_idx]], dtype=float)\n                        # larger perturbation\n                        inj = center + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * (5.0 * gscale))\n                    else:\n                        inj = self._uniform_array(lb, ub)\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    try:\n                        f_inj = float(func(inj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    archive_x.append(np.array(inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(inj, dtype=float)\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                recent_success.clear()\n\n            # prune memory periodically\n            if len(archive_x) > 3 * self.keep_memory:\n                sorted_idx = np.argsort(np.array(archive_f))\n                n_best_keep = max(int(0.6 * self.keep_memory), min(len(sorted_idx), self.keep_memory // 2))\n                best_idx = list(sorted_idx[:n_best_keep])\n                recent_cnt = int(self.keep_memory // 4)\n                recent_idx = list(range(len(archive_x) - recent_cnt, len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            x_try = self._uniform_array(lb, ub)\n            try:\n                f_try = float(func(x_try))\n            except Exception:\n                f_try = np.inf\n            if f_try < self.f_opt:\n                self.f_opt = f_try\n                self.x_opt = np.array(x_try, dtype=float)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 55, in _extract_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "error": "In the code, line 55, in _extract_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "edf43dd3-55e1-47b9-b4af-80676293e5d6", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — memory-guided sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local search, heavy-tailed Cauchy jumps and occasional micro-restarts to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, F=0.8, cr=0.9, seed=None,\n                 keep_memory=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.F = float(F)\n        self.cr = float(cr)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # memory settings\n        if keep_memory is None:\n            self.keep_memory = min(200, max(20, 10 * self.dim))\n        else:\n            self.keep_memory = int(keep_memory)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = float(\"inf\")\n\n        # base scales per-dim (initial)\n        self.base_scale_vec = np.maximum(1e-6, 0.2 * np.ones(self.dim))\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim\n        Supports:\n         - func.bounds = (lb, ub) or object with .lb .ub or .lower .upper\n         - func.lower / func.upper arrays\n         - func.bounds.lb / bounds.ub etc.\n        \"\"\"\n        lb = None\n        ub = None\n\n        # common top-level attributes\n        for attr in (\"bounds\", \"lower_bounds\", \"lower\", \"lb\"):\n            if hasattr(func, attr):\n                b = getattr(func, attr)\n                # tuple/list-like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    try:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n                    except Exception:\n                        pass\n                else:\n                    # object with attributes\n                    lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                    ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n                    if lb_attr is not None and ub_attr is not None:\n                        lb = np.asarray(lb_attr, dtype=float)\n                        ub = np.asarray(ub_attr, dtype=float)\n                if lb is not None and ub is not None:\n                    break\n\n        # other direct attributes\n        if lb is None or ub is None:\n            if hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n                try:\n                    lb = np.asarray(getattr(func, \"lower\"), dtype=float)\n                    ub = np.asarray(getattr(func, \"upper\"), dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # final fallback to [-5,5]^dim\n        if lb is None or ub is None:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n        else:\n            lb = np.resize(lb, self.dim).astype(float).copy()\n            ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid\n        bad = np.any(lb >= ub)\n        if bad:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.asarray(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = 2.0 * lb[below] - x[below]\n            if above.any():\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n\n        # bookkeeping\n        evals = 0\n        archive_x = []\n        archive_f = []\n\n        # adaptation\n        gscale = 1.0\n        stagnation_counter = 0\n        best_since = 0\n\n        short_term_window = max(20, 4 * self.dim)\n        recent_successes = []\n\n        # initial sampling: ensure some initial diversity but not consume too much budget\n        init_n = int(min(max(10, 3 * self.dim), max(1, int(0.05 * self.budget))))\n        init_n = max(1, min(init_n, self.budget))\n        for _ in range(init_n):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            evals += 1\n            if not np.isfinite(f):\n                f = float(\"inf\")\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n                stagnation_counter = 0\n                best_since = evals\n            else:\n                stagnation_counter += 1\n\n        # Ensure at least one valid candidate exists\n        if len(archive_x) == 0:\n            if evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                try:\n                    f = float(func(x))\n                except Exception:\n                    f = float(\"inf\")\n                evals += 1\n                archive_x.append(np.array(x, dtype=float))\n                archive_f.append(float(f))\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # move probabilities\n        p_local = 0.45\n        p_de = 0.20\n        p_pca = 0.20\n        # rest heavy-tailed\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness\n            archive_f_arr = np.asarray(archive_f, dtype=float)\n            sorted_idx = np.argsort(archive_f_arr)\n            n_archive = len(archive_x)\n\n            # pick base: mostly best, sometimes random archive or uniform\n            choose = self.rng.random()\n            if choose < 0.7 and n_archive > 0:\n                # biased toward better indices: sample index from exponential over ranks\n                ranks = np.arange(n_archive)\n                # convert ranks into probabilities that favor low ranks\n                lam = 0.5 + 1.0 / max(1, n_archive)\n                probs = np.exp(-lam * ranks)\n                probs = probs / probs.sum()\n                pick = self.rng.choice(n_archive, p=probs)\n                base = np.array(archive_x[sorted_idx[pick]], dtype=float)\n            elif choose < 0.9 and n_archive > 0:\n                # random pick from archive\n                base = np.array(archive_x[self.rng.randint(n_archive)], dtype=float)\n            else:\n                # uniform random base\n                base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_jitter = np.exp(0.1 * self.rng.normal(0.0, 1.0, size=self.dim))\n            base_scale_vec = self.base_scale_vec * per_dim_jitter\n            scale = base_scale_vec * gscale\n\n            # choose move type\n            r = self.rng.random()\n            cand = None\n\n            if r < p_local:\n                # Local anisotropic gaussian around base\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                eps = self.rng.normal(0.0, 1.0, size=self.dim) * scale * rand_mult\n                cand = base + eps\n\n            elif r < p_local + p_de and n_archive >= 3:\n                # DE-style donor move with crossover\n                # choose three distinct indices from archive\n                idxs = self.rng.choice(n_archive, size=3, replace=False)\n                x1 = archive_x[idxs[0]]\n                x2 = archive_x[idxs[1]]\n                x3 = archive_x[idxs[2]]\n                donor = x1 + self.F * (x2 - x3)\n                # crossover mask (ensure at least one dim taken)\n                mask = self.rng.rand(self.dim) < self.cr\n                if not mask.any():\n                    mask[self.rng.randint(self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand += self.rng.normal(0.0, 1.0, size=self.dim) * (0.05 * scale)\n\n            elif r < p_local + p_de + p_pca and n_archive >= 2:\n                # PCA-guided sampling using elites\n                # pick center: best or random elite\n                center_choice = self.rng.random()\n                top_k = max(2, min(n_archive, int(0.2 * n_archive)))\n                top_idx = sorted_idx[:top_k]\n                if center_choice < 0.6:\n                    center = np.array(archive_x[sorted_idx[0]], dtype=float)  # best\n                else:\n                    center = np.array(archive_x[self.rng.choice(top_idx)], dtype=float)\n\n                # compute covariance and principal components from top_k elites\n                X = np.vstack([archive_x[i] for i in top_idx])\n                # center\n                Xc = X - X.mean(axis=0, keepdims=True)\n                # regularize covariance to avoid singularity\n                cov = (Xc.T @ Xc) / max(1, Xc.shape[0] - 1)\n                cov += np.eye(self.dim) * 1e-8\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                except np.linalg.LinAlgError:\n                    eigvals = np.maximum(np.diag(cov), 1e-12)\n                    eigvecs = np.eye(self.dim)\n                # sort by descending eigenvalue\n                order = np.argsort(eigvals)[::-1]\n                eigvals = eigvals[order]\n                eigvecs = eigvecs[:, order]\n                sqrt_vals = np.sqrt(np.maximum(eigvals, 1e-12))\n                # sample coefficients along principal directions\n                coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * (sqrt_vals * (gscale * 1.0))\n                # small isotropic jitter\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim) * (0.02 * scale)\n                cand = center + eigvecs @ coeffs + jitter\n\n            else:\n                # heavy-tailed per-dim Cauchy jumps (Levy-like)\n                # sample standard Cauchy and scale, truncate extreme tails\n                c = self.rng.standard_cauchy(size=self.dim)\n                # clip extreme tails to avoid overflow\n                c = np.clip(c, -10.0, 10.0)\n                cand = base + c * (scale * (0.5 + self.rng.rand(self.dim)))\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n            if not np.isfinite(f_cand):\n                f_cand = float(\"inf\")\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            is_success = False\n            if f_cand < self.f_opt - 1e-12:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                stagnation_counter = 0\n                best_since = evals\n                is_success = True\n            else:\n                stagnation_counter += 1\n\n            # maintain recent success list\n            recent_successes.append(1 if is_success else 0)\n            if len(recent_successes) > short_term_window:\n                recent_successes.pop(0)\n            success_rate = (sum(recent_successes) / max(1, len(recent_successes)))\n\n            # adapt gscale based on short-term success rate\n            if success_rate > 0.25:\n                gscale *= 1.12\n            elif success_rate < 0.05:\n                gscale *= 0.88\n            # small multiplicative jitter for exploration\n            gscale *= np.exp(0.01 * self.rng.normal())\n\n            # prune memory periodically\n            if len(archive_x) > 2 * self.keep_memory:\n                # keep some best, some recent\n                n_best_keep = max(int(0.6 * self.keep_memory), min(len(sorted_idx), self.keep_memory // 2))\n                best_idx = list(sorted_idx[:n_best_keep])\n                # keep most recent others\n                recent_keep = self.keep_memory - n_best_keep\n                recent_idx = list(range(max(0, len(archive_x) - recent_keep), len(archive_x)))\n                keep_idx = set(best_idx) | set(recent_idx)\n                new_archive_x = [archive_x[i] for i in sorted(keep_idx)]\n                new_archive_f = [archive_f[i] for i in sorted(keep_idx)]\n                archive_x = new_archive_x\n                archive_f = new_archive_f\n\n            # stagnation detection & micro-restart (injections)\n            stagnation_limit = max(50, 10 * self.dim)\n            if stagnation_counter > stagnation_limit and evals < self.budget:\n                # compute injection budget limited by remaining budget\n                injection_budget = min(max(3, self.dim), self.budget - evals, max(3, int(0.03 * self.budget)))\n                # create some perturbed elites and some uniform samples\n                injections = []\n                # perturb a top elite and add some uniform draws\n                top_k = max(1, min(len(archive_x), max(1, int(0.05 * len(archive_x) + 1))))\n                for i_inj in range(injection_budget):\n                    if i_inj % 2 == 0 and len(archive_x) > 0:\n                        elite = np.array(archive_x[sorted_idx[self.rng.randint(min(top_k, len(sorted_idx)))]], dtype=float)\n                        # larger perturbation\n                        pert = elite + self.rng.normal(0.0, 1.0, size=self.dim) * (1.0 + 2.0 * gscale) * self.base_scale_vec\n                        injections.append(self._reflect_bounds(pert, lb, ub))\n                    else:\n                        injections.append(self._uniform_array(lb, ub))\n                # evaluate injections (respect budget)\n                for x_inj in injections:\n                    if evals >= self.budget:\n                        break\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        is_success = True\n                        stagnation_counter = 0\n                # gently reset gscale to encourage new exploration\n                gscale = max(0.2, gscale * 0.6)\n                recent_successes = []\n                stagnation_counter = 0\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            # pick best in archive if available\n            if len(archive_x) > 0:\n                idx = int(np.argmin(np.asarray(archive_f, dtype=float)))\n                self.x_opt = np.array(archive_x[idx], dtype=float)\n                self.f_opt = float(archive_f[idx])\n            else:\n                # fallback uniform\n                x = self._uniform_array(lb, ub)\n                try:\n                    f = float(func(x))\n                except Exception:\n                    f = float(\"inf\")\n                self.x_opt = np.array(x, dtype=float)\n                self.f_opt = float(f)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example quick test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 61, in _extract_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "error": "In the code, line 61, in _extract_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "fcce87ea-832d-4967-a75c-02c92110c534", "fitness": 0.15069656639914225, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — memory-guided sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local search, heavy-tailed Cauchy jumps and occasional micro-restarts to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 max_archive=200, initial_ratio=0.05):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.max_archive = int(max_archive)\n        # fraction of budget to spend on initial quasi-random exploration (not too large)\n        self.initial_ratio = float(initial_ratio)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim.\n        Returns lb, ub arrays length self.dim.\n        \"\"\"\n        # default\n        lb = None\n        ub = None\n\n        # Try structured attributes: func.bounds (object or tuple)\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # tuple-like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb_cand, ub_cand = b\n                    lb = np.asarray(lb_cand, dtype=float)\n                    ub = np.asarray(ub_cand, dtype=float)\n                else:\n                    # object-like with attributes\n                    lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                    ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n                    if lb_attr is not None and ub_attr is not None:\n                        lb = np.asarray(lb_attr, dtype=float)\n                        ub = np.asarray(ub_attr, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # Try direct attributes on func\n        if lb is None or ub is None:\n            try:\n                lb_attr = getattr(func, \"lb\", None) or getattr(func, \"lower\", None) or getattr(func, \"lower_bounds\", None)\n                ub_attr = getattr(func, \"ub\", None) or getattr(func, \"upper\", None) or getattr(func, \"upper_bounds\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                pass\n\n        # Final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # Broadcast scalars or resize arrays to dim\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb.item()), dtype=float)\n            elif lb.size != self.dim:\n                # try to broadcast by repeating or slice\n                lb = np.resize(lb, self.dim).astype(float)\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub.item()), dtype=float)\n            elif ub.size != self.dim:\n                ub = np.resize(ub, self.dim).astype(float)\n\n        # ensure valid: if any lb >= ub fallback to [-5,5]\n        if np.any(lb >= ub) or not np.all(np.isfinite(lb)) or not np.all(np.isfinite(ub)):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = x.copy().astype(float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = lb + (lb - x) = 2*lb - x\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above: x = ub - (x - ub) = 2*ub - x\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.random(self.dim) * (ub - lb) + lb\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # archive of evaluated points and their fitnesses\n        xs = []\n        fs = []\n\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # initial sampling: a modest Sobol-like / quasi-random spread using jittered uniform\n        initial_n = int(max(4, min(self.budget // 20, 8 * self.dim, int(self.budget * self.initial_ratio))))\n        for i in range(initial_n):\n            if evals >= self.budget:\n                break\n            # low-discrepancy-ish by using stratified sampling on first dims\n            x = self._uniform_array(lb, ub)\n            # small deterministic stagger to diversify first few\n            x = lb + ( (i + self.rng.random(self.dim)) / max(1, initial_n) ) * span\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(x)\n            fs.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if nothing evaluated yet (extreme fallback)\n        if evals == 0:\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals = 1\n            xs.append(x)\n            fs.append(f)\n            self.f_opt = f\n            self.x_opt = x.copy()\n\n        # parameters\n        max_archive = max(20, self.max_archive, 10 * self.dim)\n        p_de = 0.20\n        p_pca = 0.25\n        p_local = 0.35\n        # rest heavy-tailed (cauchy)\n        gscale = 0.02  # global scale relative to span\n        per_dim_scale = np.maximum(span * (gscale + 1e-12), 1e-12)\n        # keep short history of successes\n        recent_success = []\n        recent_window = max(20, 5 * self.dim)\n        stagnation_counter = 0\n        stagnation_limit = max(50, 10 * self.dim)\n        no_improve_iters = 0\n\n        # main loop\n        while evals < self.budget:\n            archive_f = np.array(fs)\n            archive_x = np.array(xs)\n            n_archive = archive_f.shape[0]\n\n            # sort archive by fitness ascending\n            sorted_idx = np.argsort(archive_f)\n            best_idx = sorted_idx[0]\n            best_x = archive_x[best_idx].copy()\n            best_f = float(archive_f[best_idx])\n\n            # choose base: mostly best, sometimes random archive or uniform\n            r_base = self.rng.random()\n            if r_base < 0.70:\n                base = best_x.copy()\n            elif r_base < 0.90 and n_archive >= 3:\n                # biased toward better indices: sample index from exponential over ranks\n                ranks = np.arange(n_archive)\n                # sample a rank using geometric-like bias\n                lam = 1.5\n                probs = np.exp(-lam * ranks)\n                probs /= probs.sum()\n                rank = np.searchsorted(np.cumsum(probs), self.rng.random())\n                rank = np.clip(rank, 0, n_archive - 1)\n                idx = sorted_idx[rank]\n                base = archive_x[idx].copy()\n            else:\n                base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_jitter = 0.9 + 0.2 * self.rng.random(self.dim)\n            per_dim_scale = np.maximum(per_dim_scale * per_dim_jitter, 1e-12)\n\n            # choose move type\n            r_move = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r_move < p_local:\n                # anisotropic gaussian: each dim scaled differently\n                local_scale = per_dim_scale * (0.5 + self.rng.random(self.dim))\n                cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * local_scale\n                # small directed bias toward global best from base\n                if self.rng.random() < 0.3:\n                    cand += (best_x - base) * (0.1 * self.rng.random())\n            # DE-style donor move with crossover\n            elif r_move < p_local + p_de and n_archive >= 4:\n                # choose three distinct indices different from base candidate index (if base is from archive)\n                idxs = np.arange(n_archive)\n                # choose 3 distinct at random\n                a, b, c = self.rng.choice(idxs, size=3, replace=False)\n                xa = archive_x[a]\n                xb = archive_x[b]\n                xc = archive_x[c]\n                F = 0.5 + 0.5 * self.rng.random()  # [0.5,1.0)\n                donor = xa + F * (xb - xc)\n                # crossover mask: ensure at least one dim\n                cr = 0.1 + 0.8 * self.rng.random()\n                mask = self.rng.random(self.dim) < cr\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand = base.copy()\n                cand[mask] = donor[mask]\n                # add small gaussian jitter proportional to per_dim_scale\n                cand += self.rng.normal(0, 0.2, size=self.dim) * per_dim_scale\n            # PCA-guided sampling using elites\n            elif r_move < p_local + p_de + p_pca and n_archive >= max(2, min(2*self.dim, n_archive)):\n                n_keep = max(2, min(2*self.dim, n_archive, 20))\n                elites_idx = sorted_idx[:n_keep]\n                elites = archive_x[elites_idx]\n                # center at a random elite\n                center = elites[self.rng.integers(0, n_keep)].copy()\n                # compute covariance and principal components\n                cov = np.cov(elites.T) if elites.shape[0] > 1 else np.diag(np.maximum(per_dim_scale**2, 1e-12))\n                # regularize\n                cov += np.eye(self.dim) * (1e-9 + (gscale * max(span))**2 * 1e-6)\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # sort descending\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, np.sqrt(np.maximum(eigvals, 0.0)))\n                    # scale by global span to keep consistent scale across dims\n                    coeffs *= (gscale * np.mean(span) * (1.0 + 0.5 * self.rng.random()))\n                    # reconstruct\n                    step = eigvecs @ coeffs\n                    # small isotropic jitter\n                    step += self.rng.normal(0.0, 0.05 * np.mean(per_dim_scale), size=self.dim)\n                    cand = center + step\n                except Exception:\n                    # fallback to local jitter\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # start from base and do per-dim scaled Cauchy\n                scale = np.maximum(per_dim_scale * (1.0 + self.rng.random(self.dim)), 1e-12)\n                cand = base + self.rng.standard_cauchy(self.dim) * scale\n                # truncate extreme tails by clipping to some multiple\n                max_clip = np.minimum(10.0 * span, 1e3)\n                cand = np.clip(cand, lb - max_clip, ub + max_clip)\n                # add occasional isotropic gaussian nudges\n                if self.rng.random() < 0.3:\n                    cand += self.rng.normal(0.0, 0.05 * np.mean(span), size=self.dim)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            try:\n                f_cand = float(func(cand)) if evals < self.budget else np.inf\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archives and bests\n            xs.append(cand.copy())\n            fs.append(f_cand)\n            # prune archive if too large (keep best and some recent)\n            if len(xs) > max_archive:\n                # keep top half by fitness and half of recent\n                archive_f = np.array(fs)\n                idx_sorted = np.argsort(archive_f)\n                keep_best = idx_sorted[: max_archive // 2].tolist()\n                # keep recent others\n                recent_keep = list(range(len(xs) - max_archive // 2, len(xs)))\n                keep_idx = sorted(set(keep_best + recent_keep))\n                xs = [xs[i] for i in keep_idx]\n                fs = [fs[i] for i in keep_idx]\n\n            # update global best\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                improved = True\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # adapt gscale based on short-term success rate\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n            # adjust gscale modestly\n            if len(recent_success) >= 5:\n                sr = float(sum(recent_success)) / len(recent_success)\n                if sr < 0.15:\n                    gscale = min(gscale * 1.18, 5.0)\n                elif sr > 0.45:\n                    gscale = max(gscale * 0.85, 1e-6)\n\n            # small multiplicative jitter for exploration\n            per_dim_scale = np.maximum(span * (gscale * (0.5 + self.rng.random())), 1e-12)\n\n            # stagnation detection & micro-restart (injections)\n            if no_improve_iters > stagnation_limit and evals < self.budget:\n                stagnation_counter += 1\n                # temporarily broaden gscale\n                old_gscale = gscale\n                gscale = min(gscale * (2.0 + stagnation_counter * 0.5), 1e2)\n                per_dim_scale = np.maximum(span * gscale, 1e-12)\n\n                # perturb a top elite (micro-restart injection)\n                top_k = min(len(xs), max(2, 5 + self.dim))\n                try:\n                    injection_idx = sorted_idx[self.rng.integers(0, top_k)]\n                    base_inject = archive_x[injection_idx].copy()\n                except Exception:\n                    base_inject = best_x.copy()\n                # create a handful of injected points around that elite\n                injections = min(5, self.budget - evals)\n                for _ in range(injections):\n                    if evals >= self.budget:\n                        break\n                    perturb = base_inject + self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_scale * (1.5 + self.rng.random()))\n                    perturb = self._reflect_bounds(perturb, lb, ub)\n                    try:\n                        f_inj = float(func(perturb))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    xs.append(perturb.copy())\n                    fs.append(f_inj)\n                    if f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = perturb.copy()\n                        no_improve_iters = 0\n\n                # gently reset gscale toward old value\n                gscale = max(old_gscale * 0.8, 1e-6)\n                per_dim_scale = np.maximum(span * gscale, 1e-12)\n                no_improve_iters = 0  # give another chance\n\n            # incrementally continue until budget exhausted\n            # loop back\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            # pick best from archive\n            if len(fs) > 0:\n                idx = int(np.argmin(fs))\n                self.f_opt = float(fs[idx])\n                self.x_opt = xs[idx].copy()\n            else:\n                self.x_opt = self._uniform_array(lb, ub)\n                try:\n                    self.f_opt = float(func(self.x_opt))\n                except Exception:\n                    self.f_opt = np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)\n\n\n# Quick example (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.151 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07419442967511436, 0.1547430819606087, 0.21391697671573962, 0.14654380547216117, 0.13834640046345625, 0.16993529999408463, 0.1807298230963187, 0.15389942854803418, 0.14624100169375975, 0.12841541637214515]}, "task_prompt": ""}
{"id": "04aca2ed-be39-4a31-9eba-136fc8319185", "fitness": 0.3570743641930836, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — memory-guided sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local search, heavy-tailed Cauchy jumps and occasional micro-restarts to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, mem_size=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed if seed is not None else np.random.randint(0, 2**31 - 1)\n        self.rng = np.random.RandomState(self.seed)\n\n        # memory / archive\n        self.mem_size = mem_size if mem_size is not None else max(50, 10 * self.dim)\n\n        # adaptive scales\n        self.gscale = 0.2  # global scale multiplier (adapts)\n        self._success_window = []\n        self._window_max = 50\n\n        # move probabilities\n        self.p_local = 0.35\n        self.p_de = 0.25\n        self.p_pca = 0.20\n        # remainder -> heavy-tailed Cauchy\n\n        # bookkeeping placeholders\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim\n        Accepts:\n            - func.bounds = (lb, ub) or object with lb/ub attributes or .lower/.upper\n            - func.lower, func.upper\n        Returns (lb_array, ub_array)\n        \"\"\"\n        dim = self.dim\n        lb = None\n        ub = None\n\n        # try attribute 'bounds'\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # tuple/list-like (lb, ub)\n            if isinstance(b, (tuple, list)) and len(b) == 2:\n                lb_try, ub_try = b\n                # if scalars broadcast\n                try:\n                    lb = np.broadcast_to(np.array(lb_try, dtype=float), (dim,))\n                    ub = np.broadcast_to(np.array(ub_try, dtype=float), (dim,))\n                except Exception:\n                    pass\n            else:\n                # object with attributes lb/ub or lower/upper\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.array(getattr(b, \"lb\"), dtype=float)\n                    ub = np.array(getattr(b, \"ub\"), dtype=float)\n                elif hasattr(b, \"lower\") and hasattr(b, \"upper\"):\n                    lb = np.array(getattr(b, \"lower\"), dtype=float)\n                    ub = np.array(getattr(b, \"upper\"), dtype=float)\n\n        # try direct attributes like lower/upper or bounds_lb/ub\n        if lb is None or ub is None:\n            if hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n                try:\n                    lb = np.array(getattr(func, \"lower\"), dtype=float)\n                    ub = np.array(getattr(func, \"upper\"), dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # broadcast or finalize\n        if lb is None or ub is None:\n            # fallback to [-5,5]\n            lb = np.full((dim,), -5.0, dtype=float)\n            ub = np.full((dim,), 5.0, dtype=float)\n        else:\n            # ensure 1D arrays and broadcast to dim\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full((dim,), float(lb.item()), dtype=float)\n            elif lb.size != dim:\n                try:\n                    lb = np.broadcast_to(lb, (dim,))\n                except Exception:\n                    lb = np.full((dim,), -5.0, dtype=float)\n            if ub.size == 1:\n                ub = np.full((dim,), float(ub.item()), dtype=float)\n            elif ub.size != dim:\n                try:\n                    ub = np.broadcast_to(ub, (dim,))\n                except Exception:\n                    ub = np.full((dim,), 5.0, dtype=float)\n\n        # validate\n        invalid = np.any(lb >= ub)\n        if invalid:\n            lb = np.full((dim,), -5.0, dtype=float)\n            ub = np.full((dim,), 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        assert np.all(span > 0), \"Invalid bounds detected.\"\n\n        # initialize memory\n        X = []  # list of arrays\n        F = []  # list of floats\n\n        # initial sampling (do not consume too much budget)\n        n_init = min(max(8, 2 * self.dim), max(1, int(0.08 * self.budget)))\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n            if self.evals >= self.budget:\n                break\n\n        # if nothing valid so far, fill one random\n        if len(X) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = float(func(x))\n            self.evals += 1\n            X.append(np.array(x, dtype=float))\n            F.append(f)\n            self.f_opt = f\n            self.x_opt = np.array(x, dtype=float)\n\n        # main loop\n        stagnation_counter = 0\n        last_improve_at = 0\n        adapt_interval = 20\n        prune_interval = 50\n        inject_cooldown = 200\n        last_inject = 0\n\n        while self.evals < self.budget:\n            # ensure arrays\n            X_arr = np.vstack(X)\n            F_arr = np.array(F, dtype=float)\n\n            # sort archive by fitness ascending\n            order = np.argsort(F_arr)\n            X_arr = X_arr[order]\n            F_arr = F_arr[order]\n\n            # ensure memory size\n            keep = min(self.mem_size, len(F_arr))\n            X_arr = X_arr[:keep]\n            F_arr = F_arr[:keep]\n\n            # pick a base index biased to better ranks\n            n_mem = len(F_arr)\n            ranks = np.arange(n_mem)\n            # make exponentially biased probabilities over ranks\n            scale = max(3.0, 0.05 * n_mem + 1.0)\n            probs = np.exp(-ranks / scale)\n            probs = probs / probs.sum()\n            base_idx = self.rng.choice(n_mem, p=probs)\n            base = X_arr[base_idx].copy()\n\n            # sometimes pick purely random base to increase diversity\n            if self.rng.rand() < 0.05:\n                base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scales (anisotropic)\n            jitter = 0.4 + 0.6 * self.rng.rand(self.dim)\n            scale_vec = self.gscale * jitter * span\n\n            # choose move type\n            r = self.rng.rand()\n            if r < self.p_local:\n                move_type = \"local\"\n            elif r < self.p_local + self.p_de:\n                move_type = \"de\"\n            elif r < self.p_local + self.p_de + self.p_pca:\n                move_type = \"pca\"\n            else:\n                move_type = \"cauchy\"\n\n            candidate = base.copy()\n\n            if move_type == \"local\":\n                # anisotropic gaussian around base\n                candidate = base + self.rng.normal(0.0, 1.0, size=self.dim) * scale_vec\n                # small chance to do an orthogonal step along one coordinate direction\n                if self.rng.rand() < 0.1:\n                    i = self.rng.randint(0, self.dim)\n                    candidate[i] = base[i] + self.rng.normal(0, 1) * scale_vec[i] * 2.0\n\n            elif move_type == \"de\":\n                # Differential Evolution style donor with crossover\n                if n_mem >= 3:\n                    idxs = list(range(n_mem))\n                    # remove base_idx from choices if it's from memory\n                    if base_idx < n_mem:\n                        idxs.pop(base_idx)\n                    r1, r2, r3 = self.rng.choice(idxs, size=3, replace=False)\n                    x1 = X_arr[r1]\n                    x2 = X_arr[r2]\n                    x3 = X_arr[r3]\n                    Fcoef = 0.6 + 0.4 * self.rng.rand()\n                    donor = x1 + Fcoef * (x2 - x3)\n                    # crossover mask\n                    mask = self.rng.rand(self.dim) < (0.2 + 0.6 * self.rng.rand())\n                    if not mask.any():\n                        mask[self.rng.randint(0, self.dim)] = True\n                    candidate = np.where(mask, donor, base)\n                    # small gaussian jitter\n                    candidate += self.rng.normal(0, 1.0, size=self.dim) * (0.2 * scale_vec)\n                else:\n                    # fallback to local jitter\n                    candidate = base + self.rng.normal(0.0, 1.0, size=self.dim) * scale_vec\n\n            elif move_type == \"pca\":\n                # PCA-guided sampling using elites\n                n_elite = max(3, int(0.15 * n_mem))\n                if n_elite < 3 or n_mem < 3:\n                    candidate = base + self.rng.normal(0.0, 1.0, size=self.dim) * scale_vec\n                else:\n                    elites = X_arr[:n_elite]\n                    mean = np.mean(elites, axis=0)\n                    C = np.cov(elites.T)\n                    # regularize covariance\n                    C += np.eye(self.dim) * (1e-8 + 1e-8 * np.mean(np.diag(C)))\n                    try:\n                        evals, evecs = np.linalg.eigh(C)\n                        # sort descending\n                        order_e = np.argsort(evals)[::-1]\n                        evals = evals[order_e]\n                        evecs = evecs[:, order_e]\n                        # choose number of principal components to use\n                        m = min(self.dim, max(1, int(1 + n_elite / 3)))\n                        coeffs = np.zeros(self.dim)\n                        # sample coefficients mostly along top components\n                        for i in range(m):\n                            sd = np.sqrt(max(evals[i], 0.0)) * self.gscale * (1.0 + 0.5 * self.rng.rand())\n                            coeffs[i] = self.rng.normal(0.0, sd)\n                        # transform back\n                        delta = evecs @ coeffs\n                        candidate = mean + delta\n                        # small isotropic jitter\n                        candidate += self.rng.normal(0.0, 1.0, size=self.dim) * (0.05 * span * self.gscale)\n                    except Exception:\n                        candidate = base + self.rng.normal(0.0, 1.0, size=self.dim) * scale_vec\n\n            else:  # Cauchy heavy-tailed jumps\n                # per-dim Cauchy scaled by scale_vec\n                # draw from standard Cauchy and clip extreme tails\n                raw = self.rng.standard_cauchy(size=self.dim)\n                # clip to reduce absurd outliers but keep heavy tail\n                raw = np.clip(raw, -10.0, 10.0)\n                candidate = base + raw * (0.8 * scale_vec + 0.2 * span * self.gscale)\n\n            # safety: ensure finite\n            if not np.isfinite(candidate).all():\n                candidate = self._uniform_array(lb, ub)\n\n            # reflect into bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # final safety clamp\n            candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n            # Evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            self.evals += 1\n\n            # update archive\n            X.append(np.array(candidate, dtype=float))\n            F.append(f_cand)\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = np.array(candidate, dtype=float)\n                improved = True\n                last_improve_at = self.evals\n\n            # update success window for gscale adaptation\n            self._success_window.append(1 if improved else 0)\n            if len(self._success_window) > self._window_max:\n                self._success_window.pop(0)\n\n            # adapt gscale every adapt_interval evaluations\n            if (self.evals % adapt_interval) == 0:\n                win = np.array(self._success_window, dtype=float)\n                sr = (win.mean() if win.size > 0 else 0.0)\n                # target success rate ~ 0.15\n                if sr > 0.18:\n                    self.gscale *= 1.08\n                elif sr < 0.08:\n                    self.gscale *= 0.92\n                # small multiplicative jitter\n                self.gscale *= np.exp(0.02 * self.rng.randn())\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-4, 5.0))\n\n            # stagnation detection & micro-restart\n            if improved:\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            if stagnation_counter > max(200, 20 * self.dim) and (self.evals - last_inject) > inject_cooldown:\n                # micro-restart: perturb a top elite strongly, inject a few candidates\n                n_inject = min(5, max(1, int(0.5 * self.dim)))\n                topk = max(1, min(len(F_arr), 3))\n                for j in range(n_inject):\n                    elite_idx = self.rng.randint(0, topk)\n                    base_inject = X_arr[elite_idx]\n                    big_pert = base_inject + self.rng.normal(0, 1.0, size=self.dim) * (0.7 * span)\n                    big_pert = self._reflect_bounds(big_pert, lb, ub)\n                    if self.evals >= self.budget:\n                        break\n                    try:\n                        f_new = float(func(big_pert))\n                    except Exception:\n                        f_new = np.inf\n                    self.evals += 1\n                    X.append(np.array(big_pert, dtype=float))\n                    F.append(f_new)\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = np.array(big_pert, dtype=float)\n                # reset some counters and slightly increase exploration\n                last_inject = self.evals\n                stagnation_counter = 0\n                self.gscale = min(2.0, self.gscale * 1.2)\n\n            # periodic memory pruning to keep diversity and recent history\n            if (self.evals % prune_interval) == 0:\n                X = list(X_arr[:max(10, 2 * self.dim)]) + list(X[-30:])  # keep best and recent\n                F = list(F_arr[:max(10, 2 * self.dim)]) + list(F[-30:])\n                # ensure length not too large\n                if len(X) > 5 * self.mem_size:\n                    X = X[:self.mem_size]\n                    F = F[:self.mem_size]\n\n        # final safety: ensure at least one valid point exists\n        if self.x_opt is None:\n            # pick best from evaluated archive\n            if len(F) > 0:\n                idx = int(np.argmin(F))\n                self.x_opt = np.array(X[idx], dtype=float)\n                self.f_opt = float(F[idx])\n            else:\n                # sample one random point\n                x = self._uniform_array(lb, ub)\n                f = float(func(x))\n                self.x_opt = x\n                self.f_opt = f\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick manual test (uncomment to run)\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.357 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16776362796612954, 0.15176901189545478, 0.747359268179054, 0.25940665789130724, 0.3435889394288816, 0.8665653075535156, 0.3012899009815001, 0.3386256864527575, 0.22787363980142372, 0.16650160178081153]}, "task_prompt": ""}
{"id": "b67aa60a-0835-4056-855c-485e7e8527ee", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler combining DE-style donors, PCA-projected elite moves, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and occasional micro-restarts — designed for robust box-constrained continuous optimization on [-5,5]^dim.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 keep_memory=200, elite_frac=0.15, init_scale=0.20,\n                 F=0.8, cr=0.9, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.keep_memory = int(max(5, keep_memory))\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = float(\"inf\")\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim\n        \"\"\"\n        lb = None\n        ub = None\n        # Try attributes like .bounds, .bounds.lb/.bounds.ub, .lower/.upper, or tuple (lb, ub)\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # tuple/list-like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # object with attributes\n                    lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                    ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n                    if lb_attr is not None and ub_attr is not None:\n                        lb = np.asarray(lb_attr, dtype=float)\n                        ub = np.asarray(ub_attr, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # fallback to explicit attributes on func\n        if lb is None or ub is None:\n            try:\n                lb_attr = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n                ub_attr = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast/resize to dim\n            lb = np.resize(lb, self.dim).astype(float).copy()\n            ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub fallback to [-5,5]\n        if np.any(lb >= ub) or np.any(~np.isfinite(lb)) or np.any(~np.isfinite(ub)):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.random(self.dim) * (ub - lb) + lb\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # bookkeeping\n        archive_x = []    # list of numpy arrays\n        archive_f = []    # list of floats\n        evals = 0\n\n        # initial sampling: ensure some initial diversity but not consume too much budget\n        init_n = min(max(10, int(0.05 * self.budget)), self.budget)\n        for _ in range(init_n):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_window = max(50, 5 * self.dim)\n        stagnation_limit = max(40, 10 * self.dim)\n        recent_success = deque(maxlen=recent_window)\n        stagnation_counter = 0\n\n        # move probabilities\n        p_local = 0.35\n        p_de = 0.25\n        p_pca = 0.20\n        # rest (1 - p_local - p_de - p_pca) => heavy-tailed\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # per-dim working scale\n        scale = base_scale_vec.copy()\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness\n            if len(archive_f) > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # pick base: mostly best, sometimes random archive or uniform\n            r_base = self.rng.random()\n            if len(archive_x) == 0 or r_base > 0.85:\n                base = self._uniform_array(lb, ub)\n            else:\n                # use best known with high probability, else pick a biased archive member\n                if self.x_opt is None or self.rng.random() < 0.15:\n                    base = self._uniform_array(lb, ub)\n                else:\n                    if self.rng.random() < 0.7:\n                        base = np.array(self.x_opt, dtype=float)\n                    else:\n                        # biased selection towards better ranks\n                        if len(sorted_idx) > 0:\n                            rank = int(min(len(sorted_idx) - 1, self.rng.exponential(scale=1.5)))\n                            idx = sorted_idx[rank]\n                            base = np.array(archive_x[idx], dtype=float)\n                        else:\n                            base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_jitter = np.exp(0.15 * self.rng.normal(0.0, 1.0, size=self.dim))\n            scale = np.maximum(scale * per_dim_jitter, base_scale_vec * 1e-6)\n\n            # choose move type\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult * 0.7)\n\n            # DE-style donor move with crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # choose three distinct indices from archive\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                x0 = np.array(archive_x[idxs[0]], dtype=float)\n                x1 = np.array(archive_x[idxs[1]], dtype=float)\n                x2 = np.array(archive_x[idxs[2]], dtype=float)\n                donor = x0 + self.F * (x1 - x2)\n                cand = np.array(base, dtype=float)\n                # crossover mask (ensure at least one dim taken)\n                mask = (self.rng.random(self.dim) < self.cr)\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand = cand + self.rng.normal(0.0, 0.5, size=self.dim) * (scale * 0.2)\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 3:\n                n_keep = max(2, int(max(2, self.elite_frac * len(archive_x))))\n                n_keep = min(n_keep, len(archive_x))\n                elite_idx = sorted_idx[:n_keep]\n                elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                center_choice = self.rng.random()\n                if center_choice < 0.7:\n                    center = np.mean(elites, axis=0)\n                else:\n                    center = elites[self.rng.integers(0, n_keep)]\n                # compute covariance and principal components\n                try:\n                    cov = np.cov(elites, rowvar=False)\n                    # regularize\n                    cov += 1e-10 * np.eye(self.dim)\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # sort by descending eigenvalue\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    sqrt_vals = np.sqrt(np.maximum(eigvals, 1e-12))\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * (sqrt_vals * (gscale * 1.0))\n                    # scale by global span to keep consistent scale across dims\n                    coeffs = coeffs * (span / (np.mean(span) + 1e-12))\n                    cand = center + eigvecs.dot(coeffs)\n                    # small isotropic jitter\n                    cand = cand + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.05)\n                except Exception:\n                    # fallback to local jitter\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * scale\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails by clipping\n                cauch = np.clip(cauch, -1e2, 1e2)\n                mult = 0.8 + 0.6 * self.rng.random()\n                cand = base + cauch * (scale * 0.6 * mult)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n            improved = False\n            if f_cand < self.f_opt:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= 10:\n                success_rate = float(sum(recent_success)) / len(recent_success)\n                if success_rate > 0.25:\n                    gscale *= 1.15\n                elif success_rate < 0.05:\n                    gscale *= 0.85\n                # small multiplicative jitter for exploration\n                gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter > stagnation_limit and evals < self.budget:\n                old_gscale = gscale\n                gscale = min(gscale * 6.0, 1e2)  # broaden temporarily\n                injection_budget = min(max(3, self.dim), self.budget - evals, int(max(3, 0.03 * self.budget)))\n                for _ in range(injection_budget):\n                    if evals >= self.budget:\n                        break\n                    if len(archive_x) == 0 or self.rng.random() < 0.6:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # perturb a top elite\n                        top_k = max(1, min(len(archive_x), int(0.05 * len(archive_x) + 1)))\n                        idx = sorted_idx[self.rng.integers(0, top_k)]\n                        elite = np.array(archive_x[idx], dtype=float)\n                        # larger perturbation\n                        x_inj = elite + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * gscale * (1.0 + self.rng.random()))\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        self.f_opt = float(f_inj)\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                stagnation_counter = 0\n                recent_success.clear()\n\n            # prune memory periodically\n            if len(archive_x) > 2 * self.keep_memory:\n                # keep some best, some recent\n                n_best_keep = max(int(0.6 * self.keep_memory), min(len(sorted_idx), self.keep_memory // 2))\n                best_idx = list(sorted_idx[:n_best_keep])\n                recent_cnt = min(self.keep_memory - n_best_keep, len(archive_x))\n                recent_idx = list(range(len(archive_x) - recent_cnt, len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            if evals < self.budget:\n                x_try = self._uniform_array(lb, ub)\n                try:\n                    self.f_opt = float(func(x_try))\n                    self.x_opt = np.array(x_try, dtype=float)\n                except Exception:\n                    self.f_opt = float(\"inf\")\n                    self.x_opt = self._uniform_array(lb, ub)\n            else:\n                # no budget left and no valid best; return a uniform point (not evaluated)\n                self.x_opt = self._uniform_array(lb, ub)\n                self.f_opt = float(\"inf\")\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example quick test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 146, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "error": "In the code, line 146, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4197d1a7-18cd-47cc-8075-192d08335c4d", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler mixing DE-style donors, PCA-projected elite moves, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and occasional micro-restarts for robust box-constrained continuous optimization.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 F=0.8, cr=0.9, seed=None,\n                 keep_memory=200, init_scale=0.2, elite_frac=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.F = float(F)\n        self.cr = float(cr)\n        self.seed = seed\n        self.keep_memory = int(max(5, keep_memory))\n        self.init_scale = float(init_scale)\n        self.elite_frac = float(elite_frac)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = float(\"inf\")\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim\n        Returns lb, ub arrays of length self.dim\n        \"\"\"\n        lb = None\n        ub = None\n        # Try common attribute patterns\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # tuple/list like (lb, ub)\n                try:\n                    if isinstance(b, (list, tuple)) and len(b) >= 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n                # or object with lb/ub attributes\n                if lb is None or ub is None:\n                    try:\n                        lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                        ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n                        if lb_attr is not None and ub_attr is not None:\n                            lb = np.asarray(lb_attr, dtype=float)\n                            ub = np.asarray(ub_attr, dtype=float)\n                    except Exception:\n                        lb = None\n                        ub = None\n        except Exception:\n            lb = None\n            ub = None\n\n        # try direct func attributes\n        if lb is None or ub is None:\n            try:\n                la = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n                ua = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n                if la is not None and ua is not None:\n                    lb = np.asarray(la, dtype=float)\n                    ub = np.asarray(ua, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # fallback to [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.resize(lb, self.dim).astype(float).copy()\n            ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure lb < ub, otherwise fallback\n        if np.any(~np.isfinite(lb)) or np.any(~np.isfinite(ub)) or np.any(lb >= ub):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = 2.0 * lb[below] - x[below]\n            x[above] = 2.0 * ub[above] - x[above]\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        # sample per-dim uniform using rng\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        span_mean = float(np.mean(np.maximum(span, 1e-12)))\n\n        # bookkeeping\n        archive_x = []    # list of numpy arrays\n        archive_f = []    # list of floats\n        evals = 0\n\n        # algorithmic state\n        base_scale_vec = np.maximum(self.init_scale * span, 1e-12)\n        gscale = 1.0            # global multiplier for exploratory moves\n        stagnation_counter = 0\n        recent_success = deque(maxlen=100)\n        stagnation_limit = max(40, 10 * self.dim)\n        injection_budget = max(5, min(50, int(0.02 * self.budget)))\n        top_k = max(2, min(self.keep_memory, 10))\n        n_best_keep = max(2, int(0.15 * self.keep_memory))\n\n        # move probabilities\n        p_local = 0.35\n        p_de = 0.25\n        p_pca = 0.20\n        # rest -> heavy-tailed jumps\n\n        # initial sampling: ensure some initial diversity but not consume too much budget\n        init_n = min(max(10, int(0.05 * self.budget)), max(10, self.budget // 20))\n        init_n = min(init_n, self.budget)\n        for _ in range(init_n):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # if no evaluations done (budget == 0), return\n        if self.budget <= 0:\n            return float(self.f_opt), (np.array(self.x_opt, dtype=float) if self.x_opt is not None else None)\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness (ascending)\n            if len(archive_f) > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # pick base: prefer best, occasionally random archive or uniform\n            r_base = self.rng.random()\n            if len(archive_x) == 0 or r_base > 0.85:\n                base = self._uniform_array(lb, ub)\n            else:\n                # mostly use the best so far with some probability\n                if self.x_opt is not None and self.rng.random() < 0.6:\n                    base = np.array(self.x_opt, dtype=float)\n                else:\n                    # biased selection towards better ranks in archive\n                    if len(sorted_idx) > 0:\n                        # exponential-biased rank\n                        rank = min(len(sorted_idx) - 1, int(self.rng.exponential(scale=1.5)))\n                        idx = sorted_idx[rank]\n                        base = np.array(archive_x[idx], dtype=float)\n                    else:\n                        base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_jitter = np.exp(0.15 * self.rng.normal(0.0, 1.0, size=self.dim))\n            scale_vec = base_scale_vec * per_dim_jitter * gscale\n\n            # choose move type\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropic log-normal amplitude per-dim for occasional larger steps\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                cand = base + rand_mult * (scale_vec * 0.8) * (0.5 + 0.5 * self.rng.random())\n\n            # DE-style donor move with binomial crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # choose three distinct indices from archive\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                x0 = np.array(archive_x[idxs[0]], dtype=float)\n                x1 = np.array(archive_x[idxs[1]], dtype=float)\n                x2 = np.array(archive_x[idxs[2]], dtype=float)\n                donor = x0 + self.F * (x1 - x2)\n                donor = self._reflect_bounds(donor, lb, ub)\n                # crossover mask (binomial) ensure at least one dim taken from donor\n                mask = self.rng.random(self.dim) < self.cr\n                mask[self.rng.integers(0, self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand = cand + self.rng.normal(0.0, 0.5, size=self.dim) * (scale_vec * 0.2)\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 3:\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                n_keep = min(n_keep, len(archive_x))\n                elite_idx = sorted_idx[:n_keep]\n                elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                center_choice = self.rng.random()\n                if center_choice < 0.7:\n                    center = np.mean(elites, axis=0)\n                else:\n                    center = elites[self.rng.integers(0, n_keep)]\n                try:\n                    # center data and compute covariance\n                    data = elites - np.mean(elites, axis=0)\n                    cov = np.cov(data, rowvar=False)\n                    # regularize covariance\n                    cov += 1e-10 * np.eye(self.dim)\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # sort descending\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    sqrt_vals = np.sqrt(np.maximum(eigvals, 1e-12))\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * (sqrt_vals * (0.9 * gscale))\n                    # scale by global span to keep consistent scale across dims\n                    coeffs = coeffs * (span / (span_mean + 1e-12))\n                    cand = center + eigvecs.dot(coeffs)\n                    # small isotropic jitter\n                    cand = cand + self.rng.normal(0.0, 0.2, size=self.dim) * (scale_vec * 0.3)\n                except Exception:\n                    # fallback to local jitter\n                    cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * (scale_vec * 0.7)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy samples\n                cauch = self.rng.standard_cauchy(self.dim)\n                # truncate extreme tails by clipping\n                cauch = np.clip(cauch, -1e2, 1e2)\n                heavy_mult = 1.5 + 2.0 * self.rng.random()\n                cand = base + cauch * (scale_vec * 0.6 * heavy_mult)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n            improved = False\n            if f_cand < self.f_opt:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= 10:\n                success_rate = float(sum(recent_success) / len(recent_success))\n                if success_rate > 0.25:\n                    gscale *= 1.12\n                elif success_rate < 0.05:\n                    gscale *= 0.85\n                else:\n                    # small multiplicative jitter for exploration\n                    gscale *= (1.0 + 0.06 * (self.rng.random() - 0.5))\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter > stagnation_limit and evals < self.budget:\n                old_gscale = gscale\n                gscale = min(gscale * 6.0, 1e2)  # broaden temporarily\n                # do a small injection burst\n                inj_budget = min(injection_budget, self.budget - evals)\n                for _ in range(inj_budget):\n                    if evals >= self.budget:\n                        break\n                    if len(archive_x) == 0 or self.rng.random() < 0.6:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # perturb a top elite or the best\n                        if len(sorted_idx) > 0:\n                            idx = sorted_idx[self.rng.integers(0, min(len(sorted_idx), top_k))]\n                            elite = np.array(archive_x[idx], dtype=float)\n                        else:\n                            elite = self._uniform_array(lb, ub)\n                        # larger perturbation\n                        x_inj = elite + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * gscale * (1.0 + self.rng.random()))\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    evals += 1\n                    if f_inj < self.f_opt:\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        self.f_opt = float(f_inj)\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                stagnation_counter = 0\n                recent_success.clear()\n\n            # prune memory periodically\n            if len(archive_x) > 2 * self.keep_memory:\n                # keep some best, some recent\n                sorted_idx = np.argsort(archive_f)\n                best_idx = list(sorted_idx[:n_best_keep])\n                recent_keep = self.keep_memory - len(best_idx)\n                # recent indices are the last ones appended\n                recent_idx = list(range(max(0, len(archive_x) - recent_keep), len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            # if nothing evaluated successfully, return a uniform point (not evaluated) with inf fitness\n            x_try = self._uniform_array(lb, ub)\n            self.x_opt = np.array(x_try, dtype=float)\n            self.f_opt = float(\"inf\")\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example quick test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 137, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=100)", "error": "In the code, line 137, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=100)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "30a90cc4-dfdd-42b7-a723-f1338dbcc384", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local searches, heavy-tailed Cauchy jumps and occasional micro-restarts; designed for robust box-constrained continuous optimization.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 keep_memory=200, elite_frac=0.15, init_scale=0.08,\n                 F=0.8, cr=0.9, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.keep_memory = int(max(5, keep_memory))\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = float(\"inf\")\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim\n        Accepts: func.bounds as (lb, ub) tuple/list or object with lb/ub/lower/upper attributes.\n        Also checks func.lb/ub and func.lower/upper.\n        Returns lb, ub numpy arrays of length self.dim.\n        \"\"\"\n        lb = None\n        ub = None\n\n        # try func.bounds\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # object-like: try attributes\n                    lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                    ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n                    if lb_attr is not None and ub_attr is not None:\n                        lb = np.asarray(lb_attr, dtype=float)\n                        ub = np.asarray(ub_attr, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # fallback to func.lb / func.ub etc.\n        if lb is None or ub is None:\n            try:\n                lb_attr = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n                ub_attr = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast/resize to dim\n            lb = np.resize(lb, self.dim).astype(float).copy()\n            ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub or non-finite fallback to [-5,5]\n        if np.any(lb >= ub) or np.any(~np.isfinite(lb)) or np.any(~np.isfinite(ub)):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds. Returns new array.\n        \"\"\"\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.random(self.dim) * (ub - lb) + lb\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # bookkeeping\n        archive_x = []    # list of numpy arrays\n        archive_f = []    # list of floats\n        evals = 0\n\n        # initial sampling: ensure some initial diversity but not consume too much budget\n        init_n = min(max(10, int(0.05 * self.budget)), self.budget)\n        for _ in range(init_n):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.x_opt = np.array(x, dtype=float)\n                self.f_opt = f\n\n        # adaptation & history\n        recent_window = max(50, 5 * self.dim)\n        recent_success = deque(maxlen=recent_window)\n\n        # move probabilities\n        p_local = 0.35\n        p_de = 0.25\n        p_pca = 0.20\n        # rest -> heavy-tailed\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        stagnation_counter = 0\n        stagnation_limit = max(50, 10 * self.dim)\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness\n            if len(archive_f) > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # pick base: mostly best, sometimes random archive or uniform\n            r_base = self.rng.random()\n            if len(archive_x) == 0 or r_base < 0.08:\n                base = self._uniform_array(lb, ub)\n            else:\n                if self.x_opt is None or self.rng.random() < 0.15:\n                    base = self._uniform_array(lb, ub)\n                else:\n                    if len(sorted_idx) == 0 or self.rng.random() < 0.3:\n                        base = self._uniform_array(lb, ub)\n                    else:\n                        # biased selection towards better ranks\n                        rank = int(min(len(sorted_idx) - 1, np.floor(self.rng.exponential(scale=1.5))))\n                        idx = sorted_idx[rank]\n                        base = np.array(archive_x[idx], dtype=float)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_jitter = np.exp(0.12 * self.rng.normal(0.0, 1.0, size=self.dim))\n            scale = base_scale_vec * gscale * per_dim_jitter\n\n            # choose move type\n            r = self.rng.random()\n\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult * 0.9)\n\n            # DE-style donor move with crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # choose three distinct indices from archive\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                x0 = np.array(archive_x[idxs[0]], dtype=float)\n                x1 = np.array(archive_x[idxs[1]], dtype=float)\n                x2 = np.array(archive_x[idxs[2]], dtype=float)\n                donor = x0 + self.F * (x1 - x2)\n                donor = self._reflect_bounds(donor, lb, ub)\n                # crossover mask (ensure at least one dim taken)\n                mask = (self.rng.random(self.dim) < self.cr)\n                mask[self.rng.integers(0, self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand = cand + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.07)\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 3:\n                n_keep = max(2, int(max(2, self.elite_frac * len(archive_x))))\n                n_keep = min(n_keep, len(archive_x))\n                elite_idx = sorted_idx[:n_keep]\n                elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                center_choice = self.rng.random()\n                if center_choice < 0.7:\n                    center = np.mean(elites, axis=0)\n                else:\n                    center = elites[self.rng.integers(0, n_keep)]\n                # compute covariance and principal components\n                try:\n                    cov = np.cov(elites, rowvar=False)\n                    cov += 1e-12 * np.eye(self.dim)\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # sort descending\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    sqrt_vals = np.sqrt(np.maximum(eigvals, 1e-12))\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * sqrt_vals * (gscale * 0.8)\n                    # scale by global span to keep consistent scale across dims\n                    coeffs = coeffs * (span / (np.mean(span) + 1e-12))\n                    cand = center + eigvecs.dot(coeffs)\n                    # small isotropic jitter\n                    cand = cand + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.04)\n                except Exception:\n                    # fallback to local jitter\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.6)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy; clip extremes\n                cauch = self.rng.standard_cauchy(self.dim)\n                cauch = np.clip(cauch, -1e2, 1e2)\n                mult = 0.7 + 0.6 * self.rng.random()\n                cand = base + cauch * (scale * 0.65 * mult)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            else:\n                cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            improved = False\n            if f_cand < self.f_opt:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= 10:\n                success_rate = sum(recent_success) / len(recent_success)\n                if success_rate > 0.25:\n                    gscale = max(1e-6, gscale * (0.9 - 0.1 * self.rng.random()))\n                elif success_rate < 0.05:\n                    gscale = min(1e3, gscale * (1.15 + 0.25 * self.rng.random()))\n                else:\n                    # slight jitter\n                    gscale = gscale * np.exp(0.03 * (self.rng.random() - 0.5))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter > stagnation_limit and evals < self.budget:\n                old_gscale = gscale\n                gscale = min(gscale * 8.0, 1e3)  # broaden temporarily\n                # inject a few diverse points (but not exceeding budget)\n                inj_count = min(3 + self.dim // 10, max(1, (self.budget - evals) // 8))\n                for _ in range(inj_count):\n                    if evals >= self.budget:\n                        break\n                    if len(archive_x) == 0 or self.rng.random() < 0.6:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # perturb a top elite\n                        top_k = max(1, min(len(archive_x), max(1, int(0.05 * len(archive_x) + 1))))\n                        idx = sorted_idx[self.rng.integers(0, top_k)]\n                        elite = np.array(archive_x[idx], dtype=float)\n                        # larger perturbation\n                        x_inj = elite + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * gscale * (1.0 + self.rng.random()))\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        self.f_opt = float(f_inj)\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                stagnation_counter = 0\n                recent_success.clear()\n\n            # prune memory periodically\n            if len(archive_x) > 3 * self.keep_memory:\n                # keep some best, some recent\n                n_best_keep = max(int(0.6 * self.keep_memory), min(len(sorted_idx), self.keep_memory // 2))\n                best_idx = list(sorted_idx[:n_best_keep])\n                # keep some recent tail\n                recent_keep = list(range(max(0, len(archive_x) - (self.keep_memory - n_best_keep)), len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_keep))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            # try at least one evaluation if budget allows\n            if evals < self.budget:\n                x_try = self._uniform_array(lb, ub)\n                try:\n                    self.f_opt = float(func(x_try))\n                    self.x_opt = np.array(x_try, dtype=float)\n                except Exception:\n                    self.f_opt = float(\"inf\")\n                    self.x_opt = self._uniform_array(lb, ub)\n            else:\n                # no budget left and no valid best; return a uniform point (not evaluated)\n                self.x_opt = self._uniform_array(lb, ub)\n                self.f_opt = float(\"inf\")\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example quick test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "error": "In the code, line 148, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d5b3a436-a785-4488-b10d-e70e660d37b1", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and micro-restarts — designed for robust box-constrained continuous optimization on [-5,5]^dim.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 F=0.8, cr=0.9, keep_memory=None,\n                 init_scale=0.1, elite_frac=0.15,\n                 injection_budget=20):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.F = float(F)\n        self.cr = float(cr)\n        self.init_scale = float(init_scale)\n        self.elite_frac = float(elite_frac)\n        self.injection_budget = int(injection_budget)\n        # memory sizing\n        if keep_memory is None:\n            self.keep_memory = max(50, 10 * self.dim)\n        else:\n            self.keep_memory = int(keep_memory)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = float(\"inf\")\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds in a few common formats, else fallback to [-5,5]^dim.\n        Returns lb, ub as numpy arrays of length self.dim.\n        \"\"\"\n        lb = None\n        ub = None\n\n        # common patterns\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            try:\n                # tuple/list-like (lb, ub) or object with lb/ub\n                if isinstance(b, (list, tuple)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                    ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n                    if lb_attr is not None and ub_attr is not None:\n                        lb = np.asarray(lb_attr, dtype=float)\n                        ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # other possible direct attributes on func\n        if lb is None or ub is None:\n            lb_attr = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            ub_attr = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n            try:\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # fallback to standard [-5,5]^dim\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast/rescale if scalars given\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # ensure valid\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim or np.any(lb >= ub):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = 2*lb - x\n            if below.any():\n                x[below] = 2.0 * lb[below] - x[below]\n            if above.any():\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # base per-dim scale\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        # bookkeeping\n        evals = 0\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        # initial sampling to populate archive (small fraction)\n        # ensure at least a few evaluations but not too many\n        n_init = min(max(10, 3 * self.dim), max(1, self.budget // 20))\n        n_init = max(1, min(n_init, self.budget))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        stagnation_limit = max(40, 10 * self.dim)\n        recent_window = max(20, 5 * self.dim)\n        recent_success = deque(maxlen=recent_window)\n\n        # move probabilities\n        p_local = 0.35\n        p_de = 0.25\n        p_pca = 0.20\n        # rest => heavy-tailed\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # per-dim working scale\n        scale = base_scale_vec.copy()\n\n        # stagnation tracking\n        stagnation_counter = 0\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness (ascending)\n            if len(archive_f) > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # pick base: mostly the best-known, sometimes biased archive member, rarely uniform\n            if len(archive_x) == 0 or self.rng.random() < 0.02:\n                base = self._uniform_array(lb, ub)\n            else:\n                if self.rng.random() < 0.8 and self.x_opt is not None:\n                    base = np.array(self.x_opt, dtype=float)\n                else:\n                    # biased selection towards better ranks\n                    if len(sorted_idx) > 0:\n                        rank = int(min(len(sorted_idx) - 1, int(self.rng.exponential(scale=1.5))))\n                        idx = sorted_idx[min(rank, len(sorted_idx)-1)]\n                        base = np.array(archive_x[idx], dtype=float)\n                    else:\n                        base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_jitter = np.exp(0.12 * self.rng.normal(0.0, 1.0, size=self.dim))\n            scale = np.maximum(scale * per_dim_jitter, base_scale_vec * 1e-6)\n\n            # choose move type\n            r = self.rng.random()\n\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # multiplicative log-normal jitter per-dim and small gaussian\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                jitter = 0.7 * (self.rng.normal(0.0, 1.0, size=self.dim))\n                cand = base + (scale * rand_mult) * jitter\n\n            # DE-style donor move with crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # choose three distinct indices from archive\n                n = len(archive_x)\n                idxs = self.rng.choice(n, size=3, replace=False)\n                x1 = np.array(archive_x[idxs[0]], dtype=float)\n                x2 = np.array(archive_x[idxs[1]], dtype=float)\n                x3 = np.array(archive_x[idxs[2]], dtype=float)\n                donor = x1 + self.F * (x2 - x3)\n                # crossover with base\n                mask = (self.rng.random(self.dim) < self.cr)\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand = cand + self.rng.normal(0.0, 0.5, size=self.dim) * (scale * 0.2)\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 3:\n                # choose top elites\n                n_keep = max(2, int(max(2, self.elite_frac * len(archive_x))))\n                n_keep = min(n_keep, len(archive_x))\n                elite_idx = sorted_idx[:n_keep]\n                elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                center = np.mean(elites, axis=0)\n                # compute covariance of elites (regularized)\n                A = elites - center\n                cov = (A.T @ A) / max(1.0, A.shape[0] - 1)\n                cov += np.eye(self.dim) * (1e-8 + 1e-6 * np.mean(np.diag(cov) + 1e-12))\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # sort descending\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    sqrt_vals = np.sqrt(np.maximum(eigvals, 1e-12))\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * (sqrt_vals * (gscale))\n                    # scale by global span to keep consistent scale across dims\n                    coeffs = coeffs * (span / (np.mean(span) + 1e-12))\n                    cand = center + eigvecs.dot(coeffs)\n                    # small isotropic jitter\n                    cand = cand + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.05)\n                except Exception:\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.3)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy but clip extreme tails\n                cauch = self.rng.standard_cauchy(size=self.dim)\n                cauch = np.clip(cauch, -10.0, 10.0)\n                mult = 0.8 + 0.6 * self.rng.random()\n                cand = base + cauch * (scale * 0.6 * mult)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(f_cand)\n\n            improved = False\n            if f_cand < self.f_opt:\n                improved = True\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= max(5, self.dim):\n                success_rate = float(np.mean(recent_success))\n                if success_rate > 0.25:\n                    gscale *= 1.15\n                elif success_rate < 0.05:\n                    gscale *= 0.85\n                # small multiplicative jitter for exploration\n                gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter > stagnation_limit and evals < self.budget:\n                old_gscale = gscale\n                inj = 0\n                for _ in range(self.injection_budget):\n                    if evals >= self.budget:\n                        break\n                    # sometimes uniform random injection, sometimes perturb elite\n                    if len(archive_x) == 0 or self.rng.random() < 0.3:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # perturb a top elite\n                        top_k = max(1, min(len(archive_x), int(0.05 * len(archive_x) + 1)))\n                        if len(sorted_idx) == 0:\n                            elite = self._uniform_array(lb, ub)\n                        else:\n                            idx = sorted_idx[self.rng.integers(0, top_k)]\n                            elite = np.array(archive_x[idx], dtype=float)\n                        x_inj = elite + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * gscale * (1.0 + self.rng.random()))\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(f_inj)\n                    inj += 1\n                    if f_inj < self.f_opt:\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        self.f_opt = float(f_inj)\n                        stagnation_counter = 0\n                        recent_success.append(1)\n                        # keep exploring a bit if improvement\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                stagnation_counter = 0\n\n            # prune memory periodically (keep best and some recent)\n            if len(archive_x) > 2 * self.keep_memory:\n                # keep best half of keep_memory and most recent half\n                keep_best = max(1, self.keep_memory // 2)\n                keep_recent = self.keep_memory - keep_best\n                sorted_idx = np.argsort(archive_f)\n                best_idx = list(sorted_idx[:keep_best])\n                recent_idx = list(range(max(0, len(archive_x) - keep_recent), len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            # try to get best from archive\n            if len(archive_f) > 0:\n                best = int(np.argmin(archive_f))\n                self.f_opt = float(archive_f[best])\n                self.x_opt = np.array(archive_x[best], dtype=float)\n            else:\n                # evaluate a uniform point if budget remains\n                if evals < self.budget:\n                    x_try = self._uniform_array(lb, ub)\n                    try:\n                        self.f_opt = float(func(x_try))\n                        self.x_opt = np.array(x_try, dtype=float)\n                    except Exception:\n                        self.f_opt = float(\"inf\")\n                        self.x_opt = self._uniform_array(lb, ub)\n                else:\n                    # no budget to evaluate: return a uniform point as fallback\n                    self.x_opt = self._uniform_array(lb, ub)\n                    self.f_opt = float(\"inf\")\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 161, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "error": "In the code, line 161, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=recent_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "501b0cb4-2087-4215-a26c-3727a62b7c48", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and micro-restarts (designed for robust box-constrained continuous optimization on [-5,5]^dim).", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_scale=0.1, elite_frac=0.15, keep_memory=500,\n                 injection_budget=20, F=0.8, cr=0.9,\n                 p_de=0.20, p_pca=0.15, p_local=0.55):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # sampler hyper-params\n        self.init_scale = float(init_scale)\n        self.elite_frac = float(elite_frac)\n        self.keep_memory = int(max(10, keep_memory))\n        self.injection_budget = int(max(1, injection_budget))\n\n        # differential evolution-like params\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # move probabilities (must sum <=1; rest => heavy-tailed jump)\n        self.p_de = float(p_de)\n        self.p_pca = float(p_pca)\n        self.p_local = float(p_local)\n        # ensure probabilities are sane\n        s = self.p_de + self.p_pca + self.p_local\n        if s > 0.9999:\n            self.p_local *= 0.9999 / s\n\n        # internal state outputs\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object, else fallback to [-5,5]^dim.\n        Accepts attributes like func.bounds.lb / func.bounds.ub, func.lb, func.ub,\n        func.lower_bounds, func.upper_bounds, etc.\n        Returns lb, ub (numpy arrays).\n        \"\"\"\n        lb = None\n        ub = None\n        # common patterns\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n            ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n            if lb_attr is not None and ub_attr is not None:\n                try:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # direct attributes on func\n        if lb is None:\n            for name in (\"lb\", \"lower\", \"lower_bounds\"):\n                attr = getattr(func, name, None)\n                if attr is not None:\n                    try:\n                        lb = np.asarray(attr, dtype=float)\n                        break\n                    except Exception:\n                        lb = None\n        if ub is None:\n            for name in (\"ub\", \"upper\", \"upper_bounds\"):\n                attr = getattr(func, name, None)\n                if attr is not None:\n                    try:\n                        ub = np.asarray(attr, dtype=float)\n                        break\n                    except Exception:\n                        ub = None\n\n        # fallback to standard [-5,5]^dim\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast if scalar\n            if np.isscalar(lb) or lb.size == 1:\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if np.isscalar(ub) or ub.size == 1:\n                ub = np.full(self.dim, float(ub), dtype=float)\n            # safety check\n            if lb.shape[0] != self.dim or ub.shape[0] != self.dim or np.any(lb >= ub):\n                lb = np.full(self.dim, -5.0, dtype=float)\n                ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = 2.0 * lb[below] - x[below]\n            if above.any():\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp to ensure feasibility\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.random(self.dim) * (ub - lb) + lb\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(1e-12, self.init_scale) * span  # per-dim base scale\n\n        # bookkeeping\n        evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        # initial sampling to populate archive\n        # choose a small initial budget proportional to dim but never more than total budget/4\n        n_init = max(3, min(self.budget // 20, self.dim * 2, 50))\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_window = max(50, self.dim * 5)\n        recent_success = deque(maxlen=recent_window)\n\n        # global multiplicative scale for PCA coefficients / DE scaling\n        gscale = 1.0\n\n        # stagnation tracking\n        stagnation_counter = 0\n        stagnation_threshold = max(20, self.dim * 10)\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness (ascending)\n            n_archive = len(archive_x)\n            if n_archive > 0:\n                sorted_idx = np.argsort(archive_f)\n            else:\n                sorted_idx = np.array([], dtype=int)\n\n            # pick base: mostly the best-known, sometimes biased archive member, rarely uniform\n            r_base = self.rng.random()\n            if n_archive == 0 or r_base < 0.02:\n                base = self._uniform_array(lb, ub)\n            else:\n                if self.x_opt is not None and self.rng.random() < 0.8:\n                    base = np.array(self.x_opt, dtype=float)\n                else:\n                    # biased pick towards better ranks\n                    if n_archive <= 1:\n                        base = np.array(archive_x[0], dtype=float)\n                    else:\n                        # rank-biased selection: sample index with exponential bias\n                        ranks = np.arange(1, n_archive + 1)\n                        probs = np.exp(-0.03 * ranks)\n                        probs /= probs.sum()\n                        idx = self.rng.choice(sorted_idx, p=probs)\n                        base = np.array(archive_x[int(idx)], dtype=float)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            scale = base_scale_vec * (0.5 + 1.5 * self.rng.random(self.dim)) * (0.8 + 0.4 * gscale)\n\n            # choose move type\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < self.p_local:\n                # multiplicative log-normal jitter per-dim + small gaussian\n                logj = self.rng.normal(0.0, 0.3, size=self.dim)\n                per_dim_scale = scale * np.exp(logj)\n                cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale * 0.8\n\n            # DE-style donor move with binomial crossover\n            elif r < self.p_local + self.p_de and n_archive >= 3:\n                # choose three distinct indices from archive excluding base if possible\n                idxs = None\n                try:\n                    all_idx = np.arange(n_archive)\n                    # pick without replacement\n                    idxs = self.rng.choice(all_idx, size=3, replace=False)\n                    x1 = np.array(archive_x[int(idxs[0])], dtype=float)\n                    x2 = np.array(archive_x[int(idxs[1])], dtype=float)\n                    x3 = np.array(archive_x[int(idxs[2])], dtype=float)\n                    donor = x1 + self.F * (x2 - x3) * gscale\n                except Exception:\n                    donor = base.copy()\n                # binomial crossover\n                mask = self.rng.random(self.dim) < self.cr\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small jitter\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # PCA-guided sampling using elites\n            elif r < self.p_local + self.p_de + self.p_pca and n_archive >= max(3, int(1.0 / self.elite_frac)):\n                # choose top elites\n                n_keep = max(3, int(max(3, self.elite_frac * n_archive)))\n                elite_idx = sorted_idx[:n_keep]\n                elites = np.vstack([archive_x[int(i)] for i in elite_idx])\n                center = elites.mean(axis=0)\n                A = elites - center\n                cov = (A.T @ A) / max(1.0, A.shape[0] - 1)\n                # regularize\n                cov += np.eye(self.dim) * (1e-8 + 1e-6 * np.mean(np.diag(cov) + 1e-12))\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * np.sqrt(np.maximum(eigvals, 1e-12))\n                    # scale by global scale and per-dim base\n                    coeffs = coeffs * (0.4 * gscale)\n                    cand = center + (eigvecs @ coeffs)\n                    # small isotropic jitter\n                    cand += self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.05)\n                except Exception:\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * (scale * 0.3)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy via inverse CDF\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                cauch = np.clip(cauch, -10.0, 10.0)\n                mult = 0.7 + 0.6 * self.rng.random()\n                cand = base + cauch * (scale * 0.6 * mult)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            improved = False\n            if f_cand < self.f_opt:\n                improved = True\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= max(5, self.dim):\n                success_rate = float(np.mean(recent_success))\n                # if improving often, increase exploratory scale modestly; if rarely, shrink\n                if success_rate > 0.3:\n                    gscale *= 1.08\n                elif success_rate < 0.05:\n                    gscale *= 0.85\n                # small multiplicative jitter for exploration\n                gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter >= stagnation_threshold and evals < self.budget:\n                old_gscale = gscale\n                # perform a bounded number of injections (but don't exceed budget)\n                inj = 0\n                for _ in range(self.injection_budget):\n                    if evals >= self.budget:\n                        break\n                    # sometimes uniform random injection, sometimes perturb elite\n                    if self.rng.random() < 0.4 or len(archive_x) < 3:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # perturb a top elite\n                        if len(sorted_idx) > 0:\n                            top_k = max(1, min(len(sorted_idx), int(0.05 * len(sorted_idx) + 1)))\n                            idx = int(sorted_idx[self.rng.integers(0, top_k)])\n                            elite = np.array(archive_x[idx], dtype=float)\n                        else:\n                            elite = self._uniform_array(lb, ub)\n                        # perturb along random PCA of small sample or gaussian\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * (0.5 + self.rng.random()))\n                        x_inj = self._reflect_bounds(elite + perturb, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    inj += 1\n                    if f_inj < self.f_opt:\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        self.f_opt = float(f_inj)\n                        stagnation_counter = 0\n                        recent_success.append(1)\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                stagnation_counter = 0\n\n            # prune memory periodically (keep best and some recent)\n            if len(archive_x) > 3 * self.keep_memory:\n                # keep best half of keep_memory and most recent half\n                keep_best = max(1, self.keep_memory // 2)\n                # indices of best\n                best_idx = np.argsort(archive_f)[:keep_best]\n                # indices of most recent\n                recent_idx = np.arange(max(0, len(archive_x) - self.keep_memory // 2), len(archive_x))\n                keep_idx = np.unique(np.concatenate([best_idx, recent_idx])).astype(int)\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            if len(archive_f) > 0:\n                best = int(np.argmin(archive_f))\n                self.x_opt = np.array(archive_x[best], dtype=float)\n                self.f_opt = float(archive_f[best])\n            else:\n                # no evaluations? return a uniform point\n                x_try = self._uniform_array(lb, ub)\n                try:\n                    f_try = float(func(x_try)) if evals < self.budget else float(\"inf\")\n                except Exception:\n                    f_try = float(\"inf\")\n                self.x_opt = np.array(x_try, dtype=float)\n                self.f_opt = float(f_try)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 60, in _extract_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "error": "In the code, line 60, in _extract_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a0e91d97-7854-4aa2-9b28-9c123f083e9a", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and micro-restarts — a robust hybrid for box-constrained continuous optimization.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_scale=0.15, elite_frac=0.15, keep_memory=500,\n                 injection_budget=20, F=0.8, cr=0.9,\n                 p_de=0.23, p_pca=0.22, p_local=0.4):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(None if seed is None else int(seed))\n\n        # sampler hyper-params\n        self.init_scale = float(init_scale)\n        self.elite_frac = float(elite_frac)\n        self.keep_memory = int(keep_memory)\n\n        # differential evolution-like params\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # move probabilities (sum <=1; remainder => heavy-tailed jump)\n        self.p_de = float(p_de)\n        self.p_pca = float(p_pca)\n        self.p_local = float(p_local)\n        if (self.p_de + self.p_pca + self.p_local) > 0.9999:\n            # normalize down to leave tiny prob for Cauchy\n            s = (self.p_de + self.p_pca + self.p_local)\n            self.p_de *= 0.9999 / s\n            self.p_pca *= 0.9999 / s\n            self.p_local *= 0.9999 / s\n\n        self.injection_budget = int(injection_budget)\n\n        # internal state outputs\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object, else fallback to [-5,5]^dim.\n        Accepts attributes like func.bounds.lb / func.bounds.ub, func.lb, func.ub,\n        func.lower_bounds, func.upper_bounds, etc.\n        Returns lb, ub (numpy arrays).\n        \"\"\"\n        lb = None\n        ub = None\n\n        # common patterns\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            for la in (\"lb\", \"lower\", \"lower_bounds\", \"low\"):\n                va = getattr(b, la, None)\n                if va is not None:\n                    try:\n                        lb = np.asarray(va, dtype=float)\n                        break\n                    except Exception:\n                        lb = None\n            for ua in (\"ub\", \"upper\", \"upper_bounds\", \"high\"):\n                va = getattr(b, ua, None)\n                if va is not None:\n                    try:\n                        ub = np.asarray(va, dtype=float)\n                        break\n                    except Exception:\n                        ub = None\n\n        # direct attributes on func\n        if lb is None or ub is None:\n            for name in (\"lb\", \"lower\", \"lower_bounds\", \"low\"):\n                if lb is None:\n                    attr = getattr(func, name, None)\n                    if attr is not None:\n                        try:\n                            lb = np.asarray(attr, dtype=float)\n                        except Exception:\n                            lb = None\n            for name in (\"ub\", \"upper\", \"upper_bounds\", \"high\"):\n                if ub is None:\n                    attr = getattr(func, name, None)\n                    if attr is not None:\n                        try:\n                            ub = np.asarray(attr, dtype=float)\n                        except Exception:\n                            ub = None\n\n        # fallback to standard [-5,5]^dim\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast scalars if needed\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n\n        # final safety: ensure shapes match\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            lb = np.resize(lb, (self.dim,))\n            ub = np.resize(ub, (self.dim,))\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = 2.0 * lb[below] - x[below]\n            if above.any():\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp to ensure feasibility\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(1e-12, self.init_scale) * span  # per-dim base scale\n\n        # bookkeeping\n        evals = 0\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        # initial sampling to populate archive\n        n_init = max(3, min(self.budget // 20 if self.budget >= 20 else 3, self.dim * 2, 50))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n            evals += 1\n\n        # adaptation & history\n        recent_success = deque(maxlen=40)\n        gscale = 1.0  # global multiplicative scale for DE/PCA\n        stagnation_counter = 0\n        stagnation_threshold = max(50, self.dim * 20)\n\n        # main loop\n        while evals < self.budget:\n            n_archive = len(archive_x)\n            if n_archive == 0:\n                base = self._uniform_array(lb, ub)\n                base_idx = None\n            else:\n                # sort archive by fitness ascending\n                sorted_idx = np.argsort(np.asarray(archive_f, dtype=float))\n                best_idx = int(sorted_idx[0])\n                # pick base: mostly the best-known, sometimes biased archive member, rarely uniform\n                rpick = self.rng.random()\n                if rpick < 0.85 or n_archive <= 3:\n                    base_idx = best_idx\n                    base = np.array(archive_x[base_idx], dtype=float)\n                elif rpick < 0.98:\n                    # rank-biased selection\n                    ranks = np.arange(n_archive)\n                    probs = np.exp(-0.03 * ranks)\n                    probs /= probs.sum()\n                    idx = int(self.rng.choice(sorted_idx, p=probs))\n                    base_idx = idx\n                    base = np.array(archive_x[base_idx], dtype=float)\n                else:\n                    base_idx = None\n                    base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale (introduce some jitter per-dim)\n            scale = max(1e-12, self.init_scale) * (1.0 + 0.5 * (gscale - 1.0))\n            per_dim_base = base_scale_vec * (scale * (1.0 + 0.3 * (self.rng.random(self.dim) - 0.5)))\n\n            # choose move type\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < self.p_local:\n                # multiplicative log-normal jitter per-dim + small gaussian\n                logj = self.rng.normal(0.0, 0.08, size=self.dim)\n                per_dim_scale = per_dim_base * np.exp(logj) * (0.8 + 0.4 * gscale)\n                cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale\n\n            # DE-style donor move with binomial crossover\n            elif r < self.p_local + self.p_de:\n                if n_archive >= 3:\n                    # choose three distinct indices possibly excluding base_idx\n                    choices = list(range(n_archive))\n                    if base_idx is not None and base_idx in choices and n_archive > 3:\n                        choices.remove(base_idx)\n                    a, b, c = self.rng.choice(choices, size=3, replace=False)\n                    x1 = np.array(archive_x[a], dtype=float)\n                    x2 = np.array(archive_x[b], dtype=float)\n                    x3 = np.array(archive_x[c], dtype=float)\n                    donor = x1 + self.F * (x2 - x3) * (0.8 + 0.6 * gscale)\n                    # binomial crossover\n                    mask = self.rng.rand(self.dim) < self.cr\n                    cand = np.array(base, dtype=float)\n                    cand[mask] = donor[mask]\n                    # small jitter\n                    cand += self.rng.normal(0.0, 1e-3, size=self.dim) * per_dim_base\n                else:\n                    # fallback to local\n                    cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_base * 0.5\n\n            # PCA-guided sampling using elites\n            elif r < self.p_local + self.p_de + self.p_pca:\n                if n_archive >= 4:\n                    n_keep = max(3, int(max(3, self.elite_frac * n_archive)))\n                    # take best n_keep\n                    idxs = np.argsort(np.asarray(archive_f))[:n_keep]\n                    elites = np.vstack([archive_x[int(i)] for i in idxs])\n                    center = np.mean(elites, axis=0)\n                    A = elites - center\n                    # compute covariance on small sample\n                    cov = (A.T @ A) / max(1.0, float(n_keep - 1))\n                    # regularize\n                    cov += np.eye(self.dim) * (1e-8 + 1e-6 * np.mean(np.diag(cov)))\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sample coefficients along principal directions\n                        coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * np.sqrt(np.maximum(eigvals, 1e-12))\n                        coeffs *= (0.5 + 1.5 * gscale)\n                        cand = center + (eigvecs @ coeffs)\n                        # small isotropic jitter\n                        cand += self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_base * 0.05)\n                    except Exception:\n                        cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_base * 0.3)\n                else:\n                    # fallback to local if insufficient elites\n                    cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_base * 0.6\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy via inverse CDF\n                u = self.rng.rand(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # scale larger to allow long jumps\n                cand = base + cauch * (per_dim_base * (2.0 + 3.0 * gscale))\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.array(cand, dtype=float)\n            # replace any non-finite with uniform in-bounds\n            mask_nonfinite = ~np.isfinite(cand)\n            if mask_nonfinite.any():\n                cand[mask_nonfinite] = self._uniform_array(lb[mask_nonfinite], ub[mask_nonfinite])\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= 8:\n                sr = float(sum(recent_success)) / float(len(recent_success))\n                if sr > 0.18:\n                    gscale = min(10.0, gscale * 1.06)\n                elif sr < 0.05:\n                    gscale = max(1e-4, gscale * 0.92)\n                # small multiplicative jitter for exploration\n                gscale *= (1.0 + 0.03 * (self.rng.random() - 0.5))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter >= stagnation_threshold and evals < self.budget:\n                old_gscale = gscale\n                injections = min(self.injection_budget, self.budget - evals)\n                for _ in range(injections):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.random() < 0.4 or len(archive_x) < 3:\n                        # uniform injection\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # perturb a top elite\n                        idx = int(sorted_idx[self.rng.randint(min(3, len(sorted_idx)))])\n                        elite = np.array(archive_x[idx], dtype=float)\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * (0.6 + 0.8 * self.rng.random()))\n                        x_inj = self._reflect_bounds(elite + perturb, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        stagnation_counter = 0\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n\n            # prune memory periodically (keep best and some recent)\n            if len(archive_x) > 2 * self.keep_memory:\n                # keep best half of keep_memory and most recent half\n                k = self.keep_memory\n                arr_f = np.asarray(archive_f, dtype=float)\n                best_idx = np.argsort(arr_f)[: (k // 2)]\n                recent_idx = np.arange(max(0, len(archive_x) - (k // 2)), len(archive_x))\n                keep_idx = np.unique(np.concatenate((best_idx, recent_idx))).astype(int)\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None and len(archive_x) > 0:\n            best = int(np.argmin(np.asarray(archive_f, dtype=float)))\n            self.x_opt = np.array(archive_x[best], dtype=float)\n            self.f_opt = float(archive_f[best])\n        if self.x_opt is None:\n            # fallback: uniform try\n            x_try = self._uniform_array(lb, ub)\n            try:\n                f_try = float(func(x_try))\n            except Exception:\n                f_try = float(\"inf\")\n            self.x_opt = np.array(x_try, dtype=float)\n            self.f_opt = float(f_try)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test:\nif __name__ == \"__main__\":\n    import math\n    def sphere(x):\n        return float(np.sum(x**2))\n    sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n    fbest, xbest = sampler(sphere)\n    print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 178, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=40)", "error": "In the code, line 178, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=40)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "50de3801-c770-4b76-b186-2d16ebdb98e2", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and micro-restarts for robust box-constrained continuous optimization.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 injection_budget=20,\n                 F=0.8, cr=0.9,\n                 p_de=0.25, p_pca=0.25, p_local=0.35,\n                 elite_frac=0.15,\n                 init_scale=0.2,\n                 keep_memory=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # sampler hyper-params\n        self.injection_budget = int(max(1, injection_budget))\n\n        # differential evolution-like params\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # move probabilities (rest -> heavy-tailed Cauchy)\n        self.p_de = float(p_de)\n        self.p_pca = float(p_pca)\n        self.p_local = float(p_local)\n        # ensure probabilities sum <= 1\n        s = (self.p_de + self.p_pca + self.p_local)\n        if s > 0.9999:\n            self.p_local *= 0.9999 / s\n\n        # other hyper-params\n        self.elite_frac = float(max(0.01, min(0.5, elite_frac)))\n        self.init_scale = float(init_scale)  # relative fraction of span\n        self.keep_memory = int(max(10, keep_memory))\n\n        # internal state outputs\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object, else fallback to [-5,5]^dim.\n        Returns lb, ub (numpy arrays).\n        \"\"\"\n        # Default bounds\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Helper to try various attribute patterns\n        def maybe_array(obj):\n            if obj is None:\n                return None\n            try:\n                arr = np.asarray(obj, dtype=float)\n                if arr.size == 1:\n                    arr = np.full(self.dim, float(arr), dtype=float)\n                if arr.size == self.dim:\n                    return arr\n            except Exception:\n                return None\n            return None\n\n        # Try func.bounds.lb / ub, func.lb / ub, etc.\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                for name in (\"lb\", \"lower\", \"lower_bounds\"):\n                    candidate = getattr(b, name, None)\n                    a = maybe_array(candidate)\n                    if a is not None:\n                        lb = a\n                        break\n                for name in (\"ub\", \"upper\", \"upper_bounds\"):\n                    candidate = getattr(b, name, None)\n                    a = maybe_array(candidate)\n                    if a is not None:\n                        ub = a\n                        break\n        except Exception:\n            pass\n\n        # direct attributes on func\n        for name in (\"lb\", \"lower_bounds\", \"lower\"):\n            a = maybe_array(getattr(func, name, None))\n            if a is not None:\n                lb = a\n                break\n        for name in (\"ub\", \"upper_bounds\", \"upper\"):\n            a = maybe_array(getattr(func, name, None))\n            if a is not None:\n                ub = a\n                break\n\n        # safety: ensure shapes\n        lb = np.asarray(lb, dtype=float).reshape(self.dim)\n        ub = np.asarray(ub, dtype=float).reshape(self.dim)\n\n        # final safety: if any ub <= lb, replace with defaults\n        span = ub - lb\n        if np.any(span <= 0):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = x.astype(float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect those below: x = 2*lb - x ; those above: x = 2*ub - x\n            x[below] = 2.0 * lb[below] - x[below]\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp to ensure feasibility\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(1e-12, self.init_scale) * span  # per-dim base scale\n\n        # bookkeeping\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n        evals = 0\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        # small circular memory of recent successes\n        recent_success = deque(maxlen=50)\n\n        # initial sampling to populate archive\n        n_init = int(min(max(4, self.dim * 4), max(1, self.budget // 8)))\n        n_init = max(1, n_init)\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        gscale = 1.0  # global multiplicative scale for PCA coefficients / DE scaling\n        stagnation_counter = 0\n        stagnation_threshold = max(20, self.dim * 10)\n        injections = 0\n\n        # main loop\n        while evals < self.budget:\n            n_archive = len(archive_x)\n            if n_archive == 0:\n                base = self._uniform_array(lb, ub)\n            else:\n                # sort archive by fitness (ascending)\n                sorted_idx = np.argsort(np.asarray(archive_f))\n                # pick base: mostly the best-known, sometimes biased archive member, rarely uniform\n                r_base = self.rng.random()\n                if r_base < 0.80 and self.x_opt is not None:\n                    base = np.array(self.x_opt, dtype=float)\n                elif n_archive > 1 and r_base < 0.995:\n                    # rank-biased selection towards better ranks\n                    ranks = np.arange(1, n_archive + 1)\n                    # exponential decay weights\n                    weights = np.exp(-0.15 * (ranks - 1))\n                    weights = weights / weights.sum()\n                    pick = self.rng.choice(sorted_idx, p=weights)\n                    base = np.array(archive_x[int(pick)], dtype=float)\n                else:\n                    base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_scale = base_scale_vec * (0.5 + 1.5 * gscale)\n            # small multiplicative log-normal jitter per-dim\n            per_dim_scale *= np.exp(self.rng.normal(0.0, 0.03, size=self.dim))\n\n            # choose move type\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < self.p_local:\n                # multiplicative log-normal jitter per-dim + small gaussian\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * per_dim_scale * 0.8\n                cand += self.rng.normal(0.0, 1e-3, size=self.dim)  # numerical jitter\n\n            # DE-style donor move with binomial crossover\n            elif r < (self.p_local + self.p_de) and n_archive >= 3:\n                all_idx = np.arange(n_archive)\n                # pick three distinct indices\n                try:\n                    idxs = self.rng.choice(all_idx, size=3, replace=False)\n                    x1 = np.array(archive_x[int(idxs[0])], dtype=float)\n                    x2 = np.array(archive_x[int(idxs[1])], dtype=float)\n                    x3 = np.array(archive_x[int(idxs[2])], dtype=float)\n                    donor = x1 + self.F * (x2 - x3)\n                    # binomial crossover\n                    mask = self.rng.random(self.dim) < self.cr\n                    if not mask.any():\n                        mask[self.rng.integers(0, self.dim)] = True\n                    cand = np.array(base, dtype=float)\n                    cand[mask] = donor[mask]\n                    # small jitter\n                    scale = np.maximum(1e-12, per_dim_scale)\n                    cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n                except Exception:\n                    cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale\n\n            # PCA-guided sampling using elites\n            elif r < (self.p_local + self.p_de + self.p_pca) and n_archive >= max(3, int(1.0 / max(1e-6, self.elite_frac))):\n                # choose top elites\n                sorted_idx = np.argsort(np.asarray(archive_f))\n                n_elite = max(3, int(max(3, self.elite_frac * n_archive)))\n                top_idx = sorted_idx[:n_elite]\n                elites = np.vstack([archive_x[int(i)] for i in top_idx])\n                center = elites.mean(axis=0)\n                try:\n                    cov = np.cov((elites - center).T)\n                    # regularize\n                    cov += np.eye(self.dim) * (1e-9 * np.mean(np.diag(cov)) + 1e-12)\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # sort descending\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * np.sqrt(np.maximum(eigvals, 0.0))\n                    # scale by global scale and per-dim base\n                    coeffs *= (0.5 + gscale) * (np.mean(per_dim_scale) / (np.mean(span) + 1e-12))\n                    cand = center + (eigvecs @ coeffs)\n                    # small isotropic jitter\n                    cand += self.rng.normal(0.0, 1e-3, size=self.dim) * np.mean(per_dim_scale)\n                except Exception:\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * (np.mean(per_dim_scale) * 0.3)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # standard Cauchy via standard generator\n                cauch = self.rng.standard_cauchy(self.dim)\n                # clip extreme tails for numerical stability\n                cauch = np.clip(cauch, -12.0, 12.0)\n                mult = 0.5 + 1.2 * self.rng.random()\n                cand = base + cauch * (per_dim_scale * mult)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.isfinite(cand).all():\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= 8:\n                success_rate = float(sum(recent_success)) / float(len(recent_success))\n                if success_rate > 0.25:\n                    gscale *= 1.08\n                elif success_rate < 0.05:\n                    gscale *= 0.85\n                # keep gscale within sensible bounds\n                gscale = float(max(0.1, min(5.0, gscale)))\n                # small multiplicative jitter for exploration\n                gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter > stagnation_threshold and injections < self.injection_budget and evals < self.budget:\n                injections += 1\n                # sometimes uniform random injection, sometimes perturb elite\n                if (self.rng.random() < 0.4) or (len(archive_x) == 0):\n                    x_inj = self._uniform_array(lb, ub)\n                else:\n                    # perturb a top elite\n                    sorted_idx = np.argsort(np.asarray(archive_f))\n                    top_k = max(1, min(len(sorted_idx), int(0.05 * len(sorted_idx) + 1)))\n                    pick = sorted_idx[self.rng.integers(0, top_k)]\n                    elite = np.array(archive_x[int(pick)], dtype=float)\n                    # perturb along principal directions of small sample or gaussian\n                    sample_k = min(len(archive_x), max(3, int(self.dim * 2)))\n                    sample_idx = self.rng.choice(np.arange(len(archive_x)), size=sample_k, replace=False)\n                    sample = np.vstack([archive_x[int(i)] for i in sample_idx])\n                    try:\n                        center_s = sample.mean(axis=0)\n                        cov_s = np.cov((sample - center_s).T)\n                        cov_s += np.eye(self.dim) * 1e-12\n                        eigvals_s, eigvecs_s = np.linalg.eigh(cov_s)\n                        coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * np.sqrt(np.maximum(eigvals_s, 0.0))\n                        coeffs *= 0.5 * (np.mean(per_dim_scale) / (np.mean(span) + 1e-12))\n                        x_inj = elite + (eigvecs_s @ coeffs)\n                    except Exception:\n                        x_inj = elite + self.rng.normal(0.0, 1.0, size=self.dim) * (np.mean(per_dim_scale) * 2.0)\n                x_inj = self._reflect_bounds(x_inj, lb, ub)\n                if evals < self.budget:\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.array(x_inj, dtype=float)\n                    stagnation_counter = 0\n                    # gently reset gscale\n                    gscale = max(0.5, gscale * 0.7)\n\n            # prune memory periodically (keep best and some recent)\n            if len(archive_x) > max(self.keep_memory * 3, 500):\n                # keep best half of keep_memory and most recent half\n                f_arr = np.asarray(archive_f)\n                sorted_idx = np.argsort(f_arr)\n                keep_best_k = max(1, self.keep_memory // 2)\n                best_idx = sorted_idx[:keep_best_k].tolist()\n                recent_k = self.keep_memory - keep_best_k\n                recent_idx = list(range(max(0, len(archive_x) - recent_k), len(archive_x)))\n                keep_idx = set(best_idx + recent_idx)\n                archive_x = [archive_x[i] for i in sorted(keep_idx)]\n                archive_f = [archive_f[i] for i in sorted(keep_idx)]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None and len(archive_f) > 0:\n            best = int(np.argmin(np.asarray(archive_f)))\n            self.x_opt = np.array(archive_x[best], dtype=float)\n            self.f_opt = float(archive_f[best])\n\n        if self.x_opt is None:\n            # no evaluations possible (budget==0?), return a uniform point\n            x0 = self._uniform_array(lb, ub)\n            return float(func(x0)) if self.budget > 0 else float(\"inf\"), x0\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 159, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=50)", "error": "In the code, line 159, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ebbd4986-ee6c-4897-89fb-c1878910c4f2", "fitness": 0.3404434789819469, "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and micro-restarts for robust box-constrained continuous optimization.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 keep_memory=300, init_scale=0.5,\n                 p_local=0.45, p_de=0.25, p_pca=0.15, p_cauchy=0.15,\n                 F=0.8, CR=0.5):\n        \"\"\"\n        budget: total number of function evaluations allowed\n        dim: problem dimensionality\n        seed: RNG seed (optional)\n        keep_memory: approximate memory size for archive management\n        init_scale: initial fraction of span used per-dim\n        p_local, p_de, p_pca, p_cauchy: move probabilities (should sum <=1; remainder behaves like cauchy)\n        F, CR: DE-style scale and crossover probability\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.keep_memory = int(max(10, keep_memory))\n        self.init_scale = float(init_scale)\n        # normalized move probabilities (they will be normalized later if needed)\n        self.p_local = float(p_local)\n        self.p_de = float(p_de)\n        self.p_pca = float(p_pca)\n        self.p_cauchy = float(p_cauchy)\n        # DE params\n        self.F = float(F)\n        self.CR = float(CR)\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n        # outputs\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object, else fallback to [-5,5]^dim.\n        Accepts attributes like func.bounds.lb / func.bounds.ub, func.lb, func.ub,\n        func.lower_bounds, func.upper_bounds, etc.\n        Returns lb, ub (numpy arrays length dim).\n        \"\"\"\n        lb = None\n        ub = None\n        # common patterns to try\n        try_names = [\n            (\"bounds\", \"lb\", \"ub\"),\n            (None, \"lb\", \"ub\"),\n            (None, \"lower_bounds\", \"upper_bounds\"),\n            (None, \"lower\", \"upper\"),\n            (None, \"lbound\", \"ubound\"),\n            (None, \"xlow\", \"xup\"),\n        ]\n        for main, lnm, unm in try_names:\n            try:\n                if main is None:\n                    l_attr = getattr(func, lnm, None)\n                    u_attr = getattr(func, unm, None)\n                else:\n                    main_attr = getattr(func, main, None)\n                    if main_attr is not None:\n                        l_attr = getattr(main_attr, lnm, None)\n                        u_attr = getattr(main_attr, unm, None)\n                    else:\n                        l_attr = u_attr = None\n                if l_attr is not None and u_attr is not None:\n                    lb = np.asarray(l_attr, dtype=float)\n                    ub = np.asarray(u_attr, dtype=float)\n                    break\n            except Exception:\n                continue\n\n        # try generic attributes\n        if lb is None or ub is None:\n            for name in (\"lb\", \"ub\", \"lower_bounds\", \"upper_bounds\", \"lower\", \"upper\"):\n                try:\n                    attr = getattr(func, name, None)\n                except Exception:\n                    attr = None\n                if attr is not None:\n                    arr = np.asarray(attr, dtype=float)\n                    if name in (\"lb\", \"lower_bounds\", \"lower\"):\n                        lb = arr\n                    else:\n                        ub = arr\n\n        # If only scalars provided, broadcast\n        if lb is not None and lb.ndim == 0:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub is not None and ub.ndim == 0:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # Validate shapes\n        if lb is None or ub is None or lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            # fallback to default [-5,5]^dim\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure lb <= ub per-dim\n        swap = lb > ub\n        if np.any(swap):\n            # swap where needed\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect candidate x back into bounds. Perform up to max_reflect reflections.\n        Finally clamp to bounds.\n        \"\"\"\n        x = np.asarray(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect individually\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        # fix nans/infs\n        bad = ~np.isfinite(x)\n        if np.any(bad):\n            x[bad] = lb[bad] + self.rng.random(np.sum(bad)) * (ub[bad] - lb[bad])\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return lb + self.rng.random(self.dim) * (ub - lb)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        # normalize move probabilities so their sum <=1; if >1 scale them down proportionally\n        total_p = self.p_local + self.p_de + self.p_pca + self.p_cauchy\n        if total_p > 0.999999:\n            self.p_local /= total_p\n            self.p_de /= total_p\n            self.p_pca /= total_p\n            self.p_cauchy /= total_p\n            total_p = self.p_local + self.p_de + self.p_pca + self.p_cauchy\n\n        # bounds\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(1e-12, self.init_scale) * span  # per-dim base scale\n\n        evals = 0\n        archive_x = []\n        archive_f = []\n\n        # small initial sampling\n        n_init = min(max(8, 4 * self.dim), max(1, self.budget // 10))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # ensure we have at least one point\n        if len(archive_x) == 0:\n            x = self._uniform_array(lb, ub)\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            self.f_opt = f\n            self.x_opt = np.array(x, dtype=float)\n\n        # adaptation state\n        gscale = 1.0  # global multiplicative scale\n        recent_success = []\n        recent_window = max(10, min(100, self.budget // 50))\n        stagnation_counter = 0\n        best_since_last_inject = 0\n\n        # main loop\n        while evals < self.budget:\n            n_archive = len(archive_x)\n            # sort archive\n            sorted_idx = np.argsort(archive_f)\n            ranks = np.empty(n_archive, dtype=float)\n            ranks[sorted_idx] = np.arange(n_archive)\n\n            # pick base\n            r_base = self.rng.random()\n            if r_base < 0.6:\n                base = archive_x[sorted_idx[0]].copy()\n            elif r_base < 0.95 and n_archive > 1:\n                # rank-biased pick\n                probs = np.exp(-0.05 * ranks)\n                probs /= probs.sum()\n                idx = self.rng.choice(n_archive, p=probs)\n                base = archive_x[idx].copy()\n            else:\n                base = self._uniform_array(lb, ub)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            per_dim_scale = base_scale_vec * (0.5 + 0.5 * self.rng.random(self.dim)) * gscale\n            # ensure non-zero\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12 * (ub - lb + 1.0))\n\n            # choose move type\n            r = self.rng.random()\n            cand = base.copy()\n\n            try:\n                if r < self.p_local:\n                    # Local anisotropic gaussian around base\n                    # multiplicative log-normal jitter per-dim + small gaussian\n                    log_jitter = np.exp(self.rng.normal(0.0, 0.12, size=self.dim))\n                    cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_scale * log_jitter)\n                    # small correlated perturbation along mean direction\n                    if n_archive > 1 and self.rng.random() < 0.15:\n                        other = archive_x[self.rng.randint(n_archive)]\n                        cand += 0.15 * (other - base) * self.rng.random()\n                elif r < self.p_local + self.p_de and n_archive >= 4:\n                    # DE-style donor move with binomial crossover\n                    # choose three distinct indices\n                    idxs = self.rng.choice(n_archive, size=3, replace=False)\n                    x1 = archive_x[idxs[0]]\n                    x2 = archive_x[idxs[1]]\n                    x3 = archive_x[idxs[2]]\n                    donor = x1 + self.F * (x2 - x3) * gscale\n                    # binomial crossover\n                    jrand = self.rng.randint(self.dim)\n                    mask = self.rng.random(self.dim) < self.CR\n                    mask[jrand] = True\n                    cand = base.copy()\n                    cand[mask] = donor[mask]\n                    # small gaussian jitter\n                    cand += self.rng.normal(0.0, 0.02, size=self.dim) * per_dim_scale\n                elif r < self.p_local + self.p_de + self.p_pca and n_archive >= max(3, self.dim // 2):\n                    # PCA-guided sampling using elites\n                    k = max(3, min(n_archive, 2 * self.dim))\n                    elites_idx = sorted_idx[:k]\n                    elites = np.vstack([archive_x[i] for i in elites_idx])\n                    center = elites.mean(axis=0)\n                    A = elites - center\n                    # robust covariance\n                    cov = (A.T @ A) / max(1.0, A.shape[0] - 1)\n                    # small regularization\n                    reg = 1e-8 * np.eye(self.dim)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov + reg)\n                        order = np.argsort(-eigvals)\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # sample coefficients along principal directions (more on top components)\n                        coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * np.sqrt(np.maximum(eigvals, 0.0))\n                        # scale coefficients by global scale and base_scale_vec\n                        coeffs = coeffs * (0.6 * gscale)\n                        cand = center + (eigvecs @ coeffs)\n                        # small isotropic jitter\n                        cand += self.rng.normal(0.0, 1e-2, size=self.dim) * per_dim_scale\n                    except Exception:\n                        cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_scale * 0.3)\n                else:\n                    # heavy-tailed per-dim Cauchy jumps (Levy-like)\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n                    # clip extreme values to avoid infinities\n                    cauch = np.clip(cauch, -10.0, 10.0)\n                    cand = base + cauch * (per_dim_scale * (0.6 + 0.8 * self.rng.random(self.dim)))\n            except Exception:\n                # fallback small gaussian move\n                cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_scale * 0.5)\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.sum(bad)) * (ub[bad] - lb[bad])\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = float(\"inf\")\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(np.array(cand, dtype=float))\n            archive_f.append(float(f_cand))\n\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = np.array(cand, dtype=float)\n                improved = True\n                stagnation_counter = 0\n                best_since_last_inject = 0\n            else:\n                stagnation_counter += 1\n                best_since_last_inject += 1\n\n            # record recent success\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= 5:\n                success_rate = float(np.mean(recent_success))\n                target = 0.15\n                if success_rate > target:\n                    gscale *= 1.06\n                else:\n                    gscale *= 0.93\n                # small multiplicative jitter\n                gscale *= np.exp(self.rng.normal(0.0, 0.02))\n\n                # clamp gscale reasonably\n                gscale = float(np.clip(gscale, 1e-4, 1e2))\n\n            # stagnation detection & micro-restart (injections)\n            if stagnation_counter > (20 + 2 * self.dim):\n                # perform up to few injections but don't exceed budget\n                inj_count = min(5, max(1, (self.budget - evals) // 3))\n                for _ in range(inj_count):\n                    if evals >= self.budget:\n                        break\n                    # sometimes uniform, sometimes perturb elite\n                    if self.rng.random() < 0.6 and n_archive > 0:\n                        # perturb a top elite\n                        top_k = max(1, min(len(sorted_idx), int(0.03 * len(sorted_idx) + 1)))\n                        elite_idx = sorted_idx[self.rng.randint(top_k)]\n                        elite = np.array(archive_x[elite_idx], dtype=float)\n                        # perturb along small PCA of sample or gaussian\n                        small_sample_idx = self.rng.choice(len(archive_x), size=min(len(archive_x), 1 + self.dim // 4), replace=False)\n                        sample = np.vstack([archive_x[i] for i in small_sample_idx])\n                        center = sample.mean(axis=0)\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * (0.25 + self.rng.random() * 0.75))\n                        inj = elite + 0.5 * (center - elite) + perturb * (0.5 + 0.5 * self.rng.random())\n                    else:\n                        inj = self._uniform_array(lb, ub)\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    try:\n                        f_inj = float(func(inj))\n                    except Exception:\n                        f_inj = float(\"inf\")\n                    evals += 1\n                    archive_x.append(np.array(inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = np.array(inj, dtype=float)\n                        improved = True\n                # gentle reset of gscale and stagnation counters\n                gscale = max(0.5, min(2.0, gscale * 0.6))\n                stagnation_counter = 0\n                recent_success = []\n                best_since_last_inject = 0\n\n            # prune memory periodically (keep best and some recent)\n            if len(archive_x) > 2 * self.keep_memory:\n                # keep best half and most recent half\n                m = self.keep_memory\n                sorted_idx = np.argsort(archive_f)\n                keep_best = sorted_idx[: (m // 2)]\n                recent_idx = np.arange(max(0, len(archive_x) - (m // 2)), len(archive_x))\n                keep_idx = np.unique(np.concatenate([keep_best, recent_idx]))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return\n        if self.x_opt is None:\n            # fallback try one random point (if no valid evals occurred)\n            x_try = self._uniform_array(lb, ub)\n            try:\n                f_try = float(func(x_try)) if evals < self.budget else float(\"inf\")\n            except Exception:\n                f_try = float(\"inf\")\n            if f_try < self.f_opt:\n                self.f_opt = f_try\n                self.x_opt = np.array(x_try, dtype=float)\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.340 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11345328628301388, 0.23909990471850961, 0.477570810737717, 0.6739794507369603, 0.2691263670496742, 0.6297997088169363, 0.2687030710835946, 0.37868167117257323, 0.21026435296828339, 0.14375616625220622]}, "task_prompt": ""}
{"id": "bf5dfe3f-7276-4ca4-bb1c-af6b6f8ed0cd", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided hybrid sampler combining DE-style donors, PCA-guided elite moves, anisotropic Gaussian local moves, heavy-tailed Cauchy jumps and occasional micro-restarts — adapts scale from short-term success and keeps a compact archive to explore/exploit robustly on [-5,5]^dim.", "code": "import numpy as np\nfrom collections import deque\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 F=0.8,\n                 p_de=0.25, p_pca=0.25, p_local=0.35,  # rest => heavy-tailed jump\n                 initial_factor=3.0,  # initial global scale multiplier\n                 keep_memory=250,\n                 injection_budget=10,\n                 stagnation_threshold=150,\n                 window_success=50):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # DE-like scaling\n        self.F = float(F)\n\n        # move probabilities\n        self.p_de = float(p_de)\n        self.p_pca = float(p_pca)\n        self.p_local = float(p_local)\n        if (self.p_de + self.p_pca + self.p_local) > 1.0:\n            # normalize down proportionally\n            s = self.p_de + self.p_pca + self.p_local\n            self.p_de /= s\n            self.p_pca /= s\n            self.p_local /= s\n\n        # internal controls\n        self.initial_factor = float(initial_factor)\n        self.keep_memory = int(keep_memory)\n        self.injection_budget = int(injection_budget)\n        self.stagnation_threshold = int(stagnation_threshold)\n        self.window_success = int(window_success)\n\n        # state outputs (to be filled during run)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object, else fallback to [-5,5]^dim.\n        Accepts attributes like func.bounds.lb / func.bounds.ub, func.lb, func.ub,\n        func.lower_bounds, func.upper_bounds, etc.\n        Returns lb, ub (numpy arrays).\n        \"\"\"\n        # possible attribute names to probe (in priority order)\n        candidates = [\n            (\"bounds\", \"lb\", \"ub\"),\n            (\"bounds\", \"lower\", \"upper\"),\n            (\"lb\", None, None),\n            (\"ub\", None, None),\n            (\"lower_bounds\", None, None),\n            (\"upper_bounds\", None, None),\n            (\"lower\", None, None),\n            (\"upper\", None, None),\n            (\"bounds_lb\", None, None),\n            (\"bounds_ub\", None, None),\n        ]\n        lb = None\n        ub = None\n\n        for cand in candidates:\n            obj_name = cand[0]\n            try:\n                attr = getattr(func, obj_name, None)\n            except Exception:\n                attr = None\n            if attr is None:\n                # maybe direct attributes lb/ub on func\n                if cand[1] is None:\n                    # try direct attribute names like 'lb' or 'ub'\n                    if obj_name in (\"lb\", \"lower_bounds\", \"lower\", \"bounds_lb\"):\n                        try:\n                            arr = getattr(func, obj_name)\n                            lb = np.asarray(arr, dtype=float)\n                        except Exception:\n                            lb = None\n                    if obj_name in (\"ub\", \"upper_bounds\", \"upper\", \"bounds_ub\"):\n                        try:\n                            arr = getattr(func, obj_name)\n                            ub = np.asarray(arr, dtype=float)\n                        except Exception:\n                            ub = None\n                continue\n            # if attr is a simple array-like or scalar, treat as lb or ub if applicable\n            if isinstance(attr, (list, tuple, np.ndarray, float, int)):\n                # ambiguous: if attribute name looked like lb lower etc\n                if obj_name in (\"lb\", \"lower_bounds\", \"lower\", \"bounds_lb\"):\n                    lb = np.asarray(attr, dtype=float)\n                elif obj_name in (\"ub\", \"upper_bounds\", \"upper\", \"bounds_ub\"):\n                    ub = np.asarray(attr, dtype=float)\n                else:\n                    # attr might be a bounds-like object with .lb/.ub or [lb,ub]\n                    if hasattr(attr, \"lb\") and hasattr(attr, \"ub\"):\n                        try:\n                            lb = np.asarray(attr.lb, dtype=float)\n                            ub = np.asarray(attr.ub, dtype=float)\n                            break\n                        except Exception:\n                            pass\n                    elif hasattr(attr, \"__len__\") and len(attr) >= 2:\n                        try:\n                            lb = np.asarray(attr[0], dtype=float)\n                            ub = np.asarray(attr[1], dtype=float)\n                            break\n                        except Exception:\n                            pass\n                continue\n            # attr might be object with .lb/.ub\n            if hasattr(attr, \"lb\") and hasattr(attr, \"ub\"):\n                try:\n                    lb = np.asarray(attr.lb, dtype=float)\n                    ub = np.asarray(attr.ub, dtype=float)\n                    break\n                except Exception:\n                    pass\n            # attr might have attributes named as cand[1], cand[2]\n            if cand[1] is not None and cand[2] is not None:\n                a1 = getattr(attr, cand[1], None)\n                a2 = getattr(attr, cand[2], None)\n                if a1 is not None and a2 is not None:\n                    try:\n                        lb = np.asarray(a1, dtype=float)\n                        ub = np.asarray(a2, dtype=float)\n                        break\n                    except Exception:\n                        pass\n\n        # fallback to defaults\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # if scalars, broadcast\n        if np.isscalar(lb) or lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()) if hasattr(lb, \"item\") else float(lb), dtype=float)\n        if np.isscalar(ub) or ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()) if hasattr(ub, \"item\") else float(ub), dtype=float)\n\n        lb = np.asarray(lb, dtype=float).ravel()\n        ub = np.asarray(ub, dtype=float).ravel()\n        # safety check: if shapes mismatch, broadcast to dim\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect candidate x back into bounds along each axis.\n        Perform up to max_reflect reflections per dimension then clamp.\n        \"\"\"\n        x = x.copy()\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = lb + (lb - x)\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n            # small nudges if still out due to repeated overshoot\n            x = np.where(x < lb, lb + (lb + ub) * 1e-6, x)\n            x = np.where(x > ub, ub - (lb + ub) * 1e-6, x)\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            return self.rng.uniform(lb.item(), ub.item(), size=self.dim)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        # extract bounds\n        lb, ub = self._extract_bounds(func)\n\n        # bookkeeping\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # archives\n        archive_x = []\n        archive_f = []\n\n        # initial sampling to populate archive\n        init_pop = max(6, min(int(self.dim * 4), max(6, self.budget // 20)))\n        init_pop = min(init_pop, max(1, self.budget // 4))\n        # ensure at least 1\n        init_pop = max(1, init_pop)\n\n        for _ in range(init_pop):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_x.append(np.asarray(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.asarray(x, dtype=float)\n\n        # adaptation & history\n        gscale = self.initial_factor  # global multiplicative scale\n        recent_success = deque(maxlen=self.window_success)\n        iter_since_improve = 0\n        total_iters = 0\n\n        # main loop\n        while evals < self.budget:\n            total_iters += 1\n            n_archive = max(1, len(archive_x))\n            archive_x_arr = np.vstack(archive_x)\n\n            # sort archive by fitness ascending\n            sorted_idx = np.argsort(archive_f)\n            # pick base: biased towards best\n            if self.rng.rand() < 0.85:\n                # rank-biased selection exponential bias\n                ranks = np.arange(n_archive)\n                probs = np.exp(-ranks / max(1.0, 0.07 * n_archive))\n                probs = probs / probs.sum()\n                # map ranks to sorted indices\n                pick_rank = np.searchsorted(np.cumsum(probs), self.rng.rand())\n                pick_rank = min(max(0, pick_rank), n_archive - 1)\n                base_idx = sorted_idx[pick_rank]\n            else:\n                base_idx = self.rng.randint(0, n_archive)\n\n            base = np.array(archive_x[int(base_idx)], dtype=float)\n\n            # adaptive per-dim base scale (introduce jitter)\n            per_dim_scale = 0.15 * (1.0 + 0.5 * self.rng.rand(self.dim)) * (ub - lb)\n\n            # choose move type\n            r = self.rng.rand()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < self.p_local:\n                # multiplicative log-normal jitter per-dim + gaussian\n                jitter = np.exp(self.rng.normal(0.0, 0.15, size=self.dim))\n                local_noise = self.rng.normal(0.0, 1.0, size=self.dim) * (gscale * 0.1 * (ub - lb))\n                cand = base * jitter + local_noise\n                # small pull towards center to prevent runaway\n                center = 0.5 * (lb + ub)\n                cand = cand * 0.97 + 0.03 * center\n\n            # DE-style donor move with binomial crossover\n            elif r < self.p_local + self.p_de:\n                # choose three distinct indices\n                if n_archive >= 3:\n                    idxs = list(range(n_archive))\n                    # ensure we can exclude base_idx\n                    if base_idx in idxs and n_archive > 1:\n                        idxs.remove(int(base_idx))\n                    choices = self.rng.choice(idxs, size=min(2, len(idxs)), replace=False)\n                    # if we have at least 2 others\n                    if len(choices) >= 2:\n                        x1 = np.array(archive_x[int(choices[0])], dtype=float)\n                        x2 = np.array(archive_x[int(choices[1])], dtype=float)\n                        donor = base + self.F * (x1 - x2)\n                    else:\n                        donor = base + self.F * (self._uniform_array(lb, ub) - base)\n                else:\n                    donor = base + self.F * (self._uniform_array(lb, ub) - base)\n\n                # binomial crossover\n                cr = 0.2 + 0.6 * self.rng.rand()\n                mask = self.rng.rand(self.dim) < cr\n                if not mask.any():\n                    mask[self.rng.randint(0, self.dim)] = True\n                cand = np.where(mask, donor, base)\n                # small jitter\n                cand = cand + self.rng.normal(0, 1e-2, size=self.dim) * (ub - lb)\n\n            # PCA-guided sampling using elites\n            elif r < self.p_local + self.p_de + self.p_pca:\n                num_elites = max(2, min(1 + int(0.15 * n_archive), n_archive))\n                elite_idx = sorted_idx[:num_elites]\n                elites = np.vstack([archive_x[int(i)] for i in elite_idx])\n                center = elites.mean(axis=0)\n                # covariance and PCA\n                cov = np.cov((elites - center).T)\n                # regularize\n                cov += np.eye(self.dim) * 1e-8 * np.mean(np.diag(cov) + 1e-12)\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # sort descending\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    # sample coefficients along principal directions\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * np.sqrt(np.maximum(eigvals, 1e-12))\n                    # scale by global scale and per-dim base\n                    cand = center + eigvecs.dot(coeffs) * (0.6 * gscale)\n                    # small isotropic jitter\n                    cand = cand + self.rng.normal(0.0, 1.0, size=self.dim) * (0.15 * gscale * (ub - lb))\n                except Exception:\n                    # fallback to Gaussian local\n                    cand = center + self.rng.normal(0.0, 1.0, size=self.dim) * (0.2 * gscale * (ub - lb))\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                # mixture of global and per-dim cauchy\n                if self.rng.rand() < 0.5:\n                    scale = 0.5 * gscale * (ub - lb)\n                    cauch = self.rng.standard_cauchy(size=self.dim)\n                    cand = base + cauch * scale\n                else:\n                    # whole new heavy tail jump from center\n                    center = 0.5 * (lb + ub)\n                    scale = 0.8 * (ub - lb)\n                    cauch = self.rng.standard_cauchy(size=self.dim)\n                    cand = center + cauch * scale * (0.5 + 0.5 * self.rng.rand())\n\n            # Safety: ensure candidate finite and within bounds (reflect)\n            if cand is None or not np.isfinite(cand).all():\n                cand = self._uniform_array(lb, ub)\n            cand = np.asarray(cand, dtype=float)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # small chance to mix in an archived recent point (diversify)\n            if self.rng.rand() < 0.03 and n_archive > 1:\n                idx = self.rng.randint(0, n_archive)\n                cand = 0.7 * cand + 0.3 * archive_x[int(idx)]\n                cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(cand)\n            archive_f.append(f_cand)\n\n            improved = False\n            if f_cand < self.f_opt:\n                improved = True\n                self.f_opt = f_cand\n                self.x_opt = np.array(cand, dtype=float)\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            recent_success.append(1 if improved else 0)\n\n            # adapt gscale based on short-term success rate\n            if len(recent_success) >= max(5, self.window_success // 3):\n                rate = float(sum(recent_success)) / len(recent_success)\n                if rate > 0.25:\n                    # increase exploration\n                    gscale *= 1.08\n                elif rate < 0.05:\n                    # shrink exploration\n                    gscale *= 0.85\n                # small jitter\n                gscale *= np.exp(self.rng.normal(0.0, 0.01))\n                # clamp gscale\n                gscale = np.clip(gscale, 0.02, 20.0)\n\n            # stagnation detection & micro-restart (injections)\n            if iter_since_improve >= self.stagnation_threshold:\n                # perform a bounded number of injections (but don't exceed budget)\n                injections = min(self.injection_budget, max(1, (self.budget - evals) // 2))\n                for _ in range(injections):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.rand() < 0.5 or len(archive_x) < 2:\n                        # uniform random injection\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # perturb a top elite\n                        top_k = max(1, min(5, len(archive_x)))\n                        top_idx = np.argsort(archive_f)[:top_k]\n                        chosen = archive_x[int(self.rng.choice(top_idx))]\n                        # perturb along random PCA of a small neighbourhood or gaussian\n                        noise = self.rng.normal(0.0, 1.0, size=self.dim) * (0.2 * (ub - lb))\n                        x_inj = chosen + noise * (0.5 + 0.5 * self.rng.rand())\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    archive_x.append(np.asarray(x_inj, dtype=float))\n                    archive_f.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = np.asarray(x_inj, dtype=float)\n                        iter_since_improve = 0\n                        recent_success.append(1)\n                # gently reset gscale to encourage new exploration directions\n                gscale = max(0.5, gscale * 0.7)\n                iter_since_improve = 0\n\n            # prune memory periodically (every so many iterations prune)\n            if len(archive_x) > self.keep_memory * 2 or (total_iters % 50 == 0 and len(archive_x) > self.keep_memory):\n                # keep best half of keep_memory and most recent half\n                km = max(4, self.keep_memory)\n                half = km // 2\n                sorted_idx = np.argsort(archive_f)\n                best_idx = sorted_idx[:half]\n                recent_idx = np.arange(max(0, len(archive_x) - half), len(archive_x))\n                keep_idx = np.unique(np.concatenate([best_idx, recent_idx])).astype(int)\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n            # safety small cap on archive size\n            if len(archive_x) > max(500, 5 * self.dim):\n                # keep best 70% and random 30%\n                sorted_idx = np.argsort(archive_f)\n                keepN = max(200, int(0.7 * self.keep_memory))\n                keep_idx = sorted_idx[:keepN].tolist()\n                # add some random samples to maintain diversification\n                remaining = list(set(range(len(archive_x))) - set(keep_idx))\n                self.rng.shuffle(remaining)\n                keep_idx += remaining[:max(0, self.keep_memory - len(keep_idx))]\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return: ensure at least one valid point\n        if self.x_opt is None:\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.x_opt = x\n            self.f_opt = f\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 237, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.window_success)", "error": "In the code, line 237, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.window_success)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ee843aa4-a145-4b9a-9ee3-713e2e4c7dac", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided Hybrid Adaptive Directional Sampling — a compact, memory-driven hybrid sampler that mixes DE-style donors, PCA-guided elite sampling, anisotropic local Gaussian moves and heavy-tailed Cauchy jumps with micro-restarts and short-term scale adaptation.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_pop=20, max_archive=300, elite_frac=0.2,\n                 gscale_init=0.3, adapt_window=50, target_success=0.2,\n                 min_gscale=1e-6, max_gscale=5.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population/archive settings\n        self.init_pop = max(2, int(init_pop))\n        self.max_archive = max_archive\n        self.elite_frac = float(elite_frac)\n\n        # global scale & adaptation\n        self.gscale = float(gscale_init)  # relative to bound range (0.. range)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.min_gscale = float(min_gscale)\n        self.max_gscale = float(max_gscale)\n\n        # bookkeeping to be filled on run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object, else fallback to [-5,5]^dim.\n        Accepts attributes like func.bounds.lb / func.bounds.ub, func.lb, func.ub,\n        func.lower_bounds, func.upper_bounds, etc.\n        Returns lb, ub (numpy arrays).\n        \"\"\"\n        # default\n        default_lb = -5.0\n        default_ub = 5.0\n        # candidate attribute names to probe\n        cands = [\n            ('bounds', 'lb', 'ub'),\n            ('bounds', 'lower', 'upper'),\n            ('lb',), ('ub',),\n            ('lower_bounds',), ('upper_bounds',),\n            ('lower',), ('upper',)\n        ]\n\n        lb = None\n        ub = None\n\n        # helper to attempt read attribute(s)\n        def try_get(obj, name):\n            if obj is None:\n                return None\n            if hasattr(obj, name):\n                return getattr(obj, name)\n            if isinstance(obj, dict) and name in obj:\n                return obj[name]\n            return None\n\n        # try nested possibilities\n        for cand in cands:\n            if len(cand) == 3:\n                outer = try_get(func, cand[0])\n                if outer is not None:\n                    a = try_get(outer, cand[1])\n                    b = try_get(outer, cand[2])\n                    if a is not None and b is not None:\n                        lb = a; ub = b; break\n            else:\n                val = try_get(func, cand[0])\n                if val is not None:\n                    # ambiguous: if val is tuple/list of length 2, interpret as (lb,ub)\n                    if (isinstance(val, (list, tuple, np.ndarray)) and len(val) == 2):\n                        lb, ub = val[0], val[1]; break\n                    # otherwise check if this name is lb or ub assignment\n                    if cand[0] in ('lb', 'lower_bounds', 'lower', 'bounds'):\n                        lb = val\n                    elif cand[0] in ('ub', 'upper_bounds', 'upper'):\n                        ub = val\n                if lb is not None and ub is not None:\n                    break\n\n        # final fallback\n        if lb is None:\n            lb = default_lb\n        if ub is None:\n            ub = default_ub\n\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n\n        # if scalars, broadcast to dim\n        if lb_arr.ndim == 0:\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.ndim == 0:\n            ub_arr = np.full(self.dim, float(ub_arr))\n\n        # safety: if shapes mismatch, broadcast\n        if lb_arr.shape[0] != self.dim:\n            lb_arr = np.resize(lb_arr, self.dim)\n        if ub_arr.shape[0] != self.dim:\n            ub_arr = np.resize(ub_arr, self.dim)\n\n        # ensure lb < ub per-dim\n        for i in range(self.dim):\n            if lb_arr[i] >= ub_arr[i]:\n                # fallback on default for that dimension\n                lb_arr[i] = default_lb\n                ub_arr[i] = default_ub\n\n        return lb_arr, ub_arr\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect candidate x back into bounds along each axis.\n        Perform up to max_reflect reflections per dimension then clamp.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for i in range(self.dim):\n            tries = 0\n            while (x[i] < lb[i] or x[i] > ub[i]) and tries < max_reflect:\n                if x[i] < lb[i]:\n                    x[i] = lb[i] + (lb[i] - x[i])\n                elif x[i] > ub[i]:\n                    x[i] = ub[i] - (x[i] - ub[i])\n                tries += 1\n            # small nudge if still out\n            if x[i] < lb[i]:\n                x[i] = lb[i] + 1e-12\n            if x[i] > ub[i]:\n                x[i] = ub[i] - 1e-12\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.max(range_vec)\n        if range_norm <= 0:\n            range_norm = 10.0\n\n        # scale gscale in absolute units\n        self.gscale = float(self.gscale) * range_norm\n\n        # internal archive: list of (f,x); will maintain arrays for speed\n        xs = []\n        fs = []\n\n        evals = 0\n\n        # initial sampling (ensure at least 1)\n        n_init = min(self.init_pop, max(1, self.budget // 10))\n        n_init = max(1, n_init)\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(x.copy())\n            fs.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # success history for adaptation\n        recent_success = []\n        no_improve_iters = 0\n\n        # main loop propose-and-evaluate\n        while evals < self.budget:\n            # derive working arrays\n            xs_arr = np.array(xs)\n            fs_arr = np.array(fs)\n            # sort archive\n            order = np.argsort(fs_arr)\n            xs_sorted = xs_arr[order]\n            fs_sorted = fs_arr[order]\n\n            # elite\n            elite_k = max(1, int(np.ceil(len(xs) * self.elite_frac)))\n            elite_x = xs_sorted[:elite_k]\n            elite_f = fs_sorted[:elite_k]\n\n            # compute center of archive (for small pulls)\n            center = np.mean(xs_arr, axis=0)\n\n            # rank-biased base selection (exponential bias to best)\n            ranks = np.arange(1, len(xs) + 1)  # 1..N\n            # bias factor adaptive to archive size\n            beta = max(1.0, len(xs) / 5.0)\n            weights = np.exp(- (ranks - 1) / beta)\n            weights = weights / np.sum(weights)\n            chosen_pos = rng.choice(len(xs), p=weights)\n            base_idx = order[chosen_pos]  # index in original xs\n            base = np.array(xs[base_idx], dtype=float)\n\n            # adaptive per-dim base scale\n            per_dim_scale = np.abs(self.gscale * (1.0 + 0.15 * rng.randn(self.dim)))\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12 * range_norm)\n\n            # pick move type by probabilities (can be tuned)\n            p_local = 0.35\n            p_de = 0.25\n            p_pca = 0.20\n            p_cauchy = 0.15\n            p_mix = 0.05\n            move_choice = rng.rand()\n            candidate = None\n\n            # LOCAL anisotropic gaussian\n            if move_choice < p_local:\n                # anisotropic gaussian: multiplicative log-normal jitter * normal\n                jitter = np.exp(0.02 * rng.randn(self.dim))  # small multiplicative jitter\n                candidate = base + per_dim_scale * jitter * rng.randn(self.dim)\n                # small pull towards center\n                pull = 0.02 * (center - base)\n                candidate += pull\n\n            # DE-style donor + binomial crossover\n            elif move_choice < p_local + p_de:\n                N = len(xs)\n                if N >= 3:\n                    # pick three distinct indices different from base_idx\n                    idxs = list(range(N))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    # if insufficient variety, fallback to random points from bounds\n                    if len(idxs) >= 3:\n                        r = rng.choice(idxs, size=3, replace=False)\n                        x1 = xs[r[0]].astype(float)\n                        x2 = xs[r[1]].astype(float)\n                        x3 = xs[r[2]].astype(float)\n                        F = 0.6 + 0.4 * rng.rand()  # differential weight in [0.6,1.0]\n                        donor = x1 + F * (x2 - x3)\n                    else:\n                        donor = self._uniform_array(lb, ub)\n                else:\n                    donor = self._uniform_array(lb, ub)\n                # binomial crossover\n                CR = 0.9\n                mask = rng.rand(self.dim) < CR\n                # ensure at least one component from donor\n                if not np.any(mask):\n                    mask[rng.randint(self.dim)] = True\n                candidate = np.array(base, dtype=float)\n                candidate[mask] = donor[mask]\n                # slight local jitter\n                candidate += 0.05 * per_dim_scale * rng.randn(self.dim)\n\n            # PCA-guided sampling using elites\n            elif move_choice < p_local + p_de + p_pca:\n                # if not enough elites, fallback to gaussian around best\n                M = elite_x.shape[0]\n                if M >= 2:\n                    # compute covariance among elites and PCA\n                    mat = elite_x - np.mean(elite_x, axis=0)\n                    cov = np.cov(mat, rowvar=False)\n                    # regularize\n                    eps = 1e-8 * range_norm\n                    cov += np.eye(self.dim) * eps\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort descending\n                        idx = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[idx]\n                        eigvecs = eigvecs[:, idx]\n                        # sample coefficients along principal axes\n                        coeffs = rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0))\n                        # scale by global scale\n                        scale_fac = 0.8 + 0.4 * rng.rand()\n                        coeffs *= scale_fac * (self.gscale / (range_norm + 1e-12))\n                        candidate = np.mean(elite_x, axis=0) + eigvecs.dot(coeffs)\n                        # small isotropic jitter\n                        candidate += 0.02 * self.gscale * rng.randn(self.dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * rng.randn(self.dim)\n                else:\n                    candidate = base + per_dim_scale * rng.randn(self.dim)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            elif move_choice < p_local + p_de + p_pca + p_cauchy:\n                # choose between whole-scale jump from center or local cauchy around base\n                if rng.rand() < 0.35:\n                    # big global Cauchy from center\n                    cauchy_scale = 0.7 * range_norm\n                    candidate = center + cauchy_scale * rng.standard_cauchy(self.dim)\n                else:\n                    # anisotropic per-dim Cauchy around base\n                    cauchy_scale = 0.5 * per_dim_scale\n                    candidate = base + cauchy_scale * rng.standard_cauchy(self.dim)\n                # cap extremes\n                candidate = np.clip(candidate, lb - 10 * range_vec, ub + 10 * range_vec)\n\n            # rare mixture / archive mixing\n            else:\n                # small chance to take a recent archived point and jitter\n                if len(xs) > 0 and rng.rand() < 0.9:\n                    rec_idx = rng.randint(len(xs))\n                    candidate = np.array(xs[rec_idx], dtype=float) + 0.1 * per_dim_scale * rng.randn(self.dim)\n                else:\n                    candidate = self._uniform_array(lb, ub)\n\n            # safety: ensure finite\n            if not np.all(np.isfinite(candidate)):\n                candidate = self._uniform_array(lb, ub)\n\n            # reflect/correct to bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # small mixing with an archived recent point (diversify)\n            if len(xs) > 0 and rng.rand() < 0.06:\n                mix_idx = rng.randint(len(xs))\n                alpha = 0.1 * rng.rand()\n                candidate = (1 - alpha) * candidate + alpha * xs[mix_idx]\n                candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archive\n            xs.append(candidate.copy())\n            fs.append(f_cand)\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt:\n                improved = True\n                self.f_opt = f_cand\n                self.x_opt = candidate.copy()\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # update success record for adaptation:\n            # count a success if candidate is in top 25% of archive or improved best\n            threshold_rank = max(1, int(0.25 * len(xs)))\n            # get current rank\n            combined_fs = np.array(fs)\n            rank = np.sum(combined_fs <= f_cand)\n            recent_success.append(1 if (improved or rank <= threshold_rank) else 0)\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            # adapt gscale every adapt_window evaluations\n            if (evals % max(1, self.adapt_window // 4)) == 0 and len(recent_success) > 0:\n                succ_rate = np.mean(recent_success)\n                # if success above target, increase explore; else shrink\n                if succ_rate > self.target_success:\n                    self.gscale *= (1.0 + 0.08 * (succ_rate - self.target_success) + 0.02 * rng.rand())\n                else:\n                    self.gscale *= (1.0 - 0.06 * (self.target_success - succ_rate) - 0.01 * rng.rand())\n                # jitter & clamp\n                self.gscale *= (1.0 + 0.01 * rng.randn())\n                self.gscale = float(np.clip(self.gscale, self.min_gscale * range_norm, self.max_gscale * range_norm))\n\n            # stagnation detection & micro-restart\n            if no_improve_iters > max(200, 5 * self.dim):\n                # perform a few micro-injections (but don't exceed budget)\n                injections = min( max(1, self.dim // 2), (self.budget - evals) )\n                for _ in range(injections):\n                    inj_type = rng.rand()\n                    if inj_type < 0.5:\n                        xinj = self._uniform_array(lb, ub)\n                    elif inj_type < 0.85 and len(xs) > 0:\n                        # perturb a top elite\n                        elite_pick = rng.randint(max(1, elite_k))\n                        xinj = elite_x[elite_pick] + 0.5 * self.gscale * rng.randn(self.dim)\n                    else:\n                        # random PCA perturbation of a small neighborhood if possible\n                        if len(xs) >= 3:\n                            sample_idx = rng.choice(len(xs), size=min(6, len(xs)), replace=False)\n                            neighborhood = np.array([xs[i] for i in sample_idx])\n                            m = np.mean(neighborhood, axis=0)\n                            cov = np.cov((neighborhood - m), rowvar=False)\n                            cov += np.eye(self.dim) * 1e-8\n                            try:\n                                eigvals, eigvecs = np.linalg.eigh(cov)\n                                coeffs = rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0))\n                                xinj = m + 0.8 * (self.gscale / (range_norm + 1e-12)) * eigvecs.dot(coeffs)\n                            except Exception:\n                                xinj = self._uniform_array(lb, ub)\n                        else:\n                            xinj = self._uniform_array(lb, ub)\n                    xinj = self._reflect_bounds(xinj, lb, ub)\n                    if evals >= self.budget:\n                        break\n                    try:\n                        fj = float(func(xinj))\n                    except Exception:\n                        fj = np.inf\n                    evals += 1\n                    xs.append(xinj.copy()); fs.append(fj)\n                    if fj < self.f_opt:\n                        self.f_opt = fj; self.x_opt = xinj.copy()\n                        no_improve_iters = 0\n                # nudge gscale to encourage exploration after injection\n                self.gscale *= (1.0 + 0.25 * rng.rand())\n\n            # prune memory periodically to keep it compact\n            if len(xs) > self.max_archive:\n                # keep best 70% and random 30%\n                N = len(xs)\n                keep_best = int(0.7 * self.max_archive)\n                keep_random = self.max_archive - keep_best\n                fs_arr = np.array(fs)\n                order_all = np.argsort(fs_arr)\n                keep_idxs = list(order_all[:keep_best])\n                # random from the remaining\n                remaining = [i for i in range(N) if i not in keep_idxs]\n                if len(remaining) > 0 and keep_random > 0:\n                    randpick = list(rng.choice(remaining, size=min(len(remaining), keep_random), replace=False))\n                    keep_idxs += randpick\n                # build new lists\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n        # final safety: ensure we have at least one valid point\n        if self.x_opt is None:\n            if len(xs) > 0:\n                idx = int(np.argmin(np.array(fs)))\n                self.x_opt = np.array(xs[idx], dtype=float)\n                self.f_opt = float(fs[idx])\n            else:\n                # return center of bounds\n                xc = (lb + ub) / 2.0\n                try:\n                    fc = float(func(xc))\n                except Exception:\n                    fc = np.inf\n                self.x_opt = xc\n                self.f_opt = fc\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3ccc0bf9-b673-47fe-8e31-b3519647d562", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided Hybrid Adaptive Directional Sampling — a compact memory-driven hybrid sampler mixing DE-style donors, PCA-guided elite sampling, anisotropic local Gaussian moves and heavy-tailed Cauchy jumps with micro-restarts and short-term scale adaptation.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Memory-guided Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 gscale_init=0.3, adapt_window=50, target_success=0.2,\n                 min_gscale=1e-6, max_gscale=5.0,\n                 init_pop=None, max_archive=None, elite_frac=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population/archive settings\n        if init_pop is None:\n            self.init_pop = min(50, max(5, self.budget // 20))\n        else:\n            self.init_pop = int(init_pop)\n        if max_archive is None:\n            self.max_archive = max(200, 5 * self.init_pop)\n        else:\n            self.max_archive = int(max_archive)\n        self.elite_frac = float(elite_frac)\n\n        # global scale & adaptation (gscale is relative to search-range and later scaled)\n        self.gscale_init = float(gscale_init)\n        self.gscale = None\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.min_gscale = float(min_gscale)\n        self.max_gscale = float(max_gscale)\n\n        # bookkeeping to be filled on run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _try_get_attr(self, obj, name):\n        \"\"\"Try to read attribute or dict key, return None if not present.\"\"\"\n        if obj is None:\n            return None\n        # attribute\n        if hasattr(obj, name):\n            return getattr(obj, name)\n        # dictionary-like\n        try:\n            if isinstance(obj, dict) and name in obj:\n                return obj[name]\n        except Exception:\n            pass\n        return None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object, else fallback to [-5,5]^dim.\n        Accepts patterns: func.bounds.lb / func.bounds.ub, func.lb/func.ub, func.lower_bounds, func.upper_bounds.\n        Returns lb, ub (numpy arrays).\n        \"\"\"\n        default_lb = -5.0\n        default_ub = 5.0\n\n        lb = None\n        ub = None\n\n        # try common attribute patterns\n        # 1) func.bounds.lb / func.bounds.ub or func.bounds.lower / func.bounds.upper\n        bounds = self._try_get_attr(func, 'bounds')\n        if bounds is not None:\n            lb_c = self._try_get_attr(bounds, 'lb') or self._try_get_attr(bounds, 'lower')\n            ub_c = self._try_get_attr(bounds, 'ub') or self._try_get_attr(bounds, 'upper')\n            if lb_c is not None and ub_c is not None:\n                lb = lb_c; ub = ub_c\n\n        # 2) direct names\n        if lb is None:\n            for name in ('lower_bounds', 'lower', 'lb'):\n                val = self._try_get_attr(func, name)\n                if val is not None:\n                    lb = val; break\n        if ub is None:\n            for name in ('upper_bounds', 'upper', 'ub'):\n                val = self._try_get_attr(func, name)\n                if val is not None:\n                    ub = val; break\n\n        # if one is a pair (tuple/list) and the other is None, try to interpret\n        if lb is None and ub is None:\n            # maybe func.bounds returns (lb,ub) or func.bounds is a tuple\n            if isinstance(bounds, (tuple, list)) and len(bounds) == 2:\n                lb, ub = bounds[0], bounds[1]\n\n        # final fallback to scalars\n        if lb is None:\n            lb = default_lb\n        if ub is None:\n            ub = default_ub\n\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n\n        # scalars -> broadcast\n        if lb_arr.ndim == 0:\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.ndim == 0:\n            ub_arr = np.full(self.dim, float(ub_arr))\n\n        # if shapes mismatch, try to resize/broadcast to dim\n        if lb_arr.shape[0] != self.dim:\n            lb_arr = np.resize(lb_arr, self.dim)\n        if ub_arr.shape[0] != self.dim:\n            ub_arr = np.resize(ub_arr, self.dim)\n\n        # ensure lb < ub per-dim; fallback to defaults for offending dims\n        for i in range(self.dim):\n            if not np.isfinite(lb_arr[i]) or not np.isfinite(ub_arr[i]) or lb_arr[i] >= ub_arr[i]:\n                lb_arr[i] = default_lb\n                ub_arr[i] = default_ub\n\n        return lb_arr, ub_arr\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect candidate x back into bounds along each axis.\n        Perform up to max_reflect reflections per dimension then clamp.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for i in range(self.dim):\n            tries = 0\n            while (x[i] < lb[i] or x[i] > ub[i]) and tries < max_reflect:\n                if x[i] < lb[i]:\n                    x[i] = lb[i] + (lb[i] - x[i])\n                elif x[i] > ub[i]:\n                    x[i] = ub[i] - (x[i] - ub[i])\n                tries += 1\n            # if it is still outside after many reflections, clamp with a tiny nudge inside\n            if x[i] < lb[i]:\n                x[i] = lb[i] + 1e-12\n            if x[i] > ub[i]:\n                x[i] = ub[i] - 1e-12\n        # final clamp for safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        # rng.uniform supports array args that broadcast\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        rng = self.rng\n\n        # normalized range: use mean of per-dim lengths as normalization\n        per_dim_range = ub - lb\n        # avoid zero range\n        per_dim_range = np.where(per_dim_range <= 0, 1.0, per_dim_range)\n        range_norm = float(np.mean(per_dim_range))\n        if range_norm <= 0 or not np.isfinite(range_norm):\n            range_norm = 10.0\n\n        # initialize absolute gscale\n        self.gscale = float(self.gscale_init) * range_norm\n\n        # internal archive: lists of arrays and function values\n        xs = []\n        fs = []\n\n        evals = 0\n\n        # initial sampling\n        n_init = min(self.init_pop, max(1, self.budget // 10))\n        n_init = max(1, n_init)\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(x.copy())\n            fs.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # success history for adaptation\n        recent_success = []\n        no_improve_iters = 0\n\n        # move probabilities\n        p_local = 0.35\n        p_de = 0.25\n        p_pca = 0.20\n        p_cauchy = 0.15\n        p_mix = 0.05\n        cum_probs = np.cumsum([p_local, p_de, p_pca, p_cauchy, p_mix])\n        # differential crossover probability\n        CR = 0.9\n\n        # main loop propose-and-evaluate\n        while evals < self.budget:\n            # derive working arrays\n            xs_arr = np.array(xs)\n            fs_arr = np.array(fs)\n            # sort archive ascending\n            order = np.argsort(fs_arr)\n            xs_sorted = xs_arr[order]\n            fs_sorted = fs_arr[order]\n\n            # elite set\n            elite_k = max(1, int(np.ceil(len(xs) * self.elite_frac)))\n            elite_x = xs_sorted[:elite_k]\n\n            # compute center of archive\n            center = np.mean(xs_arr, axis=0)\n\n            # rank-biased base selection\n            N = len(xs)\n            ranks = np.arange(1, N + 1)  # 1..N (note: these correspond to unsorted index order below)\n            # We'll bias selection toward better individuals by sampling from sorted indices with exponential bias\n            beta = max(1.0, N / 5.0)\n            weights = np.exp(- (np.arange(N)) / beta)\n            weights = weights / np.sum(weights)\n            # choose index in sorted array, but then map to original\n            chosen_sorted_idx = rng.choice(N, p=weights)\n            base = xs_sorted[chosen_sorted_idx].astype(float)\n            # find base_idx in original xs list for DE exclusion decisions\n            # for speed, attempt direct match (works since elements are arrays); else approximate by searching closest\n            base_idx = None\n            for i, xi in enumerate(xs):\n                if np.allclose(xi, base, atol=1e-12):\n                    base_idx = i\n                    break\n            if base_idx is None:\n                # fallback: choose a random index\n                base_idx = rng.randint(N)\n\n            # adaptive per-dim base scale\n            per_dim_scale = np.abs(self.gscale * (1.0 + 0.15 * rng.randn(self.dim)))\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12 * range_norm)\n\n            # pick move type\n            move_choice = rng.rand()\n            candidate = None\n\n            # LOCAL anisotropic gaussian\n            if move_choice < cum_probs[0]:\n                # anisotropic gaussian: multiplicative log-normal jitter * normal\n                jitter = np.exp(0.02 * rng.randn(self.dim))  # multiplicative jitter\n                local_step = per_dim_scale * jitter * rng.randn(self.dim)\n                # small pull towards center\n                pull = 0.02 * (center - base)\n                candidate = base + pull + local_step\n\n            # DE-style donor + binomial crossover\n            elif move_choice < cum_probs[1]:\n                if N >= 3:\n                    # choose r1,r2 distinct from base_idx\n                    idxs = list(range(N))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    if len(idxs) >= 2:\n                        r = rng.choice(idxs, size=2, replace=False)\n                        x2 = xs[r[0]].astype(float)\n                        x3 = xs[r[1]].astype(float)\n                        x1 = base.copy()\n                        F = 0.6 + 0.4 * rng.rand()  # differential weight in [0.6,1.0]\n                        donor = x1 + F * (x2 - x3)\n                    else:\n                        donor = self._uniform_array(lb, ub)\n                else:\n                    donor = self._uniform_array(lb, ub)\n                # binomial crossover\n                mask = rng.rand(self.dim) < CR\n                if not np.any(mask):\n                    mask[rng.randint(self.dim)] = True\n                candidate = base.copy()\n                candidate[mask] = donor[mask]\n                # slight local jitter\n                candidate += 0.05 * per_dim_scale * rng.randn(self.dim)\n\n            # PCA-guided sampling using elites\n            elif move_choice < cum_probs[2]:\n                if elite_x.shape[0] >= 2:\n                    mat = elite_x - np.mean(elite_x, axis=0)\n                    # covariance with rowvar=False\n                    cov = np.cov(mat, rowvar=False)\n                    eps = 1e-8 * range_norm\n                    cov = cov + np.eye(self.dim) * eps\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort by descending eigenvalue\n                        idx = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[idx]\n                        eigvecs = eigvecs[:, idx]\n                        coeffs = rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0) + 1e-16)\n                        scale_fac = 0.8 + 0.4 * rng.rand()\n                        coeffs *= scale_fac * (self.gscale / (range_norm + 1e-12))\n                        candidate = np.mean(elite_x, axis=0) + eigvecs.dot(coeffs)\n                        candidate += 0.02 * self.gscale * rng.randn(self.dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * rng.randn(self.dim)\n                else:\n                    candidate = base + per_dim_scale * rng.randn(self.dim)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            elif move_choice < cum_probs[3]:\n                if rng.rand() < 0.35:\n                    # big global Cauchy from center\n                    cauchy_scale = 0.7 * range_norm\n                    # limit extremes\n                    raw = rng.standard_cauchy(self.dim)\n                    raw = np.clip(raw, -1e6, 1e6)\n                    candidate = center + cauchy_scale * raw\n                else:\n                    cauchy_scale = 0.5 * per_dim_scale\n                    raw = rng.standard_cauchy(self.dim)\n                    raw = np.clip(raw, -1e6, 1e6)\n                    candidate = base + cauchy_scale * raw\n                # cap extremes to avoid inf/nan\n                candidate = np.clip(candidate, lb - 10 * range_norm, ub + 10 * range_norm)\n\n            # rare mixture / archive mixing\n            else:\n                if len(xs) > 0 and rng.rand() < 0.9:\n                    rec_idx = rng.randint(len(xs))\n                    candidate = np.array(xs[rec_idx], dtype=float) + 0.1 * per_dim_scale * rng.randn(self.dim)\n                else:\n                    candidate = self._uniform_array(lb, ub)\n\n            # safety: ensure finite\n            if candidate is None or not np.all(np.isfinite(candidate)):\n                candidate = self._uniform_array(lb, ub)\n\n            # reflect/correct to bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # small mixing with an archived recent point (diversify)\n            if len(xs) > 0 and rng.rand() < 0.06:\n                mix_idx = rng.randint(len(xs))\n                alpha = 0.1 * rng.rand()\n                candidate = (1 - alpha) * candidate + alpha * xs[mix_idx]\n                candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archive\n            xs.append(candidate.copy())\n            fs.append(f_cand)\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt:\n                improved = True\n                self.f_opt = f_cand\n                self.x_opt = candidate.copy()\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # update success record for adaptation:\n            # count a success if candidate is in top 25% of archive or improved best\n            threshold_rank = max(1, int(0.25 * len(xs)))\n            combined_fs = np.array(fs)\n            # rank is number of elements strictly better or equal\n            rank = np.sum(combined_fs <= f_cand)\n            recent_success.append(1 if (improved or rank <= threshold_rank) else 0)\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            # adapt gscale periodically\n            adapt_step = max(1, self.adapt_window // 4)\n            if (evals % adapt_step) == 0 and len(recent_success) > 0:\n                succ_rate = np.mean(recent_success)\n                if succ_rate > self.target_success:\n                    self.gscale *= (1.0 + 0.08 * (succ_rate - self.target_success) + 0.02 * rng.rand())\n                else:\n                    self.gscale *= (1.0 - 0.06 * (self.target_success - succ_rate) - 0.01 * rng.rand())\n                # small jitter and clamp\n                self.gscale *= (1.0 + 0.01 * rng.randn())\n                self.gscale = float(np.clip(self.gscale, self.min_gscale * range_norm, self.max_gscale * range_norm))\n\n            # stagnation detection & micro-restart (inject a few candidates)\n            if no_improve_iters > max(200, 5 * self.dim):\n                injections = min(3, max(1, (no_improve_iters // (50 + 5 * self.dim))))\n                for _inj in range(injections):\n                    if evals >= self.budget:\n                        break\n                    inj_type = rng.rand()\n                    if inj_type < 0.5:\n                        xinj = self._uniform_array(lb, ub)\n                    elif inj_type < 0.85 and len(xs) > 0:\n                        # perturb a top elite\n                        elite_pick = rng.randint(max(1, elite_k))\n                        base_elite = elite_x[elite_pick - 1] if elite_pick - 1 >= 0 else elite_x[0]\n                        xinj = base_elite + 0.5 * self.gscale * rng.randn(self.dim)\n                        # try a small PCA neighborhood perturbation\n                        if len(xs) >= 3:\n                            sample_idx = rng.choice(len(xs), size=min(6, len(xs)), replace=False)\n                            neighborhood = np.array([xs[i] for i in sample_idx])\n                            m = np.mean(neighborhood, axis=0)\n                            cov = np.cov(neighborhood - m, rowvar=False)\n                            cov = cov + np.eye(self.dim) * 1e-8\n                            try:\n                                eigvals, eigvecs = np.linalg.eigh(cov)\n                                coeffs = rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0) + 1e-16)\n                                xinj = m + 0.8 * (self.gscale / (range_norm + 1e-12)) * eigvecs.dot(coeffs)\n                            except Exception:\n                                xinj = self._uniform_array(lb, ub)\n                    else:\n                        xinj = self._uniform_array(lb, ub)\n                    xinj = self._reflect_bounds(xinj, lb, ub)\n                    try:\n                        fj = float(func(xinj))\n                    except Exception:\n                        fj = np.inf\n                    evals += 1\n                    xs.append(xinj.copy()); fs.append(fj)\n                    if fj < self.f_opt:\n                        self.f_opt = fj; self.x_opt = xinj.copy()\n                        no_improve_iters = 0\n                # encourage exploration after injection\n                self.gscale *= (1.0 + 0.25 * rng.rand())\n                # reset some success history to avoid immediate repeated injections\n                recent_success = []\n\n            # prune memory periodically to keep it compact\n            if len(xs) > self.max_archive:\n                Ntot = len(xs)\n                keep_best = int(0.7 * self.max_archive)\n                keep_best = max(1, keep_best)\n                keep_random = self.max_archive - keep_best\n                fs_arr = np.array(fs)\n                order_all = np.argsort(fs_arr)\n                keep_idxs = list(order_all[:keep_best])\n                remaining = [i for i in range(Ntot) if i not in keep_idxs]\n                if len(remaining) > 0 and keep_random > 0:\n                    randpick = list(rng.choice(remaining, size=min(len(remaining), keep_random), replace=False))\n                    keep_idxs += randpick\n                # in case we still have less than required, pad with some of remaining\n                if len(keep_idxs) < self.max_archive:\n                    remaining2 = [i for i in range(Ntot) if i not in keep_idxs]\n                    if len(remaining2) > 0:\n                        addp = min(len(remaining2), self.max_archive - len(keep_idxs))\n                        keep_idxs += list(rng.choice(remaining2, size=addp, replace=False))\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n        # final safety: ensure we have at least one valid point\n        if self.x_opt is None:\n            if len(xs) > 0:\n                best_idx = int(np.argmin(np.array(fs)))\n                self.x_opt = np.array(xs[best_idx], dtype=float)\n                self.f_opt = float(fs[best_idx])\n            else:\n                xc = (lb + ub) / 2.0\n                try:\n                    fc = float(func(xc))\n                except Exception:\n                    fc = np.inf\n                self.x_opt = xc\n                self.f_opt = fc\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 78, in _extract_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_c = self._try_get_attr(bounds, 'lb') or self._try_get_attr(bounds, 'lower')", "error": "In the code, line 78, in _extract_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_c = self._try_get_attr(bounds, 'lb') or self._try_get_attr(bounds, 'lower')", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "dae90317-4f93-4e39-bf94-795f2fc8f0f0", "fitness": "-inf", "name": "MemoryHybridADS", "description": "Memory-guided Hybrid Adaptive Directional Sampler — a compact hybrid sampler mixing DE-style donors, PCA-guided elite proposals, anisotropic local Gaussians and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryHybridADS:\n    \"\"\"\n    Memory-guided Hybrid Adaptive Directional Sampler (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts\n    and short-term scale adaptation.\n\n    Usage:\n        sampler = MemoryHybridADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_pop=None, max_archive=None, elite_frac=0.20,\n                 target_success=0.25, gscale=0.08, min_gscale=1e-6, max_gscale=5.0,\n                 adapt_window=50):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population/archive settings\n        if init_pop is None:\n            init_pop = min(max(20, 4 * self.dim), max(1, self.budget // 10))\n        if max_archive is None:\n            max_archive = min(max(50, 10 * self.dim), max(10, self.budget))\n        self.init_pop = int(init_pop)\n        self.max_archive = int(max_archive)\n        self.elite_frac = float(elite_frac)\n\n        # global scale & adaptation\n        self.target_success = float(target_success)\n        self.gscale = float(gscale)        # relative initial scale (will be multiplied by range_norm)\n        self.min_gscale = float(min_gscale)\n        self.max_gscale = float(max_gscale)\n\n        self.adapt_window = int(adapt_window)\n\n        # bookkeeping\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object, else fallback to [-5,5]^dim.\n        Accepts a few common attribute names; otherwise returns scalars broadcasted.\n        \"\"\"\n        default_lb = -5.0\n        default_ub = 5.0\n\n        lb = None\n        ub = None\n\n        # try several known patterns\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # b may be dict-like, object-like or a tuple/list (lb,ub)\n                if isinstance(b, (list, tuple, np.ndarray)) and len(b) == 2:\n                    lb, ub = b[0], b[1]\n                else:\n                    # try object attributes\n                    if hasattr(b, \"lb\"):\n                        lb = b.lb\n                    if hasattr(b, \"ub\"):\n                        ub = b.ub\n                    if hasattr(b, \"lower\"):\n                        lb = getattr(b, \"lower\", lb)\n                    if hasattr(b, \"upper\"):\n                        ub = getattr(b, \"upper\", ub)\n        except Exception:\n            pass\n\n        # other possible attributes directly on func\n        if lb is None and hasattr(func, \"lb\"):\n            lb = func.lb\n        if ub is None and hasattr(func, \"ub\"):\n            ub = func.ub\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            lb = func.lower_bounds\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            ub = func.upper_bounds\n\n        if lb is None:\n            lb = default_lb\n        if ub is None:\n            ub = default_ub\n\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n\n        # broadcast scalars\n        if lb_arr.ndim == 0:\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.ndim == 0:\n            ub_arr = np.full(self.dim, float(ub_arr))\n\n        if lb_arr.size != self.dim:\n            lb_arr = np.resize(lb_arr, self.dim)\n        if ub_arr.size != self.dim:\n            ub_arr = np.resize(ub_arr, self.dim)\n\n        # ensure valid per-dim\n        for i in range(self.dim):\n            if not np.isfinite(lb_arr[i]) or not np.isfinite(ub_arr[i]) or lb_arr[i] >= ub_arr[i]:\n                lb_arr[i] = default_lb\n                ub_arr[i] = default_ub\n\n        return lb_arr, ub_arr\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=30):\n        x = np.array(x, dtype=float)\n        for i in range(self.dim):\n            tries = 0\n            while (x[i] < lb[i] or x[i] > ub[i]) and tries < max_reflect:\n                if x[i] < lb[i]:\n                    x[i] = lb[i] + (lb[i] - x[i])\n                elif x[i] > ub[i]:\n                    x[i] = ub[i] - (x[i] - ub[i])\n                tries += 1\n            # clamp residual tiny overshoot\n            if x[i] < lb[i]:\n                x[i] = lb[i] + 1e-12\n            if x[i] > ub[i]:\n                x[i] = ub[i] - 1e-12\n        # final clip\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        return self.rng.rand(self.dim) * (ub - lb) + lb\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        range_vec = ub - lb\n        range_norm = float(np.max(range_vec))\n        if range_norm <= 0 or not np.isfinite(range_norm):\n            range_norm = 10.0\n\n        # make gscale absolute\n        self.gscale = float(self.gscale) * range_norm\n        # clamp initial\n        self.gscale = float(np.clip(self.gscale, self.min_gscale * range_norm, self.max_gscale * range_norm))\n\n        xs = []   # list of np arrays\n        fs = []   # list of floats\n\n        evals = 0\n\n        # initialize with uniform samples\n        n_init = max(1, min(self.init_pop, self.budget))\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(x.copy())\n            fs.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if none valid yet, insert center\n        if len(xs) == 0:\n            x = (lb + ub) / 2.0\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            xs.append(x.copy()); fs.append(f); evals += 1\n            self.f_opt = f; self.x_opt = x.copy()\n\n        # adaptation / success window\n        adapt_window = max(4, int(self.adapt_window))\n        recent_success = [0] * adapt_window\n        no_improve_iters = 0\n\n        # hyperparameters for move probabilities\n        p_local = 0.30\n        p_de = 0.28\n        p_pca = 0.20\n        p_cauchy = 0.12\n        p_mix = 0.10\n        CR = 0.9  # crossover for DE-style\n\n        # main loop\n        while evals < self.budget:\n            N = len(xs)\n            xs_arr = np.array(xs)\n            fs_arr = np.array(fs)\n            order = np.argsort(fs_arr)  # ascending\n            center = np.mean(xs_arr, axis=0)\n\n            # rank-biased selection for base index\n            ranks = np.arange(1, N + 1)  # 1..N\n            beta = max(1.0, N / 5.0)\n            weights = np.exp(- (np.arange(N)) / beta)  # best has highest weight after sorting\n            weights = weights / np.sum(weights)\n            # choose position in sorted order, then map back\n            chosen_pos = np.searchsorted(np.cumsum(weights), self.rng.rand())\n            chosen_pos = int(np.clip(chosen_pos, 0, N - 1))\n            base_idx = int(order[chosen_pos])\n            base = xs_arr[base_idx].copy()\n\n            # elite subset\n            elite_k = max(1, int(np.ceil(N * self.elite_frac)))\n            elite_x = xs_arr[order[:elite_k]]\n\n            # adaptive per-dim scale (anisotropic)\n            per_dim_scale = np.abs(self.gscale * (1.0 + 0.15 * self.rng.randn(self.dim)))\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12 * range_norm)\n\n            # select move type\n            mv = self.rng.rand()\n            candidate = None\n\n            # LOCAL anisotropic gaussian\n            if mv < p_local:\n                jitter = np.exp(0.03 * self.rng.randn(self.dim))\n                candidate = base + per_dim_scale * jitter * self.rng.randn(self.dim)\n                # small pull towards center to encourage convergence\n                pull = 0.02 * (center - base)\n                candidate += pull\n\n            # DE-style donor + binomial crossover\n            elif mv < p_local + p_de:\n                if N >= 3:\n                    idxs = list(range(N))\n                    idxs.remove(base_idx)\n                    if len(idxs) >= 3:\n                        r = self.rng.choice(idxs, size=3, replace=False)\n                        x1 = xs[r[0]].astype(float)\n                        x2 = xs[r[1]].astype(float)\n                        x3 = xs[r[2]].astype(float)\n                        F = 0.5 + 0.5 * self.rng.rand()  # 0.5..1.0\n                        donor = x1 + F * (x2 - x3)\n                    else:\n                        donor = self._uniform_array(lb, ub)\n                else:\n                    donor = self._uniform_array(lb, ub)\n                # binomial crossover\n                mask = self.rng.rand(self.dim) < CR\n                if not np.any(mask):\n                    mask[self.rng.randint(self.dim)] = True\n                candidate = base.copy()\n                candidate[mask] = donor[mask]\n                # small local jitter\n                candidate += 0.02 * per_dim_scale * self.rng.randn(self.dim)\n\n            # PCA-guided sampling using elites\n            elif mv < p_local + p_de + p_pca:\n                if elite_x.shape[0] >= 2:\n                    mat = elite_x - np.mean(elite_x, axis=0)\n                    # covariance and regularize\n                    cov = np.cov(mat, rowvar=False) if mat.shape[0] > 1 else np.atleast_2d(np.var(mat, axis=0))\n                    eps = 1e-8 * range_norm\n                    cov = cov + np.eye(self.dim) * eps\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort descending\n                        idx = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[idx]\n                        eigvecs = eigvecs[:, idx]\n                        coeffs = self.rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0))\n                        scale_fac = 0.8 + 0.4 * self.rng.rand()\n                        candidate = np.mean(elite_x, axis=0) + eigvecs.dot(coeffs) * (scale_fac * (self.gscale / (range_norm + 1e-12)))\n                        candidate += 0.02 * self.gscale * self.rng.randn(self.dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * self.rng.randn(self.dim)\n                else:\n                    candidate = base + per_dim_scale * self.rng.randn(self.dim)\n\n            # heavy-tailed per-dim Cauchy jumps\n            elif mv < p_local + p_de + p_pca + p_cauchy:\n                if self.rng.rand() < 0.45:\n                    cauchy_scale = 0.6 * range_norm\n                    candidate = center + cauchy_scale * self.rng.standard_cauchy(self.dim)\n                else:\n                    cauchy_scale = 0.5 * per_dim_scale\n                    candidate = base + cauchy_scale * self.rng.standard_cauchy(self.dim)\n                # cap extremes to a few multiples of range to avoid NaNs/inf\n                cap = 20.0 * range_norm\n                candidate = np.clip(candidate, lb - cap, ub + cap)\n\n            # rare mixture / archive mixing\n            else:\n                if N > 0 and self.rng.rand() < 0.9:\n                    rec_idx = self.rng.randint(N)\n                    candidate = xs[rec_idx].copy() + 0.12 * per_dim_scale * self.rng.randn(self.dim)\n                else:\n                    candidate = self._uniform_array(lb, ub)\n\n            # safety: ensure finite\n            if candidate is None or not np.all(np.isfinite(candidate)):\n                candidate = self._uniform_array(lb, ub)\n\n            # reflect/correct to bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # small mixing with an archived random point (diversify)\n            if N > 0 and self.rng.rand() < 0.06:\n                mix_idx = self.rng.randint(N)\n                alpha = 0.08 * self.rng.rand()\n                candidate = (1.0 - alpha) * candidate + alpha * xs[mix_idx]\n                candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            xs.append(candidate.copy())\n            fs.append(f_cand)\n\n            improved = False\n            if f_cand < self.f_opt:\n                improved = True\n                self.f_opt = f_cand\n                self.x_opt = candidate.copy()\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # success: consider top 25% or improvement\n            success = 0\n            if len(fs) > 0:\n                thr = np.percentile(np.array(fs), 25)\n                if f_cand <= thr or improved:\n                    success = 1\n\n            # update recent_success sliding window\n            recent_success.pop(0)\n            recent_success.append(success)\n\n            # adapt gscale periodically\n            if evals % max(1, adapt_window // 4) == 0:\n                succ_rate = float(np.mean(recent_success))\n                if succ_rate > self.target_success:\n                    # increase exploration a bit (proportional)\n                    factor = 1.0 + 0.08 * (succ_rate - self.target_success) + 0.02 * self.rng.rand()\n                    self.gscale *= factor\n                else:\n                    # reduce scale slowly\n                    factor = 1.0 - 0.06 * (self.target_success - succ_rate)\n                    # small jitter\n                    factor *= (1.0 + 0.01 * self.rng.randn())\n                    self.gscale *= max(0.5, factor)\n                # clamp\n                self.gscale = float(np.clip(self.gscale, self.min_gscale * range_norm, self.max_gscale * range_norm))\n\n            # stagnation detection & micro-restart\n            stagn_thresh = max(200, 8 * self.dim)\n            if no_improve_iters > stagn_thresh and evals < self.budget:\n                injections = min(max(1, self.dim // 2), self.budget - evals)\n                for _ in range(injections):\n                    if evals >= self.budget:\n                        break\n                    itype = self.rng.rand()\n                    if itype < 0.5:\n                        xinj = self._uniform_array(lb, ub)\n                    elif itype < 0.85 and elite_x.shape[0] > 0:\n                        pick = int(self.rng.randint(max(1, elite_x.shape[0])))\n                        base_el = elite_x[pick % elite_x.shape[0]]\n                        xinj = base_el + 0.6 * self.gscale * self.rng.randn(self.dim)\n                    else:\n                        # random neighborhood PCA injection\n                        if N >= 3:\n                            sample_idx = self.rng.choice(N, size=min(6, N), replace=False)\n                            neighborhood = xs_arr[sample_idx]\n                            m = np.mean(neighborhood, axis=0)\n                            cov = np.cov(neighborhood - m, rowvar=False)\n                            cov = cov + np.eye(self.dim) * 1e-8\n                            try:\n                                eigvals, eigvecs = np.linalg.eigh(cov)\n                                coeffs = self.rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0))\n                                xinj = m + 0.8 * (self.gscale / (range_norm + 1e-12)) * eigvecs.dot(coeffs)\n                            except Exception:\n                                xinj = self._uniform_array(lb, ub)\n                        else:\n                            xinj = self._uniform_array(lb, ub)\n                    xinj = self._reflect_bounds(xinj, lb, ub)\n                    try:\n                        fj = float(func(xinj))\n                    except Exception:\n                        fj = np.inf\n                    evals += 1\n                    xs.append(xinj.copy()); fs.append(fj)\n                    if fj < self.f_opt:\n                        self.f_opt = fj; self.x_opt = xinj.copy()\n                # encourage more exploration afterwards\n                self.gscale *= (1.0 + 0.12 * self.rng.rand())\n                self.gscale = float(np.clip(self.gscale, self.min_gscale * range_norm, self.max_gscale * range_norm))\n                no_improve_iters = 0\n                recent_success = [0] * adapt_window\n\n            # prune memory periodically to keep it compact\n            if len(xs) > self.max_archive:\n                N = len(xs)\n                keep_best = int(0.7 * self.max_archive)\n                keep_random = self.max_archive - keep_best\n                order_all = np.argsort(np.array(fs))\n                keep_idxs = list(map(int, order_all[:keep_best]))\n                remaining = [i for i in range(N) if i not in keep_idxs]\n                if len(remaining) > 0 and keep_random > 0:\n                    randpick = list(self.rng.choice(remaining, size=min(len(remaining), keep_random), replace=False))\n                    keep_idxs += randpick\n                # rebuild lists\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n        # final safety: ensure we have at least one valid point\n        if self.x_opt is None:\n            if len(xs) > 0:\n                idx = int(np.argmin(np.array(fs)))\n                self.x_opt = np.array(xs[idx], dtype=float)\n                self.f_opt = float(fs[idx])\n            else:\n                xc = (lb + ub) / 2.0\n                try:\n                    fc = float(func(xc))\n                except Exception:\n                    fc = np.inf\n                self.x_opt = xc\n                self.f_opt = fc\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = MemoryHybridADS(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3a1f1360-c51d-47a9-be23-3e92c61ae297", "fitness": "-inf", "name": "AdaptiveDirectionalSampling", "description": "Memory-guided Hybrid Adaptive Directional Sampling — a compact memory-driven hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic local Gaussian moves and heavy-tailed Cauchy jumps with micro-restarts and short-term scale adaptation.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Memory-guided Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = AdaptiveDirectionalSampling(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_pop=None, max_archive=None, elite_frac=0.15,\n                 gscale_init=0.25, adapt_window=50, target_success=0.2,\n                 min_gscale=1e-6, max_gscale=5.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population/archive settings\n        if init_pop is None:\n            self.init_pop = max(5, min(100, self.budget // 50))\n        else:\n            self.init_pop = max(2, int(init_pop))\n        if max_archive is None:\n            self.max_archive = max(200, 10 * self.dim)\n        else:\n            self.max_archive = int(max_archive)\n        self.elite_frac = float(elite_frac)\n\n        # global scale & adaptation (gscale is in absolute units after seeing bounds)\n        self.gscale_init = float(gscale_init)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.min_gscale = float(min_gscale)\n        self.max_gscale = float(max_gscale)\n\n        # bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to extract bounds if provided on the function object; otherwise default to [-5,5]^dim.\n        Accepts simple attributes like func.lb / func.ub or func.bounds = (lb, ub) or dicts.\n        Returns lb, ub as np arrays of length self.dim.\n        \"\"\"\n        default_lb = -5.0\n        default_ub = 5.0\n\n        lb = None\n        ub = None\n\n        # try common attribute names\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # bounds could be tuple (lb, ub) or object with lb/ub\n            if isinstance(b, (list, tuple, np.ndarray)) and len(b) == 2:\n                lb, ub = b\n            elif hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = getattr(b, \"lb\")\n                ub = getattr(b, \"ub\")\n            elif isinstance(b, dict) and \"lb\" in b and \"ub\" in b:\n                lb = b[\"lb\"]; ub = b[\"ub\"]\n        if lb is None and hasattr(func, \"lb\"):\n            lb = getattr(func, \"lb\")\n        if ub is None and hasattr(func, \"ub\"):\n            ub = getattr(func, \"ub\")\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            lb = getattr(func, \"lower_bounds\")\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            ub = getattr(func, \"upper_bounds\")\n\n        # if any still None, fallback to scalars\n        if lb is None:\n            lb = default_lb\n        if ub is None:\n            ub = default_ub\n\n        lb_arr = np.array(lb, dtype=float)\n        ub_arr = np.array(ub, dtype=float)\n\n        # broadcast scalars\n        if lb_arr.shape == ():\n            lb_arr = np.full(self.dim, float(lb_arr))\n        if ub_arr.shape == ():\n            ub_arr = np.full(self.dim, float(ub_arr))\n\n        # resize if necessary\n        if lb_arr.size != self.dim:\n            lb_arr = np.resize(lb_arr, self.dim)\n        if ub_arr.size != self.dim:\n            ub_arr = np.resize(ub_arr, self.dim)\n\n        # sanity: ensure lb < ub componentwise\n        for i in range(self.dim):\n            if not np.isfinite(lb_arr[i]) or not np.isfinite(ub_arr[i]) or lb_arr[i] >= ub_arr[i]:\n                lb_arr[i] = default_lb\n                ub_arr[i] = default_ub\n\n        return lb_arr, ub_arr\n\n    def _reflect(self, x, lb, ub, max_reflect=20):\n        \"\"\"Reflect coordinates outside [lb,ub] back into the box (per-dim).\"\"\"\n        x = np.array(x, dtype=float)\n        for i in range(self.dim):\n            tries = 0\n            while (x[i] < lb[i] or x[i] > ub[i]) and tries < max_reflect:\n                if x[i] < lb[i]:\n                    x[i] = lb[i] + (lb[i] - x[i])  # reflect\n                elif x[i] > ub[i]:\n                    x[i] = ub[i] - (x[i] - ub[i])\n                tries += 1\n            # clamp tiny remainder\n            if x[i] < lb[i]:\n                x[i] = lb[i] + 1e-12\n            if x[i] > ub[i]:\n                x[i] = ub[i] - 1e-12\n        # final clamp to be safe\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def _uniform(self, lb, ub):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        return lb + (ub - lb) * self.rng.rand(self.dim)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        range_norm = float(np.max(range_vec))\n        # initialize absolute gscale\n        self.gscale = max(self.min_gscale * range_norm, min(self.max_gscale * range_norm, self.gscale_init * range_norm))\n\n        rng = self.rng\n        eps = 1e-8\n\n        xs = []\n        fs = []\n        evals = 0\n        no_improve_iters = 0\n        recent_success = []\n\n        # initial sampling\n        n_init = min(self.init_pop, max(1, self.budget // 10))\n        n_init = max(1, n_init)\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            x = self._uniform(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(x.copy())\n            fs.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n                no_improve_iters = 0\n\n        # main loop\n        while evals < self.budget:\n            N = len(xs)\n            if N == 0:\n                # sample uniformly if archive empty\n                candidate = self._uniform(lb, ub)\n            else:\n                xs_arr = np.array(xs)\n                fs_arr = np.array(fs)\n                order = np.argsort(fs_arr)\n                xs_sorted = xs_arr[order]\n                fs_sorted = fs_arr[order]\n\n                # elite set\n                elite_k = max(1, int(np.ceil(self.elite_frac * N)))\n                elite_x = xs_sorted[:elite_k]\n                elite_f = fs_sorted[:elite_k]\n\n                # center of archive\n                center = np.mean(xs_arr, axis=0)\n\n                # rank-biased selection of base (exponential bias)\n                beta = max(1.0, N / 6.0)\n                ranks = np.arange(1, N + 1)\n                weights = np.exp(- (ranks - 1) / beta)\n                weights = weights / np.sum(weights)\n                chosen_in_sorted = rng.choice(len(xs), p=weights)\n                base = xs_sorted[chosen_in_sorted].astype(float)\n                # determine base_idx in original array (rarely needed)\n                # base_idx = order[chosen_in_sorted]\n\n                # adaptive per-dim base scale\n                per_dim_scale = np.abs(self.gscale * (1.0 + 0.15 * rng.randn(self.dim))) + 1e-12\n\n                # choose move type\n                p_local = 0.25\n                p_de = 0.25\n                p_pca = 0.20\n                p_cauchy = 0.15\n                move_choice = rng.rand()\n                candidate = None\n\n                # LOCAL anisotropic gaussian\n                if move_choice < p_local:\n                    jitter = np.exp(0.02 * rng.randn(self.dim))\n                    candidate = base + per_dim_scale * jitter * rng.randn(self.dim)\n                    # small pull towards center\n                    candidate += 0.02 * (center - base)\n\n                # DE-style donor + binomial crossover\n                elif move_choice < p_local + p_de:\n                    if N >= 4:\n                        idxs = list(range(N))\n                        # choose three distinct indices from xs_sorted (to prefer better points)\n                        r = rng.choice(idxs, size=3, replace=False)\n                        x1 = xs_arr[r[0]].astype(float)\n                        x2 = xs_arr[r[1]].astype(float)\n                        x3 = xs_arr[r[2]].astype(float)\n                        F = 0.5 + 0.5 * rng.rand()  # differential weight in [0.5,1.0)\n                        donor = x1 + F * (x2 - x3)\n                    else:\n                        donor = self._uniform(lb, ub)\n                    # binomial crossover with base\n                    CR = 0.9\n                    mask = rng.rand(self.dim) < CR\n                    if not np.any(mask):\n                        mask[rng.randint(self.dim)] = True\n                    candidate = np.array(base, dtype=float)\n                    candidate[mask] = donor[mask]\n                    # small jitter\n                    candidate += 0.02 * per_dim_scale * rng.randn(self.dim)\n\n                # PCA-guided sampling using elites\n                elif move_choice < p_local + p_de + p_pca:\n                    M = elite_x.shape[0]\n                    if M >= 2:\n                        mat = elite_x - np.mean(elite_x, axis=0)\n                        cov = np.cov(mat, rowvar=False)\n                        # regularize\n                        cov = cov + np.eye(self.dim) * eps\n                        try:\n                            eigvals, eigvecs = np.linalg.eigh(cov)\n                            # sort descending\n                            idx = np.argsort(eigvals)[::-1]\n                            eigvals = eigvals[idx]\n                            eigvecs = eigvecs[:, idx]\n                            # sample along principal axes with scale proportional to eigenvalues\n                            coeffs = rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0) + eps)\n                            scale_fac = 0.6 + 0.8 * rng.rand()\n                            candidate = np.mean(elite_x, axis=0) + scale_fac * (eigvecs.dot(coeffs))\n                            candidate += 0.02 * self.gscale * rng.randn(self.dim)\n                        except Exception:\n                            candidate = base + per_dim_scale * rng.randn(self.dim)\n                    else:\n                        candidate = base + per_dim_scale * rng.randn(self.dim)\n\n                # heavy-tailed per-dim Cauchy jumps (Levy-like)\n                elif move_choice < p_local + p_de + p_pca + p_cauchy:\n                    if rng.rand() < 0.5:\n                        # big global jump from center\n                        cauchy_scale = 0.6 * range_norm + 1e-12\n                        candidate = center + cauchy_scale * rng.standard_cauchy(self.dim)\n                    else:\n                        # anisotropic local Cauchy around base\n                        cauchy_scale = 0.5 * per_dim_scale\n                        candidate = base + cauchy_scale * rng.standard_cauchy(self.dim)\n                    # clip extreme outliers to a reasonable multiple of range\n                    cap = 50.0 * (range_norm + 1e-12)\n                    candidate = np.clip(candidate, lb - cap, ub + cap)\n\n                # rare mixture / uniform injection\n                else:\n                    if rng.rand() < 0.7 and N > 0:\n                        # jitter a random archived point\n                        idx = rng.randint(N)\n                        candidate = xs[idx].astype(float) + 0.5 * per_dim_scale * rng.randn(self.dim)\n                    else:\n                        candidate = self._uniform(lb, ub)\n\n            # safety: finite\n            if not np.all(np.isfinite(candidate)):\n                candidate = self._uniform(lb, ub)\n\n            # small mixing with an archived recent point (diversify)\n            if len(xs) > 0 and rng.rand() < 0.06:\n                mix_idx = rng.randint(len(xs))\n                alpha = 0.15 * rng.rand()\n                candidate = (1 - alpha) * candidate + alpha * xs[mix_idx]\n\n            # reflect/correct to bounds\n            candidate = self._reflect(candidate, lb, ub)\n\n            # evaluate\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archive\n            xs.append(candidate.copy())\n            fs.append(f_cand)\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = candidate.copy()\n                no_improve_iters = 0\n                improved = True\n            else:\n                no_improve_iters += 1\n\n            # success record (top 25% or improved)\n            N = len(xs)\n            threshold_rank = max(1, int(0.25 * N))\n            combined_fs = np.array(fs)\n            # rank of f_cand among current archive (1 = best)\n            rank = int(np.sum(combined_fs <= f_cand))\n            success = 1 if (improved or rank <= threshold_rank) else 0\n            recent_success.append(success)\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            # adapt gscale periodically (short-term adaption)\n            if len(recent_success) > 0 and (evals % max(1, (self.adapt_window // 4))) == 0:\n                succ_rate = float(np.mean(recent_success))\n                if succ_rate > self.target_success:\n                    # increase exploration slowly\n                    factor = 1.0 + 0.08 * (succ_rate - self.target_success)\n                    self.gscale *= factor\n                else:\n                    # decrease to focus exploitation\n                    factor = 1.0 - 0.06 * (self.target_success - succ_rate) - 0.01 * rng.rand()\n                    self.gscale *= max(0.5, factor)\n                # jitter & clamp in absolute units\n                self.gscale *= (1.0 + 0.02 * rng.randn())\n                self.gscale = float(np.clip(self.gscale, self.min_gscale * range_norm, self.max_gscale * range_norm))\n\n            # stagnation detection & micro-restart injections\n            if no_improve_iters > max(200, 8 * self.dim) and evals < self.budget:\n                injections = min(8, self.budget - evals)\n                for _inj in range(injections):\n                    if rng.rand() < 0.5:\n                        # uniform injection\n                        xinj = self._uniform(lb, ub)\n                    else:\n                        # perturb a top elite\n                        pick = rng.randint(max(1, elite_k))\n                        base_inj = elite_x[pick - 1] if pick - 1 < elite_x.shape[0] else xs_arr[rng.randint(N)]\n                        # small PCA around neighborhood if possible\n                        if N >= 4:\n                            nb_idx = rng.choice(N, size=min(6, N), replace=False)\n                            neighborhood = xs_arr[nb_idx]\n                            m = np.mean(neighborhood, axis=0)\n                            cov = np.cov((neighborhood - m), rowvar=False) + np.eye(self.dim) * 1e-8\n                            try:\n                                eigvals, eigvecs = np.linalg.eigh(cov)\n                                coeffs = rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0))\n                                xinj = m + 0.8 * (self.gscale / (range_norm + 1e-12)) * eigvecs.dot(coeffs)\n                            except Exception:\n                                xinj = base_inj + 0.5 * per_dim_scale * rng.randn(self.dim)\n                        else:\n                            xinj = base_inj + 0.5 * per_dim_scale * rng.randn(self.dim)\n                    xinj = self._reflect(xinj, lb, ub)\n                    if evals >= self.budget:\n                        break\n                    try:\n                        fj = float(func(xinj))\n                    except Exception:\n                        fj = np.inf\n                    evals += 1\n                    xs.append(xinj.copy()); fs.append(fj)\n                    if fj < self.f_opt:\n                        self.f_opt = fj\n                        self.x_opt = xinj.copy()\n                        no_improve_iters = 0\n                # encourage exploration after injections\n                self.gscale = float(np.clip(1.2 * self.gscale, self.min_gscale * range_norm, self.max_gscale * range_norm))\n                recent_success = []\n\n            # prune memory periodically to keep it compact\n            if len(xs) > self.max_archive:\n                fs_arr = np.array(fs)\n                order_all = np.argsort(fs_arr)\n                keep_best = int(0.7 * self.max_archive)\n                keep_random = self.max_archive - keep_best\n                keep_idxs = list(order_all[:keep_best])\n                remaining = list(order_all[keep_best:])\n                if len(remaining) > 0 and keep_random > 0:\n                    randpick = list(rng.choice(remaining, size=min(len(remaining), keep_random), replace=False))\n                    keep_idxs += randpick\n                # build new lists preserving order\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n        # final safety: if nothing evaluated successfully set to center\n        if self.x_opt is None:\n            try:\n                xc = 0.5 * (lb + ub)\n                fc = float(func(xc))\n                self.x_opt = xc\n                self.f_opt = fc\n            except Exception:\n                # last resort: return best found in archive if any\n                if len(xs) > 0:\n                    best_i = int(np.argmin(fs))\n                    self.x_opt = xs[best_i].copy()\n                    self.f_opt = float(fs[best_i])\n                else:\n                    self.x_opt = np.full(self.dim, 0.0)\n                    self.f_opt = np.inf\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example quick test (uncomment to run)\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = AdaptiveDirectionalSampling(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 365, in __call__, the following error occurred:\nIndexError: index 200 is out of bounds for axis 0 with size 200\nOn line: neighborhood = xs_arr[nb_idx]", "error": "In the code, line 365, in __call__, the following error occurred:\nIndexError: index 200 is out of bounds for axis 0 with size 200\nOn line: neighborhood = xs_arr[nb_idx]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6103d6ae-ba42-47ce-b84f-e16ecf63d70c", "fitness": "-inf", "name": "MemoryHybridADS", "description": "Memory-guided Hybrid Adaptive Directional Sampling — a compact memory-driven hybrid sampler that mixes DE-style donors, PCA-guided elite sampling, anisotropic local Gaussian moves and heavy-tailed Cauchy jumps with micro-restarts and short-term scale adaptation.", "code": "import numpy as np\n\nclass MemoryHybridADS:\n    \"\"\"\n    Memory-guided Hybrid Adaptive Directional Sampling (compact implementation).\n\n    One-line: A compact memory-driven hybrid sampler combining DE-like donors,\n    PCA-guided elite sampling, anisotropic local Gaussian moves and heavy-tailed\n    Cauchy jumps with short-term scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryHybridADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_pop=20, max_archive=300, elite_frac=0.2,\n                 gscale_init=0.3, adapt_window=50, target_success=0.2,\n                 min_gscale=1e-6, max_gscale=5.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.init_pop = int(init_pop)\n        self.max_archive = int(max_archive)\n        self.elite_frac = float(elite_frac)\n        self.gscale = float(gscale_init)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.min_gscale = float(min_gscale)\n        self.max_gscale = float(max_gscale)\n\n        # bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # RNG will be created per-run\n        self.rng = None\n\n    def _get_bounds(self, func):\n        # Try to extract common bound attributes; fallback to [-5,5]^dim\n        lb = None; ub = None\n        # helper to coerce scalar/sequence to array\n        def to_arr(v):\n            v = np.asarray(v, dtype=float)\n            if v.ndim == 0:\n                return np.full(self.dim, float(v))\n            if v.shape[0] != self.dim:\n                return np.resize(v, self.dim)\n            return v\n\n        # candidate attribute patterns\n        try_attrs = [\n            (\"bounds\", \"lb\", \"ub\"),\n            (\"bounds\", \"lower\", \"upper\"),\n            (\"lower_bounds\", None, None),\n            (\"upper_bounds\", None, None),\n            (\"lb\", None, None),\n            (\"ub\", None, None),\n            (\"lower\", None, None),\n            (\"upper\", None, None)\n        ]\n        # If func provides a dict-like attribute bounds or simple attributes\n        for pat in try_attrs:\n            outer = getattr(func, pat[0], None) if pat[1] is None else getattr(func, pat[0], None)\n            try:\n                if pat[1] is not None and outer is not None:\n                    a = getattr(outer, pat[1], None) if hasattr(outer, pat[1]) else outer.get(pat[1]) if isinstance(outer, dict) and pat[1] in outer else None\n                    b = getattr(outer, pat[2], None) if hasattr(outer, pat[2]) else outer.get(pat[2]) if isinstance(outer, dict) and pat[2] in outer else None\n                    if a is not None and b is not None:\n                        lb = to_arr(a); ub = to_arr(b); break\n                else:\n                    val = getattr(func, pat[0], None)\n                    if val is None:\n                        continue\n                    # dict-like\n                    if isinstance(val, dict) and \"lb\" in val and \"ub\" in val:\n                        lb = to_arr(val[\"lb\"]); ub = to_arr(val[\"ub\"]); break\n                    # tuple/list (lb, ub)\n                    if isinstance(val, (list, tuple, np.ndarray)) and len(val) == 2:\n                        lb = to_arr(val[0]); ub = to_arr(val[1]); break\n                    # single scalar attribute interpreted as both sides? skip\n            except Exception:\n                pass\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return np.array(lb, dtype=float), np.array(ub, dtype=float)\n\n    def _reflect_into_bounds(self, x, lb, ub):\n        # robust reflection using modular reflection trick, then clamp tiny numeric overshoot\n        x = np.array(x, dtype=float)\n        span = ub - lb\n        # avoid zero spans\n        span = np.maximum(span, 1e-12)\n        # map to [0, 2*span) relative to lb\n        y = (x - lb) % (2.0 * span)\n        # fold second half back\n        mask = y > span\n        y[mask] = 2.0 * span[mask] - y[mask]\n        x_ref = lb + y\n        # final clamp\n        x_ref = np.minimum(np.maximum(x_ref, lb), ub)\n        return x_ref\n\n    def _uniform(self, lb, ub):\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        # initialize RNG\n        self.rng = np.random.RandomState(self.seed)\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.any(range_vec != 0.0) else 1.0\n\n        # internal memory\n        xs = []\n        fs = []\n        evals = 0\n\n        # initial sampling\n        n_init = min(self.init_pop, max(1, self.budget // 10))\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(np.array(x, dtype=float))\n            fs.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f); self.x_opt = np.array(x, dtype=float)\n\n        # adaptation helpers\n        recent_success = []\n        no_improve_iters = 0\n        adapt_counter = 0\n\n        # main loop\n        while evals < self.budget:\n            N = len(xs)\n            if N == 0:\n                # safety: random sample\n                candidate = self._uniform(lb, ub)\n                try:\n                    f_cand = float(func(candidate))\n                except Exception:\n                    f_cand = np.inf\n                evals += 1\n                xs.append(np.array(candidate, dtype=float)); fs.append(f_cand)\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand; self.x_opt = np.array(candidate, dtype=float)\n                continue\n\n            xs_arr = np.array(xs)\n            fs_arr = np.array(fs)\n            order = np.argsort(fs_arr)\n            best_idx = order[0]\n            worst_idx = order[-1]\n            center = np.mean(xs_arr, axis=0)\n\n            # rank-biased selection for base\n            ranks = np.empty(N, dtype=float)\n            ranks[order] = np.arange(1, N+1)  # 1..N for sorted (best=1)\n            beta = max(1.0, N / 5.0)\n            weights = np.exp(-(ranks - 1) / beta)\n            weights = weights / np.sum(weights)\n            base_idx = self.rng.choice(N, p=weights)\n            base = xs_arr[base_idx].astype(float)\n\n            # probabilities for move types\n            p_local = 0.35\n            p_de = 0.25\n            p_pca = 0.20\n            p_cauchy = 0.15\n            # rem = 0.05 reserved for archive jitter / micro\n            move_r = self.rng.rand()\n\n            # prepare per-dim scale (adaptive)\n            per_dim_scale = np.maximum(range_vec * 0.02, 1e-8) * (self.gscale * (1.0 / (range_norm + 1e-12)))\n\n            candidate = None\n\n            # LOCAL anisotropic gaussian (small moves)\n            if move_r < p_local:\n                # anisotropic gaussian jitter around base with slight pull toward mean\n                pull = 0.02 * (center - base)\n                jitter = per_dim_scale * self.rng.randn(self.dim)\n                # multiplicative log-normal factor to create anisotropy\n                logfac = np.exp(0.1 * self.rng.randn(self.dim))\n                candidate = base + pull + jitter * logfac\n\n            # DE-style donor + binomial crossover\n            elif move_r < p_local + p_de:\n                if N >= 3:\n                    # pick three distinct indices different from base_idx\n                    idxs = list(range(N))\n                    idxs.remove(base_idx)\n                    r = self.rng.choice(idxs, size=3, replace=False)\n                    x1 = xs_arr[r[0]]\n                    x2 = xs_arr[r[1]]\n                    x3 = xs_arr[r[2]]\n                    F = 0.5 + 0.5 * self.rng.rand()  # differential weight in [0.5,1.0]\n                    donor = x1 + F * (x2 - x3)\n                    # binomial crossover\n                    CR = 0.9\n                    mask = self.rng.rand(self.dim) < CR\n                    # ensure at least one component from donor\n                    if not np.any(mask):\n                        mask[self.rng.randint(self.dim)] = True\n                    candidate = base.copy()\n                    candidate[mask] = donor[mask]\n                    # slight local jitter\n                    candidate += 0.02 * per_dim_scale * self.rng.randn(self.dim)\n                else:\n                    # fallback to local gaussian\n                    candidate = base + per_dim_scale * self.rng.randn(self.dim)\n\n            # PCA-guided sampling using elites\n            elif move_r < p_local + p_de + p_pca:\n                M = max(1, int(np.ceil(N * self.elite_frac)))\n                elite_idx = order[:M]\n                elite_x = xs_arr[elite_idx]\n                if elite_x.shape[0] >= 2:\n                    m = np.mean(elite_x, axis=0)\n                    cov = np.cov(elite_x - m, rowvar=False)\n                    # regularize\n                    cov += np.eye(self.dim) * (1e-8 * (range_norm + 1.0))\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sample along principal components with magnitudes proportional to sqrt(eigvals)\n                        coeffs = self.rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0))\n                        scale_fac = 0.6 + 0.8 * self.rng.rand()\n                        coeffs *= scale_fac * (self.gscale / (range_norm + 1e-12))\n                        candidate = m + eigvecs.dot(coeffs)\n                        candidate += 0.02 * self.gscale * self.rng.randn(self.dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * self.rng.randn(self.dim)\n                else:\n                    candidate = base + per_dim_scale * self.rng.randn(self.dim)\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            elif move_r < p_local + p_de + p_pca + p_cauchy:\n                # choose between global big jump from center or local cauchy around base\n                if self.rng.rand() < 0.4:\n                    cauchy_scale = 0.7 * range_norm\n                    # sample from multivariate independent cauchy\n                    cand = self.rng.standard_cauchy(size=self.dim) * (cauchy_scale / (range_norm + 1e-12))\n                    candidate = center + cand\n                else:\n                    # anisotropic per-dim Cauchy around base\n                    scales = np.maximum(per_dim_scale, 1e-8) * (1.0 + 0.5 * self.rng.rand(self.dim))\n                    candidate = base + self.rng.standard_cauchy(size=self.dim) * scales\n                # cap extremes to avoid inf/nan\n                large_cap = 10.0 * range_vec\n                candidate = np.clip(candidate, lb - large_cap, ub + large_cap)\n\n            # archive mixing / jitter / fallback\n            else:\n                if N > 0 and self.rng.rand() < 0.9:\n                    rec_idx = self.rng.randint(N)\n                    candidate = xs_arr[rec_idx] + 0.1 * per_dim_scale * self.rng.randn(self.dim)\n                else:\n                    # pure random injection\n                    candidate = self._uniform(lb, ub)\n\n            # safety: if candidate is None for any reason, sample uniformly\n            if candidate is None:\n                candidate = self._uniform(lb, ub)\n\n            # reflect into bounds\n            candidate = self._reflect_into_bounds(candidate, lb, ub)\n\n            # small mixing with an archived recent point to diversify\n            if N > 0 and self.rng.rand() < 0.06:\n                mix_idx = self.rng.randint(N)\n                alpha = 0.1 * self.rng.rand()\n                candidate = self._reflect_into_bounds((1 - alpha) * candidate + alpha * xs_arr[mix_idx], lb, ub)\n\n            # evaluate candidate (respect budget)\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archive\n            xs.append(np.array(candidate, dtype=float))\n            fs.append(f_cand)\n\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(candidate, dtype=float)\n                improved = True\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # success = improved or in top 25% of current archive\n            combined_fs = np.array(fs)\n            threshold_rank = max(1, int(np.ceil(0.25 * len(combined_fs))))\n            rank = np.sum(combined_fs <= f_cand)\n            success = bool(improved or (rank <= threshold_rank))\n            recent_success.append(1 if success else 0)\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            # adapt global gscale periodically\n            adapt_counter += 1\n            if adapt_counter >= max(1, self.adapt_window // 4):\n                adapt_counter = 0\n                if len(recent_success) > 0:\n                    succ_rate = float(np.mean(recent_success))\n                    if succ_rate > self.target_success:\n                        # encourage exploration\n                        self.gscale *= 1.12\n                    else:\n                        # shrink scale to fine-tune\n                        self.gscale *= 0.88\n                    # clamp\n                    self.gscale = min(max(self.gscale, self.min_gscale), self.max_gscale)\n\n            # micro-restart / injection if stagnation\n            if no_improve_iters > max(200, 5 * self.dim):\n                injections = min(20, max(5, self.dim))\n                for _ in range(injections):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.rand() < 0.6:\n                        xinj = self._uniform(lb, ub)\n                    else:\n                        # perturb a random elite\n                        if N >= 2:\n                            pick = order[self.rng.randint(min(len(order), max(1, int(0.2 * len(order)))))]\n                            xinj = xs_arr[pick] + 0.5 * self.gscale * per_dim_scale * self.rng.randn(self.dim)\n                            xinj = self._reflect_into_bounds(xinj, lb, ub)\n                        else:\n                            xinj = self._uniform(lb, ub)\n                    try:\n                        fj = float(func(xinj))\n                    except Exception:\n                        fj = np.inf\n                    evals += 1\n                    xs.append(np.array(xinj, dtype=float)); fs.append(fj)\n                    if fj < self.f_opt:\n                        self.f_opt = float(fj); self.x_opt = np.array(xinj, dtype=float)\n                        no_improve_iters = 0\n                # encourage exploration after injection\n                self.gscale = min(self.gscale * 1.5, self.max_gscale)\n                no_improve_iters = 0\n\n            # prune memory periodically\n            if len(xs) > self.max_archive:\n                fs_arr = np.array(fs)\n                order_all = np.argsort(fs_arr)\n                keep_best = int(0.7 * self.max_archive)\n                keep_random = self.max_archive - keep_best\n                keep_idxs = list(order_all[:keep_best])\n                remaining = list(order_all[keep_best:])\n                if len(remaining) > 0 and keep_random > 0:\n                    randpick = list(self.rng.choice(remaining, size=min(len(remaining), keep_random), replace=False))\n                    keep_idxs += randpick\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n        # final safety: ensure some valid best exists\n        if self.x_opt is None:\n            if len(xs) > 0:\n                # pick best from archive\n                fs_arr = np.array(fs)\n                best_i = int(np.argmin(fs_arr))\n                self.x_opt = np.array(xs[best_i], dtype=float)\n                self.f_opt = float(fs[best_i])\n            else:\n                # return center of bounds\n                xc = 0.5 * (lb + ub)\n                try:\n                    self.f_opt = float(func(xc))\n                    self.x_opt = np.array(xc, dtype=float)\n                except Exception:\n                    self.f_opt = np.inf\n                    self.x_opt = np.array(xc, dtype=float)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = MemoryHybridADS(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "65ec648a-1978-4aa3-9423-377ed7cd1247", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler mixing DE-style donors, PCA-guided elite sampling, anisotropic local Gaussians and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_pop=None, max_archive=None, elite_frac=0.2,\n                 gscale_init=0.3, adapt_window=50, target_success=0.2,\n                 min_gscale=1e-6, max_gscale=5.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # initialization sizes\n        if init_pop is None:\n            self.init_pop = max(6, min(100, 10 + self.dim))\n        else:\n            self.init_pop = int(init_pop)\n        if max_archive is None:\n            self.max_archive = max(200, 10 * self.dim)\n        else:\n            self.max_archive = int(max_archive)\n\n        self.elite_frac = float(elite_frac)\n\n        # global scale parameters (relative to coordinate range)\n        self.gscale_init = float(gscale_init)\n        self.gscale = float(gscale_init)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.min_gscale = float(min_gscale)\n        self.max_gscale = float(max_gscale)\n\n        # bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        \"\"\"\n        Try to extract bounds from the function object, otherwise fallback to [-5,5]^dim.\n        Accept common attribute names: func.bounds.lb/ub, func.lb/ub, func.lower_bounds/upper_bounds.\n        Returns lb, ub as numpy arrays of length self.dim.\n        \"\"\"\n        default_lb = -5.0\n        default_ub = 5.0\n\n        lb = None\n        ub = None\n\n        # try nested bound structures\n        try_names = [\n            ('bounds', 'lb', 'ub'),\n            ('bounds', 'lower', 'upper'),\n            ('', 'lb', 'ub'),\n            ('', 'lower_bounds', 'upper_bounds'),\n            ('', 'lower', 'upper'),\n            ('', 'lb', 'ub')\n        ]\n\n        for outer, low_name, up_name in try_names:\n            try:\n                if outer:\n                    container = getattr(func, outer, None)\n                    if container is None:\n                        continue\n                    maybe_lb = getattr(container, low_name, None)\n                    maybe_ub = getattr(container, up_name, None)\n                else:\n                    maybe_lb = getattr(func, low_name, None)\n                    maybe_ub = getattr(func, up_name, None)\n\n                if maybe_lb is not None and maybe_ub is not None:\n                    lb = np.array(maybe_lb, dtype=float)\n                    ub = np.array(maybe_ub, dtype=float)\n                    break\n            except Exception:\n                continue\n\n        # final fallback: maybe func.bounds is a tuple/list\n        if lb is None or ub is None:\n            try:\n                b = getattr(func, \"bounds\", None)\n                if b is not None:\n                    if isinstance(b, (list, tuple)) and len(b) == 2:\n                        lb = np.array(b[0], dtype=float)\n                        ub = np.array(b[1], dtype=float)\n            except Exception:\n                pass\n\n        if lb is None:\n            lb = np.full(self.dim, default_lb, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, default_ub, dtype=float)\n\n        # broadcast scalars if necessary\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # ensure correct length\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim).astype(float)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim).astype(float)\n\n        # enforce lb < ub elementwise; if not, swap or fallback to defaults\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                lb[i] = default_lb\n                ub[i] = default_ub\n\n        return np.array(lb, dtype=float), np.array(ub, dtype=float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        \"\"\"\n        Reflect coordinates outside [lb,ub] back into the box, per dimension.\n        If repeated reflections exceed max_reflect, clamp to boundary +/- tiny epsilon.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for i in range(self.dim):\n            tries = 0\n            while (x[i] < lb[i] or x[i] > ub[i]) and tries < max_reflect:\n                if x[i] < lb[i]:\n                    x[i] = lb[i] + (lb[i] - x[i])\n                elif x[i] > ub[i]:\n                    x[i] = ub[i] - (x[i] - ub[i])\n                tries += 1\n            if x[i] < lb[i]:\n                x[i] = lb[i] + 1e-12 * (1.0 + abs(lb[i]))\n            if x[i] > ub[i]:\n                x[i] = ub[i] - 1e-12 * (1.0 + abs(ub[i]))\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        Returns: (best_f, best_x)\n        \"\"\"\n        # extract bounds\n        lb, ub = self._extract_bounds(func)\n        range_vec = ub - lb\n        range_norm = np.max(range_vec)\n        if range_norm <= 0:\n            range_norm = 10.0\n\n        # scale gscale to absolute units\n        self.gscale = float(self.gscale_init) * range_norm\n\n        # archive\n        xs = []   # list of numpy arrays\n        fs = []   # list of floats\n\n        evals = 0\n        no_improve_iters = 0\n\n        # initial sampling\n        n_init = min(self.init_pop, self.budget)\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(np.array(x, dtype=float))\n            fs.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # safety if no initial points evaluated\n        if len(xs) == 0 and self.budget > 0:\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            xs.append(np.array(x, dtype=float)); fs.append(f); evals += 1\n            self.f_opt = f; self.x_opt = np.array(x, dtype=float)\n\n        # recent success record for adaptation\n        recent_success = []\n\n        # main loop\n        stagnation_threshold = max(50, self.adapt_window * 4)\n        F = 0.8   # DE differential weight\n        CR = 0.9  # crossover probability\n\n        while evals < self.budget:\n            N = len(xs)\n            # build arrays for convenience\n            xs_arr = np.array(xs)\n            fs_arr = np.array(fs)\n            order = np.argsort(fs_arr)  # ascending (best first)\n            xs_sorted = xs_arr[order]\n            fs_sorted = fs_arr[order]\n\n            # elite set\n            elite_k = max(1, int(np.ceil(N * self.elite_frac)))\n            elite_x = xs_sorted[:elite_k]\n            center = np.mean(xs_arr, axis=0)\n\n            # per-dim adaptive scale: base on gscale and archive spread\n            if N >= 2:\n                spread = np.std(xs_arr, axis=0)\n                per_dim_scale = np.maximum(spread, 1e-12 * range_norm)\n            else:\n                per_dim_scale = np.full(self.dim, 0.5 * range_norm)\n            # blend with global gscale\n            per_dim_scale = per_dim_scale + (self.gscale / (1.0 + range_norm))  # absolute scale\n\n            # choose a base index biased to better ranks\n            # exponential bias: sample rank r ~ Geometric-like via negative log\n            u = self.rng.rand()\n            bias = 0.7  # more bias -> more likely to pick best\n            # map u to rank via power-law\n            chosen_pos = int((u ** (1.0 / (1.0 - bias + 1e-12))) * (N - 1))\n            chosen_pos = np.clip(chosen_pos, 0, N - 1)\n            base_idx = order[chosen_pos]\n            base = xs[base_idx].astype(float)\n\n            # move-type probabilities\n            p_local = 0.35\n            p_de = 0.25\n            p_pca = 0.2\n            p_cauchy = 0.15\n            p_mix = 1.0 - (p_local + p_de + p_pca + p_cauchy)\n            move_choice = self.rng.rand()\n\n            candidate = None\n\n            # LOCAL anisotropic gaussian (multiplicative jitter + additive)\n            if move_choice < p_local:\n                # multiplicative log-normal jitter to allow anisotropic scaling\n                logf = np.exp(0.3 * self.rng.randn(self.dim))\n                jitter = per_dim_scale * 0.8 * self.rng.randn(self.dim) * logf\n                # small pull towards center proportional to distance\n                pull = 0.2 * (center - base) * (self.rng.rand(self.dim) ** 2)\n                candidate = base + jitter + pull\n\n            # DE-style donor vector with binomial crossover\n            elif move_choice < p_local + p_de:\n                if N >= 3:\n                    # pick three distinct indices != base_idx if possible\n                    idxs = [i for i in range(N) if i != base_idx]\n                    if len(idxs) >= 3:\n                        r = self.rng.choice(idxs, size=3, replace=False)\n                        x1 = xs[r[0]].astype(float)\n                        x2 = xs[r[1]].astype(float)\n                        x3 = xs[r[2]].astype(float)\n                        donor = x1 + F * (x2 - x3)\n                    else:\n                        donor = self._uniform_array(lb, ub).astype(float)\n                else:\n                    donor = self._uniform_array(lb, ub).astype(float)\n                # binomial crossover against base\n                mask = self.rng.rand(self.dim) < CR\n                # ensure at least one component from donor\n                if not np.any(mask):\n                    mask[self.rng.randint(self.dim)] = True\n                candidate = base.copy()\n                candidate[mask] = donor[mask]\n                # slight local jitter\n                candidate += 0.02 * per_dim_scale * self.rng.randn(self.dim)\n\n            # PCA-guided sampling using elites\n            elif move_choice < p_local + p_de + p_pca:\n                if elite_k >= 2:\n                    M = elite_x - np.mean(elite_x, axis=0)\n                    # regularize covariance\n                    cov = np.cov(M, rowvar=False)\n                    cov += np.eye(self.dim) * (1e-8 * (range_norm + 1.0))\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort descending\n                        idx = np.argsort(eigvals)[::-1]\n                        eigvals = np.maximum(eigvals[idx], 0.0)\n                        eigvecs = eigvecs[:, idx]\n                        # sample coefficients along principal axes with scaled std\n                        coeffs = np.zeros(self.dim, dtype=float)\n                        # concentrate on leading axes but allow exploration on smaller ones\n                        for j in range(self.dim):\n                            scale = np.sqrt(eigvals[j]) if eigvals[j] > 0 else 1e-6\n                            # scale by global gscale and random factor\n                            coeffs[j] = (self.gscale / (range_norm + 1e-12)) * scale * self.rng.randn()\n                        candidate = np.mean(elite_x, axis=0) + eigvecs.dot(coeffs)\n                        # small isotropic jitter\n                        candidate += 0.02 * per_dim_scale * self.rng.randn(self.dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * 0.5 * self.rng.randn(self.dim)\n                else:\n                    candidate = base + per_dim_scale * 0.5 * self.rng.randn(self.dim)\n\n            # heavy-tailed Cauchy jumps\n            elif move_choice < p_local + p_de + p_pca + p_cauchy:\n                # decide between global jump from center or local cauchy around base\n                if self.rng.rand() < 0.4:\n                    cauchy_scale = 0.8 * (range_norm)\n                    candidate = center + cauchy_scale * self.rng.standard_cauchy(self.dim)\n                else:\n                    cauchy_scale = 0.5 * per_dim_scale\n                    candidate = base + cauchy_scale * self.rng.standard_cauchy(self.dim)\n                # cap extremes to a comfortable margin\n                candidate = np.clip(candidate, lb - 10.0 * range_vec, ub + 10.0 * range_vec)\n\n            # rare mixture / archive mixing\n            else:\n                if N > 0:\n                    rec_idx = self.rng.randint(N)\n                    candidate = xs[rec_idx].astype(float) + 0.15 * per_dim_scale * self.rng.randn(self.dim)\n                else:\n                    candidate = self._uniform_array(lb, ub)\n\n            # safety: ensure finite values\n            if not np.all(np.isfinite(candidate)):\n                # fallback to bound-centered random\n                candidate = self._uniform_array(lb, ub)\n\n            # reflect/correct to bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # short additional mixing with a random archive member to diversify\n            if len(xs) > 0 and self.rng.rand() < 0.08:\n                mix_idx = self.rng.randint(len(xs))\n                alpha = 0.05 * self.rng.rand()\n                candidate = (1 - alpha) * candidate + alpha * xs[mix_idx]\n                candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archive\n            xs.append(np.array(candidate, dtype=float))\n            fs.append(f_cand)\n\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = np.array(candidate, dtype=float)\n                improved = True\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # success if candidate is among top 25% or improved global best\n            threshold_rank = max(1, int(0.25 * len(xs)))\n            combined_fs = np.array(fs)\n            rank = int(np.argsort(combined_fs).tolist().index(len(fs) - 1))  # rank position of latest (0=best)\n            success_flag = (rank < threshold_rank) or improved\n            recent_success.append(1 if success_flag else 0)\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            # adapt gscale every quarter window evaluations (responsive)\n            if (evals % max(1, self.adapt_window // 4)) == 0 and len(recent_success) > 0:\n                succ_rate = float(np.mean(recent_success))\n                # push gscale up if success high, else shrink\n                factor = 1.0 + 0.10 * (succ_rate - self.target_success)\n                # small randomness to escape cycling\n                factor *= 1.0 + 0.03 * (self.rng.randn())\n                self.gscale *= factor\n                # clamp\n                self.gscale = float(np.clip(self.gscale, self.min_gscale * range_norm, self.max_gscale * range_norm))\n\n            # prune memory periodically to keep it compact\n            if len(xs) > self.max_archive:\n                Ncur = len(xs)\n                keep_best = int(0.7 * self.max_archive)\n                order_all = np.argsort(np.array(fs))\n                keep_idxs = list(order_all[:keep_best])\n                remaining = [i for i in range(Ncur) if i not in keep_idxs]\n                keep_random = self.max_archive - keep_best\n                if remaining and keep_random > 0:\n                    randpick = list(self.rng.choice(remaining, size=min(len(remaining), keep_random), replace=False))\n                    keep_idxs.extend(randpick)\n                # build new lists\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n            # stagnation detection & micro-restart injections\n            if no_improve_iters >= stagnation_threshold and evals < self.budget:\n                # perform a few small injections (but not exceeding budget)\n                n_inj = min(5, max(1, (self.budget - evals) // 10))\n                for _ in range(n_inj):\n                    if evals >= self.budget:\n                        break\n                    inj_type = self.rng.rand()\n                    if inj_type < 0.6 and len(xs) > 0:\n                        # perturb a top elite\n                        elite_pick = self.rng.randint(max(1, elite_k))\n                        xbase = elite_x[elite_pick - 1] if elite_pick - 1 < len(elite_x) else elite_x[0]\n                        xinj = xbase + 0.6 * (self.gscale / (range_norm + 1e-12)) * (self.rng.randn(self.dim) * np.sqrt(per_dim_scale))\n                    else:\n                        # random PCA perturbation of a small neighborhood if possible\n                        sample_idx = self.rng.choice(len(xs), size=min(6, len(xs)), replace=False)\n                        neighborhood = np.array([xs[i] for i in sample_idx])\n                        m = np.mean(neighborhood, axis=0)\n                        cov = np.cov((neighborhood - m), rowvar=False)\n                        cov += np.eye(self.dim) * (1e-8 * (range_norm + 1.0))\n                        try:\n                            eigvals, eigvecs = np.linalg.eigh(cov)\n                            idx = np.argsort(eigvals)[::-1]\n                            eigvals = np.maximum(eigvals[idx], 0.0)\n                            eigvecs = eigvecs[:, idx]\n                            coeffs = (self.gscale / (range_norm + 1e-12)) * np.sqrt(eigvals) * self.rng.randn(self.dim)\n                            xinj = m + eigvecs.dot(coeffs)\n                        except Exception:\n                            xinj = self._uniform_array(lb, ub)\n                    xinj = self._reflect_bounds(xinj, lb, ub)\n                    try:\n                        fj = float(func(xinj))\n                    except Exception:\n                        fj = np.inf\n                    evals += 1\n                    xs.append(np.array(xinj, dtype=float))\n                    fs.append(fj)\n                    if fj < self.f_opt:\n                        self.f_opt = fj\n                        self.x_opt = np.array(xinj, dtype=float)\n                        no_improve_iters = 0\n                # encourage exploration after injection\n                self.gscale *= (1.0 + 0.25 * self.rng.rand())\n                no_improve_iters = 0  # reset\n\n        # final safety: ensure we have at least one valid point\n        if self.x_opt is None and len(xs) > 0:\n            idx = int(np.argmin(np.array(fs)))\n            self.x_opt = np.array(xs[idx], dtype=float)\n            self.f_opt = float(fs[idx])\n\n        if self.x_opt is None:\n            # fallback center\n            self.x_opt = (lb + ub) / 2.0\n            try:\n                self.f_opt = float(func(self.x_opt))\n            except Exception:\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = MemoryGuidedADS(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "98e54ebd-5501-41ce-9169-e61fd231a5a8", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler mixing archive-biased DE donors, PCA-guided elite sampling, anisotropic local Gaussians and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 gscale_init=0.25, adapt_window=50, target_success=0.2,\n                 min_gscale=1e-4, max_archive=250, elite_frac=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # adaptation & scales\n        self.gscale = float(gscale_init)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.min_gscale = float(min_gscale)\n        self.max_archive = int(max_archive)\n        self.elite_frac = float(elite_frac)\n        # bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _extract_bounds(self, func):\n        # Try common attributes, otherwise default [-5,5]^dim\n        default_lb = -5.0\n        default_ub = 5.0\n        lb = None\n        ub = None\n        # common candidate attributes\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # maybe object with lb/ub or tuple\n                lb_c = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                ub_c = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n                if lb_c is not None and ub_c is not None:\n                    lb = np.array(lb_c, dtype=float)\n                    ub = np.array(ub_c, dtype=float)\n                elif isinstance(b, (list, tuple)) and len(b) == 2:\n                    lb = np.array(b[0], dtype=float)\n                    ub = np.array(b[1], dtype=float)\n        except Exception:\n            lb = None; ub = None\n        # fallback to direct func attributes\n        if lb is None or ub is None:\n            try:\n                lb_c = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n                ub_c = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n                if lb_c is not None and ub_c is not None:\n                    lb = np.array(lb_c, dtype=float)\n                    ub = np.array(ub_c, dtype=float)\n            except Exception:\n                lb = None; ub = None\n        # defaults\n        if lb is None:\n            lb = np.full(self.dim, default_lb, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, default_ub, dtype=float)\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub), dtype=float)\n        # ensure lengths\n        if lb.shape[0] != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.shape[0] != self.dim:\n            ub = np.resize(ub, self.dim)\n        # enforce lb < ub\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                lb[i] = default_lb\n                ub[i] = default_ub\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        # reflect coordinate-wise\n        x = np.array(x, dtype=float)\n        for i in range(self.dim):\n            xi = x[i]\n            lbi = lb[i]; ubi = ub[i]\n            if not np.isfinite(xi):\n                xi = 0.5 * (lbi + ubi)\n            count = 0\n            while (xi < lbi or xi > ubi) and count < max_reflect:\n                if xi < lbi:\n                    xi = lbi + (lbi - xi)  # reflect\n                elif xi > ubi:\n                    xi = ubi - (xi - ubi)\n                count += 1\n            if xi < lbi:\n                xi = lbi + 1e-12 * (ubi - lbi)\n            if xi > ubi:\n                xi = ubi - 1e-12 * (ubi - lbi)\n            x[i] = xi\n        return x\n\n    def _uniform_array(self, lb, ub):\n        # vectorized uniform\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        # Prepare bounds\n        lb, ub = self._extract_bounds(func)\n        range_vec = ub - lb\n        range_norm = np.max(range_vec)\n        if range_norm <= 0:\n            range_norm = 1.0\n        # absolute per-dim base scale\n        base_scale = self.gscale * range_vec\n\n        # initialize archive\n        xs = []\n        fs = []\n        evals = 0\n\n        # initial sampling (small latin-ish / uniform)\n        n_init = min(max(10, self.dim + 4), max(4, self.budget // 20))\n        n_init = max(2, n_init)\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            xs.append(np.array(x, dtype=float))\n            fs.append(f)\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n\n        # safety: if nothing evaluated, seed center\n        if len(xs) == 0 and evals < self.budget:\n            x = 0.5 * (lb + ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            xs.append(np.array(x, dtype=float))\n            fs.append(f)\n            evals += 1\n            self.f_opt = f\n            self.x_opt = np.array(x, dtype=float)\n\n        # recent success record\n        recent_success = []\n\n        # main loop\n        stagnation_threshold = max(100, self.adapt_window * 6)\n        no_improve_iters = 0\n        CR = 0.9  # crossover probability for DE\n\n        # mixing probabilities\n        p_local = 0.35\n        p_de = 0.30\n        p_pca = 0.20\n        p_cauchy = 0.10\n        p_archive_mix = 0.05\n\n        while evals < self.budget:\n            N = len(xs)\n            xs_arr = np.array(xs)\n            fs_arr = np.array(fs)\n            order = np.argsort(fs_arr)\n            # elite set\n            elite_k = max(2, int(max(1, self.elite_frac * N)))\n            elite_k = min(elite_k, N)\n            elite_x = xs_arr[order[:elite_k]]\n\n            center = np.mean(xs_arr, axis=0)\n            # per-dim adaptive scale: blend global gscale with archive spread\n            spread = np.std(xs_arr, axis=0) if N > 1 else 0.5 * range_vec\n            per_dim_scale = np.maximum(self.gscale * range_vec, 1e-12 + 0.3 * spread)\n\n            # choose base index biased to better ranks (exponential bias)\n            ranks = np.arange(N)\n            bias_scale = max(1.0, 0.5 * N)\n            probs = np.exp(-ranks / bias_scale)\n            probs = probs / probs.sum()\n            chosen_rank = self.rng.choice(ranks, p=probs)\n            base_idx = order[chosen_rank]\n            base = xs_arr[base_idx].astype(float)\n\n            # choose move\n            u = self.rng.rand()\n            move_choice = u\n            candidate = None\n\n            # LOCAL anisotropic gaussian\n            if move_choice < p_local:\n                # multiplicative jitter + additive anisotropic noise + small pull to center\n                mult = 1.0 + 0.06 * self.rng.standard_normal(self.dim)\n                jitter = per_dim_scale * (0.8 * self.rng.standard_normal(self.dim) * (0.3 + self.rng.rand(self.dim)))\n                pull = 0.15 * (center - base) * self.rng.rand()\n                candidate = base * mult + jitter + pull\n\n            # DE-style donor vector with binomial crossover\n            elif move_choice < p_local + p_de:\n                if N >= 4:\n                    # pick three distinct indices != base_idx\n                    idxs = [i for i in range(N) if i != base_idx]\n                    r = self.rng.choice(idxs, size=min(3, len(idxs)), replace=False)\n                    x1 = xs_arr[r[0]].astype(float)\n                    x2 = xs_arr[r[1]].astype(float)\n                    x3 = xs_arr[r[2]].astype(float) if len(r) > 2 else xs_arr[r[0]].astype(float)\n                    F = 0.5 + 0.4 * self.rng.rand()\n                    donor = x1 + F * (x2 - x3)\n                    # binomial crossover\n                    mask = self.rng.random(self.dim) < CR\n                    # ensure at least one component from donor\n                    mask[self.rng.integers(self.dim)] = True\n                    candidate = np.where(mask, donor, base)\n                    # slight local jitter\n                    candidate = candidate + 0.08 * per_dim_scale * self.rng.standard_normal(self.dim)\n                else:\n                    # fallback to local jitter\n                    candidate = base + 0.5 * per_dim_scale * self.rng.standard_normal(self.dim)\n\n            # PCA-guided sampling using elites\n            elif move_choice < p_local + p_de + p_pca:\n                try:\n                    if elite_x.shape[0] >= 2:\n                        cov = np.cov(elite_x.T)\n                    else:\n                        cov = np.diag(per_dim_scale**2 * 0.1)\n                    # regularize\n                    cov = cov + np.eye(self.dim) * (1e-8 * (range_norm + 1.0))\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    eigvals = np.maximum(eigvals, 0.0)\n                    # sample in PCA basis, emphasize leading axes\n                    coeffs = np.zeros(self.dim)\n                    scales = np.sqrt(eigvals + 1e-12)\n                    # concentrate on leading axes but allow exploration\n                    for j in range(self.dim):\n                        scale = scales[j] * (0.6 + 0.8 * self.rng.rand())\n                        coeffs[j] = scale * self.gscale * self.rng.standard_normal()\n                    candidate = np.mean(elite_x, axis=0) + eigvecs.dot(coeffs)\n                    # small isotropic jitter\n                    candidate = candidate + 0.03 * per_dim_scale * self.rng.standard_normal(self.dim)\n                except Exception:\n                    candidate = self._uniform_array(lb, ub)\n\n            # heavy-tailed Cauchy jumps\n            elif move_choice < p_local + p_de + p_pca + p_cauchy:\n                # decide between global jump from center or local cauchy around base\n                if self.rng.rand() < 0.45:\n                    cauchy_scale = 0.9 * range_norm\n                    center_pt = center\n                else:\n                    cauchy_scale = 0.5 * np.mean(per_dim_scale)\n                    center_pt = base\n                # draw per-dim Cauchy (clamp extremes)\n                rv = self.rng.standard_cauchy(self.dim)\n                # cap extremes by percentile scaling\n                rv = np.clip(rv, -10.0, 10.0)\n                candidate = center_pt + (cauchy_scale * 0.2) * rv\n\n            # rare archive mix\n            else:\n                rec_idx = self.rng.integers(N)\n                candidate = xs_arr[rec_idx].astype(float) + 0.12 * per_dim_scale * self.rng.standard_normal(self.dim)\n\n            # safety: ensure finite values\n            if candidate is None or not np.all(np.isfinite(candidate)):\n                candidate = self._uniform_array(lb, ub)\n\n            # reflect/correct to bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # small mixing with random archive member occasionally\n            if self.rng.rand() < 0.07 and len(xs) > 0:\n                mix_idx = self.rng.integers(len(xs))\n                alpha = 0.04 * self.rng.rand()\n                candidate = (1 - alpha) * candidate + alpha * xs[mix_idx]\n                candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            xs.append(np.array(candidate, dtype=float))\n            fs.append(f_cand)\n            evals += 1\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = np.array(candidate, dtype=float)\n                improved = True\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # success if candidate is among top 25% or improved global best\n            combined_fs = np.array(fs)\n            thresh = np.percentile(combined_fs, 25) if len(combined_fs) > 0 else self.f_opt\n            success_flag = (f_cand <= thresh) or improved\n            recent_success.append(1 if success_flag else 0)\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            # adapt gscale every adapt_window evaluations (responsive)\n            if len(recent_success) >= self.adapt_window and (evals % max(1, self.adapt_window//4) == 0):\n                succ_rate = float(np.mean(recent_success))\n                # multiplicative adaptation (gentle)\n                factor = np.exp(0.6 * (succ_rate - self.target_success))\n                self.gscale *= factor\n                # small jitter to escape cycles\n                self.gscale *= (1.0 + 0.03 * (self.rng.random() - 0.5))\n                # clamp\n                self.gscale = max(self.min_gscale, min(self.gscale, 5.0))\n                base_scale = self.gscale * range_vec\n\n            # prune memory periodically to keep it compact\n            if len(xs) > self.max_archive * 1.5:\n                Ncur = len(xs)\n                keep_best = int(0.7 * self.max_archive)\n                order_all = np.argsort(np.array(fs))\n                keep_idxs = list(order_all[:keep_best])\n                remaining = [i for i in range(Ncur) if i not in keep_idxs]\n                keep_random = self.max_archive - keep_best\n                if keep_random > 0 and len(remaining) > 0:\n                    randpick = list(self.rng.choice(remaining, size=min(keep_random, len(remaining)), replace=False))\n                    keep_idxs.extend(randpick)\n                # build new lists\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n            # stagnation detection & micro-restart injections\n            if no_improve_iters > stagnation_threshold and evals < self.budget:\n                # inject a few perturbed elites or randoms\n                inj_count = min(6, max(2, self.dim // 2))\n                for k in range(inj_count):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.rand() < 0.6 and len(elite_x) > 0:\n                        pick = int(self.rng.integers(0, elite_x.shape[0]))\n                        xbase = elite_x[pick]\n                        xinj = xbase + 0.2 * per_dim_scale * self.rng.standard_normal(self.dim)\n                    else:\n                        # random PCA perturbation of a small neighborhood\n                        sample_idx = self.rng.choice(len(xs), size=min(6, len(xs)), replace=False)\n                        neighborhood = np.array([xs[i] for i in sample_idx])\n                        cov = np.cov(neighborhood.T) if neighborhood.shape[0] > 1 else np.diag(per_dim_scale**2 * 0.2)\n                        cov = cov + np.eye(self.dim) * 1e-8\n                        try:\n                            eigvals, eigvecs = np.linalg.eigh(cov)\n                            scales = np.sqrt(np.maximum(eigvals, 0.0))\n                            coeffs = scales * (0.8 * self.rng.standard_normal(self.dim))\n                            xinj = np.mean(neighborhood, axis=0) + eigvecs.dot(coeffs)\n                        except Exception:\n                            xinj = self._uniform_array(lb, ub)\n                    xinj = self._reflect_bounds(xinj, lb, ub)\n                    try:\n                        f_inj = float(func(xinj))\n                    except Exception:\n                        f_inj = np.inf\n                    xs.append(np.array(xinj, dtype=float))\n                    fs.append(f_inj)\n                    evals += 1\n                    if f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = np.array(xinj, dtype=float)\n                        no_improve_iters = 0\n                # encourage exploration: slightly enlarge scale\n                self.gscale = min(2.0, self.gscale * (1.0 + 0.25 * self.rng.rand()))\n                no_improve_iters = 0\n\n        # final safety: ensure we have at least one valid point\n        if self.x_opt is None and len(xs) > 0:\n            idx = int(np.argmin(np.array(fs)))\n            self.x_opt = np.array(xs[idx], dtype=float)\n            self.f_opt = float(fs[idx])\n\n        # final ensure within bounds\n        if self.x_opt is None:\n            self.x_opt = 0.5 * (lb + ub)\n            try:\n                self.f_opt = float(func(self.x_opt))\n            except Exception:\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = MemoryGuidedADS(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 197, in __call__, the following error occurred:\nAttributeError: 'numpy.random._generator.Generator' object has no attribute 'rand'. Did you mean: 'random'?\nOn line: u = self.rng.rand()", "error": "In the code, line 197, in __call__, the following error occurred:\nAttributeError: 'numpy.random._generator.Generator' object has no attribute 'rand'. Did you mean: 'random'?\nOn line: u = self.rng.rand()", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "57e44d3c-fef5-4be3-9fb7-4b0b8c13b864", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler combining archive-biased DE donors, PCA-guided elite moves, anisotropic local Gaussians and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS).\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 min_gscale=1e-5, max_archive=250, elite_frac=0.20):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.min_gscale = float(min_gscale)\n        self.max_archive = int(max_archive)\n        self.elite_frac = float(elite_frac)\n\n        # adaptation & state\n        self.gscale = 0.06  # base multiplicative scale (relative to range)\n        self.adapt_window = max(20, min(100, self.budget // 50))\n        self.recent_success = deque(maxlen=self.adapt_window)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _extract_bounds(self, func):\n        # Try common attributes; otherwise default [-5,5]^dim (BBOB)\n        lb = None; ub = None\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # bounds could be object with lb/ub, tuple (lb,ub), or array-like\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                try:\n                    lb = np.array(b.lb, dtype=float)\n                    ub = np.array(b.ub, dtype=float)\n                except Exception:\n                    lb = None; ub = None\n            elif isinstance(b, (list, tuple)) and len(b) == 2:\n                try:\n                    lb = np.array(b[0], dtype=float)\n                    ub = np.array(b[1], dtype=float)\n                except Exception:\n                    lb = None; ub = None\n        if lb is None or ub is None:\n            lb_c = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            ub_c = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n            if lb_c is not None and ub_c is not None:\n                try:\n                    lb = np.array(lb_c, dtype=float)\n                    ub = np.array(ub_c, dtype=float)\n                except Exception:\n                    lb = None; ub = None\n        # fallback default [-5,5]\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub), dtype=float)\n        lb = np.resize(lb, self.dim)\n        ub = np.resize(ub, self.dim)\n        # enforce lb < ub\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # coordinate-wise reflect into [lb,ub] robustly\n        x = np.array(x, dtype=float)\n        for i in range(self.dim):\n            lbi = lb[i]; ubi = ub[i]\n            xi = x[i]\n            if not np.isfinite(xi):\n                xi = lbi + self.rng.random() * (ubi - lbi)\n            # reflect until inside (with safety)\n            count = 0\n            span = ubi - lbi\n            if span <= 0:\n                x[i] = lbi\n                continue\n            while (xi < lbi or xi > ubi) and count < 20:\n                if xi < lbi:\n                    xi = lbi + (lbi - xi)  # mirror below\n                elif xi > ubi:\n                    xi = ubi - (xi - ubi)  # mirror above\n                count += 1\n            # final clamp as fallback\n            if xi < lbi:\n                xi = lbi + 1e-12 * span\n            if xi > ubi:\n                xi = ubi - 1e-12 * span\n            x[i] = xi\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return lb + self.rng.random(self.dim) * (ub - lb)\n\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        range_vec = ub - lb\n        # prevent degenerate ranges\n        range_vec = np.maximum(range_vec, 1e-9)\n\n        # archive storage: lists of arrays and function values\n        xs = []\n        fs = []\n        evals = 0\n\n        # initial sampling (small latin-ish / uniform)\n        n_init = int(min(max(10, self.dim + 4), max(4, self.budget // 20)))\n        n_init = min(n_init, max(1, self.budget))\n        for _ in range(n_init):\n            x = self._uniform_array(lb, ub)\n            if evals >= self.budget:\n                break\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(np.array(x, dtype=float))\n            fs.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # safety: if nothing evaluated, seed center\n        if len(xs) == 0 and evals < self.budget:\n            x = 0.5 * (lb + ub)\n            f = float(func(x))\n            evals += 1\n            xs.append(np.array(x, dtype=float))\n            fs.append(float(f))\n            self.f_opt = float(f); self.x_opt = np.array(x, dtype=float)\n\n        # main loop\n        p_local = 0.40\n        p_de = 0.30\n        p_pca = 0.15\n        p_cauchy = 0.10\n        # rest: archive mix\n        assert abs(p_local + p_de + p_pca + p_cauchy - 1.0) < 1e-8 or p_local + p_de + p_pca + p_cauchy < 1.0\n\n        no_improve_iters = 0\n        iter_since_adapt = 0\n\n        while evals < self.budget:\n            N = len(xs)\n            xs_arr = np.array(xs)\n            fs_arr = np.array(fs)\n            order = np.argsort(fs_arr)\n            # elite subset\n            elite_k = max(2, int(max(1, self.elite_frac * N)))\n            elite_x = xs_arr[order[:elite_k]]\n\n            center = np.mean(xs_arr, axis=0)\n\n            # per-dim adaptive scale: blend global gscale with archive spread\n            spread = np.std(xs_arr, axis=0) if N >= 2 else 0.5 * range_vec\n            per_dim_scale = np.maximum(spread, 1e-12 * range_vec) * self.gscale\n            per_dim_scale = np.maximum(per_dim_scale, self.min_gscale * range_vec)\n\n            # choose base index biased to better ranks (exponential bias)\n            ranks = np.arange(N)\n            bias_scale = max(3.0, N / 4.0)\n            probs = np.exp(-ranks / bias_scale)\n            probs = probs / probs.sum()\n            chosen_rank = self.rng.choice(ranks, p=probs)\n            base = xs_arr[order[chosen_rank]].astype(float)\n\n            # choose move\n            u = self.rng.random()\n            candidate = None\n\n            # LOCAL anisotropic gaussian\n            if u < p_local:\n                # multiplicative jitter + additive anisotropic noise + small pull to center\n                jitter = 1.0 + 0.15 * (self.rng.random(self.dim) - 0.5)\n                noise = per_dim_scale * jitter * self.rng.standard_normal(self.dim)\n                pull = 0.08 * (center - base)\n                candidate = base + noise + pull\n                # small chance to do directional step along difference to best\n                if self.rng.rand() < 0.08 and N >= 1:\n                    best_dir = xs_arr[order[0]] - base\n                    candidate += 0.6 * (self.rng.random() * best_dir)\n\n            # DE-style donor vector with binomial crossover\n            elif u < p_local + p_de:\n                # pick three distinct indices != base_idx\n                idxs = list(range(N))\n                base_idx = order[chosen_rank]\n                if len(idxs) >= 4:\n                    # choose from population excluding base_idx\n                    others = [i for i in idxs if i != base_idx]\n                    r = self.rng.choice(others, size=3, replace=False)\n                    x1 = xs_arr[r[0]].astype(float)\n                    x2 = xs_arr[r[1]].astype(float)\n                    x3 = xs_arr[r[2]].astype(float)\n                    F = 0.8 * (0.7 + 0.6 * self.rng.random())  # variant F\n                    donor = x1 + F * (x2 - x3)\n                    # binomial crossover\n                    CR = 0.9\n                    mask = self.rng.random(self.dim) < CR\n                    jrand = self.rng.integers(0, self.dim)\n                    mask[jrand] = True\n                    candidate = np.where(mask, donor, base)\n                    # slight local jitter\n                    candidate += 0.02 * per_dim_scale * self.rng.standard_normal(self.dim)\n                else:\n                    # fallback to local jitter\n                    candidate = base + per_dim_scale * self.rng.standard_normal(self.dim)\n\n            # PCA-guided sampling using elites\n            elif u < p_local + p_de + p_pca:\n                if elite_x.shape[0] >= 2:\n                    # build covariance around elite center\n                    e_center = np.mean(elite_x, axis=0)\n                    dev = elite_x - e_center\n                    cov = np.cov(dev.T) if dev.shape[0] > 1 else np.atleast_2d(np.diag(np.var(dev, axis=0)))\n                    cov = cov + np.eye(self.dim) * 1e-8\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        scales = np.sqrt(np.maximum(eigvals, 0.0))\n                        # emphasize leading axes: sample coefficients with decaying weights\n                        weight = np.exp(-np.arange(self.dim) / max(1.0, self.dim / 3.0))\n                        coeffs = (self.rng.standard_normal(self.dim) * scales) * (0.6 + 1.2 * weight / (weight.sum()))\n                        candidate = e_center + eigvecs.dot(coeffs)\n                        # mix small isotropic jitter based on per_dim_scale average\n                        candidate += 0.03 * np.mean(per_dim_scale) * self.rng.standard_normal(self.dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * self.rng.standard_normal(self.dim)\n                else:\n                    candidate = base + per_dim_scale * self.rng.standard_normal(self.dim)\n\n            # heavy-tailed Cauchy jumps\n            elif u < p_local + p_de + p_pca + p_cauchy:\n                # decide between global jump from center or local cauchy around base\n                if self.rng.rand() < 0.45:\n                    loc = center\n                    mult = 1.6 + 1.2 * self.rng.random()\n                else:\n                    loc = base\n                    mult = 1.0 + 0.8 * self.rng.random()\n                # draw per-dim standard Cauchy, scale and clamp extremes\n                raw = self.rng.standard_cauchy(self.dim)\n                # cap extremes to avoid absurd proposals\n                cut = np.percentile(np.abs(raw), 95)\n                if cut == 0 or not np.isfinite(cut):\n                    cut = 10.0\n                raw = np.clip(raw, -cut, cut)\n                candidate = loc + mult * per_dim_scale * raw\n                # clamp to a fraction of the full range\n                candidate = loc + np.clip(candidate - loc, -0.5 * range_vec, 0.5 * range_vec)\n\n            # rare archive mix or fallback\n            else:\n                # mix two random archive members (weighted)\n                if N >= 2 and self.rng.rand() < 0.9:\n                    i1, i2 = self.rng.choice(N, size=2, replace=False)\n                    w = self.rng.random()\n                    candidate = w * xs_arr[i1] + (1.0 - w) * xs_arr[i2]\n                    candidate += 0.01 * per_dim_scale * self.rng.standard_normal(self.dim)\n                else:\n                    candidate = self._uniform_array(lb, ub)\n\n            # ensure finite values & reflect to bounds\n            candidate = np.array(candidate, dtype=float)\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # small mixing with random archive member occasionally\n            if self.rng.rand() < 0.03 and len(xs) > 0:\n                j = self.rng.integers(0, len(xs))\n                candidate = 0.85 * candidate + 0.15 * xs[j]\n                candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archive\n            xs.append(np.array(candidate, dtype=float))\n            fs.append(float(f_cand))\n            # keep archive sorted and pruned\n            if len(xs) > self.max_archive:\n                # prune worst ones, keep some random for diversity\n                idxs = np.argsort(fs)\n                keep_best = max( min(self.max_archive, len(idxs)), int(self.max_archive * 0.6) )\n                keep_idxs = list(idxs[:keep_best])\n                # add a few random picks from remaining\n                remaining = [i for i in range(len(idxs)) if i not in keep_idxs]\n                if len(remaining) > 0:\n                    keep_random = min(int(self.max_archive * 0.1), len(remaining))\n                    if keep_random > 0:\n                        pick_rand = list(self.rng.choice(remaining, size=keep_random, replace=False))\n                        keep_idxs.extend(pick_rand)\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = np.array(candidate, dtype=float)\n                improved = True\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # success criterion: among top 25% or improved global best\n            thresh_idx = max(1, int(max(1, 0.25 * len(fs))))\n            thresh = np.partition(np.array(fs), thresh_idx - 1)[thresh_idx - 1]\n            success_flag = (f_cand <= thresh) or improved\n            self.recent_success.append(bool(success_flag))\n\n            # adapt gscale every adapt_window evaluations\n            iter_since_adapt += 1\n            if iter_since_adapt >= self.adapt_window:\n                succ_rate = float(np.mean(self.recent_success)) if len(self.recent_success) > 0 else 0.0\n                target = 0.2\n                # if we are improving too rarely, increase scale (explore); else reduce (exploit)\n                if succ_rate < target:\n                    factor = 1.12\n                else:\n                    factor = 0.88\n                # gentle multiplicative adaptation with small noise to escape cycles\n                self.gscale = max(self.min_gscale, min(2.0, self.gscale * (factor * (1.0 + 0.04 * (self.rng.random() - 0.5)))))\n                iter_since_adapt = 0\n\n            # stagnation detection & micro-restart injections\n            if no_improve_iters > max(200, 20 * self.dim):\n                # inject a few perturbed elites or randoms into archive\n                injections = min(6, max(2, int(self.dim / 2)))\n                for _ in range(injections):\n                    if len(elite_x) > 0 and self.rng.rand() < 0.8:\n                        pick = self.rng.integers(0, elite_x.shape[0])\n                        xbase = elite_x[pick]\n                        xinj = xbase + 0.2 * per_dim_scale * self.rng.standard_normal(self.dim)\n                    else:\n                        xinj = self._uniform_array(lb, ub)\n                    xinj = self._reflect_bounds(xinj, lb, ub)\n                    if evals >= self.budget:\n                        break\n                    try:\n                        f_inj = float(func(xinj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    xs.append(np.array(xinj, dtype=float))\n                    fs.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = float(f_inj); self.x_opt = np.array(xinj, dtype=float)\n                # encourage exploration: slightly enlarge scale\n                self.gscale = min(2.0, self.gscale * (1.0 + 0.25 * self.rng.rand()))\n                no_improve_iters = 0\n                self.recent_success.clear()\n\n        # final safety: ensure we have at least one valid point\n        if self.x_opt is None and len(xs) > 0:\n            idx = int(np.argmin(fs))\n            self.x_opt = np.array(xs[idx], dtype=float)\n            self.f_opt = float(fs[idx])\n\n        # ensure x_opt within bounds\n        self.x_opt = self._reflect_bounds(self.x_opt, lb, ub)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = MemoryGuidedADS(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "In the code, line 27, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.recent_success = deque(maxlen=self.adapt_window)", "error": "In the code, line 27, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.recent_success = deque(maxlen=self.adapt_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "52d7e745-c430-4020-ab7d-d85d53547011", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler mixing archive-biased DE donors, PCA-guided elite moves, anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 gscale=0.06, adapt_window=50, min_gscale=1e-8,\n                 max_archive=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.gscale = float(gscale)        # global multiplicative scale (relative to problem range)\n        self.adapt_window = int(adapt_window)\n        self.min_gscale = float(min_gscale)\n        self.max_archive = int(max_archive)\n\n        # bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- helper utilities -----\n    def _extract_bounds(self, func):\n        # Try common attrs, otherwise default [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                elif isinstance(b, (list, tuple)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n            # Common alternatives\n            if lb is None and hasattr(func, \"lower\"):\n                lb = np.asarray(func.lower, dtype=float)\n            if ub is None and hasattr(func, \"upper\"):\n                ub = np.asarray(func.upper, dtype=float)\n            if lb is None and hasattr(func, \"lb\"):\n                lb = np.asarray(func.lb, dtype=float)\n            if ub is None and hasattr(func, \"ub\"):\n                ub = np.asarray(func.ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, lb, dtype=float)\n        if ub.shape == ():\n            ub = np.full(self.dim, ub, dtype=float)\n\n        if lb.shape[0] != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.shape[0] != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        # enforce lb < ub\n        for i in range(self.dim):\n            if not (lb[i] < ub[i]):\n                ub[i] = lb[i] + 1e-6\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        # coordinate-wise reflection: reflect until within bounds or give up and clamp\n        x = x.copy()\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        # vectorized uniform sampling\n        return self.rng.random(self.dim) * (ub - lb) + lb\n\n    # ----- main call -----\n    def __call__(self, func):\n        # prepare bounds and base quantities\n        lb, ub = self._extract_bounds(func)\n        range_vec = ub - lb\n        # normalize range scalar\n        range_norm = float(np.max(range_vec))\n        base_scale = self.gscale * range_vec  # absolute per-dim base scale\n\n        # archive storage\n        xs = []\n        fs = []\n\n        evals = 0\n\n        # small initial sampling to seed archive: Latin-ish by stratified sampling\n        n_init = min(max(6, 4 * self.dim), max(6, self.budget // 20))\n        n_init = min(n_init, self.budget)\n        for i in range(n_init):\n            # stratified sampling: shift by i/n_init\n            u = (self.rng.random(self.dim) + (i / max(1, n_init))) % 1.0\n            x = lb + u * range_vec\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(np.copy(x))\n            fs.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.copy(x)\n            if evals >= self.budget:\n                break\n\n        # safety: if nothing evaluated, seed center\n        if len(xs) == 0 and evals < self.budget:\n            x = lb + 0.5 * range_vec\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            xs.append(np.copy(x))\n            fs.append(float(f))\n            self.f_opt = float(f)\n            self.x_opt = np.copy(x)\n\n        # recent success record (boolean list)\n        recent_success = []\n\n        no_improve_iters = 0\n        best_since_adapt = self.f_opt\n\n        # main loop\n        while evals < self.budget:\n            N = len(xs)\n            xs_arr = np.array(xs)\n            fs_arr = np.array(fs)\n\n            # sort for elite selection and ranking\n            order = np.argsort(fs_arr)\n            elite_k = max(2, min(N, 1 + self.dim // 2))\n            elite_x = xs_arr[order[:elite_k]]\n\n            # per-dim adaptive scale: blend global gscale with archive spread\n            archive_spread = np.std(xs_arr, axis=0) if N > 1 else np.zeros(self.dim)\n            per_dim_scale = np.maximum(self.gscale * range_vec, 0.001 * (archive_spread + 1e-12))\n\n            # choose base index biased to better ranks (exponential bias)\n            if N > 0:\n                ranks = np.empty(N)\n                ranks[order] = np.arange(N)\n                # exponential bias toward low ranks\n                probs = np.exp(-0.15 * ranks)\n                probs = probs / probs.sum()\n                base_idx = int(self.rng.choice(N, p=probs))\n                base = xs[base_idx].astype(float)\n            else:\n                base_idx = None\n                base = lb + 0.5 * range_vec\n\n            # choose move type\n            r = self.rng.random()\n            candidate = None\n\n            # LOCAL anisotropic gaussian (additive & multiplicative jitter + pull to center)\n            if r < 0.25:\n                pull = 0.15 * ((np.mean(elite_x, axis=0) if elite_x.size else lb + 0.5 * range_vec) - base) * self.rng.random()\n                # multiplicative jitter\n                mult_jitter = 1.0 + 0.2 * (self.rng.random(self.dim) - 0.5)\n                noise = self.rng.standard_normal(self.dim) * per_dim_scale * mult_jitter\n                candidate = base + noise + pull\n                # small isotropic jitter\n                candidate += 0.03 * range_norm * (self.rng.random(self.dim) - 0.5)\n\n            # DE-style donor vector with binomial crossover\n            elif r < 0.65 and N >= 4:\n                # differential evolution donor: x1 + F * (x2 - x3)\n                # pick three distinct indices != base_idx\n                idxs = [i for i in range(N) if i != base_idx]\n                rsel = self.rng.choice(idxs, size=3, replace=False)\n                x1 = xs_arr[rsel[0]].astype(float)\n                x2 = xs_arr[rsel[1]].astype(float)\n                x3 = xs_arr[rsel[2]].astype(float)\n                F = 0.6 + 0.3 * self.rng.random()  # scale factor\n                donor = x1 + F * (x2 - x3)\n                # binomial crossover\n                CR = 0.9\n                mask = self.rng.random(self.dim) < CR\n                if not mask.any():\n                    mask[self.rng.integers(self.dim)] = True\n                candidate = base.copy()\n                candidate[mask] = donor[mask]\n                # slight local jitter\n                candidate += 0.02 * per_dim_scale * self.rng.standard_normal(self.dim)\n\n            # PCA-guided sampling using elites\n            elif r < 0.85 and elite_x.shape[0] >= 2:\n                # build covariance in elite subspace scaled by per-dim-scale\n                cov = np.cov(elite_x.T) if elite_x.shape[0] > 1 else np.diag(per_dim_scale ** 2 * 0.1)\n                # regularize\n                cov = cov + np.diag((per_dim_scale ** 2) * 1e-6)\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # emphasize leading axes but allow exploration\n                    weights = np.sqrt(np.maximum(eigvals, 0.0))\n                    # concentrate on leading axes\n                    weights = weights * (1.0 + 3.0 * (weights / (weights.sum() + 1e-12)))\n                    coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * weights\n                    candidate = (np.mean(elite_x, axis=0) + eigvecs.dot(coeffs))\n                    # small isotropic jitter\n                    candidate += 0.02 * range_norm * self.rng.standard_normal(self.dim)\n                except Exception:\n                    # fallback to local jitter\n                    candidate = base + 0.5 * per_dim_scale * self.rng.standard_normal(self.dim)\n\n            # heavy-tailed Cauchy jumps\n            else:\n                # decide between global (from center) or local cauchy\n                if self.rng.random() < 0.4:\n                    center = np.mean(xs_arr, axis=0) if N > 0 else lb + 0.5 * range_vec\n                    scale = max(0.5 * range_norm * self.gscale, 0.01 * range_norm)\n                    step = np.tan(np.pi * (self.rng.random(self.dim) - 0.5))  # standard cauchy samples via inv cdf\n                    candidate = center + scale * step\n                else:\n                    scale = per_dim_scale * (1.0 + 0.5 * self.rng.random(self.dim))\n                    # cauchy per-dim\n                    step = np.tan(np.pi * (self.rng.random(self.dim) - 0.5))\n                    candidate = base + scale * step\n\n                # cap extremes by percentiles relative to lb/ub\n                cap = 4.0 * range_norm\n                candidate = np.clip(candidate, lb - cap, ub + cap)\n\n            # rare archive mix\n            if self.rng.random() < 0.05 and len(xs) > 0:\n                mix_idx = self.rng.integers(len(xs))\n                alpha = 0.2 + 0.6 * self.rng.random()\n                candidate = (1 - alpha) * candidate + alpha * xs[mix_idx]\n\n            # safety: ensure finite values and within bounds\n            candidate = np.nan_to_num(candidate, nan=0.5 * (lb + ub), posinf=ub, neginf=lb)\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archive (append)\n            xs.append(np.copy(candidate))\n            fs.append(float(f_cand))\n            # prune archive to keep compact (keep best and some random diversity)\n            if len(xs) > self.max_archive:\n                # keep top 2*dim and sample rest randomly to keep diversity\n                arr_f = np.array(fs)\n                idx_sorted = np.argsort(arr_f)\n                keep_best = list(idx_sorted[:max(2 * self.dim, 10)])\n                remaining = [i for i in range(len(xs)) if i not in keep_best]\n                keep_random = max(0, self.max_archive - len(keep_best))\n                if remaining and keep_random > 0:\n                    randpick = list(self.rng.choice(remaining, size=min(len(remaining), keep_random), replace=False))\n                else:\n                    randpick = []\n                keep_idxs = sorted(set(keep_best + randpick))\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n            # update best\n            improved = False\n            if f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = np.copy(candidate)\n                improved = True\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # success if candidate is among top 25% or improved global best\n            combined_fs = np.array(fs)\n            thresh = np.percentile(combined_fs, 25) if combined_fs.size > 0 else self.f_opt\n            success_flag = (f_cand <= thresh) or improved\n            recent_success.append(1 if success_flag else 0)\n            # keep recent window\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            # adapt gscale every adapt_window evaluations\n            if evals % max(1, self.adapt_window) == 0:\n                succ_rate = float(np.mean(recent_success)) if recent_success else 0.0\n                # gentle multiplicative adaptation: aim for success ~ 0.2\n                target = 0.2\n                if succ_rate > target:\n                    factor = 1.0 + 0.2 * (succ_rate - target)\n                else:\n                    factor = 0.85\n                # apply factor with small jitter\n                self.gscale = float(np.clip(self.gscale * factor * (1.0 + 0.02 * (self.rng.random() - 0.5)),\n                                            self.min_gscale, 2.0))\n                # reset recent success after adaptation\n                recent_success = []\n\n            # stagnation detection & micro-restarts\n            if no_improve_iters > max(200, 10 * self.dim):\n                # inject a few perturbed elites or randoms\n                inj_count = min(6, max(2, self.dim // 2))\n                for j in range(inj_count):\n                    if evals >= self.budget:\n                        break\n                    if self.rng.random() < 0.6 and elite_x.shape[0] > 0:\n                        # perturb an elite using PCA local noise\n                        base_inj = elite_x[self.rng.integers(elite_x.shape[0])]\n                        cov = np.diag(per_dim_scale ** 2 * 0.5)\n                        try:\n                            eigvals, eigvecs = np.linalg.eigh(cov)\n                            scales = np.sqrt(np.maximum(eigvals, 0.0))\n                            coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * scales\n                            x_inj = base_inj + eigvecs.dot(coeffs)\n                        except Exception:\n                            x_inj = base_inj + 0.5 * per_dim_scale * self.rng.standard_normal(self.dim)\n                    else:\n                        # random restart in domain\n                        x_inj = self._uniform_array(lb, ub)\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    xs.append(np.copy(x_inj))\n                    fs.append(float(f_inj))\n                    if f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = np.copy(x_inj)\n                        no_improve_iters = 0\n                # encourage exploration: slightly enlarge scale\n                self.gscale = float(min(2.0, self.gscale * (1.0 + 0.25 * self.rng.random())))\n                no_improve_iters = 0\n\n        # final safety: ensure we have at least one valid point\n        if self.x_opt is None and len(xs) > 0:\n            best_idx = int(np.argmin(fs))\n            self.x_opt = np.copy(xs[best_idx])\n            self.f_opt = float(fs[best_idx])\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Quick example test (uncomment to run):\n# if __name__ == \"__main__\":\n#     def sphere(x):\n#         return np.sum(x**2)\n#     sampler = MemoryGuidedADS(budget=2000, dim=10, seed=123)\n#     fbest, xbest = sampler(sphere)\n#     print(\"best f:\", fbest)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "f9e1c26e-2d24-41d4-8cab-7e62e89ad43a", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler mixing archive-biased DE donors, PCA-guided elite moves, anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 gscale=0.06, adapt_window=50, min_gscale=1e-8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # global scale controlling typical step sizes (fraction of range)\n        self.gscale = float(gscale)\n        self.min_gscale = float(min_gscale)\n        self.adapt_window = int(adapt_window)\n\n        # internal bookkeeping defaults\n        self.archive_max = max(50, 10 * self.dim)   # keep archive compact but useful\n        self.n_init = min(max(10, 5 * self.dim), max(1, self.budget // 10))\n        self.max_reflect = 20\n\n    # ---------- small utilities ----------\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        # coordinate-wise reflection until inside bounds (or clamp)\n        x = x.copy()\n        tries = 0\n        while tries < max_reflect:\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                return x\n            # reflect those below and above independently\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n            tries += 1\n        # final clamp\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def _uniform_array(self, lb, ub, size=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if size is None:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            # size: (n, dim)\n            return lb + self.rng.rand(*size) * (ub - lb)\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # Assume canonical bounds [-5,5] unless func provides explicit attributes\n        try:\n            # Some functions provide .bounds: try to extract, else use [-5,5]\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            elif hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n                lb = np.asarray(func.lower, dtype=float)\n                ub = np.asarray(func.upper, dtype=float)\n            elif hasattr(func, \"ub\") and hasattr(func, \"lb\"):\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n            else:\n                lb = -5.0 * np.ones(self.dim, dtype=float)\n                ub = +5.0 * np.ones(self.dim, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = +5.0 * np.ones(self.dim, dtype=float)\n\n        # enforce shapes\n        lb = np.broadcast_to(lb, (self.dim,))\n        ub = np.broadcast_to(ub, (self.dim,))\n        # ensure valid\n        range_vec = ub - lb\n        range_vec[range_vec == 0] = 1.0\n        center = lb + 0.5 * range_vec\n\n        # storage\n        xs = []   # list of x arrays\n        fs = []   # corresponding function values\n        evals = 0\n\n        # small Latin-hypercube-ish seeding\n        n_init = min(self.n_init, max(1, self.budget - max(0, self.budget // 20)))\n        # generate LHS-like samples\n        n = n_init\n        if n > 0:\n            # permutations per dimension\n            perms = np.vstack([self.rng.permutation(n) for _ in range(self.dim)]).T\n            for i in range(n):\n                u = (perms[i] + self.rng.rand(self.dim)) / float(n)\n                x = lb + u * range_vec\n                if evals >= self.budget:\n                    break\n                try:\n                    f = float(func(x))\n                except Exception:\n                    f = np.inf\n                xs.append(x)\n                fs.append(f)\n                evals += 1\n\n        # safety: if nothing evaluated, evaluate center\n        if len(xs) == 0 and evals < self.budget:\n            x0 = center.copy()\n            try:\n                f0 = float(func(x0))\n            except Exception:\n                f0 = np.inf\n            xs.append(x0)\n            fs.append(f0)\n            evals += 1\n\n        # bookkeeping of best\n        xs_arr = np.asarray(xs) if xs else np.zeros((0, self.dim))\n        fs_arr = np.asarray(fs) if fs else np.array([])\n\n        if len(fs) > 0:\n            best_idx = int(np.argmin(fs_arr))\n            best_f = float(fs_arr[best_idx])\n            best_x = xs[best_idx].copy()\n        else:\n            best_f = np.inf\n            best_x = center.copy()\n\n        # success window\n        recent_success = []\n        adapt_counter = 0\n        no_improve_counter = 0\n\n        # main loop\n        while evals < self.budget:\n            # refresh arrays\n            xs_arr = np.asarray(xs)\n            fs_arr = np.asarray(fs)\n            N = xs_arr.shape[0]\n\n            # rank-based ordering\n            order = np.argsort(fs_arr)\n            # per-dim adaptive scale: blend global gscale with archive spread (spread normalized)\n            if N > 1:\n                spread = np.std(xs_arr, axis=0)\n            else:\n                spread = 0.5 * range_vec\n            per_dim_scale = self.gscale * (0.5 + (spread / (range_vec + 1e-12)))  # fraction of full range\n\n            # choose base index biased to better ranks (exponential bias)\n            if N > 0:\n                ranks = np.empty(N, dtype=float)\n                ranks[order] = np.arange(N)\n                # exponential bias: probability ~ exp(-rank / tau)\n                tau = max(1.0, 0.1 * N)\n                probs = np.exp(-ranks / tau)\n                probs = probs / np.sum(probs)\n                base_idx = int(self.rng.choice(N, p=probs))\n                base = xs_arr[base_idx].copy()\n            else:\n                base_idx = None\n                base = self._uniform_array(lb, ub)\n\n            # pick move type\n            # probabilities: mostly local gaussian, then DE, then PCA, rare Cauchy heavy jump\n            p_local = 0.5\n            p_de = 0.2\n            p_pca = 0.2\n            p_cauchy = 0.1\n\n            r = self.rng.rand()\n            candidate = None\n\n            # LOCAL anisotropic gaussian (additive & multiplicative jitter + pull to center)\n            if r < p_local or N < 5:\n                mult_jitter = 1.0 + 0.2 * (self.rng.rand(self.dim) - 0.5)  # multiplicative jitter\n                additive = per_dim_scale * (self.rng.randn(self.dim) * mult_jitter) * range_vec\n                # small isotropic jitter proportional to gscale\n                isotropic = (self.gscale * 0.05) * range_vec * self.rng.randn(self.dim)\n                # pull to center of archive to encourage exploitation\n                if N > 0:\n                    archive_center = np.mean(xs_arr, axis=0)\n                    pull = 0.1 * (archive_center - base)\n                else:\n                    pull = 0.0\n                candidate = base + additive + isotropic + pull\n\n            # DE-style donor vector with binomial crossover\n            elif r < p_local + p_de and N >= 4:\n                # pick three distinct indices != base_idx\n                idxs = list(range(N))\n                if base_idx is not None and base_idx in idxs:\n                    idxs.remove(base_idx)\n                if len(idxs) >= 3:\n                    sel = self.rng.choice(idxs, size=3, replace=False)\n                    x1, x2, x3 = xs_arr[sel[0]], xs_arr[sel[1]], xs_arr[sel[2]]\n                    F = 0.6 * (1.0 + 0.5 * (self.rng.rand() - 0.5))  # differential weight with jitter\n                    donor = x1 + F * (x2 - x3)\n                    # binomial crossover\n                    cr = 0.9\n                    mask = (self.rng.rand(self.dim) < cr)\n                    if not np.any(mask):\n                        mask[self.rng.randint(self.dim)] = True\n                    candidate = base.copy()\n                    candidate[mask] = donor[mask]\n                    # slight local jitter\n                    candidate += per_dim_scale * 0.05 * (self.rng.randn(self.dim) * range_vec)\n                else:\n                    # fallback to local\n                    mult_jitter = 1.0 + 0.2 * (self.rng.rand(self.dim) - 0.5)\n                    candidate = base + per_dim_scale * (self.rng.randn(self.dim) * mult_jitter) * range_vec\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and N >= 3:\n                k_elite = max(2, min(2 * self.dim, N // 2))\n                elite_idx = order[:k_elite]\n                elite = xs_arr[elite_idx]\n                # build covariance in elite subspace scaled by per-dim-scale\n                cov = np.cov(elite.T) if elite.shape[0] > 1 else np.diag((per_dim_scale ** 2) * (range_vec ** 2))\n                # regularize\n                cov = cov + np.diag((per_dim_scale ** 2) * (range_vec ** 2) * 1e-6)\n                # try sampling from multivariate normal in principal directions\n                try:\n                    # emphasize leading axes but allow exploration\n                    # sample in eigenbasis to control concentration\n                    vals, vecs = np.linalg.eigh(cov)\n                    vals = np.maximum(vals, 1e-12)\n                    # scale eigenvalues by global gscale and small randomness\n                    vals = vals * (1.0 + 2.0 * self.rng.rand(len(vals)) * self.gscale)\n                    z = self.rng.randn(self.dim) * np.sqrt(vals)\n                    candidate = np.mean(elite, axis=0) + (vecs @ z)\n                    # small isotropic jitter\n                    candidate += (self.gscale * 0.02) * range_vec * self.rng.randn(self.dim)\n                except Exception:\n                    # fallback to local jitter\n                    candidate = base + per_dim_scale * (self.rng.randn(self.dim) * range_vec)\n\n            # heavy-tailed Cauchy jumps\n            else:\n                # decide between global (from center) or local cauchy\n                if self.rng.rand() < 0.5 or N < 2:\n                    c_base = center\n                else:\n                    c_base = base\n                # cauchy per-dim scaled by per_dim_scale and range\n                # use standard Cauchy sampling via tan(pi*(u-0.5))\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                candidate = c_base + (1.0 + 2.0 * self.gscale) * per_dim_scale * cauchy * range_vec\n                # cap extremes by percentiles relative to lb/ub (avoid extremely far outside)\n                # we'll reflect below which will handle bounds\n\n            # rare archive mix: replace some coords with elite coordinates occasionally\n            if candidate is None:\n                candidate = base.copy()\n            if self.rng.rand() < 0.08 and N >= 2:\n                # choose a random elite and mix some coordinates\n                elite_idx = order[self.rng.randint(min(N, max(2, N // 4)))]\n                mask = (self.rng.rand(self.dim) < 0.3)\n                candidate[mask] = xs_arr[elite_idx][mask]\n\n            # safety: ensure finite values and within bounds\n            candidate = np.asarray(candidate, dtype=float)\n            candidate = np.nan_to_num(candidate, nan=0.0, posinf=ub, neginf=lb)\n            candidate = self._reflect_bounds(candidate, lb, ub, max_reflect=self.max_reflect)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(candidate))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archive (append) then prune to keep compact\n            xs.append(candidate.copy())\n            fs.append(f_cand)\n\n            # update best\n            improved = False\n            if f_cand < best_f:\n                best_f = float(f_cand)\n                best_x = candidate.copy()\n                improved = True\n                no_improve_counter = 0\n            else:\n                no_improve_counter += 1\n\n            # success if candidate is among top 25% or improved global best\n            recent_flag = False\n            if len(fs) >= 4:\n                # compute percentile threshold\n                cur_fs = np.asarray(fs)\n                thresh = np.percentile(cur_fs, 25.0)\n                if f_cand <= thresh or improved:\n                    recent_flag = True\n            else:\n                if improved:\n                    recent_flag = True\n\n            recent_success.append(bool(recent_flag))\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            # prune archive if needed\n            if len(xs) > self.archive_max:\n                # keep top 2*dim and then random sample of rest\n                keep_top = max(2 * self.dim, 4)\n                order = np.argsort(fs)\n                top_idxs = list(order[:keep_top])\n                remaining = list(set(range(len(xs))) - set(top_idxs))\n                n_keep = self.archive_max - len(top_idxs)\n                if n_keep > 0 and remaining:\n                    keep_rand = list(self.rng.choice(remaining, size=min(n_keep, len(remaining)), replace=False))\n                else:\n                    keep_rand = []\n                keep_idxs = top_idxs + keep_rand\n                xs = [xs[i] for i in keep_idxs]\n                fs = [fs[i] for i in keep_idxs]\n\n            # adapt gscale every adapt_window evaluations\n            adapt_counter += 1\n            if adapt_counter >= self.adapt_window:\n                succ_rate = float(np.mean(recent_success)) if recent_success else 0.0\n                target = 0.2\n                # gentle multiplicative adaptation: increase if success high, decrease otherwise\n                if succ_rate > target:\n                    factor = 1.1 + 0.2 * (succ_rate - target)\n                else:\n                    factor = 0.9 - 0.2 * (target - succ_rate)\n                # jitter for robustness\n                factor *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n                self.gscale = float(np.clip(self.gscale * factor, self.min_gscale, 5.0))\n                # reset counters\n                recent_success = []\n                adapt_counter = 0\n\n            # stagnation detection & micro-restarts\n            # if no improvement for long relative to adapt_window, inject micro-restarts\n            if no_improve_counter > max(5 * self.adapt_window, 200) and evals < self.budget:\n                # inject a few perturbed elites or randoms\n                n_inject = min(5 + self.dim // 2, max(1, (self.budget - evals) // 20))\n                for _ in range(n_inject):\n                    if evals >= self.budget:\n                        break\n                    # pick an elite and perturb along PCA local noise if possible\n                    if N >= 3 and self.rng.rand() < 0.6:\n                        elite_idx = order[self.rng.randint(min(N, max(2, N // 3)))]\n                        x_inj = xs_arr[elite_idx].copy()\n                        # local perturbation with slightly larger scale\n                        x_inj += 1.5 * per_dim_scale * (self.rng.randn(self.dim) * range_vec)\n                        x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    else:\n                        x_inj = self._uniform_array(lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = np.inf\n                    xs.append(x_inj.copy())\n                    fs.append(f_inj)\n                    evals += 1\n                    if f_inj < best_f:\n                        best_f = float(f_inj)\n                        best_x = x_inj.copy()\n                # encourage exploration: slightly enlarge scale\n                self.gscale = float(np.clip(self.gscale * (1.0 + 0.5 * self.rng.rand()), self.min_gscale, 5.0))\n                no_improve_counter = 0  # reset after micro-restart\n\n        # final safety: ensure we have at least one valid point\n        if best_x is None:\n            if len(xs) > 0:\n                i = int(np.argmin(fs))\n                best_x = xs[i].copy()\n                best_f = float(fs[i])\n            else:\n                best_x = center.copy()\n                try:\n                    best_f = float(func(best_x)) if evals < self.budget else np.inf\n                except Exception:\n                    best_f = np.inf\n\n        return float(best_f), np.asarray(best_x, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 357, in __call__, the following error occurred:\nIndexError: index 100 is out of bounds for axis 0 with size 100\nOn line: x_inj = xs_arr[elite_idx].copy()", "error": "In the code, line 357, in __call__, the following error occurred:\nIndexError: index 100 is out of bounds for axis 0 with size 100\nOn line: x_inj = xs_arr[elite_idx].copy()", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a9351797-5432-4eea-9766-b3d75ee3147a", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler mixing archive-biased DE donors, PCA-guided elite moves, anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_archive=20, max_archive=200, adapt_window=50,\n                 target_success=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_archive = int(init_archive)\n        self.max_archive = int(max_archive)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n\n        # random number generator\n        self.rng = np.random.RandomState(seed)\n\n    # ----- helper utilities -----\n    def _extract_bounds(self, func):\n        # try to get bounds from function if available, otherwise default [-5,5]^dim\n        try:\n            lb = np.asarray(func.bounds.lb)\n            ub = np.asarray(func.bounds.ub)\n        except Exception:\n            try:\n                # some interfaces use tuple\n                lb, ub = func.bounds\n                lb = np.asarray(lb); ub = np.asarray(ub)\n            except Exception:\n                lb = -5.0 * np.ones(self.dim)\n                ub = 5.0 * np.ones(self.dim)\n\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # enforce lb < ub\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        # coordinate-wise reflection until within bounds or give up and clamp\n        x = x.copy()\n        for _ in range(max_reflect):\n            below = x < lb\n            if not np.any(below):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n\n            above = x > ub\n            if not np.any(above):\n                break\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n        total_range = ub - lb\n        center = (lb + ub) / 2.0\n\n        # archive storage (numpy arrays)\n        X = []\n        F = []\n\n        evals = 0\n\n        # initial sampling to seed archive: stratified-ish\n        n_init = min(self.init_archive, max(4, budget // 20))\n        for i in range(n_init):\n            # stratified shift\n            shift = (i + rng.rand()) / n_init\n            u = (rng.rand(dim) + shift) / n_init  # vectorized shift, wrap-around uniform-ish\n            x = lb + (u % 1.0) * total_range\n            try:\n                f = float(func(x))\n            except Exception:\n                # If evaluation fails, skip\n                continue\n            X.append(x)\n            F.append(f)\n            evals += 1\n            if evals >= budget:\n                break\n\n        # safety: if nothing evaluated, seed center\n        if len(X) == 0 and evals < budget:\n            x0 = center.copy()\n            try:\n                f0 = float(func(x0))\n            except Exception:\n                # give up with a random point\n                x0 = self._uniform_array(lb, ub)\n                f0 = float(func(x0))\n            X.append(x0)\n            F.append(f0)\n            evals += 1\n\n        # convert to arrays\n        X = np.array(X)\n        F = np.array(F)\n\n        # best\n        best_idx = int(np.argmin(F))\n        f_best = float(F[best_idx])\n        x_best = X[best_idx].copy()\n\n        # short-term adaptation state\n        gscale = 0.05  # relative to box range; initial global scale fraction\n        recent_success = []  # booleans\n        adapt_counter = 0\n        stagnation_since = 0\n        last_improvement_eval = evals\n\n        # main loop\n        while evals < budget:\n            n_archive = len(F)\n            # ensure at least 3 archive entries\n            if n_archive < 3:\n                # sample random\n                x = self._uniform_array(lb, ub)\n                try:\n                    f = float(func(x))\n                except Exception:\n                    break\n                X = np.vstack([X, x])\n                F = np.append(F, f)\n                evals += 1\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                    last_improvement_eval = evals\n                continue\n\n            # ranking and elites\n            ranks = np.argsort(F)\n            # define elites as top min(max(3, dim), n_archive)\n            k_elite = max(3, min(dim, n_archive))\n            elite_idx = ranks[:k_elite]\n            elites = X[elite_idx]\n\n            # per-dim adaptive scale: blend global gscale*range with archive spread\n            spread = np.std(X, axis=0)\n            per_dim_scale = np.maximum(gscale * total_range, 0.5 * spread + 1e-12)\n\n            # choose base index biased to better ranks (exponential bias)\n            rank_positions = np.empty(n_archive, dtype=float)\n            rank_positions[ranks] = np.arange(n_archive)\n            bias = np.exp(-rank_positions / max(1.0, 0.15 * n_archive))\n            probs = bias / bias.sum()\n            base_idx = rng.choice(n_archive, p=probs)\n            base = X[base_idx].copy()\n\n            # choose move type\n            move_rand = rng.rand()\n            # probabilities: LOCAL 35%, DE 30%, PCA 18%, CAUCHY 12%, ARCHIVE_MIX 5%\n            if move_rand < 0.35:\n                move = \"LOCAL\"\n            elif move_rand < 0.65:\n                move = \"DE\"\n            elif move_rand < 0.83:\n                move = \"PCA\"\n            elif move_rand < 0.95:\n                move = \"CAUCHY\"\n            else:\n                move = \"ARCHIVE_MIX\"\n\n            # create candidate\n            if move == \"LOCAL\":\n                # anisotropic gaussian local move with multiplicative jitter & pull to center\n                mult_jitter = 1.0 + 0.5 * (rng.rand(dim) - 0.5)\n                noise = rng.normal(scale=1.0, size=dim)\n                candidate = base + per_dim_scale * mult_jitter * noise\n                # small isotropic jitter\n                candidate += (gscale * np.mean(total_range)) * 0.05 * rng.randn(dim)\n                # pull to center a bit\n                pull = 0.08 * rng.rand() * (center - base)\n                candidate += pull\n\n            elif move == \"DE\":\n                # DE-style donor vector with binomial crossover\n                # pick three distinct indices != base_idx\n                idxs = list(range(n_archive))\n                idxs.remove(base_idx)\n                a, b, c = rng.choice(idxs, size=3, replace=False)\n                x1 = X[a]; x2 = X[b]; x3 = X[c]\n                Fscale = 0.5 + 0.5 * rng.rand()  # in (0.5,1)\n                donor = x1 + Fscale * (x2 - x3)\n                donor = self._reflect_bounds(donor, lb, ub)\n                cr = 0.2 + 0.6 * rng.rand()\n                candidate = base.copy()\n                jrand = rng.randint(dim)\n                mask = rng.rand(dim) < cr\n                mask[jrand] = True\n                candidate[mask] = donor[mask]\n                # slight local jitter\n                candidate += per_dim_scale * 0.1 * rng.randn(dim)\n\n            elif move == \"PCA\":\n                # PCA-guided sampling using elites\n                # center elites\n                mean_elite = np.mean(elites, axis=0)\n                C = elites - mean_elite\n                # small regularization by per-dim scale\n                cov = np.cov(C.T) if C.shape[0] > 1 else np.diag(per_dim_scale**2)\n                # ensure covariance matrix is well-formed\n                if not np.all(np.isfinite(cov)) or np.linalg.matrix_rank(cov) < 1:\n                    # fallback to local jitter\n                    candidate = base + per_dim_scale * rng.randn(dim) * 0.5\n                else:\n                    # regularize\n                    cov += np.diag((per_dim_scale * 0.1) ** 2)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # emphasize leading axes but allow exploration\n                        sort_idx = np.argsort(-eigvals)\n                        eigvals = np.maximum(eigvals[sort_idx], 1e-12)\n                        eigvecs = eigvecs[:, sort_idx]\n                        # sample in PC-space with emphasis on top axes\n                        # heavier drawing on top components\n                        emphasis = 1.0 + 2.0 * (np.exp(-np.arange(dim) / max(1.0, dim / 3.0)))\n                        # draw coefficients\n                        coeffs = rng.randn(dim) * np.sqrt(eigvals) * np.sqrt(emphasis)\n                        perturb = eigvecs.dot(coeffs)\n                        candidate = mean_elite + perturb\n                        # small isotropic jitter\n                        candidate += (gscale * np.mean(total_range)) * 0.02 * rng.randn(dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * rng.randn(dim) * 0.5\n\n            elif move == \"CAUCHY\":\n                # heavy-tailed jumps\n                use_global = (rng.rand() < 0.5)\n                # sample standard cauchy, but cap extremes to avoid numeric explosion\n                rc = rng.standard_cauchy(size=dim)\n                rc = np.clip(rc, a_min=-10.0, a_max=10.0)\n                if use_global:\n                    candidate = center + rc * (gscale * 5.0) * total_range\n                else:\n                    candidate = base + rc * per_dim_scale * 2.0\n                # cap a bit relative to box\n                candidate = np.minimum(np.maximum(candidate, lb - 0.5 * total_range),\n                                       ub + 0.5 * total_range)\n\n            else:  # ARCHIVE_MIX\n                # rare archive mix: average a couple of good points and jitter\n                i1, i2 = rng.choice(elite_idx, size=2, replace=False)\n                candidate = 0.5 * (X[i1] + X[i2]) + per_dim_scale * 0.2 * rng.randn(dim)\n\n            # occasional archive mixing: mix some dims from a random archive member\n            if rng.rand() < 0.08:\n                partner = X[rng.randint(len(X))]\n                mask = rng.rand(dim) < 0.2\n                candidate[mask] = partner[mask] + 0.02 * total_range[mask] * rng.randn(np.sum(mask))\n\n            # safety: ensure finite values and within bounds via reflection\n            candidate = np.asarray(candidate, dtype=float)\n            candidate[~np.isfinite(candidate)] = lb[~np.isfinite(candidate)] + rng.rand() * (ub[~np.isfinite(candidate)] - lb[~np.isfinite(candidate)])\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= budget:\n                break\n            try:\n                f_candidate = float(func(candidate))\n            except Exception:\n                # bad evaluation, skip counting? We should count as an evaluation attempt.\n                f_candidate = np.inf\n            # consume one eval\n            evals += 1\n\n            # update archive\n            X = np.vstack([X, candidate])\n            F = np.append(F, f_candidate)\n\n            # prune archive to keep compact (keep best and some random diversity)\n            if len(F) > self.max_archive:\n                # keep top 2*dim\n                keep_top = min(2 * dim, len(F))\n                top_idx = np.argsort(F)[:keep_top]\n                # sample remainder randomly from the rest until max_archive reached\n                rest_idx = np.setdiff1d(np.arange(len(F)), top_idx)\n                n_needed = self.max_archive - keep_top\n                if n_needed > 0 and len(rest_idx) > 0:\n                    chosen_rest = rng.choice(rest_idx, size=min(n_needed, len(rest_idx)), replace=False)\n                    keep_idx = np.concatenate([top_idx, chosen_rest])\n                else:\n                    keep_idx = top_idx\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # update best\n            if f_candidate < f_best:\n                f_best = f_candidate\n                x_best = candidate.copy()\n                last_improvement_eval = evals\n\n            # success if candidate is among top 25% or improved global best\n            success = False\n            try:\n                q25 = np.percentile(F, 25)\n                if f_candidate <= q25 or f_candidate < f_best:\n                    success = True\n            except Exception:\n                # fallback: improvement means success\n                success = (f_candidate < f_best)\n\n            recent_success.append(bool(success))\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            adapt_counter += 1\n\n            # adapt gscale every adapt_window evaluations\n            if adapt_counter >= self.adapt_window:\n                adapt_counter = 0\n                succ_rate = np.mean(recent_success) if len(recent_success) > 0 else 0.0\n                # if success rate too low -> increase scale; if too high -> decrease\n                if succ_rate < self.target_success:\n                    factor = 1.08 + 0.04 * (rng.rand() - 0.5)\n                else:\n                    factor = 0.92 + 0.04 * (rng.rand() - 0.5)\n                gscale *= factor\n                # gentle jitter\n                gscale *= np.exp(0.03 * (rng.rand() - 0.5))\n                gscale = float(np.clip(gscale, 1e-6, 1.0))\n                # reset success window\n                recent_success = []\n\n            # stagnation detection & micro-restarts\n            stagnation_evals = evals - last_improvement_eval\n            stagnation_threshold = max(200, 30 * dim)\n            if stagnation_evals > stagnation_threshold:\n                # perform a few small micro restarts: inject perturbed elites or randoms\n                n_micro = min(5, max(1, dim // 2))\n                for m in range(n_micro):\n                    if evals >= budget:\n                        break\n                    if rng.rand() < 0.6:\n                        # perturb an elite using PCA local noise\n                        e = elites[rng.randint(len(elites))]\n                        perturb = per_dim_scale * (0.5 + rng.rand() * 0.5) * rng.randn(dim)\n                        x_new = self._reflect_bounds(e + perturb, lb, ub)\n                    else:\n                        # random restart in domain\n                        x_new = self._uniform_array(lb, ub)\n                    try:\n                        f_new = float(func(x_new))\n                    except Exception:\n                        f_new = np.inf\n                    evals += 1\n                    X = np.vstack([X, x_new])\n                    F = np.append(F, f_new)\n                    if f_new < f_best:\n                        f_best = f_new\n                        x_best = x_new.copy()\n                        last_improvement_eval = evals\n                # encourage exploration: slightly enlarge scale\n                gscale = float(min(1.0, gscale * (1.12 + 0.06 * rng.rand())))\n                # reset stagnation counter\n                last_improvement_eval = evals\n\n            # end main loop iteration\n\n        # final safety: ensure we have at least one valid point\n        if not np.isfinite(f_best):\n            # pick any finite evaluation from archive\n            finite_idx = np.where(np.isfinite(F))[0]\n            if finite_idx.size > 0:\n                i = finite_idx[0]\n                f_best = float(F[i])\n                x_best = X[i].copy()\n            else:\n                # fallback to center\n                x_best = center.copy()\n                try:\n                    f_best = float(func(x_best))\n                except Exception:\n                    f_best = np.inf\n\n        return f_best, x_best", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "61584dae-e295-4d42-a8d3-cb4d5023538a", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler mixing archive-biased DE donors, PCA-guided elite moves, anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_archive=20, max_archive=200, adapt_window=50,\n                 target_success=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_archive = int(init_archive)\n        self.max_archive = int(max_archive)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n\n        # random number generator\n        self.rng = np.random.RandomState(seed)\n\n    # ----- helper utilities -----\n    def _extract_bounds(self, func):\n        # try to get bounds from function if available, otherwise default [-5,5]^dim\n        try:\n            lb = np.asarray(func.bounds.lb)\n            ub = np.asarray(func.bounds.ub)\n        except Exception:\n            try:\n                lb, ub = func.bounds\n                lb = np.asarray(lb); ub = np.asarray(ub)\n            except Exception:\n                lb = -5.0 * np.ones(self.dim)\n                ub = 5.0 * np.ones(self.dim)\n\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # enforce lb < ub and finite\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        # coordinate-wise reflection until within bounds or clamp\n        x = x.copy()\n        for _ in range(max_reflect):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not np.any(below) and not np.any(above):\n                break\n        # final clamp to avoid pathological cases\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n        total_range = ub - lb\n        center = (lb + ub) / 2.0\n\n        # archive storage (numpy arrays) - start empty\n        X_list = []\n        F_list = []\n\n        evals = 0\n\n        # initial sampling to seed archive: stratified-ish Latin-hypercube style\n        n_init = min(self.init_archive, max(1, budget // 10))\n        for i in range(n_init):\n            if evals >= budget:\n                break\n            # stratified shift per-dimension\n            u = (rng.rand(dim) + i) / float(n_init)\n            x = lb + (u % 1.0) * total_range\n            try:\n                f = float(func(x))\n            except Exception:\n                # If evaluation fails, place an infinite value and continue\n                f = np.inf\n            X_list.append(x.copy())\n            F_list.append(f)\n            evals += 1\n\n        # safety: if nothing evaluated (or all were infinities), seed center\n        if len(X_list) == 0 and evals < budget:\n            x0 = center.copy()\n            try:\n                f0 = float(func(x0))\n            except Exception:\n                # give up with a random point\n                x0 = self._uniform_array(lb, ub)\n                f0 = float(func(x0))\n            X_list.append(x0.copy())\n            F_list.append(f0)\n            evals += 1\n\n        # convert to arrays\n        X = np.array(X_list)\n        F = np.array(F_list)\n\n        # ensure at least one point exists\n        if len(F) == 0:\n            # random single point\n            x0 = self._uniform_array(lb, ub)\n            try:\n                f0 = float(func(x0))\n            except Exception:\n                f0 = np.inf\n            X = np.array([x0])\n            F = np.array([f0])\n            evals += 1\n\n        # best\n        best_idx = int(np.nanargmin(F))\n        f_best = float(F[best_idx])\n        x_best = X[best_idx].copy()\n\n        # short-term adaptation state\n        gscale = 0.05  # relative to box range; initial global scale fraction\n        recent_success = []  # booleans\n        adapt_counter = 0\n        last_improvement_eval = evals\n\n        # main loop\n        while evals < budget:\n            n_archive = len(F)\n            # ensure at least 3 archive entries\n            if n_archive < 3:\n                # sample random\n                if evals >= budget:\n                    break\n                x = self._uniform_array(lb, ub)\n                try:\n                    f = float(func(x))\n                except Exception:\n                    f = np.inf\n                X = np.vstack([X, x])\n                F = np.append(F, f)\n                evals += 1\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                    last_improvement_eval = evals\n                continue\n\n            # ranking and elites\n            ranks = np.argsort(F)\n            # define elites as top min(max(3, dim), n_archive)\n            k_elite = min(max(3, dim), n_archive)\n            elite_idx = ranks[:k_elite]\n            elites = X[elite_idx]\n\n            # per-dim adaptive scale: blend global gscale*range with archive spread\n            spread = np.std(X, axis=0) if len(X) > 1 else 0.5 * total_range\n            per_dim_scale = np.maximum(gscale * total_range, 0.5 * spread + 1e-12)\n\n            # choose base index biased to better ranks (exponential bias)\n            rank_positions = np.empty(n_archive, dtype=float)\n            rank_positions[ranks] = np.arange(n_archive)\n            bias = np.exp(-rank_positions / max(1.0, n_archive / 6.0))\n            probs = bias / bias.sum()\n            base_idx = int(rng.choice(n_archive, p=probs))\n            base = X[base_idx].copy()\n\n            # choose move type\n            move_rand = rng.rand()\n            # probabilities: LOCAL 35%, DE 30%, PCA 18%, CAUCHY 12%, ARCHIVE_MIX 5%\n            if move_rand < 0.35:\n                move = \"LOCAL\"\n            elif move_rand < 0.65:\n                move = \"DE\"\n            elif move_rand < 0.83:\n                move = \"PCA\"\n            elif move_rand < 0.95:\n                move = \"CAUCHY\"\n            else:\n                move = \"ARCHIVE_MIX\"\n\n            # create candidate\n            candidate = base.copy()\n\n            if move == \"LOCAL\":\n                # anisotropic gaussian local move with multiplicative jitter & pull to center\n                noise = rng.normal(scale=1.0, size=dim)\n                mult_jitter = 1.0 + 0.6 * (rng.rand(dim) - 0.5)\n                candidate = base + per_dim_scale * mult_jitter * noise\n                # small isotropic jitter\n                candidate += (gscale * np.mean(total_range)) * 0.05 * rng.randn(dim)\n                # pull to center a bit\n                pull = 0.08 * rng.rand() * (center - base)\n                candidate += pull\n\n            elif move == \"DE\":\n                # DE-style donor vector with binomial crossover\n                # pick three distinct indices != base_idx\n                idxs = list(range(n_archive))\n                idxs.remove(base_idx)\n                a, b, c = rng.choice(idxs, size=3, replace=False)\n                x1 = X[a]; x2 = X[b]; x3 = X[c]\n                Fscale = 0.5 + 0.5 * rng.rand()  # in (0.5,1)\n                donor = x1 + Fscale * (x2 - x3)\n                donor = self._reflect_bounds(donor, lb, ub)\n                cr = 0.2 + 0.6 * rng.rand()\n                candidate = base.copy()\n                # binomial crossover mask\n                jrand = rng.randint(dim)\n                mask = rng.rand(dim) < cr\n                mask[jrand] = True\n                candidate[mask] = donor[mask]\n                # slight local jitter\n                candidate += per_dim_scale * 0.1 * rng.randn(dim)\n\n            elif move == \"PCA\":\n                # PCA-guided sampling using elites\n                mean_elite = np.mean(elites, axis=0)\n                C = elites - mean_elite\n                if C.shape[0] <= 1:\n                    # fallback to local jitter\n                    candidate = base + per_dim_scale * 0.5 * rng.randn(dim)\n                else:\n                    cov = np.cov(C.T)\n                    # regularize\n                    cov += np.diag((per_dim_scale * 0.1) ** 2)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        sort_idx = np.argsort(-eigvals)\n                        eigvals = np.maximum(eigvals[sort_idx], 1e-12)\n                        eigvecs = eigvecs[:, sort_idx]\n                        # emphasize leading axes but allow exploration\n                        emphasis = 1.0 + 2.0 * (np.exp(-np.arange(dim) / max(1.0, dim / 3.0)))\n                        coeffs = rng.randn(dim) * np.sqrt(eigvals) * np.sqrt(emphasis)\n                        perturb = eigvecs.dot(coeffs)\n                        candidate = mean_elite + perturb\n                        # small isotropic jitter\n                        candidate += (gscale * np.mean(total_range)) * 0.02 * rng.randn(dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * rng.randn(dim) * 0.5\n\n            elif move == \"CAUCHY\":\n                # heavy-tailed jumps\n                use_global = (rng.rand() < 0.5)\n                # sample standard cauchy, but cap extremes to avoid numeric explosion\n                rc = rng.standard_cauchy(size=dim)\n                rc = np.clip(rc, a_min=-10.0, a_max=10.0)\n                if use_global:\n                    candidate = center + rc * (gscale * 5.0) * total_range\n                else:\n                    candidate = base + rc * (gscale * 1.5) * total_range\n                # cap a bit relative to box\n                candidate = np.minimum(np.maximum(candidate, lb - 0.5 * total_range),\n                                       ub + 0.5 * total_range)\n\n            else:  # ARCHIVE_MIX\n                # rare archive mix: average a couple of good points and jitter\n                i1, i2 = rng.choice(elite_idx, size=2, replace=False)\n                candidate = 0.5 * (X[i1] + X[i2])\n                candidate += per_dim_scale * 0.2 * rng.randn(dim)\n\n            # occasional archive mixing: mix some dims from a random archive member\n            if rng.rand() < 0.08 and len(X) > 0:\n                partner = X[rng.randint(len(X))]\n                mix_mask = rng.rand(dim) < 0.2\n                if mix_mask.sum() > 0:\n                    candidate[mix_mask] = partner[mix_mask] + 0.02 * total_range[mix_mask] * rng.randn(mix_mask.sum())\n\n            # safety: ensure finite values and within bounds via reflection\n            bad_mask = ~np.isfinite(candidate)\n            if np.any(bad_mask):\n                candidate[bad_mask] = lb[bad_mask] + rng.rand(np.sum(bad_mask)) * (ub[bad_mask] - lb[bad_mask])\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= budget:\n                break\n            try:\n                f_candidate = float(func(candidate))\n            except Exception:\n                # bad evaluation counts as an evaluation and is treated as very bad\n                f_candidate = np.inf\n            evals += 1\n\n            # update archive\n            X = np.vstack([X, candidate])\n            F = np.append(F, f_candidate)\n\n            improved = False\n            if np.isfinite(f_candidate) and f_candidate < f_best:\n                f_best = f_candidate\n                x_best = candidate.copy()\n                last_improvement_eval = evals\n                improved = True\n\n            # prune archive to keep compact (keep best and some random diversity)\n            if len(F) > self.max_archive:\n                keep_top = min(2 * dim, self.max_archive)\n                top_idx = np.argsort(F)[:keep_top]\n                rest_idx = np.setdiff1d(np.arange(len(F)), top_idx)\n                n_needed = self.max_archive - keep_top\n                if n_needed > 0 and len(rest_idx) > 0:\n                    chosen_rest = rng.choice(rest_idx, size=min(n_needed, len(rest_idx)), replace=False)\n                    keep_idx = np.concatenate([top_idx, chosen_rest])\n                else:\n                    keep_idx = top_idx\n                # reorder to keep stable arrays\n                keep_idx = np.unique(keep_idx)\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # success if candidate is among top 25% or improved global best\n            success = False\n            try:\n                q25 = np.percentile(F, 25)\n                if np.isfinite(f_candidate) and (f_candidate <= q25 or improved):\n                    success = True\n            except Exception:\n                success = improved\n\n            recent_success.append(bool(success))\n            if len(recent_success) > self.adapt_window:\n                # keep it as a sliding window\n                recent_success.pop(0)\n\n            adapt_counter += 1\n\n            # adapt gscale every adapt_window evaluations\n            if adapt_counter >= self.adapt_window:\n                adapt_counter = 0\n                succ_rate = np.mean(recent_success) if len(recent_success) > 0 else 0.0\n                # if success rate too low -> increase scale; if too high -> decrease\n                if succ_rate < self.target_success:\n                    factor = 1.08 + 0.04 * (rng.rand() - 0.5)\n                else:\n                    factor = 0.92 + 0.04 * (rng.rand() - 0.5)\n                gscale *= factor\n                # gentle jitter\n                gscale *= np.exp(0.03 * (rng.rand() - 0.5))\n                gscale = float(np.clip(gscale, 1e-6, 1.0))\n                # reset success window\n                recent_success = []\n\n            # stagnation detection & micro-restarts\n            stagnation_evals = evals - last_improvement_eval\n            stagnation_threshold = max(200, 30 * dim)\n            if stagnation_evals > stagnation_threshold and evals < budget:\n                # perform a few small micro restarts: inject perturbed elites or randoms\n                n_micro = min(5, max(1, dim // 2))\n                for _ in range(n_micro):\n                    if evals >= budget:\n                        break\n                    if rng.rand() < 0.6 and len(elites) > 0:\n                        # perturb an elite using PCA local noise\n                        e = elites[rng.randint(len(elites))]\n                        perturb = per_dim_scale * (0.5 + rng.rand() * 0.5) * rng.randn(dim)\n                        x_new = self._reflect_bounds(e + perturb, lb, ub)\n                    else:\n                        # random restart in domain\n                        x_new = self._uniform_array(lb, ub)\n                    try:\n                        f_new = float(func(x_new))\n                    except Exception:\n                        f_new = np.inf\n                    evals += 1\n                    X = np.vstack([X, x_new])\n                    F = np.append(F, f_new)\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = f_new\n                        x_best = x_new.copy()\n                        last_improvement_eval = evals\n                # encourage exploration: slightly enlarge scale\n                gscale = float(min(1.0, gscale * (1.12 + 0.06 * rng.rand())))\n                # reset stagnation counter by updating last_improvement if we found better\n                # otherwise move last_improvement forward a bit to avoid immediate repeated restarts\n                if evals - last_improvement_eval < stagnation_threshold:\n                    pass\n                else:\n                    last_improvement_eval = evals - (stagnation_threshold // 2)\n\n            # loop continues until budget exhausted\n\n        # final safety: ensure we have at least one finite point and return best seen\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            # return best finite\n            idx = int(np.argmin(F[finite_idx]))\n            idx = int(finite_idx[idx])\n            f_best = float(F[idx])\n            x_best = X[idx].copy()\n        else:\n            # fallback to best tracked or center (no extra function calls)\n            if np.isfinite(f_best):\n                pass\n            else:\n                x_best = center.copy()\n                # Do not call func if budget exhausted; leave f_best as inf\n\n        return f_best, x_best", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "75f022a6-cb11-4ef2-be68-1f13f06c5b0c", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler mixing archive-biased differential donors, PCA-guided elite moves, anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_archive=20, max_archive=200, adapt_window=50,\n                 target_success=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_archive = int(init_archive)\n        self.max_archive = int(max_archive)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.rng = np.random.RandomState(seed)\n\n    # ----- helper utilities -----\n    def _extract_bounds(self, func):\n        # Attempt to fetch bounds from func; otherwise default to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = np.asarray(b.lb)\n                ub = np.asarray(b.ub)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n        else:\n            # broadcast scalars\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n            lb = lb.astype(float)\n            ub = ub.astype(float)\n            # ensure shapes match dim\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim)\n\n        # enforce finite and reasonable\n        lb = np.where(np.isfinite(lb), lb, -5.0)\n        ub = np.where(np.isfinite(ub), ub, 5.0)\n        for i in range(self.dim):\n            if not (lb[i] < ub[i]):\n                lb[i] = -5.0\n                ub[i] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        # coordinate-wise reflect; if still out of bounds after attempts, clamp\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if np.all((x >= lb) & (x <= ub)):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        rng = self.rng\n        dim = self.dim\n        budget = max(1, int(self.budget))\n        total_range = ub - lb\n        center = (lb + ub) / 2.0\n\n        # archive\n        X_list = []\n        F_list = []\n\n        evals = 0\n\n        # small helper to safely evaluate (counts toward budget)\n        def _safe_eval(x):\n            nonlocal evals\n            # Ensure we don't exceed budget here (caller must check)\n            try:\n                f = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                f = np.inf\n            evals += 1\n            return f\n\n        # initial sampling\n        n_init = min(self.init_archive, budget)\n        # stratified-ish initialization (Latin-hypercube style)\n        for i in range(n_init):\n            if evals >= budget:\n                break\n            # stratified shift\n            u = (rng.rand(dim) + i) / float(n_init)\n            x = lb + (u % 1.0) * total_range\n            x = self._reflect_bounds(x, lb, ub)\n            f = _safe_eval(x)\n            X_list.append(x.copy())\n            F_list.append(f)\n\n        # if archive empty, seed with center and a random point (if budget allows)\n        if len(X_list) == 0 and evals < budget:\n            x0 = center.copy()\n            f0 = _safe_eval(x0)\n            X_list.append(x0.copy())\n            F_list.append(f0)\n        if len(X_list) == 1 and evals < budget:\n            x1 = self._uniform_array(lb, ub)\n            f1 = _safe_eval(x1)\n            X_list.append(x1.copy())\n            F_list.append(f1)\n\n        X = np.array(X_list, dtype=float)\n        F = np.array(F_list, dtype=float)\n\n        # initialize best trackers\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            best_idx = int(finite_idx[np.argmin(F[finite_idx])])\n        else:\n            best_idx = int(np.nanargmin(F)) if F.size > 0 else 0\n        x_best = X[best_idx].copy()\n        f_best = float(F[best_idx])\n\n        # short-term adaptation\n        recent_success = []\n        adapt_counter = 0\n        gscale = 0.08  # global scale (fraction of box)\n        last_improvement_eval = evals\n\n        # main loop\n        while evals < budget:\n            n_archive = len(F)\n            if n_archive == 0:\n                # sample a random point\n                if evals >= budget:\n                    break\n                x = self._uniform_array(lb, ub)\n                f = _safe_eval(x)\n                X = np.vstack([X, x]) if X.size else np.array([x])\n                F = np.append(F, f)\n                if np.isfinite(f) and f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                    last_improvement_eval = evals\n                continue\n\n            # enforce at least a few archive samples: if only 1-2 entries, inject randoms\n            if n_archive < 3 and evals + (3 - n_archive) <= budget:\n                needed = 3 - n_archive\n                for _ in range(needed):\n                    x = self._uniform_array(lb, ub)\n                    f = _safe_eval(x)\n                    X = np.vstack([X, x])\n                    F = np.append(F, f)\n                    if np.isfinite(f) and f < f_best:\n                        f_best = f\n                        x_best = x.copy()\n                        last_improvement_eval = evals\n                n_archive = len(F)\n\n            # ranking and elites\n            ranks = np.argsort(F)\n            # elites: top k\n            k_elite = min(max(3, dim), n_archive)\n            elite_idx = ranks[:k_elite]\n            elites = X[elite_idx]\n\n            # per-dim adaptive scale: blend global gscale*range with archive spread\n            spread = np.std(X, axis=0) if X.shape[0] > 1 else 0.5 * total_range\n            per_dim_scale = np.maximum(gscale * total_range, 0.5 * spread + 1e-12)\n\n            # choose base index biased to better ranks (exponential bias)\n            # better ranks (small F) get higher chance\n            scores = np.arange(n_archive)[np.argsort(np.argsort(-F))]  # rough\n            # simpler: use softmax on negative ranks\n            bias_raw = np.exp(-5.0 * (np.argsort(F) / max(1.0, n_archive - 1.0)))\n            probs = bias_raw / np.sum(bias_raw)\n            base_idx = int(rng.choice(n_archive, p=probs))\n            base = X[base_idx].copy()\n            f_base = F[base_idx]\n\n            # decide move type\n            move_rand = rng.rand()\n            # probabilities: LOCAL 35%, DE 30%, PCA 18%, CAUCHY 12%, ARCHIVE_MIX 5%\n            if move_rand < 0.35:\n                move = \"LOCAL\"\n            elif move_rand < 0.65:\n                move = \"DE\"\n            elif move_rand < 0.83:\n                move = \"PCA\"\n            elif move_rand < 0.95:\n                move = \"CAUCHY\"\n            else:\n                move = \"ARCHIVE_MIX\"\n\n            candidate = base.copy()\n            # LOCAL: anisotropic gaussian local move with multiplicative jitter & pull to center\n            if move == \"LOCAL\":\n                mult_jitter = 1.0 + 0.6 * (rng.rand(dim) - 0.5)\n                candidate += per_dim_scale * (0.6 * rng.randn(dim)) * mult_jitter\n                # small isotropic jitter\n                candidate += (gscale * np.mean(total_range)) * 0.03 * rng.randn(dim)\n                # pull to center a bit\n                candidate += 0.06 * rng.rand() * (center - base)\n\n            elif move == \"DE\":\n                # DE-style donor vector with binomial crossover\n                if n_archive >= 4:\n                    idxs = list(range(n_archive))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    x1, x2, x3 = X[a], X[b], X[c]\n                    Fscale = 0.5 + 0.6 * rng.rand()  # in (0.5,1.1)\n                    donor = x1 + Fscale * (x2 - x3)\n                    donor = self._reflect_bounds(donor, lb, ub)\n                    cr = 0.2 + 0.6 * rng.rand()\n                    candidate = base.copy()\n                    jrand = rng.randint(dim)\n                    mask = (rng.rand(dim) < cr)\n                    mask[jrand] = True\n                    candidate[mask] = donor[mask]\n                    # slight local jitter\n                    candidate += per_dim_scale * 0.08 * rng.randn(dim)\n                else:\n                    # fallback to local\n                    candidate = base + per_dim_scale * 0.5 * rng.randn(dim)\n\n            elif move == \"PCA\":\n                # PCA-guided sampling using elites\n                C = elites.copy()\n                if C.shape[0] <= 1:\n                    candidate = base + per_dim_scale * 0.6 * rng.randn(dim)\n                else:\n                    cov = np.cov(C.T)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        sort_idx = np.argsort(-eigvals)\n                        eigvals = np.maximum(eigvals[sort_idx], 1e-12)\n                        eigvecs = eigvecs[:, sort_idx]\n                        # emphasize leading axes\n                        emphasis = 1.0 + 2.0 * np.exp(-np.arange(dim) / max(1.0, dim / 3.0))\n                        coeffs = rng.randn(dim) * np.sqrt(eigvals) * np.sqrt(emphasis)\n                        perturb = eigvecs.dot(coeffs)\n                        candidate = base + 0.6 * perturb\n                        # small isotropic jitter\n                        candidate += (gscale * np.mean(total_range)) * 0.02 * rng.randn(dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * rng.randn(dim) * 0.5\n                # occasionally do a heavy-tailed directional jump after PCA\n                if rng.rand() < 0.28:\n                    rc = rng.standard_cauchy(size=dim)\n                    rc = np.clip(rc, -8.0, 8.0)\n                    if rng.rand() < 0.5:\n                        candidate = center + rc * (gscale * 4.0) * total_range\n                    else:\n                        candidate = base + rc * (gscale * 1.8) * total_range\n\n            elif move == \"CAUCHY\":\n                # heavy-tailed jump centered on base or occasionally global\n                rc = rng.standard_cauchy(size=dim)\n                rc = np.clip(rc, -10.0, 10.0)\n                if rng.rand() < 0.4:\n                    candidate = base + rc * (gscale * 1.2) * total_range\n                else:\n                    candidate = center + rc * (gscale * 6.0) * total_range\n\n            else:  # ARCHIVE_MIX\n                # average a couple of good points and jitter\n                if k_elite >= 2:\n                    i1, i2 = rng.choice(elite_idx, size=2, replace=False)\n                    candidate = 0.5 * (X[i1] + X[i2])\n                else:\n                    candidate = base.copy()\n                candidate += per_dim_scale * 0.25 * rng.randn(dim)\n\n            # occasional archive mixing: mix some dims from a random archive member\n            if rng.rand() < 0.08 and len(X) > 0:\n                partner = X[rng.randint(len(X))]\n                mix_mask = rng.rand(dim) < 0.2\n                if mix_mask.sum() > 0:\n                    candidate[mix_mask] = partner[mix_mask] + 0.02 * total_range[mix_mask] * rng.randn(mix_mask.sum())\n\n            # safety: finite values and reflect to bounds\n            bad_mask = ~np.isfinite(candidate)\n            if np.any(bad_mask):\n                candidate[bad_mask] = lb[bad_mask] + rng.rand(np.sum(bad_mask)) * (ub[bad_mask] - lb[bad_mask])\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= budget:\n                break\n            f_candidate = _safe_eval(candidate)\n\n            # update archive\n            X = np.vstack([X, candidate])\n            F = np.append(F, f_candidate)\n\n            improved = False\n            if np.isfinite(f_candidate) and f_candidate < f_best:\n                f_best = f_candidate\n                x_best = candidate.copy()\n                last_improvement_eval = evals\n                improved = True\n\n            # prune archive if too large: keep top best and some randoms\n            if len(F) > self.max_archive:\n                keep_top = min(2 * dim, self.max_archive // 2)\n                top_idx = np.argsort(F)[:keep_top]\n                rest_idx = np.setdiff1d(np.arange(len(F)), top_idx)\n                n_needed = self.max_archive - keep_top\n                if n_needed > 0 and len(rest_idx) > 0:\n                    chosen_rest = rng.choice(rest_idx, size=min(n_needed, len(rest_idx)), replace=False)\n                    keep_idx = np.concatenate([top_idx, chosen_rest])\n                else:\n                    keep_idx = top_idx\n                keep_idx = np.unique(keep_idx)\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # success: improved global best or placed among top 25%\n            success = False\n            try:\n                if np.isfinite(f_candidate):\n                    q25 = np.percentile(F, 25)\n                    if f_candidate <= q25 or improved:\n                        success = True\n            except Exception:\n                success = improved\n\n            recent_success.append(bool(success))\n            # sliding window\n            if len(recent_success) > self.adapt_window:\n                recent_success.pop(0)\n\n            adapt_counter += 1\n            # adapt gscale occasionally\n            if adapt_counter >= self.adapt_window:\n                adapt_counter = 0\n                succ_rate = np.mean(recent_success) if len(recent_success) > 0 else 0.0\n                if succ_rate < self.target_success:\n                    factor = 1.06 + 0.03 * (rng.rand() - 0.5)\n                else:\n                    factor = 0.94 + 0.03 * (rng.rand() - 0.5)\n                gscale = float(np.clip(gscale * factor, 1e-6, 1.0))\n                # gentle jitter to avoid getting stuck\n                gscale *= (1.0 + 0.02 * (rng.rand() - 0.5))\n                # shrink recent history a bit\n                if len(recent_success) > 0:\n                    recent_success = recent_success[-(self.adapt_window // 2):]\n\n            # stagnation detection & micro-restarts\n            stagnation_evals = evals - last_improvement_eval\n            stagnation_threshold = max(200, 30 * dim)\n            if stagnation_evals > stagnation_threshold and evals < budget:\n                # perform micro restarts: inject perturbed elites or randoms\n                n_micro = min(8, max(1, dim // 2))\n                for _ in range(n_micro):\n                    if evals >= budget:\n                        break\n                    if rng.rand() < 0.65 and len(elites) > 0:\n                        e = elites[rng.randint(len(elites))]\n                        perturb = per_dim_scale * (0.5 + rng.rand() * 0.7) * rng.randn(dim)\n                        x_new = self._reflect_bounds(e + perturb, lb, ub)\n                    else:\n                        x_new = self._uniform_array(lb, ub)\n                    f_new = _safe_eval(x_new)\n                    X = np.vstack([X, x_new])\n                    F = np.append(F, f_new)\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = f_new\n                        x_best = x_new.copy()\n                        last_improvement_eval = evals\n                # encourage exploration: slightly enlarge scale\n                gscale = float(min(1.0, gscale * (1.10 + 0.05 * rng.rand())))\n                # if still no improvement, move last_improvement forward to avoid immediate repeated restarts\n                if evals - last_improvement_eval > stagnation_threshold:\n                    last_improvement_eval = evals - (stagnation_threshold // 2)\n\n        # final safety: return best finite point\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            idx = int(finite_idx[np.argmin(F[finite_idx])])\n            f_best = float(F[idx])\n            x_best = X[idx].copy()\n        else:\n            # fallback: keep tracked best if finite else center\n            if not np.isfinite(f_best):\n                x_best = center.copy()\n\n        return f_best, x_best", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e3101a6e-dfd1-4a0d-9a9a-42b33c9b32d2", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler mixing archive-biased differential donors, PCA-guided elite moves, anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_archive=20, max_archive=200, adapt_window=50,\n                 target_success=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_archive = int(init_archive)\n        self.max_archive = int(max_archive)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.rng = np.random.RandomState(seed)\n\n    # ----- helper utilities -----\n    def _extract_bounds(self, func):\n        # Attempt to fetch bounds from func; otherwise default to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # try several possible attribute patterns\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb)\n                    ub = np.asarray(b.ub)\n                elif hasattr(b, \"lower\") and hasattr(b, \"upper\"):\n                    lb = np.asarray(b.lower)\n                    ub = np.asarray(b.upper)\n                else:\n                    # maybe b is a tuple/list (lb, ub)\n                    try:\n                        lb, ub = b\n                        lb = np.asarray(lb)\n                        ub = np.asarray(ub)\n                    except Exception:\n                        lb = None\n                        ub = None\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n        else:\n            # broadcast scalars\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n            lb = lb.astype(float)\n            ub = ub.astype(float)\n            # ensure shapes match dim\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim)\n\n        # enforce finite and reasonable\n        lb = np.where(np.isfinite(lb), lb, -5.0)\n        ub = np.where(np.isfinite(ub), ub, 5.0)\n        for i in range(self.dim):\n            if not (lb[i] < ub[i]):\n                lb[i] = -5.0\n                ub[i] = 5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=20):\n        # coordinate-wise reflect; if still out of bounds after attempts, clamp\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            if not np.any(below) and not np.any(x > ub):\n                break\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        rng = self.rng\n        lb, ub = self._extract_bounds(func)\n        total_range = ub - lb\n        center = 0.5 * (lb + ub)\n\n        # archive\n        X_list = []\n        F_list = []\n\n        evals = 0\n\n        # small helper to safely evaluate (counts toward budget)\n        def _safe_eval(x):\n            nonlocal evals\n            # Ensure we don't exceed budget here (caller must check)\n            try:\n                f = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                f = np.inf\n            evals += 1\n            return f\n\n        # initial sampling\n        n_init = min(self.init_archive, self.budget)\n        # stratified-ish initialization (Latin-hypercube style)\n        if n_init > 0:\n            # For each dimension create a random permutation of strata\n            strata = np.zeros((n_init, self.dim))\n            for d in range(self.dim):\n                perm = rng.permutation(n_init)\n                # offsets in (0,1)\n                offsets = rng.rand(n_init)\n                strata[:, d] = (perm + offsets) / float(n_init)\n            for i in range(n_init):\n                if evals >= self.budget:\n                    break\n                u = strata[i]\n                x = lb + u * total_range\n                f = _safe_eval(x)\n                X_list.append(x.copy())\n                F_list.append(f)\n\n        # if archive empty, seed with center and a random point (if budget allows)\n        if len(X_list) == 0 and evals < self.budget:\n            x0 = center.copy()\n            f0 = _safe_eval(x0)\n            X_list.append(x0.copy())\n            F_list.append(f0)\n        if len(X_list) == 1 and evals < self.budget:\n            x1 = self._uniform_array(lb, ub)\n            f1 = _safe_eval(x1)\n            X_list.append(x1.copy())\n            F_list.append(f1)\n\n        X = np.array(X_list, dtype=float) if len(X_list) > 0 else np.zeros((0, self.dim))\n        F = np.array(F_list, dtype=float) if len(F_list) > 0 else np.array([], dtype=float)\n\n        # initialize best trackers\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            best_idx = finite_idx[np.argmin(F[finite_idx])]\n            x_best = X[best_idx].copy()\n            f_best = float(F[best_idx])\n            last_improvement_eval = evals\n        else:\n            # no finite evals yet\n            x_best = center.copy()\n            f_best = np.inf\n            last_improvement_eval = 0\n\n        # short-term adaptation\n        recent_success = []\n        adapt_counter = 0\n        gscale = 0.08  # global scale (fraction of box)\n\n        # main loop\n        while evals < self.budget:\n            n_archive = len(F)\n            if n_archive == 0:\n                # sample a random point\n                if evals >= self.budget:\n                    break\n                x = self._uniform_array(lb, ub)\n                f = _safe_eval(x)\n                X = np.vstack([X, x]) if X.size else np.array([x])\n                F = np.append(F, f)\n                if np.isfinite(f) and f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                    last_improvement_eval = evals\n                continue\n\n            # ensure shapes\n            if X.ndim == 1:\n                X = X.reshape(1, -1)\n            # ranking and elites\n            # ranks: ascending fitness\n            sorted_idx = np.argsort(F)\n            ranks = np.empty_like(sorted_idx)\n            ranks[sorted_idx] = np.arange(len(sorted_idx))\n            k_elite = max(1, int(0.15 * n_archive))  # top 15% as elites, at least 1\n            elite_idx = sorted_idx[:k_elite]\n            elites = X[elite_idx] if len(elite_idx) > 0 else np.zeros((0, self.dim))\n\n            # per-dim adaptive scale: blend global gscale*range with archive spread\n            spread = np.std(X, axis=0) if X.shape[0] > 1 else 0.5 * total_range\n            per_dim_scale = (gscale * total_range) * (0.6 + 0.8 * (spread / (total_range + 1e-12)))\n\n            # choose base index biased to better ranks (exponential bias)\n            bias_raw = np.exp(-4.0 * (ranks.astype(float) / max(1.0, n_archive - 1.0)))\n            probs = bias_raw / np.sum(bias_raw)\n            base_idx = rng.choice(n_archive, p=probs)\n            base = X[base_idx].copy()\n            f_base = F[base_idx]\n\n            # decide move type\n            move_rand = rng.rand()\n            # probabilities: LOCAL 35%, DE 30%, PCA 18%, CAUCHY 12%, ARCHIVE_MIX 5%\n            if move_rand < 0.35:\n                move = \"LOCAL\"\n            elif move_rand < 0.65:\n                move = \"DE\"\n            elif move_rand < 0.83:\n                move = \"PCA\"\n            elif move_rand < 0.95:\n                move = \"CAUCHY\"\n            else:\n                move = \"ARCHIVE_MIX\"\n\n            candidate = base.copy()\n            f_candidate = np.inf\n\n            # LOCAL: anisotropic gaussian local move with multiplicative jitter & pull to center\n            if move == \"LOCAL\":\n                mult_jitter = 1.0 + 0.6 * (rng.rand(self.dim) - 0.5)\n                candidate += per_dim_scale * (0.6 * rng.randn(self.dim)) * mult_jitter\n                # small isotropic jitter\n                candidate += 0.02 * np.mean(total_range) * rng.randn(self.dim)\n                # pull to center a bit\n                candidate += 0.02 * (center - base) * (rng.rand(self.dim))\n\n            elif move == \"DE\":\n                # DE-style donor vector with binomial crossover\n                if n_archive >= 4:\n                    idxs = list(range(n_archive))\n                    for _ in ():\n                        pass\n                    # pick three distinct indices different from base_idx\n                    choices = [i for i in range(n_archive) if i != base_idx]\n                    i1, i2, i3 = rng.choice(choices, size=3, replace=False)\n                    x1 = X[i1]; x2 = X[i2]; x3 = X[i3]\n                    Fscale = 0.5 + 0.6 * rng.rand()  # in (0.5,1.1)\n                    donor = x1 + Fscale * (x2 - x3)\n                    donor = self._reflect_bounds(donor, lb, ub)\n                    cr = 0.2 + 0.6 * rng.rand()\n                    trial = base.copy()\n                    jrand = rng.randint(self.dim)\n                    mask = (rng.rand(self.dim) < cr)\n                    mask[jrand] = True\n                    trial[mask] = donor[mask]\n                    # slight local jitter\n                    trial += per_dim_scale * 0.08 * rng.randn(self.dim)\n                    candidate = trial\n                else:\n                    # fallback to local\n                    candidate = base + per_dim_scale * 0.5 * rng.randn(self.dim)\n\n            elif move == \"PCA\":\n                # PCA-guided sampling using elites\n                C = elites.copy()\n                if C.shape[0] <= 1:\n                    candidate = base + per_dim_scale * 0.6 * rng.randn(self.dim)\n                else:\n                    cov = np.cov(C.T)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                        sort_idx = np.argsort(-eigvals)\n                        eigvals = eigvals[sort_idx]\n                        eigvecs = eigvecs[:, sort_idx]\n                        # emphasize leading axes\n                        emphasis = 1.0 + 2.0 * np.exp(-np.arange(self.dim) / max(1.0, self.dim / 3.0))\n                        coeffs = rng.randn(self.dim) * np.sqrt(np.maximum(eigvals, 0.0)) * np.sqrt(emphasis)\n                        perturb = eigvecs.dot(coeffs)\n                        candidate = base + 0.6 * (perturb / (np.linalg.norm(perturb) + 1e-12)) * (gscale * np.mean(total_range))\n                        # small isotropic jitter\n                        candidate += 0.03 * np.mean(total_range) * rng.randn(self.dim)\n                    except Exception:\n                        candidate = base + per_dim_scale * rng.randn(self.dim) * 0.5\n                # occasionally do a heavy-tailed directional jump after PCA\n                if rng.rand() < 0.28:\n                    rc = rng.standard_cauchy(size=self.dim)\n                    rc = np.clip(rc, -8.0, 8.0)\n                    if rng.rand() < 0.5:\n                        candidate = center + rc * (gscale * 4.0) * total_range\n                    else:\n                        candidate = base + rc * (gscale * 1.8) * total_range\n\n            elif move == \"CAUCHY\":\n                # heavy-tailed jump centered on base or occasionally global\n                rc = rng.standard_cauchy(size=self.dim)\n                rc = np.clip(rc, -10.0, 10.0)\n                if rng.rand() < 0.4:\n                    candidate = base + rc * (gscale * 1.2) * total_range\n                else:\n                    # more global jump\n                    candidate = center + rc * (gscale * 3.0) * total_range\n\n            else:  # ARCHIVE_MIX\n                # average a couple of good points and jitter\n                if len(elite_idx) >= 2:\n                    i1, i2 = rng.choice(elite_idx, size=2, replace=False)\n                    candidate = 0.5 * (X[i1] + X[i2])\n                else:\n                    partner = X[rng.randint(len(X))]\n                    candidate = 0.5 * (partner + base)\n                candidate += per_dim_scale * 0.2 * rng.randn(self.dim)\n\n            # occasional archive mixing: mix some dims from a random archive member\n            if rng.rand() < 0.08 and len(X) > 0:\n                partner = X[rng.randint(len(X))]\n                mix_mask = (rng.rand(self.dim) < 0.25)\n                if np.any(mix_mask):\n                    candidate[mix_mask] = partner[mix_mask] + 0.02 * total_range[mix_mask] * rng.randn(np.sum(mix_mask))\n\n            # safety: finite values and reflect to bounds\n            bad_mask = ~np.isfinite(candidate)\n            if np.any(bad_mask):\n                candidate[bad_mask] = lb[bad_mask] + rng.rand(np.sum(bad_mask)) * (ub[bad_mask] - lb[bad_mask])\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            f_candidate = _safe_eval(candidate)\n\n            # update archive\n            X = np.vstack([X, candidate])\n            F = np.append(F, f_candidate)\n\n            improved = False\n            if np.isfinite(f_candidate) and f_candidate < f_best:\n                x_best = candidate.copy()\n                f_best = float(f_candidate)\n                last_improvement_eval = evals\n                improved = True\n\n            # prune archive if too large: keep top best and some randoms\n            if len(F) > self.max_archive:\n                top_k = max(2, int(0.5 * self.max_archive))\n                top_idx = np.argsort(F)[:top_k]\n                rest_idx = np.setdiff1d(np.arange(len(F)), top_idx)\n                n_needed = self.max_archive - top_k\n                if n_needed > 0 and len(rest_idx) > 0:\n                    chosen_rest = rng.choice(rest_idx, size=min(n_needed, len(rest_idx)), replace=False)\n                    keep_idx = np.concatenate([top_idx, chosen_rest])\n                else:\n                    keep_idx = top_idx\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # success: improved global best or placed among top 25%\n            success = False\n            try:\n                if np.isfinite(f_candidate):\n                    q25 = np.percentile(F, 25)\n                    if f_candidate <= q25 or improved:\n                        success = True\n            except Exception:\n                success = improved\n\n            recent_success.append(bool(success))\n            # sliding window\n            if len(recent_success) > self.adapt_window:\n                recent_success = recent_success[-self.adapt_window:]\n\n            adapt_counter += 1\n            # adapt gscale occasionally\n            if adapt_counter >= self.adapt_window:\n                adapt_counter = 0\n                succ_rate = np.mean(recent_success) if len(recent_success) > 0 else 0.0\n                if succ_rate < self.target_success:\n                    # enlarge scale to explore more\n                    factor = 1.06 + 0.03 * (rng.rand() - 0.5)\n                    gscale *= factor\n                else:\n                    # shrink a bit to exploit\n                    gscale *= (0.95 + 0.06 * rng.rand())\n                # gentle jitter to avoid getting stuck\n                gscale *= (1.0 + 0.02 * (rng.rand() - 0.5))\n                # keep gscale in [1e-6, 1.0]\n                gscale = float(min(1.0, max(1e-6, gscale)))\n                # shrink recent history a bit\n                if len(recent_success) > (self.adapt_window // 2):\n                    recent_success = recent_success[-(self.adapt_window // 2):]\n\n            # stagnation detection & micro-restarts\n            stagnation_evals = evals - last_improvement_eval\n            stagnation_threshold = max(200, 30 * self.dim)\n            if stagnation_evals > stagnation_threshold:\n                # perform micro restarts: inject perturbed elites or randoms\n                n_micro = min(8, max(1, self.dim // 2))\n                for _ in range(n_micro):\n                    if evals >= self.budget:\n                        break\n                    if rng.rand() < 0.65 and len(elites) > 0:\n                        e = elites[rng.randint(len(elites))]\n                        perturb = per_dim_scale * (0.5 + rng.rand() * 0.7) * rng.randn(self.dim)\n                        x_new = self._reflect_bounds(e + perturb, lb, ub)\n                    else:\n                        x_new = self._uniform_array(lb, ub)\n                    f_new = _safe_eval(x_new)\n                    X = np.vstack([X, x_new])\n                    F = np.append(F, f_new)\n                    if np.isfinite(f_new) and f_new < f_best:\n                        x_best = x_new.copy()\n                        f_best = float(f_new)\n                        last_improvement_eval = evals\n                # encourage exploration: slightly enlarge scale\n                gscale = float(min(1.0, gscale * (1.10 + 0.05 * rng.rand())))\n                # if still no improvement, move last_improvement forward to avoid immediate repeated restarts\n                if evals - last_improvement_eval > stagnation_threshold:\n                    last_improvement_eval = evals\n\n        # final safety: return best finite point\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            idx = finite_idx[np.argmin(F[finite_idx])]\n            x_best = X[idx].copy()\n            f_best = float(F[idx])\n        else:\n            # fallback: keep tracked best if finite else center\n            if not np.isfinite(f_best):\n                x_best = center.copy()\n\n        return f_best, x_best", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "eb3d4408-ff1d-4fc6-9666-547940d048a4", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a compact hybrid sampler combining archive-biased differential donors, PCA-guided elite moves, anisotropic Gaussian locals and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_archive=20, max_archive=200, adapt_window=50,\n                 target_success=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_archive = int(init_archive)\n        self.max_archive = int(max_archive)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n        self.rng = np.random.RandomState(seed)\n\n    # ----- helper utilities -----\n    def _extract_bounds(self, func):\n        # try to fetch bounds as func.bounds.lb/ub or func.bounds[0]/[1]\n        try:\n            lb = np.asarray(func.bounds.lb)\n            ub = np.asarray(func.bounds.ub)\n        except Exception:\n            try:\n                lb = np.asarray(func.bounds[0])\n                ub = np.asarray(func.bounds[1])\n            except Exception:\n                lb = None\n                ub = None\n\n        if lb is None or ub is None:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n        else:\n            # broadcast scalars\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim)\n\n        # enforce finite and sane defaults\n        lb = np.where(np.isfinite(lb), lb, -5.0)\n        ub = np.where(np.isfinite(ub), ub, 5.0)\n        for i in range(self.dim):\n            if not (lb[i] < ub[i]):\n                lb[i] = -5.0\n                ub[i] = 5.0\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflect: reflect out-of-bounds back into [lb,ub]\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        dim = self.dim\n        budget = max(1, int(self.budget))\n        total_range = ub - lb\n\n        # archive storage\n        X_list = []\n        F_list = []\n\n        evals = 0\n\n        # safe evaluation wrapper\n        def _safe_eval(x):\n            nonlocal evals\n            # Strict budget checking: do not call if already used up\n            if evals >= budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            return f\n\n        # initialization: stratified-ish (latin-hypercube style)\n        n_init = min(max(8, self.init_archive), max(8, budget // 10))\n        # ensure at least 2 initial points\n        n_init = max(2, n_init)\n        for i in range(n_init):\n            if evals >= budget:\n                break\n            # stratified shift across a single random permutation for each dim\n            u = (self.rng.rand(dim) + i) / float(n_init)\n            x = lb + u * (ub - lb)\n            # small shuffle to avoid lattice\n            x += (self.rng.rand(dim) - 0.5) * 0.02 * total_range\n            x = self._reflect_bounds(x, lb, ub)\n            f = _safe_eval(x)\n            X_list.append(x.copy())\n            F_list.append(f)\n\n        # ensure at least two distinct points\n        if len(X_list) < 2 and evals < budget:\n            x = self._uniform_array(lb, ub)\n            f = _safe_eval(x)\n            X_list.append(x.copy())\n            F_list.append(f)\n\n        X = np.array(X_list, dtype=float)\n        F = np.array(F_list, dtype=float)\n\n        # initialize best trackers\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            best_idx = int(finite_idx[np.argmin(F[finite_idx])])\n        else:\n            best_idx = 0\n        f_best = float(F[best_idx])\n        x_best = X[best_idx].copy()\n\n        # short-term adaptation\n        adapt_counter = 0\n        gscale = 0.08  # global scale as fraction of box\n        recent_success = deque(maxlen=self.adapt_window)\n        last_improvement_eval = evals\n        stagnation_evals = 0\n\n        # random helper alias\n        rng = self.rng\n\n        # main loop: generate candidates until budget used\n        while evals < budget:\n            n_archive = X.shape[0]\n\n            # if archive empty (should not happen), add a random\n            if n_archive == 0:\n                x = self._uniform_array(lb, ub)\n                f = _safe_eval(x)\n                X = np.vstack([X, x])\n                F = np.append(F, f)\n                if np.isfinite(f) and f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                continue\n\n            # if very small archive, ensure diversity by sampling randoms\n            if n_archive < 3 and evals < budget:\n                needed = 3 - n_archive\n                for _ in range(needed):\n                    if evals >= budget:\n                        break\n                    x = self._uniform_array(lb, ub)\n                    f = _safe_eval(x)\n                    X = np.vstack([X, x])\n                    F = np.append(F, f)\n                    if np.isfinite(f) and f < f_best:\n                        f_best = f\n                        x_best = x.copy()\n                n_archive = X.shape[0]\n\n            # compute ranking and elites\n            ranks = np.argsort(F)\n            # number of elites\n            k_elite = max(2, min(n_archive, dim))\n            elite_idx = ranks[:k_elite]\n            elites = X[elite_idx]\n\n            # per-dim adaptive scale: combine global and archive spread\n            spread = np.nanstd(X, axis=0)\n            per_dim_scale = np.maximum(gscale * total_range, 0.25 * spread + 1e-12)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            mask_finite = np.isfinite(F)\n            if np.any(mask_finite):\n                scores = np.zeros_like(F, dtype=float)\n                scores[mask_finite] = -F[mask_finite]\n                # numerical stability\n                scores = scores - np.nanmax(scores)\n                probs = np.exp(scores / (np.std(scores[mask_finite]) + 1e-9))\n                probs = probs / (np.sum(probs) + 1e-12)\n                base_idx = rng.choice(len(X), p=probs)\n            else:\n                base_idx = rng.randint(len(X))\n            base = X[base_idx].copy()\n            f_base = F[base_idx]\n\n            # decide move type by probabilities\n            move_rand = rng.rand()\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            if move_rand < 0.35:\n                move = \"LOCAL\"\n            elif move_rand < 0.60:\n                move = \"PCA\"\n            elif move_rand < 0.80:\n                move = \"DE\"\n            elif move_rand < 0.95:\n                move = \"CAUCHY\"\n            else:\n                move = \"ARCHIVE_MIX\"\n\n            candidate = base.copy()\n\n            # LOCAL: anisotropic gaussian local move with multiplicative jitter & pull to center\n            if move == \"LOCAL\":\n                # small pull towards center of box to encourage convergence\n                center = 0.5 * (lb + ub)\n                pull = 0.06 * rng.rand() * (center - base)\n                # anisotropic gaussian with per-dim scales and multiplicative jitter\n                jitter = per_dim_scale * (0.6 + rng.rand(dim) * 1.4)\n                candidate = base + jitter * rng.randn(dim) + pull\n                # occasionally add a small directional DE donor to escape small traps\n                if rng.rand() < 0.25 and n_archive >= 3:\n                    idxs = list(range(n_archive))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    donor = X[a] + (0.6 + 0.6 * rng.rand()) * (X[b] - X[c])\n                    cr = 0.2 + 0.6 * rng.rand()\n                    mask = (rng.rand(dim) < cr)\n                    if not np.any(mask):\n                        mask[rng.randint(dim)] = True\n                    candidate[mask] = donor[mask] + 0.3 * per_dim_scale[mask] * rng.randn(np.sum(mask))\n\n            elif move == \"DE\":\n                # Differential-like donor from archive with archive-biased sampling\n                if n_archive >= 3:\n                    idxs = list(range(n_archive))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    Fscale = 0.5 + 0.8 * rng.rand()  # (0.5,1.3)\n                    donor = X[a] + Fscale * (X[b] - X[c])\n                    donor = self._reflect_bounds(donor, lb, ub)\n                    # binomial crossover\n                    cr = 0.2 + 0.6 * rng.rand()\n                    jrand = rng.randint(dim)\n                    mask = (rng.rand(dim) < cr)\n                    mask[jrand] = True\n                    candidate = base.copy()\n                    candidate[mask] = donor[mask]\n                    # small local jitter\n                    candidate += 0.15 * per_dim_scale * rng.randn(dim)\n                else:\n                    # fallback to LOCAL behavior\n                    candidate = base + per_dim_scale * rng.randn(dim) * 0.5\n\n            elif move == \"PCA\":\n                # PCA-guided sampling using elites\n                try:\n                    C = elites - np.mean(elites, axis=0)\n                    if C.shape[0] >= 2:\n                        cov = np.cov(C.T)\n                        eigvals, eigvecs = np.linalg.eigh(cov + 1e-12 * np.eye(dim))\n                        # sort eigenpairs descending\n                        sort_idx = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[sort_idx]\n                        eigvecs = eigvecs[:, sort_idx]\n                        # emphasize leading axes\n                        emphasis = 1.0 + 2.0 * np.exp(-np.arange(dim) / max(1.0, dim / 3.0))\n                        coeffs = rng.randn(dim) * np.sqrt(np.maximum(eigvals, 1e-12)) * np.sqrt(emphasis)\n                        perturb = eigvecs.dot(coeffs)\n                        candidate = base + 0.6 * perturb\n                    else:\n                        candidate = base + per_dim_scale * rng.randn(dim) * 0.6\n                    # occasionally do a heavy-tailed directional jump after PCA\n                    if rng.rand() < 0.08:\n                        rc = rng.standard_cauchy(size=dim)\n                        candidate = base + rc * (gscale * 2.0) * total_range\n                except Exception:\n                    candidate = base + per_dim_scale * rng.randn(dim) * 0.5\n\n            elif move == \"CAUCHY\":\n                # heavy-tailed jump centered on base or global\n                rc = rng.standard_cauchy(size=dim)\n                if rng.rand() < 0.4:\n                    candidate = base + rc * (gscale * 1.2) * total_range\n                else:\n                    # occasionally do a global-ish cauchy\n                    candidate = 0.5 * (lb + ub) + rc * (gscale * 3.0) * total_range\n\n            else:  # ARCHIVE_MIX\n                # average a couple of good points and jitter\n                if k_elite >= 2:\n                    i1, i2 = rng.choice(elite_idx, size=2, replace=False)\n                    candidate = 0.5 * (X[i1] + X[i2]) + 0.2 * per_dim_scale * rng.randn(dim)\n                else:\n                    candidate = base + per_dim_scale * rng.randn(dim) * 0.8\n\n            # occasional archive mixing: mix some dims from a random archive member\n            if rng.rand() < 0.08 and n_archive > 0:\n                partner = X[rng.randint(n_archive)]\n                mix_mask = rng.rand(dim) < 0.2\n                candidate[mix_mask] = partner[mix_mask]\n\n            # safety: finite values and reflect to bounds\n            candidate[~np.isfinite(candidate)] = lb[~np.isfinite(candidate)] + rng.rand(np.sum(~np.isfinite(candidate))) * (ub[~np.isfinite(candidate)] - lb[~np.isfinite(candidate)])\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if evals >= budget:\n                break\n            f_candidate = _safe_eval(candidate)\n\n            # append to archive\n            X = np.vstack([X, candidate])\n            F = np.append(F, f_candidate)\n            n_archive = X.shape[0]\n\n            improved = False\n            if np.isfinite(f_candidate) and f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = candidate.copy()\n                last_improvement_eval = evals\n                improved = True\n                stagnation_evals = 0\n            else:\n                stagnation_evals += 1\n\n            # prune archive if too large: keep best ones and some randoms\n            if n_archive > self.max_archive:\n                # keep top best_n and some of the rest randomly to maintain diversity\n                top_n = max(self.max_archive // 2, dim + 5)\n                top_idx = np.argsort(F)[:top_n]\n                rest_idx = np.setdiff1d(np.arange(n_archive), top_idx)\n                n_needed = self.max_archive - top_n\n                if rest_idx.size > 0 and n_needed > 0:\n                    chosen_rest = rng.choice(rest_idx, size=min(n_needed, rest_idx.size), replace=False)\n                    keep_idx = np.concatenate([top_idx, chosen_rest])\n                else:\n                    keep_idx = top_idx\n                keep_idx = np.unique(keep_idx)\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # success: improved global best or placed among top 25%\n            try:\n                q25 = np.percentile(F[np.isfinite(F)], 25) if np.any(np.isfinite(F)) else np.inf\n                success = improved or (np.isfinite(f_candidate) and f_candidate <= q25)\n            except Exception:\n                success = improved\n\n            recent_success.append(bool(success))\n\n            # adapt gscale occasionally based on short-term success\n            adapt_counter += 1\n            if adapt_counter >= max(5, int(self.adapt_window / 5)):\n                adapt_counter = 0\n                if len(recent_success) > 0:\n                    succ_rate = float(np.mean(recent_success))\n                    if succ_rate < self.target_success:\n                        # increase exploratory scale\n                        factor = 1.04 + 0.06 * (rng.rand() - 0.5)\n                        gscale = min(0.9, gscale * factor)\n                    else:\n                        # reduce to focus search\n                        factor = 0.96 + 0.03 * (rng.rand() - 0.5)\n                        gscale = max(1e-4, gscale * factor)\n                    # gentle jitter to avoid locking\n                    gscale *= (1.0 + 0.01 * (rng.rand() - 0.5))\n\n            # stagnation detection & micro-restarts\n            stagnation_threshold = max(200, 30 * dim)\n            if (evals - last_improvement_eval) > stagnation_threshold and evals < budget:\n                # perform micro restarts: inject perturbed elites or randoms\n                n_micro = min(12, max(2, dim // 2))\n                for _ in range(n_micro):\n                    if evals >= budget:\n                        break\n                    if (rng.rand() < 0.7) and elites.shape[0] > 0:\n                        e = elites[rng.randint(elites.shape[0])]\n                        perturb = per_dim_scale * (0.4 + rng.rand() * 1.0) * rng.randn(dim)\n                        x_new = self._reflect_bounds(e + perturb, lb, ub)\n                        f_new = _safe_eval(x_new)\n                    else:\n                        x_new = self._uniform_array(lb, ub)\n                        f_new = _safe_eval(x_new)\n                    X = np.vstack([X, x_new])\n                    F = np.append(F, f_new)\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = x_new.copy()\n                        last_improvement_eval = evals\n                # encourage exploration: slightly enlarge scale\n                gscale = min(0.9, gscale * (1.0 + 0.25 * rng.rand()))\n                # move last_improvement forward artificially to avoid repeated immediate restarts\n                last_improvement_eval = evals - (stagnation_threshold // 3)\n\n        # final safety: return best finite point found, else return center\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            idx = int(finite_idx[np.argmin(F[finite_idx])])\n            f_best_final = float(F[idx])\n            x_best_final = X[idx].copy()\n        else:\n            f_best_final = f_best\n            x_best_final = x_best if x_best is not None else 0.5 * (lb + ub)\n\n        # ensure not None\n        if x_best_final is None:\n            x_best_final = 0.5 * (lb + ub)\n\n        return f_best_final, x_best_final", "configspace": "", "generation": 0, "feedback": "In the code, line 149, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.adapt_window)", "error": "In the code, line 149, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.adapt_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9ed64b59-e01a-4fc1-a367-7c67e5c53fb1", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — hybrid heuristic mixing archive-biased differential donors, PCA-guided elite moves, anisotropic Gaussian locals and heavy-tailed Cauchy jumps, with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 max_archive=200, init_archive=20, adapt_window=60, target_success=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.max_archive = int(max_archive)\n        self.init_archive = int(init_archive)\n        self.adapt_window = int(adapt_window)\n        self.target_success = float(target_success)\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # try different possible containers for bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            try:\n                b0 = func.bounds[0]\n                b1 = func.bounds[1]\n                lb = np.asarray(b0, dtype=float)\n                ub = np.asarray(b1, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        if lb is None or ub is None:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub =  5.0 * np.ones(self.dim, dtype=float)\n        else:\n            # broadcast scalars if necessary\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub), dtype=float)\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim)\n            # enforce finite and sane defaults\n            lb = np.where(np.isfinite(lb), lb, -5.0)\n            ub = np.where(np.isfinite(ub), ub, 5.0)\n            for i in range(self.dim):\n                if not (lb[i] < ub[i]):\n                    lb[i] = -5.0\n                    ub[i] = 5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        total_range = ub - lb\n\n        # archive storage\n        X = []     # list of points\n        F = []     # list of function values (may contain np.inf)\n        evals = 0\n\n        # safe evaluation wrapper\n        def _safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            try:\n                val = float(func(x))\n            except Exception:\n                val = np.inf\n            evals += 1\n            return val\n\n        # initialization: stratified-ish (latin-hypercube style)\n        n_init = min(max(4, self.init_archive), max(2, self.budget // 10))\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            # stratified shift across a single random permutation for each dim\n            u = (np.arange(self.dim, dtype=float) * 0.0 + (i + self.rng.rand(self.dim))) / float(n_init)\n            x = lb + u * total_range\n            # small shuffle to avoid lattice\n            x += (self.rng.rand(self.dim) - 0.5) * 0.02 * total_range\n            x = self._reflect_bounds(x, lb, ub)\n            f = _safe_eval(x)\n            X.append(x.copy())\n            F.append(f)\n\n        # ensure at least two points\n        if len(X) == 0:\n            x = self._uniform_array(lb, ub)\n            f = _safe_eval(x)\n            X.append(x.copy())\n            F.append(f)\n        if len(X) == 1 and evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = _safe_eval(x)\n            X.append(x.copy())\n            F.append(f)\n\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # initialize best trackers\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            best_idx = finite_idx[np.argmin(F[finite_idx])]\n            f_best = float(F[best_idx])\n            x_best = X[best_idx].copy()\n        else:\n            # no finite value found yet\n            f_best = np.inf\n            x_best = None\n\n        # short-term adaptation\n        adapt_counter = 0\n        gscale = 0.08  # global scale as fraction of box\n        last_improvement_eval = evals\n        recent_success = deque(maxlen=self.adapt_window)\n\n        stagnation_threshold = max(50, int(self.budget * 0.02), 5 * self.dim)\n\n        # main loop: generate candidates until budget used\n        while evals < self.budget:\n            n_archive = X.shape[0]\n\n            # compute ranking and elites\n            ranks = np.argsort(F)\n            k_elite = max(2, int(np.ceil(0.15 * n_archive)))\n            elite_idx = ranks[:k_elite]\n            elites = X[elite_idx]\n\n            # per-dim adaptive scale: combine global and archive spread\n            spread = np.nanstd(X, axis=0)\n            per_dim_scale = np.maximum(gscale * total_range, 0.25 * spread + 1e-12)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                scores = -F.copy()\n                # numerical stability\n                s = scores[finite_mask]\n                temp = (np.std(s) + 1e-9)\n                probs = np.exp((scores - np.max(scores[finite_mask])) / temp)\n                probs = probs * finite_mask\n                probs_sum = probs.sum()\n                if probs_sum <= 0 or not np.isfinite(probs_sum):\n                    probs = finite_mask.astype(float) / float(finite_mask.sum())\n                else:\n                    probs = probs / probs_sum\n                base_idx = self.rng.choice(len(X), p=probs)\n            else:\n                base_idx = self.rng.randint(len(X))\n            base = X[base_idx].copy()\n\n            # decide move type by probabilities\n            move_rand = self.rng.rand()\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            if move_rand < 0.35:\n                move = \"LOCAL\"\n            elif move_rand < 0.60:\n                move = \"PCA\"\n            elif move_rand < 0.80:\n                move = \"DE\"\n            elif move_rand < 0.95:\n                move = \"CAUCHY\"\n            else:\n                move = \"ARCHIVE_MIX\"\n\n            candidate = None\n\n            if move == \"LOCAL\":\n                # anisotropic gaussian local move with multiplicative jitter & pull to center\n                jitter_f = 0.8 + 0.6 * self.rng.rand()\n                jitter = per_dim_scale * jitter_f\n                center = 0.5 * (lb + ub)\n                pull = 0.06 * self.rng.rand() * (center - base)\n                candidate = base + jitter * self.rng.randn(self.dim) + pull\n                # occasionally add a small directional DE donor to escape small traps\n                if self.rng.rand() < 0.25 and n_archive >= 4:\n                    idxs = list(range(n_archive))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    if len(idxs) >= 3:\n                        a, b, c = self.rng.choice(idxs, size=3, replace=False)\n                        cr = 0.5 + 0.4 * self.rng.rand()\n                        donor = X[a] + (0.5 + 0.8 * self.rng.rand()) * (X[b] - X[c])\n                        donor = self._reflect_bounds(donor, lb, ub)\n                        mask = (self.rng.rand(self.dim) < cr)\n                        if np.any(mask):\n                            candidate[mask] = donor[mask] + 0.3 * per_dim_scale[mask] * self.rng.randn(np.sum(mask))\n\n            elif move == \"DE\":\n                # Differential-like donor from archive with archive-biased sampling\n                if n_archive >= 4:\n                    idxs = list(range(n_archive))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    # pick distinct\n                    a, b, c = self.rng.choice(idxs, size=3, replace=False)\n                    Fscale = 0.5 + 0.8 * self.rng.rand()  # (0.5,1.3)\n                    donor = X[a] + Fscale * (X[b] - X[c])\n                    donor = self._reflect_bounds(donor, lb, ub)\n                    # binomial crossover\n                    cr = 0.3 + 0.6 * self.rng.rand()\n                    jrand = self.rng.randint(self.dim)\n                    mask = (self.rng.rand(self.dim) < cr)\n                    mask[jrand] = True\n                    candidate = base.copy()\n                    candidate[mask] = donor[mask]\n                    # small local jitter fallback\n                    if self.rng.rand() < 0.15:\n                        candidate = base + 0.5 * per_dim_scale * self.rng.randn(self.dim)\n\n                else:\n                    # fallback to local\n                    candidate = base + per_dim_scale * (0.8 + 0.2 * self.rng.rand()) * self.rng.randn(self.dim)\n\n            elif move == \"PCA\":\n                # PCA-guided sampling using elites\n                try:\n                    C = elites - np.mean(elites, axis=0)\n                    if C.shape[0] >= 2:\n                        cov = np.cov(C.T)\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort descending\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # sample directional perturbation emphasizing top components\n                        n_top = max(1, min(self.dim, 1 + int(0.3 * eigvecs.shape[1])))\n                        weights = eigvals[:n_top] + 1e-12\n                        weights = weights / np.sum(weights)\n                        coeffs = self.rng.randn(n_top) * np.sqrt(weights) * (gscale * 2.0)\n                        perturb = eigvecs[:, :n_top].dot(coeffs)\n                        candidate = base + 0.6 * perturb + 0.3 * per_dim_scale * self.rng.randn(self.dim)\n                    else:\n                        candidate = base + per_dim_scale * self.rng.randn(self.dim)\n                    # occasionally heavy-tailed after PCA\n                    if self.rng.rand() < 0.08:\n                        rc = self.rng.standard_cauchy(self.dim)\n                        candidate = base + rc * (gscale * 2.5) * total_range\n                except Exception:\n                    candidate = base + per_dim_scale * self.rng.randn(self.dim)\n\n            elif move == \"CAUCHY\":\n                # heavy-tailed jump centered on base or global center\n                rc = self.rng.standard_cauchy(self.dim)\n                if self.rng.rand() < 0.3:\n                    candidate = base + rc * (gscale * 2.5) * total_range\n                else:\n                    # global-ish cauchy\n                    center = 0.5 * (lb + ub)\n                    candidate = center + rc * (gscale * 3.0) * total_range\n\n            else:  # ARCHIVE_MIX\n                if k_elite >= 2:\n                    i1, i2 = self.rng.choice(elite_idx, size=2, replace=False)\n                    partner = 0.5 * (X[i1] + X[i2])\n                    candidate = 0.5 * (base + partner) + 0.2 * per_dim_scale * self.rng.randn(self.dim)\n                else:\n                    candidate = base + per_dim_scale * self.rng.randn(self.dim)\n                # occasional archive mixing: mix some dims from a random archive member\n                if n_archive >= 2 and self.rng.rand() < 0.5:\n                    partner_idx = self.rng.randint(n_archive)\n                    partner = X[partner_idx]\n                    mix_mask = self.rng.rand(self.dim) < 0.2\n                    candidate[mix_mask] = partner[mix_mask]\n\n            # safety: finite values and reflect to bounds\n            if candidate is None:\n                candidate = self._uniform_array(lb, ub)\n            # replace any non-finite dims with uniform draws\n            bad = ~np.isfinite(candidate)\n            if np.any(bad):\n                candidate[bad] = lb[bad] + self.rng.rand(np.sum(bad)) * (ub[bad] - lb[bad])\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_candidate = _safe_eval(candidate)\n            # if budget exhausted and returned inf but evals might have reached budget, we still append to keep archive consistent\n            # append to archive\n            X = np.vstack([X, candidate.copy()])\n            F = np.append(F, float(f_candidate))\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_candidate) and f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = candidate.copy()\n                improved = True\n                last_improvement_eval = evals\n\n            # prune archive if too large: keep best ones and some randoms\n            if X.shape[0] > self.max_archive:\n                keep_top = int(max(2, 0.6 * self.max_archive))\n                top_idx = np.argsort(F)[:keep_top]\n                rest_idx = np.setdiff1d(np.arange(len(F)), top_idx)\n                n_needed = self.max_archive - top_idx.size\n                if rest_idx.size > 0 and n_needed > 0:\n                    chosen_rest = self.rng.choice(rest_idx, size=min(n_needed, rest_idx.size), replace=False)\n                    keep_idx = np.concatenate([top_idx, chosen_rest])\n                else:\n                    keep_idx = top_idx\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # success: improved global best or placed among top 25%\n            q25 = np.percentile(F[np.isfinite(F)], 25) if np.any(np.isfinite(F)) else np.inf\n            success = improved or (np.isfinite(f_candidate) and f_candidate <= q25)\n            recent_success.append(bool(success))\n\n            # adapt gscale occasionally based on short-term success\n            adapt_counter += 1\n            if adapt_counter >= max(5, int(self.adapt_window / 5)):\n                adapt_counter = 0\n                if len(recent_success) > 0:\n                    succ_rate = float(np.mean(recent_success))\n                    # if success below target, increase exploration, else reduce\n                    if succ_rate < self.target_success:\n                        factor = 1.05 + 0.10 * (self.rng.rand() - 0.5)\n                        gscale = min(0.9, gscale * factor)\n                    else:\n                        factor = 0.92 + 0.08 * (self.rng.rand() - 0.5)\n                        gscale = max(1e-4, gscale * factor)\n                    # gentle jitter to avoid locking\n                    gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n\n            # stagnation detection & micro-restarts\n            if (evals - last_improvement_eval) > stagnation_threshold and evals < self.budget:\n                # perform micro restarts: inject perturbed elites or randoms\n                n_micro = min(12, max(2, self.dim // 2))\n                for _ in range(n_micro):\n                    if evals >= self.budget:\n                        break\n                    if elites.shape[0] > 0 and self.rng.rand() < 0.7:\n                        e = elites[self.rng.randint(elites.shape[0])]\n                        perturb = (0.5 + self.rng.rand() * 1.5) * per_dim_scale * self.rng.randn(self.dim)\n                        x_new = self._reflect_bounds(e + perturb, lb, ub)\n                    else:\n                        x_new = self._uniform_array(lb, ub)\n                    f_new = _safe_eval(x_new)\n                    X = np.vstack([X, x_new.copy()])\n                    F = np.append(F, float(f_new))\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = x_new.copy()\n                        last_improvement_eval = evals\n                # encourage exploration: slightly enlarge scale\n                gscale = min(0.9, gscale * (1.0 + 0.25 * self.rng.rand()))\n                # move last_improvement forward artificially to avoid repeated immediate restarts\n                last_improvement_eval = evals - int(0.5 * stagnation_threshold)\n\n            # loop continues until budget consumed\n\n        # final safety: return best finite point found, else return center\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            f_best_final = f_best\n            x_best_final = x_best if x_best is not None else X[finite_idx[np.argmin(F[finite_idx])]].copy()\n        else:\n            f_best_final = np.inf\n            x_best_final = 0.5 * (lb + ub)\n\n        return f_best_final, x_best_final", "configspace": "", "generation": 0, "feedback": "In the code, line 152, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.adapt_window)", "error": "In the code, line 152, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.adapt_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b99f260a-f5b3-47f5-aecd-1fa74d971e86", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid sampler mixing archive-biased differential donors, PCA-guided elite directional moves, anisotropic Gaussian locals, heavy-tailed Cauchy jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 max_archive=500, adapt_window=50, stagnation_limit=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.max_archive = int(max_archive)\n        self.adapt_window = int(adapt_window)\n        self.stagnation_limit = int(stagnation_limit)\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # try different possible containers for bounds; default to [-5,5]^dim\n        try:\n            lb = getattr(func.bounds, \"lb\", None)\n            ub = getattr(func.bounds, \"ub\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            # Fall back to standard BBOB bounds\n            lb = -5.0\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # Broadcast scalars to vectors\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        else:\n            lb = np.resize(lb, self.dim)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n        else:\n            ub = np.resize(ub, self.dim)\n\n        # Ensure finite sensible defaults\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if lb[i] >= ub[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp if still out\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.uniform(lb, ub)\n\n    # safe evaluation: increments eval count and returns np.inf if exception\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        total_range = ub - lb\n\n        evals = 0\n        X = []   # list of np.ndarray points\n        F = []   # list of floats (may contain np.inf)\n\n        # safe eval subroutine\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            try:\n                val = float(func(x))\n            except Exception:\n                val = np.inf\n            evals += 1\n            return val\n\n        # initialization: modest stratified (latin-hypercube-ish)\n        n_init = min(max(4, 2 * self.dim), self.budget, 40)\n        # stratified per-dim sampling\n        if n_init > 0:\n            # create stratified indices\n            seq = np.linspace(0.0, 1.0, n_init + 1)[:-1]\n            base_per_dim = np.zeros((n_init, self.dim), dtype=float)\n            for d in range(self.dim):\n                perm = self.rng.permutation(n_init)\n                jitter = self.rng.random(n_init) / n_init\n                base_per_dim[:, d] = (perm + jitter)\n            base_per_dim = base_per_dim / float(n_init)\n            # scale to bounds\n            for i in range(n_init):\n                if evals >= self.budget:\n                    break\n                x = lb + base_per_dim[i] * total_range\n                f = safe_eval(x)\n                X.append(np.array(x, dtype=float))\n                F.append(float(f))\n\n        # ensure at least two points\n        if len(X) == 0:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X.append(x.copy())\n            F.append(float(f))\n        if len(X) == 1 and evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X.append(x.copy())\n            F.append(float(f))\n\n        # convert to arrays lazily when needed\n        X = [np.array(x, dtype=float) for x in X]\n        F = [float(f) for f in F]\n\n        # initialize best trackers\n        finite_idx = [i for i, v in enumerate(F) if np.isfinite(v)]\n        if len(finite_idx) > 0:\n            best_idx = finite_idx[int(np.argmin([F[i] for i in finite_idx]))]\n            x_best = X[best_idx].copy()\n            f_best = float(F[best_idx])\n            last_improvement_eval = evals\n        else:\n            x_best = None\n            f_best = np.inf\n            last_improvement_eval = 0\n\n        # short-term adaptation\n        adapt_counter = 0\n        gscale = 0.08  # global scale as fraction of box\n        recent_success = deque(maxlen=self.adapt_window)\n\n        # main loop\n        while evals < self.budget:\n            n_archive = len(X)\n            X_arr = np.vstack(X)\n            F_arr = np.asarray(F, dtype=float)\n\n            # per-dim adaptive scale: combine global and archive spread\n            spread = np.nanstd(X_arr, axis=0) if n_archive > 1 else np.zeros(self.dim)\n            per_dim_scale = np.maximum(gscale * total_range, 0.25 * spread + 1e-12)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_mask = np.isfinite(F_arr)\n            if np.any(finite_mask):\n                scores = -F_arr.copy()\n                s = scores[finite_mask]\n                temp = (np.std(s) + 1e-9)\n                # subtract max for stability\n                shifted = np.zeros_like(scores)\n                shifted[finite_mask] = (scores[finite_mask] - np.max(scores[finite_mask])) / temp\n                probs = np.exp(shifted)\n                probs = probs * finite_mask\n                probs_sum = probs.sum()\n                if probs_sum <= 0 or not np.isfinite(probs_sum):\n                    probs = finite_mask.astype(float) / float(finite_mask.sum())\n                else:\n                    probs = probs / probs_sum\n                base_idx = self.rng.choice(n_archive, p=probs)\n            else:\n                base_idx = self.rng.integers(n_archive)\n            base = X[base_idx].copy()\n\n            # rank-based elites\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                order = np.argsort(F_arr[finite_idx])\n                elites_idx = finite_idx[order[:max(2, int(np.ceil(0.2 * finite_idx.size)))]]\n                elites = X_arr[elites_idx, :]\n            else:\n                elites = X_arr[:min(2, n_archive), :]\n\n            # decide move type by probabilities\n            r = self.rng.random()\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            if r < 0.35:\n                move = \"LOCAL\"\n            elif r < 0.60:\n                move = \"PCA\"\n            elif r < 0.80:\n                move = \"DE\"\n            elif r < 0.95:\n                move = \"CAUCHY\"\n            else:\n                move = \"ARCHIVE_MIX\"\n\n            candidate = base.copy()\n\n            if move == \"LOCAL\":\n                # anisotropic gaussian local move with multiplicative jitter & pull to center\n                jitter_f = 0.8 + 0.6 * self.rng.random()\n                jitter = jitter_f * per_dim_scale * self.rng.standard_normal(self.dim)\n                center = 0.5 * (lb + ub)\n                pull = 0.06 * self.rng.random() * (center - base)\n                candidate = base + jitter + pull\n\n                # occasionally add a small directional DE donor to escape small traps\n                if self.rng.random() < 0.25 and n_archive >= 4:\n                    idxs = list(range(n_archive))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    if len(idxs) >= 3:\n                        r1, r2, r3 = self.rng.choice(idxs, size=3, replace=False)\n                        donor = X[r1] + 0.6 * (X[r2] - X[r3]) * (per_dim_scale / (per_dim_scale + 1e-12))\n                        donor = self._reflect_bounds(donor, lb, ub)\n                        cr = 0.3 + 0.6 * self.rng.random()\n                        mask = (self.rng.random(self.dim) < cr)\n                        jrand = self.rng.integers(self.dim)\n                        mask[jrand] = True\n                        candidate[mask] = donor[mask]\n                    else:\n                        # fallback local jitter\n                        candidate = base + 0.5 * per_dim_scale * self.rng.standard_normal(self.dim)\n\n            elif move == \"PCA\":\n                # PCA-guided sampling using elites\n                if elites.shape[0] >= 2:\n                    C = elites - np.mean(elites, axis=0)\n                    # covariance and eigen\n                    cov = np.cov(C, rowvar=False)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                    except Exception:\n                        eigvals = np.ones(self.dim) * 1e-6\n                        eigvecs = np.eye(self.dim)\n                    order_e = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order_e]\n                    eigvecs = eigvecs[:, order_e]\n                    n_top = max(1, min(self.dim, 1 + int(0.3 * eigvecs.shape[1])))\n                    top_vecs = eigvecs[:, :n_top]\n                    # sample coefficients emphasising top components\n                    coeffs = (np.sqrt(np.maximum(eigvals[:n_top], 0)) *\n                              (0.6 + 0.4 * self.rng.random(n_top)) *\n                              self.rng.standard_normal(n_top))\n                    perturb = top_vecs.dot(coeffs)\n                    candidate = base + 0.6 * perturb + 0.3 * per_dim_scale * self.rng.standard_normal(self.dim)\n                    # occasionally heavy-tailed after PCA\n                    if self.rng.random() < 0.08:\n                        candidate = base + per_dim_scale * self.rng.standard_cauchy(self.dim) * 0.5\n                else:\n                    # fallback to local\n                    jitter_f = 0.9 + 0.6 * self.rng.random()\n                    candidate = base + jitter_f * per_dim_scale * self.rng.standard_normal(self.dim)\n\n            elif move == \"DE\":\n                # Differential-like donor from archive with archive-biased sampling\n                if n_archive >= 4:\n                    idxs = list(range(n_archive))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    # pick r1, r2, r3 distinct\n                    r1, r2, r3 = self.rng.choice(idxs, size=3, replace=False)\n                    F_scale = 0.4 + 0.5 * self.rng.random()\n                    donor = X[r1] + F_scale * (X[r2] - X[r3])\n                    donor = self._reflect_bounds(donor, lb, ub)\n                    cr = 0.2 + 0.7 * self.rng.random()\n                    mask = (self.rng.random(self.dim) < cr)\n                    jrand = self.rng.integers(self.dim)\n                    mask[jrand] = True\n                    candidate = base.copy()\n                    candidate[mask] = donor[mask]\n                    # small local jitter fallback if donor is identical\n                    if np.allclose(candidate, base):\n                        candidate = base + 0.5 * per_dim_scale * self.rng.standard_normal(self.dim)\n                else:\n                    candidate = base + 0.6 * per_dim_scale * self.rng.standard_normal(self.dim)\n\n            elif move == \"CAUCHY\":\n                # heavy-tailed jump centered on base\n                scale = 0.08 + 0.5 * self.rng.random()\n                candidate = base + scale * per_dim_scale * self.rng.standard_cauchy(self.dim)\n                # occasional global-ish mixing with partner\n                if self.rng.random() < 0.3 and n_archive >= 1:\n                    partner_idx = self.rng.integers(n_archive)\n                    partner = X[partner_idx]\n                    candidate = 0.5 * (base + partner) + 0.2 * per_dim_scale * self.rng.standard_normal(self.dim)\n\n            elif move == \"ARCHIVE_MIX\":\n                if n_archive >= 1:\n                    partner_idx = self.rng.integers(n_archive)\n                    partner = X[partner_idx]\n                    mask = (self.rng.random(self.dim) < (0.2 + 0.4 * self.rng.random()))\n                    candidate = base.copy()\n                    candidate[mask] = partner[mask] + 0.25 * per_dim_scale[mask] * self.rng.standard_normal(np.sum(mask))\n                else:\n                    candidate = self._uniform_array(lb, ub)\n\n            # sanitize candidate: replace non-finite dims with uniform draws\n            bad = ~np.isfinite(candidate)\n            if np.any(bad):\n                candidate[bad] = self._uniform_array(lb[bad], ub[bad])\n\n            # reflect to bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate\n            if evals >= self.budget:\n                break\n            f_candidate = safe_eval(candidate)\n\n            # append to archive\n            X.append(candidate.copy())\n            F.append(float(f_candidate))\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_candidate) and f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = candidate.copy()\n                improved = True\n                last_improvement_eval = evals\n\n            # prune archive if too large: keep best ones and some randoms\n            if len(X) > self.max_archive:\n                X_arr = np.vstack(X)\n                F_arr = np.asarray(F, dtype=float)\n                finite_idx = np.where(np.isfinite(F_arr))[0]\n                # keep top 70% of archive (by finite value) and fill rest with random survivors\n                keep_best = int(np.ceil(0.7 * self.max_archive))\n                if finite_idx.size > 0:\n                    best_order = np.argsort(F_arr[finite_idx])\n                    top_idx = finite_idx[best_order[:min(keep_best, finite_idx.size)]]\n                else:\n                    top_idx = np.arange(min(keep_best, len(X)))\n                remaining = [i for i in range(len(X)) if i not in top_idx]\n                n_needed = self.max_archive - top_idx.size\n                if n_needed < 0:\n                    n_needed = 0\n                if len(remaining) > 0 and n_needed > 0:\n                    chosen = self.rng.choice(remaining, size=min(n_needed, len(remaining)), replace=False)\n                else:\n                    chosen = np.array([], dtype=int)\n                keep_idx = np.concatenate([np.array(top_idx, dtype=int), np.array(chosen, dtype=int)])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # success: improved global best or placed among top 25%\n            finite_vals = np.asarray([v for v in F if np.isfinite(v)], dtype=float)\n            q25 = np.percentile(finite_vals, 25) if finite_vals.size > 0 else np.inf\n            success = improved or (np.isfinite(f_candidate) and f_candidate <= q25)\n            recent_success.append(bool(success))\n\n            # adapt gscale occasionally based on short-term success\n            adapt_counter += 1\n            if adapt_counter >= max(5, self.adapt_window // 5):\n                adapt_counter = 0\n                if len(recent_success) > 0:\n                    succ_rate = float(np.mean(recent_success))\n                else:\n                    succ_rate = 0.0\n                if succ_rate < 0.15:\n                    factor = 1.03 + 0.06 * (self.rng.random() - 0.5)\n                else:\n                    factor = 0.94 + 0.06 * (self.rng.random() - 0.5)\n                gscale = max(1e-5, gscale * factor)\n                # small jitter\n                gscale *= (1.0 + 1e-3 * (self.rng.random() - 0.5))\n\n            # stagnation detection & micro-restarts\n            if (evals - last_improvement_eval) > self.stagnation_limit and evals < self.budget:\n                # perform micro-restart: inject perturbed elites or randoms\n                n_inject = min(8, self.budget - evals)\n                injected_any_improve = False\n                for _ in range(n_inject):\n                    if np.any(np.isfinite(F_arr)) and self.rng.random() < 0.7:\n                        # perturb an elite\n                        if 'elites' in locals() and elites.shape[0] > 0:\n                            e = elites[self.rng.integers(elites.shape[0])]\n                        else:\n                            e = X[self.rng.integers(len(X))]\n                        scale = max(0.05, 0.5 * gscale)\n                        x_new = e + scale * total_range * self.rng.standard_normal(self.dim)\n                    else:\n                        # random inject\n                        x_new = self._uniform_array(lb, ub)\n                    x_new = self._reflect_bounds(x_new, lb, ub)\n                    if evals >= self.budget:\n                        break\n                    f_new = safe_eval(x_new)\n                    X.append(x_new.copy())\n                    F.append(float(f_new))\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = x_new.copy()\n                        injected_any_improve = True\n                        last_improvement_eval = evals\n                # enlarge scale to encourage exploration after restart\n                gscale = min(1.0, gscale * (1.0 + 0.25 * self.rng.random()))\n                # advance last_improvement slightly to avoid immediate repeated restarts if there was improvement\n                if injected_any_improve:\n                    last_improvement_eval = evals\n\n        # final safety: return best finite point found, else return box center\n        if x_best is None:\n            center = 0.5 * (lb + ub)\n            # if we still have budget, attempt one evaluation at center\n            if evals < self.budget:\n                f_center = safe_eval(center)\n                if np.isfinite(f_center):\n                    return float(f_center), center\n            # else return center and infinite value as fallback\n            return np.inf, center\n        return float(f_best), x_best", "configspace": "", "generation": 0, "feedback": "In the code, line 159, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.adapt_window)", "error": "In the code, line 159, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.adapt_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e8025b80-5e9e-488d-a8b6-22ce2d2423ed", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-Guided Adaptive Directional Sampling (MG-ADS) — hybrid heuristic mixing archive-biased differential donors, PCA-guided elite moves, anisotropic Gaussian locals and heavy-tailed Cauchy jumps, with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_archive=None, max_archive=None,\n                 adapt_window=40, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # archive sizes\n        self.init_archive = init_archive if init_archive is not None else min(max(6, self.dim * 4), max(6, self.budget // 50))\n        self.max_archive = max_archive if max_archive is not None else max(50, 10 * self.dim)\n        # adaptation\n        self.adapt_window = int(adapt_window)\n        self.verbose = verbose\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # Prefer func.bounds if available, else assume [-5,5] per instruction\n        lb = None\n        ub = None\n        # try several common container names\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # try attributes lb/ub or low/high or tuple/list\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                elif hasattr(b, \"low\") and hasattr(b, \"high\"):\n                    lb = np.asarray(b.low, dtype=float)\n                    ub = np.asarray(b.high, dtype=float)\n                else:\n                    # maybe it's a tuple (lb, ub)\n                    try:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n                    except Exception:\n                        lb = None\n                        ub = None\n        except Exception:\n            lb = None\n            ub = None\n\n        # fallback to provided func.limits or attribute search\n        if lb is None or ub is None:\n            try:\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                pass\n\n        # default to [-5,5]\n        if lb is None or ub is None:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n\n        # broadcast scalars to vector\n        if lb.size == 1:\n            lb = np.resize(lb, self.dim)\n        if ub.size == 1:\n            ub = np.resize(ub, self.dim)\n\n        # ensure finite and lb<ub per coordinate\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if not (lb[i] < ub[i]):\n                # enforce sane defaults\n                lb[i] = -5.0\n                ub[i] = 5.0\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = x.copy()\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._extract_bounds(func)\n        total_range = ub - lb\n\n        # archive storage\n        X = []     # list of vectors\n        F = np.array([], dtype=float)     # array of function values (may contain np.inf)\n\n        # counters & trackers\n        evals = 0\n        f_best = np.inf\n        x_best = None\n        last_improvement_eval = 0\n\n        # safe evaluation wrapper (ensures budget respected)\n        def _safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            try:\n                # ensure numpy array of correct shape\n                xx = np.asarray(x, dtype=float)\n                if xx.ndim != 1:\n                    xx = xx.ravel()[:self.dim]\n                val = float(func(xx))\n            except Exception:\n                val = np.inf\n            evals += 1\n            return val\n\n        # initialization: stratified-ish (simple LHS-like)\n        n_init = int(min(max(4, self.init_archive), max(2, self.budget // 10)))\n        # create stratified positions along each dim\n        perm = [self.rng.permutation(n_init) for _ in range(self.dim)]\n        for i in range(n_init):\n            x = np.empty(self.dim, dtype=float)\n            for d in range(self.dim):\n                # stratified coordinate\n                frac = (perm[d][i] + self.rng.rand()) / float(n_init)\n                x[d] = lb[d] + frac * (ub[d] - lb[d])\n            # tiny jitter to avoid lattice regularity\n            x += (self.rng.rand(self.dim) - 0.5) * 0.02 * total_range\n            x = self._reflect_bounds(x, lb, ub)\n            fx = _safe_eval(x)\n            X.append(x.copy())\n            F = np.append(F, float(fx))\n            if np.isfinite(fx) and fx < f_best:\n                f_best = float(fx)\n                x_best = x.copy()\n                last_improvement_eval = evals\n\n            if evals >= self.budget:\n                break\n\n        # if nothing evaluated (budget small), return center\n        if evals == 0:\n            x_center = 0.5 * (lb + ub)\n            f_center = _safe_eval(x_center)\n            return float(f_center), x_center\n\n        # short-term adaptation variables\n        gscale = 0.2  # global multiplicative exploration scale (relative to range)\n        success_history = []  # booleans of recent proposals\n        stagnation_threshold = max(50, int(self.budget * 0.02), 5 * self.dim)\n\n        # main loop: generate candidates until budget used\n        while evals < self.budget:\n            n_archive = len(X)\n            if n_archive == 0:\n                # refill with a random point\n                x = self._uniform_array(lb, ub)\n                fx = _safe_eval(x)\n                X.append(x.copy()); F = np.append(F, float(fx))\n                if np.isfinite(fx) and fx < f_best:\n                    f_best = float(fx); x_best = x.copy(); last_improvement_eval = evals\n                continue\n\n            # compute ranking and elites\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                ranks = np.argsort(F)  # ascending, best first (np.inf will be at end)\n                # number of elites\n                n_elite = max(2, int(0.25 * n_archive))\n                elite_idx = ranks[:n_elite]\n            else:\n                elite_idx = np.arange(n_archive)\n\n            # per-dim adaptive scale: combine global and archive spread\n            X_arr = np.vstack(X)\n            spread = np.nanstd(X_arr, axis=0)\n            # avoid zero spread\n            spread = np.where(spread <= 0, total_range * 0.05, spread)\n            per_dim_scale = gscale * spread\n\n            # choose base index biased to better ranks (softmax on negative F)\n            probs = None\n            if np.any(finite_mask):\n                scores = -F.copy()\n                scores[~finite_mask] = np.min(scores[finite_mask]) - 1.0\n                # softmax with temperature scaled by std\n                s = scores - np.max(scores)\n                exp_s = np.exp(s / (np.std(s[finite_mask]) + 1e-9))\n                probs = exp_s / np.sum(exp_s)\n            else:\n                probs = np.ones(n_archive) / float(n_archive)\n            base_idx = int(self.rng.choice(np.arange(n_archive), p=probs))\n            base = X[base_idx].copy()\n\n            # decide move type by probabilities\n            r = self.rng.rand()\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            if r < 0.35:\n                move = \"LOCAL\"\n            elif r < 0.60:\n                move = \"PCA\"\n            elif r < 0.80:\n                move = \"DE\"\n            elif r < 0.95:\n                move = \"CAUCHY\"\n            else:\n                move = \"ARCHIVE_MIX\"\n\n            candidate = None\n\n            # LOCAL: anisotropic gaussian local move with multiplicative jitter & pull to center\n            if move == \"LOCAL\":\n                jitter = per_dim_scale * (1.0 + 0.5 * (self.rng.rand(self.dim) - 0.5))\n                candidate = base + self.rng.randn(self.dim) * jitter\n                # occasional small pull to center of mass to avoid drifting\n                if self.rng.rand() < 0.2 and n_archive >= 2:\n                    center = np.nanmean(X_arr, axis=0)\n                    pull = 0.1 * (center - base)\n                    candidate += pull\n                # occasional tiny directional DE donor\n                if self.rng.rand() < 0.12 and n_archive >= 4:\n                    ids = list(range(n_archive))\n                    if base_idx in ids:\n                        ids.remove(base_idx)\n                    a, b, c = self.rng.choice(ids, size=3, replace=False)\n                    Fscale = 0.8 + 0.6 * self.rng.rand()\n                    donor = X[a] + Fscale * (X[b] - X[c])\n                    mask = self.rng.rand(self.dim) < (0.1 + 0.3 * self.rng.rand())\n                    candidate[mask] = donor[mask]\n\n            # PCA-guided sampling using elites\n            elif move == \"PCA\":\n                if len(elite_idx) >= 2:\n                    try:\n                        elites = X_arr[elite_idx]\n                        # center elites\n                        m = np.mean(elites, axis=0)\n                        C = np.cov((elites - m).T)\n                        # eigendecomposition\n                        eigvals, eigvecs = np.linalg.eigh(C + np.eye(self.dim) * 1e-12)\n                        # emphasize top components\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        n_top = min(max(1, int(len(eigvals) * 0.5)), self.dim)\n                        weights = eigvals[:n_top] + 1e-12\n                        coeffs = self.rng.randn(n_top) * np.sqrt(weights) * (gscale * 2.0)\n                        perturb = eigvecs[:, :n_top].dot(coeffs)\n                        candidate = base + perturb\n                        # occasionally add small isotropic jitter\n                        candidate += self.rng.randn(self.dim) * (gscale * 0.3 * total_range)\n                    except Exception:\n                        candidate = base + self.rng.randn(self.dim) * per_dim_scale\n\n            # DE-like donor from archive with archive-biased sampling\n            elif move == \"DE\":\n                if n_archive >= 4:\n                    ids = list(range(n_archive))\n                    if base_idx in ids:\n                        ids.remove(base_idx)\n                    a, b, c = self.rng.choice(ids, size=3, replace=False)\n                    Fscale = 0.8 + 0.6 * self.rng.rand()\n                    donor = X[a] + Fscale * (X[b] - X[c])\n                    donor = self._reflect_bounds(donor, lb, ub)\n                    # binomial crossover\n                    cr = 0.3 + 0.6 * self.rng.rand()\n                    mask = self.rng.rand(self.dim) < cr\n                    if not np.any(mask):\n                        mask[self.rng.randint(self.dim)] = True\n                    candidate = base.copy()\n                    candidate[mask] = donor[mask]\n                    # small local jitter fallback\n                    if self.rng.rand() < 0.2:\n                        candidate += self.rng.randn(self.dim) * (per_dim_scale * 0.5)\n                else:\n                    candidate = base + self.rng.randn(self.dim) * per_dim_scale\n\n            # CAUCHY heavy-tailed jump\n            elif move == \"CAUCHY\":\n                # center may be base or global center\n                if self.rng.rand() < 0.5:\n                    center = base\n                else:\n                    center = np.nanmean(X_arr, axis=0)\n                # standard Cauchy using inverse CDF\n                u = self.rng.rand(self.dim)\n                rc = np.tan(np.pi * (u - 0.5))\n                scale = max(0.05, gscale) * total_range\n                candidate = center + rc * scale\n                # occasionally shrink to near base\n                if self.rng.rand() < 0.3:\n                    candidate = base + 0.4 * (candidate - base)\n\n            # Archive mixing: mix some dims from a random archive member or random draw\n            elif move == \"ARCHIVE_MIX\":\n                if n_archive >= 2 and self.rng.rand() < 0.8:\n                    a_idx = int(self.rng.choice(np.arange(n_archive)))\n                    mix = X[a_idx].copy()\n                    mix_mask = self.rng.rand(self.dim) < 0.2\n                    candidate = base.copy()\n                    candidate[mix_mask] = mix[mix_mask]\n                    candidate += self.rng.randn(self.dim) * (per_dim_scale * 0.3)\n                else:\n                    candidate = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite dims with uniform draws and reflect to bounds\n            if candidate is None or (not np.all(np.isfinite(candidate))):\n                # fill non-finite with uniform\n                cand = np.asarray(candidate, dtype=float) if candidate is not None else np.full(self.dim, np.nan)\n                bad = ~np.isfinite(cand)\n                if np.any(bad):\n                    cand[bad] = self._uniform_array(lb, ub)[bad]\n                candidate = cand\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_candidate = _safe_eval(candidate)\n            F = np.append(F, float(f_candidate))\n            X.append(candidate.copy())\n\n            improved = False\n            if np.isfinite(f_candidate) and f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = candidate.copy()\n                last_improvement_eval = evals\n                improved = True\n\n            # maintain success history for adaptation (consider top-25% or improvement)\n            if improved:\n                success_history.append(1)\n            else:\n                # success if candidate is among top 25% of finite ones\n                finite_vals = F[np.isfinite(F)]\n                if finite_vals.size > 0:\n                    q25 = np.percentile(finite_vals, 25)\n                    success_history.append(1 if (np.isfinite(f_candidate) and f_candidate <= q25) else 0)\n                else:\n                    success_history.append(0)\n            # keep adapt window size\n            if len(success_history) > self.adapt_window:\n                success_history.pop(0)\n\n            # prune archive if too large: keep best ones and some randoms\n            if len(X) > self.max_archive:\n                # keep top K and some random others to retain diversity\n                K = max(10, int(0.2 * self.max_archive))\n                keep_top = min(len(F), K)\n                top_idx = np.argsort(F)[:keep_top]\n                rest_idx = np.setdiff1d(np.arange(len(F)), top_idx)\n                n_needed = self.max_archive - keep_top\n                if n_needed > 0 and rest_idx.size > 0:\n                    chosen_rest = self.rng.choice(rest_idx, size=min(n_needed, rest_idx.size), replace=False)\n                    keep_idx = np.concatenate([top_idx, chosen_rest])\n                else:\n                    keep_idx = top_idx\n                keep_idx_sorted = np.sort(keep_idx)\n                X = [X[i] for i in keep_idx_sorted]\n                F = F[keep_idx_sorted]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) >= min(5, self.adapt_window):\n                succ_rate = float(np.sum(success_history)) / float(len(success_history))\n                target = 0.2\n                # if success below target, increase exploration; else shrink slightly\n                if succ_rate < target:\n                    factor = 1.07 + 0.03 * self.rng.rand()\n                    gscale = min(5.0, gscale * factor)\n                else:\n                    factor = 0.92 - 0.05 * self.rng.rand()\n                    gscale = max(1e-5, gscale * factor)\n                # small jitter to avoid locking\n                gscale *= (1.0 + (self.rng.rand() - 0.5) * 0.02)\n\n            # stagnation detection & micro-restarts\n            if (evals - last_improvement_eval) > stagnation_threshold and evals < self.budget:\n                # perform a micro-restart: replace a fraction of worst points with perturbed elites or randoms\n                n_replace = max(1, int(0.05 * max(10, len(X))))\n                # identify worst indices\n                worst_idx = np.argsort(F)[-n_replace:]\n                for wi in worst_idx:\n                    if self.rng.rand() < 0.6 and len(elite_idx) > 0:\n                        # perturb a random elite\n                        e = X_arr[self.rng.choice(elite_idx)]\n                        perturb = self.rng.randn(self.dim) * (0.5 * per_dim_scale + 1e-9)\n                        x_new = self._reflect_bounds(e + perturb, lb, ub)\n                    else:\n                        x_new = self._uniform_array(lb, ub)\n                    f_new = _safe_eval(x_new)\n                    X[wi] = x_new.copy()\n                    F[wi] = float(f_new)\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = x_new.copy()\n                        last_improvement_eval = evals\n                # encourage exploration: slightly enlarge scale\n                gscale = min(3.0, gscale * 1.12)\n                # move last_improvement forward artificially to avoid immediate repeated restarts\n                last_improvement_eval = evals - int(0.5 * stagnation_threshold)\n\n            # verbose quick logging\n            if self.verbose and evals % max(1, self.budget // 10) == 0:\n                print(f\"evals={evals}, best={f_best:.6g}, gscale={gscale:.4g}, archive={len(X)}\")\n\n        # final safety: return best finite point found, else return center\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            best_idx = int(np.argmin(F))\n            x_best_final = X[best_idx].copy()\n            f_best_final = float(F[best_idx])\n            return f_best_final, x_best_final\n        else:\n            x_center = 0.5 * (lb + ub)\n            f_center = _safe_eval(x_center)\n            return float(f_center), x_center", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "691ab75e-678a-4722-9ed2-f8b854368eac", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-Guided Adaptive Directional Sampling (MG-ADS) — a hybrid sampler mixing archive-biased differential donors, PCA-guided elite moves, anisotropic Gaussian locals and heavy-tailed Cauchy jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n\n    Usage:\n        sampler = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = sampler(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 max_archive=200, init_archive=20, adapt_window=20):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.max_archive = int(max_archive)\n        self.init_archive = int(init_archive)\n        self.adapt_window = int(adapt_window)\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # Try to extract bounds from func if available, else use [-5, 5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # Broadcast scalars or single-dim arrays to full dimension\n        if lb.shape == ():\n            lb = np.resize(lb, self.dim)\n        if ub.shape == ():\n            ub = np.resize(ub, self.dim)\n\n        # Ensure shapes\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        # Enforce finite bounds\n        lb = np.where(np.isfinite(lb), lb, -5.0)\n        ub = np.where(np.isfinite(ub), ub, 5.0)\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # Coordinate-wise reflection into [lb, ub]\n        # If a coordinate leaves bounds, reflect it across the violated boundary.\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp to ensure numerical safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.random(self.dim) * (ub - lb) + lb\n\n    # ----- main call -----\n    def __call__(self, func):\n        # Archive storage (lists for dynamic append)\n        X = []     # list of points (np.array)\n        F = []     # list of function values\n\n        lb, ub = self._extract_bounds(func)\n        total_range = ub - lb\n        if np.any(total_range <= 0):\n            total_range = np.where(total_range <= 0, 1.0, total_range)\n\n        evals = 0\n        exhausted = False\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            nonlocal evals, exhausted\n            if exhausted or evals >= self.budget:\n                exhausted = True\n                return np.inf\n            try:\n                val = float(func(x))\n            except Exception:\n                val = np.inf\n            evals += 1\n            if evals >= self.budget:\n                exhausted = True\n            return val\n\n        # initialization: a small stratified-ish sample\n        n_init = int(min(max(4, self.init_archive), max(2, max(2, self.budget // 10))))\n        # simple Latin-hypercube-like stratification\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        perm = [self.rng.permutation(n_init) for _ in range(self.dim)]\n        for i in range(n_init):\n            u = np.empty(self.dim)\n            for d in range(self.dim):\n                # sample uniformly within the i-th stratum but permuted across dims\n                j = perm[d][i]\n                u[d] = self.rng.random() * (strata[j + 1] - strata[j]) + strata[j]\n            x = lb + u * (ub - lb)\n            x = self._reflect_bounds(x, lb, ub)\n            f = _safe_eval(x)\n            X.append(x)\n            F.append(f)\n            if exhausted:\n                break\n\n        # ensure at least two points if budget allows\n        while len(X) < 2 and not exhausted:\n            x = self._uniform_array(lb, ub)\n            x = self._reflect_bounds(x, lb, ub)\n            f = _safe_eval(x)\n            X.append(x)\n            F.append(f)\n\n        if len(X) == 0:\n            # no evaluations possible - return center\n            x0 = 0.5 * (lb + ub)\n            return np.inf, x0\n\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # initialize best trackers\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            best_idx = finite_idx[np.argmin(F[finite_idx])]\n            f_best = float(F[best_idx])\n            x_best = X[best_idx].copy()\n        else:\n            f_best = np.inf\n            x_best = 0.5 * (lb + ub)\n\n        # short-term adaptation\n        recent_success = deque(maxlen=self.adapt_window)\n        gscale = 0.2  # global multiplicative scale (relative)\n        adapt_counter = 0\n        last_improvement_eval = evals\n\n        # main loop: generate candidates until budget used\n        while evals < self.budget:\n            n_archive = len(F)\n            # compute ranking and elites\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                ranks = np.argsort(F[finite_mask])\n                finite_idx = np.where(finite_mask)[0]\n                ranked_idx = finite_idx[ranks]\n            else:\n                ranked_idx = np.arange(n_archive)\n\n            n_elites = max(2, int(0.25 * n_archive)) if n_archive > 0 else 0\n            elites_idx = ranked_idx[:n_elites] if n_elites > 0 else np.array([], dtype=int)\n\n            # per-dim adaptive scale: combine global and archive spread\n            if n_archive > 1:\n                per_dim_spread = np.std(X, axis=0, ddof=1)\n                # stabilize\n                per_dim_spread = np.where(per_dim_spread <= 1e-12, (ub - lb) * 0.02, per_dim_spread)\n            else:\n                per_dim_spread = (ub - lb) * 0.25\n\n            per_dim_scale = gscale * (per_dim_spread + 1e-12)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            base_idx = None\n            if n_archive > 0 and np.any(np.isfinite(F)):\n                vals = np.array(F, dtype=float)\n                mask = np.isfinite(vals)\n                tmp = -vals[mask]\n                # numeric stability softmax\n                tmp = tmp - np.max(tmp)\n                probs = np.exp(tmp / max(1e-6, np.std(tmp) + 1e-6))\n                probs = probs / (np.sum(probs) + 1e-12)\n                # map to indices\n                masked_indices = np.where(mask)[0]\n                try:\n                    base_idx = self.rng.choice(masked_indices, p=probs)\n                except Exception:\n                    base_idx = int(self.rng.integers(0, n_archive))\n            elif n_archive > 0:\n                base_idx = int(self.rng.integers(0, n_archive))\n            else:\n                base_idx = None\n\n            if base_idx is None:\n                base = self._uniform_array(lb, ub)\n            else:\n                base = X[base_idx].copy()\n\n            # decide move type by probabilities\n            move_rand = self.rng.random()\n            candidate = None\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            if move_rand < 0.35:\n                # anisotropic gaussian local move with multiplicative jitter & pull to center\n                jitter = self.rng.normal(0.0, 1.0, self.dim)\n                # multiplicative per-dim scaling\n                candidate = base + per_dim_scale * jitter\n                # small pull to center to encourage modest bias to global center\n                center = np.mean(X, axis=0) if n_archive > 0 else 0.5 * (lb + ub)\n                pull = 0.06 * (center - base)\n                candidate += pull\n                # occasional DE-like directional small donor\n                if n_archive >= 4 and self.rng.random() < 0.12:\n                    idxs = list(range(n_archive))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    if len(idxs) >= 3:\n                        a, b, c = self.rng.choice(idxs, size=3, replace=False)\n                        donor = X[a] + 0.5 * (X[b] - X[c])\n                        cr = 0.3 + 0.4 * self.rng.random()\n                        mask = (self.rng.random(self.dim) < cr)\n                        candidate[mask] = donor[mask]\n\n            elif move_rand < 0.35 + 0.25:\n                # PCA-guided sampling using elites\n                if n_elites >= 2:\n                    elites = X[elites_idx]\n                    mean_e = np.mean(elites, axis=0)\n                    C = np.cov((elites - mean_e).T) + 1e-9 * np.eye(self.dim)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        # order descending\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # sample in principal component space: emphasize top components\n                        z = np.zeros(self.dim)\n                        # give larger weight to the first few components\n                        component_weights = (eigvals / (np.sum(eigvals) + 1e-12))\n                        for i in range(self.dim):\n                            # sample normal scaled by sqrt(eigval) and component weight\n                            z += (np.sqrt(max(eigvals[i], 0.0)) * self.rng.normal()) * eigvecs[:, i] * (0.8 + 0.5 * component_weights[i])\n                        perturb = (0.6 * gscale) * z\n                        candidate = base + perturb + 0.25 * per_dim_scale * self.rng.normal(size=self.dim)\n                        # occasionally heavy-tailed after PCA\n                        if self.rng.random() < 0.06:\n                            rc = self.rng.standard_cauchy(self.dim)\n                            rc = np.clip(rc, -20, 20)\n                            candidate = base + rc * (gscale * 2.5) * total_range\n                    except Exception:\n                        # fallback to local noisy move\n                        candidate = base + per_dim_scale * self.rng.normal(size=self.dim)\n                else:\n                    candidate = base + per_dim_scale * self.rng.normal(size=self.dim)\n\n            elif move_rand < 0.35 + 0.25 + 0.20:\n                # Differential-like donor from archive with archive-biased sampling\n                if n_archive >= 4:\n                    ids = list(range(n_archive))\n                    if base_idx in ids:\n                        ids.remove(base_idx)\n                    if len(ids) >= 3:\n                        a, b, c = self.rng.choice(ids, size=3, replace=False)\n                        Fscale = 0.6 + 0.6 * self.rng.random()\n                        donor = X[a] + Fscale * (X[b] - X[c])\n                        cr = 0.2 + 0.6 * self.rng.random()\n                        mask = (self.rng.random(self.dim) < cr)\n                        candidate = base.copy()\n                        candidate[mask] = donor[mask]\n                        # ensure some jitter\n                        candidate += 0.05 * per_dim_scale * self.rng.normal(size=self.dim)\n                    else:\n                        candidate = base + per_dim_scale * self.rng.normal(size=self.dim)\n                else:\n                    candidate = base + per_dim_scale * self.rng.normal(size=self.dim)\n\n            elif move_rand < 0.35 + 0.25 + 0.20 + 0.15:\n                # heavy-tailed jump centered on base or global center\n                rc = self.rng.standard_cauchy(self.dim)\n                rc = np.clip(rc, -30, 30)\n                if self.rng.random() < 0.6:\n                    candidate = base + rc * (gscale * 2.5) * total_range\n                else:\n                    # global-ish cauchy between base and center\n                    center = np.mean(X, axis=0) if n_archive > 0 else 0.5 * (lb + ub)\n                    partner = center\n                    candidate = 0.5 * (base + partner) + 0.2 * per_dim_scale * self.rng.normal(size=self.dim)\n\n            else:\n                # occasional archive mixing: mix some dims from a random archive member\n                if n_archive >= 1:\n                    partner_idx = int(self.rng.integers(0, n_archive))\n                    partner = X[partner_idx]\n                    candidate = 0.5 * (base + partner) + 0.2 * per_dim_scale * self.rng.normal(size=self.dim)\n                    mix_mask = self.rng.random(self.dim) < 0.2\n                    candidate[mix_mask] = partner[mix_mask]\n                else:\n                    candidate = base + per_dim_scale * self.rng.normal(size=self.dim)\n\n            # safety: finite values and reflect to bounds\n            if candidate is None:\n                candidate = self._uniform_array(lb, ub)\n            # replace any non-finite dims with uniform draws in bounds\n            bad = ~np.isfinite(candidate)\n            if np.any(bad):\n                candidate[bad] = self._uniform_array(lb, ub)[bad]\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_candidate = _safe_eval(candidate)\n            # Append candidate to archive (even if inf) to keep memory consistent\n            X = np.vstack([X, candidate])\n            F = np.concatenate([F, np.array([f_candidate])])\n            n_archive += 1\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_candidate) and f_candidate < f_best:\n                f_best = float(f_candidate)\n                x_best = candidate.copy()\n                improved = True\n                last_improvement_eval = evals\n\n            # define success as either improving global best or improving relative rank among archive\n            if improved:\n                recent_success.append(True)\n            else:\n                # placed among top 25%?\n                if n_archive >= 4:\n                    sorted_idx = np.argsort(F)\n                    rankpos = np.where(sorted_idx == (n_archive - 1))[0][0]  # position of last added\n                    recent_success.append(rankpos < max(1, int(0.25 * n_archive)))\n                else:\n                    recent_success.append(False)\n\n            # prune archive if too large: keep best ones and some randoms\n            if len(F) > self.max_archive:\n                # keep top fraction and random subset of the remainder\n                top_k = int(round(self.max_archive * 0.6))\n                if top_k < 2:\n                    top_k = min(2, self.max_archive)\n                order = np.argsort(F)\n                top_idx = order[:top_k]\n                rest_idx = np.setdiff1d(np.arange(len(F)), top_idx)\n                n_needed = self.max_archive - top_idx.size\n                if n_needed > 0 and rest_idx.size > 0:\n                    rnd_idx = self.rng.choice(rest_idx, size=min(n_needed, rest_idx.size), replace=False)\n                    keep_idx = np.concatenate([top_idx, rnd_idx])\n                else:\n                    keep_idx = top_idx\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            adapt_counter += 1\n            if adapt_counter >= max(1, self.adapt_window // 2):\n                adapt_counter = 0\n                if len(recent_success) > 0:\n                    succ_rate = float(np.mean(recent_success))\n                else:\n                    succ_rate = 0.0\n                target = 0.2\n                if succ_rate < target * 0.6:\n                    # not enough success -> increase exploration\n                    factor = 1.12\n                    gscale = min(1.5, gscale * factor)\n                elif succ_rate > target * 1.4:\n                    # too many successes -> refine (reduce)\n                    factor = 0.92\n                    gscale = max(1e-6, gscale * factor)\n                # gentle jitter to avoid locking\n                gscale = np.clip(gscale * (1.0 + 0.02 * (self.rng.random() - 0.5)), 1e-7, 2.0)\n\n            # stagnation detection & micro-restarts\n            if (evals - last_improvement_eval) > max(50, 5 * self.adapt_window) and evals < self.budget:\n                # perform micro restarts: inject perturbed elites or randoms\n                num_inject = min(4, max(1, int(0.05 * self.max_archive)))\n                inject_points = []\n                if n_archive >= 2:\n                    # pick some elites to perturb strongly\n                    elite_count = max(1, n_elites)\n                    for _ in range(num_inject):\n                        if elite_count > 0:\n                            eidx = elites_idx[self.rng.integers(0, elite_count)]\n                            newp = X[eidx] + (1.5 * gscale) * (0.8 + self.rng.random()) * (ub - lb) * self.rng.normal(size=self.dim)\n                            newp = self._reflect_bounds(newp, lb, ub)\n                            inject_points.append(newp)\n                        else:\n                            inject_points.append(self._uniform_array(lb, ub))\n                else:\n                    for _ in range(num_inject):\n                        inject_points.append(self._uniform_array(lb, ub))\n                for p in inject_points:\n                    if evals >= self.budget:\n                        break\n                    fp = _safe_eval(p)\n                    X = np.vstack([X, p])\n                    F = np.concatenate([F, np.array([fp])])\n                    if np.isfinite(fp) and fp < f_best:\n                        f_best = float(fp)\n                        x_best = p.copy()\n                        last_improvement_eval = evals\n                # encourage exploration: slightly enlarge gscale\n                gscale = min(2.0, gscale * 1.18)\n                # move last_improvement forward artificially to avoid repeated immediate restarts\n                last_improvement_eval = evals - int(0.5 * self.adapt_window)\n\n            # loop continues until budget consumed\n\n        # final safety: return best finite point found, else return center\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            idx = finite_idx[np.argmin(F[finite_idx])]\n            f_best_final = float(F[idx])\n            x_best_final = X[idx].copy()\n        else:\n            f_best_final = np.inf\n            x_best_final = 0.5 * (lb + ub)\n\n        return f_best_final, x_best_final", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.adapt_window)", "error": "In the code, line 148, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.adapt_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0f8ca89f-6ef5-4fbd-a73f-edf8279df4f4", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-Guided Adaptive Directional Sampling (MG-ADS) — hybrid archive-driven sampler mixing anisotropic Gaussian locals, PCA-guided elite moves, DE-like donors, heavy-tailed Cauchy jumps and archive-mixing with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_archive=12, max_archive=None,\n                 gscale_init=0.2, recent_window=50):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.init_archive = int(init_archive)\n        self.max_archive = int(max_archive) if max_archive is not None else max(200, 10 * self.dim)\n        self.gscale = float(gscale_init)\n        self.recent_window = int(recent_window)\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # Try to extract bounds from func if available, else use [-5, 5]\n        if hasattr(func, \"bounds\"):\n            try:\n                lb = np.asarray(func.bounds.lb, dtype=float)\n                ub = np.asarray(func.bounds.ub, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n        else:\n            lb = None\n            ub = None\n        if lb is None or ub is None:\n            lb = -5.0\n            ub = 5.0\n\n        # Broadcast scalars or single-dim arrays to full dimension\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, lb, dtype=float)\n        if ub.shape == ():\n            ub = np.full(self.dim, ub, dtype=float)\n\n        # Ensure shapes\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            # Try to broadcast if one-dim mismatch\n            lb = np.resize(lb, (self.dim,))\n            ub = np.resize(ub, (self.dim,))\n\n        # Enforce finite bounds\n        lb = np.where(np.isfinite(lb), lb, -5.0)\n        ub = np.where(np.isfinite(ub), ub, 5.0)\n        # Safety: if any lb >= ub, fix to [-5,5]\n        bad = lb >= ub\n        if np.any(bad):\n            lb[bad] = -5.0\n            ub[bad] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # Coordinate-wise reflection into [lb, ub]\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp to ensure numerical safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        # Archive storage\n        X = []  # list of numpy arrays\n        F = []  # list of floats\n\n        lb, ub = self._extract_bounds(func)\n\n        evals = 0\n        exhausted = False\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            nonlocal evals, exhausted\n            if exhausted or evals >= self.budget:\n                exhausted = True\n                return np.inf\n            # ensure shape\n            x = np.asarray(x, dtype=float)\n            # call function\n            val = func(x)\n            evals += 1\n            if evals >= self.budget:\n                exhausted = True\n            return float(val)\n\n        # initialization: a small stratified-ish sample\n        n_init = int(min(max(4, self.init_archive), max(2, max(2, self.budget // 10))))\n        # Latin-hypercube-like stratification\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        for i in range(n_init):\n            # permute strata across dims\n            perm = self.rng.permutation(n_init)\n            u = np.zeros(self.dim, dtype=float)\n            for d in range(self.dim):\n                j = perm[d % n_init]\n                u[d] = self.rng.random() * (strata[j + 1] - strata[j]) + strata[j]\n            x = lb + u * (ub - lb)\n            f = _safe_eval(x)\n            X.append(np.asarray(x, dtype=float))\n            F.append(float(f))\n            if exhausted:\n                break\n\n        # ensure at least two points if budget allows\n        while len(X) < 2 and not exhausted:\n            x = self._uniform_array(lb, ub)\n            f = _safe_eval(x)\n            X.append(np.asarray(x, dtype=float))\n            F.append(float(f))\n\n        if len(X) == 0:\n            # no evaluations possible - return center\n            center = 0.5 * (lb + ub)\n            return np.inf, center\n\n        # initialize best trackers\n        F = np.array(F, dtype=float)\n        X = [np.array(x, dtype=float) for x in X]\n        best_idx = int(np.nanargmin(F))\n        fbest = float(F[best_idx])\n        xbest = X[best_idx].copy()\n        center = np.mean(np.vstack(X), axis=0)\n\n        # short-term adaptation\n        recent_success = deque(maxlen=self.recent_window)\n        no_improve_iter = 0\n        last_improve_evals = evals\n\n        # main loop: generate candidates until budget used\n        while evals < self.budget:\n            n_archive = len(X)\n            if n_archive == 0:\n                # refill\n                x = self._uniform_array(lb, ub)\n                f = _safe_eval(x)\n                X.append(x); F = np.append(F, f)\n                if f < fbest:\n                    fbest = f; xbest = x.copy()\n                continue\n\n            # compute ranking and elites\n            ranked_idx = np.argsort(F)  # ascending (best first)\n            top_fraction = 0.2\n            n_elite = max(2, int(np.ceil(top_fraction * n_archive)))\n            elite_idx = ranked_idx[:n_elite]\n            elites = np.vstack([X[i] for i in elite_idx])\n\n            # per-dim adaptive scale: combine global and archive spread\n            Xmat = np.vstack(X)\n            per_dim_spread = np.std(Xmat, axis=0, ddof=1)\n            per_dim_spread = np.where(per_dim_spread <= 1e-8, (ub - lb) * 0.01, per_dim_spread)\n            per_dim_scale = self.gscale * (per_dim_spread + 1e-12)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            tmp = -F  # higher for better\n            # numeric stability softmax\n            tmp = tmp - np.max(tmp)\n            probs = np.exp(tmp / max(1e-6, np.std(tmp) + 1e-6))\n            probs = probs / np.sum(probs)\n            try:\n                base_idx = int(self.rng.choice(n_archive, p=probs))\n            except Exception:\n                base_idx = int(self.rng.randint(0, n_archive))\n            base = np.array(X[base_idx], dtype=float)\n\n            move_rand = self.rng.rand()\n            candidate = None\n\n            # decide move type by probabilities\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            if move_rand < 0.35:\n                # anisotropic gaussian local move with multiplicative jitter & pull to center\n                pull = 0.06 * (center - base)\n                jitter = 1.0 + 0.15 * (self.rng.randn(self.dim))\n                candidate = base + per_dim_scale * jitter * self.rng.randn(self.dim) + pull\n\n            elif move_rand < 0.35 + 0.25:\n                # PCA-guided sampling using elites\n                try:\n                    E = elites - np.mean(elites, axis=0)\n                    cov = np.cov(E.T)\n                    # eigen decomposition\n                    eigvals, eigvecs = np.linalg.eigh(cov + np.eye(self.dim) * 1e-12)\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    # component weights: emphasize top components\n                    comp_weights = 1.0 + 3.0 * (eigvals / (np.sum(eigvals) + 1e-12))\n                    z = np.zeros(self.dim)\n                    for k in range(self.dim):\n                        scale_k = np.sqrt(max(eigvals[k], 0.0)) * np.sqrt(comp_weights[k])\n                        # occasionally heavy-tailed after PCA\n                        if self.rng.rand() < 0.15:\n                            z[k] = scale_k * self.rng.standard_cauchy()\n                        else:\n                            z[k] = scale_k * self.rng.randn()\n                    delta = eigvecs.dot(z) * self.gscale\n                    candidate = base + delta + 0.02 * per_dim_scale * self.rng.randn(self.dim)\n                except Exception:\n                    # fallback to local noisy move\n                    candidate = base + per_dim_scale * self.rng.randn(self.dim)\n\n            elif move_rand < 0.35 + 0.25 + 0.20:\n                # Differential-like donor from archive with archive-biased sampling\n                ids = list(range(n_archive))\n                if base_idx in ids:\n                    ids.remove(base_idx)\n                if len(ids) >= 2:\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    Fscale = 0.6 + 0.6 * self.rng.rand()\n                    jitter = 0.02 * (per_dim_spread * (1.0 + 0.5 * self.rng.randn(self.dim)))\n                    candidate = base + Fscale * (X[a] - X[b]) + jitter\n                else:\n                    candidate = base + per_dim_scale * self.rng.randn(self.dim)\n\n            elif move_rand < 0.35 + 0.25 + 0.20 + 0.15:\n                # heavy-tailed jump centered on base or global center\n                if self.rng.rand() < 0.5:\n                    centerish = base\n                else:\n                    centerish = 0.5 * (xbest + center)\n                scale = 1.2 * self.gscale\n                c = self.rng.standard_cauchy(size=self.dim)\n                candidate = centerish + c * (per_dim_spread * scale + 1e-12)\n\n            else:\n                # occasional archive mixing: mix some dims from a random archive member\n                donor_idx = int(self.rng.randint(0, n_archive))\n                donor = X[donor_idx]\n                mask = self.rng.rand(self.dim) < 0.2\n                if not np.any(mask):\n                    mask[self.rng.randint(0, self.dim)] = True\n                candidate = base.copy()\n                candidate[mask] = donor[mask]\n                candidate += per_dim_scale * 0.5 * self.rng.randn(self.dim)\n\n            # small chance to completely randomize candidate for exploration\n            if self.rng.rand() < 0.02:\n                candidate = self._uniform_array(lb, ub)\n\n            # safety: finite values and reflect to bounds\n            if candidate is None:\n                candidate = self._uniform_array(lb, ub)\n            candidate = np.asarray(candidate, dtype=float)\n            bad = ~np.isfinite(candidate)\n            if np.any(bad):\n                candidate[bad] = self._uniform_array(lb[bad], ub[bad])\n\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_c = _safe_eval(candidate)\n            # Append candidate to archive (even if inf) to keep memory consistent\n            X.append(candidate.copy())\n            F = np.append(F, float(f_c))\n\n            # update best trackers\n            if np.isfinite(f_c) and f_c < fbest:\n                fbest = float(f_c)\n                xbest = candidate.copy()\n                last_improve_evals = evals\n\n            # define success as either improving global best or improving relative rank among archive\n            # placed among top 25%?\n            cur_rank = int(np.sum(F < f_c))\n            if (np.isfinite(f_c) and f_c <= fbest) or (cur_rank <= max(1, int(0.25 * len(F)))):\n                recent_success.append(True)\n                no_improve_iter = 0\n            else:\n                recent_success.append(False)\n                no_improve_iter += 1\n\n            # prune archive if too large: keep best ones and some randoms\n            if len(X) > self.max_archive:\n                top_k = int(round(self.max_archive * 0.6))\n                idx_sorted = np.argsort(F)\n                keep_idx = list(idx_sorted[:top_k])\n                # keep some randoms from the rest\n                remaining = [i for i in range(len(X)) if i not in keep_idx]\n                n_to_keep = max(0, self.max_archive - len(keep_idx))\n                if len(remaining) > 0 and n_to_keep > 0:\n                    rand_keep = list(self.rng.choice(remaining, size=min(n_to_keep, len(remaining)), replace=False))\n                    keep_idx.extend(rand_keep)\n                # rebuild X and F\n                X = [X[i] for i in keep_idx]\n                F = np.array([F[i] for i in keep_idx], dtype=float)\n\n            # adapt gscale occasionally based on short-term success\n            if len(recent_success) == recent_success.maxlen:\n                succ_rate = float(np.mean(recent_success))\n                if succ_rate < 0.2:\n                    # not enough success -> increase exploration\n                    self.gscale *= 1.08 + 0.02 * self.rng.rand()\n                elif succ_rate > 0.6:\n                    # too many successes -> refine (reduce)\n                    self.gscale *= 0.92 - 0.02 * self.rng.rand()\n                # gentle jitter to avoid locking\n                self.gscale *= 1.0 + 0.02 * (self.rng.randn() * 0.5)\n                # clamp\n                self.gscale = float(np.clip(self.gscale, 1e-6, 5.0))\n\n            # stagnation detection & micro-restarts\n            stagnation_thresh = max(20, int(0.1 * self.budget / max(1, self.dim)))\n            if (evals - last_improve_evals) > stagnation_thresh:\n                # perform micro restarts: inject perturbed elites or randoms\n                n_inject = max(1, int(0.05 * self.max_archive))\n                inject_points = []\n                for _ in range(n_inject):\n                    if self.rng.rand() < 0.7 and len(elites) > 0:\n                        # pick an elite and perturb strongly\n                        elite = elites[self.rng.randint(0, elites.shape[0])]\n                        newp = elite + (1.2 * self.gscale) * per_dim_spread * self.rng.standard_cauchy(self.dim)\n                    else:\n                        newp = self._uniform_array(lb, ub)\n                    newp = self._reflect_bounds(newp, lb, ub)\n                    inject_points.append(newp)\n                # replace some worst points with injects\n                if len(X) > 0:\n                    idx_sorted = np.argsort(F)  # ascending\n                    worst_idx = idx_sorted[::-1][:len(inject_points)]\n                    for wi, npnt in zip(worst_idx, inject_points):\n                        # safe eval and replace\n                        fval = _safe_eval(npnt)\n                        if wi >= len(X):\n                            continue\n                        X[wi] = npnt.copy()\n                        F[wi] = float(fval)\n                        if np.isfinite(fval) and fval < fbest:\n                            fbest = float(fval)\n                            xbest = npnt.copy()\n                # encourage exploration: slightly enlarge gscale\n                self.gscale = float(min(5.0, max(self.gscale * 1.3, 0.001)))\n                last_improve_evals = evals  # avoid immediate repeated restarts\n\n        # final safety: return best finite point found, else return center\n        finite_idx = np.where(np.isfinite(F))[0]\n        if finite_idx.size > 0:\n            best_i = int(np.nanargmin(F))\n            x_best_final = X[best_i]\n            f_best_final = float(F[best_i])\n        else:\n            x_best_final = 0.5 * (lb + ub)\n            f_best_final = np.inf\n\n        return f_best_final, x_best_final", "configspace": "", "generation": 0, "feedback": "In the code, line 151, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.recent_window)", "error": "In the code, line 151, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: recent_success = deque(maxlen=self.recent_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8e5439c3-5caa-433e-8a43-b0c9e8c11969", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven sampler mixing anisotropic Gaussian locals, PCA-guided elite moves, DE-style donors, heavy-tailed Cauchy jumps and archive mixing with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_size=None, archive_max=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # initial number of points to seed the archive\n        self.init_size = int(init_size) if init_size is not None else min(max(6, 2*self.dim), 2*self.dim+20)\n        self.archive_max = int(archive_max)\n        # adaptive global scale\n        self.gscale = 0.2  # initial global scale in unit [0,1] proportion of domain span\n        self.gscale_min = 1e-6\n        self.gscale_max = 5.0\n        # short-term adaptation memory\n        self.short_window = 50\n        self.success_history = []\n        # counters\n        self.evals = 0\n        self.func = None\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # Try several common attributes, else use [-5,5]\n        dim = self.dim\n        lb = None\n        ub = None\n        # common BBOB-like wrapper\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.array(b.lb, dtype=float)\n                ub = np.array(b.ub, dtype=float)\n        # try attributes used in some other benchmarks\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            lb = np.array(getattr(func, \"lower_bounds\"), dtype=float)\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            ub = np.array(getattr(func, \"upper_bounds\"), dtype=float)\n        # last resort: check attributes named lb/ub\n        if lb is None and hasattr(func, \"lb\"):\n            lb = np.array(getattr(func, \"lb\"), dtype=float)\n        if ub is None and hasattr(func, \"ub\"):\n            ub = np.array(getattr(func, \"ub\"), dtype=float)\n\n        if lb is None or ub is None:\n            # fallback to [-5,5]\n            lb = np.full(dim, -5.0)\n            ub = np.full(dim, 5.0)\n        else:\n            lb = np.atleast_1d(lb).astype(float)\n            ub = np.atleast_1d(ub).astype(float)\n            # broadcast scalars\n            if lb.size == 1:\n                lb = np.full(dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(dim, ub.item())\n            # if shapes mismatch try to broadcast\n            if lb.size != dim or ub.size != dim:\n                try:\n                    lb = np.broadcast_to(lb, (dim,))\n                    ub = np.broadcast_to(ub, (dim,))\n                except Exception:\n                    lb = np.full(dim, -5.0)\n                    ub = np.full(dim, 5.0)\n        # enforce finite bounds and sane orientation\n        finite_mask = np.isfinite(lb) & np.isfinite(ub)\n        if not np.all(finite_mask):\n            lb[~finite_mask] = -5.0\n            ub[~finite_mask] = 5.0\n        # ensure lb < ub\n        bad = lb >= ub\n        if np.any(bad):\n            lb[bad] = -5.0\n            ub[bad] = 5.0\n        return lb.copy(), ub.copy()\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = lb + (lb - x) = 2*lb - x\n            x[below] = 2*lb[below] - x[below]\n            # reflect above: x = ub - (x - ub) = 2*ub - x\n            x[above] = 2*ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return self.rng.uniform(lb, ub, size=self.dim)\n\n    # ----- main call -----\n    def __call__(self, func):\n        self.func = func\n        self.evals = 0\n        dim = self.dim\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            x = np.atleast_1d(np.array(x, dtype=float)).reshape(dim,)\n            if self.evals >= self.budget:\n                # No budget left - return +inf\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return float(f) if np.isfinite(f) else np.inf\n\n        # initialization: stratified-ish LHS-like sample\n        init_n = min(self.init_size, max(1, self.budget))\n        # make strata\n        X_init = []\n        if init_n == 1:\n            x0 = lb + 0.5 * span\n            X_init.append(x0)\n        else:\n            # Latin-hypercube-like: for each dim produce stratified intervals\n            strata = np.linspace(0.0, 1.0, init_n + 1)\n            for i in range(init_n):\n                # sample within strata for each dim, but permute strata across dims\n                u = self.rng.rand(dim)\n                perm = self.rng.permutation(init_n)\n                xi = np.empty(dim, dtype=float)\n                for d in range(dim):\n                    s = perm[d]  # which stratum this dim uses\n                    t_lo = strata[s]\n                    t_hi = strata[s+1]\n                    fraction = t_lo + u[d]*(t_hi - t_lo)\n                    xi[d] = lb[d] + fraction * span[d]\n                X_init.append(xi)\n        # ensure at least two if budget allows\n        if self.budget >= 2 and len(X_init) < 2:\n            X_init.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population\n        X_list = []\n        F_list = []\n        for x in X_init:\n            f = _safe_eval(x)\n            X_list.append(np.array(x, dtype=float))\n            F_list.append(f)\n            if self.evals >= self.budget:\n                break\n\n        # If no evaluations possible return center\n        if len(F_list) == 0:\n            center = lb + 0.5*span\n            return np.inf, center\n\n        X = np.vstack(X_list)\n        F = np.array(F_list, dtype=float)\n\n        # trackers\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = np.argmin(F)\n            f_best = float(F[best_idx])\n            x_best = X[best_idx].copy()\n        else:\n            # fallback\n            f_best = np.inf\n            x_best = lb + 0.5*span\n\n        # short-term adaptation\n        self.success_history = []\n        iter_since_improve = 0\n        total_iters = 0\n\n        # main loop: generate candidates until budget used\n        while self.evals < self.budget:\n            total_iters += 1\n            N = X.shape[0]\n\n            # compute ranking and elites\n            # rank by F, smaller is better. Treat inf as worst.\n            order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            # elite set: top 10%-40% depending on size\n            elite_frac = 0.2\n            n_elite = max(1, int(np.ceil(elite_frac * N)))\n            elites_idx = order[:n_elite]\n            elites_X = X[elites_idx]\n            elites_F = F[elites_idx]\n\n            # per-dim adaptive scale: combine global and archive spread\n            spread = np.nanstd(X, axis=0)\n            # if spread zero, fallback to span * small factor\n            spread = np.where(spread > 0, spread, 0.5 * span)\n            per_dim_scale = self.gscale * spread + 1e-12\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_F = np.where(np.isfinite(F), F, np.max(F[np.isfinite(F)]) + 1.0 if np.any(np.isfinite(F)) else 1e6)\n            # numerical stability\n            negF = -finite_F\n            negF = negF - np.max(negF)\n            weights = np.exp(negF / (np.std(negF) + 1e-8))\n            weights = weights / np.sum(weights)\n            base_idx = self.rng.choice(np.arange(N), p=weights)\n            base_x = X[base_idx].copy()\n\n            # decide move type by probabilities\n            r = self.rng.rand()\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            if r < 0.35:\n                # anisotropic gaussian local move with multiplicative jitter & pull to center\n                jitter = 1.0 + 0.3 * (self.rng.randn(dim) * self.rng.rand(dim))\n                noise = self.rng.randn(dim) * per_dim_scale * jitter\n                # pull to archive center a bit\n                center = np.nanmean(X, axis=0)\n                pull = (center - base_x) * (0.05 * self.rng.rand())\n                cand = base_x + noise + pull\n            elif r < 0.60:\n                # PCA-guided sampling using elites\n                # build covariance of elites\n                try:\n                    emean = np.mean(elites_X, axis=0)\n                    Xc = elites_X - emean\n                    # SVD for numerical stability\n                    U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    # principal directions are rows of Vt\n                    comps = Vt\n                    # emphasize top components with larger scales\n                    comp_weights = (svals / (np.sum(svals) + 1e-12)) ** 0.75\n                    comp_weights = comp_weights / (np.max(comp_weights) + 1e-12)\n                    # perturb along components\n                    z = self.rng.randn(min(len(svals), dim))\n                    # occasionally heavy-tailed after PCA\n                    if self.rng.rand() < 0.12:\n                        z = np.random.standard_cauchy(size=z.shape)\n                    step = np.zeros(dim)\n                    for j in range(z.size):\n                        step += (z[j] * comp_weights[j]) * comps[j]\n                    step = step * (self.gscale * 0.8 * np.mean(spread))\n                    cand = emean + step + 0.05 * self.rng.randn(dim) * per_dim_scale\n                except Exception:\n                    # fallback to local noisy move\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n            elif r < 0.80:\n                # Differential-like donor from archive with archive-biased sampling\n                if N >= 3:\n                    ids = list(range(N))\n                    ids.remove(base_idx)\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    donor = X[a]\n                    diff = X[b] - X[base_idx]\n                    Fscale = 0.5 + self.rng.rand()  # crossover factor\n                    cand = donor + Fscale * diff + self.rng.randn(dim) * 0.02 * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n            elif r < 0.95:\n                # heavy-tailed Cauchy jumps centered on base or global center\n                if self.rng.rand() < 0.6:\n                    center_choice = base_x\n                else:\n                    center_choice = np.nanmean(X, axis=0)\n                c = np.random.standard_cauchy(size=dim)\n                # scale cauchy by per-dim scale with occasional large multiplier\n                multiplier = self.rng.lognormal(mean=0.0, sigma=0.5)\n                cand = center_choice + (c * per_dim_scale * multiplier * 0.5)\n            else:\n                # ARCHIVE_MIX: mix some dims from a random archive member\n                rand_idx = self.rng.randint(0, N)\n                donor = X[rand_idx]\n                cand = base_x.copy()\n                mix_mask = self.rng.rand(dim) < (0.3 + 0.2 * self.rng.rand())\n                cand[mix_mask] = donor[mix_mask] + 0.01 * self.rng.randn(np.sum(mix_mask)) * per_dim_scale[mix_mask]\n\n            # small chance to completely randomize candidate for exploration\n            if self.rng.rand() < 0.03:\n                cand = self._uniform_array(lb, ub)\n\n            # safety: finite values and reflect to bounds\n            cand = np.where(np.isfinite(cand), cand, lb + 0.5*span)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_cand = _safe_eval(cand)\n            # Append candidate to archive (even if inf) to keep memory consistent\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, [f_cand]])\n            N += 1\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as either improving global best or improving relative rank among archive\n            # placed among top 25%?\n            cur_rank = np.sum(np.isfinite(F) & (F <= f_cand))\n            success = False\n            if improved:\n                success = True\n            else:\n                if cur_rank <= max(1, int(0.25 * N)):\n                    success = True\n\n            # update short-term success history\n            self.success_history.append(1 if success else 0)\n            if len(self.success_history) > self.short_window:\n                self.success_history.pop(0)\n\n            # prune archive if too large: keep best ones and some randoms\n            if N > self.archive_max:\n                # keep top K best and sample some from remaining\n                keep_best = int(max(10, 0.4 * self.archive_max))\n                keep_rand = int(max(5, 0.2 * self.archive_max))\n                order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_idx = list(order[:keep_best])\n                rest = [i for i in range(N) if i not in keep_idx]\n                if len(rest) > 0:\n                    rnd_keep = list(self.rng.choice(rest, size=min(keep_rand, len(rest)), replace=False))\n                    keep_idx += rnd_keep\n                # ensure at least some kept\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n                N = X.shape[0]\n\n            # adapt gscale occasionally based on short-term success\n            if total_iters % 10 == 0:\n                if len(self.success_history) >= 5:\n                    succ_rate = np.mean(self.success_history)\n                    # if not enough success -> increase exploration\n                    if succ_rate < 0.08:\n                        self.gscale = min(self.gscale * 1.25 + 1e-6, self.gscale_max)\n                    # too many successes -> refine (reduce)\n                    elif succ_rate > 0.35:\n                        self.gscale = max(self.gscale * 0.85 - 1e-6, self.gscale_min)\n                    # gentle jitter to avoid locking\n                    self.gscale *= (1.0 + (self.rng.rand()-0.5)*0.02)\n                    # clamp\n                    self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 10 * dim):\n                # perform micro restarts: inject perturbed elites or randoms\n                n_inject = max(1, int(0.05 * max(2, N)))\n                # pick from elites if possible else from bests\n                injects = []\n                for _ in range(n_inject):\n                    if len(elites_idx) > 0 and self.rng.rand() < 0.8:\n                        e_idx = self.rng.choice(elites_idx)\n                        base = X[e_idx]\n                        pert = base + self.rng.randn(dim) * (1.0 + 3.0*self.rng.rand()) * per_dim_scale\n                    else:\n                        pert = self._uniform_array(lb, ub)\n                    pert = self._reflect_bounds(pert, lb, ub)\n                    injects.append(pert)\n                # replace some worst points with injects\n                order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                worst_idx = order[-len(injects):] if len(order) >= len(injects) else list(range(N))\n                for i, w in enumerate(worst_idx):\n                    if self.evals < self.budget:\n                        f_new = _safe_eval(injects[i])\n                    else:\n                        f_new = np.inf\n                    X[w] = injects[i]\n                    F[w] = f_new\n                # encourage exploration: slightly enlarge gscale\n                self.gscale = min(self.gscale * 1.5, self.gscale_max)\n                iter_since_improve = 0  # reset stagnation counter\n\n            # if budget exhausted break\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if not np.isfinite(f_best):\n            center = lb + 0.5 * span\n            return float(np.inf), center\n        return float(f_best), x_best.copy()\n\n\n# Example usage:\n# mg = MemoryGuidedADS(budget=1000, dim=10, seed=42)\n# f_opt, x_opt = mg(my_func)", "configspace": "", "generation": 0, "feedback": "In the code, line 362, in __call__, the following error occurred:\nIndexError: index 160 is out of bounds for axis 0 with size 120\nOn line: base = X[e_idx]", "error": "In the code, line 362, in __call__, the following error occurred:\nIndexError: index 160 is out of bounds for axis 0 with size 120\nOn line: base = X[e_idx]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "27509e6a-b6d7-4ec7-b9c5-ca6094839d8f", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven sampler mixing anisotropic Gaussian locals, PCA-guided elite moves, DE-style donors, heavy-tailed Cauchy jumps and archive mixing with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_size=None, archive_max=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # initial number of points to seed the archive\n        if init_size is None:\n            self.init_size = min(max(6, 2 * self.dim), 2 * self.dim + 20)\n        else:\n            self.init_size = int(init_size)\n        self.archive_max = int(archive_max)\n        # adaptive global scale (relative to problem span)\n        self.gscale = 0.2\n        self.gscale_min = 1e-6\n        self.gscale_max = 2.0\n        # short-term adaptation memory\n        self.short_window = 50\n        self.success_history = []\n        # counters\n        self.evals = 0\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # Extract bounds if available, otherwise fallback to [-5,5]\n        dim = self.dim\n        lb = None\n        ub = None\n\n        # common attribute names\n        if hasattr(func, \"bounds\"):\n            try:\n                b = getattr(func, \"bounds\")\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.array(b.lb, dtype=float)\n                    ub = np.array(b.ub, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            try:\n                lb = np.array(getattr(func, \"lower_bounds\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            try:\n                ub = np.array(getattr(func, \"upper_bounds\"), dtype=float)\n            except Exception:\n                ub = None\n\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.array(getattr(func, \"lb\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"ub\"):\n            try:\n                ub = np.array(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(dim, -5.0)\n            ub = np.full(dim, 5.0)\n        else:\n            lb = np.atleast_1d(lb).astype(float)\n            ub = np.atleast_1d(ub).astype(float)\n            # broadcast scalars\n            if lb.size == 1:\n                lb = np.full(dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(dim, ub.item())\n            # if shapes mismatch try to broadcast; otherwise fallback\n            if lb.size != dim or ub.size != dim:\n                try:\n                    lb = np.broadcast_to(lb, (dim,))\n                    ub = np.broadcast_to(ub, (dim,))\n                except Exception:\n                    lb = np.full(dim, -5.0)\n                    ub = np.full(dim, 5.0)\n\n        # enforce finite and orientation\n        finite_mask = np.isfinite(lb) & np.isfinite(ub)\n        if not np.all(finite_mask):\n            lb[~finite_mask] = -5.0\n            ub[~finite_mask] = 5.0\n        bad = lb >= ub\n        if np.any(bad):\n            lb[bad] = -5.0\n            ub[bad] = 5.0\n        return lb.copy(), ub.copy()\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.array(x, dtype=float)\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = 2 * lb[below] - x[below]\n            x[above] = 2 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.uniform(lb, ub)\n\n    # ----- main call -----\n    def __call__(self, func):\n        self.func = func\n        self.evals = 0\n        dim = self.dim\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        span = np.where(span > 0, span, 10.0)  # safety\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            x = np.atleast_1d(np.array(x, dtype=float)).reshape(dim,)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            if not np.isfinite(f):\n                return np.inf\n            return float(f)\n\n        # initialization: Latin-hypercube-like seeding\n        init_n = min(self.init_size, max(1, self.budget))\n        X_init = []\n        if init_n == 1:\n            x0 = lb + 0.5 * span\n            X_init.append(x0)\n        else:\n            # Create LHS-like design: for each dim, generate permuted strata\n            strata_edges = np.linspace(0.0, 1.0, init_n + 1)\n            # For each sample i produce vector using per-dimension permutations\n            perms = [self.rng.permutation(init_n) for _ in range(dim)]\n            for i in range(init_n):\n                xi = np.empty(dim, dtype=float)\n                for d in range(dim):\n                    s = perms[d][i]\n                    lo = strata_edges[s]\n                    hi = strata_edges[s + 1]\n                    frac = lo + self.rng.rand() * (hi - lo)\n                    xi[d] = lb[d] + frac * span[d]\n                X_init.append(xi)\n\n        # ensure at least two if budget allows\n        if self.budget >= 2 and len(X_init) < 2:\n            X_init.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population\n        X_list = []\n        F_list = []\n        for x in X_init:\n            f = _safe_eval(x)\n            X_list.append(np.array(x, dtype=float))\n            F_list.append(f)\n            if self.evals >= self.budget:\n                break\n\n        if len(X_list) == 0:\n            center = lb + 0.5 * span\n            return float(np.inf), center\n\n        X = np.vstack(X_list)\n        F = np.array(F_list, dtype=float)\n\n        # trackers\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = np.nanargmin(np.where(np.isfinite(F), F, np.inf))\n            f_best = float(F[best_idx])\n            x_best = X[best_idx].copy()\n        else:\n            f_best = np.inf\n            x_best = lb + 0.5 * span\n\n        # short-term adaptation\n        self.success_history = []\n        iter_since_improve = 0\n        total_iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            total_iters += 1\n            N = X.shape[0]\n\n            # compute ranking and elites\n            order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elite_frac = 0.2\n            n_elite = max(1, int(np.ceil(elite_frac * N)))\n            elites_idx = order[:n_elite]\n            elites_X = X[elites_idx]\n            elites_F = F[elites_idx]\n\n            # per-dim adaptive scale: combine global and archive spread\n            spread = np.nanstd(X, axis=0)\n            spread = np.where(spread > 1e-12, spread, 0.25 * span)\n            per_dim_scale = self.gscale * spread + 1e-12\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_F = np.where(np.isfinite(F), F, np.nan)\n            # If some finite, use them; otherwise uniform\n            if np.any(np.isfinite(F)):\n                # create negF with finite entries; inf entries get large positive so they get low weights\n                negF = -np.where(np.isfinite(F), F, np.nanmax(np.where(np.isfinite(F), F, np.nan)) + 1.0)\n                # stabilize and softmax\n                negF = negF - np.nanmax(negF)\n                ex = np.exp(np.where(np.isfinite(negF), negF, -np.inf))\n                if np.sum(ex) <= 0 or not np.isfinite(np.sum(ex)):\n                    weights = np.ones(N) / N\n                else:\n                    weights = ex / np.sum(ex)\n            else:\n                weights = np.ones(N) / N\n            try:\n                base_idx = int(self.rng.choice(np.arange(N), p=weights))\n            except Exception:\n                base_idx = int(self.rng.randint(0, N))\n            base_x = X[base_idx].copy()\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n            cand = None\n\n            # LOCAL\n            if r < 0.35:\n                jitter = 1.0 + 0.3 * (self.rng.randn(dim) * self.rng.rand(dim))\n                noise = self.rng.randn(dim) * per_dim_scale * jitter\n                pull = (np.nanmean(X, axis=0) - base_x) * (0.05 * self.rng.rand())\n                cand = base_x + noise + pull\n\n            # PCA-guided elite moves\n            elif r < 0.60:\n                try:\n                    emean = np.mean(elites_X, axis=0)\n                    Xc = elites_X - emean\n                    if Xc.shape[0] == 1:\n                        # not enough to build covariance; fallback\n                        cand = emean + self.rng.randn(dim) * per_dim_scale\n                    else:\n                        U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # rows = principal components\n                        # emphasize top components\n                        comp_weights = (svals / (np.sum(svals) + 1e-12)) ** 0.75\n                        if comp_weights.size == 0:\n                            comp_weights = np.ones(min(dim, Xc.shape[0]))\n                        comp_weights = comp_weights / (np.max(comp_weights) + 1e-12)\n                        z = self.rng.randn(min(len(svals), dim))\n                        # occasionally heavy-tailed\n                        if self.rng.rand() < 0.12:\n                            z = self.rng.standard_cauchy(size=z.shape)\n                        step = np.zeros(dim)\n                        for j in range(z.size):\n                            step += (z[j] * comp_weights[j]) * comps[j]\n                        cand = emean + step * np.maximum(per_dim_scale, 1e-8) + 0.05 * self.rng.randn(dim) * per_dim_scale\n                except Exception:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # Differential-like donor\n            elif r < 0.80:\n                if N >= 3:\n                    ids = list(range(N))\n                    ids.remove(base_idx)\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    Fscale = 0.8 * (0.7 + 0.6 * self.rng.rand())  # adaptive-ish\n                    donor = X[a]\n                    diff = X[b] - X[base_idx]\n                    cand = donor + Fscale * diff + self.rng.randn(dim) * 0.02 * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < 0.95:\n                if self.rng.rand() < 0.6:\n                    center_choice = base_x\n                else:\n                    center_choice = np.nanmean(X, axis=0)\n                multiplier = self.rng.lognormal(mean=0.0, sigma=0.5)\n                cauch = self.rng.standard_cauchy(size=dim)\n                cand = center_choice + (0.5 * multiplier) * cauch * per_dim_scale\n\n            # ARCHIVE_MIX\n            else:\n                rand_idx = self.rng.randint(0, N)\n                donor = X[rand_idx]\n                cand = base_x.copy()\n                mix_mask = self.rng.rand(dim) < (0.3 + 0.2 * self.rng.rand())\n                if np.any(mix_mask):\n                    noise = 0.01 * self.rng.randn(np.sum(mix_mask)) * per_dim_scale[mix_mask]\n                    cand[mix_mask] = donor[mix_mask] + noise\n                # small chance to replace entirely with uniform\n                if self.rng.rand() < 0.03:\n                    cand = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite entries and reflect to bounds\n            cand = np.where(np.isfinite(cand), cand, lb + 0.5 * span)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, [f_cand]])\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25%\n            success = False\n            if improved:\n                success = True\n            else:\n                # rank among finite values\n                finite_idx = np.where(np.isfinite(F))[0]\n                if finite_idx.size > 0 and np.isfinite(f_cand):\n                    cur_rank = np.sum((F[finite_idx] <= f_cand))\n                    if cur_rank <= max(1, int(0.25 * max(1, N))):\n                        success = True\n\n            # update short-term success history\n            self.success_history.append(1 if success else 0)\n            if len(self.success_history) > self.short_window:\n                self.success_history.pop(0)\n\n            # prune archive if too large\n            if X.shape[0] > self.archive_max:\n                Ncurr = X.shape[0]\n                keep_best = int(max(10, 0.4 * self.archive_max))\n                keep_rand = int(max(5, 0.2 * self.archive_max))\n                order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_idx = list(order_all[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                if len(rest) > 0:\n                    k = min(keep_rand, len(rest))\n                    rnd_keep = list(self.rng.choice(rest, size=k, replace=False))\n                    keep_idx += rnd_keep\n                # ensure some kept\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(self.success_history) >= 5:\n                succ_rate = np.mean(self.success_history)\n                if succ_rate < 0.08:\n                    self.gscale = min(self.gscale * 1.25 + 1e-6, self.gscale_max)\n                elif succ_rate > 0.35:\n                    # reduce scale gently to exploit more\n                    self.gscale *= 0.95 * (1.0 + (self.rng.rand() - 0.5) * 0.02)\n                # clamp\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 10 * dim) and self.evals < self.budget:\n                # perform micro restarts: inject perturbed elites or randoms\n                n_inject = max(1, int(0.05 * max(2, X.shape[0])))\n                injects = []\n                for _ in range(n_inject):\n                    if elites_X.shape[0] > 0 and self.rng.rand() < 0.9:\n                        e_idx = int(self.rng.choice(elites_idx))\n                        base = X[e_idx]\n                        scale_mult = 1.0 + 3.0 * self.rng.rand()\n                        pert = base + self.rng.randn(dim) * scale_mult * per_dim_scale\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        injects.append(pert)\n                    else:\n                        pert = self._uniform_array(lb, ub)\n                        injects.append(pert)\n\n                # replace some worst points with injects (evaluate them)\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                worst_idx = order_now[-len(injects):] if order_now.size >= len(injects) else list(range(X.shape[0]))\n                for i, w in enumerate(worst_idx):\n                    if self.evals >= self.budget:\n                        break\n                    newx = injects[i]\n                    f_new = _safe_eval(newx)\n                    X[w] = newx\n                    F[w] = f_new\n\n                # encourage exploration: enlarge gscale a bit\n                self.gscale = min(self.gscale * 1.5, self.gscale_max)\n                iter_since_improve = 0\n\n            # break if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if not np.isfinite(f_best):\n            center = lb + 0.5 * span\n            return float(np.inf), center\n        return float(f_best), x_best.copy()\n\n\n# Example usage:\n# mg = MemoryGuidedADS(budget=1000, dim=10, seed=42)\n# f_opt, x_opt = mg(my_func)", "configspace": "", "generation": 0, "feedback": "In the code, line 392, in __call__, the following error occurred:\nIndexError: index 188 is out of bounds for axis 0 with size 120\nOn line: base = X[e_idx]", "error": "In the code, line 392, in __call__, the following error occurred:\nIndexError: index 188 is out of bounds for axis 0 with size 120\nOn line: base = X[e_idx]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e1beb653-2dfd-49d1-a7b1-eb26ee6092de", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — hybrid archive-driven sampler combining anisotropic Gaussian locals, PCA-guided elite moves, DE-like donors, heavy-tailed Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_size=None, archive_max=200,\n                 gscale=0.2, gscale_min=1e-6, gscale_max=2.0, short_window=50):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # initial number of points to seed the archive\n        if init_size is None:\n            self.init_size = min(max(6, 2 * self.dim), 2 * self.dim + 20)\n        else:\n            self.init_size = int(init_size)\n        self.archive_max = int(archive_max)\n        # adaptive global scale (relative to problem span)\n        self.gscale = float(gscale)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        # short-term adaptation memory\n        self.short_window = int(short_window)\n        self.success_history = []\n        # counters\n        self.evals = 0\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # Extract bounds if available, otherwise fallback to [-5,5]\n        dim = self.dim\n        lb = None\n        ub = None\n\n        # try common patterns\n        if hasattr(func, \"bounds\"):\n            try:\n                b = getattr(func, \"bounds\")\n                # object with lb/ub or array-like\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.array(b.lb, dtype=float)\n                    ub = np.array(b.ub, dtype=float)\n                else:\n                    try:\n                        arr = np.asarray(b, dtype=float)\n                        if arr.shape == (2, dim):\n                            lb = arr[0]\n                            ub = arr[1]\n                    except Exception:\n                        pass\n            except Exception:\n                pass\n\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            try:\n                lb = np.array(getattr(func, \"lower_bounds\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            try:\n                ub = np.array(getattr(func, \"upper_bounds\"), dtype=float)\n            except Exception:\n                ub = None\n\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.array(getattr(func, \"lb\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"ub\"):\n            try:\n                ub = np.array(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                ub = None\n\n        # fallback to [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(dim, -5.0)\n            ub = np.full(dim, 5.0)\n        else:\n            lb = np.atleast_1d(lb).astype(float)\n            ub = np.atleast_1d(ub).astype(float)\n            # broadcast scalars\n            if lb.size == 1:\n                lb = np.full(dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(dim, ub.item())\n            # if shapes mismatch try to broadcast; otherwise fallback\n            if lb.size != dim or ub.size != dim:\n                try:\n                    lb = np.broadcast_to(lb, (dim,))\n                    ub = np.broadcast_to(ub, (dim,))\n                except Exception:\n                    lb = np.full(dim, -5.0)\n                    ub = np.full(dim, 5.0)\n\n        # enforce finite and orientation\n        finite_mask = np.isfinite(lb) & np.isfinite(ub)\n        if not np.all(finite_mask):\n            lb[~finite_mask] = -5.0\n            ub[~finite_mask] = 5.0\n        bad = lb >= ub\n        if np.any(bad):\n            lb[bad] = -5.0\n            ub[bad] = 5.0\n        return lb.copy(), ub.copy()\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.asarray(x, dtype=float).copy()\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            x[below] = 2 * lb[below] - x[below]\n            x[above] = 2 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        self.evals = 0\n        dim = self.dim\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            x = np.atleast_1d(np.array(x, dtype=float)).reshape(dim,)\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                f = float(func(x))\n                if not np.isfinite(f):\n                    f = float(np.inf)\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # initialization: Latin-hypercube-like seeding\n        init_n = min(self.init_size, max(1, self.budget))\n        X_init = []\n        if init_n == 1:\n            x0 = lb + 0.5 * span\n            X_init.append(x0)\n        else:\n            # Create LHS-like design: for each dim, generate permuted strata\n            strata_edges = np.linspace(0.0, 1.0, init_n + 1)\n            perms = [self.rng.permutation(init_n) for _ in range(dim)]\n            for i in range(init_n):\n                xi = np.empty(dim, dtype=float)\n                for d in range(dim):\n                    s = perms[d][i]\n                    lo = strata_edges[s]\n                    hi = strata_edges[s + 1]\n                    frac = lo + self.rng.rand() * (hi - lo)\n                    xi[d] = lb[d] + frac * span[d]\n                X_init.append(xi)\n\n        # ensure at least two if budget allows\n        if self.budget >= 2 and len(X_init) < 2:\n            X_init.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population\n        X_list = []\n        F_list = []\n        for x in X_init:\n            f = _safe_eval(x)\n            X_list.append(np.array(x, dtype=float))\n            F_list.append(f)\n            if self.evals >= self.budget:\n                break\n\n        if len(X_list) == 0:\n            # no evaluations possible\n            center = lb + 0.5 * span\n            return float(np.inf), center\n\n        X = np.vstack(X_list)\n        F = np.array(F_list, dtype=float)\n\n        # trackers\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = int(np.nanargmin(np.where(np.isfinite(F), F, np.inf)))\n            f_best = float(F[best_idx])\n            x_best = X[best_idx].copy()\n        else:\n            x_best = lb + 0.5 * span\n            f_best = float(np.inf)\n\n        # short-term adaptation\n        self.success_history = []\n        iter_since_improve = 0\n        total_iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            total_iters += 1\n            N = X.shape[0]\n\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elite_frac = 0.2\n            n_elite = max(1, int(np.ceil(elite_frac * N)))\n            elites_idx = order_all[:n_elite]\n            elites_X = X[elites_idx]\n            elites_F = F[elites_idx]\n\n            # per-dim adaptive scale: combine global and archive spread\n            # use robust spread (trimmed std or range)\n            if N > 1:\n                spread = np.nanstd(X, axis=0)\n            else:\n                spread = 0.25 * span\n            spread = np.where(spread > 1e-12, spread, 0.25 * span)\n            per_dim_scale = np.maximum(self.gscale * spread, 1e-12)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            if np.any(np.isfinite(F)):\n                finite_vals = np.where(np.isfinite(F), F, np.nan)\n                # Coerce nan to large positive (worse)\n                max_finite = np.nanmax(finite_vals)\n                negF = -np.where(np.isfinite(F), F, max_finite + 1.0)\n                negF = negF - np.nanmax(negF)\n                expv = np.exp(negF / (1.0 + np.std(negF)))\n                if not np.isfinite(np.sum(expv)) or np.sum(expv) <= 0:\n                    weights = np.ones(N) / N\n                else:\n                    weights = expv / np.sum(expv)\n            else:\n                weights = np.ones(N) / N\n            try:\n                base_idx = int(self.rng.choice(np.arange(N), p=weights))\n            except Exception:\n                base_idx = int(self.rng.randint(0, N))\n            base_x = X[base_idx].copy()\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n\n            cand = None\n\n            # LOCAL\n            if r < 0.35:\n                # anisotropic gaussian jitter (jitter per-dim)\n                jitter = 1.0 + 0.3 * (self.rng.randn(dim) * self.rng.rand(dim))\n                noise = self.rng.randn(dim) * per_dim_scale * jitter\n                pull = (np.nanmean(X, axis=0) - base_x) * (0.05 * self.rng.rand())\n                cand = base_x + noise + pull\n\n            # PCA-guided elite moves\n            elif r < 0.60:\n                # build PCA on elites\n                try:\n                    emean = np.mean(elites_X, axis=0)\n                    Xc = elites_X - emean\n                    # If too few elites, fallback to normal\n                    if Xc.shape[0] >= 2:\n                        U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # rows = principal components\n                        comp_weights = (svals / (np.sum(svals) + 1e-12)) ** 0.75\n                        comp_weights = comp_weights / (np.max(comp_weights) + 1e-12)\n                        # sample coefficients emphasizing top comps\n                        k = min(len(svals), dim)\n                        z = self.rng.randn(k)\n                        if self.rng.rand() < 0.12:\n                            z = self.rng.standard_cauchy(size=z.shape)\n                        step = np.zeros(dim)\n                        for j in range(k):\n                            step += (z[j] * comp_weights[j]) * comps[j]\n                        cand = emean + step * (1.5 * per_dim_scale) + 0.05 * self.rng.randn(dim) * per_dim_scale\n                        # occasionally move from base instead of elite mean\n                        if self.rng.rand() < 0.35:\n                            cand = base_x + 0.4 * (cand - base_x)\n                    else:\n                        # fall back to anisotropic gaussian around elite mean\n                        cand = emean + self.rng.randn(dim) * per_dim_scale\n                except Exception:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # Differential-like donor\n            elif r < 0.80:\n                if N >= 3:\n                    ids = list(range(N))\n                    ids.remove(base_idx)\n                    # pick two distinct donors\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    Fscale = 0.8 * (0.7 + 0.6 * self.rng.rand())  # adaptive-ish\n                    donor = X[a]\n                    diff = X[a] - X[b]\n                    cand = base_x + Fscale * diff + 0.2 * self.rng.randn(dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < 0.95:\n                # heavy-tailed exploration centered sometimes on best, sometimes on base\n                if self.rng.rand() < 0.6:\n                    center_choice = x_best\n                else:\n                    center_choice = base_x\n                cauch = self.rng.standard_cauchy(size=dim)\n                multiplier = self.rng.lognormal(mean=0.0, sigma=0.5)\n                cand = center_choice + (0.5 * multiplier) * cauch * per_dim_scale\n                # small gaussian nudges\n                cand += 0.01 * self.rng.randn(dim) * per_dim_scale\n\n            # ARCHIVE_MIX\n            else:\n                rand_idx = self.rng.randint(0, N)\n                donor = X[rand_idx]\n                mix_mask = self.rng.rand(dim) < (0.2 + 0.4 * self.rng.rand())  # random subset to mix\n                cand = base_x.copy()\n                if np.any(mix_mask):\n                    noise = 0.01 * self.rng.randn(np.sum(mix_mask)) * per_dim_scale[mix_mask]\n                    cand[mix_mask] = donor[mix_mask] + noise\n                # small chance to replace entirely with uniform\n                if self.rng.rand() < 0.03:\n                    cand = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite entries and reflect to bounds\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.where(np.isfinite(cand), cand, lb + 0.5 * span)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, [f_cand]])\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                x_best = cand.copy()\n                f_best = float(f_cand)\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25%\n            success = False\n            finite_idx = np.where(np.isfinite(F))[0]\n            if finite_idx.size > 0 and np.isfinite(f_cand):\n                # compute rank among finite values (1 = best)\n                sorted_finite = np.sort(F[finite_idx])\n                # find cur_rank (1-based)\n                cur_rank = int(np.searchsorted(sorted_finite, f_cand, side=\"left\") + 1)\n                if f_cand < f_best:\n                    success = True\n                else:\n                    if cur_rank <= max(1, int(0.25 * max(1, N))):\n                        success = True\n            else:\n                success = False\n\n            # update short-term success history\n            self.success_history.append(1 if success else 0)\n            if len(self.success_history) > self.short_window:\n                self.success_history.pop(0)\n\n            # prune archive if too large\n            if X.shape[0] > self.archive_max:\n                Ncurr = X.shape[0]\n                keep_best = int(max(10, 0.4 * self.archive_max))\n                keep_rand = int(max(5, 0.2 * self.archive_max))\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_idx = list(order_now[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                if len(rest) > 0:\n                    k = min(keep_rand, len(rest))\n                    rnd_keep = list(self.rng.choice(rest, size=k, replace=False))\n                    keep_idx += rnd_keep\n                # ensure some kept\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(self.success_history) >= 5:\n                succ_rate = np.mean(self.success_history)\n                if succ_rate < 0.08:\n                    self.gscale = min(self.gscale * 1.25 + 1e-6, self.gscale_max)\n                elif succ_rate > 0.35:\n                    # reduce scale gently to exploit more\n                    self.gscale *= 0.95 * (1.0 + (self.rng.rand() - 0.5) * 0.02)\n                # clamp\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 10 * dim) and self.evals < self.budget:\n                # perform micro restarts: inject perturbed elites or randoms\n                n_inject = max(1, int(0.05 * max(2, X.shape[0])))\n                injects = []\n                for _ in range(n_inject):\n                    if elites_X.shape[0] > 0 and self.rng.rand() < 0.9:\n                        e_idx = int(self.rng.randint(0, elites_X.shape[0]))\n                        base = elites_X[e_idx]\n                        scale_mult = 1.0 + 2.0 * self.rng.rand()\n                        pert = base + self.rng.randn(dim) * scale_mult * per_dim_scale\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        injects.append(pert)\n                    else:\n                        pert = self._uniform_array(lb, ub)\n                        injects.append(pert)\n\n                # replace some worst points with injects (evaluate them)\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                worst_idx = order_now[-len(injects):] if order_now.size >= len(injects) else list(range(X.shape[0]))\n                for i, w in enumerate(worst_idx):\n                    if self.evals >= self.budget:\n                        break\n                    newx = injects[i]\n                    f_new = _safe_eval(newx)\n                    X[w] = newx\n                    F[w] = f_new\n\n                # encourage exploration: enlarge gscale a bit\n                self.gscale = min(self.gscale * 1.5, self.gscale_max)\n                iter_since_improve = 0\n\n            # break if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if np.isfinite(f_best):\n            return float(f_best), x_best\n        else:\n            center = lb + 0.5 * span\n            return float(np.inf), center\n\n\n# Example usage:\n# mg = MemoryGuidedADS(budget=1000, dim=10, seed=42)\n# f_opt, x_opt = mg(my_func)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "54ebc759-a7c1-4fe2-a793-a6a870741893", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven sampler mixing anisotropic Gaussian locals, PCA-guided elite moves, DE-like donors, heavy-tailed Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_size=None, archive_max=200,\n                 gscale=0.2, gscale_min=1e-6, gscale_max=2.0,\n                 short_window=50):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # initial number of points to seed the archive\n        if init_size is None:\n            self.init_size = min(max(6, 2 * self.dim), 2 * self.dim + 20)\n        else:\n            self.init_size = int(init_size)\n\n        self.archive_max = int(archive_max)\n\n        # adaptive global scale (relative to problem span)\n        self.gscale = float(gscale)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n\n        # short-term adaptation memory\n        self.short_window = int(short_window)\n\n        # counters\n        self.evals = 0\n\n    # ----- utilities -----\n    def _extract_bounds(self, func):\n        # Try some common attributes; fallback to [-5,5]^dim\n        dim = self.dim\n        lb = None\n        ub = None\n        # try func.bounds\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            try:\n                arr = np.asarray(b, dtype=float)\n                if arr.ndim == 2 and arr.shape[0] == 2 and arr.shape[1] == dim:\n                    lb = arr[0].astype(float)\n                    ub = arr[1].astype(float)\n                elif arr.ndim == 1 and arr.size == 2:\n                    # maybe bounding scalars\n                    lb = np.full(dim, float(arr[0]))\n                    ub = np.full(dim, float(arr[1]))\n                else:\n                    # maybe tuple of (lb,ub)\n                    try:\n                        a0 = np.asarray(b[0], dtype=float)\n                        a1 = np.asarray(b[1], dtype=float)\n                        lb = np.broadcast_to(a0, (dim,)).astype(float)\n                        ub = np.broadcast_to(a1, (dim,)).astype(float)\n                    except Exception:\n                        pass\n            except Exception:\n                pass\n\n        # other common names\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            try:\n                lb = np.asarray(getattr(func, \"lower_bounds\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            try:\n                ub = np.asarray(getattr(func, \"upper_bounds\"), dtype=float)\n            except Exception:\n                ub = None\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"ub\"):\n            try:\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                ub = None\n\n        # broadcast scalars if needed\n        if lb is not None:\n            lb = np.atleast_1d(lb).astype(float)\n            if lb.size == 1:\n                lb = np.full(dim, lb.item())\n        if ub is not None:\n            ub = np.atleast_1d(ub).astype(float)\n            if ub.size == 1:\n                ub = np.full(dim, ub.item())\n\n        # final fallback\n        if lb is None or ub is None or lb.size != dim or ub.size != dim:\n            lb = np.full(dim, -5.0)\n            ub = np.full(dim, 5.0)\n\n        # enforce finite\n        finite_mask = np.isfinite(lb) & np.isfinite(ub)\n        if not np.all(finite_mask):\n            lb[~finite_mask] = -5.0\n            ub[~finite_mask] = 5.0\n\n        # ensure lb < ub\n        bad = lb >= ub\n        if np.any(bad):\n            ub[bad] = lb[bad] + 1.0\n            ub = np.minimum(ub, lb + 10.0)  # arbitrary clamp\n\n        return lb.astype(float).copy(), ub.astype(float).copy()\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.asarray(x, dtype=float).copy()\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            x[below] = 2 * lb[below] - x[below]\n            x[above] = 2 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        self.evals = 0\n        dim = self.dim\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                x_in = np.asarray(x, dtype=float)\n                f = func(x_in)\n                # ensure scalar\n                f = float(np.asarray(f).item())\n                if not np.isfinite(f):\n                    f = float(np.inf)\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # initialization: LHS-like seeding\n        X_list = []\n        F_list = []\n        init_n = max(1, min(self.init_size, self.budget))\n        # stratified LHS-like\n        if init_n == 1:\n            X_list.append(self._uniform_array(lb, ub))\n        else:\n            strata_edges = np.linspace(0.0, 1.0, init_n + 1)\n            perms = [self.rng.permutation(init_n) for _ in range(dim)]\n            for i in range(init_n):\n                xi = np.empty(dim, dtype=float)\n                for d in range(dim):\n                    s = perms[d][i]\n                    lo = strata_edges[s]\n                    hi = strata_edges[s + 1]\n                    frac = lo + self.rng.rand() * (hi - lo)\n                    xi[d] = lb[d] + frac * span[d]\n                X_list.append(xi)\n\n        # ensure at least two if budget allows\n        if self.budget >= 2 and len(X_list) < 2:\n            X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = _safe_eval(x)\n            X_list.append  # noqa: F401 (placeholder removed by garbage collector)\n            F_list.append(f)\n        # Note: above accidentally appended a no-op; fix properly by building arrays below\n\n        # Proper evaluation and arrays (re-evaluate in case previous loop was cut)\n        X_list = []\n        F_list = []\n        for i in range(init_n):\n            if self.evals >= self.budget:\n                break\n            x = lb + self.rng.rand(dim) * span\n            f = _safe_eval(x)\n            X_list.append(np.array(x, dtype=float))\n            F_list.append(float(f))\n\n        # If no initial points evaluated (budget 0), return\n        if len(X_list) == 0:\n            center = lb + 0.5 * span\n            return float(np.inf), center\n\n        X = np.vstack(X_list)\n        F = np.array(F_list, dtype=float)\n\n        # trackers\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = int(np.nanargmin(np.where(np.isfinite(F), F, np.inf)))\n            x_best = X[best_idx].copy()\n            f_best = float(F[best_idx])\n        else:\n            x_best = lb + 0.5 * span\n            f_best = float(np.inf)\n\n        # short-term adaptation\n        self.success_history = []\n        total_iters = 0\n        iter_since_improve = 0\n\n        # main loop\n        while self.evals < self.budget:\n            total_iters += 1\n            N = X.shape[0]\n\n            # ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            n_elite = max(2, int(max(2, 0.12 * N)))\n            elites_idx = order_all[:n_elite] if N > 0 else np.array([], dtype=int)\n            elites_X = X[elites_idx] if elites_idx.size > 0 else np.empty((0, dim))\n            elites_F = F[elites_idx] if elites_idx.size > 0 else np.empty((0,))\n\n            # per-dim adaptive scale: combine global and archive spread\n            if N > 1:\n                # use robust spread: 80% trimmed range\n                low_q = np.percentile(X, 10, axis=0)\n                high_q = np.percentile(X, 90, axis=0)\n                spread = np.maximum(high_q - low_q, 1e-12)\n            else:\n                spread = 0.25 * span\n            per_dim_scale = np.maximum(self.gscale * spread, 1e-12)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            base_idx = 0\n            try:\n                finite_vals = np.where(np.isfinite(F), F, np.nan)\n                if np.all(np.isnan(finite_vals)):\n                    weights = np.ones(N) / N\n                else:\n                    max_finite = np.nanmax(np.where(np.isfinite(F), F, np.nan))\n                    negF = -np.where(np.isfinite(F), F, max_finite + 1.0)\n                    # temperature scaling\n                    t = max(1e-3, 0.1 + 0.9 * self.rng.rand())\n                    expv = np.exp(negF / t - np.nanmax(negF / t))\n                    expv[np.isnan(expv)] = 0.0\n                    if np.sum(expv) <= 0:\n                        weights = np.ones(N) / N\n                    else:\n                        weights = expv / np.sum(expv)\n                base_idx = int(self.rng.choice(np.arange(N), p=weights))\n            except Exception:\n                base_idx = int(self.rng.randint(0, N))\n            base_x = X[base_idx].copy()\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n            cand = None\n\n            # LOCAL\n            if r < 0.35:\n                # anisotropic gaussian jitter (jitter per-dim)\n                jitter = 1.0 + 0.3 * (self.rng.randn(dim) * self.rng.rand(dim))\n                noise = self.rng.randn(dim) * per_dim_scale * jitter\n                pull = (np.nanmean(X, axis=0) - base_x) * (0.05 * self.rng.rand())\n                cand = base_x + noise + pull\n\n            # PCA-guided elite moves\n            elif r < 0.60:\n                try:\n                    if elites_X.shape[0] >= 2:\n                        emean = np.mean(elites_X, axis=0)\n                        Xc = elites_X - emean\n                        U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # principal components rows\n                        # sample coefficients emphasizing top comps\n                        k = min(len(svals), dim)\n                        comp_weights = (svals[:k] / (np.sum(svals[:k]) + 1e-12))\n                        # generate coefficients with heavier sampling on top components\n                        z = self.rng.randn(k) * (1.0 + 1.5 * (1.0 - comp_weights))\n                        step = np.zeros(dim)\n                        for j in range(k):\n                            step += z[j] * comps[j]\n                        cand = emean + step * (1.5 * np.mean(per_dim_scale)) + 0.05 * self.rng.randn(dim) * per_dim_scale\n                        # occasionally move from base instead of elite mean\n                        if self.rng.rand() < 0.35:\n                            cand = base_x + 0.4 * (cand - base_x)\n                    else:\n                        # fallback to anisotropic gaussian around base\n                        cand = base_x + self.rng.randn(dim) * per_dim_scale\n                except Exception:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # Differential-like donor\n            elif r < 0.80:\n                if N >= 3:\n                    ids = list(range(N))\n                    ids.remove(base_idx)\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    diff = X[a] - X[b]\n                    Fscale = 0.8 * (0.7 + 0.6 * self.rng.rand())  # adaptive-ish\n                    cand = base_x + Fscale * diff + 0.2 * self.rng.randn(dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < 0.95:\n                # heavy-tailed exploration centered sometimes on best, sometimes on base\n                if self.rng.rand() < 0.6:\n                    center_choice = x_best\n                else:\n                    center_choice = base_x\n                cauch = self.rng.standard_cauchy(size=dim)\n                # scale cauchy by per_dim_scale but avoid extreme infinities\n                cauch = np.clip(cauch, -1e6, 1e6)\n                multiplier = self.rng.lognormal(mean=0.0, sigma=0.5)\n                cand = center_choice + (cauch * (0.5 * per_dim_scale) * multiplier)\n                # small gaussian nudges\n                cand = cand + 0.01 * self.rng.randn(dim) * per_dim_scale\n\n            # ARCHIVE_MIX (rare)\n            else:\n                rand_idx = self.rng.randint(0, N)\n                donor = X[rand_idx]\n                mix_mask = self.rng.rand(dim) < (0.2 + 0.4 * self.rng.rand())  # random subset to mix\n                cand = base_x.copy()\n                if np.any(mix_mask):\n                    noise = 0.01 * self.rng.randn(np.sum(mix_mask)) * per_dim_scale[mix_mask]\n                    cand[mix_mask] = donor[mix_mask] + noise\n                # small chance to replace entirely with uniform\n                if self.rng.rand() < 0.03:\n                    cand = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite entries and reflect to bounds\n            if cand is None:\n                cand = lb + 0.5 * span\n            cand = np.where(np.isfinite(cand), cand, lb + 0.5 * span)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if self.evals >= self.budget:\n                break\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, [f_cand]])\n            N = X.shape[0]\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25%\n            success = False\n            finite_idx = np.where(np.isfinite(F))[0]\n            if finite_idx.size > 0:\n                sorted_finite = np.sort(F[finite_idx])\n                # find current rank (1-based) among finite values\n                # use searchsorted into sorted_finite\n                try:\n                    cur_rank = int(np.searchsorted(sorted_finite, f_cand, side=\"left\") + 1)\n                    if cur_rank <= max(1, int(0.25 * max(1, sorted_finite.size))):\n                        success = True\n                except Exception:\n                    success = False\n            if improved:\n                success = True\n\n            # update short-term success history\n            self.success_history.append(1 if success else 0)\n            if len(self.success_history) > self.short_window:\n                self.success_history.pop(0)\n\n            # prune archive if too large\n            if X.shape[0] > self.archive_max:\n                Ncurr = X.shape[0]\n                keep_best = int(max(10, 0.4 * self.archive_max))\n                keep_rand = int(max(5, 0.2 * self.archive_max))\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_idx = list(order_now[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                k = min(len(rest), keep_rand)\n                if k > 0:\n                    rnd_keep = list(self.rng.choice(rest, size=k, replace=False))\n                    keep_idx += rnd_keep\n                # ensure at least 2 kept\n                if len(keep_idx) < 2:\n                    keep_idx = list(order_now[:min(2, Ncurr)])\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(self.success_history) >= 5:\n                succ_rate = np.mean(self.success_history)\n                if succ_rate < 0.08:\n                    # increase exploration\n                    self.gscale *= 1.08 * (1.0 + (self.rng.rand() - 0.5) * 0.02)\n                elif succ_rate > 0.35:\n                    # reduce scale gently to exploit more\n                    self.gscale *= 0.95 * (1.0 + (self.rng.rand() - 0.5) * 0.02)\n                # clamp\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            stagn_thresh = max(50, 10 * dim)\n            if iter_since_improve > stagn_thresh and self.evals < self.budget:\n                # perform micro restarts: inject perturbed elites or randoms\n                n_inject = max(1, int(0.05 * max(2, X.shape[0])))\n                injects = []\n                for _ in range(n_inject):\n                    if elites_X.shape[0] > 0 and self.rng.rand() < 0.7:\n                        e_idx = int(self.rng.randint(0, elites_X.shape[0]))\n                        base_inj = elites_X[e_idx]\n                        scale_mult = 1.0 + 2.0 * self.rng.rand()\n                        pert = base_inj + self.rng.randn(dim) * scale_mult * per_dim_scale\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        injects.append(pert)\n                    else:\n                        # fresh uniform points\n                        inj = self._uniform_array(lb, ub)\n                        injects.append(inj)\n                # replace some worst points with injects (evaluate them)\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                worst_idx = order_now[-len(injects):] if order_now.size >= len(injects) else list(range(X.shape[0]))\n                for i, w in enumerate(worst_idx):\n                    if self.evals >= self.budget:\n                        break\n                    newx = injects[i]\n                    f_new = _safe_eval(newx)\n                    X[w] = newx\n                    F[w] = f_new\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = newx.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit\n                self.gscale *= 1.08\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n                iter_since_improve = 0\n\n            # break if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if np.isfinite(f_best):\n            return float(f_best), x_best\n        else:\n            center = lb + 0.5 * span\n            return float(np.inf), center\n\n\n# Example usage:\n# mg = MemoryGuidedADS(budget=1000, dim=10, seed=42)\n# f_opt, x_opt = mg(my_func)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a2b06e08-d07b-4070-b1e4-57049449c2be", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid, archive-driven continuous optimizer mixing anisotropic Gaussians, PCA-guided elite moves, DE-like differential donors, heavy-tailed Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_size=None, archive_max=200,\n                 gscale=0.2, gscale_min=1e-6, gscale_max=4.0,\n                 short_window=50):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # initial number of points to seed the archive\n        if init_size is None:\n            self.init_size = max(4, min(int(10 * self.dim), self.budget))\n        else:\n            self.init_size = int(init_size)\n        self.archive_max = int(archive_max)\n        # adaptive global scale (relative to problem span)\n        self.gscale = float(gscale)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        # short-term adaptation memory\n        self.short_window = int(short_window)\n        self.success_history = deque(maxlen=self.short_window)\n        # counters\n        self.evals = 0\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try to extract bounds; otherwise default to [-5,5]\n        dim = self.dim\n        lb = None\n        ub = None\n\n        # common attribute patterns\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    arr = np.asarray(b, dtype=float)\n                    if arr.ndim == 2 and arr.shape[0] == 2 and arr.shape[1] == dim:\n                        lb = arr[0]\n                        ub = arr[1]\n            except Exception:\n                lb = None\n                ub = None\n\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            try:\n                lb = np.asarray(func.lower_bounds, dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            try:\n                ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                ub = None\n\n        # simple scalar attributes\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"ub\"):\n            try:\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                ub = None\n\n        # fallback:\n        if lb is None:\n            lb = np.full(dim, -5.0)\n        else:\n            lb = np.atleast_1d(lb).astype(float)\n            if lb.size == 1:\n                lb = np.full(dim, lb.item())\n            elif lb.size != dim:\n                try:\n                    lb = np.broadcast_to(lb, (dim,))\n                except Exception:\n                    lb = np.full(dim, -5.0)\n\n        if ub is None:\n            ub = np.full(dim, 5.0)\n        else:\n            ub = np.atleast_1d(ub).astype(float)\n            if ub.size == 1:\n                ub = np.full(dim, ub.item())\n            elif ub.size != dim:\n                try:\n                    ub = np.broadcast_to(ub, (dim,))\n                except Exception:\n                    ub = np.full(dim, 5.0)\n\n        # enforce finite and orientation\n        finite_mask = np.isfinite(lb) & np.isfinite(ub)\n        if not np.all(finite_mask):\n            lb[~finite_mask] = -5.0\n            ub[~finite_mask] = 5.0\n\n        # ensure lb < ub elementwise\n        swapped = lb >= ub\n        if np.any(swapped):\n            tmp = lb.copy()\n            lb[swapped] = -5.0\n            ub[swapped] = 5.0\n\n        return lb.copy(), ub.copy()\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.asarray(x, dtype=float).copy()\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            x[below] = 2 * lb[below] - x[below]\n            x[above] = 2 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        self.evals = 0\n        dim = self.dim\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 10.0  # fallback\n        center = lb + 0.5 * span\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                x_arr = np.asarray(x, dtype=float).reshape((dim,))\n            except Exception:\n                return float(np.inf)\n            try:\n                f = float(func(x_arr))\n            except Exception:\n                f = float(np.inf)\n            # only increment when a call was attempted\n            self.evals += 1\n            return f\n\n        # initialization: Latin-hypercube-like seeding\n        init_n = min(self.init_size, max(1, self.budget))\n        X_init = []\n        if init_n == 1:\n            X_init.append(center.copy())\n        else:\n            # LHS-like: for each dim create permuted strata\n            strata_edges = np.linspace(0.0, 1.0, init_n + 1)\n            perms = [self.rng.permutation(init_n) for _ in range(dim)]\n            for s in range(init_n):\n                xi = np.empty(dim, dtype=float)\n                for d in range(dim):\n                    lo = strata_edges[perms[d][s]]\n                    hi = strata_edges[perms[d][s] + 1]\n                    frac = lo + self.rng.rand() * (hi - lo)\n                    xi[d] = lb[d] + frac * span[d]\n                X_init.append(xi)\n\n        # ensure at least two if budget allows\n        if self.budget >= 2 and len(X_init) < 2:\n            X_init.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population\n        X_list = []\n        F_list = []\n        for x in X_init:\n            if self.evals >= self.budget:\n                break\n            f = _safe_eval(x)\n            X_list.append(np.array(x, dtype=float))\n            F_list.append(float(f))\n\n        if len(X_list) == 0:\n            # no evaluations possible\n            return float(np.inf), center\n\n        X = np.vstack(X_list)\n        F = np.array(F_list, dtype=float)\n\n        # track best\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = int(np.nanargmin(F))\n            x_best = X[best_idx].copy()\n            f_best = float(F[best_idx])\n        else:\n            x_best = center.copy()\n            f_best = float(np.inf)\n\n        # short-term adaptation\n        self.success_history.clear()\n        iter_since_improve = 0\n        total_iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            total_iters += 1\n            N = X.shape[0]\n\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elite_frac = 0.2\n            n_elite = max(2, int(np.ceil(elite_frac * max(2, N))))\n            elites_idx = order_all[:n_elite]\n            elites_X = X[elites_idx]\n\n            # per-dim adaptive scale: combine global and archive spread\n            if N > 1:\n                # robust spread: interquartile range ~ IQR/1.349 ~ std\n                q75 = np.nanpercentile(X, 75, axis=0)\n                q25 = np.nanpercentile(X, 25, axis=0)\n                spread = (q75 - q25) / 1.349\n                spread = np.maximum(spread, 0.05 * span)  # ensure not too small\n            else:\n                spread = 0.25 * span\n            per_dim_scale = np.maximum(self.gscale * spread, 1e-12)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            base_idx = None\n            if np.any(np.isfinite(F)):\n                finite_vals = np.where(np.isfinite(F), F, np.nan)\n                max_finite = np.nanmax(finite_vals)\n                negF = -np.where(np.isfinite(F), F, max_finite + 1.0)\n                negF = negF - np.nanmax(negF)\n                expv = np.exp(negF / (1e-8 + np.std(negF) + 1.0))\n                if not np.isfinite(np.sum(expv)) or np.sum(expv) <= 0:\n                    base_idx = int(self.rng.randint(0, N))\n                else:\n                    weights = expv / np.sum(expv)\n                    base_idx = int(self.rng.choice(np.arange(N), p=weights))\n            else:\n                base_idx = int(self.rng.randint(0, N))\n\n            base_x = X[base_idx].copy()\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n            cand = None\n\n            # LOCAL move\n            if r < 0.35:\n                jitter = 0.8 + 0.8 * self.rng.rand()\n                noise = self.rng.randn(dim) * per_dim_scale * jitter\n                pull = (np.nanmean(X, axis=0) - base_x) * (0.02 + 0.08 * self.rng.rand())\n                cand = base_x + noise + pull\n\n            # PCA-guided elite moves\n            elif r < 0.60:\n                try:\n                    emean = np.mean(elites_X, axis=0)\n                    Xc = elites_X - emean\n                    if Xc.shape[0] >= 2:\n                        U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # rows principal components\n                        # weights on components proportional to singular values\n                        comp_weights = svals / (np.sum(svals) + 1e-12)\n                        k = min(len(svals), dim)\n                        # sample coefficients: emphasize top comps\n                        coeff = self.rng.randn(k) * (1.0 + 1.5 * self.rng.rand(k) * comp_weights[:k])\n                        step = np.dot(coeff, comps[:k, :])\n                        cand = emean + step * (1.2 * per_dim_scale) + 0.05 * self.rng.randn(dim) * per_dim_scale\n                        # occasionally start from base_x instead of emean\n                        if self.rng.rand() < 0.15:\n                            cand = base_x + (cand - emean) * (0.7 + 0.6 * self.rng.rand())\n                    else:\n                        raise np.linalg.LinAlgError(\"Not enough elites\")\n                except Exception:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # Differential-like donor\n            elif r < 0.80:\n                if N >= 3:\n                    ids = list(range(N))\n                    ids.remove(base_idx)\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    Fscale = 0.7 * (0.6 + 0.8 * self.rng.rand())\n                    donor = X[a]\n                    diff = X[a] - X[b]\n                    cand = base_x + Fscale * diff + 0.3 * self.rng.randn(dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < 0.95:\n                if self.rng.rand() < 0.6 and np.isfinite(f_best):\n                    center_choice = x_best\n                else:\n                    center_choice = base_x\n                cauch = self.rng.standard_cauchy(size=dim)\n                # temper very large outliers\n                cauch = np.tanh(cauch / 10.0) * 10.0\n                multiplier = self.rng.lognormal(mean=0.0, sigma=0.6)\n                cand = center_choice + (0.6 * multiplier) * cauch * per_dim_scale\n                cand += 0.03 * self.rng.randn(dim) * per_dim_scale\n\n            # ARCHIVE_MIX\n            else:\n                # mix with a random (or archived) donor\n                if X.shape[0] > 1 and self.rng.rand() < 0.9:\n                    rand_idx = self.rng.randint(0, N)\n                    donor = X[rand_idx]\n                else:\n                    donor = self._uniform_array(lb, ub)\n                mix_mask = self.rng.rand(dim) < (0.15 + 0.5 * self.rng.rand())\n                noise = 0.4 * self.rng.randn(dim) * per_dim_scale\n                cand = base_x.copy()\n                if np.any(mix_mask):\n                    cand[mix_mask] = donor[mix_mask] + noise[mix_mask]\n                if self.rng.rand() < 0.08:\n                    # small prob to inject uniform\n                    cand = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite entries and reflect to bounds\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.rand(np.sum(bad)) * (ub[bad] - lb[bad])\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive (X/F) - keep duplicates allowed (low overhead)\n            X = np.vstack([X, cand.reshape((1, dim))])\n            F = np.concatenate([F, [f_cand]])\n            Ncurr = X.shape[0]\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25%\n            success = False\n            if np.isfinite(f_cand):\n                finite_idx = np.where(np.isfinite(F))[0]\n                if finite_idx.size > 0:\n                    sorted_finite = np.sort(F[finite_idx])\n                    # find cur_rank (1-based)\n                    cur_rank = int(np.searchsorted(sorted_finite, f_cand, side=\"left\") + 1)\n                    if f_cand < f_best or cur_rank <= max(1, int(0.25 * max(1, Ncurr))):\n                        success = True\n            if success:\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n            # prune archive if too large: keep the best ones and some randoms\n            if Ncurr > self.archive_max:\n                keep_best = int(max(5, 0.6 * self.archive_max))\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_idx = list(order_now[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                # keep some random survivors\n                k = min(int(max(1, 0.2 * self.archive_max)), len(rest))\n                if k > 0:\n                    rnd_keep = list(self.rng.choice(rest, size=k, replace=False))\n                    keep_idx += rnd_keep\n                # ensure some kept\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(self.success_history) >= min(5, self.short_window):\n                succ_rate = float(np.sum(self.success_history)) / len(self.success_history)\n                if succ_rate < 0.08:\n                    # reduce scale gently to exploit more\n                    self.gscale = max(self.gscale_min, self.gscale * (0.92 + 0.02 * self.rng.rand()))\n                elif succ_rate > 0.4:\n                    # increase scale to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.05 + 0.03 * self.rng.rand()))\n                else:\n                    # small jitter\n                    self.gscale = np.clip(self.gscale * (1.0 + (self.rng.rand() - 0.5) * 0.02), self.gscale_min, self.gscale_max)\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 12 * dim) and self.evals < self.budget:\n                n_inject = max(1, int(0.06 * max(2, X.shape[0])))\n                injects = []\n                for _ in range(n_inject):\n                    if self.rng.rand() < 0.6 and np.isfinite(f_best):\n                        # perturb elite/best\n                        pert = x_best + 0.5 * self.rng.randn(dim) * per_dim_scale\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        injects.append(pert)\n                    else:\n                        injects.append(self._uniform_array(lb, ub))\n\n                # replace some worst points with injects (evaluate them)\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                worst_idx = order_now[::-1][:len(injects)]\n                for i, w in enumerate(worst_idx):\n                    if self.evals >= self.budget:\n                        break\n                    newx = injects[i]\n                    f_new = _safe_eval(newx)\n                    X[w] = newx\n                    F[w] = f_new\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = newx.copy()\n                        iter_since_improve = 0\n\n                # encourage exploration: enlarge gscale a bit\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                iter_since_improve = 0  # reset stagnation counter\n\n            # safety stop if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if np.isfinite(f_best):\n            return float(f_best), x_best.copy()\n        else:\n            return float(np.inf), center.copy()\n\n\n# Example usage:\n# mg = MemoryGuidedADS(budget=1000, dim=10, seed=42)\n# f_opt, x_opt = mg(my_func)", "configspace": "", "generation": 0, "feedback": "In the code, line 32, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_history = deque(maxlen=self.short_window)", "error": "In the code, line 32, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_history = deque(maxlen=self.short_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "85ccac27-acb3-4596-bc35-d813d6a95990", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite exploration, DE-style donors, heavy-tailed Cauchy jumps, short-term scale adaptation and micro-restarts to robustly handle diverse continuous problems.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    Hybrid archive-driven continuous optimizer mixing anisotropic Gaussians,\n    PCA-guided elite moves, DE-like differential donors, heavy-tailed Cauchy\n    jumps, archive mixing, short-term scale adaptation and micro-restarts.\n    One-line: memory-guided hybrid sampler mixing directionally-adapted proposals\n    to balance exploitation and heavy-tailed exploration.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_size=None, archive_max=500,\n                 gscale=0.25, gscale_min=1e-6, gscale_max=4.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.init_size = int(init_size) if init_size is not None else max(4, min(10 * self.dim, max(4, int(0.05 * self.budget))))\n        self.archive_max = int(archive_max)\n        self.gscale = float(gscale)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n\n        # bookkeeping\n        self.evals = 0\n\n    # ----- helpers -----\n    def _get_bounds(self, func):\n        # Try to extract bounds from common attributes; else fallback to [-5,5]\n        dim = self.dim\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.atleast_1d(np.asarray(b.lb, dtype=float))\n                    ub = np.atleast_1d(np.asarray(b.ub, dtype=float))\n            # try various common names\n            if lb is None and hasattr(func, \"lower_bounds\"):\n                lb = np.atleast_1d(np.asarray(func.lower_bounds, dtype=float))\n            if ub is None and hasattr(func, \"upper_bounds\"):\n                ub = np.atleast_1d(np.asarray(func.upper_bounds, dtype=float))\n            if lb is None and hasattr(func, \"lb\"):\n                lb = np.atleast_1d(np.asarray(getattr(func, \"lb\"), dtype=float))\n            if ub is None and hasattr(func, \"ub\"):\n                ub = np.atleast_1d(np.asarray(getattr(func, \"ub\"), dtype=float))\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(dim, -5.0)\n            ub = np.full(dim, 5.0)\n        else:\n            lb = np.atleast_1d(lb).astype(float)\n            ub = np.atleast_1d(ub).astype(float)\n            if lb.size != dim:\n                try:\n                    lb = np.broadcast_to(lb, (dim,))\n                except Exception:\n                    lb = np.full(dim, -5.0)\n            if ub.size != dim:\n                try:\n                    ub = np.broadcast_to(ub, (dim,))\n                except Exception:\n                    ub = np.full(dim, 5.0)\n            # ensure finite\n            finite_mask = np.isfinite(lb) & np.isfinite(ub)\n            if not np.all(finite_mask):\n                lb[~finite_mask] = -5.0\n                ub[~finite_mask] = 5.0\n            # enforce lb < ub\n            swapped = lb >= ub\n            if np.any(swapped):\n                # for problematic dims revert to [-5,5]\n                lb[swapped] = -5.0\n                ub[swapped] = 5.0\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        x = np.asarray(x, dtype=float).copy()\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            x[below] = 2 * lb[below] - x[below]\n            x[above] = 2 * ub[above] - x[above]\n        # final clamp\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            arr = self.rng.rand(n, self.dim) * (ub - lb) + lb\n            return arr\n\n    # ----- main call -----\n    def __call__(self, func):\n        self.evals = 0\n        dim = self.dim\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 10.0\n\n        # safe evaluation wrapper: increments evals only when a call is attempted and stops at budget\n        def _safe_eval(x):\n            if self.evals >= self.budget:\n                return float(np.inf)\n            self.evals += 1\n            try:\n                fx = func(x)\n                # ensure numeric\n                if fx is None:\n                    return float(np.inf)\n                return float(fx)\n            except Exception:\n                # on failure treat as very bad\n                return float(np.inf)\n\n        # initialize archive X,F with Latin-hypercube-like seeding (simple stratified per-dim)\n        init_n = min(self.init_size, max(1, self.budget))\n        X_list = []\n        F_list = []\n        # simple LHS-like: for each dim create permutation of strata and sample\n        perms = [self.rng.permutation(init_n) for _ in range(dim)]\n        strata_edges = np.linspace(0.0, 1.0, init_n + 1)\n        for i in range(init_n):\n            xi = np.empty(dim, dtype=float)\n            for d in range(dim):\n                lo = strata_edges[perms[d][i]]\n                hi = strata_edges[perms[d][i] + 1]\n                frac = lo + self.rng.rand() * (hi - lo)\n                xi[d] = lb[d] + frac * span[d]\n            f = _safe_eval(xi)\n            X_list.append(xi.copy())\n            F_list.append(float(f))\n            if self.evals >= self.budget:\n                break\n\n        if len(X_list) == 0:\n            # no evals possible\n            return float(np.inf), (lb + ub) / 2.0\n\n        X = np.vstack(X_list)\n        F = np.array(F_list, dtype=float)\n\n        # track best finite\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            idx_best = int(np.nanargmin(np.where(finite_mask, F, np.inf)))\n            f_best = float(F[idx_best])\n            x_best = X[idx_best].copy()\n        else:\n            f_best = float(np.inf)\n            x_best = (lb + ub) / 2.0\n\n        # short-term adaptation windows\n        success_history = []\n        max_hist = 50\n        iter_since_improve = 0\n        total_iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            total_iters += 1\n            N = X.shape[0]\n\n            # compute ranking and elites\n            elite_frac = 0.2\n            n_elite = max(2, int(np.ceil(elite_frac * max(2, N))))\n            # robust per-dim spread\n            if N > 1:\n                try:\n                    q75 = np.nanpercentile(X, 75, axis=0)\n                    q25 = np.nanpercentile(X, 25, axis=0)\n                    spread = (q75 - q25) / 1.349\n                    # fallback if spread small\n                    small = spread <= 0\n                    spread[small] = 0.25 * span[small]\n                except Exception:\n                    spread = 0.25 * span\n            else:\n                spread = 0.25 * span\n            per_dim_scale = np.maximum(self.gscale * spread, 1e-12 * np.ones(dim))\n\n            # choose base index biased to better ranks (softmax on -F)\n            if N > 1:\n                finite_idx = np.isfinite(F)\n                if np.any(finite_idx):\n                    vals = F.copy()\n                    vals[~finite_idx] = np.nanmax(F[finite_idx]) + 10.0\n                    neg = -vals\n                    # stabilize\n                    neg = neg - np.nanmax(neg)\n                    denom = (1e-8 + np.std(neg) + 1.0)\n                    expv = np.exp(neg / denom)\n                    weights = expv / np.sum(expv)\n                    # sample\n                    base_idx = int(self.rng.choice(np.arange(N), p=weights))\n                else:\n                    base_idx = int(self.rng.randint(0, N))\n            else:\n                base_idx = 0\n\n            base_x = X[base_idx].copy()\n\n            # decide move type\n            r = self.rng.rand()\n            cand = None\n\n            # LOCAL move (35%)\n            if r < 0.35:\n                jitter = 0.8 + 0.8 * self.rng.rand()\n                noise = self.rng.randn(dim) * per_dim_scale * jitter\n                pull = (np.nanmean(X, axis=0) - base_x) * (0.02 + 0.08 * self.rng.rand())\n                cand = base_x + noise + pull\n\n            # PCA-guided elite moves (next 25%)\n            elif r < 0.35 + 0.25:\n                # pick elites\n                ids_sorted = np.argsort(F)\n                elites_idx = ids_sorted[:min(n_elite, len(ids_sorted))]\n                elites_X = X[elites_idx]\n                try:\n                    emean = np.mean(elites_X, axis=0)\n                    Xc = elites_X - emean\n                    # SVD\n                    U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    comps = Vt  # rows principal components\n                except Exception:\n                    # fallback to gaussian around mean\n                    cand = emean + self.rng.randn(dim) * per_dim_scale\n                if cand is None:\n                    comp_weights = svals / (np.sum(svals) + 1e-12)\n                    # sample coefficients emphasizing top components\n                    coeff_scale = 1.0 + 2.0 * self.rng.rand()\n                    coeffs = (self.rng.randn(len(svals)) * (comp_weights ** 0.5)) * coeff_scale\n                    if self.rng.rand() < 0.15:\n                        center = base_x\n                    else:\n                        center = emean\n                    proposal = center + (comps.T @ coeffs) * (np.mean(per_dim_scale) / (np.mean(svals) + 1e-12))\n                    # add small anisotropic noise\n                    noise = 0.2 * self.rng.randn(dim) * per_dim_scale\n                    cand = proposal + noise\n\n            # DE-like differential donors (20%)\n            elif r < 0.35 + 0.25 + 0.20:\n                if N >= 3:\n                    ids = list(range(N))\n                    # ensure indices exclude base_idx\n                    ids.remove(base_idx)\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    donor = X[a] + 0.8 * (X[a] - X[b])\n                    cand = base_x + 0.6 * (donor - base_x) + 0.5 * self.rng.randn(dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # CAUCHY jumps (15%)\n            elif r < 0.35 + 0.25 + 0.20 + 0.15:\n                center_choice = base_x if (self.rng.rand() < 0.6 or not np.isfinite(f_best)) else x_best\n                cauch = self.rng.standard_cauchy(size=dim)\n                multiplier = self.rng.lognormal(mean=0.0, sigma=0.6)\n                cand = center_choice + (0.6 * multiplier) * cauch * per_dim_scale\n                # temper extreme values:\n                cand = center_choice + np.tanh((cand - center_choice) / (5.0 * per_dim_scale + 1e-12)) * (5.0 * per_dim_scale)\n\n            # ARCHIVE_MIX (remaining small probability)\n            else:\n                # pick donor from archive or uniform\n                if self.rng.rand() < 0.6 and N > 1:\n                    donor_idx = self.rng.randint(0, N)\n                    donor = X[donor_idx]\n                else:\n                    donor = self._uniform_array(lb, ub)\n                mix_mask = self.rng.rand(dim) < (0.15 + 0.5 * self.rng.rand())\n                noise = 0.4 * self.rng.randn(dim) * per_dim_scale\n                cand = base_x.copy()\n                cand[mix_mask] = donor[mix_mask]\n                cand += noise\n                if self.rng.rand() < 0.08:\n                    # randomly inject a few uniform coords\n                    uni = self._uniform_array(lb, ub)\n                    mask2 = self.rng.rand(dim) < 0.06\n                    cand[mask2] = uni[mask2]\n\n            # safety: if candidate is None or contains NaN/inf produce uniform sample\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            # reflect to bounds (handles occasional outside values)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # Evaluate candidate (respect budget)\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape((1, dim))])\n            F = np.concatenate([F, [f_cand]])\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25%\n            success = False\n            finite_idx = np.where(np.isfinite(F))[0]\n            if finite_idx.size > 0:\n                finite_vals = F[finite_idx]\n                # position of this candidate among finite values\n                sorted_finite = np.sort(finite_vals)\n                # NB: there may be duplicates; use left insertion\n                # compute rank among existing (including current)\n                rank = int(np.searchsorted(sorted_finite, f_cand, side=\"left\") + 1)\n                if improved or rank <= max(1, int(0.25 * len(sorted_finite))):\n                    success = True\n            else:\n                # if everything is bad, any finite is success\n                success = np.isfinite(f_cand)\n\n            success_history.append(1 if success else 0)\n            if len(success_history) > max_hist:\n                success_history.pop(0)\n\n            # prune archive if too large: keep best ones and some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                keep_best = int(max(5, 0.6 * self.archive_max))\n                idx_sorted = np.argsort(F)\n                keep_idx = list(idx_sorted[:keep_best])\n                rest = list(idx_sorted[keep_best:])\n                # keep some random survivors from rest\n                k = min(int(max(1, 0.2 * self.archive_max)), len(rest))\n                if k > 0:\n                    rnd_keep = list(self.rng.choice(rest, size=k, replace=False))\n                    keep_idx += rnd_keep\n                # ensure unique and sorted\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success rate\n            if len(success_history) >= 8 and (total_iters % 8 == 0):\n                succ_rate = float(np.sum(success_history)) / len(success_history)\n                # if too successful reduce scale gently to exploit\n                if succ_rate > 0.45:\n                    self.gscale = max(self.gscale * 0.85, self.gscale_min)\n                # if failure increase to explore\n                elif succ_rate < 0.15:\n                    self.gscale = min(self.gscale * 1.25 + 1e-6, self.gscale_max)\n                else:\n                    # small jitter\n                    self.gscale = min(max(self.gscale * (0.98 + 0.04 * self.rng.rand()), self.gscale_min), self.gscale_max)\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 12 * dim) and self.evals < self.budget:\n                n_inject = max(1, int(0.06 * max(2, X.shape[0])))\n                injects = []\n                for _ in range(n_inject):\n                    if self.rng.rand() < 0.6 and np.isfinite(f_best):\n                        # perturb best\n                        pert = x_best + 0.5 * self.rng.randn(dim) * per_dim_scale\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        injects.append(pert)\n                    elif N > 0:\n                        # perturb random elite\n                        idx = self.rng.randint(0, min(N, max(1, n_elite)))\n                        pert = X[idx] + 0.8 * self.rng.randn(dim) * per_dim_scale\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        injects.append(pert)\n                    else:\n                        injects.append(self._uniform_array(lb, ub))\n                # replace some worst points with injects (evaluate them)\n                worst_idx = np.argsort(F)[-len(injects):]\n                for w_idx, newx in zip(worst_idx, injects):\n                    if self.evals >= self.budget:\n                        break\n                    f_new = _safe_eval(newx)\n                    X[w_idx] = newx\n                    F[w_idx] = f_new\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = newx.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                iter_since_improve = 0\n\n            # safety stop if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else center\n        if np.isfinite(f_best):\n            return float(f_best), x_best.copy()\n        else:\n            return float(np.inf), (lb + ub) / 2.0\n\n\n# Example usage:\n# mg = MemoryGuidedADS(budget=1000, dim=10, seed=42)\n# f_opt, x_opt = mg(my_func)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "eb9e6246-0a6c-42f0-9994-8be005b7a286", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid, archive-driven continuous optimizer mixing anisotropic Gaussians, PCA-guided elite moves, DE-like differential donors, heavy-tailed Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 short_window=50, init_size=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.short_window = int(short_window)\n        self.init_size = init_size if init_size is not None else max(6, min(20 + dim, int(0.05 * self.budget)))\n        # archive control\n        self.archive_max = max(50, 20 * dim)\n        # global scale (relative to problem span; will be adapted)\n        self.gscale = 0.1\n        self.gscale_max = 10.0\n        # tracking\n        self.evals = 0\n        self.success_history = []\n        # bookkeeping\n        self._verbose = False\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try to extract bounds from common attributes; otherwise default to [-5,5]^dim\n        dim = self.dim\n        lb = None\n        ub = None\n\n        # common attribute: bounds with .lb/.ub or .bounds.lb/.ub\n        if hasattr(func, \"bounds\"):\n            try:\n                b = getattr(func, \"bounds\")\n                if hasattr(b, \"lb\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                if hasattr(b, \"ub\"):\n                    ub = np.asarray(b.ub, dtype=float)\n            except Exception:\n                lb = lb\n                ub = ub\n\n        # other common names\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            try:\n                lb = np.asarray(getattr(func, \"lower_bounds\"), dtype=float)\n            except Exception:\n                lb = lb\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            try:\n                ub = np.asarray(getattr(func, \"upper_bounds\"), dtype=float)\n            except Exception:\n                ub = ub\n\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n            except Exception:\n                lb = lb\n        if ub is None and hasattr(func, \"ub\"):\n            try:\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                ub = ub\n\n        # fallback to default scalar bounds if needed\n        if lb is None:\n            lb = np.full(dim, -5.0)\n        if ub is None:\n            ub = np.full(dim, 5.0)\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # broadcast scalars to dimension\n        if lb.size == 1:\n            lb = np.full(dim, lb.item())\n        elif lb.size != dim:\n            lb = np.broadcast_to(lb, (dim,)).copy()\n\n        if ub.size == 1:\n            ub = np.full(dim, ub.item())\n        elif ub.size != dim:\n            ub = np.broadcast_to(ub, (dim,)).copy()\n\n        # ensure finite and lb < ub\n        lb[~np.isfinite(lb)] = -5.0\n        ub[~np.isfinite(ub)] = 5.0\n        swapped = lb >= ub\n        if np.any(swapped):\n            # fix swapped by using defaults\n            lb[swapped] = -5.0\n            ub[swapped] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.asarray(x, dtype=float).copy()\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            lb = lb.reshape((1, -1))\n            ub = ub.reshape((1, -1))\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        # get bounds\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        dim = self.dim\n        # fallback if any span non-positive\n        span[span <= 0] = 10.0\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x_arr = np.asarray(x, dtype=float).reshape((dim,))\n            try:\n                f = float(func(x_arr))\n            except Exception:\n                f = float(np.inf)\n            # only increment when a call was attempted\n            self.evals += 1\n            return f\n\n        # initialization: Latin-hypercube-like seeding\n        init_n = int(min(self.init_size, max(1, self.budget)))\n        X_list = []\n        # add center\n        center = (lb + ub) / 2.0\n        X_list.append(center.copy())\n        if init_n == 1:\n            pass\n        else:\n            # simple LHS-like seeding\n            strata = [self.rng.permutation(init_n) for _ in range(dim)]\n            for i in range(init_n - 1):\n                xi = np.empty(dim, dtype=float)\n                for d in range(dim):\n                    frac = (strata[d][i] + self.rng.rand()) / float(init_n)\n                    xi[d] = lb[d] + frac * span[d]\n                X_list.append(xi)\n            # ensure at least one uniform random\n            X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population\n        X = []\n        F = []\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = _safe_eval(x)\n            X.append(np.asarray(x, dtype=float))\n            F.append(float(f))\n        if len(X) == 0:\n            # no evaluations possible\n            return np.inf, center\n\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # track best\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = int(np.nanargmin(F))\n            x_best = X[best_idx].copy()\n            f_best = float(F[best_idx])\n        else:\n            # worst-case\n            best_idx = 0\n            x_best = X[0].copy()\n            f_best = float(np.inf)\n\n        iter_since_improve = 0\n        total_iters = 0\n\n        # short-term adaptation variables\n        self.gscale = 0.1 * np.median(span)  # initial proportional to problem span\n        self.gscale_max = max(self.gscale_max, 10.0 * np.median(span))\n\n        # main loop\n        while self.evals < self.budget:\n            total_iters += 1\n            N = X.shape[0]\n\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elites_k = max(2, int(0.2 * N))\n            elites_idx = order_all[:elites_k]\n            elites_X = X[elites_idx]\n            emean = np.nanmean(elites_X, axis=0)\n\n            # per-dim adaptive scale: combine global and archive spread\n            # robust spread: IQR -> approx 1.349 * std\n            q25 = np.nanpercentile(X, 25, axis=0)\n            q75 = np.nanpercentile(X, 75, axis=0)\n            iqr = (q75 - q25) / 1.349 + 1e-12\n            per_dim_scale = self.gscale * np.maximum(iqr, 0.02 * span)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_vals = np.where(np.isfinite(F), F, np.nan)\n            negF = -finite_vals.copy()\n            # replace nan with large negative so they get low prob\n            nanmask = ~np.isfinite(negF)\n            negF[nanmask] = np.nanmin(negF[~nanmask]) - 10.0 if np.any(~nanmask) else 0.0\n            # softmax\n            shift = np.nanmax(negF)\n            expw = np.exp(negF - shift)\n            if np.any(expw > 0) and np.isfinite(np.sum(expw)):\n                weights = expw / np.sum(expw)\n                base_idx = int(self.rng.choice(np.arange(N), p=weights))\n            else:\n                base_idx = int(self.rng.randint(0, N))\n            base_x = X[base_idx].copy()\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n            cand = None\n\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < 0.35:\n                pull = (np.nanmean(X, axis=0) - base_x) * (0.02 + 0.08 * self.rng.rand())\n                noise = self.rng.randn(dim) * per_dim_scale * (0.6 + 0.8 * self.rng.rand())\n                cand = base_x + pull + noise\n\n            # PCA-guided elite moves\n            elif r < 0.60:\n                try:\n                    Xc = elites_X - emean\n                    if Xc.shape[0] >= 2:\n                        U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        # sample coefficients emphasizing top components\n                        comp_weights = svals / (np.sum(svals) + 1e-12)\n                        # draw coefficients with heavier emphasis on top comps\n                        coeffs = self.rng.randn(len(svals)) * (0.5 + 2.0 * comp_weights)\n                        step = coeffs @ Vt  # linear combination in original space\n                        cand = emean + step * per_dim_scale * (0.8 + 0.6 * self.rng.rand())\n                    else:\n                        cand = base_x + self.rng.randn(dim) * per_dim_scale\n                except Exception:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # Differential-like donor\n            elif r < 0.80:\n                if N >= 3:\n                    # pick two distincts not equal to base_idx\n                    ids = [i for i in range(N) if i != base_idx]\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    donor = X[a]\n                    diff = X[a] - X[b]\n                    Fscale = 0.6 + 0.6 * self.rng.rand()\n                    cand = base_x + Fscale * diff + 0.3 * self.rng.randn(dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < 0.95:\n                center_choice = base_x if self.rng.rand() < 0.85 else emean\n                cauch = self.rng.standard_cauchy(size=dim)\n                # temper very large outliers and scale to per-dim\n                cauch = np.tanh(0.5 * cauch)  # compress tails\n                cand = center_choice + 0.6 * cauch * per_dim_scale\n                cand += 0.03 * self.rng.randn(dim) * per_dim_scale\n\n            # ARCHIVE_MIX\n            else:\n                rand_idx = int(self.rng.randint(0, N))\n                donor = X[rand_idx]\n                cand = base_x.copy()\n                # mix a random subset of coordinates from donor\n                mix_mask = self.rng.rand(dim) < (0.15 + 0.35 * self.rng.rand())\n                cand[mix_mask] = donor[mix_mask]\n                # small gaussian perturbation\n                cand += 0.2 * self.rng.randn(dim) * per_dim_scale\n                # occasionally inject uniform\n                if self.rng.rand() < 0.03:\n                    cand = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite entries and reflect to bounds\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.rand(np.sum(bad)) * (ub[bad] - lb[bad])\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if self.evals >= self.budget:\n                break\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, np.array([f_cand], dtype=float)])\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                iter_since_improve = 0\n                improved = True\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25%\n            if np.isfinite(f_cand):\n                # find current rank\n                cur_rank = int(np.sum(F < f_cand))  # number strictly better\n                if improved or cur_rank <= max(1, int(0.25 * max(1, X.shape[0]))):\n                    self.success_history.append(1)\n                else:\n                    self.success_history.append(0)\n            else:\n                self.success_history.append(0)\n            # keep success_history window small\n            if len(self.success_history) > self.short_window:\n                self.success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                keep_best = int(max(5, 0.6 * self.archive_max))\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_idx = list(order_now[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                # keep some random survivors\n                n_random_keep = int(max(0, min(len(rest), self.archive_max - keep_best)))\n                if n_random_keep > 0:\n                    random_keep = list(self.rng.choice(rest, size=n_random_keep, replace=False))\n                    keep_idx.extend(random_keep)\n                # ensure some kept\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(self.success_history) >= min(5, self.short_window):\n                succ_rate = float(np.sum(self.success_history)) / len(self.success_history)\n                if succ_rate > 0.35:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(1e-12, self.gscale * 0.85)\n                elif succ_rate < 0.07:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale * 1.25, self.gscale_max)\n                else:\n                    # small jitter\n                    self.gscale *= 1.0 + (self.rng.rand() - 0.5) * 0.06\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(40, 8 * dim):\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = min(5 + dim // 2, max(1, int(0.1 * self.archive_max)))\n                injects = []\n                for _ in range(n_inject):\n                    if self.rng.rand() < 0.6 and np.isfinite(f_best):\n                        # perturb elite/best\n                        perturb = x_best + self.rng.randn(dim) * per_dim_scale * (1.5 + self.rng.rand() * 2.0)\n                        injects.append(self._reflect_bounds(perturb, lb, ub))\n                    else:\n                        injects.append(self._uniform_array(lb, ub))\n                # evaluate them (replace worst)\n                for newx in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_new = _safe_eval(newx)\n                    # put into archive: replace worst if archive full-ish\n                    if X.shape[0] < self.archive_max:\n                        X = np.vstack([X, newx.reshape(1, -1)])\n                        F = np.concatenate([F, np.array([f_new], dtype=float)])\n                    else:\n                        order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                        w = order_now[-1]  # worst index\n                        X[w] = newx\n                        F[w] = f_new\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = newx.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                iter_since_improve = 0\n\n            # safety stop if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if not np.isfinite(f_best):\n            return float(np.inf), center\n        return float(f_best), x_best", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4ade5b3f-671a-4c0e-8697-5f7e5e32493e", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven continuous optimizer mixing anisotropic Gaussians, PCA-guided elite moves, DE-like donors, heavy-tailed Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 short_window=32, archive_max=200, init_pop=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.short_window = int(short_window)\n        self.archive_max = int(archive_max)\n        # global scale (relative to problem span; will be adapted)\n        self.gscale = 0.5  # initial global scale (relative)\n        self.gscale_max = 5.0\n        # tracking\n        self.success_history = []\n        # bookkeeping\n        self._verbose = False\n        # initial population size\n        if init_pop is None:\n            self.init_pop = min(max(4, 8 + self.dim), max(4, int(0.08 * self.budget)))\n        else:\n            self.init_pop = int(init_pop)\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        dim = self.dim\n        lb = None\n        ub = None\n        # try common attributes\n        try:\n            if hasattr(func, \"bounds\"):\n                b = getattr(func, \"bounds\")\n                # obj with lb/ub or bounds.lb/bounds.ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # try func.lb / func.ub\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"ub\"):\n            try:\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                ub = None\n\n        # fall back to [-5,5]^dim\n        if lb is None:\n            lb = np.full(dim, -5.0)\n        if ub is None:\n            ub = np.full(dim, 5.0)\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # broadcast scalars to dimension\n        if lb.size == 1:\n            lb = np.broadcast_to(lb, (dim,)).copy()\n        if ub.size == 1:\n            ub = np.broadcast_to(ub, (dim,)).copy()\n\n        # ensure finite and lb < ub; otherwise fallback default\n        for i in range(dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        x = np.asarray(x, dtype=float).copy()\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not np.any(below) and not np.any(above):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        dim = self.dim\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 10.0\n\n        evals = 0\n        self.evals = 0\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            nonlocal evals\n            # do not call if budget exhausted\n            if evals >= self.budget:\n                return float(\"inf\")\n            # ensure numpy array\n            x_arr = np.asarray(x, dtype=float)\n            # clip to box to be safe\n            x_arr = np.minimum(np.maximum(x_arr, lb), ub)\n            try:\n                f = float(func(x_arr))\n            except Exception:\n                f = float(\"inf\")\n            evals += 1\n            self.evals = evals\n            return f\n\n        # initialization: Latin-hypercube-like seeding\n        init_n = max(1, min(self.init_pop, self.budget))\n        X_list = []\n        if init_n == 1:\n            X_list.append((lb + ub) / 2.0)\n        else:\n            # strata per dimension\n            strata = [self.rng.permutation(init_n) for _ in range(dim)]\n            for i in range(init_n):\n                xi = np.empty(dim, dtype=float)\n                for d in range(dim):\n                    frac = (strata[d][i] + self.rng.rand()) / float(init_n)\n                    xi[d] = lb[d] + frac * (ub[d] - lb[d])\n                X_list.append(xi)\n            # ensure at least one uniform random\n            if len(X_list) < init_n:\n                X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population\n        X = []\n        F = []\n        for x in X_list:\n            if evals >= self.budget:\n                break\n            f = _safe_eval(x)\n            X.append(np.asarray(x, dtype=float))\n            F.append(float(f))\n\n        if len(X) == 0:\n            # budget exhausted immediately\n            center = (lb + ub) / 2.0\n            return float(\"inf\"), center\n\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # track best\n        best_idx = int(np.argmin(F))\n        x_best = X[best_idx].copy()\n        f_best = float(F[best_idx])\n\n        # short-term adaptation variables\n        total_iters = 0\n        stagnation_counter = 0\n        last_improve_iter = 0\n\n        # main loop\n        while evals < self.budget:\n            total_iters += 1\n            N = X.shape[0]\n\n            # compute elites (top 20% but at least 2)\n            n_elite = max(2, int(np.ceil(0.2 * N)))\n            order_now = np.argsort(F)\n            elites_idx = order_now[:n_elite]\n            elites_X = X[elites_idx]\n            emean = np.nanmean(elites_X, axis=0)\n\n            # per-dim adaptive scale: combine global and archive spread\n            # robust spread: IQR -> approx 1.349 * std\n            try:\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n                iqr = (q75 - q25) / 1.349 + 1e-12\n            except Exception:\n                iqr = np.maximum(np.std(X, axis=0), 1e-12)\n            per_dim_scale = self.gscale * np.maximum(iqr, 0.02 * span)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_mask = np.isfinite(F)\n            scores = np.full_like(F, -1e6)\n            scores[finite_mask] = -F[finite_mask]\n            # subtract max for stability\n            maxs = np.max(scores)\n            exp_scores = np.exp((scores - maxs) / (np.std(scores[finite_mask]) + 1e-12))\n            probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n            base_idx = self.rng.choice(N, p=probs)\n            base_x = X[base_idx].copy()\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n            cand = None\n\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < 0.35:\n                pull = 0.15 * (emean - base_x) * (self.rng.rand() * 2.0)\n                cand = base_x + self.rng.randn(dim) * per_dim_scale + pull\n\n            # PCA-guided elite moves\n            elif r < 0.60:\n                # compute PCA from elites (centered)\n                Xc = elites_X - emean\n                try:\n                    U, S, VT = np.linalg.svd(Xc, full_matrices=False)\n                    # emphasize top components\n                    svals = S\n                    # component weights normalized\n                    comp_weights = (svals / (np.sum(svals) + 1e-12))\n                    # sample coefficients emphasizing top comps\n                    coeffs = self.rng.randn(len(svals)) * (0.5 + 2.0 * comp_weights)\n                    step = coeffs @ VT\n                    cand = emean + step * (self.gscale * 0.7)\n                    # small isotropic jitter\n                    cand += 0.3 * self.rng.randn(dim) * per_dim_scale\n                except Exception:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # Differential-like donor\n            elif r < 0.80:\n                # pick two distincts not equal to base_idx\n                idxs = list(range(N))\n                if len(idxs) >= 3:\n                    idxs.remove(base_idx)\n                    a, b = self.rng.choice(idxs, size=2, replace=False)\n                    Fscale = 0.6 + 0.6 * self.rng.rand()\n                    donor = base_x + Fscale * (X[a] - X[b])\n                    # crossover mixing\n                    mask = self.rng.rand(dim) < (0.2 + 0.6 * self.rng.rand())\n                    cand = base_x.copy()\n                    cand[mask] = donor[mask]\n                    cand += 0.1 * self.rng.randn(dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < 0.95:\n                # heavy-tailed jumps scaled per-dim\n                c = self.rng.standard_cauchy(size=dim)\n                # temper very large outliers\n                c = np.tanh(c / 4.0)\n                cand = base_x + 0.6 * c * per_dim_scale\n                # small gaussian temper\n                cand += 0.03 * self.rng.randn(dim) * per_dim_scale\n\n            # ARCHIVE_MIX\n            else:\n                donor_idx = self.rng.randint(0, N)\n                donor = X[donor_idx]\n                # mix a random subset of coordinates from donor\n                mask = self.rng.rand(dim) < (0.2 + 0.6 * self.rng.rand())\n                cand = base_x.copy()\n                cand[mask] = donor[mask]\n                cand += 0.2 * self.rng.randn(dim) * per_dim_scale\n                # occasionally inject uniform\n                if self.rng.rand() < 0.02:\n                    cand = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite entries and reflect to bounds\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.rand(np.sum(bad)) * (ub[bad] - lb[bad])\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_cand = _safe_eval(cand)\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, np.array([f_cand], dtype=float)])\n\n            # update best trackers\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                last_improve_iter = total_iters\n                stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n                stagnation_counter += 1\n\n            # keep success_history window small\n            if len(self.success_history) > self.short_window:\n                self.success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > max(self.archive_max * 2, 2 * self.dim):\n                # keep best and some random survivors\n                keep_best = int(max(5, 0.6 * self.archive_max))\n                order_now = np.argsort(F)\n                keep_idx = list(order_now[:keep_best])\n                # ensure some random survivors from rest\n                rest = list(order_now[keep_best:])\n                n_rand_keep = max(0, int(0.4 * self.archive_max) - len(keep_idx))\n                if len(rest) > 0 and n_rand_keep > 0:\n                    rand_keep = list(self.rng.choice(rest, size=min(len(rest), n_rand_keep), replace=False))\n                    keep_idx.extend(rand_keep)\n                keep_idx = np.unique(keep_idx).tolist()\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if (total_iters % 10) == 0:\n                if len(self.success_history) >= 4:\n                    succ_rate = np.mean(self.success_history)\n                    if succ_rate > 0.35:\n                        # many successes -> tighten search (exploit)\n                        self.gscale = max(1e-12, self.gscale * 0.85)\n                    elif succ_rate < 0.05:\n                        # few successes -> enlarge to explore\n                        self.gscale = min(self.gscale * 1.25, self.gscale_max)\n                    else:\n                        # slight jitter\n                        self.gscale *= 1.0 + (self.rng.rand() - 0.5) * 0.06\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter > max(20, 8 + dim):\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = min(5 + dim // 2, max(1, int(0.1 * self.archive_max)))\n                injects = []\n                for k in range(n_inject):\n                    if self.rng.rand() < 0.7:\n                        # perturb best / elite\n                        anchor = x_best if self.rng.rand() < 0.9 else emean\n                        newx = anchor + self.rng.randn(dim) * (1.5 * per_dim_scale)\n                    else:\n                        newx = self._uniform_array(lb, ub)\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    injects.append(newx)\n                # evaluate them (replace worst if better)\n                for newx in injects:\n                    if evals >= self.budget:\n                        break\n                    f_new = _safe_eval(newx)\n                    # put into archive: replace worst if archive full-ish\n                    if X.shape[0] >= self.archive_max:\n                        order_now = np.argsort(F)\n                        w = order_now[-1]  # worst index\n                        if f_new < F[w]:\n                            X[w] = newx\n                            F[w] = f_new\n                            if f_new < f_best:\n                                f_best = float(f_new)\n                                x_best = newx.copy()\n                                last_improve_iter = total_iters\n                                stagnation_counter = 0\n                    else:\n                        X = np.vstack([X, newx.reshape(1, -1)])\n                        F = np.concatenate([F, np.array([f_new], dtype=float)])\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                stagnation_counter = 0\n\n            # small safety stop if budget exhausted\n            if evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        center = (lb + ub) / 2.0\n        if not np.isfinite(f_best):\n            return float(np.inf), center\n        return float(f_best), x_best", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d2bad041-ea7f-457d-a25d-d2e597758e6a", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid, archive-driven continuous optimizer mixing anisotropic Gaussians, PCA-guided elite moves, DE-like differential donors, heavy-tailed Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 lb=None, ub=None,\n                 archive_max=None,\n                 init_pop=10,\n                 success_window=60):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # bounds (if user provides; otherwise use [-5,5] per problem statement)\n        if lb is None:\n            self.lb = np.full(self.dim, -5.0, dtype=float)\n        else:\n            self.lb = np.asarray(lb, dtype=float)\n            if self.lb.size == 1:\n                self.lb = np.full(self.dim, float(self.lb), dtype=float)\n        if ub is None:\n            self.ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            self.ub = np.asarray(ub, dtype=float)\n            if self.ub.size == 1:\n                self.ub = np.full(self.dim, float(self.ub), dtype=float)\n\n        # basic parameters\n        self.init_pop = max(2, int(init_pop))\n        self.archive_max = archive_max if archive_max is not None else max(200, 20 * self.dim)\n        self.success_window = int(success_window)\n\n        # algorithmic state\n        self.evals = 0\n        self.gscale = 0.2  # global relative scale (fraction of span)\n        self.gscale_max = 1.0\n        self.success_history = []\n        self.rng = np.random.RandomState(seed)\n\n    # utilities\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.broadcast_to(lb, (self.dim,))\n        if ub.size == 1:\n            ub = np.broadcast_to(ub, (self.dim,))\n        if n is None:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.rand(n, self.dim) * (ub - lb) + lb\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflect into [lb, ub], repeating up to max_reflect times\n        x = np.asarray(x, dtype=float).copy()\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.broadcast_to(lb, (self.dim,))\n        if ub.size == 1:\n            ub = np.broadcast_to(ub, (self.dim,))\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # clamp as final safeguard\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _safe_eval(self, func, x):\n        # Evaluates func once if budget remains, otherwise returns inf\n        if self.evals >= self.budget:\n            return float(\"inf\")\n        try:\n            fx = func(np.asarray(x, dtype=float))\n        except Exception:\n            fx = float(\"inf\")\n        # treat non-finite as inf\n        if fx is None or not np.isfinite(fx):\n            fx = float(\"inf\")\n        self.evals += 1\n        return float(fx)\n\n    def __call__(self, func):\n        # try to extract bounds from func if present (common conventions), else use provided\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    self.lb = np.asarray(b.lb, dtype=float)\n                    self.ub = np.asarray(b.ub, dtype=float)\n            if hasattr(func, \"lower_bounds\"):\n                self.lb = np.asarray(func.lower_bounds, dtype=float)\n            if hasattr(func, \"upper_bounds\"):\n                self.ub = np.asarray(func.upper_bounds, dtype=float)\n        except Exception:\n            pass\n\n        # ensure shapes\n        if self.lb.size == 1:\n            self.lb = np.broadcast_to(self.lb.item(), (self.dim,))\n        if self.ub.size == 1:\n            self.ub = np.broadcast_to(self.ub.item(), (self.dim,))\n\n        # center\n        center = 0.5 * (self.lb + self.ub)\n\n        # archive containers\n        X = []  # list of vectors\n        F = []  # list of floats\n\n        # initial seeding: small Latin-hypercube-ish + center + randoms\n        # LHS-like: stratify each coordinate\n        n_init = min(self.init_pop, max(2, self.budget // 20))\n        # produce n_init unique samples\n        if n_init >= 2:\n            seg = np.linspace(0.0, 1.0, n_init + 1)\n            perms = [self.rng.permutation(n_init) for _ in range(self.dim)]\n            for i in range(n_init):\n                u = np.empty(self.dim, dtype=float)\n                for d in range(self.dim):\n                    # pick the i-th stratum in a permuted order for each dim\n                    j = perms[d][i]\n                    lo = seg[j]\n                    hi = seg[j + 1]\n                    u[d] = self.rng.rand() * (hi - lo) + lo\n                xi = self.lb + u * (self.ub - self.lb)\n                X.append(xi)\n        # ensure center and a pure uniform\n        X.append(center.copy())\n        X.append(self._uniform_array(self.lb, self.ub))\n\n        # Evaluate initial population (respect budget)\n        X_evaluated = []\n        F_evaluated = []\n        for x in X:\n            if self.evals >= self.budget:\n                break\n            f = self._safe_eval(func, x)\n            X_evaluated.append(np.asarray(x, dtype=float))\n            F_evaluated.append(float(f))\n        X = X_evaluated\n        F = F_evaluated\n\n        # If no evaluations possible, return center with inf\n        if len(X) == 0:\n            return float(\"inf\"), center\n\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # track best\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = int(np.nanargmin(F))\n            f_best = float(F[best_idx])\n            x_best = X[best_idx].copy()\n        else:\n            f_best = float(\"inf\")\n            x_best = center.copy()\n\n        iter_since_improve = 0\n\n        # immediate spans\n        span = np.maximum(1e-12, self.ub - self.lb)\n        self.gscale_max = max(self.gscale_max, 10.0 * np.median(span))\n\n        # main loop: produce candidates until budget exhausted\n        while self.evals < self.budget:\n            Ncurr = X.shape[0]\n            # compute ranking and elites (top 20%)\n            finite_vals = np.where(np.isfinite(F), F, np.inf)\n            order = np.argsort(finite_vals)\n            n_elite = max(2, int(np.ceil(0.2 * Ncurr)))\n            elites_idx = order[:n_elite]\n            elites_X = X[elites_idx]\n\n            # robust per-dimension spread: IQR based\n            try:\n                q75 = np.nanpercentile(X, 75, axis=0)\n                q25 = np.nanpercentile(X, 25, axis=0)\n                iqr = np.maximum(1e-12, q75 - q25)\n                per_dim_scale = 0.75 * iqr  # a relative measure\n            except Exception:\n                per_dim_scale = 0.5 * span\n\n            # combine with global scale\n            per_dim_scale = np.maximum(1e-12, (self.gscale * span) * (1.0 + per_dim_scale / (span + 1e-12)))\n\n            # choose base index biased to better ranks (softmax on negative F)\n            negF = -finite_vals.copy()\n            # set inf to a large negative so they get very low weight\n            negF[~np.isfinite(negF)] = np.nan\n            shift = np.nanmax(negF) if np.any(np.isfinite(negF)) else 0.0\n            safe = np.exp(np.nan_to_num(negF - shift))\n            if np.sum(safe) <= 0 or not np.isfinite(np.sum(safe)):\n                probs = np.ones(Ncurr) / Ncurr\n            else:\n                probs = safe / np.nansum(safe)\n                probs = np.nan_to_num(probs, nan=1.0 / Ncurr)\n            base_idx = self.rng.choice(Ncurr, p=probs)\n            base_x = X[base_idx].copy()\n\n            # decide move type\n            r = self.rng.rand()\n            cand = None\n            # LOCAL 35%: anisotropic Gaussian around base with mild pull to elite mean\n            if r < 0.35:\n                emean = np.nanmean(elites_X, axis=0)\n                pull = 0.15 * (emean - base_x) * self.rng.rand()\n                noise = self.rng.randn(self.dim) * per_dim_scale * (0.6 + 0.8 * self.rng.rand())\n                cand = base_x + pull + noise\n\n            # PCA-guided elite moves 25%\n            elif r < 0.60:\n                # choose a random elite as base\n                if elites_X.shape[0] >= 2:\n                    pick = elites_X[self.rng.randint(elites_X.shape[0])]\n                    Xc = elites_X - np.mean(elites_X, axis=0)\n                    try:\n                        U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        # emphasize top components\n                        coef = self.rng.randn(len(svals)) * (0.5 + 1.5 * (svals / (svals[0] + 1e-12)))\n                        step = (Vt.T @ coef)\n                        cand = pick + step * (0.5 + self.rng.rand()) * (self.gscale * span)\n                    except Exception:\n                        # fallback to gaussian around elite mean\n                        cand = np.nanmean(elites_X, axis=0) + self.rng.randn(self.dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale\n\n            # DE-like differential donor 20%\n            elif r < 0.80:\n                if Ncurr >= 3:\n                    ids = list(range(Ncurr))\n                    ids.remove(base_idx)\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    donor = X[a] + 0.8 * (X[b] - X[base_idx])\n                    # small gaussian jitter\n                    cand = donor + 0.1 * self.rng.randn(self.dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale\n\n            # CAUCHY heavy-tailed jumps 12%\n            elif r < 0.92:\n                # standard cauchy, tempered and scaled per-dim\n                jump = np.random.standard_cauchy(self.dim)\n                # temper\n                jump = np.tanh(0.7 * jump)\n                cand = base_x + jump * (0.8 * per_dim_scale) + 0.03 * self.rng.randn(self.dim) * per_dim_scale\n\n            # ARCHIVE_MIX 8%\n            else:\n                cand = base_x.copy()\n                if Ncurr >= 2:\n                    donor_idx = self.rng.randint(Ncurr)\n                    donor = X[donor_idx]\n                    # mix random subset of coordinates\n                    mask = self.rng.rand(self.dim) < (0.2 + 0.6 * self.rng.rand())\n                    cand[mask] = donor[mask]\n                    # small gaussian perturb\n                    cand += 0.05 * self.rng.randn(self.dim) * per_dim_scale\n                    # occasional uniform injection\n                    if self.rng.rand() < 0.08:\n                        i = self.rng.randint(self.dim)\n                        cand[i] = self.rng.rand() * (self.ub[i] - self.lb[i]) + self.lb[i]\n\n            # safety: ensure cand exists\n            if cand is None:\n                cand = self._uniform_array(self.lb, self.ub)\n\n            # fix non-finite entries and reflect to bounds\n            cand = np.asarray(cand, dtype=float)\n            nanmask = ~np.isfinite(cand)\n            if np.any(nanmask):\n                cand[nanmask] = self._uniform_array(self.lb[nanmask], self.ub[nanmask], n=1).ravel()\n            cand = self._reflect_bounds(cand, self.lb, self.ub)\n\n            # evaluate candidate\n            f_cand = self._safe_eval(func, cand)\n\n            # append to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, np.array([f_cand], dtype=float)])\n\n            # update best trackers\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                iter_since_improve = 0\n                self.success_history.append(1)\n            else:\n                iter_since_improve += 1\n                # success also if candidate within top 25% of current archive\n                threshold_idx = max(0, int(np.ceil(0.25 * X.shape[0])) - 1)\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                rank_pos = int(np.where(order_now == (len(F) - 1))[0][0]) if (len(order_now) > 0 and (len(F) - 1) in order_now) else X.shape[0]\n                if rank_pos <= threshold_idx:\n                    self.success_history.append(1)\n                else:\n                    self.success_history.append(0)\n\n            # keep success_history window small\n            if len(self.success_history) > self.success_window:\n                self.success_history.pop(0)\n\n            # prune archive if too large: keep best ones + some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                order_now = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_best = int(np.ceil(0.6 * self.archive_max))\n                keep_idx = list(order_now[:keep_best])\n                # some random survivors\n                remaining = list(order_now[keep_best:])\n                n_random_keep = max(0, self.archive_max - len(keep_idx))\n                if len(remaining) > 0 and n_random_keep > 0:\n                    random_keep = list(self.rng.choice(remaining, size=min(n_random_keep, len(remaining)), replace=False))\n                else:\n                    random_keep = []\n                keep_idx.extend(random_keep)\n                # ensure at least 2 kept\n                if len(keep_idx) < 2:\n                    keep_idx = list(order_now[:2])\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(self.success_history) >= 8:\n                succ_rate = float(np.mean(self.success_history))\n                if succ_rate > 0.35:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(1e-12, self.gscale * 0.85)\n                elif succ_rate < 0.07:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.32 * self.rng.rand()))\n                else:\n                    # slight random jitter\n                    self.gscale = min(self.gscale_max, max(1e-12, self.gscale * (0.95 + 0.1 * self.rng.rand())))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(200, 10 * self.dim):\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = min(8, max(2, self.dim))\n                for k in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if (self.rng.rand() < 0.6) and np.isfinite(f_best):\n                        # perturb best / elite slightly but with occasionally larger scale\n                        scale_factor = (1.0 + 4.0 * self.rng.rand()) if (self.rng.rand() < 0.3) else (1.0 + 0.6 * self.rng.rand())\n                        x_new = x_best + self.rng.randn(self.dim) * (self.gscale * scale_factor * span)\n                        # occasional heavier jump\n                        if self.rng.rand() < 0.08:\n                            x_new += np.random.standard_cauchy(self.dim) * 0.2 * span\n                    else:\n                        # random candidate biased to elites\n                        elite_mean = np.nanmean(elites_X, axis=0)\n                        x_new = elite_mean + self.rng.randn(self.dim) * (self.gscale * 2.0 * span)\n                    x_new = self._reflect_bounds(x_new, self.lb, self.ub)\n                    f_new = self._safe_eval(func, x_new)\n                    # add to archive (replace worst if full)\n                    X = np.vstack([X, x_new.reshape(1, -1)])\n                    F = np.concatenate([F, np.array([f_new], dtype=float)])\n                    if np.isfinite(f_new) and f_new < f_best:\n                        f_best = float(f_new)\n                        x_best = x_new.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.7 * self.rng.rand()))\n                iter_since_improve = 0\n\n            # safety stop if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if np.isfinite(f_best):\n            return float(f_best), np.asarray(x_best, dtype=float)\n        else:\n            return float(np.inf), np.asarray(center, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6843e33f-70e7-4b06-8525-7969d21f11ef", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven sampler mixing anisotropic Gaussians, PCA-guided elite moves, DE-like donors, heavy-tailed Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=None,\n                 init_samples=None,\n                 gscale_init=0.3):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = archive_max if archive_max is not None else max(50, 20 * self.dim)\n        self.init_samples = init_samples if init_samples is not None else min(max(8, 3 * self.dim), self.budget)\n        # global search scale (relative to span)\n        self.gscale = float(gscale_init)\n        self.gscale_max = 10.0\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try to extract bounds from common attributes; otherwise default to [-5,5]^dim\n        lb = None\n        ub = None\n        # common attribute: bounds with .lb/.ub or .bounds.lb/.ub\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # other common names\n        if lb is None:\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # fallback to default scalar bounds if needed\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            # broadcast scalars to dimension\n            if lb.ndim == 0:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.ndim == 0:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n            # ensure correct shape\n            if lb.size != self.dim:\n                lb = np.resize(lb, (self.dim,))\n            if ub.size != self.dim:\n                ub = np.resize(ub, (self.dim,))\n            # ensure finite and lb < ub; else replace with defaults\n            for i in range(self.dim):\n                if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                    lb[i] = -5.0\n                    ub[i] = 5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.asarray(x, dtype=float)\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.ndim == 1:\n            dim = lb.shape[0]\n            if n is None:\n                return lb + self.rng.rand(dim) * (ub - lb)\n            else:\n                return lb.reshape((1, -1)) + self.rng.rand(n, lb.size) * (ub - lb).reshape((1, -1))\n        else:\n            # fallback\n            if n is None:\n                return lb + self.rng.rand(*lb.shape) * (ub - lb)\n            else:\n                shape = (n,) + lb.shape\n                return lb + self.rng.rand(*shape) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        # fallback if any span non-positive\n        span[span <= 0] = 10.0\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return float(\"inf\")\n            try:\n                f = func(x)\n            except Exception:\n                f = float(\"inf\")\n            self.evals += 1\n            # keep track of best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # initialization: Latin-hypercube-like seeding\n        self.evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        center = (lb + ub) / 2.0\n        X_list = []\n\n        # ensure center\n        X_list.append(center.copy())\n        n0 = max(1, min(self.init_samples - 1, self.budget))\n        # simple LHS-like seeding: stratify each dimension\n        if n0 > 0:\n            # create n0 samples using independent stratification per-dim\n            cut = np.linspace(0, 1, n0 + 1)\n            for i in range(n0):\n                r = self.rng.rand(self.dim)\n                vals = (cut[i] + r * (cut[i + 1] - cut[i]))\n                samp = lb + vals * span\n                X_list.append(samp)\n\n            # ensure at least one uniform random\n            X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        X = []\n        F = []\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = _safe_eval(x)\n            X.append(x.copy())\n            F.append(f)\n\n        if len(X) == 0:\n            # cannot evaluate at all: return center and inf\n            return float(\"inf\"), center\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # track best\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = np.nanargmin(np.where(finite_mask, F, np.inf))\n            f_best = float(F[best_idx])\n        else:\n            f_best = float(np.inf)\n\n        # short-term adaptation variables\n        success_history = []\n        success_window = 20\n        stagnation_counter = 0\n        stagnation_threshold = max(20, 5 * self.dim)\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elites_k = max(2, int(0.2 * N))\n\n            # per-dim adaptive scale: combine global and archive spread\n            # robust spread: IQR -> approx 1.349 * std\n            q25 = np.nanpercentile(X, 25, axis=0)\n            q75 = np.nanpercentile(X, 75, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            per_dim_std_approx = iqr / 1.349\n            per_dim_scale = np.maximum(per_dim_std_approx, 1e-12) * self.gscale\n            # ensure relative to span\n            per_dim_scale = np.minimum(per_dim_scale, span * 0.5)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finF = np.where(np.isfinite(F), F, np.max(np.where(np.isfinite(F), F, np.nan)) + 1e9)\n            # make sure lower is better -> weights ~ exp(-rank/temperature)\n            vals = finF.copy()\n            # convert to scores (lower is better)\n            maxv = np.nanmax(vals)\n            expw = np.exp(-(vals - np.nanmin(vals)) / (1e-9 + (maxv - np.nanmin(vals))))\n            if np.any(expw > 0) and np.isfinite(np.sum(expw)):\n                probs = expw / np.sum(expw)\n            else:\n                probs = np.ones(N) / N\n            base_idx = self.rng.choice(N, p=probs)\n            base_x = X[base_idx].copy()\n            center_choice = base_x.copy()\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n            cand = None\n\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < 0.35:\n                # slight pull to archive mean to encourage exploitation\n                archive_mean = np.nanmean(X, axis=0)\n                pull = (archive_mean - base_x) * (0.1 * self.rng.rand())\n                noise = self.rng.randn(self.dim) * per_dim_scale * (0.8 + 0.4 * self.rng.rand())\n                cand = base_x + pull + noise\n\n            # PCA-guided elite moves\n            elif r < 0.60:\n                elites_idx = order_all[:elites_k]\n                Xc = X[elites_idx] - np.mean(X[elites_idx], axis=0)\n                try:\n                    if Xc.shape[0] >= 2:\n                        # PCA via SVD\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # rows are components\n                        # sample coefficients emphasizing top components\n                        # draw coefficients with heavier emphasis on top comps\n                        comp_weights = np.exp(-np.arange(comps.shape[0]) / max(1.0, 0.7 * comps.shape[0]))\n                        comp_weights = comp_weights / (np.sum(comp_weights) + 1e-12)\n                        # number of components to use\n                        k_use = max(1, min(self.dim, int(1 + self.rng.poisson(1.5))))\n                        chosen = self.rng.choice(comps.shape[0], size=k_use, replace=False, p=comp_weights)\n                        coeffs = np.zeros(self.dim)\n                        for j, cidx in enumerate(chosen):\n                            coef_scale = (1.5 / (1 + j)) * self.gscale\n                            coeffs += coeff_scale * (self.rng.randn() * comps[cidx])\n                        # center on an elite sampled proportionally to quality\n                        elite_choice = elites_idx[self.rng.randint(0, len(elites_idx))]\n                        cand = X[elite_choice] + coeffs * span * 0.5\n                    else:\n                        # fallback to local\n                        noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand())\n                        cand = base_x + noise\n                except Exception:\n                    noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand())\n                    cand = base_x + noise\n\n            # Differential-like donor\n            elif r < 0.80:\n                # pick two distincts not equal to base_idx\n                if N >= 3:\n                    idxs = list(range(N))\n                    idxs.remove(base_idx)\n                    a, b = self.rng.choice(idxs, size=2, replace=False)\n                    Fscale = 0.6 + 0.6 * self.rng.rand()\n                    cand = base_x + Fscale * (X[a] - X[b]) + 0.05 * self.rng.randn(self.dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < 0.95:\n                # temper very large outliers and scale to per-dim\n                cauch = np.random.standard_cauchy(self.dim)\n                cauch = np.tanh(cauch / 3.0)  # temper extreme tails\n                cand = center_choice + 0.6 * cauch * per_dim_scale * (1.0 + self.rng.rand())\n\n            # ARCHIVE_MIX\n            else:\n                if N >= 2:\n                    donor = X[self.rng.randint(0, N)]\n                else:\n                    donor = self._uniform_array(lb, ub)\n                cand = base_x.copy()\n                mix_mask = self.rng.rand(self.dim) < (0.15 + 0.35 * self.rng.rand())\n                if np.any(mix_mask):\n                    cand[mix_mask] = donor[mix_mask]\n                # small gaussian perturbation\n                cand += 0.2 * self.rng.randn(self.dim) * per_dim_scale\n                # occasionally inject uniform\n                if self.rng.rand() < 0.05:\n                    replace_dim = self.rng.randint(0, self.dim)\n                    cand[replace_dim] = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite entries and reflect to bounds\n            cand = np.asarray(cand, dtype=float)\n            cand[~np.isfinite(cand)] = lb[~np.isfinite(cand)] + self.rng.rand(np.sum(~np.isfinite(cand))) * span[~np.isfinite(cand)]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if self.evals >= self.budget:\n                break\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.copy()])\n            F = np.concatenate([F, np.array([f_cand], dtype=float)])\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25%\n            if np.isfinite(f_cand):\n                rank = int(np.sum(np.isfinite(F) & (F < f_cand)))\n                if improved or (rank <= max(1, int(0.25 * X.shape[0]))):\n                    success_history.append(1)\n                else:\n                    success_history.append(0)\n            else:\n                success_history.append(0)\n            if len(success_history) > success_window:\n                success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_best = int(0.6 * self.archive_max)\n                keep_idx = list(order[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                # keep some random survivors\n                num_random = max(0, self.archive_max - keep_best)\n                if len(rest) > 0 and num_random > 0:\n                    rand_keep = list(self.rng.choice(rest, size=min(len(rest), num_random), replace=False))\n                else:\n                    rand_keep = []\n                keep_idx += rand_keep\n                if len(keep_idx) < self.archive_max:\n                    # pad by best remaining\n                    need = self.archive_max - len(keep_idx)\n                    extra = [i for i in order if i not in keep_idx][:need]\n                    keep_idx += extra\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) == success_window:\n                succ_rate = sum(success_history) / float(success_window)\n                if succ_rate > 0.35:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(1e-6, self.gscale * (0.7 + 0.2 * self.rng.rand()))\n                elif succ_rate < 0.1:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.5 * self.rng.rand()))\n                else:\n                    # small jitter\n                    self.gscale *= (0.95 + 0.1 * self.rng.rand())\n                # keep within bounds\n                self.gscale = float(np.clip(self.gscale, 1e-8, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter >= stagnation_threshold:\n                # create some injected candidates (prefer perturbations of best)\n                injects = []\n                num_inject = min(8 + self.dim, max(2, int(0.1 * self.archive_max)))\n                best_idx = int(np.nanargmin(np.where(np.isfinite(F), F, np.inf)))\n                best_x = X[best_idx]\n                for k in range(num_inject):\n                    if self.rng.rand() < 0.6:\n                        # perturb elite/best\n                        scale = (1.5 ** (self.rng.rand() - 0.5)) * self.gscale * (1.0 + 0.5 * self.rng.rand())\n                        inj = best_x + self.rng.randn(self.dim) * per_dim_scale * scale\n                    else:\n                        # random uniform\n                        inj = self._uniform_array(lb, ub)\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    injects.append(inj)\n                # evaluate them (replace worst)\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_new = _safe_eval(inj)\n                    # put into archive: replace worst if archive full-ish\n                    X = np.vstack([X, inj.copy()])\n                    F = np.concatenate([F, np.array([f_new], dtype=float)])\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                stagnation_counter = 0\n                success_history = []\n\n            # safety stop if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None or not np.isfinite(self.f_opt):\n            return float(self.f_opt), center\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 299, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: cand[replace_dim] = self._uniform_array(lb, ub)", "error": "In the code, line 299, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: cand[replace_dim] = self._uniform_array(lb, ub)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ff63c5bf-52b1-4d2d-8f23-aac0418c45f6", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — hybrid archive-driven sampler mixing anisotropic Gaussians, PCA-guided elite moves, DE-like donors, Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop_size=None,\n                 archive_size=None,\n                 init_scale=0.2):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimensionality\n        pop_size: initial population size (if None derived from dim/budget)\n        archive_size: maximum archive capacity\n        init_scale: initial global scale relative to span (0-1)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible defaults\n        if pop_size is None:\n            self.pop_size = max(8, min(40, 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        if archive_size is None:\n            self.archive_size = max(200, 10 * self.pop_size)\n        else:\n            self.archive_size = int(archive_size)\n        self.init_scale = float(init_scale)\n\n        # runtime state to be set in __call__\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try to extract common bound attributes; otherwise default to [-5,5]^dim\n        default_lb = -5.0\n        default_ub = 5.0\n        lb = None\n        ub = None\n        # common patterns\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n            except Exception:\n                lb = ub = None\n        if lb is None and hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n            try:\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                lb = ub = None\n        if lb is None:\n            # fallback to default scalars\n            lb = np.full(self.dim, default_lb, dtype=float)\n            ub = np.full(self.dim, default_ub, dtype=float)\n        else:\n            # broadcast to dimension if scalars\n            lb = np.asarray(lb, dtype=float).ravel()\n            ub = np.asarray(ub, dtype=float).ravel()\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb.item()))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub.item()))\n        # ensure shapes\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, default_lb, dtype=float)\n            ub = np.full(self.dim, default_ub, dtype=float)\n        # validity check\n        finite_mask = np.isfinite(lb) & np.isfinite(ub) & (ub > lb)\n        if not np.all(finite_mask):\n            lb[~finite_mask] = default_lb\n            ub[~finite_mask] = default_ub\n        return lb.copy(), ub.copy()\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = x.copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            if not np.any(low_mask):\n                break\n            x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n        if n is None:\n            return self.rng.uniform(lb, ub)\n        else:\n            arr = self.rng.uniform(0.0, 1.0, size=(n, self.dim)) * (ub - lb) + lb\n            return arr\n\n    # ----- main call -----\n    def __call__(self, func):\n        # extract bounds and span\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        if np.any(span <= 0) or not np.all(np.isfinite(span)):\n            # fallback\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n            span = ub - lb\n\n        # bookkeeping for budget\n        self.evals_done = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe evaluation wrapper that respects budget and tracks best\n        def _safe_eval(x):\n            # x: 1d array\n            if self.evals_done >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.evals_done += 1\n            # update best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # initialization: Latin-hypercube-like seeding\n        n_pop = min(self.pop_size, max(4, self.budget // 5))\n        n_init = max(3, min(n_pop, int(self.budget // 10)))\n        # strata per-dim\n        arr = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            # create stratified positions and permute\n            pts = (np.arange(n_init) + self.rng.rand(n_init)) / n_init\n            self.rng.shuffle(pts)\n            arr[:, d] = lb[d] + pts * span[d]\n        # ensure at least one uniform random and the center\n        extra = []\n        extra.append((lb + ub) / 2.0)\n        extra.append(self._uniform_array(lb, ub))\n        for e in extra:\n            if arr.shape[0] < n_pop:\n                arr = np.vstack([arr, e])\n        # fill up to n_pop with uniform if needed\n        while arr.shape[0] < n_pop:\n            arr = np.vstack([arr, self._uniform_array(lb, ub)])\n        # trim to n_pop\n        arr = arr[:n_pop]\n\n        # evaluate initial population\n        xs = []\n        fs = []\n        for i in range(arr.shape[0]):\n            if self.evals_done >= self.budget:\n                break\n            x = arr[i].copy()\n            x = self._reflect_bounds(x, lb, ub)\n            f = _safe_eval(x)\n            xs.append(x)\n            fs.append(f)\n        if len(xs) == 0:\n            # cannot evaluate at all\n            center = (lb + ub) / 2.0\n            return np.inf, center\n        X = np.array(xs)\n        F = np.array(fs)\n\n        # archive: store all evaluated solutions\n        archive_X = X.copy()\n        archive_F = F.copy()\n\n        # short-term adaptation variables\n        gscale = self.init_scale  # relative to span\n        gscale = float(np.clip(gscale, 1e-6, 1.0))\n        success_window = []\n        adapt_interval = max(10, min(50, self.dim * 2))\n        adapt_counter = 0\n        no_improve_counter = 0\n        stagnation_limit = max(100, 10 * self.dim)\n\n        # main loop: produce new candidates until budget exhausted\n        while self.evals_done < self.budget:\n            n_archive = archive_X.shape[0]\n            # ranking and elites\n            # lower is better\n            ranks = np.argsort(np.argsort(archive_F))  # 0 best\n            # elites: top 20% but at least 2\n            n_elite = max(2, int(np.ceil(0.20 * n_archive)))\n            elite_idx = np.argsort(archive_F)[:n_elite]\n            elites_X = archive_X[elite_idx]\n            elites_F = archive_F[elite_idx]\n\n            # per-dim adaptive scale: combine global and archive spread\n            # robust spread: IQR -> sigma ~ IQR / 1.349\n            if n_archive >= 4:\n                q75 = np.percentile(archive_X, 75, axis=0)\n                q25 = np.percentile(archive_X, 25, axis=0)\n                iqr = np.maximum(1e-12, q75 - q25)\n                robust_sigma = iqr / 1.349\n                # ensure relative to span\n                per_dim_sigma = np.maximum(robust_sigma, 1e-12 + (span * 1e-6))\n            else:\n                per_dim_sigma = 0.5 * span  # coarse initial\n            # combine with global scale\n            per_dim_sigma = per_dim_sigma * gscale\n\n            # choose base index biased to better ranks (softmax on negative rank)\n            temp = max(1.0, 0.15 * max(1, n_archive))\n            scores = np.exp(-ranks / temp)\n            probs = scores / np.sum(scores)\n            try:\n                base_idx = self.rng.choice(n_archive, p=probs)\n            except Exception:\n                base_idx = int(np.argmin(archive_F))\n            base_x = archive_X[base_idx]\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n            candidate = None\n            if r < 0.35:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                # compute archive mean\n                arch_mean = np.mean(archive_X, axis=0)\n                pull_strength = self.rng.uniform(0.0, 0.5) * gscale\n                noise = self.rng.randn(self.dim) * per_dim_sigma\n                candidate = base_x + pull_strength * (arch_mean - base_x) + noise\n\n            elif r < 0.60:\n                # PCA-guided elite moves\n                # pick an elite as center, weighted by quality\n                ef = elites_F.copy()\n                # convert to weights: better lower f => larger weight\n                ef_rel = (ef - np.min(ef)) + 1e-12\n                w = np.exp(- (ef_rel / (np.median(ef_rel) + 1e-12)))\n                w = w / np.sum(w)\n                chosen_elite = elites_X[self.rng.choice(len(elites_X), p=w)]\n                # PCA via SVD on elites\n                try:\n                    X_centered = elites_X - np.mean(elites_X, axis=0)\n                    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n                    # emphasize top components: sample coefficients with variance ~ S_k^alpha\n                    if S.size == 0:\n                        raise np.linalg.LinAlgError\n                    alpha = self.rng.uniform(0.8, 1.8)\n                    k = max(1, int(min(self.dim, 1 + np.searchsorted(np.cumsum(S) / np.sum(S), 0.9))))\n                    # sample along top components\n                    coeffs = self.rng.randn(k) * (S[:k] ** (alpha / 2.0)) * (gscale * 0.6)\n                    delta = coeffs @ Vt[:k, :]\n                    # small isotropic perturbation too\n                    iso = self.rng.randn(self.dim) * per_dim_sigma * 0.2\n                    candidate = chosen_elite + delta + iso\n                except Exception:\n                    # fallback to local\n                    candidate = base_x + self.rng.randn(self.dim) * per_dim_sigma * 0.8\n\n            elif r < 0.80:\n                # Differential-like donor\n                # pick two distinct not equal to base_idx\n                if n_archive >= 3:\n                    idxs = list(range(n_archive))\n                    idxs.remove(base_idx)\n                    a, b = self.rng.choice(idxs, size=2, replace=False)\n                    donor = archive_X[a]\n                    donor2 = archive_X[b]\n                    Fscale = self.rng.uniform(0.6, 1.0) * gscale\n                    candidate = base_x + Fscale * (donor - donor2) + self.rng.randn(self.dim) * per_dim_sigma * 0.1\n                else:\n                    candidate = base_x + self.rng.randn(self.dim) * per_dim_sigma\n\n            elif r < 0.95:\n                # CAUCHY jumps (heavy-tailed)\n                # temper very large outliers and scale to per-dim\n                cauch = self.rng.standard_cauchy(self.dim)\n                # scale down extreme values\n                cauch = np.tanh(cauch / 6.0)  # limit extremes\n                candidate = base_x + cauch * per_dim_sigma * self.rng.uniform(0.8, 2.5)\n\n            else:\n                # ARCHIVE_MIX\n                if n_archive >= 2:\n                    a, b = self.rng.choice(n_archive, size=2, replace=False)\n                    mix = self.rng.rand(self.dim) < 0.5\n                    candidate = np.where(mix, archive_X[a], archive_X[b])\n                    # small gaussian perturbation\n                    candidate = candidate + self.rng.randn(self.dim) * per_dim_sigma * 0.15\n                    # occasionally inject uniform\n                    if self.rng.rand() < 0.05:\n                        candidate = self._uniform_array(lb, ub)\n                else:\n                    candidate = self._uniform_array(lb, ub)\n\n            # safety: replace any non-finite entries and reflect to bounds\n            candidate = np.array(candidate, dtype=float)\n            bad_mask = ~np.isfinite(candidate)\n            if np.any(bad_mask):\n                candidate[bad_mask] = lb[bad_mask] + self.rng.rand(np.sum(bad_mask)) * span[bad_mask]\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_c = _safe_eval(candidate)\n            # Append candidate to archive\n            archive_X = np.vstack([archive_X, candidate])\n            archive_F = np.concatenate([archive_F, np.array([f_c])])\n\n            # update best trackers is done in _safe_eval already\n\n            # define success as improving global best or placed among top 25%\n            placed_top_q = False\n            if np.isfinite(f_c):\n                cutoff = np.percentile(archive_F, 25) if archive_F.size >= 4 else np.min(archive_F)\n                if f_c <= cutoff or (np.isfinite(self.f_opt) and (f_c < self.f_opt)):\n                    success = True\n                    placed_top_q = True\n                else:\n                    success = False\n            else:\n                success = False\n\n            success_window.append(1 if success else 0)\n            if len(success_window) > 200:\n                success_window.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms\n            if archive_X.shape[0] > self.archive_size:\n                keep_best = max(self.pop_size, int(self.archive_size * 0.6))\n                keep_random = int(self.archive_size * 0.15)\n                order = np.argsort(archive_F)\n                keep_idx = list(order[:keep_best])\n                remaining = list(order[keep_best:])\n                if remaining and keep_random > 0:\n                    rnd = self.rng.choice(remaining, size=min(keep_random, len(remaining)), replace=False)\n                    keep_idx.extend(list(rnd))\n                # fill by best remaining if needed\n                keep_idx = np.array(sorted(set(keep_idx)))[:self.archive_size]\n                archive_X = archive_X[keep_idx]\n                archive_F = archive_F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            adapt_counter += 1\n            if adapt_counter >= adapt_interval:\n                adapt_counter = 0\n                win = np.array(success_window[-adapt_interval:]) if len(success_window) >= adapt_interval else np.array(success_window)\n                succ_rate = (np.sum(win) / (len(win) + 1e-12)) if win.size > 0 else 0.0\n                if succ_rate > 0.25:\n                    # many successes -> tighten search (exploit)\n                    gscale *= 0.85\n                elif succ_rate < 0.05:\n                    # few successes -> enlarge to explore\n                    gscale *= 1.15\n                else:\n                    # small jitter\n                    gscale *= np.exp(self.rng.normal(0, 0.02))\n                # keep within bounds\n                gscale = float(np.clip(gscale, 1e-6, 1.5))\n\n            # stagnation detection & micro-restarts\n            if (self.evals_done >= 1) and (self.f_opt < np.inf):\n                # detect stagnation measured by no improvement\n                no_improve_counter += 1\n                # reset if recently improved\n                if placed_top_q:\n                    no_improve_counter = 0\n            else:\n                no_improve_counter += 1\n\n            if no_improve_counter > stagnation_limit and self.evals_done < self.budget:\n                # micro-restart: inject some perturbed elites and randoms\n                k_restart = min(8, max(2, int(self.dim // 2)))\n                injected = []\n                for _ in range(k_restart):\n                    if self.rng.rand() < 0.7 and self.x_opt is not None:\n                        # perturb the best\n                        pert = self.x_opt + self.rng.randn(self.dim) * per_dim_sigma * self.rng.uniform(0.8, 2.5)\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        injected.append(pert)\n                    else:\n                        injected.append(self._uniform_array(lb, ub))\n                # evaluate them (replace worst)\n                for cand in injected:\n                    if self.evals_done >= self.budget:\n                        break\n                    f_c = _safe_eval(cand)\n                    archive_X = np.vstack([archive_X, cand])\n                    archive_F = np.concatenate([archive_F, np.array([f_c])])\n                # prune aggressively\n                if archive_X.shape[0] > self.archive_size:\n                    order = np.argsort(archive_F)\n                    archive_X = archive_X[order[:self.archive_size]]\n                    archive_F = archive_F[order[:self.archive_size]]\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale = float(np.clip(gscale * 1.25, 1e-6, 2.0))\n                no_improve_counter = 0\n                success_window = []\n\n            # safety stop if budget exhausted (while condition handles it)\n            # continue loop\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None or not np.all(np.isfinite(self.x_opt)):\n            center = (lb + ub) / 2.0\n            return float(self.f_opt), center\n        return float(self.f_opt), np.array(self.x_opt)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "499e80f4-84eb-427c-b820-ed1c645be309", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — hybrid archive-driven sampler mixing anisotropic Gaussians, PCA-guided elite moves, DE-like donors, heavy-tailed Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=None, gscale_init=0.3, init_samples=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = archive_max if archive_max is not None else max(50, 20 * self.dim)\n        # number of initial samples (including center)\n        if init_samples is None:\n            self.init_samples = int(min(max(8, 3 * self.dim), max(1, self.budget)))\n        else:\n            self.init_samples = int(init_samples)\n        # global search scale (relative to robust per-dim spread)\n        self.gscale = float(gscale_init)\n        self.gscale_max = 10.0\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        \"\"\"Try several common ways to extract bounds; fallback to [-5,5]^dim\"\"\"\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # b could be tuple (lb, ub) or object with lb/ub\n                if isinstance(b, tuple) and len(b) == 2:\n                    lb, ub = b\n                else:\n                    if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                        lb, ub = b.lb, b.ub\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            try:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb, ub = func.lb, func.ub\n            except Exception:\n                lb = None\n                ub = None\n\n        # Default\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            # allow scalar bounds\n            if lb.ndim == 0:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.ndim == 0:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n            # reshape/broadcast if needed\n            if lb.size != self.dim:\n                lb = np.resize(lb, (self.dim,))\n            if ub.size != self.dim:\n                ub = np.resize(ub, (self.dim,))\n            # validate entries\n            for i in range(self.dim):\n                if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                    lb[i] = -5.0\n                    ub[i] = 5.0\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.asarray(x, dtype=float)\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            # if all inside break\n            if np.all((x >= lb) & (x <= ub)):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        dim = lb.shape[0]\n        if n is None:\n            return lb + self.rng.rand(dim) * (ub - lb)\n        else:\n            shape = (n, dim)\n            return lb + self.rng.rand(*shape) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        # fallback if any span non-positive\n        span[span <= 0] = 10.0\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return float(\"inf\")\n            try:\n                f = func(x)\n            except Exception:\n                f = float(\"inf\")\n            self.evals += 1\n            # keep track of best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # initialization: Latin-hypercube-like seeding\n        self.evals = 0\n        self.x_opt = None\n        self.f_opt = float(\"inf\")\n\n        center = (lb + ub) / 2.0\n\n        X_list = []\n        X_list.append(center.copy())\n        n0 = max(0, min(self.init_samples - 1, max(0, self.budget - 1)))\n        # simple LHS-like seeding: stratify each dimension\n        if n0 > 0:\n            cut = np.linspace(0, 1, n0 + 1)\n            for i in range(n0):\n                r = self.rng.rand(self.dim)\n                vals = (cut[i] + r * (cut[i + 1] - cut[i]))\n                samp = lb + vals * span\n                X_list.append(samp)\n            # ensure at least one uniform random (if budget allows)\n            X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        X = []\n        F = []\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = _safe_eval(x)\n            X.append(x.copy())\n            F.append(f)\n\n        if len(X) == 0:\n            # cannot evaluate at all: return center and inf\n            return float(\"inf\"), center\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # track best\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = int(np.nanargmin(np.where(finite_mask, F, np.inf)))\n            f_best = float(F[best_idx])\n        else:\n            f_best = float(np.inf)\n\n        # short-term adaptation variables\n        success_history = []\n        success_window = 20\n        stagnation_counter = 0\n        stagnation_threshold = max(20, 5 * self.dim)\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elites_k = max(2, int(0.2 * max(2, N)))\n\n            # per-dim adaptive scale: combine global and archive spread\n            # robust spread: IQR -> approx 1.349 * std\n            try:\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n            except Exception:\n                # fallback if something weird happened\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            per_dim_std_approx = iqr / 1.349\n            per_dim_scale = np.maximum(per_dim_std_approx, 1e-12) * self.gscale\n            # ensure relative to span\n            per_dim_scale = np.minimum(per_dim_scale, span * 0.5)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            fin_mask = np.isfinite(F)\n            finF = F.copy()\n            if not np.any(fin_mask):\n                finF = np.zeros_like(F)\n            else:\n                # replace non-finite with a large worse value\n                worst = np.nanmax(np.where(fin_mask, F, np.nan))\n                if not np.isfinite(worst):\n                    worst = 1e6\n                finF = np.where(fin_mask, F, worst + 1e3)\n            # make sure lower is better -> weights ~ exp(-scaled value)\n            vals = finF.copy()\n            vmin = np.nanmin(vals)\n            vmax = np.nanmax(vals)\n            denom = (vmax - vmin) if (vmax - vmin) > 0 else 1.0\n            expw = np.exp(-(vals - vmin) / (denom + 1e-12))\n            if np.any(np.isfinite(expw)) and np.sum(expw) > 0:\n                probs = expw / np.sum(expw)\n            else:\n                probs = np.ones(N) / N\n            base_idx = int(self.rng.choice(N, p=probs))\n            base_x = X[base_idx].copy()\n            center_choice = base_x.copy()\n\n            # decide move type by probabilities:\n            # LOCAL 35%, PCA 25%, DE 20%, CAUCHY 15%, ARCHIVE_MIX 5%\n            r = self.rng.rand()\n            cand = None\n\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < 0.35:\n                # slight pull to archive mean to encourage exploitation\n                archive_mean = np.nanmean(X, axis=0)\n                pull = (archive_mean - base_x) * (0.1 * self.rng.rand())\n                noise = self.rng.randn(self.dim) * per_dim_scale * (0.8 + 0.4 * self.rng.rand())\n                cand = base_x + pull + noise\n\n            # PCA-guided elite moves\n            elif r < 0.60:\n                elites_idx = order_all[:elites_k]\n                Xc = X[elites_idx] - np.mean(X[elites_idx], axis=0)\n                try:\n                    if Xc.shape[0] >= 2:\n                        # PCA via SVD\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # rows are components\n                        # weights emphasize top components\n                        comp_weights = np.exp(-np.arange(comps.shape[0]) / max(1.0, 0.7 * comps.shape[0]))\n                        comp_weights = comp_weights / (np.sum(comp_weights) + 1e-12)\n                        # choose how many components to mix\n                        k_use = max(1, int(0.4 * comps.shape[0]))\n                        chosen = self.rng.choice(comps.shape[0], size=k_use, replace=False, p=comp_weights)\n                        coeffs = np.zeros(self.dim)\n                        coeff_scale = np.mean(per_dim_scale) if np.all(np.isfinite(per_dim_scale)) else np.mean(span) * 0.1\n                        for j, cidx in enumerate(chosen):\n                            coeffs += (self.rng.randn() * comps[cidx])\n                        # scale coefficients moderately\n                        coeffs = coeffs * (0.6 * coeff_scale)\n                        # center on an elite sampled proportionally to quality\n                        elite_choice = int(elites_idx[self.rng.randint(0, len(elites_idx))])\n                        cand = X[elite_choice] + coeffs\n                    else:\n                        noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand())\n                        cand = base_x + noise\n                except Exception:\n                    noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand())\n                    cand = base_x + noise\n\n            # Differential-like donor\n            elif r < 0.80:\n                if N >= 3:\n                    idxs = list(range(N))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    # choose two distincts\n                    a, b = self.rng.choice(idxs, size=2, replace=False)\n                    Fscale = 0.6 + 0.6 * self.rng.rand()\n                    cand = base_x + Fscale * (X[a] - X[b])\n                    # small gaussian jitter\n                    cand += 0.3 * self.rng.randn(self.dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < 0.95:\n                # temper very large outliers and scale to per-dim\n                cauch = self.rng.standard_cauchy(self.dim)\n                # clip extremely large cauchy samples\n                cauch = np.clip(cauch, -1e3, 1e3)\n                scale = 0.6 * (1.0 + self.rng.rand())\n                cand = center_choice + scale * cauch * per_dim_scale\n\n            # ARCHIVE_MIX\n            else:\n                if N >= 2:\n                    donor = X[int(self.rng.randint(0, N))]\n                else:\n                    donor = self._uniform_array(lb, ub)\n                cand = base_x.copy()\n                mix_prob = 0.25\n                mix_mask = self.rng.rand(self.dim) < mix_prob\n                if np.any(mix_mask):\n                    cand[mix_mask] = donor[mix_mask]\n                # small gaussian perturbation\n                cand += 0.2 * self.rng.randn(self.dim) * per_dim_scale\n                # occasionally inject uniform coordinate\n                if self.rng.rand() < 0.05:\n                    replace_dim = int(self.rng.randint(0, self.dim))\n                    cand[replace_dim] = self._uniform_array(lb, ub)[replace_dim]\n\n            # safety: replace any non-finite entries and reflect to bounds\n            cand = np.asarray(cand, dtype=float)\n            bad_mask = ~np.isfinite(cand)\n            if np.any(bad_mask):\n                # replace with uniform on those dims\n                rnd = self.rng.rand(np.sum(bad_mask))\n                cand[bad_mask] = lb[bad_mask] + rnd * span[bad_mask]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if self.evals >= self.budget:\n                break\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.copy()])\n            F = np.concatenate([F, np.array([f_cand], dtype=float)])\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25%\n            if np.isfinite(f_cand):\n                rank = int(np.sum(np.isfinite(F) & (F < f_cand)))\n                if improved or (rank <= max(1, int(0.25 * X.shape[0]))):\n                    success_history.append(1)\n                else:\n                    success_history.append(0)\n            else:\n                success_history.append(0)\n            # keep window\n            if len(success_history) > success_window:\n                success_history = success_history[-success_window:]\n\n            # prune archive if too large: keep the best ones and some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_best = int(0.6 * self.archive_max)\n                keep_idx = list(order[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                # keep some random survivors\n                num_random = max(0, self.archive_max - keep_best)\n                rand_keep = []\n                if len(rest) > 0 and num_random > 0:\n                    sel = min(len(rest), num_random)\n                    rand_keep = list(self.rng.choice(rest, size=sel, replace=False))\n                keep_idx += rand_keep\n                if len(keep_idx) < self.archive_max:\n                    # pad by best remaining\n                    need = self.archive_max - len(keep_idx)\n                    extra = [i for i in order if i not in keep_idx][:need]\n                    keep_idx += extra\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) >= success_window:\n                succ_rate = sum(success_history) / float(len(success_history))\n                if succ_rate > 0.6:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(1e-6, self.gscale * (0.6 + 0.3 * self.rng.rand()))\n                elif succ_rate < 0.1:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.6 * self.rng.rand()))\n                else:\n                    # small jitter\n                    self.gscale = max(1e-8, self.gscale * (0.9 + 0.2 * self.rng.rand()))\n                # keep within bounds\n                self.gscale = float(np.clip(self.gscale, 1e-8, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter >= stagnation_threshold and self.evals < self.budget:\n                # create some injected candidates (prefer perturbations of best)\n                injects = []\n                num_inject = min(8 + self.dim, max(2, int(0.1 * self.archive_max)))\n                best_idx = int(np.nanargmin(np.where(np.isfinite(F), F, np.inf)))\n                best_x = X[best_idx].copy()\n                for k in range(num_inject):\n                    if self.rng.rand() < 0.6:\n                        # perturb elite/best\n                        scale = (1.5 ** (self.rng.rand() - 0.5)) * self.gscale * (1.0 + 0.5 * self.rng.rand())\n                        inj = best_x + self.rng.randn(self.dim) * per_dim_scale * scale\n                    else:\n                        # random uniform\n                        inj = self._uniform_array(lb, ub)\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    injects.append(inj)\n                # evaluate them (append to archive)\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_new = _safe_eval(inj)\n                    X = np.vstack([X, inj.copy()])\n                    F = np.concatenate([F, np.array([f_new], dtype=float)])\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                stagnation_counter = 0\n                success_history = []\n\n            # safety stop if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None or not np.isfinite(self.f_opt):\n            return float(self.f_opt), center\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a7e54802-00a1-48ce-8acf-da39854679a6", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven sampler mixing anisotropic Gaussians, PCA-guided elite moves, DE-like donors, tempered Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=None,\n                 gscale_init=0.3,\n                 init_samples=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = archive_max if archive_max is not None else max(50, 20 * self.dim)\n        # global search scale (relative)\n        self.gscale = float(gscale_init)\n        self.gscale_max = 10.0\n        # initial samples besides center\n        if init_samples is None:\n            self.init_samples = min(2 + self.dim, max(5, int(0.2 * self.budget)))\n        else:\n            self.init_samples = int(init_samples)\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try to extract bounds from common attributes; otherwise default to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # b may have .lb/.ub or be a sequence\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe bounds is array-like pair\n                    b = np.asarray(b, dtype=float)\n                    if b.ndim == 2 and b.shape[0] == 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        try:\n            if lb is None or ub is None:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            # broadcast scalars\n            if lb.ndim == 0:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.ndim == 0:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n            # resize if needed\n            if lb.size != self.dim:\n                lb = np.resize(lb, (self.dim,))\n            if ub.size != self.dim:\n                ub = np.resize(ub, (self.dim,))\n            # ensure validity\n            for i in range(self.dim):\n                if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                    lb[i] = -5.0\n                    ub[i] = 5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.asarray(x, dtype=float)\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp as a safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.ndim == 1:\n            if n is None:\n                return lb + self.rng.rand(self.dim) * (ub - lb)\n            else:\n                shape = (n, self.dim)\n                return lb + self.rng.rand(*shape) * (ub - lb)\n        else:\n            # fallback\n            if n is None:\n                return lb + self.rng.rand(*lb.shape) * (ub - lb)\n            else:\n                shape = (n,) + lb.shape\n                return lb + self.rng.rand(*shape) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 1.0\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            if self.evals >= self.budget:\n                return float(\"inf\")\n            try:\n                f = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                f = float(\"inf\")\n            self.evals += 1\n            # keep track of best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.asarray(x, dtype=float).copy()\n            return f\n\n        # initialization: prepare archive X, F\n        self.evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        center = (lb + ub) / 2.0\n        X_list = []\n        # ensure center\n        X_list.append(center.copy())\n\n        # LHS-like seeding stratified per-dimension but independent across dims\n        n0 = max(1, min(self.init_samples - 1, self.budget - 1))\n        if n0 > 0:\n            cut = np.linspace(0.0, 1.0, n0 + 1)\n            for i in range(n0):\n                r = self.rng.rand(self.dim)\n                vals = (cut[i] + r * (cut[i + 1] - cut[i]))\n                samp = lb + vals * (ub - lb)\n                X_list.append(samp)\n\n        # ensure at least one purely uniform random sample\n        if len(X_list) < self.init_samples:\n            X_list.append(lb + self.rng.rand(self.dim) * (ub - lb))\n\n        # Evaluate initial population (respecting budget)\n        X = []\n        F = []\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = _safe_eval(x)\n            X.append(np.asarray(x, dtype=float).copy())\n            F.append(float(f))\n\n        if len(X) == 0:\n            # cannot evaluate at all: return center and inf\n            return float(\"inf\"), center\n\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # track best finite\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = int(np.nanargmin(np.where(finite_mask, F, np.inf)))\n            f_best = float(F[best_idx])\n            best_x = X[best_idx].copy()\n        else:\n            f_best = float(np.inf)\n            best_x = center.copy()\n\n        # short-term adaptation variables\n        success_history = []\n        success_window = 20\n        stagnation_counter = 0\n        stagnation_threshold = max(20, 5 * self.dim)\n\n        # constants for proposals\n        p_local = 0.35\n        p_pca = 0.60  # cumulative\n        p_de = 0.80\n        p_cauchy = 0.95\n        # archive mixing otherwise\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n            N = max(N, 1)\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elites_k = max(2, int(0.2 * N))\n\n            # per-dim adaptive scale: robust spread from IQR\n            try:\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n            except Exception:\n                # fallback\n                q25 = np.percentile(X, 25, axis=0)\n                q75 = np.percentile(X, 75, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            per_dim_std_approx = iqr / 1.349\n            per_dim_scale = np.maximum(per_dim_std_approx, 1e-12) * max(1e-9, self.gscale)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            vals = np.where(np.isfinite(F), F, np.inf).astype(float)\n            finite_idx = np.isfinite(vals)\n            probs = np.ones(N, dtype=float) / float(N)\n            if np.any(finite_idx):\n                finite_vals = vals[finite_idx]\n                vmin = np.min(finite_vals)\n                vmax = np.max(finite_vals)\n                temperature = max(1e-9, (vmax - vmin))\n                scores = np.zeros(N, dtype=float)\n                scores[finite_idx] = np.exp(-(vals[finite_idx] - vmin) / temperature)\n                if np.isfinite(np.sum(scores)) and np.sum(scores) > 0:\n                    probs = scores / np.sum(scores)\n            base_idx = int(self.rng.choice(N, p=probs))\n            base_x = X[base_idx].copy()\n            center_choice = base_x.copy()\n\n            # choose move type by probabilities\n            r = self.rng.rand()\n\n            # prepare some donors for DE and mixing\n            donor = None\n            if N >= 2:\n                idxs = list(range(N))\n                if base_idx in idxs:\n                    idxs.remove(base_idx)\n                # pick a donor uniformly\n                donor_idx = self.rng.choice(idxs)\n                donor = X[donor_idx].copy()\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < p_local:\n                archive_mean = np.nanmean(X, axis=0)\n                pull_strength = 0.02 + 0.18 * self.rng.rand()\n                pull = (archive_mean - base_x) * pull_strength\n                noise = self.rng.randn(self.dim) * per_dim_scale * (0.6 + 0.8 * self.rng.rand(self.dim))\n                cand = base_x + pull + noise\n\n            # PCA-guided elite moves\n            elif r < p_pca:\n                elites_idx = order_all[:elites_k]\n                Xel = X[elites_idx]\n                if Xel.shape[0] >= 2:\n                    Xc = Xel - np.mean(Xel, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # rows are principal components\n                        # component weights from singular values\n                        comp_weights = (S + 1e-12) / (np.sum(S) + 1e-12)\n                        # number of components to use\n                        k_use = max(1, min(self.dim, 1 + int(self.rng.poisson(1.5))))\n                        chosen = self.rng.choice(comps.shape[0], size=k_use, replace=False, p=comp_weights)\n                        coeffs = np.zeros(self.dim)\n                        for j, cidx in enumerate(chosen):\n                            coef_scale = (1.2 / (1 + j)) * self.gscale\n                            coeffs += coef_scale * (self.rng.randn() * comps[cidx])\n                        # center on a sampled elite\n                        elite_choice = elites_idx[self.rng.randint(0, len(elites_idx))]\n                        cand = X[elite_choice].copy() + coeffs * span * 0.3\n                    except Exception:\n                        # fallback to local move\n                        noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand())\n                        cand = base_x + noise\n                else:\n                    noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand())\n                    cand = base_x + noise\n\n            # Differential-like donor\n            elif r < p_de:\n                if N >= 3:\n                    # pick two distinct donors different from base\n                    idxs = list(range(N))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    a, b = self.rng.choice(idxs, size=2, replace=False)\n                    Fscale = 0.6 + 1.2 * self.rng.rand()\n                    cand = base_x + Fscale * (X[a] - X[b]) + 0.05 * self.rng.randn(self.dim) * per_dim_scale\n                elif donor is not None:\n                    cand = base_x + 0.5 * (donor - base_x) + 0.1 * self.rng.randn(self.dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < p_cauchy:\n                # temper very large outliers and scale to per-dim\n                cauch = self.rng.standard_cauchy(self.dim)\n                cauch = np.tanh(cauch / 3.0)  # temper extreme tails\n                scale_factors = per_dim_scale * (0.4 + self.rng.rand(self.dim))\n                cand = center_choice + 0.8 * cauch * scale_factors\n\n            # ARCHIVE_MIX\n            else:\n                if donor is None:\n                    # fallback uniform\n                    cand = lb + self.rng.rand(self.dim) * span\n                else:\n                    cand = base_x.copy()\n                    mix_mask = self.rng.rand(self.dim) < (0.12 + 0.3 * self.rng.rand())\n                    if np.any(mix_mask):\n                        cand[mix_mask] = donor[mix_mask]\n                    # small gaussian perturbation\n                    cand += 0.02 * span * (self.rng.randn(self.dim) * self.rng.rand())\n                    # occasionally inject uniform in random dimensions\n                    if self.rng.rand() < 0.06:\n                        replace_dim = self.rng.randint(0, self.dim)\n                        cand[replace_dim] = lb[replace_dim] + self.rng.rand() * span[replace_dim]\n\n            # safety: replace any non-finite entries and reflect to bounds\n            cand = np.asarray(cand, dtype=float)\n            mask_nonfinite = ~np.isfinite(cand)\n            if np.any(mask_nonfinite):\n                # replace non-finite entries with uniform inside bounds\n                cand[mask_nonfinite] = lb[mask_nonfinite] + self.rng.rand(np.sum(mask_nonfinite)) * span[mask_nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if self.evals >= self.budget:\n                break\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.copy()])\n            F = np.concatenate([F, np.array([f_cand], dtype=float)])\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                best_x = cand.copy()\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            success = 0\n            if np.isfinite(f_cand):\n                finite_mask = np.isfinite(F)\n                finite_vals = F[finite_mask]\n                if finite_vals.size > 0:\n                    thresh = np.percentile(finite_vals, 25)\n                    if f_cand <= thresh or improved:\n                        success = 1\n            success_history.append(success)\n            if len(success_history) > success_window:\n                success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_best = int(0.6 * self.archive_max)\n                keep_idx = list(order[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                num_random = int(0.2 * self.archive_max)\n                rand_keep = []\n                if len(rest) > 0 and num_random > 0:\n                    k = min(len(rest), num_random)\n                    rand_keep = list(self.rng.choice(rest, size=k, replace=False))\n                keep_idx += rand_keep\n                # pad by best remaining\n                need = self.archive_max - len(keep_idx)\n                if need > 0:\n                    extra = [i for i in order if i not in keep_idx][:need]\n                    keep_idx += extra\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) == success_window:\n                succ_rate = sum(success_history) / float(success_window)\n                if succ_rate > 0.4:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(1e-6, self.gscale * (0.7 + 0.2 * self.rng.rand()))\n                elif succ_rate < 0.08:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.5 * self.rng.rand()))\n                else:\n                    # small jitter\n                    self.gscale *= (0.95 + 0.1 * self.rng.rand())\n                # keep within bounds\n                self.gscale = float(np.clip(self.gscale, 1e-8, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter >= stagnation_threshold:\n                # create some injected candidates (prefer perturbations of best)\n                injects = []\n                num_inject = min(8 + self.dim, max(2, int(0.08 * self.archive_max)))\n                for k in range(num_inject):\n                    if self.rng.rand() < 0.7 and self.x_opt is not None:\n                        # perturb elite/best\n                        scale = (1.5 ** (self.rng.rand() - 0.5)) * self.gscale * (1.0 + 0.5 * self.rng.rand())\n                        inj = self.x_opt + self.rng.randn(self.dim) * per_dim_scale * scale\n                    else:\n                        # random uniform\n                        inj = lb + self.rng.rand(self.dim) * span\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    injects.append(inj)\n                # evaluate them (replace worst)\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_new = _safe_eval(inj)\n                    X = np.vstack([X, inj.copy()])\n                    F = np.concatenate([F, np.array([f_new], dtype=float)])\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                stagnation_counter = 0\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None or not np.isfinite(self.f_opt):\n            return float(self.f_opt), center\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "84ca146d-4005-4a2a-b333-f7657a130e86", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — hybrid archive-driven sampler mixing anisotropic Gaussians, PCA-guided elite moves, DE-like donors, tempered Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=None,\n                 gscale_init=0.3,\n                 init_samples=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = archive_max if archive_max is not None else max(50, 20 * self.dim)\n        # global search scale (relative)\n        self.gscale = float(gscale_init)\n        self.gscale_max = 10.0\n        # initial samples besides center\n        if init_samples is None:\n            self.init_samples = min(2 + self.dim, max(5, int(0.2 * self.budget)))\n        else:\n            self.init_samples = int(init_samples)\n        # bookkeeping\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try to extract bounds from common attributes; otherwise default to [-5,5]^dim\n        lb = None\n        ub = None\n        # try common attributes in order\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            try:\n                # object with .lb/.ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe array-like shape (2, dim) or (dim,2)\n                    b_arr = np.asarray(b, dtype=float)\n                    if b_arr.ndim == 2 and b_arr.shape[0] == 2:\n                        lb = b_arr[0].astype(float)\n                        ub = b_arr[1].astype(float)\n            except Exception:\n                lb = None\n                ub = None\n        # try func.lb/func.ub\n        if lb is None or ub is None:\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n        # fallback to scalar bounds if provided as attributes\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n                    lb = np.asarray(func.lower, dtype=float)\n                    ub = np.asarray(func.upper, dtype=float)\n            except Exception:\n                pass\n        # default\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # broadcast scalars if needed\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.ndim == 0:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.ndim == 0:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n        if lb.size != self.dim:\n            lb = np.resize(lb, (self.dim,))\n        if ub.size != self.dim:\n            ub = np.resize(ub, (self.dim,))\n        # ensure validity\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        # coordinate-wise reflection into [lb, ub]\n        x = np.asarray(x, dtype=float)\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp as a safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.rand(*lb.shape) * (ub - lb)\n        else:\n            shape = (n,) + lb.shape\n            return lb + self.rng.rand(*shape) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 1.0\n        center = (lb + ub) / 2.0\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return float(\"inf\")\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            self.evals += 1\n            # keep track of best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f\n\n        # initialization: prepare archive X, F\n        self.evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        X_list = []\n        F_list = []\n\n        # ensure center\n        X_list.append(center.copy())\n        F_list.append(_safe_eval(center.copy()))\n\n        # LHS-like seeding stratified per-dimension but independent across dims\n        n0 = max(1, min(self.init_samples - 1, self.budget - self.evals))\n        if n0 > 0:\n            # For each dimension, create a random permutation of strata\n            strata = np.arange(n0)\n            perms = [self.rng.permutation(n0) for _ in range(self.dim)]\n            for i in range(n0):\n                u = np.empty(self.dim, dtype=float)\n                for d in range(self.dim):\n                    idx = perms[d][i]\n                    low = idx / float(n0)\n                    high = (idx + 1) / float(n0)\n                    u[d] = low + self.rng.rand() * (high - low)\n                samp = lb + u * (ub - lb)\n                samp = self._reflect_bounds(samp, lb, ub)\n                X_list.append(samp.copy())\n                F_list.append(_safe_eval(samp))\n                if self.evals >= self.budget:\n                    break\n\n        # ensure at least one purely uniform random sample\n        while len(X_list) < self.init_samples and self.evals < self.budget:\n            samp = self._uniform_array(lb, ub)\n            X_list.append(samp.copy())\n            F_list.append(_safe_eval(samp))\n\n        # convert to arrays\n        X = np.asarray(X_list, dtype=float)\n        F = np.asarray(F_list, dtype=float)\n\n        # track best finite\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            finite_idx = np.where(finite_mask)[0]\n            best_idx = finite_idx[np.argmin(F[finite_mask])]\n            f_best = float(F[best_idx])\n            best_x = X[best_idx].copy()\n            # ensure global trackers\n            if np.isfinite(f_best):\n                self.f_opt = f_best\n                self.x_opt = best_x.copy()\n        else:\n            f_best = float(np.inf)\n            best_x = center.copy()\n\n        # short-term adaptation variables\n        success_window = max(10, min(60, int(0.02 * self.budget) or 20))\n        success_history = [0] * success_window\n        stagnation_counter = 0\n\n        # constants for proposals\n        p_local = 0.35\n        p_pca = 0.60  # cumulative\n        p_de = 0.80\n        p_cauchy = 0.95\n        # archive mixing otherwise\n\n        # main loop\n        stagnation_threshold = max(20, int(0.05 * self.budget))\n        while self.evals < self.budget:\n            N = X.shape[0]\n            N = max(N, 1)\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elites_k = max(2, int(0.2 * N))\n\n            # per-dim adaptive scale: robust spread from IQR\n            try:\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n            except Exception:\n                q25 = np.percentile(X, 25, axis=0)\n                q75 = np.percentile(X, 75, axis=0)\n            iqr = np.maximum(q75 - q25, 1e-12)\n            per_dim_std_approx = iqr / 1.349\n            per_dim_scale = np.maximum(per_dim_std_approx, 1e-12) * max(1e-9, self.gscale)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            vals = np.where(np.isfinite(F), F, np.inf).astype(float)\n            finite_idx = np.isfinite(vals)\n            probs = np.ones(N, dtype=float) / float(N)\n            if np.any(finite_idx):\n                finite_vals = vals[finite_idx]\n                vmin = np.min(finite_vals)\n                vmax = np.max(finite_vals)\n                temperature = max(1e-9, (vmax - vmin))\n                scores = np.zeros(N, dtype=float)\n                scores[finite_idx] = np.exp(-(vals[finite_idx] - vmin) / temperature)\n                if np.isfinite(np.sum(scores)) and np.sum(scores) > 0:\n                    probs = scores / float(np.sum(scores))\n            base_idx = int(self.rng.choice(N, p=probs))\n            base_x = X[base_idx].copy()\n            center_choice = base_x.copy()\n\n            # prepare some donors for DE and mixing\n            donor = None\n            donor_idx = None\n            if N >= 2:\n                idxs = list(range(N))\n                if base_idx in idxs:\n                    idxs.remove(base_idx)\n                if len(idxs) > 0:\n                    donor_idx = int(self.rng.choice(idxs))\n                    donor = X[donor_idx].copy()\n\n            # choose move type by probabilities\n            r = self.rng.rand()\n\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < p_local:\n                pull_strength = 0.02 + 0.18 * self.rng.rand()\n                mean_pull = pull_strength * (np.nanmean(X, axis=0) - base_x)\n                noise = self.rng.randn(self.dim) * per_dim_scale * (0.6 + 0.8 * self.rng.rand(self.dim))\n                cand = base_x + mean_pull + noise\n\n            # PCA-guided elite moves\n            elif r < p_pca:\n                elites_idx = order_all[:elites_k]\n                Xel = X[elites_idx, :]\n                if Xel.shape[0] >= 2:\n                    Xc = Xel - np.mean(Xel, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # rows are components\n                        # component weights from singular values\n                        comp_weights = (S + 1e-12) / (np.sum(S) + 1e-12)\n                        # number of components to use\n                        k_use = max(1, min(self.dim, 1 + int(self.rng.poisson(1.5))))\n                        chosen = self.rng.choice(comps.shape[0], size=k_use, replace=False, p=comp_weights)\n                        coeffs = np.zeros(self.dim)\n                        for j, cidx in enumerate(chosen):\n                            coef_scale = (1.2 / (1 + j)) * self.gscale\n                            coeffs += coef_scale * (self.rng.randn() * comps[cidx])\n                        # center on a sampled elite\n                        elite_choice = int(elites_idx[self.rng.randint(0, len(elites_idx))])\n                        cand = X[elite_choice].copy() + coeffs * span * 0.3\n                    except Exception:\n                        # fallback to local move\n                        noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand())\n                        cand = base_x + noise\n                else:\n                    noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand())\n                    cand = base_x + noise\n\n            # Differential-like donor\n            elif r < p_de:\n                if N >= 3:\n                    idxs = list(range(N))\n                    if base_idx in idxs:\n                        idxs.remove(base_idx)\n                    # pick two distinct donors different from base\n                    a, b = self.rng.choice(idxs, size=2, replace=False)\n                    Fscale = 0.6 + 1.2 * self.rng.rand()\n                    cand = base_x + Fscale * (X[a] - X[b]) + 0.05 * self.rng.randn(self.dim) * per_dim_scale\n                elif donor is not None:\n                    cand = base_x + 0.5 * (donor - base_x) + 0.1 * self.rng.randn(self.dim) * per_dim_scale\n                else:\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale\n\n            # CAUCHY jumps\n            elif r < p_cauchy:\n                # tempered Cauchy\n                cauch = self.rng.standard_cauchy(size=self.dim)\n                cauch = np.tanh(cauch / 3.0)  # temper extreme tails\n                scale_factors = per_dim_scale * (0.4 + self.rng.rand(self.dim))\n                cand = center_choice + 0.8 * cauch * scale_factors\n\n            # ARCHIVE_MIX\n            else:\n                if donor is None:\n                    # fallback uniform\n                    cand = lb + self.rng.rand(self.dim) * span\n                else:\n                    cand = base_x.copy()\n                    # mix random subset of dimensions from donor\n                    mix_prob = 0.2 + 0.3 * self.rng.rand()\n                    mix_mask = self.rng.rand(self.dim) < mix_prob\n                    if np.any(mix_mask):\n                        cand[mix_mask] = donor[mix_mask]\n                    # small gaussian perturbation\n                    cand += 0.02 * span * (self.rng.randn(self.dim) * self.rng.rand())\n                    # occasionally inject uniform in random dimensions\n                    if self.rng.rand() < 0.06:\n                        k_replace = max(1, int(0.05 * self.dim))\n                        replace_dim = self.rng.choice(self.dim, size=k_replace, replace=False)\n                        cand[replace_dim] = lb[replace_dim] + self.rng.rand(k_replace) * span[replace_dim]\n\n            # safety: replace any non-finite entries and reflect to bounds\n            mask_nonfinite = ~np.isfinite(cand)\n            if np.any(mask_nonfinite):\n                cand[mask_nonfinite] = lb[mask_nonfinite] + self.rng.rand(np.sum(mask_nonfinite)) * span[mask_nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_cand = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.copy()])\n            F = np.concatenate([F, np.array([f_cand], dtype=float)])\n\n            # update best trackers\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_best:\n                f_best = float(f_cand)\n                best_x = cand.copy()\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            finite_mask = np.isfinite(F)\n            finite_vals = F[finite_mask] if np.any(finite_mask) else np.array([])\n            success = 0\n            if improved:\n                success = 1\n            elif finite_vals.size > 0:\n                try:\n                    thresh = np.percentile(finite_vals, 25)\n                    if np.isfinite(f_cand) and f_cand <= thresh:\n                        success = 1\n                except Exception:\n                    success = 0\n            # update success history as FIFO\n            success_history.pop(0)\n            success_history.append(success)\n\n            # prune archive if too large: keep the best ones and some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_best = max(2, int(0.7 * self.archive_max))\n                keep_idx = list(order[:keep_best])\n                rest = [i for i in range(Ncurr) if i not in keep_idx]\n                num_random = int(0.2 * self.archive_max)\n                rand_keep = []\n                if len(rest) > 0 and num_random > 0:\n                    k = min(len(rest), num_random)\n                    rand_keep = list(self.rng.choice(rest, size=k, replace=False))\n                keep_idx += rand_keep\n                # pad by best remaining if needed\n                need = self.archive_max - len(keep_idx)\n                if need > 0:\n                    remaining = [i for i in order if i not in keep_idx]\n                    keep_idx += list(remaining[:need])\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) == success_window:\n                succ_rate = sum(success_history) / float(success_window)\n                if succ_rate > 0.4:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(1e-6, self.gscale * (0.7 + 0.2 * self.rng.rand()))\n                elif succ_rate < 0.15:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.5 * self.rng.rand()))\n                else:\n                    # small jitter\n                    self.gscale *= (0.95 + 0.1 * self.rng.rand())\n                # keep within bounds\n                self.gscale = float(np.clip(self.gscale, 1e-8, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter >= stagnation_threshold and self.evals < self.budget:\n                # create some injected candidates (prefer perturbations of best)\n                injects = []\n                num_inject = min(8 + self.dim, max(2, int(0.08 * self.archive_max)))\n                for _ in range(num_inject):\n                    if self.rng.rand() < 0.7 and np.isfinite(self.f_opt):\n                        # perturb elite/best\n                        scale = (1.5 ** (self.rng.rand() - 0.5)) * self.gscale * (1.0 + 0.5 * self.rng.rand())\n                        inj = self.x_opt + self.rng.randn(self.dim) * (per_dim_scale * scale)\n                        inj = self._reflect_bounds(inj, lb, ub)\n                    else:\n                        # random uniform\n                        inj = lb + self.rng.rand(self.dim) * span\n                    injects.append(inj)\n                # evaluate them (append)\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_new = _safe_eval(inj)\n                    X = np.vstack([X, inj.copy()])\n                    F = np.concatenate([F, np.array([f_new], dtype=float)])\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                stagnation_counter = 0\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None or not np.isfinite(self.f_opt):\n            return float(self.f_opt), center\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5af3523f-6109-4fb2-9e4f-63c29766534f", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — hybrid archive-driven sampler mixing anisotropic Gaussians, PCA-guided elite moves, DE-like donors, tempered Cauchy jumps, archive mixing, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # global search scale (relative)\n        self.gscale_max = 10.0\n        self.gscale = 0.5\n\n        # initial samples besides center\n        if init_samples is None:\n            # few initial samples but not exceeding budget\n            self.init_samples = int(min(max(5, 2 + self.dim), max(2, self.budget // 10)))\n        else:\n            self.init_samples = int(init_samples)\n\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # archive parameters\n        self.archive_max = max(50, 10 * self.dim)\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try common attributes, else default to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\") and func.bounds is not None:\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe array-like pair\n                    b_arr = np.asarray(b, dtype=float)\n                    if b_arr.ndim >= 2 and b_arr.shape[0] >= 2:\n                        lb = np.asarray(b_arr[0], dtype=float)\n                        ub = np.asarray(b_arr[1], dtype=float)\n            # try direct attributes\n            if lb is None and hasattr(func, \"lb\"):\n                lb = np.asarray(func.lb, dtype=float)\n            if ub is None and hasattr(func, \"ub\"):\n                ub = np.asarray(func.ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # broadcast scalars\n        if lb.ndim == 0:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.ndim == 0:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # resize if needed\n        if lb.size != self.dim:\n            lb = np.resize(lb, (self.dim,))\n        if ub.size != self.dim:\n            ub = np.resize(ub, (self.dim,))\n\n        # validate\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or lb[i] >= ub[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp as safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            shape = (n,) + lb.shape\n            u = self.rng.rand(*shape)\n            return lb + u * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 1.0\n\n        # safe evaluation wrapper that respects budget\n        def _safe_eval(x):\n            if self.evals >= self.budget:\n                return float(\"inf\")\n            try:\n                fval = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                fval = float(\"inf\")\n            self.evals += 1\n            # update best if finite\n            if np.isfinite(fval) and fval < self.f_opt:\n                self.f_opt = float(fval)\n                self.x_opt = np.asarray(x, dtype=float).copy()\n            return fval\n\n        # initialization: prepare archive X, F\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        center = (lb + ub) / 2.0\n        X_list = []\n        X_list.append(center.copy())\n\n        # LHS-like seeding stratified per-dimension but independent across dims\n        n0 = max(1, min(self.init_samples - 1, max(0, self.budget - 1)))\n        if n0 > 0:\n            cut = np.linspace(0.0, 1.0, n0 + 1)\n            for i in range(n0):\n                r = self.rng.rand(self.dim)\n                vals = cut[i] + r * (cut[i + 1] - cut[i])\n                x = lb + vals * span\n                X_list.append(x)\n\n        # ensure at least one purely uniform random sample\n        if len(X_list) < self.init_samples:\n            X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        X = []\n        F = []\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = _safe_eval(x)\n            X.append(np.asarray(x, dtype=float))\n            F.append(float(f))\n\n        if len(X) == 0:\n            # no evaluations possible -> return center and inf\n            return float(\"inf\"), center\n\n        X = np.vstack(X)\n        F = np.asarray(F, dtype=float)\n\n        # track best finite\n        finite_mask_all = np.isfinite(F)\n        if np.any(finite_mask_all):\n            best_idx = int(np.nanargmin(np.where(finite_mask_all, F, np.inf)))\n            self.f_opt = float(F[best_idx])\n            self.x_opt = X[best_idx].copy()\n        else:\n            self.f_opt = float(np.inf)\n            self.x_opt = center.copy()\n\n        # short-term adaptation variables\n        stagnation_counter = 0\n        stagnation_threshold = max(40, 10 * self.dim)\n\n        # constants for proposals\n        p_local = 0.35\n        p_pca = 0.60  # cumulative\n        p_de = 0.80\n        p_cauchy = 0.95\n\n        # success history for adaptation\n        success_history = []\n        success_window = 30\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n            N = max(N, 1)\n\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elites_k = max(2, int(0.2 * N))\n\n            # per-dim adaptive scale: robust spread from IQR\n            try:\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n            except Exception:\n                q25 = np.percentile(X, 25, axis=0)\n                q75 = np.percentile(X, 75, axis=0)\n            iqr = q75 - q25\n            per_dim_std_approx = iqr / 1.349\n            per_dim_scale = np.maximum(per_dim_std_approx, 1e-12) * max(1e-9, self.gscale)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            vals = F.copy()\n            finite_idx = np.isfinite(vals)\n            probs = np.ones(N) / float(N)\n            if np.any(finite_idx):\n                finite_vals = vals[finite_idx]\n                vmin = np.nanmin(finite_vals)\n                vmax = np.nanmax(finite_vals)\n                temperature = max(1e-9, (vmax - vmin))\n                scores = np.zeros(N, dtype=float)\n                scores[finite_idx] = np.exp(-(vals[finite_idx] - vmin) / temperature)\n                # ensure some small positive for infinite entries\n                scores[~finite_idx] = np.min(scores[finite_idx]) * 0.01 if np.any(finite_idx) else 1.0\n                ssum = np.sum(scores)\n                if ssum <= 0 or not np.isfinite(ssum):\n                    probs = np.ones(N) / float(N)\n                else:\n                    probs = scores / ssum\n            base_idx = int(self.rng.choice(N, p=probs))\n            base_x = X[base_idx].copy()\n\n            # choose move type by probabilities\n            r = self.rng.rand()\n\n            cand = None\n            donor = None\n\n            # prepare some donors for DE and mixing if possible\n            if N >= 3:\n                idxs = list(range(N))\n                if base_idx in idxs:\n                    idxs.remove(base_idx)\n                # pick two distinct donors uniformly\n                if len(idxs) >= 2:\n                    d1, d2 = self.rng.choice(idxs, size=2, replace=False)\n                    donor1 = X[d1].copy()\n                    donor2 = X[d2].copy()\n                else:\n                    donor1 = donor2 = None\n            else:\n                donor1 = donor2 = None\n\n            if r < p_local:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                pull_strength = 0.02 + 0.18 * self.rng.rand()\n                pop_mean = np.nanmean(X, axis=0)\n                pull = pull_strength * (pop_mean - base_x)\n                noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.6 * self.rng.rand(self.dim))\n                cand = base_x + pull + noise\n\n            elif r < p_pca:\n                # PCA-guided elite moves\n                elites_idx = order_all[:elites_k]\n                Xel = X[elites_idx]\n                Xc = Xel - np.mean(Xel, axis=0)\n                try:\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    comps = Vt  # rows are principal components\n                    comp_weights = np.maximum(S, 1e-12)\n                    comp_weights = comp_weights / np.sum(comp_weights)\n                    # number of components to use\n                    k_use = max(1, min(self.dim, 1 + int(self.rng.poisson(1.5))))\n                    chosen = self.rng.choice(comps.shape[0], size=k_use, replace=False, p=comp_weights)\n                    coeffs = np.zeros(self.dim)\n                    coef_scale = 1.0 + 0.8 * self.rng.rand()\n                    for cidx in chosen:\n                        coeff = self.rng.randn() * (S[cidx] / (S[0] + 1e-12))\n                        coeffs += coeff * comps[cidx]\n                    # center on a sampled elite\n                    elite_choice = elites_idx[self.rng.randint(0, len(elites_idx))]\n                    cand = X[elite_choice].copy() + coeffs * span * 0.3 * coef_scale\n                    # small gaussian jitter\n                    cand += self.rng.randn(self.dim) * per_dim_scale * 0.5\n                except Exception:\n                    noise = self.rng.randn(self.dim) * per_dim_scale * (1.0 + 0.5 * self.rng.rand(self.dim))\n                    cand = base_x + noise\n\n            elif r < p_de:\n                # Differential-like donor\n                if donor1 is not None and donor2 is not None:\n                    Fscale = 0.6 + 1.2 * self.rng.rand()\n                    cand = base_x + Fscale * (donor1 - donor2) + self.rng.randn(self.dim) * per_dim_scale * 0.3\n                    # occasionally replace a few components with donor1\n                    mix_mask = self.rng.rand(self.dim) < (0.12 + 0.2 * self.rng.rand())\n                    cand[mix_mask] = donor1[mix_mask]\n                else:\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale\n\n            elif r < p_cauchy:\n                # CAUCHY jumps\n                cauch = self.rng.standard_cauchy(self.dim)\n                # temper very large outliers and scale to per-dim\n                cauch = np.tanh(cauch / 3.0)  # temper extreme tails\n                scale_factors = per_dim_scale * (0.4 + self.rng.rand(self.dim))\n                cand = base_x + cauch * scale_factors\n\n            else:\n                # ARCHIVE_MIX\n                # pick a donor randomly (not base if possible)\n                if N >= 2:\n                    choices = list(range(N))\n                    if base_idx in choices:\n                        choices.remove(base_idx)\n                    donor_idx = self.rng.choice(choices)\n                    donor = X[donor_idx].copy()\n                if donor is None:\n                    # fallback uniform\n                    cand = self._uniform_array(lb, ub)\n                else:\n                    cand = base_x.copy()\n                    mix_mask = self.rng.rand(self.dim) < (0.12 + 0.3 * self.rng.rand())\n                    cand[mix_mask] = donor[mix_mask]\n                    # small gaussian perturbation\n                    cand += 0.02 * span * (self.rng.randn(self.dim) * self.rng.rand(self.dim))\n                    # occasionally inject uniform in a random dimension\n                    if self.rng.rand() < 0.12:\n                        replace_dim = self.rng.randint(0, self.dim)\n                        cand[replace_dim] = lb[replace_dim] + self.rng.rand() * span[replace_dim]\n\n            # safety: replace any non-finite entries and reflect to bounds\n            mask_nonfinite = ~np.isfinite(cand)\n            if np.any(mask_nonfinite):\n                cand[mask_nonfinite] = lb[mask_nonfinite] + self.rng.rand(np.sum(mask_nonfinite)) * span[mask_nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if self.evals >= self.budget:\n                break\n            f = _safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            prev_best = self.f_opt\n            if np.isfinite(f) and f < prev_best:\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            success = 0\n            finite_mask = np.isfinite(F)\n            finite_vals = F[finite_mask]\n            if np.isfinite(f):\n                if f <= self.f_opt:\n                    success = 1\n                else:\n                    # find rank among finite\n                    if finite_vals.size > 0:\n                        rank = np.sum(finite_vals <= f)\n                        if rank <= max(1, int(0.25 * finite_vals.size)):\n                            success = 1\n            success_history.append(success)\n            if len(success_history) > success_window:\n                success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_best = int(0.6 * self.archive_max)\n                keep_idx = list(order[:keep_best])\n                rest = [i for i in order if i not in keep_idx]\n                num_random = self.archive_max - len(keep_idx)\n                if len(rest) > 0 and num_random > 0:\n                    k = min(len(rest), num_random)\n                    rand_keep = list(self.rng.choice(rest, size=k, replace=False))\n                    keep_idx += rand_keep\n                # pad by best remaining if still short\n                need = self.archive_max - len(keep_idx)\n                if need > 0:\n                    extra = [i for i in order if i not in keep_idx][:need]\n                    keep_idx += extra\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) == success_window:\n                succ_rate = sum(success_history) / float(success_window)\n                if succ_rate > 0.4:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(1e-6, self.gscale * (0.7 + 0.2 * self.rng.rand()))\n                elif succ_rate < 0.15:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.6 * self.rng.rand()))\n                else:\n                    # small jitter\n                    self.gscale = self.gscale * (0.95 + 0.1 * self.rng.rand())\n                # keep within bounds\n                self.gscale = float(np.clip(self.gscale, 1e-8, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter >= stagnation_threshold and self.evals < self.budget:\n                # create some injected candidates (prefer perturbations of best)\n                num_inject = min(8, max(2, self.dim))\n                injects = []\n                for k in range(num_inject):\n                    if self.rng.rand() < 0.7 and self.x_opt is not None:\n                        # perturb elite/best\n                        scale = (1.5 ** (self.rng.rand() - 0.5)) * self.gscale * (1.0 + 0.5 * self.rng.rand())\n                        inj = self.x_opt + self.rng.randn(self.dim) * (per_dim_scale * scale + 1e-12)\n                    else:\n                        # random uniform\n                        inj = lb + self.rng.rand(self.dim) * span\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    injects.append(inj)\n                # evaluate them (append while respecting budget)\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_new = _safe_eval(inj)\n                    X = np.vstack([X, inj])\n                    F = np.concatenate([F, np.array([f_new], dtype=float)])\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                stagnation_counter = 0  # reset\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            return float(np.inf), np.asarray(center, dtype=float)\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3fd73871-7411-4a4c-a3b7-48df819490b4", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — hybrid archive-driven sampler mixing anisotropic Gaussians, PCA-guided elite moves, DE-like donors, tempered Cauchy jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n\n        # initial seeding count (including center)\n        self.init_samples = max(2, int(init_samples))\n\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # archive parameters\n        self.archive_max = max(50, 10 * self.dim)\n\n        # global search scale (relative to local per-dim spread)\n        self.gscale = 0.35  # starting relative multiplier\n        self.gscale_max = 10.0\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # tries several common interfaces; fall back to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            # prefer a .bounds object with .lb and .ub (common in benchmarks)\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe an array-like of two rows\n                    b_arr = np.asarray(b, dtype=float)\n                    if b_arr.ndim >= 2 and b_arr.shape[0] >= 2:\n                        lb = np.asarray(b_arr[0], dtype=float)\n                        ub = np.asarray(b_arr[1], dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # direct attributes .lb / .ub\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                lb = lb\n                ub = ub\n\n        # fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            # broadcast scalar bounds\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub), dtype=float)\n            # resize or pad if mismatched\n            if lb.size != self.dim:\n                tmp = np.full(self.dim, -5.0, dtype=float)\n                for i in range(min(self.dim, lb.size)):\n                    tmp[i] = lb[i]\n                lb = tmp\n            if ub.size != self.dim:\n                tmp = np.full(self.dim, 5.0, dtype=float)\n                for i in range(min(self.dim, ub.size)):\n                    tmp[i] = ub[i]\n                ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp as safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        shape = (n,) + lb.shape\n        u = self.rng.rand(*shape)\n        return lb + u * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 1.0\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return float(\"inf\")\n            try:\n                f = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                f = float(\"inf\")\n            self.evals += 1\n            return f\n\n        # initialization\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n        self.evals = 0\n\n        center = 0.5 * (lb + ub)\n        X_list = []\n        X_list.append(center.copy())\n\n        # LHS-like seeding: per-dim stratified but independent across dims (cheap)\n        n0 = max(1, min(self.init_samples - 1, max(0, self.budget - 1)))\n        if n0 > 0:\n            for i in range(n0):\n                # independent stratification per-dim\n                vals = (i + self.rng.rand(self.dim)) / float(max(1, n0))\n                x = lb + vals * span\n                X_list.append(x)\n\n        # ensure at least one purely uniform random sample\n        X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        F = []\n        for x in X_list:\n            if self.evals >= self.budget:\n                F.append(float(\"inf\"))\n                continue\n            f = safe_eval(x)\n            F.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.asarray(x, dtype=float).copy()\n\n        X = np.vstack(X_list)\n        F = np.asarray(F, dtype=float)\n\n        # track best finite\n        finite_mask_all = np.isfinite(F)\n        if np.any(finite_mask_all):\n            best_idx = int(np.nanargmin(np.where(finite_mask_all, F, np.inf)))\n            self.f_opt = float(F[best_idx])\n            self.x_opt = X[best_idx].copy()\n        else:\n            self.f_opt = float(np.inf)\n            self.x_opt = center.copy()\n\n        # short-term adaptation variables\n        stagnation_counter = 0\n        stagnation_thresh = max(50, 8 * self.dim)\n\n        # constants for proposals\n        p_local = 0.35\n        p_de = 0.80  # cumulative: local if r<p_local; DE-like if p_local<=r<p_de; else archive/cauchy\n\n        # success history for adaptation\n        success_history = []\n        success_window = max(10, min(50, int(0.04 * self.budget)))\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n\n            # compute ranking and elites\n            order_all = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            finite_mask = np.isfinite(F)\n            finite_vals = F[finite_mask] if np.any(finite_mask) else np.array([], dtype=float)\n\n            # per-dim adaptive scale: robust spread from IQR\n            try:\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n                iqr = q75 - q25\n            except Exception:\n                iqr = np.clip(np.std(X, axis=0), 1e-12, None)\n            per_dim_std_approx = iqr / 1.349\n            # fallback positive\n            per_dim_std_approx = np.maximum(per_dim_std_approx, 1e-12)\n\n            per_dim_scale = per_dim_std_approx * self.gscale  # relative\n            # To translate to absolute distances use span\n            per_dim_scale_abs = per_dim_scale * (span / np.maximum(span, 1.0))\n\n            # choose base index biased to better ranks (softmax on negative F)\n            probs = np.ones(N, dtype=float) / float(N)\n            if np.any(finite_mask):\n                vals = F.copy()\n                finite_idx = np.where(finite_mask)[0]\n                vmin = float(np.nanmin(F[finite_idx]))\n                vmax = float(np.nanmax(F[finite_idx]))\n                temperature = max(1e-9, (vmax - vmin))\n                scores = np.ones(N, dtype=float) * 1e-12\n                scores[finite_idx] = np.exp(-(vals[finite_idx] - vmin) / temperature)\n                ssum = np.sum(scores)\n                if ssum > 0:\n                    probs = scores / ssum\n            base_idx = int(self.rng.choice(N, p=probs))\n            base_x = X[base_idx].copy()\n\n            # pick donor indices for DE-like moves (if available)\n            donor1 = None\n            donor2 = None\n            if N >= 3:\n                idxs = list(range(N))\n                idxs.remove(base_idx)\n                dsel = self.rng.choice(idxs, size=2, replace=False)\n                donor1 = X[dsel[0]].copy()\n                donor2 = X[dsel[1]].copy()\n\n            # choose move type\n            r = self.rng.rand()\n            cand = None\n\n            if r < p_local:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                pull = 0.06 * (np.nanmean(X, axis=0) - base_x) * (0.5 + self.rng.rand())\n                noise = self.rng.randn(self.dim) * (per_dim_scale_abs * (0.6 + 1.0 * self.rng.rand(self.dim)))\n                cand = base_x + pull + noise\n\n                # Occasionally do PCA-guided elite moves\n                if N >= 4 and self.rng.rand() < 0.22:\n                    elites_k = max(2, int(0.15 * N))\n                    elites_idx = order_all[:elites_k]\n                    Xel = X[elites_idx, :]\n                    try:\n                        Xc = Xel - np.mean(Xel, axis=0)\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        comps = Vt  # rows are components\n                        # number of components to use\n                        k_use = max(1, min(self.dim, 1 + int(self.rng.poisson(1.5))))\n                        k_use = min(k_use, comps.shape[0])\n                        coeffs = np.zeros(self.dim)\n                        # sample combination of top components\n                        for j in range(k_use):\n                            cidx = j\n                            coeff = (self.rng.randn() * (0.5 + self.rng.rand()))\n                            coeffs += coeff * comps[cidx]\n                        # center on a sampled elite\n                        elite_choice = int(elites_idx[self.rng.randint(0, len(elites_idx))])\n                        cand = X[elite_choice].copy() + coeffs * span * 0.3 * (1.0 + 0.8 * self.rng.rand())\n                        # small gaussian jitter\n                        cand += self.rng.randn(self.dim) * (0.06 * per_dim_scale_abs)\n                    except Exception:\n                        pass\n\n            elif r < p_de:\n                # Differential-like donor\n                if donor1 is not None and donor2 is not None:\n                    Fscale = 0.6 + 1.2 * self.rng.rand()\n                    cand = base_x.copy()\n                    diff = donor1 - donor2\n                    # mix some components from differential vector\n                    mix_mask = self.rng.rand(self.dim) < (0.25 + 0.2 * self.rng.rand())\n                    cand += Fscale * diff * (0.6 + 0.8 * self.rng.rand())\n                    # occasionally replace a few components with donor1 directly\n                    if self.rng.rand() < 0.28:\n                        replace_mask = self.rng.rand(self.dim) < 0.08\n                        cand[replace_mask] = donor1[replace_mask]\n                    # slight gaussian to avoid exact copies\n                    cand += self.rng.randn(self.dim) * (0.03 * per_dim_scale_abs)\n                else:\n                    # fallback to local gaussian\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale_abs\n\n                # occasionally perform a tempered Cauchy jump after DE step\n                if self.rng.rand() < 0.12:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    scale_factors = per_dim_scale_abs * (0.4 + 0.6 * self.rng.rand(self.dim))\n                    cand = base_x + cauch * scale_factors\n\n            else:\n                # ARCHIVE_MIX or CAUCHY jumps\n                if self.rng.rand() < 0.5 and N >= 2:\n                    # pick a donor randomly (not base if possible)\n                    choices = list(range(N))\n                    if base_idx in choices and len(choices) > 1:\n                        choices.remove(base_idx)\n                    donor_idx = int(self.rng.choice(choices))\n                    donor = X[donor_idx].copy()\n                    cand = base_x.copy()\n                    mix_mask = self.rng.rand(self.dim) < (0.20 + 0.25 * self.rng.rand())\n                    cand[mix_mask] = donor[mix_mask]\n                    # small gaussian perturbation\n                    cand += 0.02 * span * (self.rng.randn(self.dim) * self.rng.rand(self.dim))\n                    # occasionally inject uniform in a random dimension\n                    if self.rng.rand() < 0.12:\n                        replace_dim = int(self.rng.randint(0, self.dim))\n                        cand[replace_dim] = lb[replace_dim] + self.rng.rand() * span[replace_dim]\n                else:\n                    # tempered Cauchy global jump from a random archive point or base\n                    anchor = base_x if self.rng.rand() < 0.5 else X[self.rng.randint(0, N)]\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    scale_factors = per_dim_scale_abs * (0.4 + 0.6 * self.rng.rand(self.dim))\n                    cand = anchor + cauch * scale_factors\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            mask_nonfinite = ~np.isfinite(cand)\n            if np.any(mask_nonfinite):\n                cand[mask_nonfinite] = lb[mask_nonfinite] + self.rng.rand(np.sum(mask_nonfinite)) * span[mask_nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.asarray(cand, dtype=float).copy()\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            success = 0\n            if np.isfinite(f):\n                if f <= self.f_opt:\n                    success = 1\n                else:\n                    finite_vals = F[np.isfinite(F)]\n                    if finite_vals.size > 0:\n                        rank = np.sum(finite_vals <= f)\n                        if rank <= max(1, int(0.25 * finite_vals.size)):\n                            success = 1\n            success_history.append(success)\n            if len(success_history) > success_window:\n                success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                # keep best 60% and random rest\n                keep_best = int(0.6 * self.archive_max)\n                order = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_idx = list(order[:keep_best])\n                remaining = [int(i) for i in order[keep_best:]]\n                num_random = self.archive_max - len(keep_idx)\n                if len(remaining) > 0 and num_random > 0:\n                    rand_keep = list(self.rng.choice(remaining, size=min(len(remaining), num_random), replace=False))\n                    keep_idx += rand_keep\n                # pad by best remaining if still short\n                if len(keep_idx) < self.archive_max:\n                    extra = list(order[keep_best:keep_best + (self.archive_max - len(keep_idx))])\n                    keep_idx += extra\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) >= success_window:\n                succ_rate = float(sum(success_history)) / float(len(success_history))\n                if succ_rate > 0.4:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(1e-6, self.gscale * (0.7 + 0.2 * self.rng.rand()))\n                elif succ_rate < 0.15:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.6 * self.rng.rand()))\n                    # small jitter\n                    self.gscale = self.gscale * (0.95 + 0.1 * self.rng.rand())\n                # keep within bounds\n                self.gscale = float(np.clip(self.gscale, 1e-8, self.gscale_max))\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter >= stagnation_thresh:\n                # create some injected candidates (prefer perturbations of best)\n                num_inject = min(8, max(2, self.dim))\n                injects = []\n                for k in range(num_inject):\n                    if self.rng.rand() < 0.6 and self.x_opt is not None:\n                        # perturb elite/best\n                        scale = 1.0 + 2.0 * self.rng.rand()\n                        inj = self.x_opt + self.rng.randn(self.dim) * (per_dim_scale_abs * scale + 1e-12)\n                    else:\n                        # random uniform\n                        inj = lb + self.rng.rand(self.dim) * span\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    injects.append(inj)\n                # evaluate them (append while respecting budget)\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_inj = safe_eval(inj)\n                    X = np.vstack([X, inj])\n                    F = np.concatenate([F, np.array([f_inj], dtype=float)])\n                    if np.isfinite(f_inj) and f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = inj.copy()\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                stagnation_counter = 0  # reset\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            return float(np.inf), np.asarray(center, dtype=float)\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "f8f08cf6-ff4f-4ec8-8794-8e707a8dc261", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven sampler mixing anisotropic Gaussian local moves, PCA-guided elite moves, DE-like donors, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12,\n                 archive_max=200, gscale_init=0.8, gscale_min=1e-3, gscale_max=8.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n\n        # initial seeding count (including center)\n        self.init_samples = max(2, int(init_samples))\n\n        # archive parameters\n        self.archive_max = int(archive_max)\n\n        # global search scale (relative to robust per-dim spread)\n        self.gscale = float(gscale_init)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # tries several common interfaces; fall back to [-5,5]^dim\n        lb = None\n        ub = None\n\n        # common patterns: func.bounds.lb / func.bounds.ub\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # object with attributes or 2-row array-like\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    b_arr = np.asarray(b, dtype=float)\n                    if b_arr.ndim == 2 and b_arr.shape[0] == 2:\n                        lb = np.asarray(b_arr[0], dtype=float)\n                        ub = np.asarray(b_arr[1], dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # direct attributes .lb / .ub\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                pass\n\n        # fallback to attributes like lower_bounds / upper_bounds\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                pass\n\n        # default\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast scalars\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # if sizes mismatch, pad/truncate\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, -5.0, dtype=float)\n            for i in range(min(self.dim, lb.size)):\n                tmp[i] = lb[i]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, 5.0, dtype=float)\n            for i in range(min(self.dim, ub.size)):\n                tmp[i] = ub[i]\n            ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # reflect out-of-bounds values back into [lb,ub] repeatedly to avoid biases\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp as safety\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            L = np.empty((n, self.dim), dtype=float)\n            for i in range(n):\n                L[i] = lb + self.rng.rand(self.dim) * (ub - lb)\n            return L\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        # safe evaluator that respects budget\n        self.evals = 0\n\n        def safe_eval(x):\n            # returns +inf if budget exhausted (and does not call func)\n            if self.evals >= self.budget:\n                return float(\"inf\")\n            # ensure 1D float array\n            x = np.asarray(x, dtype=float).ravel()\n            # evaluate\n            val = func(x)\n            self.evals += 1\n            return float(val)\n\n        # initialization\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        center = 0.5 * (lb + ub)\n\n        # seeding: stratified per-dim independent (cheap LHS-like)\n        X_list = []\n        # include center\n        X_list.append(center.copy())\n\n        n0 = max(1, min(self.init_samples - 1, max(0, self.budget - 1)))\n        if n0 > 0:\n            # For each dimension create stratified intervals and shuffle\n            # produce n0 samples\n            strata = np.empty((n0, self.dim), dtype=float)\n            for d in range(self.dim):\n                perm = np.arange(n0)\n                self.rng.shuffle(perm)\n                # sample in each stratum\n                strata[:, d] = (perm + self.rng.rand(n0)) / float(n0)\n            # map to bounds\n            for i in range(n0):\n                x = lb + strata[i] * span\n                X_list.append(x)\n\n        # ensure at least one purely uniform random sample\n        X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        X = []\n        F = []\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(x)\n            X.append(np.asarray(x, dtype=float).copy())\n            F.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.asarray(x, dtype=float).copy()\n\n        # If no finite evaluations, set center as fallback (but evals still counted)\n        if self.x_opt is None:\n            self.x_opt = center.copy()\n\n        X = np.asarray(X, dtype=float)\n        F = np.asarray(F, dtype=float)\n\n        # short-term adaptation variables\n        stagnation_counter = 0\n        stagnation_thresh = max(20, int(0.03 * self.budget))\n        # constants for proposals\n        p_local = 0.72\n        p_de = 0.90  # cumulative: local if r<p_local; DE-like if p_local<=r<p_de; else archive/cauchy\n        success_history = []\n        success_window = max(10, min(50, int(0.04 * self.budget)))\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n            if N == 0:\n                # fallback random\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X = np.vstack([X, cand])\n                F = np.concatenate([F, [f]])\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = cand.copy()\n                continue\n\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0]\n\n            # per-dim adaptive scale: robust spread from IQR -> approx std\n            if N >= 4:\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n                iqr = np.maximum(q75 - q25, 1e-12)\n                per_dim_std_approx = iqr / 1.349  # robust approx to std\n                # fallback positive small value relative to span\n                per_dim_std_approx = np.maximum(per_dim_std_approx, 1e-6 * span)\n            else:\n                per_dim_std_approx = np.maximum(0.08 * span, 1e-6 * span)\n\n            per_dim_scale_abs = per_dim_std_approx * self.gscale\n            # choose base index biased to better ranks (softmax on negative F)\n            scores = np.ones(N, dtype=float)\n            if np.any(finite_mask):\n                vmin = float(np.nanmin(F[finite_idx]))\n                vmax = float(np.nanmax(F[finite_idx]))\n                temperature = max(1e-9, (vmax - vmin))\n                vals = F.copy()\n                vals[np.logical_not(finite_mask)] = vmax + 1.0  # penalize non-finite\n                scores = np.exp(-(vals - vmin) / temperature)\n                ssum = np.sum(scores)\n                if ssum <= 0.0 or not np.isfinite(ssum):\n                    probs = np.full(N, 1.0 / N)\n                else:\n                    probs = scores / ssum\n            else:\n                probs = np.full(N, 1.0 / N)\n\n            base_idx = int(self.rng.choice(N, p=probs))\n            base_x = X[base_idx].copy()\n\n            # pick donors for DE-like moves\n            donor1 = None\n            donor2 = None\n            if N >= 3:\n                mask = np.ones(N, dtype=bool)\n                mask[base_idx] = False\n                idxs = np.where(mask)[0]\n                if idxs.size >= 2:\n                    dsel = self.rng.choice(idxs, size=2, replace=False)\n                    donor1 = X[dsel[0]].copy()\n                    donor2 = X[dsel[1]].copy()\n\n            # choose move type\n            r = self.rng.rand()\n            cand = None\n\n            if r < p_local:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                noise = self.rng.randn(self.dim) * (per_dim_scale_abs * (0.6 + 1.0 * self.rng.rand(self.dim)))\n                # slight attraction to archive mean\n                anchor = np.mean(X, axis=0)\n                cand = base_x + 0.6 * (anchor - base_x) * self.rng.rand() + noise\n\n                # Occasionally do PCA-guided elite moves\n                if N >= 6 and self.rng.rand() < 0.22:\n                    elites_k = max(2, int(0.15 * N))\n                    order = np.argsort(F)\n                    elites_idx = order[:elites_k]\n                    elites = X[elites_idx]\n                    # center elites to their mean\n                    mu = np.mean(elites, axis=0)\n                    C = np.cov((elites - mu).T)\n                    # PCA via eig\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        # sort descending\n                        perm = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[perm]\n                        eigvecs = eigvecs[:, perm]\n                        k_use = max(1, min(self.dim, 1 + int(self.rng.poisson(1.5))))\n                        k_use = min(k_use, eigvecs.shape[1])\n                        coeffs = np.zeros(self.dim)\n                        for j in range(k_use):\n                            cidx = j\n                            coeff = (self.rng.randn() * (0.5 + self.rng.rand()))\n                            coeffs += coeff * eigvecs[:, cidx]\n                        # center on a sampled elite\n                        elite_choice = int(elites_idx[self.rng.randint(0, len(elites_idx))])\n                        cand = X[elite_choice].copy() + coeffs * (per_dim_scale_abs * (0.8 + 0.8 * self.rng.rand()))\n                    except Exception:\n                        # fallback keep cand as before\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if donor1 is not None and donor2 is not None and self.rng.rand() < 0.38:\n                    Fscale = 0.5 + 1.1 * self.rng.rand()\n                    diff = (donor1 - donor2) * Fscale\n                    # randomly mix components of differential\n                    mask_comp = self.rng.rand(self.dim) < (0.15 + 0.35 * self.rng.rand())\n                    temp = base_x.copy()\n                    temp[mask_comp] = temp[mask_comp] + diff[mask_comp]\n                    # slight gaussian to avoid exact copies\n                    cand = temp + 0.04 * (per_dim_scale_abs * self.rng.randn(self.dim))\n                # occasionally perform a tempered Cauchy jump after DE-like step\n                if self.rng.rand() < 0.06:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    cauch = np.clip(cauch, -10.0, 10.0)\n                    cand = base_x + 0.5 * (per_dim_scale_abs * (0.6 + 0.8 * self.rng.rand(self.dim))) * cauch\n\n            elif r < p_de:\n                # DE-like global differential move (more exploratory)\n                if donor1 is not None and donor2 is not None:\n                    Fscale = 0.6 + 1.0 * self.rng.rand()\n                    cand = base_x + Fscale * (donor1 - donor2)\n                    # occasionally replace some components with donor1 directly\n                    if self.rng.rand() < 0.28:\n                        replace_mask = self.rng.rand(self.dim) < 0.12\n                        cand[replace_mask] = donor1[replace_mask] + 0.02 * per_dim_scale_abs[replace_mask] * self.rng.randn(np.sum(replace_mask))\n                    # small gaussian jitter\n                    cand = cand + 0.03 * per_dim_scale_abs * self.rng.randn(self.dim)\n                else:\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale_abs\n\n                # occasionally perform tempered Cauchy jump\n                if self.rng.rand() < 0.08:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    cauch = np.clip(cauch, -8.0, 8.0)\n                    cand = base_x + 0.8 * per_dim_scale_abs * cauch\n\n            else:\n                # ARCHIVE_MIX or CAUCHY global jumps\n                if N >= 2 and self.rng.rand() < 0.6:\n                    # pick a donor randomly (prefer not base)\n                    pick_idx = base_idx\n                    if N >= 2:\n                        cand_idx = self.rng.randint(0, N)\n                        if cand_idx == base_idx and N > 1:\n                            cand_idx = (cand_idx + 1) % N\n                        anchor = X[cand_idx].copy()\n                    else:\n                        anchor = base_x.copy()\n                    cand = anchor + 0.3 * per_dim_scale_abs * self.rng.randn(self.dim)\n                    # occasionally inject uniform in a random dimension to diversify\n                    if self.rng.rand() < 0.22:\n                        replace_dim = self.rng.randint(0, self.dim)\n                        cand[replace_dim] = lb[replace_dim] + self.rng.rand() * span[replace_dim]\n                else:\n                    # tempered Cauchy global jump from a random archive point or base\n                    if N > 0:\n                        anchor = X[self.rng.randint(0, N)].copy()\n                    else:\n                        anchor = base_x.copy()\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    cauch = np.clip(cauch, -12.0, 12.0)\n                    scale_factors = per_dim_scale_abs * (0.4 + 0.6 * self.rng.rand(self.dim))\n                    cand = anchor + cauch * scale_factors\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            # reflect/clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            success = 0\n            if np.isfinite(f):\n                success = 1\n                finite_vals = F[np.isfinite(F)]\n                if finite_vals.size > 0:\n                    rank = np.sum(finite_vals <= f)\n                    if rank > 0 and rank <= max(1, int(0.25 * finite_vals.size)):\n                        success = 1\n                    else:\n                        success = 0\n            success_history.append(success)\n            if len(success_history) > success_window:\n                success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                order = np.argsort(F)\n                # keep best 60% and some random rest\n                keep_best = int(0.6 * self.archive_max)\n                keep_idx = list(order[:keep_best])\n                remaining = list(order[keep_best:])\n                num_random = self.archive_max - len(keep_idx)\n                if len(remaining) > 0 and num_random > 0:\n                    rand_keep = list(self.rng.choice(remaining, size=min(len(remaining), num_random), replace=False))\n                    keep_idx += rand_keep\n                # pad by best remaining if still short\n                if len(keep_idx) < self.archive_max:\n                    extra = list(order[keep_best:keep_best + (self.archive_max - len(keep_idx))])\n                    keep_idx += extra\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) >= success_window:\n                succ_rate = float(sum(success_history)) / float(len(success_history))\n                if succ_rate > 0.45:\n                    # many successes -> tighten search (exploit)\n                    self.gscale = max(self.gscale_min, self.gscale * (0.85 - 0.05 * self.rng.rand()))\n                elif succ_rate < 0.15:\n                    # few successes -> enlarge to explore\n                    self.gscale = min(self.gscale_max, self.gscale * (1.18 + 0.08 * self.rng.rand()))\n                else:\n                    # small jitter\n                    self.gscale = min(self.gscale_max, max(self.gscale_min, self.gscale * (0.98 + 0.04 * (self.rng.rand() - 0.5))))\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter >= stagnation_thresh:\n                # create some injected candidates (prefer perturbations of best)\n                num_inject = min(6, max(2, int(0.02 * self.budget)))\n                injects = []\n                for k in range(num_inject):\n                    if self.x_opt is not None and self.rng.rand() < 0.7:\n                        # perturb elite/best\n                        scale = 1.0 + 2.0 * self.rng.rand()\n                        inj = self.x_opt + scale * (per_dim_scale_abs * (0.6 + 1.4 * self.rng.rand(self.dim))) * self.rng.randn(self.dim)\n                    else:\n                        # random uniform\n                        inj = lb + self.rng.rand(self.dim) * span\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    injects.append(inj)\n                # evaluate them (append while respecting budget)\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_inj = safe_eval(inj)\n                    X = np.vstack([X, inj])\n                    F = np.concatenate([F, [f_inj]])\n                    if np.isfinite(f_inj) and f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = inj.copy()\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale * 1.4, self.gscale_max)\n                stagnation_counter = 0  # reset\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            self.x_opt = center.copy()\n            self.f_opt = safe_eval(self.x_opt) if self.evals < self.budget else float(\"inf\")\n\n        return self.f_opt, np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5a3f72ba-2fcb-46d5-a176-98858e27bdbe", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven sampler mixing anisotropic Gaussian local moves, PCA-guided elite moves, DE-like donors and tempered Cauchy global jumps with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None,\n                 archive_max=None, gscale_init=0.3, gscale_max=10.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n\n        # initial population size\n        if init_samples is None:\n            self.init_samples = max(12, 2 * self.dim)\n        else:\n            self.init_samples = int(init_samples)\n\n        # archive max size\n        if archive_max is None:\n            self.archive_max = max(200, 20 * self.dim)\n        else:\n            self.archive_max = int(archive_max)\n\n        # global search scale (relative to robust per-dim spread)\n        self.gscale = float(gscale_init)\n        self.gscale_max = float(gscale_max)\n\n        # bookkeeping for best found\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # tries several common interfaces; fall back to [-5,5]^dim\n        lb = None\n        ub = None\n\n        # common: func.bounds with attributes lb/ub or first/second rows\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # named attributes\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe 2xN array-like: [lb, ub] or [[lb],[ub]]\n                    b_arr = np.asarray(b)\n                    if b_arr.ndim == 1 and b_arr.size == 2:\n                        # ambiguous; assume scalar bounds\n                        lb = np.full(self.dim, float(b_arr[0]))\n                        ub = np.full(self.dim, float(b_arr[1]))\n                    elif b_arr.ndim == 2 and b_arr.shape[0] == 2:\n                        lb = np.asarray(b_arr[0], dtype=float)\n                        ub = np.asarray(b_arr[1], dtype=float)\n        except Exception:\n            pass\n\n        # direct attributes on func\n        try:\n            if lb is None and hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n        except Exception:\n            pass\n\n        # fallback names\n        try:\n            if lb is None and hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                lb = np.asarray(func.lower_bounds, dtype=float)\n                ub = np.asarray(func.upper_bounds, dtype=float)\n        except Exception:\n            pass\n\n        # final default\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast scalars or length-1 arrays\n        lb = np.asarray(lb, dtype=float).ravel()\n        ub = np.asarray(ub, dtype=float).ravel()\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb[0]), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub[0]), dtype=float)\n\n        # if sizes mismatch pad/truncate\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, -5.0, dtype=float)\n            for i in range(min(self.dim, lb.size)):\n                tmp[i] = lb[i]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, 5.0, dtype=float)\n            for i in range(min(self.dim, ub.size)):\n                tmp[i] = ub[i]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # reflect out-of-bounds values back into [lb,ub] repeatedly to avoid biases\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            # reflect low: x = lb + (lb - x)\n            x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            # reflect high: x = ub - (x - ub)\n            x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp as safety\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        if n is None:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            lb = np.asarray(lb)\n            ub = np.asarray(ub)\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        # safe evaluator that respects budget\n        self.evals = 0\n\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return float(\"inf\")\n            x = np.asarray(x, dtype=float).ravel()\n            # ensure dimension\n            if x.size != self.dim:\n                x = np.resize(x, self.dim)\n            self.evals += 1\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(\"inf\")\n            return f\n\n        # initialization: stratified per-dim independent (cheap LHS-like)\n        n0 = min(self.init_samples, max(3, self.budget))\n        center = 0.5 * (lb + ub)\n        X_list = []\n        # include center once\n        X_list.append(center.copy())\n        # stratified LHS-like across dims\n        strata = np.zeros((n0, self.dim), dtype=float)\n        for d in range(self.dim):\n            perm = np.arange(n0)\n            self.rng.shuffle(perm)\n            strata[:, d] = (perm + self.rng.rand(n0)) / float(n0)\n        for i in range(n0):\n            x = lb + strata[i] * (ub - lb)\n            X_list.append(x)\n        # ensure at least one uniform sample\n        X_list.append(self._uniform_array(lb, ub))\n        # Evaluate initial population (respecting budget)\n        X = []\n        F = []\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(x)\n            X.append(np.asarray(x, dtype=float).copy())\n            F.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.asarray(x, dtype=float).copy()\n        if len(X) == 0:\n            # nothing evaluated (budget 0), return fallback\n            return self.f_opt, self.x_opt\n        X = np.asarray(X, dtype=float)\n        F = np.asarray(F, dtype=float)\n\n        # short-term adaptation variables\n        stagnation_counter = 0\n        success_history = []\n        success_window = 20\n        p_local = 0.45\n        p_pca = 0.12\n        p_de = 0.18\n        p_global = 0.25  # includes cauchy\n        # ensure probabilities sum to 1\n        psum = p_local + p_pca + p_de + p_global\n        p_local /= psum; p_pca /= psum; p_de /= psum; p_global /= psum\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n            # compute robust per-dim spread -> IQR approx to std\n            try:\n                q25 = np.nanpercentile(X, 25, axis=0)\n                q75 = np.nanpercentile(X, 75, axis=0)\n                per_dim_std_approx = (q75 - q25) / 1.349  # approx std from IQR\n            except Exception:\n                per_dim_std_approx = np.std(X, axis=0)\n            per_dim_std_approx = np.maximum(per_dim_std_approx, 1e-8 * span)\n            # absolute scale factors per-dimension\n            per_dim_scale_abs = self.gscale * per_dim_std_approx + 1e-12 * span\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                vals = F.copy()\n                vals[~finite_mask] = np.nanmax(F[finite_mask]) + 1.0\n                # invert and shift\n                inv = np.nanmax(vals) - vals\n                # softmax\n                ex = np.exp(inv - np.nanmax(inv))\n                probs = ex / np.sum(ex)\n            else:\n                probs = np.full(N, 1.0 / N)\n            base_idx = self.rng.choice(N, p=probs)\n            base_x = X[base_idx].copy()\n\n            # choose anchor: best or random elite occasionally\n            elite_count = max(2, int(max(2, 0.12 * N)))\n            order = np.argsort(F)\n            elites_idx = order[:elite_count] if N >= elite_count else order\n            if self.rng.rand() < 0.7 and len(elites_idx) > 0:\n                anchor_idx = self.rng.choice(elites_idx)\n            else:\n                anchor_idx = self.rng.randint(0, N)\n            anchor = X[anchor_idx].copy()\n\n            # pick donors for DE-like moves\n            donor1 = X[self.rng.randint(0, N)].copy()\n            donor2 = X[self.rng.randint(0, N)].copy()\n            donor3 = X[self.rng.randint(0, N)].copy()\n            # ensure different indices if possible\n            # (we don't strictly enforce uniqueness to avoid errors when N small)\n\n            # choose move type\n            r = self.rng.rand()\n            cand = None\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < p_local:\n                noise = self.rng.randn(self.dim) * (per_dim_scale_abs * (0.5 + 1.0 * self.rng.rand(self.dim)))\n                # slight attraction to anchor (best/elite)\n                cand = base_x + 0.6 * (anchor - base_x) * self.rng.rand() + noise\n                # small chance to mix differential inside\n                if self.rng.rand() < 0.22:\n                    Fscale = 0.4 + 1.0 * self.rng.rand()\n                    diff = (donor1 - donor2) * Fscale\n                    mask_comp = self.rng.rand(self.dim) < (0.12 + 0.4 * self.rng.rand())\n                    cand[mask_comp] = cand[mask_comp] + diff[mask_comp]\n                # occasional slight PCA perturb around base_x using local sample of elites\n                if self.rng.rand() < 0.15 and len(elites_idx) >= 2:\n                    elites = X[elites_idx]\n                    mu = np.mean(elites, axis=0)\n                    C = np.cov((elites - mu).T) if elites.shape[0] > 1 else np.eye(self.dim)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C + 1e-12 * np.eye(self.dim))\n                        perm = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[perm]\n                        eigvecs = eigvecs[:, perm]\n                        # sample along top components\n                        k_use = min(self.dim, max(1, int(1 + self.rng.rand() * min(5, self.dim))))\n                        coeffs = (self.rng.randn(k_use) * np.sqrt(np.maximum(0.0, eigvals[:k_use])))\n                        cand = cand + eigvecs[:, :k_use].dot(coeffs) * (0.6 * self.gscale)\n                    except Exception:\n                        pass\n\n            elif r < p_local + p_pca:\n                # PCA-guided elite move (exploit along principal directions of elites)\n                if len(elites_idx) >= 2:\n                    elites = X[elites_idx]\n                    mu = np.mean(elites, axis=0)\n                    C = np.cov((elites - mu).T) + 1e-12 * np.eye(self.dim)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        perm = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[perm]\n                        eigvecs = eigvecs[:, perm]\n                        k_use = min(self.dim, max(1, int(1 + np.floor(self.rng.rand() * min(6, self.dim)))))\n                        coeffs = (self.rng.randn(k_use) * np.sqrt(np.maximum(0.0, eigvals[:k_use])))\n                        elite_choice = self.rng.choice(elites_idx)\n                        cand = X[elite_choice].copy() + eigvecs[:, :k_use].dot(coeffs) * (0.8 * self.gscale)\n                    except Exception:\n                        cand = base_x + self.rng.randn(self.dim) * per_dim_scale_abs\n                else:\n                    # fallback to local jitter\n                    cand = base_x + self.rng.randn(self.dim) * per_dim_scale_abs\n\n            elif r < p_local + p_pca + p_de:\n                # DE-like donor mixing (differential evolution inspired)\n                Fscale = 0.5 + 1.1 * self.rng.rand()\n                diff = (donor1 - donor2) * Fscale\n                cand = base_x + diff\n                # random component mixing\n                mask_comp = self.rng.rand(self.dim) < (0.12 + 0.45 * self.rng.rand())\n                if np.any(mask_comp):\n                    cand[mask_comp] = donor3[mask_comp]\n                # small jitter to avoid duplicates\n                cand = cand + 0.08 * (per_dim_scale_abs * self.rng.randn(self.dim))\n                # occasional tempered Cauchy injection\n                if self.rng.rand() < 0.18:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    scale_factors = per_dim_scale_abs * (0.8 + 1.2 * self.rng.rand(self.dim))\n                    cand = cand + 0.6 * cauch * scale_factors\n\n            else:\n                # Global archive-based / Cauchy jumps\n                # pick a donor randomly (prefer not base)\n                cand_idx = self.rng.randint(0, N)\n                if cand_idx == base_idx:\n                    cand_idx = (cand_idx + 1) % N\n                anchor2 = X[cand_idx].copy()\n                # tempered Cauchy to escape basins\n                cauch = self.rng.standard_cauchy(self.dim)\n                scale_factors = per_dim_scale_abs * (0.9 + 2.5 * self.rng.rand(self.dim))\n                cand = anchor2 + 0.9 * cauch * scale_factors\n                # occasionally inject uniform in a random dimension to diversify\n                if self.rng.rand() < 0.22:\n                    d = self.rng.randint(0, self.dim)\n                    cand[d] = lb[d] + self.rng.rand() * (ub[d] - lb[d])\n                # small mix with center to prevent drifting outside region\n                if self.rng.rand() < 0.14:\n                    cand = 0.7 * cand + 0.3 * center\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            if cand is None or not np.all(np.isfinite(cand)):\n                cand = self._uniform_array(lb, ub)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n            # Append candidate to archive\n            if np.isfinite(f):\n                X = np.vstack([X, cand])\n                F = np.concatenate([F, [float(f)]])\n            else:\n                # we still keep candidate for diversity (but mark as inf)\n                X = np.vstack([X, cand])\n                F = np.concatenate([F, [float(f)]])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.x_opt = np.asarray(cand, dtype=float).copy()\n                self.f_opt = float(f)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            finite_vals = F[np.isfinite(F)]\n            success = 0\n            if np.isfinite(f):\n                if f <= self.f_opt:\n                    success = 1\n                elif finite_vals.size > 0:\n                    rank = np.sum(finite_vals <= f)\n                    if rank <= max(1, int(0.25 * finite_vals.size)):\n                        success = 1\n            success_history.append(success)\n            if len(success_history) > 200:\n                success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            Ncurr = X.shape[0]\n            if Ncurr > self.archive_max:\n                order = np.argsort(F)\n                keep_best = int(np.ceil(0.6 * self.archive_max))\n                keep_idx = list(order[:keep_best])\n                remaining = list(order[keep_best:])\n                # keep some random from remaining to maintain diversity\n                num_random = max(0, self.archive_max - len(keep_idx))\n                if len(remaining) > 0 and num_random > 0:\n                    rand_keep = list(self.rng.choice(remaining, size=min(len(remaining), num_random), replace=False))\n                    keep_idx += rand_keep\n                # if still short pad with best remaining\n                if len(keep_idx) < self.archive_max:\n                    extra = list(order[keep_best:keep_best + (self.archive_max - len(keep_idx))])\n                    keep_idx += extra\n                keep_idx = np.unique(keep_idx).tolist()\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) >= success_window:\n                succ_rate = float(np.mean(success_history[-success_window:]))\n                # if many successes -> tighten search (exploit), else enlarge to explore\n                if succ_rate > 0.45:\n                    self.gscale = max(1e-6, self.gscale * (0.88 - 0.08 * self.rng.rand()))\n                elif succ_rate < 0.12:\n                    self.gscale = min(self.gscale_max, self.gscale * (1.18 + 0.12 * self.rng.rand()))\n                else:\n                    # small jitter\n                    self.gscale = np.clip(self.gscale * (0.95 + 0.1 * self.rng.rand()), 1e-8, self.gscale_max)\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter > max(80, 10 * self.dim):\n                # micro-restart: inject perturbations of best and some uniforms\n                injects = []\n                if self.x_opt is not None:\n                    for k in range(min(6, max(1, int(self.dim / 2)))):\n                        inj = self.x_opt + (self.rng.randn(self.dim) * (per_dim_scale_abs * (0.8 + 0.6 * self.rng.rand(self.dim))))\n                        inj = self._reflect_bounds(inj, lb, ub)\n                        injects.append(inj)\n                # some uniform injections\n                for k in range(4):\n                    injects.append(self._uniform_array(lb, ub))\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_inj = safe_eval(inj)\n                    X = np.vstack([X, inj])\n                    F = np.concatenate([F, [float(f_inj)]])\n                    if np.isfinite(f_inj) and f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = np.asarray(inj, dtype=float).copy()\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale_max, self.gscale * (1.6 + 0.8 * self.rng.rand()))\n                stagnation_counter = 0\n                success_history = []\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            return float(\"inf\"), center\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "c99440fa-ea79-4009-8792-bd1e1250ec83", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler combining anisotropic Gaussian local moves, PCA-guided elite moves, DE-like donors, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_samples=None, archive_max=500,\n                 gscale_init=0.25, gscale_max=8.0, gscale_min=1e-6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.default_rng(self.seed)\n\n        if init_samples is None:\n            # a small initial population: scale with dim but limited by budget\n            self.init_samples = max(6, min(4 * self.dim, self.budget // 5))\n        else:\n            self.init_samples = int(init_samples)\n\n        self.archive_max = int(max(20, archive_max))\n        self.gscale = float(gscale_init)\n        self.gscale_max = float(gscale_max)\n        self.gscale_min = float(gscale_min)\n\n        # runtime trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try several common interfaces for lower/upper bounds\n        lb = None\n        ub = None\n        # 1) func.bounds.lb / func.bounds.ub (often in BBOB wrappers)\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe two-row array-like\n                    b_arr = np.asarray(b, dtype=float)\n                    if b_arr.ndim == 2 and b_arr.shape[0] == 2:\n                        lb = b_arr[0].astype(float)\n                        ub = b_arr[1].astype(float)\n        except Exception:\n            pass\n\n        # 2) attributes lower_bounds / upper_bounds\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                pass\n\n        # 3) single numeric bounds attributes\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                pass\n\n        # fallback: [-5,5]^dim per problem statement\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast scalars to vectors\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # if sizes mismatch, pad/truncate\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, lb.flatten()[0] if lb.size > 0 else -5.0, dtype=float)\n            for i in range(min(self.dim, lb.size)):\n                tmp[i] = lb.flatten()[i]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, ub.flatten()[0] if ub.size > 0 else 5.0, dtype=float)\n            for i in range(min(self.dim, ub.size)):\n                tmp[i] = ub.flatten()[i]\n            ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # reflect out-of-bounds values back into [lb,ub] repeatedly to avoid biases\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            # reflect: overflow distance beyond boundary\n            x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp as safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            arr = lb + self.rng.random((n, self.dim)) * (ub - lb)\n            return arr\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # returns +inf if budget exhausted (and does not call func)\n            if self.evals >= self.budget:\n                return float(np.inf)\n            x = np.asarray(x, dtype=float)\n            # ensure correct shape (1D)\n            if x.ndim != 1:\n                x = x.ravel()\n            # call func\n            try:\n                val = float(func(x))\n            except Exception:\n                # in case function crashes, penalize\n                val = float(np.inf)\n            self.evals += 1\n            return val\n\n        # ---------- Initialization / seeding ----------\n        X = []\n        F = []\n\n        # include center\n        center = 0.5 * (lb + ub)\n        n0 = min(self.init_samples, max(1, self.budget - self.evals))\n        # stratified per-dim independent sampling: simple LHS-like\n        # create for each dim n0 strata and pick shuffled strata indices to form n0 samples\n        strata = np.empty((n0, self.dim), dtype=float)\n        for d in range(self.dim):\n            perm = np.arange(n0)\n            self.rng.shuffle(perm)\n            # sample within stratum\n            u = (perm + self.rng.random(n0)) / float(n0)\n            strata[:, d] = lb[d] + u * (ub[d] - lb[d])\n        # take up to n0-1 of them and include center to ensure centralization\n        if n0 >= 1:\n            seeds = strata[: max(1, n0 - 1), :]\n        else:\n            seeds = np.empty((0, self.dim))\n\n        # ensure at least one pure uniform sample\n        uni = self._uniform_array(lb, ub)\n        # assemble initial set\n        initial = []\n        initial.append(center.copy())\n        for r in seeds:\n            initial.append(r.copy())\n        initial.append(uni)\n        # truncate to budget\n        initial = initial[: max(1, min(len(initial), self.budget - self.evals))]\n\n        # Evaluate initial population (respecting budget)\n        for x0 in initial:\n            f0 = safe_eval(x0)\n            if np.isfinite(f0):\n                if f0 < self.f_opt:\n                    self.f_opt = f0\n                    self.x_opt = np.asarray(x0, dtype=float).copy()\n            X.append(np.asarray(x0, dtype=float).copy())\n            F.append(float(f0))\n\n        # If no finite evaluations, set center as fallback (but evals still counted)\n        if self.x_opt is None and len(X) > 0:\n            # choose best finite if exists, else center\n            finite_idx = [i for i, fv in enumerate(F) if np.isfinite(fv)]\n            if finite_idx:\n                best_i = min(finite_idx, key=lambda i: F[i])\n                self.f_opt = F[best_i]\n                self.x_opt = X[best_i].copy()\n            else:\n                # force evaluate center if budget allows\n                if self.evals < self.budget:\n                    fc = safe_eval(center)\n                    X.append(center.copy())\n                    F.append(float(fc))\n                    if np.isfinite(fc):\n                        self.f_opt = fc\n                        self.x_opt = center.copy()\n                else:\n                    # nothing to do\n                    pass\n\n        X = np.asarray(X, dtype=float)\n        F = np.asarray(F, dtype=float)\n\n        # short-term adaptation variables\n        success_window = max(10, min(200, int(0.04 * max(100, self.budget))))\n        success_history = deque(maxlen=success_window)\n        stagnation_counter = 0\n        stagnation_threshold = max(50, int(0.08 * max(100, self.budget)))\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n            finite_idx = np.where(np.isfinite(F))[0]\n            if finite_idx.size == 0:\n                # sample random uniform to try to get a finite point\n                cand = self._uniform_array(lb, ub)\n                cand = self._reflect_bounds(cand, lb, ub)\n                f = safe_eval(cand)\n                X = np.vstack([X, cand])\n                F = np.append(F, f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = cand.copy()\n                continue\n\n            # per-dim adaptive scale: robust spread from IQR -> approx std\n            per_dim_scale = np.maximum(\n                (np.nanpercentile(X[finite_idx], 75, axis=0) - np.nanpercentile(X[finite_idx], 25, axis=0)) / 1.349,\n                1e-8 * span\n            )\n            # base absolute scale: combine per-dim spread with global gscale\n            per_dim_scale_abs = np.maximum(per_dim_scale, 1e-12) * (self.gscale * (0.5 + 1.5 * self.rng.random(self.dim)))\n\n            # choose base index biased to better ranks (softmax on negative F)\n            fmin = float(np.nanmin(F[finite_idx]))\n            fmax = float(np.nanmax(F[finite_idx]))\n            # temperature adapted to spread\n            temp = max(1e-12, (fmax - fmin) / max(1.0, abs(fmin)))\n            beta = 3.0 / max(1e-12, temp)\n            scores = np.zeros(N, dtype=float)\n            scores[:] = -F  # better have higher score\n            # mask non-finite as very bad\n            scores[~np.isfinite(scores)] = np.min(scores[np.isfinite(scores)]) - 100.0 if np.any(np.isfinite(scores)) else -1e6\n            # softmax probabilities\n            prob = np.exp(beta * (scores - scores.max()))\n            prob = prob / prob.sum()\n            base_idx = int(self.rng.choice(np.arange(N), p=prob))\n            base_x = X[base_idx].copy()\n\n            # pick donors for DE-like moves (avoid base)\n            donor1 = None\n            donor2 = None\n            if N >= 3:\n                idxs = list(range(N))\n                idxs.remove(base_idx)\n                donor_idxs = self.rng.choice(idxs, size=2, replace=False)\n                donor1 = X[donor_idxs[0]].copy()\n                donor2 = X[donor_idxs[1]].copy()\n\n            # decide move type\n            r = self.rng.random()\n            cand = None\n\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < 0.45:\n                # slight attraction to archive mean depending on rank of base\n                archive_mean = np.nanmean(X[finite_idx], axis=0)\n                pull = (archive_mean - base_x) * (0.02 + 0.3 * (1.0 - prob[base_idx]))\n                anisotropy = 0.6 + 0.8 * self.rng.random(self.dim)\n                noise = self.rng.normal(0.0, per_dim_scale_abs * anisotropy)\n                cand = base_x + 0.8 * noise + pull\n\n                # sometimes mix in a DE donor fragment for diversity\n                if donor1 is not None and donor2 is not None and self.rng.random() < 0.38:\n                    mask = self.rng.random(self.dim) < 0.35\n                    diff = 0.7 * (donor1 - donor2)\n                    cand[mask] = base_x[mask] + diff[mask] + 0.05 * self.rng.normal(size=mask.sum()) * per_dim_scale_abs[mask]\n\n                # occasionally tempered Cauchy after local\n                if self.rng.random() < 0.07:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    cauch = np.clip(cauch, -10.0, 10.0)\n                    cand = base_x + 0.5 * per_dim_scale_abs * (0.5 + self.rng.random(self.dim)) * cauch\n\n            # DE-like donor mixing\n            elif r < 0.75 and donor1 is not None and donor2 is not None:\n                F_scale = 0.6 + 0.8 * self.rng.random()\n                cand = base_x.copy()\n                # binomial crossover style mixing\n                mask = self.rng.random(self.dim) < (0.2 + 0.5 * self.rng.random())\n                diff = F_scale * (donor1 - donor2)\n                cand[mask] = base_x[mask] + diff[mask]\n                # jitter\n                cand += 0.05 * self.rng.normal(0.0, per_dim_scale_abs)\n                # occasional Cauchy perturb\n                if self.rng.random() < 0.12:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    cauch = np.clip(cauch, -12.0, 12.0)\n                    cand = base_x + 0.8 * per_dim_scale_abs * cauch\n\n            # PCA-guided elite moves\n            elif r < 0.9:\n                # pick some elites (top 10-30%)\n                num_elites = max(2, min(N, int(max(2, 0.12 * N + self.rng.poisson(1.5)))))\n                order = np.argsort(F)\n                elites_idx = order[:num_elites]\n                elites = X[elites_idx]\n                # compute covariance of elites (centered)\n                mu = elites.mean(axis=0)\n                C = np.cov((elites - mu).T) if elites.shape[0] > 1 else np.diag(per_dim_scale_abs ** 2)\n                # PCA via eig\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(C + 1e-12 * np.eye(self.dim))\n                except Exception:\n                    eigvals = np.maximum(np.var(elites, axis=0), 1e-12)\n                    eigvecs = np.eye(self.dim)\n                # sort descending\n                perm = np.argsort(-eigvals)\n                eigvals = eigvals[perm]\n                eigvecs = eigvecs[:, perm]\n                # sample in PCA space: choose number of leading components\n                k_use = max(1, min(self.dim, 1 + int(self.rng.poisson(1.5))))\n                coeffs = self.rng.normal(0.0, 1.0, size=k_use)\n                # scale by sqrt eigenvals and global scale\n                step = eigvecs[:, :k_use] @ (np.sqrt(np.maximum(eigvals[:k_use], 1e-12)) * coeffs)\n                # center on a sampled elite\n                elite_choice = int(self.rng.choice(elites_idx))\n                cand = X[elite_choice].copy() + (0.6 + 0.9 * self.rng.random()) * step * (self.gscale * (0.6 + 0.8 * self.rng.random()))\n\n                # small isotropic jitter\n                cand += 0.03 * self.rng.normal(0.0, per_dim_scale_abs)\n\n            # ARCHIVE_MIX or CAUCHY global jumps\n            else:\n                # pick anchor from archive (prefer not the base)\n                if N > 1:\n                    idx_pick = self.rng.integers(0, N)\n                    if idx_pick == base_idx:\n                        idx_pick = (idx_pick + 1) % N\n                    anchor = X[idx_pick].copy()\n                else:\n                    anchor = base_x.copy()\n                # tempered Cauchy global jump from anchor toward base\n                cauch = self.rng.standard_cauchy(self.dim)\n                cauch = np.clip(cauch, -15.0, 15.0)\n                scale = 0.8 + 1.5 * self.rng.random()\n                cand = anchor + scale * per_dim_scale_abs * cauch\n                # occasionally inject uniform in one dimension\n                if self.rng.random() < 0.15:\n                    d = self.rng.integers(0, self.dim)\n                    cand[d] = lb[d] + self.rng.random() * (ub[d] - lb[d])\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.asarray(cand, dtype=float)\n            # replace nan/inf entries\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.count_nonzero(bad)) * (ub[bad] - lb[bad])\n            # reflect/clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.append(F, f)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = cand.copy()\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            if np.isfinite(f):\n                finite_vals = F[np.isfinite(F)]\n                if finite_vals.size > 0:\n                    cutoff = np.percentile(finite_vals, 25.0)\n                    success = 1 if (f <= cutoff or improved) else 0\n                else:\n                    success = 0\n            else:\n                success = 0\n            success_history.append(success)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if X.shape[0] > self.archive_max:\n                order = np.argsort(F)\n                keep_best = int(max(2, np.floor(0.6 * self.archive_max)))\n                keep_idx = list(order[:keep_best])\n                # keep some random others\n                remaining = list(order[keep_best:])\n                self.rng.shuffle(remaining)\n                rand_keep = remaining[:max(0, self.archive_max - len(keep_idx))]\n                keep_idx += rand_keep\n                keep_idx = sorted(set(keep_idx))\n                X = X[keep_idx, :]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_history) >= success_history.maxlen:\n                s_rate = float(np.mean(success_history))\n                # many successes -> tighten search (exploit)\n                if s_rate > 0.25:\n                    self.gscale *= (0.95 - 0.04 * self.rng.random())\n                # few successes -> enlarge to explore\n                elif s_rate < 0.08:\n                    self.gscale *= (1.03 + 0.1 * self.rng.random())\n                else:\n                    # small jitter\n                    self.gscale *= (0.99 + 0.02 * (self.rng.random() - 0.5))\n                # clamp\n                self.gscale = min(self.gscale_max, max(self.gscale_min, self.gscale))\n\n            # stagnation detection & micro-restarts\n            if stagnation_counter >= stagnation_threshold:\n                # perform micro-restart: inject a few perturbations of best and some uniforms\n                num_inject = min( max(3, int(0.02 * self.budget)), max(3, self.dim))\n                injects = []\n                for k in range(num_inject):\n                    if self.x_opt is not None and self.rng.random() < 0.7:\n                        # perturb best\n                        jitter = self.rng.normal(0.0, per_dim_scale_abs * (1.0 + 2.0 * self.rng.random()))\n                        inj = self.x_opt + (0.5 + 0.5 * self.rng.random()) * jitter\n                    else:\n                        inj = self._uniform_array(lb, ub)\n                    inj = self._reflect_bounds(inj, lb, ub)\n                    injects.append(inj)\n                # evaluate them while respecting budget\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_inj = safe_eval(inj)\n                    X = np.vstack([X, inj])\n                    F = np.append(F, f_inj)\n                    if np.isfinite(f_inj) and f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = inj.copy()\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale = min(self.gscale_max, self.gscale * (1.0 + 0.15 * self.rng.random()))\n                stagnation_counter = 0\n                success_history.clear()\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            return float(np.inf), 0.5 * (lb + ub)\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float).copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 219, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_history = deque(maxlen=success_window)", "error": "In the code, line 219, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_history = deque(maxlen=success_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "06e3366c-a67c-4d2d-9d38-a59687ce44a3", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(3, int(init_samples))  # at least a few\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n        # trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # common patterns: func.bounds.lb / func.bounds.ub or func.lower_bounds / func.upper_bounds\n        lb = None\n        ub = None\n\n        # try func.bounds.lb / func.bounds.ub\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # support objects with .lb/.ub or 2xN arrays\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    arr = np.asarray(b, dtype=float)\n                    if arr.ndim == 2 and arr.shape[0] == 2:\n                        lb = arr[0].astype(float)\n                        ub = arr[1].astype(float)\n        except Exception:\n            lb = None\n\n        # try lower_bounds / upper_bounds\n        if lb is None or ub is None:\n            if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                try:\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n                except Exception:\n                    lb = None\n        # try simple attributes\n        if lb is None or ub is None:\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    lb = None\n\n        # fallback to [-5,5]^dim\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Broadcast scalars to arrays if needed\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # If mismatch, pad/truncate\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, -5.0, dtype=float)\n            tmp[:min(len(lb), self.dim)] = lb.flat[:min(len(lb), self.dim)]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, 5.0, dtype=float)\n            tmp[:min(len(ub), self.dim)] = ub.flat[:min(len(ub), self.dim)]\n            ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = np.asarray(x, dtype=float).copy()\n        # iterative reflection\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            # reflect low: new = lb + (lb - x) = 2*lb - x\n            if np.any(low_mask):\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if np.any(high_mask):\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return self.rng.uniform(lb, ub)\n        else:\n            arr = self.rng.random((n, self.dim))\n            return lb + arr * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # returns +inf if budget exhausted (and does not call func)\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            # ensure 1D array\n            if x.ndim > 1:\n                x = x.ravel()[: self.dim]\n            try:\n                f = func(x)\n            except Exception:\n                # any error -> count eval but return inf\n                self.evals += 1\n                return np.inf\n            # count eval\n            self.evals += 1\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            return f\n\n        # initialization\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # seeding: stratified per-dim independent (cheap LHS-like)\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        n0 = max(2, n0)\n        X = []\n        F = []\n\n        # include center\n        center = 0.5 * (lb + ub)\n        X.append(center.copy())\n\n        # stratified per-dim independent\n        if n0 - 1 > 0:\n            # For each dimension create stratified intervals and shuffle\n            samples = np.empty((n0 - 1, self.dim), dtype=float)\n            for d in range(self.dim):\n                perm = np.arange(n0 - 1)\n                self.rng.shuffle(perm)\n                # sample one in each stratum\n                strata = (perm + self.rng.random(n0 - 1)) / (n0 - 1)\n                samples[:, d] = lb[d] + strata * (ub[d] - lb[d])\n            for i in range(n0 - 1):\n                X.append(samples[i].copy())\n\n        # ensure at least one purely uniform random sample\n        X.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        for x in X:\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(x)\n            F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        X = np.array(X, dtype=float)\n        F = np.array(F, dtype=float)\n\n        # If no finite evaluations, set center as fallback (but evals still counted)\n        finite_mask = np.isfinite(F)\n        if not np.any(finite_mask):\n            # try evaluating center explicitly (if not already)\n            if self.evals < self.budget:\n                f = safe_eval(center)\n                if np.isfinite(f):\n                    self.f_opt = float(f)\n                    self.x_opt = center.copy()\n                    X = np.vstack([X, center.copy()])\n                    F = np.concatenate([F, np.array([f], dtype=float)])\n\n        # short-term adaptation variables\n        gscale = 0.8  # global short-term scale multiplier (multiplies robust per-dim spread)\n        p_local = 0.72\n        adapt_window = 50\n        success_window = []\n        successes = 0\n        iter_since_improve = 0\n        stagnation_limit = max(200, int(5 * self.dim))\n        max_iters = max(1, self.budget - self.evals)\n\n        # main loop\n        while self.evals < self.budget:\n            N = len(X)\n            if N == 0:\n                # fallback random\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X = np.array([cand])\n                F = np.array([f])\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = cand.copy()\n                continue\n\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0]\n            if finite_idx.size == 0:\n                # make a uniform candidate\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X = np.vstack([X, cand])\n                F = np.concatenate([F, np.array([f], dtype=float)])\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = cand.copy()\n                continue\n\n            # per-dim adaptive scale: robust spread from IQR -> approx std\n            try:\n                q75 = np.percentile(X[finite_idx], 75, axis=0)\n                q25 = np.percentile(X[finite_idx], 25, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approx std from IQR\n                per_dim_scale[np.isnan(per_dim_scale)] = 0.0\n            except Exception:\n                per_dim_scale = np.std(X[finite_idx], axis=0)\n            # fallback small positive\n            mask_small = per_dim_scale <= 1e-12\n            if np.any(mask_small):\n                per_dim_scale[mask_small] = span[mask_small] * 0.05 + 1e-6\n\n            per_dim_scale_abs = per_dim_scale * gscale + 1e-12\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_F = F[finite_idx]\n            # convert to ranks (1 best)\n            order = np.argsort(finite_F)\n            ranks = np.empty_like(order)\n            ranks[order] = np.arange(len(order))\n            # weights: exponential on rank\n            beta = 0.6 + 0.4 * (self.dim / max(10, self.dim))\n            w = np.exp(-beta * (ranks / max(1, len(ranks) - 1)))\n            if np.any(w < 0) or np.all(w == 0):\n                w = np.ones_like(w)\n            probs = w / np.sum(w)\n            # sample base index among finite indices\n            sel = self.rng.choice(finite_idx, p=probs)\n            base_idx = int(sel)\n            base = X[base_idx].copy()\n\n            # pick donors for DE-like moves (prefer finite ones)\n            other_idxs = [i for i in range(N) if i != base_idx]\n            if len(other_idxs) >= 2:\n                dsel = self.rng.choice(other_idxs, size=2, replace=False)\n                donor1 = X[dsel[0]].copy()\n                donor2 = X[dsel[1]].copy()\n            elif len(other_idxs) == 1:\n                donor1 = X[other_idxs[0]].copy()\n                donor2 = self._uniform_array(lb, ub)\n            else:\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type\n            r = self.rng.random()\n            cand = base.copy()\n\n            if r < p_local:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                # slight attraction to archive mean\n                mean_archive = np.nanmean(X[finite_mask], axis=0)\n                pull = 0.08 * (mean_archive - base)\n                # anisotropic gaussian\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale_abs\n                cand = base + pull + noise\n\n                # Occasionally do PCA-guided elite moves\n                if self.rng.random() < 0.20 and len(finite_idx) >= 4:\n                    elites_k = min(max(3, int(0.25 * len(finite_idx))), len(finite_idx))\n                    elite_idx = np.argsort(F[finite_idx])[:elites_k]\n                    elites = X[finite_idx][elite_idx]  # shape (k, dim)\n                    # center elites\n                    centered = elites - np.mean(elites, axis=0)\n                    try:\n                        # covariance and eigen decomposition\n                        cov = np.cov(centered, rowvar=False)\n                        # small regularizer\n                        cov += np.eye(self.dim) * (1e-8 * np.mean(np.diag(cov) + 1e-6))\n                        vals, vecs = np.linalg.eigh(cov)\n                        order_eig = np.argsort(vals)[::-1]\n                        vals = vals[order_eig]\n                        vecs = vecs[:, order_eig]\n                        # sample along principal subspace\n                        top = min(3, len(vals))\n                        coeffs = self.rng.normal(0.0, 1.0, size=top) * (gscale * 0.8)\n                        perturb = vecs[:, :top] @ (coeffs * np.sqrt(np.maximum(vals[:top], 1e-12)))\n                        # shift around a random elite center\n                        pick_el = elites[self.rng.integers(len(elites))].copy()\n                        cand2 = pick_el + perturb\n                        # accept cand2 as alternative with some prob\n                        if self.rng.random() < 0.6:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.38:\n                    Fscale = 0.5 + 1.1 * self.rng.random()\n                    mask_comp = self.rng.random(self.dim) < (0.15 + 0.35 * self.rng.random())\n                    de_part = Fscale * (donor1 - donor2)\n                    cand[mask_comp] = base[mask_comp] + de_part[mask_comp] + self.rng.normal(0, 0.01, size=np.sum(mask_comp))\n\n                # occasionally perform a tempered Cauchy jump after DE-like step\n                if self.rng.random() < 0.06:\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    cauch = np.clip(cauch, -8.0, 8.0)\n                    cand += cauch * per_dim_scale_abs * (0.6 + 1.2 * self.rng.random(self.dim))\n\n            else:\n                # GLOBAL moves: DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                if self.rng.random() < 0.65:\n                    # DE-like global differential move (more exploratory)\n                    global_F = 0.8 + 1.4 * self.rng.random()\n                    cand = base + global_F * (donor1 - donor2)\n                    # occasionally replace some components with donor1 directly\n                    mask_replace = self.rng.random(self.dim) < 0.15\n                    cand[mask_replace] = donor1[mask_replace]\n                    # small gaussian jitter\n                    cand += self.rng.normal(0, 0.02, size=self.dim) * per_dim_scale_abs\n                    # occasionally tempered Cauchy jump\n                    if self.rng.random() < 0.08:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        cauch = np.clip(cauch, -8.0, 8.0)\n                        cand += cauch * per_dim_scale_abs * (0.8 + 1.6 * self.rng.random(self.dim))\n                else:\n                    # ARCHIVE_MIX or CAUCHY global jumps\n                    # pick a donor randomly (prefer not base)\n                    pick_idx = base_idx\n                    if N > 1:\n                        pick_idx = self.rng.integers(0, N)\n                        if pick_idx == base_idx and N > 2:\n                            pick_idx = (pick_idx + 1) % N\n                    donor = X[pick_idx].copy()\n                    # occasionally inject uniform in a random dimension to diversify\n                    cand = donor.copy()\n                    if self.rng.random() < 0.35:\n                        rand_dims = self.rng.random(self.dim) < (0.08 + 0.12 * self.dim / max(10, self.dim))\n                        cand[rand_dims] = lb[rand_dims] + self.rng.random(np.sum(rand_dims)) * (ub[rand_dims] - lb[rand_dims])\n                    # tempered Cauchy global jump from a random archive point or base\n                    scale_factors = per_dim_scale_abs * (0.4 + 0.6 * self.rng.random(self.dim))\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    cauch = np.clip(cauch, -8.0, 8.0)\n                    cand += cauch * scale_factors\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.sum(bad)) * (ub[bad] - lb[bad])\n\n            # reflect/clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            finite_vals = F[np.isfinite(F)]\n            if np.isfinite(f):\n                if improved:\n                    success = True\n                else:\n                    if finite_vals.size > 0:\n                        thr = np.percentile(finite_vals, 25)\n                        success = (f <= thr)\n                    else:\n                        success = False\n            else:\n                success = False\n\n            success_window.append(1 if success else 0)\n            if len(success_window) > adapt_window:\n                success_window.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if len(F) > self.archive_max:\n                keep_best = int(0.6 * self.archive_max)\n                order_all = np.argsort(F)\n                keep_idx = list(order_all[:keep_best])\n                # fill the rest from random among remaining\n                remaining = list(order_all[keep_best:])\n                self.rng.shuffle(remaining)\n                need = self.archive_max - len(keep_idx)\n                if need > 0:\n                    extra = remaining[:need]\n                    keep_idx.extend(extra)\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_window) == adapt_window and (self.evals % adapt_window == 0):\n                win_sum = sum(success_window)\n                rate = win_sum / float(adapt_window)\n                if rate > 0.45:\n                    # too successful -> tighten\n                    gscale *= 0.85\n                elif rate < 0.12:\n                    # unsuccessful -> enlarge\n                    gscale *= 1.18\n                else:\n                    # modest\n                    gscale *= 0.98 + 0.06 * (0.5 - rate)\n                # jitter and clip\n                gscale *= (1.0 + (self.rng.random() - 0.5) * 0.06)\n                gscale = float(np.clip(gscale, 1e-6, 1e2))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit:\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = min(12, max(4, int(3 + self.dim / 2)))\n                injects = []\n                for k in range(n_inject):\n                    if self.x_opt is None or self.rng.random() < 0.4:\n                        # random uniform\n                        injects.append(self._uniform_array(lb, ub))\n                    else:\n                        # perturb elite/best\n                        factor = (2.0 + 6.0 * (k / max(1, n_inject - 1)))\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale * gscale * factor\n                        injects.append(self._reflect_bounds(self.x_opt + perturb, lb, ub))\n                # evaluate them (append while respecting budget)\n                for cand in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f = safe_eval(cand)\n                    X = np.vstack([X, cand])\n                    F = np.concatenate([F, np.array([f], dtype=float)])\n                    if np.isfinite(f) and f < self.f_opt:\n                        self.f_opt = float(f)\n                        self.x_opt = cand.copy()\n                        improved = True\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale *= 1.6\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n                iter_since_improve = 0\n                success_window = []\n\n            # loop end\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            # pick best finite in archive if any\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                idx = np.argmin(F[finite_mask])\n                # map back to original indices\n                fin_idx = np.where(finite_mask)[0][idx]\n                self.x_opt = X[fin_idx].copy()\n                self.f_opt = float(F[fin_idx])\n            else:\n                self.x_opt = center.copy()\n                self.f_opt = safe_eval(center) if self.evals < self.budget else np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e36e156a-47b5-462f-83bc-cf3e6e185583", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.init_samples = int(init_samples) if init_samples is not None else max(12, 2 * self.dim)\n        self.archive_max = int(archive_max)\n        # runtime trackers\n        self.eval_count = 0\n        self.archive_x = []   # list of arrays\n        self.archive_f = []   # list of floats\n        self.best_x = None\n        self.best_f = np.inf\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try multiple common patterns, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # Pattern 1: func.bounds.lb / func.bounds.ub (objects)\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe a 2xN array or Nx2\n                    arr = np.asarray(b)\n                    if arr.ndim == 2:\n                        if arr.shape[0] == 2 and arr.shape[1] >= self.dim:\n                            lb = arr[0, :self.dim].astype(float)\n                            ub = arr[1, :self.dim].astype(float)\n                        elif arr.shape[1] == 2 and arr.shape[0] >= self.dim:\n                            lb = arr[:self.dim, 0].astype(float)\n                            ub = arr[:self.dim, 1].astype(float)\n        except Exception:\n            pass\n\n        # Pattern 2: lower_bounds / upper_bounds\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                pass\n\n        # Pattern 3: attributes lr/ur\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n                    lb = np.asarray(func.lower, dtype=float)\n                    ub = np.asarray(func.upper, dtype=float)\n            except Exception:\n                pass\n\n        # Fallback to [-5,5]\n        if lb is None or ub is None:\n            lb = -5.0\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float).ravel()\n        ub = np.asarray(ub, dtype=float).ravel()\n\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # pad/truncate\n        if lb.size < self.dim:\n            lb = np.concatenate([lb, np.full(self.dim - lb.size, lb[-1])])\n        if ub.size < self.dim:\n            ub = np.concatenate([ub, np.full(self.dim - ub.size, ub[-1])])\n        lb = lb[:self.dim]\n        ub = ub[:self.dim]\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # In-place style: return reflected array\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            # reflect low: x = 2*lb - x\n            if low_mask.any():\n                x[low_mask] = 2 * lb[low_mask] - x[low_mask]\n            if high_mask.any():\n                x[high_mask] = 2 * ub[high_mask] - x[high_mask]\n        # final clamp\n        x = np.maximum(np.minimum(x, ub), lb)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        if n is None:\n            return self.rng.uniform(lb, ub)\n        else:\n            lb = np.asarray(lb)\n            ub = np.asarray(ub)\n            shape = (n, self.dim)\n            # broadcast scalars or per-dim arrays\n            if lb.size == 1 and ub.size == 1:\n                return self.rng.uniform(lb.item(), ub.item(), size=shape)\n            else:\n                res = np.empty(shape)\n                for d in range(self.dim):\n                    res[:, d] = self.rng.uniform(lb[d], ub[d], size=n)\n                return res\n\n    # ----- main call -----\n    def __call__(self, func):\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # returns +inf if budget exhausted (and does not call func)\n            if self.eval_count >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                # try to pad/truncate\n                if x.size < self.dim:\n                    tmp = np.zeros(self.dim, dtype=float)\n                    tmp[:x.size] = x\n                    x = tmp\n                else:\n                    x = x[:self.dim]\n            try:\n                f = func(x)\n            except Exception:\n                # count evaluation even if it errors\n                self.eval_count += 1\n                return np.inf\n            self.eval_count += 1\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            return f\n\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        # Initialization: stratified per-dim independent (cheap LHS-like)\n        n_init = max(1, min(self.init_samples, self.budget))\n        # create strata indices for each dim\n        strata = np.arange(n_init)\n        init_X = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            # for each stratum i, sample a point in interval [i/n, (i+1)/n)\n            u = self.rng.uniform(size=n_init)\n            # map to bounds\n            seg_low = lb[d] + (perm + u) * (ub[d] - lb[d]) / n_init\n            init_X[:, d] = seg_low\n\n        # ensure at least one uniform sample\n        init_X[0, :] = self.rng.uniform(lb, ub)\n\n        # include center as a seed if budget allows\n        seeds = []\n        for i in range(n_init):\n            seeds.append(init_X[i, :].copy())\n        seeds.append(center.copy())\n\n        # evaluate initial population (respecting budget)\n        for x0 in seeds:\n            if self.eval_count >= self.budget:\n                break\n            f0 = safe_eval(x0)\n            self.archive_x.append(x0.copy())\n            self.archive_f.append(f0)\n            if np.isfinite(f0) and f0 < self.best_f:\n                self.best_f = f0\n                self.best_x = x0.copy()\n\n        # If no finite evaluations, try center explicitly (counts toward budget)\n        if self.best_x is None and self.eval_count < self.budget:\n            f_c = safe_eval(center)\n            self.archive_x.append(center.copy())\n            self.archive_f.append(f_c)\n            if np.isfinite(f_c):\n                self.best_x = center.copy()\n                self.best_f = f_c\n\n        # If still no finite, we'll keep exploring until budget used\n        # short-term adaptation variables\n        gscale = 0.5 * np.mean(ub - lb)  # global scale\n        success_history = []\n        stagnation = 0\n        iter_since_improve = 0\n        adapt_interval = 10\n        window_success = 50\n\n        # Helper to get numpy arrays of finite archive\n        def finite_archive_arrays():\n            xs = []\n            fs = []\n            for x, f in zip(self.archive_x, self.archive_f):\n                if np.isfinite(f):\n                    xs.append(x)\n                    fs.append(f)\n            if len(fs) == 0:\n                return np.zeros((0, self.dim)), np.array([])\n            return np.vstack(xs), np.array(fs, dtype=float)\n\n        # Main loop: produce candidates until budget exhausted\n        while self.eval_count < self.budget:\n            # ensure some fallback: if archive empty just sample uniform\n            Xfinite, Ffinite = finite_archive_arrays()\n            n_finite = Ffinite.size\n\n            # robust per-dim scale: use IQR -> approx std\n            if n_finite >= 3:\n                q75 = np.percentile(Xfinite, 75, axis=0)\n                q25 = np.percentile(Xfinite, 25, axis=0)\n                scales = 0.741 * (q75 - q25)  # approx std from IQR\n                scales = np.maximum(scales, 1e-8 * (ub - lb))\n            else:\n                scales = 0.25 * (ub - lb)\n            # per-dim anisotropic multiplier\n            local_scale = np.maximum(scales, 1e-8)\n\n            # choose base index biased to better ranks (softmax on negative F)\n            base_idx = None\n            if n_finite >= 1:\n                ranks = np.argsort(np.argsort(Ffinite))  # 0 best\n                # convert to 1-based rank for weighting\n                ranks1 = ranks + 1\n                weights = np.exp(-ranks1 / max(1.0, np.median(ranks1)))\n                weights = weights / weights.sum()\n                # sample an index proportionally\n                pick = self.rng.choice(n_finite, p=weights)\n                # map pick to archive index\n                # find the corresponding absolute index in archive_x\n                # create list of indices of finite entries\n                finite_indices = [i for i, f in enumerate(self.archive_f) if np.isfinite(f)]\n                base_idx = finite_indices[pick]\n            else:\n                base_idx = None\n\n            # pick donors for DE-like moves (prefer finite ones)\n            all_indices = list(range(len(self.archive_x)))\n            if base_idx is None:\n                # choose two random distinct donors\n                donors = self.rng.choice(all_indices, size=min(3, len(all_indices)), replace=False).tolist() if len(all_indices) > 0 else []\n            else:\n                candidates = [i for i in all_indices if i != base_idx]\n                if len(candidates) >= 2:\n                    donors = self.rng.choice(candidates, size=min(3, len(candidates)), replace=False).tolist()\n                else:\n                    donors = candidates.copy()\n\n            # choose move type\n            r = self.rng.rand()\n            p_local = 0.6\n            p_pca = 0.08\n            p_global = 1.0 - p_local - p_pca\n\n            cand = None\n\n            # LOCAL move: anisotropic Gaussian with slight pull to mean\n            if r < p_local and n_finite >= 1:\n                # pick base (use chosen base if available)\n                if base_idx is None:\n                    base = self.rng.uniform(lb, ub)\n                else:\n                    base = np.asarray(self.archive_x[base_idx], dtype=float)\n\n                # anisotropic gaussian perturbation\n                # scale per-dim = local_scale * gscale_fraction\n                gfrac = 0.3 * (gscale / (np.mean(ub - lb) + 1e-12))\n                sigma = local_scale * gfrac + 1e-12 * (ub - lb)\n                noise = self.rng.normal(scale=sigma)\n                # slight attraction to archive mean (if available)\n                if n_finite >= 2:\n                    arch_mean = np.mean(Xfinite, axis=0)\n                    pull = 0.05 * (arch_mean - base)\n                else:\n                    pull = 0.0\n                cand = base + noise + pull\n\n                # occasionally do differential mixing inside local\n                if len(donors) >= 2 and self.rng.rand() < 0.25:\n                    d1 = np.asarray(self.archive_x[donors[0]], dtype=float)\n                    d2 = np.asarray(self.archive_x[donors[1]], dtype=float)\n                    F = 0.6 + 0.4 * self.rng.rand()  # differential factor\n                    mask = self.rng.rand(self.dim) < 0.5\n                    cand[mask] += F * (d1[mask] - d2[mask])\n\n                # Occasionally do PCA-guided elite moves\n                if n_finite >= 6 and self.rng.rand() < 0.12:\n                    # pick elites\n                    n_elite = max(3, int(0.15 * n_finite))\n                    elite_idx = np.argsort(Ffinite)[:n_elite]\n                    elite_points = Xfinite[elite_idx, :]\n                    # center and covariance\n                    C = np.cov(elite_points, rowvar=False)\n                    # regularize\n                    C += np.eye(self.dim) * (1e-8 + 1e-6 * np.mean(np.diag(C)))\n                    # eigen decomposition\n                    try:\n                        vals, vecs = np.linalg.eigh(C)\n                        # pick top k principal directions\n                        k = max(1, min(4, int(np.sum(vals > (np.max(vals) * 1e-6)))))\n                        # sample coefficients along top-k\n                        coeffs = np.zeros(self.dim)\n                        top_idx = np.argsort(vals)[-k:]\n                        for ti in top_idx:\n                            coeffs += vecs[:, ti] * (self.rng.normal() * np.sqrt(max(0.0, vals[ti])) * (gscale / (np.mean(ub - lb) + 1e-12)))\n                        # shift around a random elite center\n                        cen = elite_points[self.rng.randint(0, elite_points.shape[0])]\n                        cand2 = cen + coeffs\n                        # with some probability accept cand2 instead\n                        if self.rng.rand() < 0.5:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # Occasionally tempered Cauchy global jump after local move\n                if self.rng.rand() < 0.06:\n                    cauch = np.tan(np.pi * (self.rng.rand(self.dim) - 0.5))\n                    cand += (gscale * 0.5) * cauch\n\n            # PCA-guided elite move (rare, separate branch)\n            elif r < p_local + p_pca and n_finite >= 6:\n                n_elite = max(4, int(0.1 * n_finite))\n                elite_idx = np.argsort(Ffinite)[:n_elite]\n                elite_points = Xfinite[elite_idx, :]\n                C = np.cov(elite_points, rowvar=False)\n                C += np.eye(self.dim) * (1e-8 + 1e-6 * np.mean(np.diag(C)))\n                try:\n                    vals, vecs = np.linalg.eigh(C)\n                    # sample in subspace spanned by top 2-5\n                    k = max(1, min(5, int(np.sum(vals > (np.max(vals) * 1e-6)))))\n                    top_idx = np.argsort(vals)[-k:]\n                    coeffs = np.zeros(self.dim)\n                    for ti in top_idx:\n                        coeffs += vecs[:, ti] * (self.rng.normal() * np.sqrt(max(0.0, vals[ti])) * (gscale / (np.mean(ub - lb) + 1e-12)))\n                    cen = elite_points[self.rng.randint(0, elite_points.shape[0])]\n                    cand = cen + coeffs\n                    # small jitter\n                    cand += self.rng.normal(scale=0.02 * (ub - lb))\n                except Exception:\n                    cand = self.rng.uniform(lb, ub)\n\n            # GLOBAL moves: DE-like global differential or archive mix or cauchy jumps\n            else:\n                # DE-like global differential move\n                # select three distinct archive points if possible\n                if len(all_indices) >= 3:\n                    idxs = self.rng.choice(all_indices, size=3, replace=False)\n                    x0 = np.asarray(self.archive_x[idxs[0]], dtype=float)\n                    x1 = np.asarray(self.archive_x[idxs[1]], dtype=float)\n                    x2 = np.asarray(self.archive_x[idxs[2]], dtype=float)\n                    F = 0.8 + 0.6 * self.rng.rand()\n                    cand = x0 + F * (x1 - x2)\n                    # occasionally replace some components with donor1 directly\n                    if self.rng.rand() < 0.2:\n                        mask = self.rng.rand(self.dim) < 0.15\n                        cand[mask] = x1[mask]\n                    # small gaussian jitter\n                    cand += self.rng.normal(scale=0.03 * (ub - lb))\n                    # tempered Cauchy jump\n                    if self.rng.rand() < 0.05:\n                        cauch = np.tan(np.pi * (self.rng.rand(self.dim) - 0.5))\n                        cand += (gscale) * 0.7 * cauch\n                else:\n                    # fallback global: pick random archive or uniform\n                    if len(all_indices) > 0 and self.rng.rand() < 0.7:\n                        donor = np.asarray(self.archive_x[self.rng.choice(all_indices)], dtype=float)\n                        cand = donor + self.rng.normal(scale=0.2 * (ub - lb))\n                    else:\n                        cand = self.rng.uniform(lb, ub)\n\n                # occasionally inject uniform in one random dimension\n                if self.rng.rand() < 0.12:\n                    d = self.rng.randint(0, self.dim)\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n\n            # safety: replace non-finite entries and reflect to bounds\n            cand = np.asarray(cand, dtype=float).ravel()\n            # replace any nan/inf with uniform\n            bad_mask = ~np.isfinite(cand)\n            if bad_mask.any():\n                cand[bad_mask] = self.rng.uniform(lb[bad_mask], ub[bad_mask])\n            # reflect\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate respecting budget\n            if self.eval_count >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_c):\n                if f_c < self.best_f:\n                    self.best_f = f_c\n                    self.best_x = cand.copy()\n                    improved = True\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            Xfinite, Ffinite = finite_archive_arrays()\n            success = False\n            if np.isfinite(f_c):\n                if f_c <= self.best_f:\n                    success = True\n                elif Ffinite.size > 0:\n                    threshold = np.percentile(Ffinite, 25)\n                    if f_c <= threshold:\n                        success = True\n            success_history.append(1 if success else 0)\n            if len(success_history) > window_success:\n                success_history.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if len(self.archive_x) > self.archive_max:\n                # keep top half\n                fs = np.array(self.archive_f)\n                idx_sorted = np.argsort(fs)\n                keep_best = max(1, int(0.5 * self.archive_max))\n                keep_idxs = list(idx_sorted[:keep_best])\n                # fill rest randomly from remaining\n                remaining = [i for i in range(len(fs)) if i not in keep_idxs]\n                if len(remaining) > 0:\n                    add = min(self.archive_max - len(keep_idxs), len(remaining))\n                    add_idxs = list(self.rng.choice(remaining, size=add, replace=False))\n                    keep_idxs += add_idxs\n                # rebuild archive\n                self.archive_x = [self.archive_x[i] for i in keep_idxs]\n                self.archive_f = [self.archive_f[i] for i in keep_idxs]\n\n            # adapt gscale occasionally based on short-term success\n            if self.eval_count % adapt_interval == 0 and len(success_history) > 5:\n                succ_rate = np.mean(success_history)\n                if succ_rate > 0.35:\n                    # too successful -> tighten\n                    gscale *= 0.85\n                elif succ_rate < 0.10:\n                    # unsuccessful -> enlarge\n                    gscale *= 1.18\n                else:\n                    # modest: small jitter\n                    gscale *= (1.0 + (self.rng.rand() - 0.5) * 0.08)\n                # clip\n                span = np.mean(ub - lb)\n                gscale = float(np.clip(gscale, 1e-6 * span, 4.0 * span))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 5 * self.dim):\n                # micro-restart: inject some perturbed candidates (prefer perturbations of best)\n                n_inject = min(6 + self.dim // 2, max(3, (self.budget - self.eval_count) // 10))\n                inject_list = []\n                for _ in range(n_inject):\n                    if self.best_x is not None and self.rng.rand() < 0.8:\n                        # perturb best\n                        pert = self.best_x + self.rng.normal(scale=gscale * 0.8 * self.rng.rand(self.dim))\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        inject_list.append(pert)\n                    else:\n                        inject_list.append(self.rng.uniform(lb, ub))\n                # evaluate injected candidates until budget exhausted\n                for inj in inject_list:\n                    if self.eval_count >= self.budget:\n                        break\n                    f_inj = safe_eval(inj)\n                    self.archive_x.append(inj.copy())\n                    self.archive_f.append(f_inj)\n                    if np.isfinite(f_inj) and f_inj < self.best_f:\n                        self.best_f = f_inj\n                        self.best_x = inj.copy()\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale *= 1.6\n                gscale = float(np.clip(gscale, 1e-6 * np.mean(ub - lb), 4.0 * np.mean(ub - lb)))\n                iter_since_improve = 0\n                success_history = []\n\n        # final safety: return best finite point found, else return center\n        if self.best_x is None:\n            # pick best finite in archive if any\n            Xfinite, Ffinite = finite_archive_arrays()\n            if Ffinite.size > 0:\n                idx = np.argmin(Ffinite)\n                return float(Ffinite[idx]), Xfinite[idx].copy()\n            else:\n                # evaluate center if budget allows\n                if self.eval_count < self.budget:\n                    f_c = safe_eval(center)\n                    return float(f_c), center.copy()\n                else:\n                    # no budget left, return a uniform guess\n                    guess = 0.5 * (lb + ub)\n                    return np.inf, guess\n        else:\n            return float(self.best_f), np.asarray(self.best_x, dtype=float).copy()\n\n\n# Simple test harness (not part of submission) -- usage example:\nif __name__ == \"__main__\":\n    # Define a toy sphere function with bounds\n    class Sphere:\n        def __init__(self, dim):\n            self.dim = dim\n            self.bounds = type(\"B\", (), {\"lb\": -5.0, \"ub\": 5.0})\n        def __call__(self, x):\n            return float(np.sum((np.asarray(x) ** 2)))\n\n    dim = 5\n    budget = 500\n    f = Sphere(dim)\n    opt = MemoryGuidedADS(budget=budget, dim=dim, seed=123, init_samples=12, archive_max=200)\n    best_f, best_x = opt(f)\n    print(\"Best f:\", best_f)\n    print(\"Best x:\", best_x)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ae8d0e4a-04b2-4ce6-b608-ea532843a628", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(3, int(init_samples))\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n\n        # trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # try several common attribute names, otherwise fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    arr = np.asarray(b, dtype=float)\n                    if arr.ndim == 2 and arr.shape[0] == 2:\n                        lb = arr[0].astype(float)\n                        ub = arr[1].astype(float)\n        except Exception:\n            lb = None\n        if lb is None or ub is None:\n            if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                try:\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n                except Exception:\n                    lb = None\n        if lb is None or ub is None:\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    lb = None\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # truncate or pad\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, -5.0, dtype=float)\n            tmp[:min(len(lb.flat), self.dim)] = lb.flat[:min(len(lb.flat), self.dim)]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, 5.0, dtype=float)\n            tmp[:min(len(ub.flat), self.dim)] = ub.flat[:min(len(ub.flat), self.dim)]\n            ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if np.any(high_mask):\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            # single sample\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            arr = self.rng.random((n, self.dim))\n            return lb + arr * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            if x.ndim > 1:\n                x = x.ravel()[: self.dim]\n            try:\n                f = func(x)\n            except Exception:\n                # count the attempted evaluation but treat as failed\n                self.evals += 1\n                return np.inf\n            # count eval\n            self.evals += 1\n            try:\n                f = float(f)\n            except Exception:\n                return np.inf\n            if not np.isfinite(f):\n                return np.inf\n            return f\n\n        # initialization\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # seeding: stratified per-dim independent (cheap LHS-like)\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        n0 = max(2, n0)\n        X_list = []\n        F_list = []\n\n        # include center\n        center = 0.5 * (lb + ub)\n        X_list.append(center.copy())\n\n        # stratified per-dim independent\n        if n0 - 1 > 0:\n            samples = np.empty((n0 - 1, self.dim), dtype=float)\n            for d in range(self.dim):\n                perm = np.arange(n0 - 1)\n                self.rng.shuffle(perm)\n                strata = (perm + self.rng.random(n0 - 1)) / (n0 - 1)\n                samples[:, d] = lb[d] + strata * (ub[d] - lb[d])\n            for i in range(n0 - 1):\n                X_list.append(samples[i].copy())\n\n        # ensure at least one purely uniform random sample\n        X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        for x in X_list[:]:\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(x)\n            F_list.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        X = np.array(X_list, dtype=float)\n        F = np.array(F_list, dtype=float)\n\n        # If no finite evaluations, set center as fallback (but evals still counted)\n        finite_mask = np.isfinite(F)\n        if not np.any(finite_mask):\n            if self.evals < self.budget:\n                f = safe_eval(center)\n                if np.isfinite(f):\n                    self.f_opt = float(f)\n                    X = np.vstack([X, center.copy()])\n                    F = np.concatenate([F, np.array([f], dtype=float)])\n\n        # short-term adaptation variables\n        gscale = 0.8  # global short-term scale multiplier\n        p_local = 0.72\n        adapt_window = 50\n        success_window = []\n        win_sum = 0\n        iter_since_improve = 0\n        stagnation_limit = max(200, int(5 * self.dim))\n\n        # main loop\n        while self.evals < self.budget:\n            N = len(X)\n            if N == 0:\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X = np.array([cand])\n                F = np.array([f], dtype=float)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = cand.copy()\n                continue\n\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0]\n            if finite_idx.size == 0:\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X = np.vstack([X, cand])\n                F = np.concatenate([F, np.array([f], dtype=float)])\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = cand.copy()\n                continue\n\n            # per-dim adaptive scale: robust spread from IQR -> approx std\n            try:\n                q75 = np.percentile(X[finite_idx], 75, axis=0)\n                q25 = np.percentile(X[finite_idx], 25, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approximate std from IQR\n                per_dim_scale = np.asarray(per_dim_scale, dtype=float)\n                per_dim_scale[np.isnan(per_dim_scale)] = 0.0\n            except Exception:\n                per_dim_scale = np.std(X[finite_idx], axis=0)\n                per_dim_scale = np.asarray(per_dim_scale, dtype=float)\n\n            # fallback small positive\n            mask_small = per_dim_scale <= 1e-12\n            if np.any(mask_small):\n                per_dim_scale[mask_small] = span[mask_small] * 0.05 + 1e-6\n\n            per_dim_scale_abs = per_dim_scale * gscale + 1e-12\n\n            # choose base index biased to better ranks (soft exponential on rank)\n            finite_F = F[finite_idx]\n            order = np.argsort(finite_F)  # best first\n            ranks = np.empty_like(order)\n            ranks[order] = np.arange(len(order))\n            beta = 0.6 + 0.4 * (self.dim / max(10, self.dim))\n            w = np.exp(-beta * ranks.astype(float))\n            if np.any(w < 0) or np.all(w == 0):\n                w = np.ones_like(w)\n            probs = w / np.sum(w)\n            chosen_local = self.rng.choice(len(finite_idx), p=probs)\n            base_idx = int(finite_idx[chosen_local])\n            base = X[base_idx].copy()\n\n            # pick donors for DE-like moves\n            all_idxs = list(range(N))\n            other_idxs = [i for i in all_idxs if i != base_idx]\n            if len(other_idxs) >= 2:\n                dsel = self.rng.choice(other_idxs, size=2, replace=False)\n                donor1 = X[dsel[0]].copy()\n                donor2 = X[dsel[1]].copy()\n            elif len(other_idxs) == 1:\n                donor1 = X[other_idxs[0]].copy()\n                donor2 = self._uniform_array(lb, ub)\n            else:\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type\n            r = self.rng.random()\n            cand = base.copy()\n\n            # compute some archive statistics\n            mean_archive = np.mean(X[finite_idx], axis=0) if finite_idx.size > 0 else base.copy()\n\n            if r < p_local:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                pull = 0.08 * (mean_archive - base)\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale_abs\n                cand = cand + pull + noise * (0.6 + 0.8 * self.rng.random(self.dim))\n\n                # PCA-guided elite moves\n                if self.rng.random() < 0.20 and len(finite_idx) >= 4:\n                    elites_k = min(max(3, int(0.25 * len(finite_idx))), len(finite_idx))\n                    elite_idx_local = np.argsort(F[finite_idx])[:elites_k]\n                    elites = X[finite_idx][elite_idx_local]  # shape (k, dim)\n                    try:\n                        centered = elites - np.mean(elites, axis=0)\n                        cov = np.cov(centered, rowvar=False)\n                        # regularize\n                        cov += np.eye(self.dim) * (1e-8 * (np.mean(np.diag(cov)) + 1e-6))\n                        vals, vecs = np.linalg.eigh(cov)\n                        order_eig = np.argsort(vals)[::-1]\n                        vals = vals[order_eig]\n                        vecs = vecs[:, order_eig]\n                        top = min(3, len(vals))\n                        coeffs = self.rng.normal(0.0, 1.0, size=top) * (gscale * 0.8)\n                        perturb = vecs[:, :top] @ (coeffs * np.sqrt(np.maximum(vals[:top], 1e-12)))\n                        pick_el = elites[self.rng.integers(len(elites))].copy()\n                        cand2 = pick_el + perturb\n                        if self.rng.random() < 0.6:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.38:\n                    Fscale = 0.5 + 1.1 * self.rng.random()\n                    mask_comp = self.rng.random(self.dim) < (0.15 + 0.35 * self.rng.random())\n                    de_part = Fscale * (donor1 - donor2)\n                    if np.any(mask_comp):\n                        cand[mask_comp] = base[mask_comp] + de_part[mask_comp] + self.rng.normal(0, 0.01, size=np.sum(mask_comp))\n\n                # occasionally perform a tempered Cauchy jump after DE-like step\n                if self.rng.random() < 0.06:\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    cauch = np.clip(cauch, -8.0, 8.0)\n                    cand += cauch * per_dim_scale_abs * (0.6 + 1.2 * self.rng.random(self.dim))\n\n            else:\n                # GLOBAL moves: DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                if self.rng.random() < 0.5:\n                    # DE-like global differential move (more exploratory)\n                    global_F = 0.8 + 1.4 * self.rng.random()\n                    cand = base + global_F * (donor1 - donor2)\n                    mask_replace = self.rng.random(self.dim) < 0.15\n                    cand[mask_replace] = donor1[mask_replace]\n                    cand += self.rng.normal(0, 0.02, size=self.dim) * per_dim_scale_abs\n                    if self.rng.random() < 0.08:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        cauch = np.clip(cauch, -8.0, 8.0)\n                        cand += cauch * per_dim_scale_abs * (0.8 + 1.6 * self.rng.random(self.dim))\n                else:\n                    # ARCHIVE_MIX or tempered Cauchy from random archive point\n                    pick_idx = base_idx\n                    if N > 1:\n                        pick_idx = int(self.rng.integers(0, N))\n                        if pick_idx == base_idx and N > 2:\n                            pick_idx = (pick_idx + 1) % N\n                    donor = X[pick_idx].copy()\n                    cand = donor.copy()\n                    if self.rng.random() < 0.35:\n                        rand_dims = self.rng.random(self.dim) < (0.08 + 0.12 * self.dim / max(10, self.dim))\n                        if np.any(rand_dims):\n                            cand[rand_dims] = lb[rand_dims] + self.rng.random(np.sum(rand_dims)) * (ub[rand_dims] - lb[rand_dims])\n                    scale_factors = per_dim_scale_abs * (0.4 + 0.6 * self.rng.random(self.dim))\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    cauch = np.clip(cauch, -8.0, 8.0)\n                    cand += cauch * scale_factors\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.sum(bad)) * (ub[bad] - lb[bad])\n\n            # reflect/clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            finite_vals = F[np.isfinite(F)]\n            success = False\n            if np.isfinite(f):\n                if improved:\n                    success = True\n                else:\n                    if finite_vals.size > 0:\n                        thr = np.percentile(finite_vals, 25)\n                        success = (f <= thr)\n            # update sliding window\n            success_window.append(bool(success))\n            win_sum += 1 if success else 0\n            if len(success_window) > adapt_window:\n                oldest = success_window.pop(0)\n                if oldest:\n                    win_sum -= 1\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if len(F) > self.archive_max:\n                keep_best = int(0.6 * self.archive_max)\n                order_all = np.argsort(F)\n                keep_idx = list(order_all[:keep_best])\n                remaining = list(order_all[keep_best:])\n                self.rng.shuffle(remaining)\n                need = self.archive_max - len(keep_idx)\n                if need > 0:\n                    extra = remaining[:need]\n                    keep_idx.extend(extra)\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_window) == adapt_window and (self.evals % adapt_window == 0):\n                rate = win_sum / float(adapt_window)\n                if rate > 0.45:\n                    # too successful -> tighten a bit\n                    gscale *= 0.92\n                elif rate < 0.12:\n                    # unsuccessful -> enlarge\n                    gscale *= 1.18\n                # jitter and clip\n                gscale *= (1.0 + (self.rng.random() - 0.5) * 0.06)\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit and self.evals < self.budget:\n                injects = []\n                n_inject = min(12, max(4, int(3 + self.dim / 2)))\n                for k in range(n_inject):\n                    if self.x_opt is None or self.rng.random() < 0.4:\n                        injects.append(self._uniform_array(lb, ub))\n                    else:\n                        factor = (2.0 + 6.0 * (k / max(1, n_inject - 1)))\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale * gscale * factor\n                        injects.append(self._reflect_bounds(self.x_opt + perturb, lb, ub))\n                # evaluate them (append while respecting budget)\n                for icand in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f = safe_eval(icand)\n                    X = np.vstack([X, icand])\n                    F = np.concatenate([F, np.array([f], dtype=float)])\n                    if np.isfinite(f) and f < self.f_opt:\n                        self.f_opt = float(f)\n                        self.x_opt = icand.copy()\n                        improved = True\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale *= 1.6\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n                iter_since_improve = 0\n                success_window = []\n                win_sum = 0\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                idx = np.argmin(F[finite_mask])\n                fin_idx = np.where(finite_mask)[0][idx]\n                self.x_opt = X[fin_idx].copy()\n                self.f_opt = float(F[fin_idx])\n            else:\n                self.x_opt = center.copy()\n                self.f_opt = safe_eval(center) if self.evals < self.budget else np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "63e62441-6bc1-41b0-bee0-b70f713499be", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(3, int(init_samples))\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n\n        # trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try several common attribute patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # 1) func.bounds.lb / func.bounds.ub or func.bounds (2xN array)\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # object with lb/ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe 2xN array-like\n                    arr = np.asarray(b, dtype=float)\n                    if arr.ndim == 2 and arr.shape[0] == 2:\n                        lb = arr[0].astype(float)\n                        ub = arr[1].astype(float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # 2) lower_bounds / upper_bounds\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                lb = lb\n                ub = ub\n\n        # 3) simple attributes func.lb / func.ub\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                pass\n\n        # fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Broadcast scalars to arrays\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # If mismatch, pad/truncate\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, -5.0, dtype=float)\n            tmp[:min(len(lb), self.dim)] = lb.flat[:min(len(lb), self.dim)]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, 5.0, dtype=float)\n            tmp[:min(len(ub), self.dim)] = ub.flat[:min(len(ub), self.dim)]\n            ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = x.copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if np.any(high_mask):\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return self.rng.uniform(lb, ub)\n        else:\n            arr = self.rng.random((n, self.dim))\n            return lb + arr * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # returns +inf if budget exhausted (and does not call func)\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            # flatten to 1D\n            if x.ndim > 1:\n                x = x.reshape(-1)\n            try:\n                f = func(x)\n            except Exception:\n                # count evaluation attempt\n                self.evals += 1\n                return np.inf\n            # count eval and return\n            self.evals += 1\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            return f\n\n        # initialization\n        self.evals = 0\n        self.x_opt = None\n        self.f_opt = np.inf\n\n        # seeding: stratified per-dim independent (cheap LHS-like)\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        X_list = []\n        F_list = []\n\n        # include center\n        center = 0.5 * (lb + ub)\n        X_list.append(center.copy())\n\n        # stratified per-dim independent (n0-1 samples)\n        if n0 - 1 > 0:\n            samples = np.zeros((n0 - 1, self.dim), dtype=float)\n            for d in range(self.dim):\n                perm = self.rng.permutation(n0 - 1)\n                # sample one in each stratum for this dim\n                samples[:, d] = lb[d] + (perm + self.rng.random(n0 - 1)) / float(n0) * (ub[d] - lb[d])\n            for i in range(n0 - 1):\n                X_list.append(samples[i].copy())\n\n        # ensure at least one purely uniform random sample\n        X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        for x in X_list:\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(x)\n            F_list.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        if len(F_list) == 0:\n            F = np.array([], dtype=float)\n            X = np.array([], dtype=float).reshape((0, self.dim))\n        else:\n            X = np.array(X_list, dtype=float)\n            F = np.array(F_list, dtype=float)\n\n        # If no finite evaluations, try evaluating center explicitly (if budget left)\n        finite_mask = np.isfinite(F)\n        if F.size == 0 or not np.any(finite_mask):\n            if self.evals < self.budget:\n                f = safe_eval(center)\n                if np.isfinite(f):\n                    self.f_opt = float(f)\n                    self.x_opt = center.copy()\n                # append center to archive regardless (unless over budget)\n                X = np.vstack([X, center.copy()]) if X.size else center.reshape((1, self.dim))\n                F = np.concatenate([F, np.array([f], dtype=float)]) if F.size else np.array([f], dtype=float)\n            finite_mask = np.isfinite(F)\n\n        # short-term adaptation variables\n        p_local = 0.72\n        adapt_window = 50\n        success_window = []\n        iter_since_improve = 0\n        stagnation_limit = max(200, int(5 * self.dim))\n        max_iters = max(1, self.budget - self.evals)\n\n        # adaptation params\n        gscale = 1.0\n        global_F = 0.9  # scale for global DE-like moves\n\n        # main loop\n        while self.evals < self.budget:\n            N = len(X)\n            if N == 0:\n                # fallback random\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X = np.array([cand])\n                F = np.array([f], dtype=float)\n                if np.isfinite(f):\n                    self.f_opt = float(f)\n                    self.x_opt = cand.copy()\n                continue\n\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0]\n            finite_vals = F[finite_mask] if finite_idx.size > 0 else np.array([], dtype=float)\n            # per-dim adaptive scale: robust spread from IQR -> approx std\n            if finite_idx.size >= 3:\n                try:\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    per_dim_scale = (q75 - q25) / 1.349  # approx std from IQR\n                    per_dim_scale[np.isnan(per_dim_scale)] = 0.0\n                except Exception:\n                    per_dim_scale = np.std(X[finite_idx], axis=0)\n            else:\n                per_dim_scale = np.std(X[finite_idx], axis=0) if finite_idx.size > 0 else span * 0.12\n\n            # fallback small positive\n            mask_small = per_dim_scale <= 1e-12\n            per_dim_scale[mask_small] = span[mask_small] * 0.05 + 1e-6\n            per_dim_scale_abs = per_dim_scale * gscale\n\n            # choose base index biased to better ranks (softmax on negative F)\n            if finite_idx.size > 0:\n                finite_F = F[finite_idx]\n                order = np.argsort(finite_F)\n                ranks = np.empty_like(order)\n                ranks[order] = np.arange(len(order))\n                beta = 0.6 + 0.4 * (self.dim / max(10, self.dim))\n                w = np.exp(-beta * (ranks / max(1, len(ranks) - 1)))\n                probs = w / np.sum(w)\n                sel = self.rng.choice(len(finite_idx), p=probs)\n                base_idx = int(finite_idx[sel])\n            else:\n                base_idx = int(self.rng.integers(0, N))\n            base = X[base_idx].copy()\n\n            # pick donors for DE-like moves (prefer finite ones)\n            other_idxs = [i for i in range(N) if i != base_idx]\n            if len(other_idxs) >= 2:\n                dsel = self.rng.choice(other_idxs, size=2, replace=False)\n                donor1 = X[dsel[0]].copy()\n                donor2 = X[dsel[1]].copy()\n            elif len(other_idxs) == 1:\n                donor1 = X[other_idxs[0]].copy()\n                donor2 = self._uniform_array(lb, ub)\n            else:\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type\n            r = self.rng.random()\n            cand = base.copy()\n\n            if r < p_local:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                if finite_idx.size > 0:\n                    mean_archive = np.nanmean(X[finite_idx], axis=0)\n                else:\n                    mean_archive = center\n                pull = 0.08 * (mean_archive - base)\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale_abs\n                cand = base + pull + noise\n\n                # Occasionally do PCA-guided elite moves\n                if self.rng.random() < 0.20 and finite_idx.size >= 4:\n                    # pick elites among finite indices\n                    k = min(max(3, int(self.dim // 2)), finite_idx.size)\n                    elite_idx = np.argsort(F[finite_idx])[:k]\n                    elites = X[finite_idx][elite_idx]  # shape (k, dim)\n                    centered = elites - np.mean(elites, axis=0)\n                    try:\n                        cov = np.cov(centered, rowvar=False)\n                        # regularize\n                        cov = cov + np.eye(self.dim) * (1e-8 * (np.mean(np.diag(cov)) + 1e-6))\n                        vals, vecs = np.linalg.eigh(cov)\n                        order_eig = np.argsort(vals)[::-1]\n                        vals = vals[order_eig]\n                        vecs = vecs[:, order_eig]\n                        top = max(1, min(self.dim, int(max(1, np.sum(vals > (np.max(vals) * 1e-6))))))\n                        coeffs = self.rng.normal(0.0, 1.0, size=top)\n                        perturb = vecs[:, :top] @ (coeffs * np.sqrt(np.maximum(vals[:top], 1e-12)))\n                        pick_el = elites[self.rng.integers(len(elites))].copy()\n                        cand2 = pick_el + 0.8 * perturb * (0.6 + 0.8 * self.rng.random())\n                        if self.rng.random() < 0.6:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.38:\n                    Fscale = 0.5 + 1.1 * self.rng.random()\n                    mask_comp = self.rng.random(self.dim) < (0.15 + 0.35 * self.rng.random())\n                    de_part = Fscale * (donor1 - donor2)\n                    # apply component-wise\n                    cand[mask_comp] = cand[mask_comp] + de_part[mask_comp] * (0.5 + self.rng.random() * 0.8)\n\n                    # occasionally tempered Cauchy jump after DE-like step\n                    if self.rng.random() < 0.28:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        cauch = np.clip(cauch, -8.0, 8.0)\n                        cand += cauch * per_dim_scale_abs * (0.6 + 1.2 * self.rng.random(self.dim))\n\n            else:\n                # GLOBAL moves: DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                if self.rng.random() < 0.65:\n                    # DE-like global differential move (more exploratory)\n                    Fscale_g = global_F * (0.7 + 0.6 * self.rng.random())\n                    cand = base + Fscale_g * (donor1 - donor2)\n                    mask_replace = self.rng.random(self.dim) < 0.15\n                    cand[mask_replace] = donor1[mask_replace]\n                    cand += self.rng.normal(0, 0.02, size=self.dim) * per_dim_scale_abs\n                    if self.rng.random() < 0.12:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        cauch = np.clip(cauch, -10.0, 10.0)\n                        cand += cauch * per_dim_scale_abs * (0.8 + 1.6 * self.rng.random(self.dim))\n                else:\n                    # ARCHIVE_MIX or CAUCHY global jumps\n                    pick_idx = base_idx\n                    if N > 2 and self.rng.random() < 0.9:\n                        pick_idx = int(self.rng.integers(0, N))\n                        if pick_idx == base_idx:\n                            pick_idx = (pick_idx + 1) % N\n                    donor = X[pick_idx].copy()\n                    cand = donor.copy()\n                    if self.rng.random() < 0.35:\n                        rand_dims = self.rng.random(self.dim) < (0.08 + 0.12 * self.dim / max(10, self.dim))\n                        rnd = self.rng.random(np.sum(rand_dims))\n                        cand[rand_dims] = lb[rand_dims] + rnd * (ub[rand_dims] - lb[rand_dims])\n                    # tempered Cauchy global jump\n                    scale_factors = per_dim_scale_abs * (0.4 + 0.6 * self.rng.random(self.dim))\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    cauch = np.clip(cauch, -12.0, 12.0)\n                    cand += cauch * scale_factors\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                rnd = self.rng.random(np.sum(bad))\n                cand[bad] = lb[bad] + rnd * (ub[bad] - lb[bad])\n\n            # reflect/clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f):\n                if improved:\n                    success = True\n                else:\n                    if finite_vals.size > 0:\n                        thr = np.percentile(finite_vals, 25)\n                        success = (f <= thr)\n                    else:\n                        success = False\n            else:\n                success = False\n\n            success_window.append(1 if success else 0)\n            if len(success_window) > adapt_window:\n                success_window.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if len(F) > self.archive_max:\n                order_all = np.argsort(F)\n                keep_best = max(5, int(self.archive_max * 0.35))\n                keep_idx = list(order_all[:keep_best])\n                remaining = list(order_all[keep_best:])\n                self.rng.shuffle(remaining)\n                need = self.archive_max - len(keep_idx)\n                if need > 0:\n                    extra = remaining[:need]\n                    keep_idx.extend(extra)\n                keep_idx = np.array(keep_idx, dtype=int)\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_window) == adapt_window and (self.evals % adapt_window == 0):\n                win_sum = sum(success_window)\n                rate = win_sum / float(adapt_window)\n                if rate > 0.45:\n                    gscale *= 0.85\n                else:\n                    gscale *= (0.98 + 0.06 * (0.5 - rate))\n                # jitter and clip\n                gscale *= (1.0 + (self.rng.random() - 0.5) * 0.06)\n                gscale = float(np.clip(gscale, 1e-6, 1e2))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit and self.evals < self.budget:\n                n_inject = min(12, max(4, int(3 + self.dim / 2)))\n                for k in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.7:\n                        # perturb best\n                        factor = (1.0 + 4.0 * (k / max(1, n_inject - 1)))\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale * gscale * factor\n                        cand_r = self.x_opt + perturb\n                    else:\n                        # random injection\n                        cand_r = self._uniform_array(lb, ub)\n                    cand_r = self._reflect_bounds(cand_r, lb, ub)\n                    f_r = safe_eval(cand_r)\n                    X = np.vstack([X, cand_r])\n                    F = np.concatenate([F, np.array([f_r], dtype=float)])\n                    if np.isfinite(f_r) and f_r < self.f_opt:\n                        self.f_opt = float(f_r)\n                        self.x_opt = cand_r.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale *= 1.6\n                iter_since_improve = 0\n\n            # loop end condition checked at top\n\n        # final safety: return best finite point found, else return center\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            idx = np.argmin(F[finite_mask])\n            fin_idx = np.where(finite_mask)[0][idx]\n            self.x_opt = X[fin_idx].copy()\n            self.f_opt = float(F[fin_idx])\n        else:\n            self.x_opt = center.copy()\n            self.f_opt = safe_eval(center) if self.evals < self.budget else np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "efa826f8-c963-4cfc-82f5-bd91c0d28374", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — a hybrid archive-driven continuous sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(3, int(init_samples))\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n\n        # trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # default\n        lb = None\n        ub = None\n\n        # try common patterns\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # object with .lb/.ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    arr = np.asarray(b)\n                    if arr.ndim == 2 and arr.shape[0] == 2:\n                        lb = arr[0].astype(float)\n                        ub = arr[1].astype(float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # try lower_bounds / upper_bounds\n        if (lb is None or ub is None):\n            if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                try:\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # try simple attributes lb / ub\n        if (lb is None or ub is None):\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n\n        # fallback scalar bounds or default [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # Broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # If length mismatch, pad/truncate\n        if lb.size != self.dim or ub.size != self.dim:\n            tmp_lb = np.full(self.dim, -5.0, dtype=float)\n            tmp_ub = np.full(self.dim, 5.0, dtype=float)\n            # fill from provided arrays as much as possible\n            lb_flat = lb.flat if hasattr(lb, \"flat\") else np.asarray(lb).flat\n            ub_flat = ub.flat if hasattr(ub, \"flat\") else np.asarray(ub).flat\n            for i, v in enumerate(lb_flat):\n                if i >= self.dim:\n                    break\n                tmp_lb[i] = float(v)\n            for i, v in enumerate(ub_flat):\n                if i >= self.dim:\n                    break\n                tmp_ub[i] = float(v)\n            lb = tmp_lb\n            ub = tmp_ub\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # reflect out-of-bounds entries iteratively\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if np.any(high_mask):\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            arr = self.rng.random((n, self.dim))\n            return lb + arr * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # returns +inf if budget exhausted (and does not call func)\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            # ensure 1D array\n            if x.ndim > 1:\n                x = x.ravel()[: self.dim]\n            try:\n                f = func(x)\n            except Exception:\n                # any error -> count eval but return inf\n                self.evals += 1\n                return np.inf\n            # count eval\n            self.evals += 1\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            return f\n\n        # initialization\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        center = 0.5 * (lb + ub)\n\n        # seeding: stratified per-dim independent (cheap LHS-like)\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        n0 = max(2, n0)\n        X = []\n        F = []\n\n        # include center\n        X.append(center.copy())\n\n        # stratified per-dim independent\n        if n0 - 1 >= 1:\n            samples = np.empty((n0 - 1, self.dim), dtype=float)\n            for d in range(self.dim):\n                perm = np.arange(n0 - 1)\n                self.rng.shuffle(perm)\n                strata = (perm + self.rng.random(n0 - 1)) / (n0 - 1)\n                samples[:, d] = lb[d] + strata * (ub[d] - lb[d])\n            for i in range(n0 - 1):\n                X.append(samples[i].copy())\n\n        # ensure at least one purely uniform random sample\n        X.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        for x in X:\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(x)\n            F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # If no finite evaluations, try center explicitly (if budget allows)\n        if not any(np.isfinite(np.array(F))):\n            if self.evals < self.budget:\n                f = safe_eval(center)\n                F.append(f)\n                if np.isfinite(f):\n                    self.f_opt = float(f)\n                    self.x_opt = center.copy()\n\n        # short-term adaptation variables\n        gscale = 0.8  # global short-term scale multiplier\n        adapt_window = 50\n        success_window = []\n        stagnation_limit = max(200, int(5 * self.dim))\n        iter_since_improve = 0\n\n        # main loop\n        while self.evals < self.budget:\n            # current arrays\n            X_np = np.asarray(X, dtype=float)\n            F_np = np.asarray(F, dtype=float)\n\n            N = len(X)\n            if N == 0:\n                # fallback random\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X.append(cand)\n                F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = cand.copy()\n                continue\n\n            finite_mask = np.isfinite(F_np)\n            finite_idx = np.where(finite_mask)[0]\n\n            # If no finite points, propose a random candidate\n            if finite_idx.size == 0:\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X.append(cand)\n                F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = cand.copy()\n                continue\n\n            # per-dim adaptive scale: robust spread from IQR -> approx std\n            try:\n                q25 = np.percentile(X_np[finite_idx], 25, axis=0)\n                q75 = np.percentile(X_np[finite_idx], 75, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approx std from IQR\n                per_dim_scale[np.isnan(per_dim_scale)] = 0.0\n            except Exception:\n                per_dim_scale = np.zeros(self.dim, dtype=float)\n\n            # fallback small positive where needed\n            mask_small = per_dim_scale <= 1e-12\n            if np.any(mask_small):\n                per_dim_scale[mask_small] = span[mask_small] * 0.05 + 1e-6\n\n            per_dim_scale_abs = per_dim_scale * gscale + 1e-12\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_F = F_np[finite_idx]\n            order = np.argsort(finite_F)  # 0 is best\n            ranks = np.empty_like(order)\n            ranks[order] = np.arange(len(order))\n            beta = 0.6 + 0.4 * (self.dim / max(10, self.dim))\n            w = np.exp(-beta * (ranks / max(1, len(ranks) - 1)))\n            if np.any(w < 0) or np.all(w == 0):\n                probs = np.ones_like(w) / len(w)\n            else:\n                probs = w / np.sum(w)\n            sel = self.rng.choice(finite_idx, p=probs)\n            base_idx = int(sel)\n            base = X_np[base_idx].copy()\n\n            # pick donors for DE-like moves (prefer finite ones)\n            other_idxs = [i for i in range(N) if i != base_idx and np.isfinite(F[i])]\n            if len(other_idxs) >= 2:\n                dsel = self.rng.choice(other_idxs, size=2, replace=False)\n                donor1 = X_np[dsel[0]].copy()\n                donor2 = X_np[dsel[1]].copy()\n            elif len(other_idxs) == 1:\n                donor1 = X_np[other_idxs[0]].copy()\n                donor2 = self._uniform_array(lb, ub)\n            else:\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type\n            r = self.rng.random()\n            cand = base.copy()\n\n            # LOCAL move (anisotropic Gaussian with slight pull to mean)\n            if r < 0.62:\n                # slight attraction to archive mean\n                mean_archive = np.nanmean(X_np[finite_idx], axis=0)\n                if np.any(np.isnan(mean_archive)):\n                    mean_archive = center.copy()\n                attract = (mean_archive - base) * (0.05 * self.rng.random())\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale_abs\n                cand = base + attract + noise\n\n                # Occasionally do PCA-guided elite moves\n                if self.rng.random() < 0.12 and len(finite_idx) >= min(3, self.dim):\n                    k_elites = min(max(2, int(0.12 * len(finite_idx))), len(finite_idx))\n                    elite_idx = np.argsort(F_np[finite_idx])[:k_elites]\n                    elites = X_np[finite_idx][elite_idx]\n                    centered = elites - np.mean(elites, axis=0)\n                    try:\n                        cov = np.cov(centered, rowvar=False)\n                        # regularize\n                        cov += np.eye(self.dim) * (1e-8 * (np.mean(np.diag(cov)) + 1e-6))\n                        vals, vecs = np.linalg.eigh(cov)\n                        # sort descending\n                        order_eig = np.argsort(vals)[::-1]\n                        vals = vals[order_eig]\n                        vecs = vecs[:, order_eig]\n                        top = min(3, len(vals))\n                        coeffs = self.rng.normal(0.0, 1.0, size=top) * (gscale * 0.8)\n                        perturb = vecs[:, :top] @ (coeffs * np.sqrt(np.maximum(vals[:top], 1e-12)))\n                        pick_el = elites[self.rng.integers(len(elites))].copy()\n                        cand2 = pick_el + perturb\n                        # accept this PCA-derived candidate with some probability\n                        if self.rng.random() < 0.6:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.28:\n                    Fscale = 0.5 + 1.1 * self.rng.random()\n                    mask_comp = self.rng.random(self.dim) < (0.15 + 0.35 * self.rng.random())\n                    de_part = Fscale * (donor1 - donor2)\n                    if np.any(mask_comp):\n                        cand[mask_comp] = base[mask_comp] + de_part[mask_comp] + self.rng.normal(0, 0.01, size=np.sum(mask_comp))\n\n                # occasionally perform a tempered Cauchy jump after DE-like step\n                if self.rng.random() < 0.06:\n                    cauch = np.random.standard_cauchy(size=self.dim)\n                    cauch = np.clip(cauch, -8.0, 8.0)\n                    cand += cauch * per_dim_scale_abs * (0.3 + 0.9 * self.rng.random(self.dim))\n\n            else:\n                # GLOBAL moves: DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                if r < 0.86:\n                    # DE-like global differential move (more exploratory)\n                    global_F = 0.8 + 1.4 * self.rng.random()\n                    mask_replace = self.rng.random(self.dim) < 0.15\n                    cand = base.copy()\n                    if np.any(mask_replace):\n                        cand[mask_replace] = donor1[mask_replace]\n                    # gaussian jitter scaled by per-dim\n                    cand += self.rng.normal(0, 0.02, size=self.dim) * per_dim_scale_abs\n                    # occasional tempered Cauchy\n                    if self.rng.random() < 0.12:\n                        cauch = np.random.standard_cauchy(size=self.dim)\n                        cauch = np.clip(cauch, -8.0, 8.0)\n                        cand += cauch * per_dim_scale_abs * (0.6 + 1.2 * self.rng.random(self.dim))\n                else:\n                    # ARCHIVE_MIX or CAUCHY global jumps\n                    pick_idx = base_idx\n                    if N > 1:\n                        pick_idx = self.rng.integers(0, N)\n                        if pick_idx == base_idx:\n                            pick_idx = (pick_idx + 1) % N\n                    donor = X_np[pick_idx].copy()\n                    cand = donor.copy()\n                    # inject uniform in some dims to diversify\n                    rand_dims = self.rng.random(self.dim) < 0.35\n                    if np.any(rand_dims):\n                        cand[rand_dims] = lb[rand_dims] + self.rng.random(np.sum(rand_dims)) * (ub[rand_dims] - lb[rand_dims])\n                    # tempered Cauchy global jump\n                    if self.rng.random() < 0.5:\n                        cauch = np.random.standard_cauchy(size=self.dim)\n                        cauch = np.clip(cauch, -8.0, 8.0)\n                        scale_factors = per_dim_scale_abs * (0.4 + 0.6 * self.rng.random(self.dim))\n                        cand += cauch * scale_factors\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb[bad], ub[bad]) if np.ndim(lb) > 0 else self._uniform_array(lb, ub)[bad]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X.append(cand)\n            F.append(f)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success: improving global best OR being in top 25% of current finite vals\n            finite_vals = np.array([v for v in F if np.isfinite(v)])\n            success = False\n            if np.isfinite(f):\n                if improved:\n                    success = True\n                elif finite_vals.size >= 4:\n                    thr = np.percentile(finite_vals, 25)\n                    success = (f <= thr)\n            success_window.append(1 if success else 0)\n            if len(success_window) > adapt_window:\n                success_window.pop(0)\n\n            # prune archive if too large: keep best and some random selection for diversity\n            if len(X) > max(self.archive_max, 2 * self.dim + 10):\n                F_np = np.asarray(F, dtype=float)\n                order_all = np.argsort(F_np)  # NaNs go to end\n                keep_best = int(0.6 * self.archive_max)\n                keep_idx = list(order_all[:keep_best])\n                remaining = [int(i) for i in order_all[keep_best:] if i not in keep_idx]\n                self.rng.shuffle(remaining)\n                # add some random remaining and some spread ones\n                extra = remaining[: max(0, self.archive_max - len(keep_idx))]\n                keep_idx.extend(extra)\n                keep_idx = sorted(set(keep_idx))\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_window) == adapt_window and (self.evals % adapt_window == 0):\n                win_sum = sum(success_window)\n                rate = win_sum / float(adapt_window)\n                if rate > 0.45:\n                    gscale *= 0.85\n                elif rate < 0.12:\n                    gscale *= 1.18\n                gscale *= (1.0 + (self.rng.random() - 0.5) * 0.06)\n                gscale = float(np.clip(gscale, 1e-6, 1e2))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit and self.evals < self.budget:\n                iter_since_improve = 0\n                # create injected candidates (prefer perturbations of best)\n                n_inject = min(12, max(4, int(3 + self.dim / 2)))\n                injects = []\n                for k in range(n_inject):\n                    if self.rng.random() < 0.4 or self.x_opt is None:\n                        # random uniform\n                        injects.append(self._uniform_array(lb, ub))\n                    else:\n                        # perturb elite/best\n                        factor = 0.1 + 0.9 * (k / max(1, n_inject - 1))\n                        perturb = self.rng.normal(0, 1.0, size=self.dim) * per_dim_scale_abs * (1.0 + factor * 2.0)\n                        injects.append(self._reflect_bounds(self.x_opt + perturb, lb, ub))\n                for cand_in in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_in = safe_eval(cand_in)\n                    X.append(cand_in)\n                    F.append(f_in)\n                    if np.isfinite(f_in) and f_in < self.f_opt:\n                        self.x_opt = cand_in.copy()\n                        self.f_opt = float(f_in)\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale = float(np.clip(gscale * (1.0 + 0.25 * self.rng.random()), 1e-6, 1e3))\n                success_window = []\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            F_np = np.asarray(F, dtype=float)\n            finite_mask = np.isfinite(F_np)\n            if np.any(finite_mask):\n                idx = np.argmin(F_np[finite_mask])\n                fin_idx = np.where(finite_mask)[0][idx]\n                self.x_opt = X[fin_idx].copy()\n                self.f_opt = float(F[fin_idx])\n            else:\n                # if nothing finite, return center (and try to evaluate if budget remains)\n                self.x_opt = center.copy()\n                if self.evals < self.budget:\n                    f = safe_eval(center)\n                    if np.isfinite(f):\n                        self.f_opt = float(f)\n                    else:\n                        self.f_opt = np.inf\n                else:\n                    self.f_opt = np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "38656e56-459c-41c1-b988-7d449b341b27", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n\n        # trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try multiple common patterns then fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # b may be object with lb/ub or 2xN array\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    arr = np.asarray(b, dtype=float)\n                    if arr.ndim == 2 and arr.shape[0] == 2:\n                        lb = arr[0].astype(float)\n                        ub = arr[1].astype(float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if (lb is None) or (ub is None):\n            try:\n                lb = np.asarray(func.lower_bounds, dtype=float)\n                ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                lb = lb if lb is not None else None\n                ub = ub if ub is not None else None\n\n        if (lb is None) or (ub is None):\n            try:\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                pass\n\n        # Fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Broadcast scalars to arrays if needed\n        lb = np.asarray(lb, dtype=float).ravel()\n        ub = np.asarray(ub, dtype=float).ravel()\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n\n        # If mismatch, pad/truncate\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, -5.0, dtype=float)\n            tmp[: min(len(lb), self.dim)] = lb.flat[: min(len(lb), self.dim)]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, 5.0, dtype=float)\n            tmp[: min(len(ub), self.dim)] = ub.flat[: min(len(ub), self.dim)]\n            ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = np.asarray(x, dtype=float).copy()\n        # iterative reflection\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            # reflect low: new = 2*lb - x\n            if np.any(low_mask):\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if np.any(high_mask):\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            arr = self.rng.random(self.dim)\n            return lb + arr * (ub - lb)\n        else:\n            arr = self.rng.random((n, self.dim))\n            return lb + arr * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float).ravel()[: self.dim].copy()\n            try:\n                f = func(x)\n            except Exception:\n                # count this as an evaluation and treat as invalid\n                self.evals += 1\n                return np.inf\n            # count eval and convert\n            self.evals += 1\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            return f\n\n        # initialization\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # seeding: stratified per-dim independent (cheap LHS-like)\n        n0 = max(2, min(self.init_samples, max(2, self.budget // 20)))\n        X = []\n        F = []\n\n        # include center\n        center = 0.5 * (lb + ub)\n\n        # stratified per-dim independent samples\n        if n0 - 1 > 0:\n            # create n0-1 stratified samples: for each dim choose random strata index and a random offset\n            K = n0 - 1\n            samples = np.empty((K, self.dim), dtype=float)\n            for d in range(self.dim):\n                strata = (np.arange(K) + self.rng.random(K)) / K  # values in (0,1)\n                self.rng.shuffle(strata)\n                samples[:, d] = lb[d] + strata * (ub[d] - lb[d])\n            for i in range(K):\n                X.append(samples[i].copy())\n\n        # ensure at least one purely uniform random sample\n        X.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        X = np.vstack(X)\n        F = np.full((X.shape[0],), np.inf, dtype=float)\n        for i, x in enumerate(X):\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(x)\n            F[i] = f\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If no finite evaluations, try center (counts toward budget)\n        if not np.isfinite(self.f_opt) and self.evals < self.budget:\n            f = safe_eval(center)\n            if np.isfinite(f):\n                self.f_opt = float(f)\n                self.x_opt = center.copy()\n            # append center to archive\n            X = np.vstack([X, center.copy()])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n        # short-term adaptation variables\n        gscale = 0.8  # global short-term scale multiplier\n        p_local = 0.72\n        adapt_window = 50\n        success_window = [0] * adapt_window\n        successes = 0\n        iter_since_improve = 0\n        stagnation_limit = max(200, int(5 * self.dim))\n        beta = 3.0  # selection sharpness\n        elites_frac = 0.15\n        cauchy_clip = 8.0\n\n        # main loop\n        while self.evals < self.budget:\n            N = X.shape[0]\n            finite_idx = np.where(np.isfinite(F))[0]\n            if finite_idx.size == 0:\n                # fallback random candidate\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X = np.vstack([X, cand])\n                F = np.concatenate([F, np.array([f], dtype=float)])\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = cand.copy()\n                continue\n\n            # per-dim adaptive scale: robust spread from IQR -> approx std\n            try:\n                q75 = np.percentile(X[finite_idx], 75, axis=0)\n                q25 = np.percentile(X[finite_idx], 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr / 1.349  # approx std\n                per_dim_scale = np.maximum(per_dim_scale, 1e-12)\n                # fallback small positive where too small\n                mask_small = per_dim_scale < (span * 0.02)\n                per_dim_scale[mask_small] = span[mask_small] * 0.05 + 1e-9\n            except Exception:\n                per_dim_scale = span * 0.25\n            per_dim_scale_abs = per_dim_scale * gscale + 1e-12\n\n            # choose base index biased to better ranks (softmax on negative F)\n            finite_F = F[finite_idx]\n            order = np.argsort(finite_F)  # ascending (best first)\n            ranks = np.empty_like(order)\n            ranks[order] = np.arange(1, len(order) + 1)  # best = 1\n            denom = max(1, len(ranks) - 1)\n            w = np.exp(-beta * (ranks - 1) / denom)\n            if np.any(w < 0) or np.all(w == 0):\n                probs = np.ones_like(w) / len(w)\n            else:\n                probs = w / np.sum(w)\n            sel = self.rng.choice(finite_idx, p=probs)\n            base_idx = int(sel)\n            base = X[base_idx].copy()\n\n            # pick donors for DE-like moves (prefer finite ones)\n            other_idxs = [i for i in range(N) if i != base_idx]\n            if len(other_idxs) >= 2:\n                dsel = self.rng.choice(other_idxs, size=2, replace=False)\n                donor1 = X[dsel[0]].copy()\n                donor2 = X[dsel[1]].copy()\n            elif len(other_idxs) == 1:\n                donor1 = X[other_idxs[0]].copy()\n                donor2 = self._uniform_array(lb, ub)\n            else:\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type\n            r = self.rng.random()\n            cand = base.copy()\n\n            if r < p_local:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                mean_archive = np.nanmean(X[finite_idx], axis=0)\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale_abs\n                # slight attraction to archive mean\n                alpha = 0.08 * self.rng.random()\n                cand = base + alpha * (mean_archive - base) + noise * (0.9 + 0.6 * self.rng.random(self.dim))\n\n                # Occasionally do PCA-guided elite moves\n                if self.rng.random() < 0.22 and finite_idx.size >= 3:\n                    elites_k = max(2, min(int(np.ceil(elites_frac * finite_idx.size)), finite_idx.size))\n                    sorted_idx = np.argsort(F[finite_idx])[:elites_k]\n                    elites = X[finite_idx][sorted_idx]  # shape (k, dim)\n                    center_elite = np.mean(elites, axis=0)\n                    # covariance and eigen decomposition\n                    cov = np.cov((elites - center_elite).T) if elites_k > 1 else np.eye(self.dim) * (per_dim_scale_abs ** 2)\n                    # regularize\n                    cov = cov + np.eye(self.dim) * (1e-8 * (np.mean(np.diag(cov)) + 1e-6))\n                    try:\n                        vals, vecs = np.linalg.eigh(cov)\n                        order_eig = np.argsort(vals)[::-1]\n                        vals = vals[order_eig]\n                        vecs = vecs[:, order_eig]\n                        top = max(1, min(self.dim, int(max(1, np.sum(vals > 1e-12)))))\n                        coeffs = self.rng.normal(0.0, 1.0, size=top) * (gscale * 0.6)\n                        perturb = vecs[:, :top] @ (coeffs * np.sqrt(np.maximum(vals[:top], 1e-12)))\n                        # shift around a random elite center sometimes\n                        if self.rng.random() < 0.7:\n                            center_choice = elites[self.rng.integers(0, elites.shape[0])]\n                            cand = center_choice + 0.6 * perturb\n                        else:\n                            cand = center_elite + 0.8 * perturb\n                    except Exception:\n                        # fallback keep cand\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.32:\n                    Fscale = 0.5 + 1.1 * self.rng.random()\n                    mask_comp = self.rng.random(self.dim) < (0.15 + 0.35 * self.rng.random())\n                    de_part = Fscale * (donor1 - donor2)\n                    if np.any(mask_comp):\n                        cand[mask_comp] = base[mask_comp] + de_part[mask_comp] + self.rng.normal(0, 0.01, size=np.sum(mask_comp))\n\n                # occasionally perform a tempered Cauchy jump after local step\n                if self.rng.random() < 0.06:\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    cauch = np.clip(cauch, -cauchy_clip, cauchy_clip)\n                    cand = cand + cauch * per_dim_scale_abs * (0.6 + 1.2 * self.rng.random(self.dim))\n\n            else:\n                # GLOBAL moves: DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                if self.rng.random() < 0.6:\n                    # DE-like global differential move (more exploratory)\n                    Fscale = 0.9 + 1.2 * self.rng.random()\n                    cand = base + Fscale * (donor1 - donor2)\n                    # occasionally replace some components with donor1 directly\n                    mask_replace = self.rng.random(self.dim) < 0.12\n                    cand[mask_replace] = donor1[mask_replace]\n                    # gaussian jitter\n                    cand += self.rng.normal(0, 0.02, size=self.dim) * per_dim_scale_abs\n                    # tempered Cauchy occasional\n                    if self.rng.random() < 0.08:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        cauch = np.clip(cauch, -cauchy_clip, cauchy_clip)\n                        cand += cauch * per_dim_scale_abs * (0.8 + 1.6 * self.rng.random(self.dim))\n                else:\n                    # ARCHIVE_MIX or CAUCHY global jumps\n                    pick_idx = self.rng.integers(0, N)\n                    if pick_idx == base_idx and N > 2:\n                        pick_idx = (pick_idx + 1) % N\n                    other = X[pick_idx]\n                    # mix with base and a global Cauchy perturbation\n                    mix_alpha = 0.2 + 0.7 * self.rng.random()\n                    cand = base * (1 - mix_alpha) + other * mix_alpha\n                    if self.rng.random() < 0.35:\n                        # inject uniform in a random subset of dimensions\n                        mask_u = self.rng.random(self.dim) < 0.12\n                        if np.any(mask_u):\n                            cand[mask_u] = self._uniform_array(lb, ub)[mask_u]\n                    if self.rng.random() < 0.6:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        cauch = np.clip(cauch, -cauchy_clip, cauchy_clip)\n                        scale_factors = per_dim_scale_abs * (0.4 + 0.6 * self.rng.random(self.dim))\n                        cand += cauch * scale_factors\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                rand = self._uniform_array(lb, ub)\n                cand[bad] = rand[bad]\n\n            # reflect/clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best OR placed among top 25% of finite vals\n            finite_vals = F[np.isfinite(F)]\n            if finite_vals.size > 0:\n                thr = np.percentile(finite_vals, 25)\n                success = (np.isfinite(f) and ((f <= thr) or improved))\n            else:\n                success = False\n\n            # update sliding window\n            success_window.append(1 if success else 0)\n            if len(success_window) > adapt_window:\n                success_window.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if len(F) > self.archive_max:\n                keep_best = int(0.6 * self.archive_max)\n                order_all = np.argsort(F)\n                keep_idx = list(order_all[:keep_best])\n                remaining = list(order_all[keep_best:])\n                self.rng.shuffle(remaining)\n                need = self.archive_max - len(keep_idx)\n                if need > 0:\n                    extra = remaining[:need]\n                    keep_idx.extend(extra)\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_window) == adapt_window:\n                rate = sum(success_window) / adapt_window\n                if rate > 0.45:\n                    # too successful -> tighten (search becomes local)\n                    gscale *= 0.90\n                elif rate < 0.12:\n                    # unsuccessful -> enlarge (more exploration)\n                    gscale *= 1.18\n                else:\n                    # modest: small drift toward 1.0\n                    gscale *= 0.98 + 0.06 * (0.5 - rate)\n                # jitter and clip\n                gscale *= (1.0 + (self.rng.random() - 0.5) * 0.06)\n                gscale = float(np.clip(gscale, 1e-6, 1e2))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit and self.evals < self.budget:\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = min(12, max(4, int(3 + self.dim / 2)))\n                injects = []\n                for k in range(n_inject):\n                    if (k % 2) == 0 and self.x_opt is not None:\n                        # perturb best/elite\n                        factor = (1.0 + 4.0 * (k / max(1, n_inject - 1)))\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale * gscale * factor\n                        injects.append(self._reflect_bounds(self.x_opt + perturb, lb, ub))\n                    else:\n                        # random uniform\n                        injects.append(self._uniform_array(lb, ub))\n                # evaluate them (append while respecting budget)\n                for cand_inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f = safe_eval(cand_inj)\n                    X = np.vstack([X, cand_inj])\n                    F = np.concatenate([F, np.array([f], dtype=float)])\n                    if np.isfinite(f) and f < self.f_opt:\n                        self.f_opt = float(f)\n                        self.x_opt = cand_inj.copy()\n                        improved = True\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale *= 1.6\n                gscale = float(np.clip(gscale, 1e-6, 1e3))\n                success_window = [0] * adapt_window\n                iter_since_improve = 0\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                idx = np.argmin(F[finite_mask])\n                fin_idx = np.where(finite_mask)[0][idx]\n                self.x_opt = X[fin_idx].copy()\n                self.f_opt = float(F[fin_idx])\n            else:\n                self.x_opt = center.copy()\n                self.f_opt = safe_eval(center) if self.evals < self.budget else np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6399755e-0ccb-4fd1-abd2-a69d97629e81", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # initial samples (including center and 1 uniform)\n        if init_samples is None:\n            self.init_samples = max(8, min(32, 2 * self.dim + 4))\n        else:\n            self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # try common attributes, otherwise default to [-5,5]^dim\n        lb = None\n        ub = None\n        # typical BBOB wrapper uses func.bounds.lb / func.bounds.ub\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # try lower_bounds / upper_bounds\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # try simple attributes lb/ub\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n        # pad/truncate to dim if necessary\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, -5.0, dtype=float)\n            tmp[:min(self.dim, lb.size)] = lb.flat[:min(self.dim, lb.size)]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, 5.0, dtype=float)\n            tmp[:min(self.dim, ub.size)] = ub.flat[:min(self.dim, ub.size)]\n            ub = tmp\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if np.any(high_mask):\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return self.rng.random(self.dim) * (ub - lb) + lb\n        else:\n            u = self.rng.random((n, self.dim))\n            return lb + u * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(np.asarray(x, dtype=float))\n            except Exception:\n                # if function call fails, still count as an evaluation\n                self.evals += 1\n                return np.inf\n            # count eval\n            self.evals += 1\n            try:\n                f = float(f)\n            except Exception:\n                return np.inf\n            return f\n\n        # initialization\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # create initial archive list\n        X_list = []\n        F_list = []\n\n        center = 0.5 * (lb + ub)\n\n        # include center\n        X_list.append(center.copy())\n\n        # stratified per-dim independent sampling for init_samples - 1\n        n0 = max(2, min(self.init_samples, max(2, self.budget)))\n        if n0 > 1:\n            # n0 - 1 additional stratified samples (per-dim)\n            m = n0 - 1\n            # for each dimension, create a permutation of strata indices\n            strata_idx = np.vstack([self.rng.permutation(m) for _ in range(self.dim)]).T\n            # sample uniform within each stratum\n            samples = np.empty((m, self.dim), dtype=float)\n            for d in range(self.dim):\n                strata = (strata_idx[:, d] + self.rng.random(m)) / float(m)\n                samples[:, d] = lb[d] + strata * (ub[d] - lb[d])\n            for i in range(m):\n                X_list.append(samples[i].copy())\n\n        # ensure at least one purely uniform random sample\n        X_list.append(self._uniform_array(lb, ub))\n\n        # Evaluate initial population (respecting budget)\n        for x in X_list:\n            if self.evals >= self.budget:\n                # stop evaluating further\n                F_list.append(np.inf)\n            else:\n                f = safe_eval(x)\n                F_list.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n\n        # If no finite evaluations, try evaluating center explicitly as fallback\n        if not any(np.isfinite(np.array(F_list, dtype=float))):\n            if self.evals < self.budget:\n                f = safe_eval(center)\n                # append center if not present\n                F_list[0] = f\n                X_list[0] = center.copy()\n                if np.isfinite(f):\n                    self.f_opt = float(f)\n                    self.x_opt = center.copy()\n\n        # short-term adaptation variables\n        gscale = 0.8  # global short-term scale multiplier\n        p_local = 0.72\n        success_window = []\n        adapt_window = max(10, min(40, int(6 + self.dim)))\n        successes = 0\n        iter_since_improve = 0\n        stagnation_limit = max(40, 6 * self.dim)\n        max_iters = max(1, self.budget - self.evals)\n\n        # main loop\n        while self.evals < self.budget:\n            # convert to arrays for analysis\n            X = np.asarray(X_list, dtype=float)\n            F = np.asarray(F_list, dtype=float)\n            N = X.shape[0]\n\n            if N == 0:\n                # fallback random\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X_list.append(cand)\n                F_list.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = cand.copy()\n                continue\n\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0]\n            finite_F = F[finite_idx] if finite_idx.size > 0 else np.array([])\n\n            # If there are no finite evaluations, propose a random candidate\n            if finite_idx.size == 0:\n                cand = self._uniform_array(lb, ub)\n                f = safe_eval(cand)\n                X_list.append(cand)\n                F_list.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = cand.copy()\n                continue\n\n            # per-dim adaptive scale: robust spread from IQR -> approx std\n            try:\n                q25 = np.percentile(X[finite_idx], 25, axis=0)\n                q75 = np.percentile(X[finite_idx], 75, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approx std from IQR\n            except Exception:\n                per_dim_scale = np.std(X[finite_idx], axis=0)\n\n            # fallback small positive\n            mask_small = (per_dim_scale <= 0) | ~np.isfinite(per_dim_scale)\n            if np.any(mask_small):\n                per_dim_scale[mask_small] = 0.1 * span[mask_small] + 1e-8\n\n            per_dim_scale_abs = per_dim_scale * gscale + 1e-12\n\n            # choose base index biased to better ranks (exponential on rank)\n            order = np.argsort(finite_F)  # ascending: best first\n            ranks = np.empty_like(order)\n            ranks[order] = np.arange(order.size)\n            # weight by exponential on rank\n            beta = 0.6 + 0.4 * (self.dim / max(10, self.dim))\n            w = np.exp(-beta * ranks.astype(float))\n            if np.any(w < 0) or np.all(w == 0):\n                w = np.ones_like(w)\n            w = w / np.sum(w)\n            sel = self.rng.choice(finite_idx, p=w)\n            base_idx = int(sel)\n            base = X[base_idx].copy()\n\n            # pick donors for DE-like moves (prefer finite ones)\n            other_idxs = list(finite_idx[finite_idx != base_idx])\n            if len(other_idxs) >= 2:\n                dsel = self.rng.choice(other_idxs, size=2, replace=False)\n                donor1 = X[dsel[0]].copy()\n                donor2 = X[dsel[1]].copy()\n            elif len(other_idxs) == 1:\n                donor1 = X[other_idxs[0]].copy()\n                donor2 = self._uniform_array(lb, ub)\n            else:\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type\n            r = self.rng.random()\n\n            cand = base.copy()\n\n            if r < p_local:\n                # LOCAL move (anisotropic Gaussian with slight pull to mean)\n                archive_mean = np.mean(X[finite_idx], axis=0)\n                pull = 0.08 * (archive_mean - base)  # slight attraction to mean\n                # anisotropic gaussian noise\n                scale_factors = per_dim_scale_abs * (0.09 + 0.9 * self.rng.random(self.dim))\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * scale_factors\n                cand = base + pull + noise\n\n                # Occasionally do PCA-guided elite moves\n                if finite_idx.size >= 4 and self.rng.random() < 0.22:\n                    elites_k = max(2, int(min(6, max(2, finite_idx.size // 4))))\n                    elites_idx = finite_idx[np.argsort(F[finite_idx])[:elites_k]]\n                    elites = X[elites_idx]\n                    centered = elites - np.mean(elites, axis=0)\n                    try:\n                        cov = np.cov(centered, rowvar=False)\n                        # small regularizer\n                        cov += np.eye(self.dim) * (1e-8 * (np.mean(np.diag(cov)) + 1e-6))\n                        vals, vecs = np.linalg.eigh(cov)\n                        # sample along principal subspace\n                        top = max(1, min(self.dim, int(np.clip(elites_k, 1, self.dim))))\n                        coeffs = self.rng.normal(0.0, 1.0, size=top) * (gscale * 0.8)\n                        perturb = vecs[:, -top:] @ (coeffs * np.sqrt(np.maximum(vals[-top:], 1e-12)))\n                        elite_center = elites[self.rng.integers(0, elites.shape[0])]\n                        cand2 = elite_center + perturb\n                        if self.rng.random() < 0.5:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.38:\n                    Fscale = 0.5 + 1.1 * self.rng.random()\n                    mask_comp = self.rng.random(self.dim) < (0.15 + 0.35 * self.rng.random())\n                    de_part = Fscale * (donor1 - donor2)\n                    if np.any(mask_comp):\n                        cand[mask_comp] = base[mask_comp] + de_part[mask_comp] + self.rng.normal(0, 0.01, size=np.sum(mask_comp))\n\n                # occasionally perform a tempered Cauchy jump after DE-like step\n                if self.rng.random() < 0.06:\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    cand += cauch * per_dim_scale_abs * (0.6 + 1.2 * self.rng.random(self.dim))\n\n            else:\n                # GLOBAL moves: DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                if self.rng.random() < 0.65:\n                    # DE-like global differential move (more exploratory)\n                    global_F = 0.8 + 1.4 * self.rng.random()\n                    cand = base + global_F * (donor1 - donor2)\n                    # occasionally replace some components with donor1 directly\n                    if self.rng.random() < 0.18:\n                        mask_comp = self.rng.random(self.dim) < (0.08 + 0.22 * self.rng.random())\n                        if np.any(mask_comp):\n                            cand[mask_comp] = donor1[mask_comp]\n                    # small gaussian jitter\n                    cand += self.rng.normal(0.0, per_dim_scale_abs * 0.05)\n                    # occasionally tempered Cauchy jump\n                    if self.rng.random() < 0.08:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        cand += cauch * per_dim_scale_abs * (0.8 + 1.6 * self.rng.random(self.dim))\n                else:\n                    # ARCHIVE mix or tempered Cauchy global jump\n                    # pick a donor randomly (prefer not base)\n                    pick_idx = self.rng.integers(0, N)\n                    if pick_idx == base_idx:\n                        pick_idx = (pick_idx + 1) % N\n                    donor = X[pick_idx].copy()\n                    # mix components\n                    mask_mix = self.rng.random(self.dim) < (0.18 + 0.22 * (self.dim / max(10, self.dim)))\n                    cand = base.copy()\n                    if np.any(mask_mix):\n                        cand[mask_mix] = donor[mask_mix]\n                    # occasionally inject uniform in a few dimensions to diversify\n                    rand_dims = self.rng.random(self.dim) < (0.06 + 0.12 * (self.dim / max(10, self.dim)))\n                    if np.any(rand_dims):\n                        cand[rand_dims] = self._uniform_array(lb, ub)[rand_dims]\n                    # tempered Cauchy global jump from a random archive point or base\n                    if self.rng.random() < 0.12:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        cand += cauch * per_dim_scale_abs * (1.2 + 2.0 * self.rng.random(self.dim))\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb, ub)[bad]\n\n            # reflect/clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X_list.append(cand.copy())\n            F_list.append(f)\n\n            # update best trackers and stagnation\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f):\n                if f <= self.f_opt:\n                    success = True\n                else:\n                    if finite_F.size > 0:\n                        thr = np.percentile(finite_F, 25)\n                        if f <= thr:\n                            success = True\n\n            success_window.append(1 if success else 0)\n            if len(success_window) > adapt_window:\n                success_window.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if len(F_list) > self.archive_max:\n                F_arr = np.asarray(F_list, dtype=float)\n                order_all = np.argsort(F_arr)  # best first (np.inf at end)\n                keep_best = max(3, int(self.archive_max * 0.6))\n                keep_idx = list(order_all[:keep_best])\n                # fill the rest from random among remaining (to keep diversity)\n                remaining = [i for i in range(len(F_arr)) if i not in keep_idx]\n                need = self.archive_max - len(keep_idx)\n                if need > 0 and len(remaining) > 0:\n                    self.rng.shuffle(remaining)\n                    extra = remaining[:need]\n                    keep_idx.extend(extra)\n                # reorder\n                keep_idx = sorted(set(keep_idx))\n                X_list = [X_list[i] for i in keep_idx]\n                F_list = [F_list[i] for i in keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_window) >= adapt_window:\n                win_sum = sum(success_window)\n                rate = win_sum / float(adapt_window)\n                if rate > 0.45:\n                    # too successful -> tighten\n                    gscale *= 0.85\n                elif rate < 0.12:\n                    # unsuccessful -> enlarge\n                    gscale *= 1.18\n                else:\n                    # modest\n                    gscale *= 1.0\n                # jitter and clip\n                gscale *= (1.0 + (self.rng.random() - 0.5) * 0.06)\n                gscale = float(np.clip(gscale, 1e-4, 8.0))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit:\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = max(4, min(12, 2 + self.dim // 2))\n                injects = []\n                for k in range(n_inject):\n                    if (k % 2) == 0 or self.x_opt is None:\n                        # random uniform\n                        injects.append(self._uniform_array(lb, ub))\n                    else:\n                        # perturb elite/best\n                        factor = (1.0 + 6.0 * (k / max(1, n_inject - 1)))\n                        perturb = self.rng.normal(0.0, per_dim_scale_abs * (0.3 + 0.6 * self.rng.random()), size=self.dim)\n                        injects.append(self._reflect_bounds(self.x_opt + perturb * factor, lb, ub))\n                # evaluate them (append while respecting budget)\n                for cand_in in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_in = safe_eval(cand_in)\n                    X_list.append(cand_in.copy())\n                    F_list.append(f_in)\n                    if np.isfinite(f_in) and f_in < self.f_opt:\n                        self.f_opt = float(f_in)\n                        self.x_opt = cand_in.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                iter_since_improve = 0\n                success_window = []\n\n            # loop end\n\n        # final safety: return best finite point found, else return center\n        if self.x_opt is None:\n            F_arr = np.asarray(F_list, dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            if np.any(finite_mask):\n                idx = np.argmin(F_arr)\n                self.x_opt = X_list[idx].copy()\n                self.f_opt = float(F_arr[idx])\n            else:\n                self.x_opt = center.copy()\n                # only evaluate center if budget remains (though probably exhausted)\n                if self.evals < self.budget:\n                    self.f_opt = safe_eval(center)\n                else:\n                    self.f_opt = np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e5967a35-cf0a-495f-8e4d-e45c6b0b30e2", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = max(3, int(init_samples))\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---- utilities ----\n    def _get_bounds(self, func):\n        # Try common interfaces then fallback to [-5,5]^dim\n        lb = None\n        ub = None\n\n        # 1) try func.bounds.lb / func.bounds.ub (array-like)\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n            except Exception:\n                lb = lb\n                ub = ub\n\n        # 2) try lower_bounds / upper_bounds\n        if (lb is None or ub is None) and hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n            try:\n                lb = np.asarray(func.lower_bounds, dtype=float)\n                ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                lb = lb\n                ub = ub\n\n        # 3) try simple attributes lb/ub\n        if (lb is None or ub is None) and hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n            try:\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                pass\n\n        # 4) fallback scalar attributes\n        if (lb is None or ub is None) and hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n            try:\n                lb = np.asarray(func.lower, dtype=float)\n                ub = np.asarray(func.upper, dtype=float)\n            except Exception:\n                pass\n\n        # default fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast scalars to arrays\n            if np.isscalar(lb):\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if np.isscalar(ub):\n                ub = np.full(self.dim, float(ub), dtype=float)\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            # pad/truncate to dim\n            if lb.size != self.dim:\n                tmp = np.full(self.dim, lb.flat[0] if lb.size > 0 else -5.0, dtype=float)\n                tmp[:min(len(lb), self.dim)] = lb.flat[:min(len(lb), self.dim)]\n                lb = tmp\n            if ub.size != self.dim:\n                tmp = np.full(self.dim, ub.flat[0] if ub.size > 0 else 5.0, dtype=float)\n                tmp[:min(len(ub), self.dim)] = ub.flat[:min(len(ub), self.dim)]\n                ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # iterative reflection for any violations (vectorized)\n        x = x.copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            if low_mask.any():\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if high_mask.any():\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=None):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.random((n, self.dim)) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n\n        # evaluation counter and safe evaluator\n        self.evals = 0\n\n        def safe_eval(x):\n            # x: 1D array of length dim\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                # try reshape or broadcast\n                x = np.resize(x, self.dim)\n            # reflect into bounds for safety\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                # count the call as it was attempted if budget allowed\n                # return inf on error\n                self.evals += 1\n                return np.inf\n            self.evals += 1\n            return f\n\n        # initialize archive\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # seeding: cheap LHS-like stratified per-dim independent\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        # create an LHS-like matrix\n        # base strata\n        strata = (np.arange(n0) + 0.5) / float(n0)\n        X = []\n        # Create n0-1 stratified samples + center\n        for i in range(n0 - 1):\n            # per-dim permuted strata\n            u = np.empty(self.dim, dtype=float)\n            for d in range(self.dim):\n                perm = self.rng.permutation(n0 - 1)\n                # select i-th stratum in the permutation, jitter inside\n                r = (perm[i] + self.rng.random()) / float(n0 - 1)\n                u[d] = r\n            x = lb + u * (ub - lb)\n            X.append(x)\n        # ensure center included\n        center = 0.5 * (lb + ub)\n        X.append(center.copy())\n        # ensure at least one purely uniform sample\n        X.append(self._uniform_array(lb, ub))\n        X = np.array(X, dtype=float)\n\n        # Evaluate initial population (respecting budget)\n        F = []\n        for x in X:\n            f = safe_eval(x)\n            F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            if self.evals >= self.budget:\n                break\n\n        F = np.array(F, dtype=float)\n\n        # If no finite evaluations, try center explicitly (if budget allows)\n        finite_mask = np.isfinite(F)\n        if not np.any(finite_mask) and self.evals < self.budget:\n            f = safe_eval(center)\n            # append center\n            X = np.vstack([X, center])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n            if np.isfinite(f):\n                self.f_opt = float(f)\n                self.x_opt = center.copy()\n\n        # short-term adaptation variables\n        gscale = 0.8  # global short-term scale multiplier\n        adapt_window = 50\n        success_window = []\n        iter_since_improve = 0\n        stagnation_limit = max(200, int(5 * self.dim))\n        max_iters = max(1, self.budget - self.evals)\n\n        # main loop: produce candidates until budget exhausted\n        itr = 0\n        while self.evals < self.budget:\n            itr += 1\n            N = len(X)\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0]\n            # robust per-dim spread estimate (IQR -> approximate std)\n            if finite_idx.size >= 2:\n                try:\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    per_dim_scale = (q75 - q25) / 1.349  # approx std\n                except Exception:\n                    per_dim_scale = np.std(X[finite_idx], axis=0)\n            else:\n                # fallback to default fraction of range\n                per_dim_scale = 0.1 * (ub - lb)\n\n            per_dim_scale[np.isnan(per_dim_scale)] = 1e-8\n            # absolute scale to multiply small gaussian jitter\n            per_dim_scale_abs = np.maximum(per_dim_scale, 1e-12)\n\n            # choose a base index biased to better ranks (softmax on -F)\n            if finite_idx.size > 0:\n                finite_F = F[finite_idx]\n                # smaller is better -> use negative values\n                vals = -finite_F.copy()\n                # stabilize\n                vals = vals - np.max(vals)\n                weights = np.exp(vals / (np.std(vals) + 1e-12))\n                if np.all(weights == 0) or np.any(np.isnan(weights)):\n                    probs = None\n                else:\n                    probs = weights / np.sum(weights)\n                if probs is None:\n                    base_idx = int(self.rng.choice(finite_idx))\n                else:\n                    base_idx = int(self.rng.choice(finite_idx, p=probs))\n            else:\n                # no finite evaluations yet -> pick a random archive point\n                base_idx = int(self.rng.integers(0, N))\n\n            base = X[base_idx].copy()\n\n            # pick donors for DE-like moves (prefer finite ones)\n            other_idxs = [i for i in range(N) if i != base_idx]\n            if len(other_idxs) >= 2:\n                dsel = self.rng.choice(other_idxs, size=2, replace=False)\n                donor1 = X[dsel[0]].copy()\n                donor2 = X[dsel[1]].copy()\n            elif len(other_idxs) == 1:\n                donor1 = X[other_idxs[0]].copy()\n                donor2 = self._uniform_array(lb, ub)\n            else:\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type: local vs global\n            if self.rng.random() < 0.62:\n                # LOCAL move (anisotropic Gaussian with slight pull to archive mean)\n                if finite_idx.size >= 1:\n                    mean_archive = np.nanmean(X[finite_idx], axis=0)\n                else:\n                    mean_archive = center\n\n                # pull term toward mean_archive\n                pull = 0.08 * (mean_archive - base) * self.rng.random()\n                # anisotropic gaussian noise scaled by per-dim scale and gscale\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale_abs * gscale\n                cand = base + pull + noise\n\n                # PCA-guided elite perturbation occasionally\n                if self.rng.random() < 0.20 and finite_idx.size >= 4:\n                    elites_k = min(max(3, int(0.25 * len(finite_idx))), len(finite_idx))\n                    elite_idx = np.argsort(F[finite_idx])[:elites_k]\n                    elites = X[finite_idx][elite_idx]  # shape (k, dim)\n                    # center elites and compute principal components\n                    centered = elites - np.mean(elites, axis=0)\n                    try:\n                        cov = np.cov(centered, rowvar=False)\n                        vals, vecs = np.linalg.eigh(cov + np.eye(self.dim) * 1e-12)\n                        order_eig = np.argsort(-vals)\n                        vals = vals[order_eig]\n                        vecs = vecs[:, order_eig]\n                        top = min(3, self.dim)\n                        coeffs = self.rng.normal(0.0, 1.0, size=top)\n                        perturb = vecs[:, :top] @ (coeffs * np.sqrt(np.maximum(vals[:top], 1e-12))) * (0.6 * gscale)\n                        # shift around a random elite center\n                        pick_el = elites[self.rng.integers(len(elites))].copy()\n                        cand2 = pick_el + perturb\n                        # accept cand2 sometimes as alternative\n                        if self.rng.random() < 0.45:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # small DE-like mixing occasionally inside local\n                if self.rng.random() < 0.18:\n                    Fscale = 0.4 + 1.0 * self.rng.random()\n                    mask_comp = self.rng.random(self.dim) < (0.12 + 0.18 * np.sqrt(self.dim) / max(1, self.dim))\n                    de_part = Fscale * (donor1 - donor2)\n                    cand[mask_comp] = base[mask_comp] + de_part[mask_comp] + \\\n                                      self.rng.normal(0, 0.01, size=np.sum(mask_comp))\n            else:\n                # GLOBAL moves: DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                global_F = 0.8 + 1.4 * self.rng.random()\n                cand = base + global_F * (donor1 - donor2)\n                # occasionally replace some components with donor1 directly\n                mask_replace = self.rng.random(self.dim) < 0.15\n                cand[mask_replace] = donor1[mask_replace]\n                # small gaussian jitter (absolute scaled)\n                cand += self.rng.normal(0, 0.02, size=self.dim) * per_dim_scale_abs\n                # tempered Cauchy jump occasionally\n                if self.rng.random() < 0.16:\n                    # sample from a tempered (clipped) Cauchy-like distribution per-dim\n                    u = self.rng.random(self.dim)\n                    # Cauchy using tan(pi*(u-0.5)), tempered by logistic-like shrink\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    # temper by scale depending on per-dim spread and gscale\n                    scale = 0.5 + 3.0 * (per_dim_scale_abs / (np.max(per_dim_scale_abs) + 1e-12)) * gscale\n                    cauch = cauch * scale\n                    # clip large jumps\n                    cauch = np.clip(cauch, -8.0, 8.0)\n                    cand += cauch\n\n                # archive mix: sometimes jump near a random archive point\n                if N > 1 and self.rng.random() < 0.12:\n                    pick_idx = int(self.rng.integers(0, N))\n                    if pick_idx == base_idx and N > 2:\n                        pick_idx = int(self.rng.choice([i for i in range(N) if i != base_idx]))\n                    donor = X[pick_idx].copy()\n                    rand_dims = self.rng.random(self.dim) < (0.08 + 0.12 * self.dim / max(10, self.dim))\n                    cand[rand_dims] = donor[rand_dims]\n                    # small perturbation around donor\n                    cand += self.rng.normal(0, 0.02, size=self.dim) * per_dim_scale_abs\n\n            # safety: replace any non-finite entries with uniform sample\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = lb[bad] + self.rng.random(np.sum(bad)) * (ub[bad] - lb[bad])\n\n            # reflect/clamp candidate to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                iter_since_improve = 0\n                improved = True\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            finite_vals = F[np.isfinite(F)]\n            success = False\n            if np.isfinite(f):\n                if improved:\n                    success = True\n                elif finite_vals.size >= 4:\n                    threshold = np.percentile(finite_vals, 25)\n                    if f <= threshold:\n                        success = True\n\n            success_window.append(1 if success else 0)\n            if len(success_window) > adapt_window:\n                # keep sliding window\n                success_window.pop(0)\n\n            # adapt gscale occasionally based on short-term success\n            if itr % max(1, int(adapt_window / 5)) == 0 and len(success_window) > 0:\n                rate = sum(success_window) / len(success_window)\n                if rate > 0.45:\n                    # too successful -> focus (tighten the local search)\n                    gscale *= 0.85\n                elif rate < 0.12:\n                    # unsuccessful -> enlarge to explore more\n                    gscale *= 1.18\n                # jitter and clip\n                gscale *= (1.0 + (self.rng.random() - 0.5) * 0.06)\n                gscale = float(np.clip(gscale, 1e-3, 8.0))\n\n            # prune archive if too large: keep best ones and a small random fraction for diversity\n            if len(F) > self.archive_max:\n                order_all = np.argsort(F)\n                keep_best = max(4, int(0.6 * self.archive_max))\n                keep_idx = list(order_all[:keep_best])\n                remaining = list(order_all[keep_best:])\n                # shuffle remaining and take some randoms\n                self.rng.shuffle(remaining)\n                keep_more = int(self.archive_max - len(keep_idx))\n                if keep_more > 0:\n                    keep_idx.extend(remaining[:keep_more])\n                keep_idx = np.array(keep_idx, dtype=int)\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit and self.evals < self.budget:\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = min(12, max(4, int(0.02 * self.budget)))\n                injects = []\n                for k in range(n_inject):\n                    if self.x_opt is None or self.rng.random() < 0.4:\n                        # random uniform\n                        injects.append(self._uniform_array(lb, ub))\n                    else:\n                        # perturb elite/best\n                        factor = (2.0 + 6.0 * (k / max(1, n_inject - 1)))\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale * gscale * factor\n                        injects.append(self._reflect_bounds(self.x_opt + perturb, lb, ub))\n                # evaluate them (append while respecting budget)\n                for cand in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f = safe_eval(cand)\n                    X = np.vstack([X, cand])\n                    F = np.concatenate([F, np.array([f], dtype=float)])\n                    if np.isfinite(f) and f < self.f_opt:\n                        self.f_opt = float(f)\n                        self.x_opt = cand.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale *= 1.25\n                iter_since_improve = 0\n\n            # break if budget exhausted\n            if self.evals >= self.budget:\n                break\n\n        # final safety: return best finite point found, else return center\n        finite_mask = np.isfinite(F)\n        if self.x_opt is None:\n            if np.any(finite_mask):\n                idx = np.nanargmin(F)\n                self.x_opt = X[idx].copy()\n            else:\n                self.x_opt = center.copy()\n                # try to evaluate center if still budget (not necessary for return)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6eb05f50-599e-407e-bb47-7caa5980a9b0", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = int(archive_max)\n\n        # trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try common patterns, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            # OpenAI/BBOB-like: func.bounds.lb / func.bounds.ub\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n            # alternative: func.lower_bounds / func.upper_bounds\n            if lb is None and hasattr(func, \"lower_bounds\"):\n                lb = np.asarray(func.lower_bounds, dtype=float)\n            if ub is None and hasattr(func, \"upper_bounds\"):\n                ub = np.asarray(func.upper_bounds, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # If still None, try attributes .lb/.ub directly\n        try:\n            if lb is None and hasattr(func, \"lb\"):\n                lb = np.asarray(func.lb, dtype=float)\n            if ub is None and hasattr(func, \"ub\"):\n                ub = np.asarray(func.ub, dtype=float)\n        except Exception:\n            pass\n\n        # Fallback to [-5,5]^dim\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Broadcast scalars\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # Pad/truncate to dim\n        if lb.size != self.dim:\n            tmp = np.full(self.dim, -5.0, dtype=float)\n            tmp[:min(lb.size, self.dim)] = lb.ravel()[:min(lb.size, self.dim)]\n            lb = tmp\n        if ub.size != self.dim:\n            tmp = np.full(self.dim, 5.0, dtype=float)\n            tmp[:min(ub.size, self.dim)] = ub.ravel()[:min(ub.size, self.dim)]\n            ub = tmp\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # iterative reflection: reflect across boundaries until inside or max_reflect\n        x = np.asarray(x, dtype=float)\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            # reflect low: new = lb + (lb - x) = 2*lb - x\n            x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            # reflect high: new = ub - (x - ub) = 2*ub - x\n            x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None or n == 1:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.random((n, self.dim)) * (ub - lb)\n\n    # ----- main call -----\n    def __call__(self, func):\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # returns +inf if budget exhausted (and does not call func)\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            # ensure 1D\n            if x.ndim > 1 and x.shape[0] == 1:\n                x = x.ravel()\n            try:\n                f = float(func(x))\n            except Exception:\n                # count the attempt but treat as infinite (robust against bad evals)\n                self.evals += 1\n                return np.inf\n            self.evals += 1\n            return f\n\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        center = (lb + ub) / 2.0\n\n        # initialization: stratified per-dim independent LHS-like\n        n0 = min(self.init_samples, max(2, self.budget))\n        # create LHS: for each dim, sample n0 strata; then combine by permuting rows\n        lhs = np.empty((n0, self.dim), dtype=float)\n        for d in range(self.dim):\n            strata = (np.arange(n0) + self.rng.random(n0)) / n0\n            self.rng.shuffle(strata)\n            lhs[:, d] = lb[d] + strata * span[d]\n        # ensure center is included and at least one pure uniform\n        X = []\n        F = []\n        # include center explicitly\n        X.append(center.copy())\n        F.append(safe_eval(center))\n        # include LHS rows (skip duplicates of center)\n        for i in range(n0):\n            if self.evals >= self.budget:\n                break\n            x = lhs[i].copy()\n            # avoid exact center duplication\n            if np.allclose(x, center):\n                x = self._uniform_array(lb, ub)\n            X.append(x)\n            F.append(safe_eval(x))\n        # ensure at least one purely uniform if budget allows\n        if self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            X.append(x)\n            F.append(safe_eval(x))\n\n        X = np.array(X, dtype=float)\n        F = np.array(F, dtype=float)\n\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            idx_best = np.nanargmin(F)\n            self.f_opt = float(F[idx_best])\n            self.x_opt = X[idx_best].copy()\n        else:\n            # nothing finite yet; ensure center counted (but may be inf). set fallback center\n            self.x_opt = center.copy()\n            self.f_opt = np.inf\n\n        # short-term adaptation variables\n        gscale = 0.8  # global short-term scale multiplier (multiplies robust per-dim spread)\n        adapt_window = 50\n        success_window = []\n        max_iters = max(1, self.budget - self.evals)\n        iter_since_improve = 0\n        stagnation_limit = 150\n        n_iter = 0\n\n        # main loop: generate one candidate per iteration until budget used\n        while self.evals < self.budget:\n            n_iter += 1\n            # recompute finite indices and stats\n            finite_idx = np.where(np.isfinite(F))[0]\n            if finite_idx.size == 0:\n                finite_idx = np.arange(len(F))  # may be empty but keep indices\n            finite_F = F[finite_idx] if finite_idx.size > 0 else np.array([])\n            # robust per-dim scale from IQR\n            if finite_idx.size >= 4:\n                vals = X[finite_idx]\n                q25 = np.percentile(vals, 25, axis=0)\n                q75 = np.percentile(vals, 75, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approx std from IQR\n            else:\n                per_dim_scale = span * 0.1\n            # fallback small positive\n            mask_small = per_dim_scale <= 0\n            if np.any(mask_small):\n                per_dim_scale[mask_small] = span[mask_small] * 0.05 + 1e-8\n            per_dim_scale_abs = per_dim_scale * gscale + 1e-12\n\n            # choose base index biased to better ranks (softmax on negative F)\n            if finite_idx.size > 0:\n                order = np.argsort(finite_F)\n                ranks = np.empty_like(order)\n                ranks[order] = np.arange(len(order))\n                beta = 6.0\n                w = np.exp(-beta * (ranks / max(1, len(ranks) - 1)))\n                if np.sum(w) == 0:\n                    w = np.ones_like(w)\n                probs = w / np.sum(w)\n                sel = self.rng.choice(finite_idx, p=probs)\n                base_idx = int(sel)\n            else:\n                base_idx = int(self.rng.integers(0, len(X)))\n\n            base = X[base_idx].copy()\n\n            # pick donors for DE-like moves\n            other_idxs = np.setdiff1d(np.arange(len(X)), [base_idx])\n            if other_idxs.size >= 2:\n                dsel = self.rng.choice(other_idxs, size=2, replace=False)\n                donor1 = X[dsel[0]].copy()\n                donor2 = X[dsel[1]].copy()\n            else:\n                # take uniform donors if not enough archive\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type: local vs global\n            p_local = 0.6 if np.isfinite(self.f_opt) else 0.4\n            r = self.rng.random()\n            cand = base.copy()\n            # local move: anisotropic gaussian with slight pull to archive mean and occasional PCA elite steps\n            if r < p_local:\n                # slight attraction to archive mean\n                if finite_idx.size > 0:\n                    mean_archive = np.nanmean(X[finite_idx], axis=0)\n                else:\n                    mean_archive = center\n                pull = 0.08 * (mean_archive - cand)\n                # anisotropic gaussian: per-dim sigma = per_dim_scale_abs * (0.6 + rand)\n                anis = per_dim_scale_abs * (0.4 + 0.8 * self.rng.random(self.dim))\n                cand = cand + pull + self.rng.normal(0.0, 1.0, size=self.dim) * anis\n\n                # Occasionally do PCA-guided elite moves\n                if finite_idx.size >= 6 and self.rng.random() < 0.18:\n                    # pick top elites (25% best)\n                    k = max(3, int(max(3, 0.25 * finite_idx.size)))\n                    elite_idxs = finite_idx[np.argsort(F[finite_idx])[:k]]\n                    elites = X[elite_idxs]\n                    centered = elites - np.mean(elites, axis=0)\n                    try:\n                        # SVD for principal components\n                        U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n                        # sample in principal subspace: combine top components with gaussian weights\n                        top_k = min(3, Vt.shape[0])\n                        coeffs = np.zeros(self.dim)\n                        for j in range(top_k):\n                            coeffs += (self.rng.normal(0, 1.0) * (S[j] / (S[0] + 1e-12))) * Vt[j]\n                        # small scale along principal direction\n                        perturb = coeffs * (np.mean(per_dim_scale) * 0.6)\n                        pick_el = elites[self.rng.integers(0, elites.shape[0])]\n                        cand2 = pick_el + perturb\n                        # accept cand2 probabilistically (only a partial replacement)\n                        if self.rng.random() < 0.6:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.22:\n                    Fscale = 0.6 + 0.8 * self.rng.random()\n                    mask_comp = self.rng.random(self.dim) < (0.15 + 0.35 * self.rng.random())\n                    de_part = Fscale * (donor1 - donor2)\n                    cand[mask_comp] = cand[mask_comp] + de_part[mask_comp]\n\n                # occasionally perform a tempered Cauchy jump after DE-like step\n                if self.rng.random() < 0.06:\n                    u = self.rng.random(self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    scale = (per_dim_scale_abs + 1e-6) * (0.2 + self.rng.random() * 1.0)\n                    cand += cauch * scale\n\n            else:\n                # GLOBAL moves: DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                if self.rng.random() < 0.55:\n                    # DE-like global differential move (more exploratory)\n                    Fscale = 0.7 + 1.2 * self.rng.random()\n                    mask_replace = self.rng.random(self.dim) < (0.2 + 0.25 * self.rng.random())\n                    cand[mask_replace] = donor1[mask_replace]\n                    cand = cand + Fscale * (donor1 - donor2) * (self.rng.random(self.dim) < 0.6)\n                    # small gaussian jitter\n                    cand += self.rng.normal(0, 0.02, size=self.dim) * per_dim_scale_abs\n                    # occasionally tempered Cauchy jump\n                    if self.rng.random() < 0.08:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        scale = (span) * (0.05 + 0.8 * self.rng.random())\n                        cand += cauch * scale\n                else:\n                    # ARCHIVE_MIX or CAUCHY global jumps\n                    pick_idx = int(self.rng.choice(np.arange(len(X))))\n                    donor = X[pick_idx].copy()\n                    cand = donor.copy()\n                    # occasionally inject uniform in a random dimension to diversify\n                    if self.rng.random() < 0.35:\n                        rand_dims = self.rng.random(self.dim) < 0.25\n                        if np.any(rand_dims):\n                            cand[rand_dims] = lb[rand_dims] + self.rng.random(np.sum(rand_dims)) * (ub[rand_dims] - lb[rand_dims])\n                    # tempered Cauchy global jump from a random archive point or base\n                    if self.rng.random() < 0.5:\n                        u = self.rng.random(self.dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        scale = span * (0.08 + 0.9 * self.rng.random(self.dim))\n                        cand += cauch * scale\n                    else:\n                        # add a moderate global jitter\n                        cand += self.rng.normal(0, 0.08, size=self.dim) * span\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb[bad], ub[bad]) if np.ndim(lb) > 0 else self._uniform_array(lb, ub)[bad]\n            cand = self._reflect_bounds(cand, lb, ub, max_reflect=10)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, np.array([f], dtype=float)])\n\n            # update best trackers and stagnation\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = cand.copy()\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            finite_vals = F[np.isfinite(F)]\n            success = False\n            if np.isfinite(f):\n                if f < self.f_opt:\n                    success = True\n                elif finite_vals.size > 0:\n                    threshold = np.percentile(finite_vals, 25)\n                    if f <= threshold:\n                        success = True\n            success_window.append(1 if success else 0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if len(X) > self.archive_max:\n                # keep top 60% best, plus random sample of remaining 40%\n                finite_good_idxs = np.where(np.isfinite(F))[0]\n                if finite_good_idxs.size > 0:\n                    sorted_idx = finite_good_idxs[np.argsort(F[finite_good_idxs])]\n                else:\n                    sorted_idx = np.argsort(F)\n                keep_best = int(0.6 * self.archive_max)\n                keep_idx = list(sorted_idx[:keep_best])\n                remaining = [i for i in range(len(X)) if i not in keep_idx]\n                self.rng.shuffle(remaining)\n                need = self.archive_max - len(keep_idx)\n                if need > 0:\n                    keep_idx.extend(remaining[:need])\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_window) >= adapt_window:\n                rate = np.mean(success_window[-adapt_window:])\n                if rate > 0.45:\n                    # too successful -> tighten\n                    gscale *= 0.85\n                elif rate < 0.12:\n                    # unsuccessful -> enlarge\n                    gscale *= 1.25\n                else:\n                    # mild drift\n                    gscale *= (0.95 + 0.1 * self.rng.random())\n                # jitter and clip\n                gscale = float(np.clip(gscale * (1.0 + 0.02 * (self.rng.random() - 0.5)), 1e-6, 1e2))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit and self.evals < self.budget:\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = min(6, max(2, int(0.02 * self.budget)))\n                injects = []\n                for k in range(n_inject):\n                    if self.x_opt is None or self.rng.random() < 0.35:\n                        injects.append(self._uniform_array(lb, ub))\n                    else:\n                        # perturb elite/best\n                        perturb = self.rng.normal(0, 1.0, size=self.dim) * (per_dim_scale_abs * (0.5 + self.rng.random()))\n                        injects.append(self._reflect_bounds(self.x_opt + perturb, lb, ub))\n                # evaluate them (append while respecting budget)\n                for inj in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_inj = safe_eval(inj)\n                    X = np.vstack([X, inj.reshape(1, -1)])\n                    F = np.concatenate([F, np.array([f_inj], dtype=float)])\n                    if np.isfinite(f_inj) and f_inj < self.f_opt:\n                        self.f_opt = float(f_inj)\n                        self.x_opt = inj.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale *= 1.6\n                iter_since_improve = 0\n\n        # final safety: return best finite point found, else center\n        finite_mask = np.isfinite(F)\n        if np.any(finite_mask):\n            best_idx = np.nanargmin(F)\n            self.x_opt = X[best_idx].copy()\n            self.f_opt = float(F[best_idx])\n        else:\n            self.x_opt = center.copy()\n            self.f_opt = np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5dd4de0d-0751-473c-8ba1-54eb1a9a7d8f", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(3, int(init_samples))\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n\n        # Trackers to be set in __call__\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try common patterns, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n\n        # try func.bounds.lb / func.bounds.ub\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            lb = getattr(b, \"lb\", None) or getattr(b, \"lower_bounds\", None) or getattr(b, \"lower\", None)\n            ub = getattr(b, \"ub\", None) or getattr(b, \"upper_bounds\", None) or getattr(b, \"upper\", None)\n\n        # other attribute patterns\n        if lb is None and hasattr(func, \"lower_bounds\"):\n            lb = getattr(func, \"lower_bounds\")\n        if ub is None and hasattr(func, \"upper_bounds\"):\n            ub = getattr(func, \"upper_bounds\")\n\n        # fallback to direct attributes\n        if lb is None and hasattr(func, \"lb\"):\n            lb = getattr(func, \"lb\")\n        if ub is None and hasattr(func, \"ub\"):\n            ub = getattr(func, \"ub\")\n\n        # If scalars, broadcast\n        if lb is None or ub is None:\n            # default common BBOB search space\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n            # pad/truncate\n            if lb.size < self.dim:\n                tmp = np.full(self.dim, lb.flat[0])\n                tmp[:lb.size] = lb\n                lb = tmp\n            else:\n                lb = lb[: self.dim].copy()\n            if ub.size < self.dim:\n                tmp = np.full(self.dim, ub.flat[0])\n                tmp[:ub.size] = ub\n                ub = tmp\n            else:\n                ub = ub[: self.dim].copy()\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # iterative reflection: reflect across boundaries until inside or max_reflect\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            if np.any(low_mask):\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if np.any(high_mask):\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n is None or n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            # return shape (n, dim)\n            return self.rng.uniform(lb, ub, size=(n, lb.size))\n\n    # ----- main call -----\n    def __call__(self, func):\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # returns +inf if budget exhausted (and does not call func)\n            if self.evals >= self.budget:\n                return np.inf\n            # ensure 1D array of length dim\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # increment count and call\n            self.evals += 1\n            try:\n                f = func(x)\n            except Exception:\n                # robust against bad evaluations\n                return np.inf\n            # ensure scalar\n            try:\n                return float(np.asarray(f).reshape(()))\n            except Exception:\n                return np.inf\n\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        if np.any(span <= 0):\n            # fallback\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n            span = ub - lb\n\n        # initialize archive with center and LHS-like and some uniforms\n        X_list = []\n        F_list = []\n\n        center = (lb + ub) / 2.0\n        f_center = safe_eval(center)\n        X_list.append(center.copy())\n        F_list.append(f_center)\n        if np.isfinite(f_center):\n            self.f_opt = f_center\n            self.x_opt = center.copy()\n        else:\n            self.f_opt = np.inf\n            self.x_opt = center.copy()\n\n        # LHS-like: per-dim strata and then permute\n        n0 = min(self.init_samples, max(1, self.budget - self.evals))\n        if n0 >= 1:\n            # Generate LHS columns\n            cols = np.zeros((n0, self.dim))\n            for d in range(self.dim):\n                strata = (np.arange(n0) + self.rng.random(n0)) / n0\n                vals = lb[d] + strata * span[d]\n                perm = self.rng.permutation(n0)\n                cols[:, d] = vals[perm]\n            # Evaluate LHS rows\n            for i in range(n0):\n                if self.evals >= self.budget:\n                    break\n                x = cols[i]\n                # avoid exact duplicate of center\n                if np.allclose(x, center):\n                    continue\n                f = safe_eval(x)\n                X_list.append(x.copy())\n                F_list.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n        # add at least one purely uniform if budget allows\n        if self.evals < self.budget:\n            u = self._uniform_array(lb, ub)\n            f = safe_eval(u)\n            X_list.append(u.copy())\n            F_list.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = u.copy()\n\n        X = np.array(X_list, dtype=float)\n        F = np.array(F_list, dtype=float)\n\n        # short-term adaptation variables\n        n_iter = 0\n        gscale = 0.8  # global scale factor that adapts\n        adapt_window = 40\n        success_window = []\n        iter_since_improve = 0\n        stagnation_thresh = max(30, int(0.2 * self.budget))\n\n        # main loop: generate one candidate per iteration until budget used\n        while self.evals < self.budget:\n            n_iter += 1\n\n            finite_idx = np.where(np.isfinite(F))[0]\n            finite_vals = F[finite_idx] if finite_idx.size > 0 else np.array([])\n            # robust per-dim scale from IQR of finite archive\n            if finite_idx.size >= 4:\n                q75 = np.percentile(X[finite_idx], 75, axis=0)\n                q25 = np.percentile(X[finite_idx], 25, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approximate std from IQR\n                per_dim_scale = np.maximum(per_dim_scale, span * 1e-6)\n            else:\n                per_dim_scale = span * 0.08\n\n            mask_small = per_dim_scale < (span * 1e-8)\n            per_dim_scale[mask_small] = span[mask_small] * 0.05 + 1e-8\n\n            # choose base index biased to better ranks (softmax on negative F)\n            if finite_idx.size > 0:\n                vals = F[finite_idx]\n                # softmax on negative with temperature\n                temp = max(1e-3, np.std(vals) if vals.size > 1 else 1.0)\n                exps = np.exp(-(vals - np.min(vals)) / (temp * 1.0))\n                probs = exps / np.sum(exps)\n                base_idx = self.rng.choice(finite_idx, p=probs)\n                base = X[base_idx].copy()\n            else:\n                base = self._uniform_array(lb, ub)\n\n            # pick donors for DE-like moves (prefer archive)\n            if X.shape[0] >= 3:\n                donor_idxs = self.rng.choice(X.shape[0], size=2, replace=False)\n                donor1 = X[donor_idxs[0]]\n                donor2 = X[donor_idxs[1]]\n            else:\n                donor1 = self._uniform_array(lb, ub)\n                donor2 = self._uniform_array(lb, ub)\n\n            # choose move type: local vs global\n            use_local = self.rng.random() < 0.75  # bias to local\n            cand = base.copy()\n\n            if use_local:\n                # anisotropic gaussian with slight pull to archive mean\n                if X.shape[0] > 0:\n                    archive_mean = np.nanmean(X, axis=0)\n                else:\n                    archive_mean = center\n                pull = (archive_mean - cand) * (0.05 * self.rng.random())\n                anisotropy = per_dim_scale * (0.6 + 0.8 * self.rng.random(self.dim))\n                cand += pull + self.rng.normal(0.0, anisotropy * gscale)\n\n                # Occasionally do PCA-guided elite moves\n                if finite_idx.size >= 4 and self.rng.random() < 0.12:\n                    k = max(3, int(max(3, 0.25 * finite_idx.size)))\n                    elite_idxs = finite_idx[np.argsort(F[finite_idx])[:k]]\n                    elites = X[elite_idxs]\n                    centered = elites - np.mean(elites, axis=0)\n                    # SVD for principal components\n                    try:\n                        U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n                        # sample coefficients along top components\n                        topk = min(3, Vt.shape[0])\n                        coeffs = self.rng.normal(0, 0.6 * np.mean(per_dim_scale), size=topk)\n                        perturb = np.zeros(self.dim)\n                        for j in range(topk):\n                            perturb += coeffs[j] * Vt[j]\n                        pick_el = elites[self.rng.integers(0, elites.shape[0])]\n                        cand2 = pick_el + perturb\n                        # accept cand2 probabilistically based on rank improvement expectation\n                        cand2 = self._reflect_bounds(cand2, lb, ub)\n                        f_est = safe_eval(cand2) if self.evals < self.budget else np.inf\n                        # if better than average elite, adopt\n                        if np.isfinite(f_est):\n                            X = np.vstack([X, cand2.reshape(1, -1)])\n                            F = np.append(F, f_est)\n                            if f_est < self.f_opt:\n                                self.f_opt = f_est\n                                self.x_opt = cand2.copy()\n                                iter_since_improve = 0\n                            # possibly continue with this cand as base for further mixing\n                            if f_est < np.median(F[finite_idx]) if finite_idx.size > 0 else True:\n                                cand = cand2.copy()\n                    except Exception:\n                        pass\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.4:\n                    scale_de = 0.6 * gscale\n                    de_part = scale_de * (donor1 - donor2)\n                    mask_comp = self.rng.random(self.dim) < (0.15 + 0.35 * self.rng.random())\n                    cand[mask_comp] = cand[mask_comp] + de_part[mask_comp]\n\n                # occasionally perform a tempered Cauchy jump after DE-like step\n                if self.rng.random() < 0.06:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    cauch = np.tanh(cauch)  # tempering to avoid extremes\n                    cand += cauch * (0.5 * np.mean(per_dim_scale) * gscale)\n\n            else:\n                # GLOBAL moves: mix DE-like global differential move or ARCHIVE_MIX or CAUCHY global jumps\n                if self.rng.random() < 0.5:\n                    # DE-like global move\n                    scale_global = 0.9 + 0.6 * self.rng.random()\n                    cand = base + scale_global * (donor1 - donor2)\n                    # small gaussian jitter\n                    cand += self.rng.normal(0.0, 0.4 * np.mean(per_dim_scale) * gscale, size=self.dim)\n                else:\n                    # Archive mix or tempered Cauchy\n                    if X.shape[0] > 0 and self.rng.random() < 0.7:\n                        pick = X[self.rng.integers(0, X.shape[0])]\n                        cand = pick + self.rng.normal(0.0, 0.5 * np.mean(per_dim_scale) * gscale, size=self.dim)\n                    else:\n                        # tempered Cauchy global jump from random base\n                        base2 = self._uniform_array(lb, ub)\n                        cauch = self.rng.standard_cauchy(self.dim)\n                        cauch = np.tanh(cauch)\n                        cand = base2 + cauch * (1.5 * np.mean(per_dim_scale) * gscale)\n\n                # occasionally inject uniform in random dimensions\n                if self.rng.random() < 0.18:\n                    rand_dims = self.rng.random(self.dim) < 0.25\n                    cand[rand_dims] = self._uniform_array(lb[rand_dims], ub[rand_dims], n=1)\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.append(F, f)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = cand.copy()\n                iter_since_improve = 0\n                improved = True\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            success = False\n            if improved:\n                success = True\n            else:\n                if finite_idx.size > 0:\n                    finite_vals = F[np.isfinite(F)]\n                    if finite_vals.size > 0:\n                        threshold = np.percentile(finite_vals, 25)\n                        if np.isfinite(f) and f <= threshold:\n                            success = True\n\n            success_window.append(1 if success else 0)\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if X.shape[0] > self.archive_max:\n                # keep top 60% best, plus random sample of remaining 40%\n                n_keep_best = int(0.6 * self.archive_max)\n                order = np.argsort(F)\n                keep_idx = list(order[:n_keep_best])\n                remaining = list(order[n_keep_best:])\n                need = self.archive_max - len(keep_idx)\n                if need > 0 and len(remaining) > 0:\n                    self.rng.shuffle(remaining)\n                    keep_idx.extend(remaining[:need])\n                keep_idx = np.array(sorted(keep_idx))\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_window) >= adapt_window:\n                rate = np.mean(success_window[-adapt_window:])\n                if rate > 0.45:\n                    # too successful -> tighten (focus)\n                    gscale *= 0.92\n                elif rate < 0.15:\n                    # unsuccessful -> enlarge (explore)\n                    gscale *= 1.12\n                else:\n                    # mild drift toward stability\n                    gscale *= (0.98 + 0.04 * self.rng.random())\n                # jitter and clip\n                gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n                gscale = float(np.clip(gscale, 0.02, 4.0))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_thresh and self.evals < self.budget:\n                # inject a few candidates: prefer perturbations of best, some pure uniform\n                n_inject = min(5, max(1, int(0.02 * self.budget)))\n                injects = []\n                best = self.x_opt.copy() if self.x_opt is not None else center\n                for k in range(n_inject):\n                    if self.rng.random() < 0.6:\n                        # perturb best\n                        scale = (1.0 + 2.0 * self.rng.random()) * np.mean(per_dim_scale)\n                        p = best + self.rng.normal(0.0, scale, size=self.dim)\n                        p = self._reflect_bounds(p, lb, ub)\n                        injects.append(p)\n                    else:\n                        injects.append(self._uniform_array(lb, ub))\n                for p in injects:\n                    if self.evals >= self.budget:\n                        break\n                    f_inj = safe_eval(p)\n                    X = np.vstack([X, p.reshape(1, -1)])\n                    F = np.append(F, f_inj)\n                    if np.isfinite(f_inj) and f_inj < self.f_opt:\n                        self.f_opt = f_inj\n                        self.x_opt = p.copy()\n                        iter_since_improve = 0\n                # encourage exploration: enlarge gscale a bit after restart\n                gscale = min(4.0, gscale * (1.0 + 0.25 * self.rng.random()))\n                iter_since_improve = 0  # give it a fresh start\n\n        # final safety: return best finite point found, else center\n        if self.x_opt is None:\n            return np.inf, center\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 34, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower_bounds\", None) or getattr(b, \"lower\", None)", "error": "In the code, line 34, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower_bounds\", None) or getattr(b, \"lower\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ec3a9f7b-e3f2-42ab-bedf-f045fefffe33", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donors, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = int(max(50, archive_max))\n        # adaptive global scale for exploratory moves\n        self.gscale = 0.35\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # Try several common patterns to find bounds; fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # try structured attributes\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            try:\n                if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                    lb = np.asarray(func.lower_bounds, dtype=float)\n                    ub = np.asarray(func.upper_bounds, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        if lb is None:\n            try:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        if lb is None:\n            # fallback to canonical [-5,5]^dim\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast scalars / pad/truncate to dimension\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            if lb.ndim == 0:\n                lb = np.full(self.dim, float(lb))\n            if ub.ndim == 0:\n                ub = np.full(self.dim, float(ub))\n            if lb.size < self.dim:\n                lb = np.resize(lb, self.dim)\n            if ub.size < self.dim:\n                ub = np.resize(ub, self.dim)\n            lb = lb[: self.dim].astype(float)\n            ub = ub[: self.dim].astype(float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        # Ensure x inside [lb,ub] by iterative reflection\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            if low_mask.any():\n                x[low_mask] = 2.0 * lb[low_mask] - x[low_mask]\n            if high_mask.any():\n                x[high_mask] = 2.0 * ub[high_mask] - x[high_mask]\n        # final clamp safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        # returns shape (n, dim) if n>1 else (dim,)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.ndim == 0:\n            lb = np.full(self.dim, float(lb))\n        if ub.ndim == 0:\n            ub = np.full(self.dim, float(ub))\n        if n is None or int(n) == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            n = int(n)\n            return self.rng.uniform(np.broadcast_to(lb, (n, self.dim)),\n                                    np.broadcast_to(ub, (n, self.dim)))\n\n    # ----- main call -----\n    def __call__(self, func):\n        # safe evaluator that respects budget\n        evals = 0\n\n        def safe_eval(x):\n            nonlocal evals\n            # Do not call func if budget exhausted\n            if evals >= self.budget:\n                return np.inf\n            try:\n                # Ensure 1D\n                x_a = np.asarray(x, dtype=float)\n                if x_a.ndim > 1 and x_a.shape[0] == 1:\n                    x_a = x_a.reshape(-1)\n                evals += 1\n                f = func(x_a)\n                # If func returns non-scalar or raises, handle\n                if isinstance(f, (list, tuple, np.ndarray)):\n                    f = float(np.asarray(f).ravel()[0])\n                return float(f)\n            except Exception:\n                # treat errors as bad evaluations (counted)\n                return np.inf\n\n        # get bounds\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        center = 0.5 * (lb + ub)\n\n        # Initialization: LHS-like stratified per-dim independent sampling\n        n0 = min(self.init_samples, max(2, self.budget))\n        # For each dim, create n0 stratified samples, then permute rows\n        strata = np.zeros((n0, self.dim), dtype=float)\n        for d in range(self.dim):\n            # n0 strata endpoints in [0,1]\n            cut = np.linspace(0.0, 1.0, n0 + 1)\n            # sample uniformly within each strata\n            u = self.rng.uniform(cut[:-1], cut[1:])\n            self.rng.shuffle(u)\n            strata[:, d] = u\n        # map to bounds\n        X_init = lb + strata * span\n        # Ensure center included explicitly (if budget allows)\n        X = []\n        X_vals = []\n        # include center if distinct\n        if len(X_init) == 0 or not np.allclose(center, X_init, atol=1e-12).any():\n            X.append(center.copy())\n            f_center = safe_eval(center)\n            X_vals.append(float(f_center))\n        # add LHS rows but skip duplicates of center\n        for i in range(X_init.shape[0]):\n            if evals >= self.budget:\n                break\n            xi = X_init[i]\n            # avoid adding exact same as center\n            if np.allclose(xi, center, atol=1e-12):\n                continue\n            fi = safe_eval(xi)\n            X.append(np.asarray(xi, dtype=float))\n            X_vals.append(float(fi))\n        # ensure at least one purely uniform if budget allows and not duplicate\n        if evals < self.budget:\n            u = self._uniform_array(lb, ub)\n            X.append(u)\n            X_vals.append(float(safe_eval(u)))\n\n        # Convert to arrays\n        if len(X) == 0:\n            X = np.zeros((1, self.dim), dtype=float)\n            X_vals = [np.inf]\n        else:\n            X = np.vstack([np.asarray(xx, dtype=float) for xx in X])\n        F = np.array(X_vals, dtype=float)\n\n        # trackers\n        finite_mask = np.isfinite(F)\n        if finite_mask.any():\n            best_idx = np.nanargmin(F)\n            x_opt = X[best_idx].copy()\n            f_opt = float(F[best_idx])\n        else:\n            x_opt = center.copy()\n            f_opt = np.inf\n\n        # short-term adaptation variables\n        success_log = []\n        adapt_window = max(20, 8 * self.dim)\n        iter_since_improve = 0\n        stagnation_limit = max(60, 8 * self.dim)\n\n        # main loop: generate one candidate per iteration until budget used\n        while evals < self.budget:\n            n_archive = X.shape[0]\n            finite_idx = np.isfinite(F)\n            if finite_idx.sum() >= 2:\n                # per-dim robust scale from IQR\n                vals = X[finite_idx]\n                q75 = np.percentile(vals, 75, axis=0)\n                q25 = np.percentile(vals, 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = np.maximum(iqr / 1.349, 1e-9 * span)\n                # fallback to global span if all zeros\n                zero_mask = per_dim_scale <= 0\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = 0.1 * span[zero_mask] + 1e-9\n            else:\n                per_dim_scale = 0.2 * span\n\n            # choose base index biased to better ranks (softmax on negative F)\n            if np.any(finite_idx):\n                scores = np.where(finite_idx, -F, np.nan)\n                # fill non-finite with large negative so they are unlikely chosen\n                minscore = np.nanmin(scores) if np.any(np.isfinite(scores)) else -1.0\n                scores = np.where(np.isfinite(scores), scores, minscore - 10.0)\n                # softmax\n                stab = scores - np.max(scores)\n                probs = np.exp(stab / (np.std(stab) + 1e-12))\n                probs = probs / np.sum(probs)\n                base_idx = self.rng.choice(len(X), p=probs)\n            else:\n                base_idx = self.rng.integers(0, len(X))\n\n            base = X[base_idx].copy()\n            mean_archive = np.nanmean(X[finite_idx], axis=0) if finite_idx.any() else center\n\n            # pick donors for DE-like moves\n            donor_indices = []\n            pool = [i for i in range(len(X)) if i != base_idx]\n            if len(pool) >= 2:\n                donor_indices = self.rng.choice(pool, size=2, replace=False)\n            elif len(pool) == 1:\n                donor_indices = [pool[0], pool[0]]\n            else:\n                donor_indices = [base_idx, base_idx]\n\n            # choose move type: local vs global\n            p_global = 0.18 + 0.25 * (1.0 - np.tanh(iter_since_improve / (stagnation_limit + 1)))\n            if self.rng.random() < p_global:\n                # GLOBAL move\n                cand = base.copy()\n                if self.rng.random() < 0.6:\n                    # DE-like global differential move (more exploratory)\n                    a = X[donor_indices[0]]\n                    b = X[donor_indices[1]]\n                    Fscale = 0.5 + self.rng.random() * 0.9\n                    cand = base + Fscale * (a - b)\n                    # add jitter proportional to gscale and span\n                    cand += self.rng.normal(0.0, 1.0, self.dim) * (self.gscale * span * (0.15 + 0.6 * self.rng.random(self.dim)))\n                else:\n                    # ARCHIVE_MIX or CAUCHY global jumps\n                    pick_idx = int(self.rng.choice(np.arange(len(X))))\n                    anchor = X[pick_idx]\n                    # tempered Cauchy jump\n                    cauchy = self.rng.standard_cauchy(self.dim)\n                    temper = (0.06 + 0.94 * self.rng.random(self.dim))\n                    scale = span * (0.06 + 0.9 * self.rng.random(self.dim)) * self.gscale\n                    cand = anchor + cauchy * scale * temper\n                    # small gaussian jitter\n                    cand += self.rng.normal(0.0, 1.0, self.dim) * (0.02 * span)\n                # sometimes inject purely uniform in one random dimension\n                if self.rng.random() < 0.06:\n                    d = self.rng.integers(0, self.dim)\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n            else:\n                # LOCAL move: anisotropic gaussian with slight pull to archive mean\n                pull = 0.08 * (mean_archive - base)\n                anisotropic_sigma = per_dim_scale * (0.6 + 0.8 * self.rng.random(self.dim))\n                cand = base + pull + self.rng.normal(0.0, 1.0, self.dim) * anisotropic_sigma\n\n                # Differential-like donor mixing inside local branch sometimes\n                if self.rng.random() < 0.28:\n                    a = X[donor_indices[0]]\n                    b = X[donor_indices[1]]\n                    cand += 0.45 * (a - b) * (0.6 + 0.8 * self.rng.random())\n\n                # Occasionally do PCA-guided elite moves\n                if self.rng.random() < 0.12 and finite_idx.sum() >= max(4, int(0.25 * finite_idx.sum())):\n                    # pick top elites (25% best)\n                    n_elite = max(4, int(0.25 * finite_idx.sum()))\n                    elite_idx = np.argsort(F)[:n_elite]\n                    elites = X[elite_idx]\n                    # center elites\n                    mean_elite = np.mean(elites, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd((elites - mean_elite), full_matrices=False)\n                        # sample coefficients in principal subspace\n                        k = min(3, Vt.shape[0])\n                        coeffs = np.zeros(self.dim)\n                        for j in range(k):\n                            vec = Vt[j]\n                            # scale by singular values relative to first\n                            s_scale = S[j] / (S[0] + 1e-12)\n                            coeffs += (self.rng.normal(0, 1.0) * s_scale) * vec\n                        perturb = coeffs * (np.mean(per_dim_scale) * 0.6)\n                        cand2 = base + 0.7 * (mean_elite - base) + perturb\n                        # reflect and evaluate cand2 to consider replacement\n                        cand2 = self._reflect_bounds(cand2, lb, ub)\n                        # evaluate cand2 but do not exceed budget\n                        f_cand2 = safe_eval(cand2)\n                        if f_cand2 < np.inf:\n                            # accept cand2 if better than current candidate after evaluating later,\n                            # but here we tentatively accept if f_cand2 better than best so far (aggressive),\n                            # else keep and we will evaluate original cand too if budget left\n                            if f_cand2 < f_opt or self.rng.random() < 0.25:\n                                cand = cand2\n                    except Exception:\n                        pass\n\n                # occasionally perform a tempered Cauchy jump after DE-like step\n                if self.rng.random() < 0.06:\n                    c = self.rng.standard_cauchy(self.dim) * (self.gscale * span * (0.04 + 0.96 * self.rng.random(self.dim)))\n                    cand += c\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb[bad], ub[bad]) if np.ndim(lb) > 0 else self._uniform_array(lb, ub)[bad]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_cand = safe_eval(cand)\n\n            # Append candidate to archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, [f_cand]])\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_cand) and f_cand < f_opt:\n                f_opt = float(f_cand)\n                x_opt = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            finite_vals = F[np.isfinite(F)]\n            success = False\n            if np.isfinite(f_cand):\n                if f_cand <= f_opt:\n                    success = True\n                else:\n                    # among top quartile?\n                    if finite_vals.size > 0:\n                        cutoff = np.percentile(finite_vals, 25)\n                        if f_cand <= cutoff:\n                            success = True\n            success_log.append(1 if success else 0)\n            if len(success_log) > adapt_window:\n                success_log.pop(0)\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if X.shape[0] > self.archive_max:\n                idx_sorted = np.argsort(F)\n                keep_best = int(0.60 * self.archive_max)\n                keep_best = max(10, keep_best)\n                keep_idx = idx_sorted[:keep_best].tolist()\n                remaining = idx_sorted[keep_best:].tolist()\n                # sample some random others to maintain diversity\n                n_remaining_to_keep = self.archive_max - len(keep_idx)\n                if n_remaining_to_keep > 0 and len(remaining) > 0:\n                    chosen = self.rng.choice(remaining, size=min(n_remaining_to_keep, len(remaining)), replace=False).tolist()\n                    keep_idx += chosen\n                keep_idx = np.array(sorted(set(keep_idx)))\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # adapt gscale occasionally based on short-term success\n            if len(success_log) >= adapt_window:\n                rate = float(np.mean(success_log[-adapt_window:]))\n                if rate > 0.35:\n                    # successful -> tighten exploration a bit\n                    self.gscale *= (0.92 + 0.04 * self.rng.random())\n                elif rate < 0.12:\n                    # unsuccessful -> enlarge exploration\n                    self.gscale *= (1.03 + 0.15 * self.rng.random())\n                else:\n                    # mild jitter\n                    self.gscale *= (0.98 + 0.04 * self.rng.random())\n                # clamp\n                self.gscale = float(np.clip(self.gscale, 0.02, 3.0))\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve >= stagnation_limit and evals < self.budget:\n                # create some injected candidates (prefer perturbations of best)\n                n_inject = min(6, self.budget - evals)\n                injects = []\n                for k in range(n_inject):\n                    if self.rng.random() < 0.6 and np.isfinite(f_opt):\n                        # perturb the best\n                        perturb = self.rng.normal(0, 1.0, self.dim) * (1.5 * self.gscale * per_dim_scale)\n                        injected = x_opt + perturb\n                        injected = self._reflect_bounds(injected, lb, ub)\n                    else:\n                        # uniform sample\n                        injected = self._uniform_array(lb, ub)\n                    injects.append(injected)\n                # evaluate them\n                for inj in injects:\n                    if evals >= self.budget:\n                        break\n                    f_inj = safe_eval(inj)\n                    X = np.vstack([X, inj.reshape(1, -1)])\n                    F = np.concatenate([F, [f_inj]])\n                    if np.isfinite(f_inj) and f_inj < f_opt:\n                        f_opt = float(f_inj)\n                        x_opt = inj.copy()\n                # encourage exploration: enlarge gscale a bit after restart\n                self.gscale *= (1.06 + 0.12 * self.rng.random())\n                self.gscale = float(np.clip(self.gscale, 0.02, 3.0))\n                iter_since_improve = 0\n\n        # final safety: return best finite point found, else center\n        finite_mask = np.isfinite(F)\n        if finite_mask.any():\n            best_idx = int(np.nanargmin(F))\n            self.f_opt = float(F[best_idx])\n            self.x_opt = X[best_idx].copy()\n        else:\n            self.f_opt = np.inf\n            self.x_opt = center.copy()\n\n        # Ensure shapes\n        self.x_opt = np.asarray(self.x_opt, dtype=float).reshape(-1)[: self.dim]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 153, in __call__, the following error occurred:\nAttributeError: 'bool' object has no attribute 'any'\nOn line: if len(X_init) == 0 or not np.allclose(center, X_init, atol=1e-12).any():", "error": "In the code, line 153, in __call__, the following error occurred:\nAttributeError: 'bool' object has no attribute 'any'\nOn line: if len(X_init) == 0 or not np.allclose(center, X_init, atol=1e-12).any():", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e0990174-847e-4b91-8dcc-f6fc0bdf15c0", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling — an archive-driven hybrid sampler mixing anisotropic Gaussian local moves, PCA-guided elite perturbations, DE-like donor recombination, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Memory-guided hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(max(50, archive_max))\n        # short-term adaptation param\n        self.gscale = 0.5  # global scale (rel to bounds)\n        # counters and trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ----- utilities -----\n    def _get_bounds(self, func):\n        # try common patterns\n        lb = None\n        ub = None\n        # pattern 1: func.bounds.lb / func.bounds.ub (e.g., many frameworks)\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n        except Exception:\n            lb = ub = None\n        # pattern 2: direct attributes\n        if lb is None or ub is None:\n            lb = lb or getattr(func, \"lower_bounds\", None) or getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            ub = ub or getattr(func, \"upper_bounds\", None) or getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        # fallback to attributes .lower/.upper\n        if lb is None or ub is None:\n            lb = lb or getattr(func, \"bounds_lb\", None)\n            ub = ub or getattr(func, \"bounds_ub\", None)\n        # final fallback [-5,5]\n        if lb is None or ub is None:\n            lb = -5.0\n            ub = 5.0\n        lb = np.atleast_1d(lb).astype(float)\n        ub = np.atleast_1d(ub).astype(float)\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # pad/truncate to dim\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        if lb.size > self.dim:\n            lb = lb[:self.dim]\n        if ub.size > self.dim:\n            ub = ub[:self.dim]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = x.copy()\n        for _ in range(max_reflect):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            if low_mask.any():\n                x[low_mask] = 2 * lb[low_mask] - x[low_mask]\n            if high_mask.any():\n                x[high_mask] = 2 * ub[high_mask] - x[high_mask]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            shape = (n, self.dim)\n            return self.rng.uniform(lb, ub, size=shape)\n\n    # ----- main call -----\n    def __call__(self, func):\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            x = np.asarray(x).astype(float)\n            if x.ndim != 1:\n                x = x.ravel()\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                # treat as failure\n                f = np.inf\n            # count attempt (even if np.inf)\n            self.evals += 1\n            return float(f)\n\n        lb, ub = self._get_bounds(func)\n        # create LHS-like initialization (per-dim stratified)\n        n0 = min(self.init_samples, max(4, self.budget // 20))\n        strata = np.linspace(0, 1, n0 + 1)\n        # sample one point per stratum per dimension, then permute rows\n        mat = np.zeros((n0, self.dim))\n        for d in range(self.dim):\n            # strat sample offsets\n            offs = strata[:-1] + self.rng.uniform(0, 1.0 / n0, size=n0)\n            self.rng.shuffle(offs)\n            mat[:, d] = offs\n        # transform to bounds\n        init_points = lb + mat * (ub - lb)\n        # ensure center included\n        center = 0.5 * (lb + ub)\n        # include one purely uniform if budget allows\n        if self.budget - self.evals > 0:\n            uni = self._uniform_array(lb, ub)\n        else:\n            uni = center.copy()\n        # build initial archive, evaluate until budget permits\n        archive_X = []\n        archive_F = []\n        # add center first\n        f_center = safe_eval(center)\n        archive_X.append(center.copy())\n        archive_F.append(f_center)\n        if np.isfinite(f_center) and f_center < self.f_opt:\n            self.f_opt = f_center\n            self.x_opt = center.copy()\n        # add LHS points (skip center duplicates)\n        for i in range(init_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = init_points[i]\n            if np.allclose(x, center):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n        # ensure at least one purely uniform\n        if self.evals < self.budget:\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = f_uni\n                self.x_opt = uni.copy()\n\n        # short-term adaptation variables\n        gscale = float(self.gscale)\n        success_window = []\n        window_len = 30\n        stagn_count = 0\n        stagn_threshold = max(50, 10 * self.dim)\n        iters = 0\n\n        # main loop: generate one candidate per iteration until budget used\n        while self.evals < self.budget:\n            iters += 1\n            # prepare arrays of finite archive items\n            X = np.array(archive_X)\n            F = np.array(archive_F)\n            finite_mask = np.isfinite(F)\n            if finite_mask.any():\n                Xf = X[finite_mask]\n                Ff = F[finite_mask]\n                # stats\n                median = np.median(Xf, axis=0)\n                # per-dim scale: IQR-based robust\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-6, (q75 - q25))\n                # fallback to bounds-based if too small\n                small_mask = per_dim_scale < 1e-6\n                if small_mask.any():\n                    per_dim_scale[small_mask] = (ub[small_mask] - lb[small_mask]) * 0.05\n            else:\n                # fallback\n                median = 0.5 * (lb + ub)\n                per_dim_scale = (ub - lb) * 0.2\n\n            n_archive = X.shape[0]\n            # choose base biased to better ranks (softmax on negative F)\n            if finite_mask.any():\n                scores = np.copy(Ff)\n                # lower is better; make logits\n                logits = - (scores - scores.min()) / (1e-8 + (scores.ptp() if scores.ptp() > 0 else 1.0))\n                # softmax with temperature\n                exp = np.exp(logits - logits.max())\n                probs = exp / exp.sum()\n                # map back to full indices\n                finite_idx = np.nonzero(finite_mask)[0]\n                base_idx = self.rng.choice(finite_idx, p=probs)\n            else:\n                base_idx = self.rng.randint(0, n_archive)\n\n            base = X[base_idx].copy()\n\n            # pick donors for DE-like moves\n            donor_idx = []\n            pick_pool = list(range(n_archive))\n            pick_pool.remove(base_idx)\n            # choose up to 3 donors\n            kdon = min(3, len(pick_pool))\n            if kdon > 0:\n                donor_idx = self.rng.choice(pick_pool, size=kdon, replace=False).tolist()\n            donors = [X[i].copy() for i in donor_idx] if donor_idx else []\n\n            # decide move type\n            p_global = 0.15 + 0.2 * (1.0 - min(1.0, gscale))  # more global when scale small\n            if self.rng.rand() < p_global and len(donors) >= 2:\n                # GLOBAL move: DE-like global differential, tempered Cauchy or archive mix\n                if self.rng.rand() < 0.6:\n                    # DE-like: base + F*(d1 - d2) + jitter\n                    F_scale = 0.8 + 0.4 * self.rng.rand()\n                    dmove = np.zeros(self.dim)\n                    if len(donors) >= 2:\n                        dmove = donors[0] - donors[1]\n                    cand = base + F_scale * dmove + self.rng.normal(0, gscale * (ub - lb) * 0.2)\n                else:\n                    # tempered Cauchy from a random archive point\n                    anchor = X[self.rng.randrange(n_archive)]\n                    # Cauchy with scale proportional to gscale * range\n                    c = self.rng.standard_cauchy(size=self.dim)\n                    scale = gscale * (ub - lb) * (0.8 + 0.4 * self.rng.rand())\n                    cand = anchor + scale * c\n                # small gaussian jitter\n                cand += self.rng.normal(0, 0.01 * (ub - lb))\n                # occasional injection of a purely uniform coordinate\n                if self.rng.rand() < 0.08:\n                    d = self.rng.randint(0, self.dim)\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n            else:\n                # LOCAL move: anisotropic gaussian with attraction to archive mean\n                # attraction\n                if finite_mask.any() and self.rng.rand() < 0.6:\n                    # pull a bit towards median of finite archive\n                    alpha = 0.05 + 0.1 * self.rng.rand()\n                    base_attracted = base + alpha * (median - base)\n                else:\n                    base_attracted = base.copy()\n\n                # anisotropic gaussian: per-dim sigma\n                anis = per_dim_scale * (0.6 + 0.8 * self.rng.rand(self.dim)) * gscale\n                cand = base_attracted + self.rng.normal(0, anis)\n                # occasionally do PCA-guided elite move\n                if finite_mask.sum() >= max(4, int(0.25 * finite_mask.sum())) and self.rng.rand() < 0.18:\n                    # choose elites: top 25% best\n                    if finite_mask.any():\n                        num_elite = max(3, int(0.25 * finite_mask.sum()))\n                        elite_idx = np.argsort(Ff)[:num_elite]\n                        elites = Xf[elite_idx]\n                        # PCA\n                        try:\n                            # center elites\n                            E_centered = elites - elites.mean(axis=0)\n                            U, S, Vt = np.linalg.svd(E_centered, full_matrices=False)\n                            # sample in top principal component directions\n                            kpc = min(2, Vt.shape[0])\n                            weights = self.rng.normal(0, 1.0, size=kpc)\n                            pc_dir = (weights @ Vt[:kpc, :])\n                            pc_dir = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                            pc_scale = gscale * np.mean(per_dim_scale) * (0.5 + self.rng.rand())\n                            cand2 = base + pc_scale * pc_dir\n                            # accept cand2 probabilistically: keep the one with small random chance to replace\n                            if self.rng.rand() < 0.6:\n                                cand = cand2\n                        except Exception:\n                            pass\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.rand() < 0.25:\n                    F_scale = 0.6 + 0.6 * self.rng.rand()\n                    cand = cand + F_scale * (donors[0] - donors[1]) * (0.5 + 0.5 * self.rng.rand())\n\n                # tempered Cauchy jump occasionally\n                if self.rng.rand() < 0.06:\n                    c = self.rng.standard_cauchy(size=self.dim)\n                    cand += (0.2 * gscale) * (ub - lb) * c\n\n            # safety: replace any non-finite entries with uniform sample and reflect to bounds\n            cand = np.asarray(cand).astype(float)\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = self._uniform_array(lb, ub)[bad]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                improved = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success as improving global best or placed among top 25% of finite vals\n            success = False\n            if finite_mask.any():\n                # recompute finite values including new\n                F_all = np.array(archive_F)\n                finite_all = np.isfinite(F_all)\n                if np.isfinite(f_cand):\n                    # rank percentile among finite\n                    franks = np.sum((F_all[finite_all] <= f_cand))\n                    pct = franks / (np.sum(finite_all) + 1e-12)\n                    if (f_cand < self.f_opt) or (pct >= 0.75):\n                        success = True\n                else:\n                    success = False\n            else:\n                success = np.isfinite(f_cand)\n\n            success_window.append(1 if success else 0)\n            if len(success_window) > window_len:\n                success_window.pop(0)\n            # adapt gscale occasionally based on short-term success\n            if iters % 5 == 0:\n                srate = np.mean(success_window) if success_window else 0.0\n                if srate > 0.25:\n                    gscale = max(0.02, gscale * 0.92)  # tighten\n                elif srate < 0.05:\n                    gscale = min(5.0, gscale * 1.08)   # enlarge\n                else:\n                    # mild drift\n                    gscale = gscale * (0.98 + 0.04 * self.rng.rand())\n                # jitter and clip\n                gscale = float(np.clip(gscale * (1.0 + 0.02 * (self.rng.rand() - 0.5)), 1e-3, 10.0))\n\n            # prune archive if too large: keep the best ones and some randoms/diverse\n            if len(archive_X) > self.archive_max:\n                X_arr = np.array(archive_X)\n                F_arr = np.array(archive_F)\n                finite_idxs = np.where(np.isfinite(F_arr))[0]\n                inf_idxs = np.where(~np.isfinite(F_arr))[0]\n                # keep top 60% of finite, rest sample randomly\n                n_keep = int(0.6 * self.archive_max)\n                keep_idx = []\n                if finite_idxs.size > 0:\n                    sorted_fin = finite_idxs[np.argsort(F_arr[finite_idxs])]\n                    keep_idx.extend(sorted_fin[:n_keep].tolist())\n                    # random sample of remaining finite\n                    rem_fin = sorted_fin[n_keep:]\n                    n_random = max(0, self.archive_max - len(keep_idx) - len(inf_idxs))\n                    if rem_fin.size > 0 and n_random > 0:\n                        choose = self.rng.choice(rem_fin, size=min(n_random, rem_fin.size), replace=False).tolist()\n                        keep_idx.extend(choose)\n                # fill with some inf ones (if any) to keep archive diverse\n                if len(keep_idx) < self.archive_max and inf_idxs.size > 0:\n                    need = self.archive_max - len(keep_idx)\n                    choose_inf = self.rng.choice(inf_idxs, size=min(need, inf_idxs.size), replace=False).tolist()\n                    keep_idx.extend(choose_inf)\n                # if still less fill randomly\n                all_idx = np.arange(len(archive_X)).tolist()\n                if len(keep_idx) < self.archive_max:\n                    remaining = list(set(all_idx) - set(keep_idx))\n                    if remaining:\n                        add = self.rng.choice(remaining, size=min(len(remaining), self.archive_max - len(keep_idx)), replace=False).tolist()\n                        keep_idx.extend(add)\n                # apply pruning\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > stagn_threshold:\n                # perform a micro-restart: inject some perturbed candidates around best or elites\n                n_inject = min(1 + max(1, self.dim // 3), max(2, self.init_samples // 2))\n                for k in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.rand() < 0.8:\n                        # perturb best\n                        scale = gscale * (1.5 + self.rng.rand() * 2.0)\n                        candr = self.x_opt + self.rng.normal(0, scale * (ub - lb) * 0.5)\n                    else:\n                        # perturb a random elite\n                        if finite_mask.any():\n                            elite_idx = np.argsort(Ff)[:max(1, int(0.1 * len(Ff)))]\n                            anchor = Xf[self.rng.choice(elite_idx)]\n                        else:\n                            anchor = self._uniform_array(lb, ub)\n                        scale = gscale * (1.0 + self.rng.rand() * 3.0)\n                        candr = anchor + self.rng.normal(0, scale * (ub - lb) * 0.6)\n                    candr = self._reflect_bounds(candr, lb, ub)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.f_opt = fr\n                        self.x_opt = candr.copy()\n                # encourage exploration: enlarge gscale a bit after restart and reset stagn_count\n                gscale = min(5.0, gscale * (1.6 + 0.4 * self.rng.rand()))\n                stagn_count = 0\n                success_window = []\n\n        # final safety: return best finite point found, else center\n        if self.x_opt is None:\n            # pick best finite in archive if exists\n            F_arr = np.array(archive_F)\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                self.x_opt = archive_X[best_i].copy()\n                self.f_opt = float(F_arr[best_i])\n            else:\n                self.x_opt = 0.5 * (lb + ub)\n                self.f_opt = safe_eval(self.x_opt) if self.evals < self.budget else np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt).astype(float)", "configspace": "", "generation": 0, "feedback": "In the code, line 201, in __call__, the following error occurred:\nAttributeError: 'numpy.ndarray' object has no attribute 'ptp'\nOn line: logits = - (scores - scores.min()) / (1e-8 + (scores.ptp() if scores.ptp() > 0 else 1.0))", "error": "In the code, line 201, in __call__, the following error occurred:\nAttributeError: 'numpy.ndarray' object has no attribute 'ptp'\nOn line: logits = - (scores - scores.min()) / (1e-8 + (scores.ptp() if scores.ptp() > 0 else 1.0))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3ef68878-5775-469b-bc05-dc8822062393", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donors, PCA-guided elite moves,\n    anisotropic Gaussian local moves and heavy-tailed Cauchy jumps with short-term\n    scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(max(50, archive_max))\n        # short-term adaptation param (relative to bound range)\n        self.gscale = 0.5\n        # counters and trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # try to extract bounds from func if available, else default [-5,5]\n        lb = None\n        ub = None\n        # common patterns\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n            ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n        lb = lb or getattr(func, \"lower_bounds\", None) or getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        ub = ub or getattr(func, \"upper_bounds\", None) or getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        lb = lb or getattr(func, \"bounds_lb\", None)\n        ub = ub or getattr(func, \"bounds_ub\", None)\n        if lb is None or ub is None:\n            lb = -5.0\n            ub = 5.0\n        lb = np.atleast_1d(lb).astype(float)\n        ub = np.atleast_1d(ub).astype(float)\n        # broadcast scalars to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # pad/truncate\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        if lb.size > self.dim:\n            lb = lb[:self.dim]\n        if ub.size > self.dim:\n            ub = ub[:self.dim]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = x.copy().astype(float)\n        for _ in range(max_reflect):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            if low.any():\n                x[low] = 2 * lb[low] - x[low]\n            if high.any():\n                x[high] = 2 * ub[high] - x[high]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs(self, n, dim):\n        # simple Latin hypercube sampling in [0,1]^dim\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        u = rng.uniform(size=(n, dim))\n        a = cut[:n]\n        b = cut[1:n + 1]\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            points[:, j] = a + u[:, j] * (b - a)\n            rng.shuffle(points[:, j])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            shape = (n, self.dim)\n            return self.rng.uniform(lb, ub, size=shape)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        def safe_eval(x):\n            x = np.asarray(x).astype(float).ravel()\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            # enforce numeric\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            return f\n\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        range_vec = ub - lb\n        # initialization: center + LHS + one uniform\n        n0 = min(self.init_samples, max(4, self.budget // 20))\n        lhs01 = self._lhs(n0, self.dim)\n        init_points = lb + lhs01 * range_vec\n        center = 0.5 * (lb + ub)\n        uni = self._uniform_array(lb, ub)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first\n        f_center = safe_eval(center)\n        archive_X.append(center.copy())\n        archive_F.append(f_center)\n        if np.isfinite(f_center) and f_center < self.f_opt:\n            self.f_opt = f_center\n            self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(init_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = init_points[i]\n            # skip duplicates (close to center)\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform\n        if self.evals < self.budget:\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = f_uni\n                self.x_opt = uni.copy()\n\n        # short-term adaptation\n        gscale = float(self.gscale)\n        success_window = []\n        window_len = 30\n        stagn_count = 0\n        iters = 0\n        stagn_threshold = max(50, 10 * self.dim)\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X = np.array(archive_X)\n            F = np.array(archive_F)\n            finite_mask = np.isfinite(F)\n            if finite_mask.any():\n                Xf = X[finite_mask]\n                Ff = F[finite_mask]\n                # robust center and per-dim scale\n                median = np.median(Xf, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-8, q75 - q25)\n                small_mask = per_dim_scale < 1e-6\n                if small_mask.any():\n                    per_dim_scale[small_mask] = range_vec[small_mask] * 0.05\n            else:\n                median = 0.5 * (lb + ub)\n                per_dim_scale = range_vec * 0.2\n                Xf = np.empty((0, self.dim))\n                Ff = np.empty((0,))\n\n            # choose a base point biased towards better archive items (softmax on negative F)\n            finite_idx = np.nonzero(finite_mask)[0]\n            if finite_idx.size > 0:\n                # logits from negative fitness (lower better)\n                logits = -F[finite_idx] / (max(1e-6, np.std(F[finite_idx])) + 1e-6)\n                logits = logits - logits.max()  # stabilize\n                exp = np.exp(logits)\n                probs = exp / np.sum(exp)\n                base_idx = finite_idx[rng.choice(len(finite_idx), p=probs)]\n            else:\n                base_idx = rng.randrange(len(archive_X))\n            base = np.array(archive_X[base_idx]).copy()\n\n            # donors for DE-like moves\n            n_archive = len(archive_X)\n            pick_pool = list(range(n_archive))\n            if base_idx in pick_pool:\n                pick_pool.remove(base_idx)\n            donor_idx = []\n            if len(pick_pool) > 0:\n                kdon = min(3, len(pick_pool))\n                donor_idx = list(rng.choice(pick_pool, size=kdon, replace=False))\n            donors = [np.array(archive_X[i]).copy() for i in donor_idx] if donor_idx else []\n\n            # decide move type\n            p_global = 0.15 + 0.25 * (1.0 - min(1.0, gscale))\n            cand = base.copy()\n            if rng.rand() < p_global and len(donors) >= 2:\n                # GLOBAL move\n                if rng.rand() < 0.6:\n                    # DE-like global: base + F_scale*(d1-d2) + jitter\n                    F_scale = 0.6 + 0.6 * rng.rand()\n                    dmove = donors[0] - donors[1]\n                    cand = base + F_scale * dmove\n                    # add tempered long jump occasionally via Cauchy\n                    if rng.rand() < 0.25:\n                        c = rng.standard_cauchy(size=self.dim)\n                        cand = cand + (0.3 * gscale) * range_vec * c\n                else:\n                    # tempered Cauchy centered on a random anchor\n                    anchor = np.array(archive_X[rng.randrange(n_archive)]).copy()\n                    c = rng.standard_cauchy(size=self.dim)\n                    scale = gscale * (0.5 + 0.5 * rng.rand())\n                    cand = anchor + scale * range_vec * (0.8 + 0.4 * rng.rand()) * c\n                # small gaussian jitter and occasional uniform injection\n                cand += rng.normal(0, 0.02, size=self.dim) * range_vec\n                if rng.rand() < 0.08:\n                    d = rng.randint(0, self.dim)\n                    cand[d] = rng.uniform(lb[d], ub[d])\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                if finite_mask.any() and rng.rand() < 0.6:\n                    alpha = 0.03 + 0.12 * rng.rand()\n                    base_attracted = base + alpha * (median - base)\n                else:\n                    base_attracted = base.copy()\n                # anisotropic gaussian step\n                sigma = (per_dim_scale / (np.mean(per_dim_scale) + 1e-12)) * (gscale * 0.15)\n                sigma = np.clip(sigma * range_vec, 1e-8, 0.6 * range_vec)\n                cand = base_attracted + rng.normal(0, 1.0, size=self.dim) * sigma\n\n                # PCA-guided elite perturbation occasionally\n                if finite_mask.sum() >= 6 and rng.rand() < 0.20:\n                    num_elite = max(3, int(0.25 * finite_mask.sum()))\n                    elite_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elite_idx]\n                    # PCA on elites\n                    E_centered = elites - elites.mean(axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(E_centered, full_matrices=False)\n                        kpc = min(2, Vt.shape[0])\n                        weights = rng.normal(0, 1.0, size=kpc)\n                        pc_dir = np.dot(weights, Vt[:kpc, :])\n                        pc_dir_norm = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                        pc_scale = gscale * np.mean(per_dim_scale) * (0.4 + rng.rand() * 0.8)\n                        cand2 = elites.mean(axis=0) + pc_dir_norm * pc_scale\n                        # probabilistically choose PCA candidate if it's promising (small local perturbation)\n                        if rng.rand() < 0.5:\n                            cand = cand2\n                    except Exception:\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and rng.rand() < 0.22:\n                    F_scale = 0.4 + 0.8 * rng.rand()\n                    cand = cand + F_scale * (donors[0] - donors[1]) * (0.3 + 0.7 * rng.rand())\n\n                # tempered Cauchy jump occasionally for escape\n                if rng.rand() < 0.06:\n                    c = rng.standard_cauchy(size=self.dim)\n                    cand += (0.18 * gscale) * range_vec * c\n\n            # safety: if any non-finite values, replace them with uniform samples\n            cand = np.asarray(cand).astype(float)\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = rng.uniform(lb[bad], ub[bad])\n            # reflect to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                improved = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand):\n                F_all = np.array(archive_F)\n                finite_all = np.isfinite(F_all)\n                if finite_all.sum() > 0:\n                    rank = np.sum(F_all[finite_all] <= f_cand)\n                    pct = rank / max(1, finite_all.sum())\n                    if pct <= 0.25 or f_cand <= self.f_opt:\n                        success = True\n                    else:\n                        success = False\n                else:\n                    success = True\n            else:\n                success = False\n\n            success_window.append(1 if success else 0)\n            if len(success_window) > window_len:\n                success_window.pop(0)\n\n            # adapt gscale every few iterations\n            if iters % 5 == 0:\n                srate = np.mean(success_window) if success_window else 0.0\n                if srate > 0.25:\n                    gscale = max(0.02, gscale * 0.92)\n                elif srate < 0.05:\n                    gscale = min(5.0, gscale * 1.10)\n                else:\n                    gscale = gscale * (0.98 + 0.04 * rng.rand())\n                # jitter and clip\n                gscale = float(np.clip(gscale * (1.0 + 0.02 * (rng.rand() - 0.5)), 1e-3, 10.0))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max:\n                X_arr = np.array(archive_X)\n                F_arr = np.array(archive_F)\n                finite_idxs = np.where(np.isfinite(F_arr))[0]\n                inf_idxs = np.where(~np.isfinite(F_arr))[0]\n                n_keep = int(0.6 * self.archive_max)\n                keep_idx = []\n                if finite_idxs.size > 0:\n                    sorted_fin = finite_idxs[np.argsort(F_arr[finite_idxs])]\n                    keep_idx.extend(sorted_fin[:min(n_keep, sorted_fin.size)].tolist())\n                    rem_fin = sorted_fin[min(n_keep, sorted_fin.size):]\n                    n_random = max(0, self.archive_max - len(keep_idx) - len(inf_idxs))\n                    if rem_fin.size > 0 and n_random > 0:\n                        choose = rng.choice(rem_fin, size=min(n_random, rem_fin.size), replace=False).tolist()\n                        keep_idx.extend(choose)\n                if len(keep_idx) < self.archive_max and inf_idxs.size > 0:\n                    need = self.archive_max - len(keep_idx)\n                    choose_inf = rng.choice(inf_idxs, size=min(need, inf_idxs.size), replace=False).tolist()\n                    keep_idx.extend(choose_inf)\n                # fill remaining randomly\n                all_idx = list(range(len(archive_X)))\n                if len(keep_idx) < self.archive_max:\n                    remaining = list(set(all_idx) - set(keep_idx))\n                    if remaining:\n                        add = rng.choice(remaining, size=min(len(remaining), self.archive_max - len(keep_idx)),\n                                         replace=False).tolist()\n                        keep_idx.extend(add)\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > stagn_threshold:\n                n_inject = min(1 + max(1, self.dim // 3), max(2, self.init_samples // 2))\n                for k in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and rng.rand() < 0.8:\n                        scale = gscale * (1.2 + rng.rand() * 2.0)\n                        candr = self.x_opt + rng.normal(0, scale, size=self.dim) * (range_vec * 0.5)\n                    else:\n                        if finite_mask.any():\n                            elite_idx = np.argsort(Ff)[:max(1, int(0.15 * len(Ff)))]\n                            anchor = Xf[rng.choice(elite_idx)]\n                        else:\n                            anchor = self._uniform_array(lb, ub)\n                        scale = gscale * (0.8 + rng.rand() * 2.5)\n                        candr = anchor + rng.normal(0, scale, size=self.dim) * (range_vec * 0.6)\n                    candr = self._reflect_bounds(candr, lb, ub)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.f_opt = fr\n                        self.x_opt = candr.copy()\n                # encourage exploration after restart\n                gscale = min(5.0, gscale * (1.5 + 0.6 * rng.rand()))\n                stagn_count = 0\n                success_window = []\n\n        # final safety: pick best finite point from archive if x_opt still None\n        if self.x_opt is None:\n            F_arr = np.array(archive_F)\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                self.x_opt = np.array(archive_X[best_i]).copy()\n                self.f_opt = float(F_arr[best_i])\n            else:\n                self.x_opt = 0.5 * (lb + ub)\n                if self.evals < self.budget:\n                    f0 = safe_eval(self.x_opt)\n                    self.f_opt = f0\n                else:\n                    self.f_opt = np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt).astype(float)", "configspace": "", "generation": 0, "feedback": "In the code, line 34, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "error": "In the code, line 34, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7cda29c9-bbcb-4e1b-ae63-4d097c7913aa", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-like donors, PCA-guided elite moves, anisotropic Gaussian local steps, tempered Cauchy jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(max(50, archive_max))\n        # short-term adaptation param (relative to bound range)\n        self.gscale = 0.5\n        # counters and trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # try to extract bounds from func if available, else default [-5,5]\n        lb = None\n        ub = None\n        # common patterns\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # try various common names\n            ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n            lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n        lb = lb or getattr(func, \"lower_bounds\", None) or getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        ub = ub or getattr(func, \"upper_bounds\", None) or getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        # fallback\n        if lb is None or ub is None:\n            lb = -5.0\n            ub = 5.0\n        lb = np.atleast_1d(lb).astype(float)\n        ub = np.atleast_1d(ub).astype(float)\n        # broadcast scalars to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # pad/truncate\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        if lb.size > self.dim:\n            lb = lb[:self.dim]\n        if ub.size > self.dim:\n            ub = ub[:self.dim]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = np.array(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            x[low] = 2 * lb[low] - x[low]\n            x[high] = 2 * ub[high] - x[high]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube sampling in [0,1]^dim\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            a = cut[:-1]\n            b = cut[1:]\n            u = rng.uniform(size=n)\n            points[:, j] = a + u * (b - a)\n            rng.shuffle(points[:, j])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            shape = (n, self.dim)\n            return self.rng.uniform(lb, ub, size=shape)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            # enforce numeric and count eval\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        range_vec = ub - lb\n        # initialization: center + LHS + one uniform\n        center = 0.5 * (lb + ub)\n        n0 = max(2, min(self.init_samples, self.budget))\n        lhs01 = self._lhs01(n0, self.dim)\n        init_points = lb + lhs01 * range_vec\n        # also include 1 uniform\n        uni = self._uniform_array(lb, ub, n=1)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(init_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = init_points[i]\n            # skip duplicates (close to center)\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform\n        if self.evals < self.budget:\n            uni = np.asarray(uni).reshape(-1)\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.x_opt = uni.copy()\n                self.f_opt = f_uni\n\n        # short-term adaptation\n        gscale = float(self.gscale)\n        success_window = []\n        window_len = 30\n        stagn_count = 0\n        iters = 0\n        stagn_threshold = max(50, 10 * self.dim)\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X = np.array(archive_X) if archive_X else np.empty((0, self.dim))\n            F = np.array(archive_F) if archive_F else np.empty((0,))\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0] if F.size > 0 else np.array([], dtype=int)\n\n            # robust center and per-dim scale from finite archive\n            if finite_idx.size > 0:\n                Xf = X[finite_idx]\n                Ff = F[finite_idx]\n                median = np.median(Xf, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-8, q75 - q25)\n                # guard against too small scales\n                small_mask = per_dim_scale < 1e-8\n                if small_mask.any():\n                    per_dim_scale[small_mask] = range_vec[small_mask] * 0.05\n            else:\n                median = center.copy()\n                per_dim_scale = range_vec * 0.2\n                Xf = np.empty((0, self.dim))\n                Ff = np.empty((0,))\n\n            # choose a base point biased towards better archive items (softmax on negative F)\n            if finite_idx.size > 0:\n                stdF = np.std(Ff)\n                denom = max(1e-6, stdF)\n                logits = -Ff / denom\n                logits = logits - logits.max()\n                exp_logits = np.exp(logits)\n                probs = exp_logits / np.sum(exp_logits)\n                base_idx = finite_idx[rng.choice(len(finite_idx), p=probs)]\n                base = np.array(archive_X[base_idx]).copy()\n            else:\n                if len(archive_X) > 0:\n                    base_idx = rng.randrange(len(archive_X))\n                    base = np.array(archive_X[base_idx]).copy()\n                else:\n                    base = center.copy()\n\n            # donors for DE-like moves\n            n_archive = len(archive_X)\n            pick_pool = [i for i in range(n_archive) if i != base_idx] if n_archive > 0 else []\n            donor_idx = []\n            if len(pick_pool) > 0:\n                kdon = min(3, len(pick_pool))\n                donor_idx = list(rng.choice(pick_pool, size=kdon, replace=False))\n            donors = [np.array(archive_X[i]).copy() for i in donor_idx] if donor_idx else []\n\n            # decide move type\n            p_global = 0.15 + 0.25 * (1.0 - min(1.0, gscale))\n            cand = base.copy()\n            if rng.rand() < p_global and len(donors) >= 2:\n                # GLOBAL move: DE-like + tempered Cauchy jumps anchored on a random archived elite or uniform\n                F_scale = 0.6 + 0.6 * rng.rand()\n                dmove = donors[0] - donors[1]\n                cand = base + F_scale * dmove\n                # tempered long jump occasionally via Cauchy\n                if rng.rand() < 0.25:\n                    c = rng.standard_cauchy(size=self.dim)\n                    cand = cand + (0.3 * gscale) * range_vec * (0.5 + 0.5 * rng.rand()) * c\n                # anchor around a random elite or uniform point\n                if finite_idx.size > 0 and rng.rand() < 0.6:\n                    anchor = Xf[rng.randint(0, Xf.shape[0])]\n                else:\n                    anchor = self._uniform_array(lb, ub)\n                c = rng.standard_cauchy(size=self.dim)\n                scale = gscale * (0.4 + 0.6 * rng.rand())\n                cand = anchor + scale * range_vec * (0.6 + 0.8 * rng.rand()) * (c / (1.0 + np.abs(c)))  # tempered\n                # small gaussian jitter and occasional uniform injection\n                cand += rng.normal(0, 0.02, size=self.dim) * range_vec\n                if rng.rand() < 0.08:\n                    d = rng.randint(0, self.dim)\n                    cand[d] = rng.uniform(lb[d], ub[d])\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                if finite_mask.any() and rng.rand() < 0.6:\n                    alpha = 0.03 + 0.12 * rng.rand()\n                    base_attracted = base + alpha * (median - base)\n                else:\n                    base_attracted = base.copy()\n                # anisotropic gaussian step\n                mean_scale = np.mean(per_dim_scale) + 1e-12\n                sigma = (per_dim_scale / mean_scale) * (gscale * 0.15)\n                sigma = np.clip(sigma * range_vec, 1e-10, 0.6 * range_vec)\n                cand = base_attracted + rng.normal(0, 1.0, size=self.dim) * sigma\n\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 6 and rng.rand() < 0.20:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elite_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elite_idx]\n                    try:\n                        # center elites and SVD\n                        E = elites - np.mean(elites, axis=0)\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        kpc = min(3, Vt.shape[0])\n                        weights = rng.normal(0, 1.0, size=kpc)\n                        pc_dir = np.dot(weights, Vt[:kpc, :])\n                        pc_dir_norm = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                        pc_scale = gscale * mean_scale * (0.4 + rng.rand() * 0.8)\n                        cand2 = elites.mean(axis=0) + pc_dir_norm * pc_scale\n                        # probabilistically choose PCA candidate if it's promising (small local perturbation)\n                        if rng.rand() < 0.5:\n                            cand = cand2\n                    except Exception:\n                        # fallback: ignore PCA\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and rng.rand() < 0.22:\n                    F_scale = 0.4 + 0.8 * rng.rand()\n                    cand = cand + F_scale * (donors[0] - donors[1]) * (0.3 + 0.7 * rng.rand())\n\n                # tempered Cauchy jump occasionally for escape\n                if rng.rand() < 0.06:\n                    c = rng.standard_cauchy(size=self.dim)\n                    cand += (0.18 * gscale) * range_vec * (c / (1.0 + np.abs(c)))\n\n            # safety: if any non-finite values, replace them with uniform samples\n            cand = np.asarray(cand).astype(float)\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = rng.uniform(lb[bad], ub[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                improved = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand):\n                F_all = np.array(archive_F)\n                finite_all = np.isfinite(F_all)\n                if finite_all.sum() > 0:\n                    sorted_vals = np.sort(F_all[finite_all])\n                    rank = np.searchsorted(sorted_vals, f_cand, side='left') + 1\n                    pct = rank / float(sorted_vals.size)\n                    if pct <= 0.25 or f_cand <= self.f_opt:\n                        success = True\n            success_window.append(1 if success else 0)\n            if len(success_window) > window_len:\n                success_window.pop(0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                srate = float(np.mean(success_window)) if success_window else 0.0\n                if srate > 0.25:\n                    gscale = max(1e-3, gscale * (0.95 - 0.02 * rng.rand()))\n                elif srate < 0.05:\n                    gscale = gscale * (1.05 + 0.1 * rng.rand())\n                else:\n                    gscale = gscale * (0.98 + 0.04 * rng.rand())\n                # small jitter and clip\n                gscale = float(np.clip(gscale * (1.0 + 0.02 * (rng.rand() - 0.5)), 1e-3, 10.0))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max:\n                X_arr = np.array(archive_X)\n                F_arr = np.array(archive_F)\n                finite_idxs = np.where(np.isfinite(F_arr))[0]\n                inf_idxs = np.where(~np.isfinite(F_arr))[0]\n                n_keep = int(0.6 * self.archive_max)\n                keep_idx = []\n                if finite_idxs.size > 0:\n                    sorted_fin = finite_idxs[np.argsort(F_arr[finite_idxs])]\n                    take = min(n_keep, sorted_fin.size)\n                    keep_idx.extend(sorted_fin[:take].tolist())\n                    rem_fin = sorted_fin[take:]\n                    n_random = max(0, self.archive_max - len(keep_idx) - len(inf_idxs))\n                    if rem_fin.size > 0 and n_random > 0:\n                        choose = rng.choice(rem_fin, size=min(n_random, rem_fin.size), replace=False).tolist()\n                        keep_idx.extend(choose)\n                # keep some infinities (failed evals) only if room\n                if len(keep_idx) < self.archive_max and inf_idxs.size > 0:\n                    need = self.archive_max - len(keep_idx)\n                    choose_inf = rng.choice(inf_idxs, size=min(need, inf_idxs.size), replace=False).tolist()\n                    keep_idx.extend(choose_inf)\n                # fill remaining randomly\n                all_idx = list(range(len(archive_X)))\n                if len(keep_idx) < self.archive_max:\n                    remaining = list(set(all_idx) - set(keep_idx))\n                    if remaining:\n                        add = rng.choice(remaining, size=min(len(remaining), self.archive_max - len(keep_idx)),\n                                         replace=False).tolist()\n                        keep_idx.extend(add)\n                keep_idx = sorted(set(keep_idx))\n                # apply pruning\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > stagn_threshold:\n                # perform micro-restarts: inject a few diversified points\n                n_inject = min(1 + max(1, self.dim // 3), max(2, self.init_samples // 2))\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and rng.rand() < 0.8:\n                        # around best\n                        scale = gscale * (1.2 + rng.rand() * 2.0)\n                        candr = self.x_opt + rng.normal(0, scale, size=self.dim) * (range_vec * 0.5)\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub)\n                    # ensure within bounds\n                    candr = self._reflect_bounds(candr, lb, ub)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.f_opt = fr\n                        self.x_opt = candr.copy()\n                # encourage exploration after restart\n                gscale = min(5.0, gscale * (1.5 + 0.6 * rng.rand()))\n                stagn_count = 0\n                success_window = []\n\n            # safety break (in case)\n            if self.evals >= self.budget:\n                break\n\n        # final safety: pick best finite point from archive if x_opt still None\n        if self.x_opt is None:\n            F_arr = np.array(archive_F)\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                self.x_opt = np.array(archive_X[best_i]).copy()\n                self.f_opt = float(F_arr[best_i])\n            else:\n                # evaluate center if possible\n                if self.evals < self.budget:\n                    f0 = safe_eval(center)\n                    self.f_opt = f0\n                    self.x_opt = center.copy()\n                else:\n                    self.f_opt = np.inf\n                    self.x_opt = center.copy()\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)", "error": "In the code, line 33, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5e0c318c-9b3d-4fab-bd06-0b70e6fa7b27", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-like recombination, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(archive_max)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # try to extract bounds from func if available, else default [-5,5]\n        lb = None\n        ub = None\n\n        # common attribute patterns\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)\n            ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"upper_bounds\", None)\n\n        lb = lb or getattr(func, \"lower_bounds\", None) or getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        ub = ub or getattr(func, \"upper_bounds\", None) or getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        if lb is None or ub is None:\n            # default BBOB-like bounds\n            lb = -5.0\n            ub = 5.0\n\n        lb = np.atleast_1d(np.asarray(lb, dtype=float))\n        ub = np.atleast_1d(np.asarray(ub, dtype=float))\n\n        # broadcast scalars to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # pad/truncate\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        if lb.size > self.dim:\n            lb = lb[:self.dim]\n        if ub.size > self.dim:\n            ub = ub[:self.dim]\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=8):\n        x = np.array(x, dtype=float).copy()\n        for _ in range(max_reflect):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            x[low] = 2 * lb[low] - x[low]\n            x[high] = 2 * ub[high] - x[high]\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube sampling in [0,1]^dim\n        cut = np.linspace(0, 1, n + 1)\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            a = cut[:-1]\n            b = cut[1:]\n            u = self.rng.uniform(size=n)\n            points[:, j] = a + u * (b - a)\n            self.rng.shuffle(points[:, j])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            shape = (n, self.dim)\n            return self.rng.uniform(lb, ub, size=shape)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            # ensure numeric\n            if not np.isfinite(f):\n                f = np.inf\n            self.evals += 1\n            return f\n\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        range_vec = ub - lb\n\n        # initialization: center + LHS + one uniform\n        center = 0.5 * (lb + ub)\n        n0 = max(1, self.init_samples - 2)\n        lhs01 = self._lhs01(n0, self.dim)\n        init_points = lb + lhs01 * range_vec\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center):\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for x in init_points:\n            if self.evals >= self.budget:\n                break\n            # skip duplicates (close to center)\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform\n        if self.evals < self.budget:\n            uni = self._uniform_array(lb, ub, n=1)\n            uni = np.asarray(uni).reshape(-1)\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = f_uni\n                self.x_opt = uni.copy()\n\n        # short-term adaptation\n        success_window = []\n        window_len = 30\n        stagn_count = 0\n        iters = 0\n        stagn_threshold = max(50, 10 * self.dim)\n\n        # adaptation scale\n        gscale = 0.2  # global scale factor relative to range_vec\n        F_scale = 0.8  # factor for DE-like donors influence\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X = np.array(archive_X) if archive_X else np.empty((0, self.dim))\n            F = np.array(archive_F) if archive_F else np.empty((0,))\n            finite_idx = np.where(np.isfinite(F))[0] if F.size > 0 else np.array([], dtype=int)\n\n            # robust center and per-dim scale from finite archive\n            if finite_idx.size > 0:\n                Xf = X[finite_idx]\n                Ff = F[finite_idx]\n                median = np.median(Xf, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                per_dim_scale = np.maximum(1e-12, q75 - q25)\n                # guard against too small scales\n                small_mask = per_dim_scale < 1e-8\n                per_dim_scale[small_mask] = range_vec[small_mask] * 0.05\n            else:\n                # fallback\n                median = center.copy()\n                per_dim_scale = range_vec * 0.2\n                Xf = np.empty((0, self.dim))\n                Ff = np.empty((0,))\n\n            # choose a base point biased towards better archive items (softmax on negative F)\n            if finite_idx.size > 0:\n                stdF = np.std(Ff)\n                denom = max(1e-6, stdF)\n                logits = -Ff / denom\n                exp_logits = np.exp(logits - logits.max())\n                probs = exp_logits / exp_logits.sum()\n                # choose an index in Xf, then map to archive index\n                rel_choice = rng.choice(len(Ff), p=probs)\n                base_idx = finite_idx[rel_choice]\n                base = np.array(archive_X[base_idx]).copy()\n            else:\n                if len(archive_X) > 0:\n                    base_idx = rng.randrange(len(archive_X))\n                    base = np.array(archive_X[base_idx]).copy()\n                else:\n                    base_idx = None\n                    base = center.copy()\n\n            # donors for DE-like moves\n            n_archive = len(archive_X)\n            pick_pool = [i for i in range(n_archive) if i != base_idx] if n_archive > 0 else []\n            donors = []\n            if len(pick_pool) >= 2:\n                donors = rng.choice(pick_pool, size=2, replace=False).tolist()\n\n            # decide move type\n            # global tendency: if gscale is big, more global moves\n            p_global = 0.15 + 0.25 * (1.0 - min(1.0, gscale))\n            cand = base.copy()\n\n            if rng.rand() < p_global and (len(donors) >= 2 or finite_idx.size > 0):\n                # GLOBAL move: anchored tempered Cauchy jump\n                if finite_idx.size > 0 and rng.rand() < 0.6:\n                    anchor = Xf[rng.randint(0, Xf.shape[0])]\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n                c = rng.standard_cauchy(size=self.dim)\n                # temper the Cauchy tails\n                tc = c / (1.0 + np.abs(c))\n                scale = max(1e-6, gscale) * (0.4 + 0.6 * rng.rand())\n                cand = anchor + scale * range_vec * (0.6 + 0.8 * rng.rand()) * tc\n                # small gaussian jitter\n                if rng.rand() < 0.08:\n                    d = rng.randint(0, self.dim)\n                    cand[d] += rng.normal(0, 0.2 * range_vec[d])\n                # sometimes mix DE donor for additional directionality\n                if len(donors) >= 2 and rng.rand() < 0.3:\n                    a = np.array(archive_X[donors[0]])\n                    b = np.array(archive_X[donors[1]])\n                    cand = cand + F_scale * (a - b) * (0.3 + 0.7 * rng.rand())\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                alpha = 0.03 + 0.12 * rng.rand()\n                if rng.rand() < 0.6:\n                    base_attracted = base + alpha * (median - base)\n                else:\n                    base_attracted = base.copy()\n                mean_scale = np.mean(per_dim_scale) + 1e-12\n                sigma = (per_dim_scale / mean_scale) * (gscale * 0.15)\n                sigma = np.clip(sigma * range_vec, 1e-10, 0.6 * range_vec)\n                cand = base_attracted + rng.normal(0, 1.0, size=self.dim) * sigma\n\n                # PCA-guided elite perturbation occasionally\n                if finite_idx.size >= 4 and rng.rand() < 0.18:\n                    try:\n                        num_elite = min(max(3, int(0.2 * len(Ff))), len(Ff))\n                        elite_idx = np.argsort(Ff)[:num_elite]\n                        elites = Xf[elite_idx]\n                        E = elites - elites.mean(axis=0)\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        kpc = min(3, Vt.shape[0])\n                        # weights favour top PCs with some randomness\n                        weights = (S[:kpc] / (S[:kpc].sum() + 1e-12)) * (0.6 + 0.8 * rng.rand(kpc))\n                        pc_dir = np.dot(weights, Vt[:kpc, :])\n                        pc_dir_norm = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                        pc_scale = gscale * mean_scale * (0.4 + rng.rand() * 0.8)\n                        cand2 = elites.mean(axis=0) + pc_dir_norm * pc_scale * range_vec\n                        if rng.rand() < 0.6:\n                            cand = cand2\n                    except Exception:\n                        # ignore PCA failures\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and rng.rand() < 0.22:\n                    a = np.array(archive_X[donors[0]])\n                    b = np.array(archive_X[donors[1]])\n                    cand = cand + F_scale * (a - b) * (0.3 + 0.7 * rng.rand())\n\n                # occasional tempered escape along local branch\n                if rng.rand() < 0.04:\n                    c = rng.standard_cauchy(size=self.dim)\n                    cand += gscale * (0.2 + rng.rand() * 0.8) * range_vec * (c / (1.0 + np.abs(c)))\n\n            # safety: if any non-finite values, replace them with uniform samples\n            cand = np.asarray(cand).astype(float)\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = rng.uniform(lb[bad], ub[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            if self.evals >= self.budget:\n                break\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                improved = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand):\n                F_all = np.array(archive_F)\n                finite_all = np.isfinite(F_all)\n                if finite_all.sum() > 0:\n                    sorted_vals = np.sort(F_all[finite_all])\n                    rank = (F_all[finite_all] <= f_cand).sum()\n                    pct = float(rank) / float(sorted_vals.size)\n                    if pct <= 0.25 or f_cand <= self.f_opt:\n                        success = True\n            success_window.append(1.0 if success or improved else 0.0)\n            if len(success_window) > window_len:\n                success_window.pop(0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                srate = float(np.mean(success_window)) if success_window else 0.0\n                if srate > 0.35:\n                    # too successful -> tighten (exploit)\n                    gscale = gscale * (0.90 + 0.05 * rng.rand())\n                elif srate < 0.08:\n                    # poor success -> expand (explore)\n                    gscale = gscale * (1.08 + 0.08 * rng.rand())\n                else:\n                    # moderate -> slight cooling\n                    gscale = gscale * (0.98 + 0.04 * rng.rand())\n                # small jitter and clip\n                gscale = float(np.clip(gscale * (1.0 + 0.02 * (rng.rand() - 0.5)), 1e-4, 5.0))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max:\n                X_arr = np.array(archive_X)\n                F_arr = np.array(archive_F)\n                finite_idxs = np.where(np.isfinite(F_arr))[0]\n                inf_idxs = np.where(~np.isfinite(F_arr))[0]\n                n_keep = int(0.6 * self.archive_max)\n                keep_idx = []\n\n                if finite_idxs.size > 0:\n                    sorted_fin = finite_idxs[np.argsort(F_arr[finite_idxs])]\n                    take = min(n_keep, sorted_fin.size)\n                    keep_idx.extend(sorted_fin[:take].tolist())\n                    rem_fin = sorted_fin[take:]\n                    n_random = max(0, self.archive_max - len(keep_idx) - len(inf_idxs))\n                    if rem_fin.size > 0 and n_random > 0:\n                        choose = rng.choice(rem_fin, size=min(n_random, rem_fin.size), replace=False).tolist()\n                        keep_idx.extend(choose)\n\n                # keep some infinities (failed evals) only if room\n                need = self.archive_max - len(keep_idx)\n                if need > 0 and inf_idxs.size > 0:\n                    choose_inf = rng.choice(inf_idxs, size=min(need, inf_idxs.size), replace=False).tolist()\n                    keep_idx.extend(choose_inf)\n\n                # fill remaining randomly\n                all_idx = list(range(len(archive_X)))\n                if len(keep_idx) < self.archive_max:\n                    remaining = list(set(all_idx) - set(keep_idx))\n                    if remaining:\n                        add = rng.choice(remaining, size=min(len(remaining), self.archive_max - len(keep_idx)),\n                                         replace=False).tolist()\n                        keep_idx.extend(add)\n\n                keep_idx = sorted(set(keep_idx))\n                # apply pruning\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > stagn_threshold:\n                # perform micro-restarts: inject a few diversified points\n                n_inject = min(1 + max(1, self.dim // 3), max(2, self.init_samples // 2))\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and rng.rand() < 0.8:\n                        # around best\n                        scale = gscale * (1.2 + rng.rand() * 2.0)\n                        candr = self.x_opt + rng.normal(0, scale, size=self.dim) * (range_vec * 0.5)\n                    else:\n                        # random injection across full domain\n                        candr = self._uniform_array(lb, ub, n=1)\n                    candr = self._reflect_bounds(candr, lb, ub)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.f_opt = fr\n                        self.x_opt = candr.copy()\n                # encourage exploration after restart\n                success_window = []\n                stagn_count = 0\n\n            # safety break (in case)\n            if self.evals >= self.budget:\n                break\n\n        # final safety: pick best finite point from archive if x_opt still None\n        if self.x_opt is None:\n            F_arr = np.array(archive_F) if archive_F else np.array([])\n            if F_arr.size > 0:\n                finite_idxs = np.where(np.isfinite(F_arr))[0]\n                if finite_idxs.size > 0:\n                    best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                    self.x_opt = np.array(archive_X[best_i]).copy()\n                    self.f_opt = float(F_arr[best_i])\n                else:\n                    # evaluate center if possible\n                    if self.evals < self.budget:\n                        f0 = safe_eval(center)\n                        self.f_opt = f0\n                        self.x_opt = center.copy()\n                    else:\n                        self.f_opt = np.inf\n                        self.x_opt = center.copy()\n            else:\n                # nothing evaluated (shouldn't happen), return center\n                if self.evals < self.budget:\n                    f0 = safe_eval(center)\n                    self.f_opt = f0\n                    self.x_opt = center.copy()\n                else:\n                    self.f_opt = np.inf\n                    self.x_opt = center.copy()\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 31, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "error": "In the code, line 31, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lower_bounds\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "deaee0ea-b3d7-403c-9fcc-35a4f7baaa92", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid search mixing DE-style donors, PCA-guided elite moves, anisotropic local Gaussian steps, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-Guided Adaptive Directional Sampling — MG-ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(archive_max)\n        self.gscale = 0.5  # short-term adaptation scale (relative to range)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        lb = None\n        ub = None\n        # common patterns\n        for name in (\"lb\", \"lower\", \"lower_bounds\", \"bounds_low\"):\n            if hasattr(func, name):\n                lb = getattr(func, name)\n                break\n        for name in (\"ub\", \"upper\", \"upper_bounds\", \"bounds_up\"):\n            if hasattr(func, name):\n                ub = getattr(func, name)\n                break\n        # also try tuple/dict property \"bounds\"\n        if lb is None or ub is None:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # b might be (lb, ub) or object with attributes\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = lb or b[0]\n                    ub = ub or b[1]\n                else:\n                    lb = lb or getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                    ub = ub or getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n        # fallback default\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n        lb = np.atleast_1d(np.array(lb, dtype=float))\n        ub = np.atleast_1d(np.array(ub, dtype=float))\n        # broadcast scalars to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n        # pad/truncate if sizes differ\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds, do a few iterations to settle\n        x = np.asarray(x, dtype=float)\n        for _ in range(8):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            if low.any():\n                x[low] = 2 * lb[low] - x[low]\n            if high.any():\n                x[high] = 2 * ub[high] - x[high]\n        # final clamp to handle any remaining numerical issues\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim\n        cut = np.linspace(0, 1, n + 1)\n        u = self.rng.random((n, dim))\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            a = cut[:n]\n            b = cut[1:n+1]\n            pts = a + u[:, j] * (b - a)\n            self.rng.shuffle(pts)\n            points[:, j] = pts\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            shape = (n, self.dim)\n            # broadcasting per-dim\n            return self.rng.uniform(0.0, 1.0, size=shape) * (ub - lb) + lb\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            # ensure numeric\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            return f\n\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        center = 0.5 * (lb + ub)\n\n        # initialization: center + LHS + one uniform\n        n0 = max(2, min(self.init_samples, max(2, self.budget // 50 + 1)))\n        lhs01 = self._lhs01(n0, self.dim)\n        init_points = lb + lhs01 * range_vec  # shape (n0, dim)\n        uni = self._uniform_array(lb, ub, n=1)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center):\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(init_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = init_points[i]\n            # skip duplicates near center\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform\n        if self.evals < self.budget:\n            x = uni\n            f_uni = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.x_opt = x.copy()\n                self.f_opt = f_uni\n\n        # short-term adaptation and control variables\n        gscale = float(self.gscale)\n        success_window = []\n        window_len = 30\n        stagn_count = 0\n        iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            # prepare arrays\n            X_arr = np.array(archive_X) if archive_X else np.empty((0, self.dim))\n            F_arr = np.array(archive_F) if archive_F else np.empty((0,))\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n\n            # robust center (median of finite points) and per-dim scale (75-25 percentile)\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                median = np.median(Xf, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-8, q75 - q25)\n                mean_scale = np.mean(per_dim_scale + 1e-12)\n            else:\n                Xf = np.empty((0, self.dim))\n                Ff = np.empty((0,))\n                median = center.copy()\n                per_dim_scale = np.maximum(1e-8, 0.1 * range_vec)\n                mean_scale = np.mean(per_dim_scale)\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_idx.size > 0:\n                stdF = np.std(Ff) + 1e-9\n                logits = -(Ff - Ff.min()) / stdF\n                logits = logits - np.max(logits)\n                exp_logits = np.exp(logits)\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = self.rng.choice(len(finite_idx), p=probs)\n                base = Xf[pick_idx].copy()\n            else:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            pick_pool = finite_idx.tolist() if finite_idx.size > 0 else []\n            donors = []\n            if len(pick_pool) >= 2:\n                kdon = min(4, max(2, len(pick_pool)//5))\n                chosen = self.rng.choice(pick_pool, size=min(kdon, len(pick_pool)), replace=False)\n                donors = [np.array(archive_X[int(i)]).copy() for i in chosen]\n\n            # decide move type\n            p_global = 0.16  # probability of global jump\n            cand = base.copy()\n            # compute a tempered cauchy vector (used optionally)\n            c = self.rng.standard_cauchy(self.dim)\n            c = c / (1.0 + np.abs(c))  # temper extremes\n\n            if self.rng.random() < p_global and len(donors) >= 2:\n                # GLOBAL move: DE-like + tempered Cauchy jumps anchored on a random archived elite or uniform\n                anchor_choice = None\n                if finite_idx.size > 0 and self.rng.random() < 0.9:\n                    # pick a random elite (biased toward better)\n                    elite_count = max(1, min(10, Xf.shape[0]//2))\n                    elite_idx = np.argsort(Ff)[:elite_count]\n                    anchor_choice = Xf[self.rng.choice(elite_idx)]\n                else:\n                    anchor_choice = self._uniform_array(lb, ub)\n                # DE difference\n                dmove = donors[0] - donors[1]\n                cand = anchor_choice + 0.7 * (gscale * (0.6 + 0.8 * self.rng.random())) * range_vec * (dmove / (np.linalg.norm(dmove)+1e-12))\n                # tempered Cauchy perturbation\n                scale = gscale * (0.6 + 0.9 * self.rng.random())\n                cand = cand + scale * range_vec * c * (0.5 + self.rng.random() * 1.0)\n                # occasional uniform injection of one coordinate\n                if self.rng.random() < 0.08:\n                    d = self.rng.integers(0, self.dim)\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                if self.rng.random() < 0.3:\n                    alpha = 0.03 + 0.12 * self.rng.random()\n                    base_attracted = base + alpha * (median - base)\n                else:\n                    base_attracted = base.copy()\n                # anisotropic gaussian step\n                sigma = (per_dim_scale / (mean_scale + 1e-12)) * (gscale * 0.12)\n                sigma = np.clip(sigma * range_vec, 1e-10, 0.6 * range_vec)\n                cand = base_attracted + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 6 and self.rng.random() < 0.20:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elite_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elite_idx]\n                    try:\n                        E = elites - np.mean(elites, axis=0, keepdims=True)\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        pc_dir = Vt[0]  # first principal direction\n                        pc_dir_norm = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                        pc_scale = gscale * mean_scale * (0.4 + self.rng.random() * 0.8)\n                        cand2 = elites.mean(axis=0) + pc_dir_norm * pc_scale\n                        # choose PCA candidate with some probability\n                        if self.rng.random() < 0.5:\n                            # small jitter around cand2\n                            cand = cand2 + self.rng.normal(0, 0.1, size=self.dim) * (sigma * 0.6)\n                    except Exception:\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.22:\n                    F_scale = 0.4 + 0.8 * self.rng.random()\n                    cand = cand + F_scale * (donors[0] - donors[1]) * (0.3 + 0.7 * self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.06:\n                    cand = cand + (0.22 * gscale) * range_vec * c\n\n            # safety: if any non-finite values, replace them with uniform samples\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = self.rng.uniform(lb[bad], ub[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                improved = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            finite_vals = F_arr[np.isfinite(F_arr)] if F_arr.size > 0 else np.array([])\n            if np.isfinite(f_cand):\n                if finite_vals.size == 0:\n                    success = True\n                else:\n                    sorted_vals = np.sort(np.concatenate((finite_vals, [f_cand])))\n                    rank = np.searchsorted(sorted_vals, f_cand, side='left') + 1\n                    pct = rank / float(sorted_vals.size)\n                    if pct <= 0.25 or (self.f_opt is not None and f_cand <= self.f_opt):\n                        success = True\n            success_window.append(1 if success else 0)\n            if len(success_window) > window_len:\n                success_window.pop(0)\n            srate = (sum(success_window) / len(success_window)) if success_window else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                # shrink slowly if many successes, grow if stagnant\n                if srate > 0.2:\n                    gscale = max(1e-3, gscale * (0.90 - 0.02 * self.rng.random()))\n                elif srate < 0.05:\n                    gscale = min(10.0, gscale * (1.06 + 0.12 * self.rng.random()))\n                else:\n                    # small jitter\n                    gscale = max(1e-3, gscale * (0.98 + 0.04 * (self.rng.random()-0.5)))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max * 2:\n                X_arr = np.array(archive_X)\n                F_arr = np.array(archive_F)\n                all_idx = np.arange(len(F_arr))\n                fin_idx = np.where(np.isfinite(F_arr))[0]\n                inf_idx = np.where(~np.isfinite(F_arr))[0]\n                keep_idx = []\n                # keep top fraction of finite\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort(F_arr[fin_idx])]\n                    take = int(self.archive_max * 0.8)\n                    keep_idx.extend(sorted_fin[:min(take, sorted_fin.size)].tolist())\n                # keep some random finite rest\n                remaining = list(set(all_idx.tolist()) - set(keep_idx))\n                if len(keep_idx) < self.archive_max and len(remaining) > 0:\n                    need = self.archive_max - len(keep_idx)\n                    choose = list(self.rng.choice(remaining, size=min(len(remaining), need), replace=False))\n                    keep_idx.extend(choose)\n                # ensure unique and sorted\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(40, 6 * self.dim) or (iters > 100 and srate < 0.02):\n                # perform micro-restarts: inject a few diversified points\n                n_inject = min(1 + max(1, self.dim // 3), max(2, self.init_samples // 2))\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.85:\n                        # around best (local diversified)\n                        scale = gscale * (1.2 + self.rng.random() * 2.0)\n                        candr = self.x_opt + self.rng.normal(0, scale, size=self.dim) * (0.5 * range_vec)\n                        candr = self._reflect_bounds(candr, lb, ub)\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.f_opt = fr\n                        self.x_opt = candr.copy()\n                # encourage exploration after restart\n                gscale = min(8.0, gscale * (1.5 + 0.6 * self.rng.random()))\n                stagn_count = 0\n                success_window = []\n\n        # final safety: pick best finite point from archive if x_opt still None\n        if self.x_opt is None and archive_F:\n            F_arr = np.array(archive_F)\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                self.x_opt = np.array(archive_X[best_i]).copy()\n                self.f_opt = float(F_arr[best_i])\n            else:\n                # fallback to center evaluation if possible (but budget exhausted usually)\n                self.x_opt = center.copy()\n                self.f_opt = float(safe_eval(center)) if self.evals < self.budget else np.inf\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 46, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = lb or getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "error": "In the code, line 46, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = lb or getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7fedf9d5-9bc2-4827-9b52-4e5eb1143f60", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler that mixes DE-style donor recombination, PCA-guided elite moves, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(archive_max)\n        self.gscale = 0.5  # short-term adaptation scale (relative to range)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        lb = None\n        ub = None\n        # Common attribute names\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # if it's a pair or object with lb/ub\n            if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                lb = np.array(b[0], dtype=float)\n                ub = np.array(b[1], dtype=float)\n            else:\n                # try objects with .lb and .ub\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n        # try other common names\n        for name in (\"lb\", \"lower\", \"lower_bounds\", \"bounds_low\"):\n            if lb is None and hasattr(func, name):\n                lb = np.asarray(getattr(func, name), dtype=float)\n        for name in (\"ub\", \"upper\", \"upper_bounds\", \"bounds_high\"):\n            if ub is None and hasattr(func, name):\n                ub = np.asarray(getattr(func, name), dtype=float)\n\n        # final fallback to scalar -5 to 5\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.atleast_1d(np.array(lb, dtype=float))\n        ub = np.atleast_1d(np.array(ub, dtype=float))\n\n        # broadcast scalars or singletons to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n\n        # pad or truncate if necessary\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        if lb.size > self.dim:\n            lb = lb[:self.dim]\n        if ub.size > self.dim:\n            ub = ub[:self.dim]\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds, do a few iterations to settle\n        x = np.asarray(x, dtype=float)\n        for _ in range(8):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            x[low] = 2 * lb[low] - x[low]\n            x[high] = 2 * ub[high] - x[high]\n        # final clamp to handle any remaining numerical issues\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        dim = int(dim)\n        # create LHS: for each dim, create a random permutation of n strata\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.uniform(size=(n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            idx = self.rng.permutation(n)\n            lhs[:, j] = cut[idx] + u[:, j] * (1.0 / n)\n        lhs = np.clip(lhs, 0.0, 1.0)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        n = int(n)\n        if n == 1:\n            # return 1-D array\n            return self.rng.uniform(lb, ub)\n        else:\n            # return (n, dim)\n            return self.rng.uniform(0.0, 1.0, size=(n, self.dim)) * (ub - lb) + lb\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                # budget exhausted\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            # allow only numeric\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        center = 0.5 * (lb + ub)\n\n        # initialization: center + LHS + one uniform\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        lhs01 = self._lhs01(n0, self.dim)\n        init_points = lb + lhs01 * range_vec  # shape (n0, dim)\n        uni = self._uniform_array(lb, ub, n=1)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        f_center = safe_eval(center)\n        archive_X.append(center.copy())\n        archive_F.append(f_center)\n        if np.isfinite(f_center) and f_center < self.f_opt:\n            self.f_opt = float(f_center)\n            self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(init_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = init_points[i]\n            # skip duplicates near center\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform\n        if self.evals < self.budget:\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = float(f_uni)\n                self.x_opt = uni.copy()\n\n        # short-term adaptation and control variables\n        gscale = float(self.gscale)\n        success_window = deque(maxlen=30)\n        window_len = 30\n\n        stagn_count = 0\n        iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X_arr = np.array(archive_X) if archive_X else np.empty((0, self.dim))\n            F_arr = np.array(archive_F) if archive_F else np.empty((0,))\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n\n            # robust center (median of finite points) and per-dim scale (75-25 percentile)\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                median = np.median(Xf, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-12, q75 - q25)\n                mean_scale = float(np.mean(per_dim_scale) + 1e-12)\n            else:\n                Xf = np.empty((0, self.dim))\n                Ff = np.array([])\n                median = center.copy()\n                per_dim_scale = np.maximum(1e-8, 0.1 * range_vec)\n                mean_scale = float(np.mean(per_dim_scale))\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_idx.size > 0:\n                stdF = np.std(Ff) + 1e-9\n                # normalized negative fitness as logits (better = larger)\n                logits = -(Ff - np.min(Ff)) / stdF\n                logits = logits - np.max(logits)\n                exp_logits = np.exp(logits)\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = self.rng.choice(len(Ff), p=probs)\n                base = Xf[pick_idx].copy()\n            else:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if finite_idx.size >= 3:\n                # pick a small pool biased to best items\n                sorted_idx = np.argsort(Ff)\n                pick_pool = sorted_idx[:max(3, min(30, len(sorted_idx)))]\n                kdon = min(4, max(2, max(2, len(pick_pool) // 5)))\n                kdon = min(kdon, len(pick_pool))\n                chosen = self.rng.choice(pick_pool, size=kdon, replace=False)\n                donors = [Xf[int(i)].copy() for i in chosen]\n            elif len(archive_X) >= 2:\n                # fallback: pick two random distinct archived points\n                idxs = self.rng.choice(len(archive_X), size=min(2, len(archive_X)), replace=False)\n                donors = [archive_X[int(i)].copy() for i in idxs]\n\n            # decide move type\n            p_global = 0.16  # probability of global jump\n            move_rand = self.rng.random()\n            c = self.rng.standard_cauchy(self.dim)  # tempered cauchy vector (unscaled)\n\n            if move_rand < p_global:\n                # GLOBAL move: DE-like + tempered Cauchy jumps anchored on a random archived elite or uniform\n                if finite_idx.size > 0 and self.rng.random() < 0.9:\n                    # pick elite among top half biased to best\n                    n_elite = max(1, Xf.shape[0] // 2)\n                    elite_idx = np.argsort(Ff)[:n_elite]\n                    anchor = Xf[self.rng.choice(elite_idx)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n\n                cand = anchor.copy()\n                # if we have donors, do DE-like difference\n                if len(donors) >= 2:\n                    dmove = donors[0] - donors[1]\n                    F_scale = 0.8 * (0.3 + 0.7 * self.rng.random())\n                    cand = cand + F_scale * dmove\n                # tempered Cauchy perturbation\n                scale = gscale * (0.6 + 0.9 * self.rng.random()) * mean_scale\n                cand = cand + scale * c\n                # occasional uniform injection of one coordinate\n                if self.rng.random() < 0.12:\n                    d = self.rng.integers(0, self.dim)\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                alpha = 0.0\n                if self.rng.random() < 0.35:\n                    alpha = 0.1 * (self.rng.random() + 0.5)  # small attraction factor\n                base_attracted = base + alpha * (median - base)\n\n                # anisotropic gaussian step\n                sigma = np.clip(gscale * per_dim_scale, 1e-12, 0.6 * range_vec)\n                cand = base_attracted + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 6 and self.rng.random() < 0.20:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elites_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elites_idx]\n                    # principal direction\n                    E = elites - np.mean(elites, axis=0, keepdims=True)\n                    try:\n                        u, svals, vt = np.linalg.svd(E, full_matrices=False)\n                        pc_dir = vt[0]\n                        pc_norm = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                        pc_scale = gscale * mean_scale * (0.4 + self.rng.random() * 0.8)\n                        cand2 = elites.mean(axis=0) + pc_norm * pc_scale\n                        if self.rng.random() < 0.5:\n                            # use PCA candidate with small jitter\n                            cand = cand2 + self.rng.normal(0, 0.1 * pc_scale, size=self.dim)\n                    except Exception:\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.3:\n                    F_scale = 0.4 + 0.8 * self.rng.random()\n                    cand = cand + F_scale * (donors[0] - donors[1]) * (0.3 + 0.7 * self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.06:\n                    scale = gscale * (0.8 + 1.2 * self.rng.random()) * mean_scale\n                    cand = cand + scale * c\n\n            # safety: if any non-finite values, replace them with uniform samples\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved_global = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            finite_vals = F_arr[np.isfinite(F_arr)] if F_arr.size > 0 else np.array([])\n            placed_top = False\n            if finite_vals.size > 0 and np.isfinite(f_cand):\n                sorted_vals = np.sort(np.concatenate([finite_vals, [f_cand]]))\n                rank = np.searchsorted(sorted_vals, f_cand, side='left') + 1\n                pct = rank / float(sorted_vals.size)\n                if pct <= 0.25:\n                    placed_top = True\n            elif np.isfinite(f_cand):\n                # if no earlier finite, any finite is considered success\n                placed_top = True\n\n            success = improved_global or placed_top\n            success_window.append(1 if success else 0)\n\n            # compute success rate\n            srate = (sum(success_window) / len(success_window)) if len(success_window) > 0 else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                # if many successes, modestly increase exploratory scale (move farther)\n                if srate > 0.25:\n                    gscale = min(10.0, gscale * (1.06 + 0.12 * self.rng.random()))\n                # if stagnating, increase scale to escape local minima\n                if srate < 0.04:\n                    gscale = min(10.0, gscale * (1.04 + 0.05 * self.rng.random()))\n                # otherwise slowly decay to refine\n                if 0.04 <= srate <= 0.25:\n                    gscale = max(1e-6, gscale * (0.98 + 0.03 * self.rng.random()))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max * 2:\n                F_arr = np.array(archive_F)\n                all_idx = np.arange(len(F_arr))\n                fin_idx = np.where(np.isfinite(F_arr))[0]\n                keep_idx = []\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort(F_arr[fin_idx])]\n                    take = int(self.archive_max * 0.8)\n                    take = max(1, min(take, sorted_fin.size))\n                    keep_idx.extend(sorted_fin[:take].tolist())\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx.tolist()) - set(keep_idx))\n                if len(remaining) > 0:\n                    add = self.rng.choice(remaining, size=min(self.archive_max - len(keep_idx), len(remaining)), replace=False)\n                    keep_idx.extend([int(i) for i in add])\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(40, 6 * self.dim) or (iters > 100 and srate < 0.02):\n                # perform micro-restarts: inject a few diversified points\n                n_inject = min(6, max(2, 2 + self.dim // 5))\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.85:\n                        # around best (local diversified)\n                        scale = gscale * (1.2 + self.rng.random() * 2.0)\n                        candr = self.x_opt + self.rng.normal(0, scale, size=self.dim) * (0.5 * range_vec)\n                        candr = self._reflect_bounds(candr, lb, ub)\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.x_opt = candr.copy()\n                        self.f_opt = float(fr)\n                # encourage exploration after restart\n                success_window.clear()\n                stagn_count = 0\n                # slightly bump gscale to explore\n                gscale = min(10.0, gscale * (1.1 + 0.2 * self.rng.random()))\n\n        # final safety: pick best finite point from archive if x_opt still None or to ensure best\n        F_arr = np.array(archive_F)\n        if F_arr.size > 0:\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                if self.x_opt is None or F_arr[best_i] < self.f_opt:\n                    self.x_opt = np.array(archive_X[int(best_i)]).copy()\n                    self.f_opt = float(F_arr[best_i])\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            # fallback to center\n            self.x_opt = center.copy()\n            self.f_opt = safe_eval(self.x_opt)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 38, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "error": "In the code, line 38, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4136b21c-6745-4f76-9bd0-4ad27fe6c73a", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(6, int(init_samples))\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n        # runtime trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        # Common attribute names: bounds, lb/ub, lower/upper\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            try:\n                # b might be pair of arrays\n                if isinstance(b, (tuple, list)) and len(b) >= 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n        # check common names\n        for name in (\"lb\", \"lower\", \"min_bounds\", \"lbound\"):\n            if lb is None and hasattr(func, name):\n                try:\n                    lb = np.asarray(getattr(func, name), dtype=float)\n                except Exception:\n                    lb = None\n        for name in (\"ub\", \"upper\", \"max_bounds\", \"ubound\"):\n            if ub is None and hasattr(func, name):\n                try:\n                    ub = np.asarray(getattr(func, name), dtype=float)\n                except Exception:\n                    ub = None\n\n        # fallback to scalar [-5,5]\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.atleast_1d(np.array(lb, dtype=float))\n        ub = np.atleast_1d(np.array(ub, dtype=float))\n\n        # broadcast scalars or singletons to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n\n        # pad or truncate if necessary\n        if lb.size < self.dim:\n            lb = np.concatenate([lb, np.full(self.dim - lb.size, lb[-1], dtype=float)])\n        if ub.size < self.dim:\n            ub = np.concatenate([ub, np.full(self.dim - ub.size, ub[-1], dtype=float)])\n        if lb.size > self.dim:\n            lb = lb[: self.dim]\n        if ub.size > self.dim:\n            ub = ub[: self.dim]\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds, do a few iterations to settle\n        x = np.asarray(x, dtype=float)\n        for _ in range(8):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            # reflect: mirror distance outside bounds\n            x[low] = lb[low] + (lb[low] - x[low])\n            x[high] = ub[high] - (x[high] - ub[high])\n        # final clamp to handle any remaining numerical issues\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        dim = int(dim)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.uniform(size=(n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            idx = self.rng.permutation(n)\n            lhs[:, j] = cut[idx] + u[:, j] * (1.0 / n)\n        lhs = np.clip(lhs, 0.0, 1.0)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        n = int(n)\n        if n == 1:\n            return lb + self.rng.uniform(size=self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.uniform(size=(n, self.dim)) * (ub - lb)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                x = np.asarray(x, dtype=float)\n            except Exception:\n                x = np.asarray(x, dtype=float)\n            try:\n                f = func(x)\n                f = float(f)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n\n        # initialization: center + LHS + one uniform\n        center = 0.5 * (lb + ub)\n        # produce LHS in bounds\n        n_lhs = max(0, self.init_samples - 2)\n        lhs01 = self._lhs01(n_lhs, self.dim) if n_lhs > 0 else np.zeros((0, self.dim))\n        lhs_points = lb + lhs01 * (ub - lb)\n\n        uni = self._uniform_array(lb, ub, n=1)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(lhs_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = lhs_points[i].copy()\n            # skip duplicates near center\n            if np.linalg.norm(x - center) < 1e-12:\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform\n        if self.evals < self.budget:\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = f_uni\n                self.x_opt = uni.copy()\n\n        # short-term adaptation and control variables\n        gscale = 0.5  # global multiplier for step sizes (adapts)\n        success_window = deque(maxlen=30)\n\n        stagn_count = 0\n        iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            # assemble arrays\n            X_arr = np.array(archive_X)\n            F_arr = np.array(archive_F)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n\n            # robust center (median of finite points) and per-dim scale (75-25 percentile)\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                per_dim_scale = np.maximum(1e-12, q75 - q25)\n                median = np.median(Xf, axis=0)\n                mean_scale = float(np.mean(per_dim_scale))\n            else:\n                Xf = np.empty((0, self.dim))\n                Ff = np.array([])\n                per_dim_scale = np.maximum(1e-12, 0.25 * range_vec)\n                median = center.copy()\n                mean_scale = float(np.mean(per_dim_scale))\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_idx.size > 0 and Ff.size > 0:\n                logits = -Ff.astype(float)\n                logits = logits - np.max(logits)\n                exp_logits = np.exp(logits)\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = self.rng.choice(len(Ff), p=probs)\n                base = Xf[pick_idx].copy()\n            else:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if Xf.shape[0] >= 3:\n                # pick a small pool biased to best items\n                best_k = max(3, int(0.25 * Xf.shape[0]))\n                best_idx = np.argsort(Ff)[:best_k]\n                kdon = min(4, max(2, best_k // 2))\n                chosen = self.rng.choice(best_idx, size=kdon, replace=False)\n                donors = [Xf[int(i)].copy() for i in chosen]\n\n            # decide move type\n            p_global = 0.16  # probability of global jump\n            move_rand = self.rng.random()\n\n            if move_rand < p_global:\n                # GLOBAL move: DE-like + tempered Cauchy jumps anchored on an archived elite or uniform\n                if finite_idx.size > 0 and self.rng.random() < 0.9:\n                    # pick anchor among top half biased to best\n                    half = max(1, Xf.shape[0] // 2)\n                    top_idx = np.argsort(Ff)[:half]\n                    anchor = Xf[self.rng.choice(top_idx)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n\n                cand = anchor.copy()\n                # if we have donors, do DE-like difference\n                if len(donors) >= 2:\n                    a, b = donors[0], donors[1]\n                    dmove = a - b\n                    F_scale = 0.8 * (0.3 + 0.7 * self.rng.random())\n                    cand = cand + F_scale * dmove * (1.0 + 0.5 * self.rng.standard_normal(self.dim))\n\n                # tempered Cauchy perturbation for escapes (heavy tails but tempered by gscale)\n                cauchy = self.rng.standard_cauchy(size=self.dim)\n                # temper and scale by gscale, per-dim scale and global range\n                cauchy_scale = gscale * (0.8 + 1.2 * self.rng.random()) * (per_dim_scale + 1e-12)\n                cand = cand + np.sign(cauchy) * np.minimum(np.abs(cauchy), 10.0) * cauchy_scale\n\n                # occasional uniform injection of one coordinate\n                if self.rng.random() < 0.05:\n                    j = self.rng.integers(0, self.dim)\n                    cand[j] = lb[j] + self.rng.random() * range_vec[j]\n\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                alpha = 0.0\n                if self.rng.random() < 0.35:\n                    alpha = 0.2 * self.rng.random()  # attraction to median\n                base_attracted = (1.0 - alpha) * base + alpha * median\n\n                sigma = np.clip(gscale * per_dim_scale, 1e-12, 0.6 * range_vec)\n                cand = base_attracted + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 6 and self.rng.random() < 0.20:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elites_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elites_idx]\n                    # compute principal direction\n                    try:\n                        C = elites - np.mean(elites, axis=0, keepdims=True)\n                        u, s, vt = np.linalg.svd(C, full_matrices=False)\n                        pc_dir = vt[0]\n                        pc_scale = s[0] / (num_elite + 1e-12)\n                        cand2 = np.mean(elites, axis=0) + 0.5 * pc_scale * pc_dir\n                        cand = cand2 + self.rng.normal(0, 0.1 * pc_scale, size=self.dim)\n                    except Exception:\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.3:\n                    a, b = donors[0], donors[1]\n                    dmove = a - b\n                    F_scale = 0.5 * (0.3 + 0.7 * self.rng.random())\n                    cand = cand + F_scale * dmove * (0.5 + 0.5 * self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape in local\n                if self.rng.random() < 0.05:\n                    cauchy = self.rng.standard_cauchy(size=self.dim)\n                    c_scale = gscale * (0.8 + 1.2 * self.rng.random()) * mean_scale\n                    cand = cand + np.sign(cauchy) * np.minimum(np.abs(cauchy), 8.0) * c_scale\n\n            # safety: if any non-finite values, replace them with uniform samples\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved_global = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            placed_top = False\n            finite_mask = np.isfinite(np.array(archive_F))\n            finite_vals = np.array(archive_F)[finite_mask]\n            if np.isfinite(f_cand) and finite_vals.size > 0:\n                sorted_vals = np.sort(finite_vals)\n                # find rank (1-based)\n                rank = np.searchsorted(sorted_vals, f_cand, side='left') + 1\n                pct = rank / float(sorted_vals.size)\n                if pct <= 0.25:\n                    placed_top = True\n            elif np.isfinite(f_cand) and finite_vals.size == 0:\n                placed_top = True\n\n            success = improved_global or placed_top\n            success_window.append(1.0 if success else 0.0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 8 == 0 and len(success_window) >= 6:\n                srate = float(np.mean(success_window))\n                # if many successes, modestly increase exploratory scale\n                if srate > 0.3:\n                    gscale = min(10.0, gscale * (1.0 + 0.08 * (srate - 0.3) * 5.0))\n                # if stagnating, increase scale to escape local minima\n                elif srate < 0.05:\n                    gscale = min(20.0, gscale * (1.3 + 0.4 * self.rng.random()))\n                # otherwise slowly decay to refine\n                else:\n                    gscale = max(1e-6, gscale * (0.95 + 0.05 * self.rng.random()))\n\n            # prune archive to maintain memory budget\n            if len(archive_F) > self.archive_max:\n                F_arr = np.array(archive_F)\n                all_idx = np.arange(len(F_arr))\n                fin_idx = np.where(np.isfinite(F_arr))[0]\n                keep_idx = []\n                if fin_idx.size > 0:\n                    take = int(self.archive_max * 0.8)\n                    best_take = min(take, fin_idx.size)\n                    best_idx = fin_idx[np.argsort(F_arr[fin_idx])[:best_take]]\n                    keep_idx.extend(best_idx.tolist())\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx) - set(keep_idx))\n                self.rng.shuffle(remaining)\n                to_take = max(0, self.archive_max - len(keep_idx))\n                keep_idx.extend(remaining[:to_take])\n                keep_idx = sorted(keep_idx)\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(40, 6 * self.dim) or (iters > 100 and (np.mean(success_window) if success_window else 1.0) < 0.02):\n                n_inject = min(6, max(2, 2 + self.dim // 5))\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    # around best (local diversified) if available\n                    if self.x_opt is not None and np.isfinite(self.f_opt):\n                        scale = gscale * (0.8 + 1.2 * self.rng.random()) * mean_scale\n                        candr = self.x_opt + self.rng.normal(0, scale, size=self.dim) * (0.5 * range_vec)\n                        candr = self._reflect_bounds(candr, lb, ub)\n                    else:\n                        candr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.f_opt = float(fr)\n                        self.x_opt = candr.copy()\n                # encourage exploration after restart\n                gscale = min(10.0, gscale * (1.1 + 0.2 * self.rng.random()))\n                stagn_count = 0\n                success_window.clear()\n\n        # final safety: pick best finite point from archive if x_opt still None or to ensure best\n        F_arr = np.array(archive_F)\n        finite_idxs = np.where(np.isfinite(F_arr))[0]\n        if finite_idxs.size > 0:\n            best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n            if (self.x_opt is None) or (F_arr[best_i] < self.f_opt):\n                self.f_opt = float(F_arr[best_i])\n                self.x_opt = np.array(archive_X[best_i], dtype=float)\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            self.x_opt = center.copy()\n            self.f_opt = safe_eval(self.x_opt)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 186, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=30)", "error": "In the code, line 186, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=30)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "34ee3723-92f9-4f6b-a581-675adb497335", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n        # short-term adaptation scale (relative to range)\n        self.gscale = 0.5\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func; fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # common patterns\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                try:\n                    lb = np.array(b[0], dtype=float)\n                    ub = np.array(b[1], dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n            else:\n                # maybe object with .lb and .ub\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n\n        # other common attributes\n        if lb is None or ub is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # final fallback to scalar -5..5\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # broadcast scalar to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # pad or truncate to match dim\n        if lb.size < self.dim:\n            lb = np.concatenate([lb, np.full(self.dim - lb.size, lb[-1])])\n        if ub.size < self.dim:\n            ub = np.concatenate([ub, np.full(self.dim - ub.size, ub[-1])])\n        if lb.size > self.dim:\n            lb = lb[: self.dim]\n        if ub.size > self.dim:\n            ub = ub[: self.dim]\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # reflect coordinates that go beyond bounds, do a few iterations to settle\n        x = np.asarray(x, dtype=float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp to handle any remaining numerical issues\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(max(1, n))\n        dim = int(dim)\n        lhs = np.empty((n, dim), dtype=float)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            u = self.rng.random(n)\n            # coordinate j: one sample inside each stratum\n            lhs[:, j] = cut[perm] + u * (1.0 / n)\n        lhs = np.clip(lhs, 0.0, 1.0)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, self.dim) if n > 1 else (self.dim,)\n        if n > 1:\n            return self.rng.random((n, self.dim)) * (ub - lb) + lb\n        else:\n            return self.rng.random(self.dim) * (ub - lb) + lb\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n        # safe evaluator that respects budget and catches exceptions\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        lb, ub = self._get_bounds(func)\n        # ensure bounds reflect problem statement if they are weird\n        lb = np.full(self.dim, -5.0) if lb is None else lb\n        ub = np.full(self.dim, 5.0) if ub is None else ub\n        range_vec = ub - lb\n        center = 0.5 * (lb + ub)\n\n        # archive: lists of arrays\n        archive_X = []\n        archive_F = []\n\n        # initialization: center + LHS + one uniform\n        n_lhs = max(0, self.init_samples - 2)\n        lhs01 = self._lhs01(n_lhs, self.dim) if n_lhs > 0 else np.empty((0, self.dim))\n        lhs_points = lhs01 * range_vec + lb  # shape (n_lhs, dim)\n        uni = self._uniform_array(lb, ub, n=1)\n        init_points = [center.copy()] + [lhs_points[i].copy() for i in range(lhs_points.shape[0])] + [uni.copy()]\n\n        # evaluate initial points (stop if budget exhausted)\n        for p in init_points:\n            if self.evals >= self.budget:\n                break\n            # skip duplicates near center (within tiny tol)\n            if self.x_opt is not None and np.linalg.norm(p - self.x_opt) < 1e-12:\n                continue\n            f = safe_eval(p)\n            archive_X.append(np.asarray(p, dtype=float).copy())\n            archive_F.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.asarray(p, dtype=float).copy()\n\n        # ensure at least one uniform if nothing evaluated\n        if len(archive_X) == 0 and self.evals < self.budget:\n            p = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(p)\n            archive_X.append(np.asarray(p, dtype=float).copy())\n            archive_F.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.asarray(p, dtype=float).copy()\n\n        # short-term adaptation and control variables\n        stagn_count = 0\n        iters = 0\n        # keep recent success history (for adapting gscale)\n        recent_success = []\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X_arr = np.array(archive_X) if len(archive_X) > 0 else np.empty((0, self.dim))\n            F_arr = np.array(archive_F) if len(archive_F) > 0 else np.array([])\n\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0] if F_arr.size > 0 else np.array([], dtype=int)\n\n            # robust center (median of finite points) and per-dim scale (75-25 percentile)\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx]\n                Ff = F_arr[finite_idx]\n                median = np.median(Xf, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-12, 0.5 * (q75 - q25))\n                mean_scale = float(np.mean(per_dim_scale))\n            else:\n                Xf = np.empty((0, self.dim))\n                Ff = np.array([])\n                median = center.copy()\n                per_dim_scale = np.maximum(1e-12, 0.1 * range_vec)\n                mean_scale = float(np.mean(per_dim_scale))\n\n            # choose base biased towards better archive items using softmax on negative F\n            base = median.copy()\n            if finite_idx.size > 0:\n                stdF = np.std(Ff) + 1e-12\n                logits = -(Ff - np.min(Ff)) / stdF\n                # softmax\n                expw = np.exp(logits - np.max(logits))\n                probs = expw / np.sum(expw)\n                pick = self.rng.choice(len(Ff), p=probs)\n                base = Xf[pick].copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if Xf.shape[0] >= 3:\n                # pick two distinct donors biased to best\n                ranks = np.argsort(Ff)\n                topk = max(2, int(0.25 * Xf.shape[0]))\n                pool_idx = ranks[:topk]\n                if pool_idx.size >= 2:\n                    a, b = self.rng.choice(pool_idx, size=2, replace=False)\n                    donors = [Xf[a].copy(), Xf[b].copy()]\n            elif Xf.shape[0] >= 2:\n                a, b = self.rng.choice(Xf.shape[0], size=2, replace=False)\n                donors = [Xf[a].copy(), Xf[b].copy()]\n\n            # decide move type probabilities adaptively\n            p_global = 0.30\n            p_local = 0.50\n            p_pca = 0.15\n            p_cauchy = 0.05\n            r = self.rng.random()\n\n            cand = None\n\n            # GLOBAL move: DE-like + tempered Cauchy jumps anchored on a random archived elite or uniform\n            if r < p_global:\n                if len(donors) >= 2:\n                    dmove = donors[0] - donors[1]\n                    F_scale = 0.6 * (0.4 + 0.6 * self.rng.random()) * self.gscale\n                    cand = base + dmove * F_scale\n                else:\n                    cand = base.copy()\n                # tempered Cauchy perturbation (light tail but tempered by per-dim scale)\n                t_scale = mean_scale * (0.8 + 0.8 * self.rng.random())\n                # draw standard Cauchy but temper with uniform mixing to avoid huge jumps always\n                if self.rng.random() < 0.35:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    # temper by dividing larger quantiles\n                    cauch = np.tanh(cauch / 3.0) * 3.0\n                    cand = cand + cauch * t_scale * 0.7\n                # occasional uniform injection of one coordinate\n                if self.rng.random() < 0.12:\n                    d = self.rng.integers(0, self.dim)\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n\n            # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n            elif r < p_global + p_local:\n                # anisotropic gaussian step\n                anis = self.rng.random(self.dim) * 0.8 + 0.2  # (0.2..1.0) per dim\n                sigma = self.gscale * per_dim_scale * anis\n                cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                # attract occasionally toward median to stabilize\n                if self.rng.random() < 0.35:\n                    alpha = 0.1 + 0.6 * self.rng.random()\n                    cand = cand * (1.0 - alpha) + median * alpha\n                # small chance to mix in DE donor inside local move\n                if len(donors) >= 2 and self.rng.random() < 0.18:\n                    diff = donors[0] - donors[1]\n                    cand = cand + 0.5 * self.gscale * diff * (self.rng.random(self.dim) * 0.5 + 0.5)\n\n            # PCA-guided elite perturbation occasionally\n            elif r < p_global + p_local + p_pca:\n                if Xf.shape[0] >= 6:\n                    num_elite = max(3, int(0.20 * Xf.shape[0]))\n                    elite_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elite_idx]\n                    E = elites - np.mean(elites, axis=0, keepdims=True)\n                    # compute first principal component via SVD\n                    try:\n                        u, s, vt = np.linalg.svd(E, full_matrices=False)\n                        pc = vt[0]\n                        pc_scale = self.gscale * mean_scale * (0.4 + self.rng.random() * 1.2)\n                        # sample along principal direction and add small jitter\n                        cand = np.mean(elites, axis=0) + pc * (self.rng.normal() * pc_scale)\n                        cand = cand + self.rng.normal(0, 0.08 * mean_scale, size=self.dim)\n                    except Exception:\n                        # fallback to local gaussian\n                        sigma = self.gscale * per_dim_scale\n                        cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                else:\n                    # fallback if not enough points\n                    sigma = self.gscale * per_dim_scale\n                    cand = base + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n\n            # tempered Cauchy jump occasionally for escape\n            else:\n                # pick an anchor: best so far or random elite\n                if self.x_opt is not None and self.rng.random() < 0.8:\n                    anchor = self.x_opt.copy()\n                elif Xf.shape[0] > 0:\n                    idx = self.rng.integers(0, Xf.shape[0])\n                    anchor = Xf[idx].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n                t_scale = (0.8 + 1.8 * self.rng.random()) * mean_scale * max(1.0, self.gscale)\n                cauch = self.rng.standard_cauchy(self.dim)\n                cauch = np.tanh(cauch / 2.0) * 4.0\n                cand = anchor + cauch * t_scale\n                # also replace some coordinates uniformly\n                for _ in range(max(1, int(0.05 * self.dim))):\n                    if self.rng.random() < 0.15:\n                        d = self.rng.integers(0, self.dim)\n                        cand[d] = self.rng.uniform(lb[d], ub[d])\n\n            # safety: if any non-finite values, replace them with uniform samples\n            if not np.all(np.isfinite(cand)):\n                inf_mask = ~np.isfinite(cand)\n                cand[inf_mask] = self.rng.uniform(lb[inf_mask], ub[inf_mask])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(float(f_cand))\n\n            improved_global = False\n            placed_top = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define placed_top: placed among top 25% of finite vals\n            finite_vals = F_arr[np.isfinite(F_arr)] if F_arr.size > 0 else np.array([])\n            if finite_vals.size > 0:\n                sorted_vals = np.sort(np.concatenate([finite_vals, [f_cand]]))\n                rank = np.searchsorted(sorted_vals, f_cand, side='left') + 1\n                pct = rank / sorted_vals.size\n                placed_top = (pct <= 0.25)\n            else:\n                placed_top = np.isfinite(f_cand)\n\n            success = improved_global or placed_top\n            recent_success.append(1 if success else 0)\n            if len(recent_success) > 100:\n                recent_success.pop(0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                srate = float(np.mean(recent_success)) if recent_success else 0.0\n                # if many successes, modestly increase exploratory scale (move farther)\n                if srate > 0.25:\n                    self.gscale = min(10.0, self.gscale * (1.03 + 0.02 * self.rng.random()))\n                # if stagnating, increase scale to escape local minima\n                elif stagn_count > 20 and srate < 0.04:\n                    self.gscale = min(20.0, self.gscale * (1.2 + 0.25 * self.rng.random()))\n                # otherwise slowly decay to refine\n                elif 0.04 <= srate <= 0.25:\n                    self.gscale = max(1e-6, self.gscale * (0.95 - 0.02 * self.rng.random()))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max:\n                # keep top 30% finite and some random rest (including some infinities)\n                X_arr = np.array(archive_X)\n                F_arr = np.array(archive_F)\n                fin_idx = np.where(np.isfinite(F_arr))[0]\n                best_keep = []\n                if fin_idx.size > 0:\n                    sorted_fin_idx = fin_idx[np.argsort(F_arr[fin_idx])]\n                    n_best = max(1, int(0.30 * self.archive_max))\n                    keep_best = list(sorted_fin_idx[:n_best])\n                    best_keep.extend(keep_best)\n                # add some random from remaining\n                all_idx = np.arange(len(archive_X))\n                remaining = list(set(all_idx.tolist()) - set(best_keep))\n                n_needed = self.archive_max - len(best_keep)\n                n_needed = max(0, n_needed)\n                if len(remaining) > 0 and n_needed > 0:\n                    add = list(self.rng.choice(remaining, size=min(n_needed, len(remaining)), replace=False))\n                else:\n                    add = []\n                keep_idx = list(best_keep) + add\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(30, 10 + self.dim) and self.evals + 3 < self.budget:\n                # perform micro-restarts: inject a few diversified points\n                n_inject = min(6, max(2, 2 + self.dim // 5))\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.85:\n                        # around best (local diversified)\n                        scale = self.gscale * (1.2 + self.rng.random() * 2.0)\n                        candr = self.x_opt + self.rng.normal(0, 1.0, size=self.dim) * (scale * 0.5 * range_vec)\n                        candr = self._reflect_bounds(candr, lb, ub)\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(candr)\n                    archive_X.append(np.asarray(candr).copy())\n                    archive_F.append(float(fr))\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.x_opt = np.asarray(candr).copy()\n                        self.f_opt = float(fr)\n                # encourage exploration after restart: bump gscale\n                self.gscale = min(10.0, self.gscale * (1.1 + 0.4 * self.rng.random()))\n                stagn_count = 0\n                recent_success = []\n\n        # final safety: pick best finite point from archive if x_opt still None or to ensure best\n        if len(archive_X) > 0:\n            F_arr = np.array(archive_F)\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                if self.x_opt is None or archive_F[best_i] < self.f_opt:\n                    self.x_opt = np.array(archive_X[int(best_i)]).copy()\n                    self.f_opt = float(archive_F[int(best_i)])\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            self.x_opt = center.copy()\n            self.f_opt = safe_eval(self.x_opt)\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 41, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "error": "In the code, line 41, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1f45a4ad-f2e3-4f79-9bd7-6a9db2e35be8", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n        self.archive_max = int(archive_max)\n        # default initial samples: a few times dimension but at least 12\n        self.init_samples = int(init_samples) if init_samples is not None else max(12, 3 * self.dim)\n        # adaptation state\n        self.gscale = 0.8  # global differential scale\n        self.local_scale = 0.05  # local anisotropic scale base\n        self.evals = 0\n        # best found\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # try a few common attribute names; fallback [-5,5]\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            try:\n                # assume pair-like (lb, ub)\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n        # other common names\n        for name in (\"lb\", \"lower\", \"lower_bounds\", \"bounds_low\"):\n            if lb is None and hasattr(func, name):\n                try:\n                    lb = np.asarray(getattr(func, name), dtype=float)\n                except Exception:\n                    lb = None\n        for name in (\"ub\", \"upper\", \"upper_bounds\", \"bounds_up\"):\n            if ub is None and hasattr(func, name):\n                try:\n                    ub = np.asarray(getattr(func, name), dtype=float)\n                except Exception:\n                    ub = None\n        # final fallback to scalar bounds [-5,5]\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n        # pad/truncate to dim\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        lb = lb[:self.dim].astype(float)\n        ub = ub[:self.dim].astype(float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that go beyond bounds until they fall back into range or reach max_iter\n        x = x.copy().astype(float)\n        for _ in range(max_iter):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            if low.any():\n                x[low] = 2 * lb[low] - x[low]\n            if high.any():\n                x[high] = 2 * ub[high] - x[high]\n            # slight clamp to avoid stuck at boundary due to numerical noise\n            x = np.minimum(np.maximum(x, lb - 1e6), ub + 1e6)\n        # final hard clamp\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def _lhs01(self, n, dim):\n        # simple Latin Hypercube Sampling in [0,1]^dim\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        pts = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            perm = rng.permutation(n)\n            # sample uniformly inside each stratum\n            r = rng.rand(n)\n            pts[:, j] = cut[perm] + r * (1.0 / n)\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            r = self.rng.rand(n, self.dim)\n            return lb + r * (ub - lb)\n\n    def _tempered_cauchy(self, scale):\n        # sample tempered Cauchy: heavy tail but regularized\n        u = self.rng.rand(self.dim) - 0.5\n        c = np.tan(np.pi * u)  # standard Cauchy-like (could be large)\n        # temper: shrink very large values smoothly\n        c = c / (1.0 + np.abs(c) / 8.0)\n        return c * float(scale)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # Ensure we do not exceed budget and handle exceptions gracefully\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        budget = int(self.budget)\n        self.evals = 0\n\n        archive_X = []\n        archive_F = []\n\n        def safe_eval(x):\n            # x must be 1-D array of length dim\n            if self.evals >= budget:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            if x.ndim != 1 or x.size != dim:\n                x = x.ravel()[:dim]\n            try:\n                f = float(func(x))\n            except Exception:\n                # on exception, treat as infinity\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # Initialization: center + LHS + one uniform\n        center = 0.5 * (lb + ub)\n        # but diversify center a bit by small jitter\n        center = center + (self.rng.rand(dim) - 0.5) * 0.01 * (ub - lb)\n        # evaluate center first\n        try:\n            f_center = safe_eval(center)\n        except RuntimeError:\n            # zero budget\n            self.x_opt = center.copy()\n            self.f_opt = np.inf\n            return float(self.f_opt), np.array(self.x_opt, dtype=float)\n        archive_X.append(center.copy())\n        archive_F.append(f_center)\n        if np.isfinite(f_center) and f_center < self.f_opt:\n            self.f_opt = f_center\n            self.x_opt = center.copy()\n\n        # LHS samples\n        n_lhs = max(1, min(self.init_samples, budget - self.evals))\n        lhs01 = self._lhs01(n_lhs, dim)\n        lhs = lb + lhs01 * (ub - lb)\n        # ensure one uniform sample too\n        uni = self._uniform_array(lb, ub, n=1)\n\n        for i in range(n_lhs):\n            if self.evals >= budget:\n                break\n            x = lhs[i]\n            # skip duplicates near center\n            if np.linalg.norm(x - center) < 1e-12:\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        if self.evals < budget:\n            try:\n                f_uni = safe_eval(uni)\n                archive_X.append(uni.copy())\n                archive_F.append(f_uni)\n                if np.isfinite(f_uni) and f_uni < self.f_opt:\n                    self.f_opt = f_uni\n                    self.x_opt = uni.copy()\n            except RuntimeError:\n                pass\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=30)\n        stagn_count = 0\n        iters = 0\n        max_archive = max(3, self.archive_max)\n\n        # main loop\n        while self.evals < budget:\n            iters += 1\n            # build numpy arrays of archived data\n            X_arr = np.array(archive_X, dtype=float) if archive_X else np.empty((0, dim))\n            F_arr = np.array(archive_F, dtype=float) if archive_F else np.empty((0,))\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.nonzero(finite_mask)[0]\n            # robust center and per-dim scales\n            if finite_idx.size >= 3:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                median = np.median(Xf, axis=0)\n                lo = np.percentile(Xf, 25, axis=0)\n                hi = np.percentile(Xf, 75, axis=0)\n                pscale = 0.5 * (hi - lo)  # interquartile half-range\n                # avoid zeros\n                pscale = np.where(pscale <= 0, 0.1 * (ub - lb) + 1e-9, pscale)\n            else:\n                Xf = np.empty((0, dim))\n                Ff = np.array([], dtype=float)\n                median = center.copy()\n                pscale = 0.1 * (ub - lb) + 1e-9\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_idx.size >= 2:\n                Ff_norm = Ff - np.min(Ff)\n                stdF = np.std(Ff_norm) + 1e-12\n                logits = -(Ff_norm) / stdF\n                # stabilize logits\n                logits = logits - np.max(logits)\n                probs = np.exp(logits)\n                probs = probs / probs.sum()\n                pick_idx = self.rng.choice(finite_idx, p=probs)\n                base = X_arr[pick_idx].copy()\n            else:\n                base = self._uniform_array(lb, ub, n=1)\n\n            # donor pool for DE-like moves: pick two distinct archived points\n            donor1 = None\n            donor2 = None\n            if X_arr.shape[0] >= 3:\n                # pick two distinct indices (could be infinite values too)\n                ids = self.rng.choice(min(X_arr.shape[0], max_archive), size=2, replace=False)\n                donor1 = X_arr[ids[0]].copy()\n                donor2 = X_arr[ids[1]].copy()\n\n            # decide move type\n            # p_global: chance to do DE-like/global jump\n            p_global = 0.35\n            move_rand = self.rng.rand()\n\n            if move_rand < p_global:\n                # GLOBAL move: DE-like + tempered Cauchy jumps anchored on a random archived elite or uniform\n                if donor1 is not None and donor2 is not None:\n                    F_scale = self.gscale * (0.5 + 0.5 * self.rng.rand())\n                    cand = base + F_scale * (donor1 - donor2)\n                else:\n                    cand = base.copy()\n                # tempered Cauchy global perturbation\n                cauch = self._tempered_cauchy(scale=0.5 * (ub - lb))\n                # apply cauchy per-dimension (if scalar multiply, shape must match)\n                if cauch.shape == (dim,):\n                    cand = cand + cauch\n                else:\n                    cand = cand + cauch * (ub - lb)\n                # occasional uniform injection of some coords\n                if self.rng.rand() < 0.12:\n                    mask = self.rng.rand(dim) < 0.2\n                    cand[mask] = lb[mask] + self.rng.rand(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n                # slight pull toward median to stabilize global moves occasionally\n                if self.rng.rand() < 0.2:\n                    alpha = 0.15 * self.rng.rand()\n                    cand = (1 - alpha) * cand + alpha * median\n\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                alpha = 0.0\n                if self.rng.rand() < 0.3:\n                    alpha = 0.05 * (0.5 + self.rng.rand())\n                # anisotropic gaussian step: per-dim sigma = local_scale * pscale * adapt factor\n                adapt_factor = 1.0 + 2.0 * (self.gscale - 0.7)\n                sigma = self.local_scale * pscale * adapt_factor\n                noise = self.rng.randn(dim) * sigma\n                cand = base.copy()\n                cand = (1 - alpha) * cand + alpha * median + noise\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 4 and self.rng.rand() < 0.18:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elite_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elite_idx] - np.mean(Xf[elite_idx], axis=0)\n                    # PCA via SVD\n                    try:\n                        U, Svals, Vt = np.linalg.svd(elites, full_matrices=False)\n                        principal = Vt[0]\n                        mag = 0.05 * (ub - lb) * (0.5 + self.rng.rand())\n                        cand = cand + principal * (self.rng.randn() * np.linalg.norm(mag))\n                    except Exception:\n                        pass\n                # occasional small DE-like donor mixing inside local branch\n                if donor1 is not None and donor2 is not None and self.rng.rand() < 0.12:\n                    cand = cand + 0.5 * self.gscale * (donor1 - donor2) * (self.rng.rand() * 0.5 + 0.5)\n\n                # occasional tempered Cauchy for small escape\n                if self.rng.rand() < 0.06:\n                    cand = cand + self._tempered_cauchy(scale=0.2 * (ub - lb))\n\n            # safety: if any non-finite values, replace them by uniform samples\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = lb[bad] + self.rng.rand(np.count_nonzero(bad)) * (ub[bad] - lb[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respect budget)\n            try:\n                f_cand = safe_eval(cand)\n            except RuntimeError:\n                break\n\n            # update archive (circular memory)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n            if len(archive_X) > self.archive_max:\n                # remove some worst / oldest:\n                # keep best 80% by fitness among finite if possible\n                Fa = np.array(archive_F)\n                finite_a = np.isfinite(Fa)\n                if finite_a.sum() > 0:\n                    order = np.argsort(Fa[finite_a])\n                    keep_num = max(1, int(0.8 * self.archive_max))\n                    keep_idx = np.where(finite_a)[0][order[:keep_num]]\n                    # fill rest with newest entries\n                    all_idx = np.arange(len(archive_X))\n                    remaining = list(set(all_idx.tolist()) - set(keep_idx))\n                    # sort remaining by recency: keep newest\n                    remaining_sorted = sorted(remaining, reverse=True)\n                    to_keep = keep_idx.tolist() + remaining_sorted[:(self.archive_max - len(keep_idx))]\n                    new_archive_X = [archive_X[i] for i in to_keep]\n                    new_archive_F = [archive_F[i] for i in to_keep]\n                    archive_X = new_archive_X\n                    archive_F = new_archive_F\n                else:\n                    # no finite; just keep the most recent ones\n                    archive_X = archive_X[-self.archive_max:]\n                    archive_F = archive_F[-self.archive_max:]\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            placed_top = False\n            Fa = np.array(archive_F)\n            finite_idxs = np.where(np.isfinite(Fa))[0]\n            if finite_idxs.size > 0:\n                sorted_vals = np.sort(Fa[finite_idxs])\n                # rank of recent candidate among finite\n                rank = np.searchsorted(sorted_vals, f_cand, side='left')\n                pct = rank / float(sorted_vals.size)\n                if np.isfinite(f_cand) and pct <= 0.25:\n                    placed_top = True\n            else:\n                if np.isfinite(f_cand):\n                    placed_top = True\n\n            success_window.append(bool(improved or placed_top))\n            srate = float(np.mean(success_window)) if len(success_window) > 0 else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 7 == 0:\n                # if many successes, modestly increase exploratory scale (move farther)\n                if srate > 0.30:\n                    self.gscale = min(1.6, self.gscale * 1.08)\n                    self.local_scale = min(0.5, self.local_scale * 1.05)\n                # if stagnating, increase scale to escape local minima\n                elif srate < 0.06:\n                    self.gscale = min(2.5, self.gscale * 1.12)\n                    self.local_scale = min(1.0, self.local_scale * 1.08)\n                # otherwise slowly decay to refine\n                else:\n                    self.gscale = max(0.3, self.gscale * 0.97)\n                    self.local_scale = max(1e-4, self.local_scale * 0.98)\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(40, 6 * dim) or (iters > 120 and srate < 0.02):\n                # perform micro-restarts: inject a few diversified points\n                n_restart = min(6, max(2, dim // 2))\n                # half around best local neighborhood, half global random\n                for k in range(n_restart):\n                    if self.evals >= budget:\n                        break\n                    if self.x_opt is not None and k < n_restart // 2:\n                        # local diversified around best\n                        jitter = self.rng.randn(dim) * (0.15 + 0.85 * self.rng.rand()) * (ub - lb) * 0.1\n                        candr = self.x_opt + jitter\n                        candr = self._reflect_bounds(candr, lb, ub)\n                    else:\n                        candr = self._uniform_array(lb, ub, n=1)\n                    try:\n                        fr = safe_eval(candr)\n                    except RuntimeError:\n                        break\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.f_opt = float(fr)\n                        self.x_opt = candr.copy()\n                stagn_count = 0\n                # bump scales to explore a bit more\n                self.gscale = min(2.0, self.gscale * 1.25)\n                self.local_scale = min(0.6, self.local_scale * 1.2)\n\n        # final safety: pick best finite point from archive if x_opt still None or to ensure best\n        Fa = np.array(archive_F) if archive_F else np.array([])\n        if Fa.size > 0:\n            finite_idxs = np.where(np.isfinite(Fa))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(Fa[finite_idxs])]\n                self.x_opt = np.array(archive_X[int(best_i)]).copy()\n                self.f_opt = float(Fa[best_i])\n        # ensure x_opt defined\n        if self.x_opt is None:\n            self.x_opt = center.copy()\n            self.f_opt = float(safe_eval(center)) if self.evals < budget else float(np.inf)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 197, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=30)", "error": "In the code, line 197, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=30)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a0b20ae3-3011-4494-9123-99b9a8b5d455", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n        # number of initial LHS samples (besides center and one uniform)\n        if init_samples is None:\n            self.init_samples = max(8, min(40, int(6 * np.sqrt(self.dim))))\n        else:\n            self.init_samples = int(init_samples)\n\n        # internal state (filled at run)\n        self.f_evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        lb = None\n        ub = None\n        # Common attribute names\n        if hasattr(func, \"bounds\") and getattr(func, \"bounds\") is not None:\n            b = func.bounds\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n        # try direct attributes\n        if lb is None and hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n            lb = np.asarray(func.lb, dtype=float)\n            ub = np.asarray(func.ub, dtype=float)\n\n        # fallback scalar defaults\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast scalars or singletons to dim\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # pad or truncate if necessary\n        if lb.size < self.dim:\n            lb = np.concatenate([lb, np.full(self.dim - lb.size, lb[-1])])\n        if ub.size < self.dim:\n            ub = np.concatenate([ub, np.full(self.dim - ub.size, ub[-1])])\n        if lb.size > self.dim:\n            lb = lb[: self.dim]\n        if ub.size > self.dim:\n            ub = ub[: self.dim]\n\n        # ensure lb < ub elementwise (or swap if reversed)\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds (mirror bounce) a few times to settle\n        x = np.array(x, dtype=float)\n        # iterative reflection to handle large steps\n        for _ in range(4):\n            low = x < lb\n            if np.any(low):\n                x[low] = lb[low] + (lb[low] - x[low])\n            high = x > ub\n            if np.any(high):\n                x[high] = ub[high] - (x[high] - ub[high])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        rng = self.rng\n        lhs = np.zeros((n, dim))\n        for j in range(dim):\n            perm = rng.permutation(n)\n            # stratified samples inside each stratum\n            lhs[:, j] = (perm + rng.uniform(0.0, 1.0, size=n)) / float(n)\n        # clip safety\n        return np.clip(lhs, 0.0, 1.0)\n\n    def _uniform_array(self, lb, ub, n=1):\n        # returns (n, dim) or (dim,) if n==1\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        samples = self.rng.uniform(0.0, 1.0, size=(n, self.dim)) * (ub - lb) + lb\n        if n == 1:\n            return samples[0]\n        return samples\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        center = 0.5 * (lb + ub)\n\n        # safe_eval uses only remaining budget\n        def safe_eval(x):\n            if self.f_evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            try:\n                x_arr = np.asarray(x, dtype=float).reshape(self.dim)\n            except Exception:\n                self.f_evals += 1\n                return np.inf\n            try:\n                f = float(func(x_arr))\n            except Exception:\n                f = np.inf\n            self.f_evals += 1\n            return f\n\n        # initialize archive\n        X_archive = []\n        F_archive = []\n\n        # Evaluate center first if budget allows\n        try:\n            f_center = safe_eval(center)\n        except StopIteration:\n            # budget 0\n            return self.f_opt, self.x_opt\n        X_archive.append(center.copy())\n        F_archive.append(f_center)\n        if np.isfinite(f_center) and f_center < self.f_opt:\n            self.f_opt = f_center\n            self.x_opt = center.copy()\n\n        # LHS initialization\n        n_init = max(0, self.init_samples - 1)\n        lhs = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            x = lb + lhs[i] * (ub - lb)\n            # skip exact center duplicates\n            if np.allclose(x, center):\n                x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform extra\n        if self.f_evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                f = np.inf\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # convert archive to arrays\n        Xf = np.array(X_archive, dtype=float)\n        F_arr = np.array(F_archive, dtype=float)\n\n        # control variables\n        gscale = 0.05  # global exploratory scale (fraction of bound span)\n        success_window = []\n        adapt_every = max(10, int(3 * np.sqrt(self.dim)))\n        stagnation_limit = max(30, int(10 * np.sqrt(self.dim)))\n        it = 0\n        no_improve = 0\n\n        # main loop\n        while self.f_evals < self.budget:\n            it += 1\n            # robust center: median of finite points\n            finite_mask = np.isfinite(F_arr)\n            if np.any(finite_mask):\n                try:\n                    med = np.median(Xf[finite_mask], axis=0)\n                except Exception:\n                    med = center.copy()\n            else:\n                med = center.copy()\n\n            # per-dim scale from percentiles\n            if np.sum(finite_mask) >= 2:\n                p25 = np.percentile(Xf[finite_mask], 25, axis=0)\n                p75 = np.percentile(Xf[finite_mask], 75, axis=0)\n                spread = (p75 - p25)\n                # ensure minimum spread\n                sigma_base = np.maximum(spread, 1e-6 * span) + 1e-12\n            else:\n                sigma_base = 0.05 * span\n\n            # choose base biased towards better archive items using softmax on negative F\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best_f = np.min(F_arr[finite_idx])\n                logits = -F_arr.copy()\n                # for infinities give large negative\n                logits[~np.isfinite(F_arr)] = np.min(logits) - 10.0 - np.abs(np.max(logits))\n                # numerical softmax\n                L = logits - np.max(logits)\n                probs = np.exp(L)\n                probs /= np.sum(probs)\n                base_idx = self.rng.choice(len(F_arr), p=probs)\n                base = Xf[base_idx].copy()\n            else:\n                base = self._uniform_array(lb, ub)\n\n            # Donor pool for DE-like moves: pick two distinct archive points biased to best\n            donors = None\n            if len(F_arr) >= 3:\n                # sample two indices with the same softmax distribution but ensure distinct\n                try:\n                    idxs = self.rng.choice(len(F_arr), size=2, replace=False, p=probs)\n                except Exception:\n                    idxs = self.rng.choice(len(F_arr), size=2, replace=False)\n                donors = [Xf[int(i)].copy() for i in idxs]\n\n            # decide move type\n            p_global = 0.18  # chance of global (Cauchy) escape\n            p_pca = 0.12     # chance PCA-guided elite perturbation\n            move_rand = self.rng.random()\n\n            # prepare candidate\n            if move_rand < p_global:\n                # GLOBAL move: tempered Cauchy jumps anchored on an elite or uniform\n                if np.any(finite_mask) and self.rng.random() < 0.85:\n                    # pick an elite among top half biased to best\n                    n_elite = max(1, int(0.5 * max(1, np.sum(finite_mask))))\n                    elite_idx = np.argsort(F_arr[finite_mask])[:n_elite]\n                    global_anchor_candidates = Xf[finite_mask][elite_idx]\n                    anchor = global_anchor_candidates[self.rng.randint(0, global_anchor_candidates.shape[0])]\n                else:\n                    anchor = self._uniform_array(lb, ub)\n                # tempered (scaled) Cauchy with truncation: larger jumps occasionally\n                # draw standard Cauchy via tan(pi*(u-0.5))\n                u = self.rng.uniform(0.0, 1.0, size=self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # temper extreme tails\n                cauch = np.clip(cauch, -10.0, 10.0)\n                scale = gscale * (0.5 + 3.0 * self.rng.random())  # sometimes large\n                cand = anchor + (cauch * scale) * span\n                # occasional coordinate-wise uniform injection (promotes exploration)\n                if self.rng.random() < 0.08:\n                    d = self.rng.randint(0, self.dim)\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                if self.rng.random() < 0.35:\n                    base_attracted = 0.5 * base + 0.5 * med\n                else:\n                    base_attracted = base.copy()\n\n                # anisotropic gaussian step scaled by sigma_base and gscale\n                # ensure sigma_base is non-zero per-dim\n                sigma = (sigma_base / (np.maximum(1e-12, span))) * gscale * span\n                # gaussian noise with dimension-wise amplitude\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                cand = base_attracted + noise\n\n                # PCA-guided elite perturbation occasionally\n                if len(F_arr) >= 6 and self.rng.random() < p_pca:\n                    # select top elites and compute principal direction\n                    fin_idx = np.where(np.isfinite(F_arr))[0]\n                    ne = max(3, int(0.25 * fin_idx.size))\n                    elite_ids = fin_idx[np.argsort(F_arr[fin_idx])[:ne]]\n                    try:\n                        X_el = Xf[elite_ids] - np.mean(Xf[elite_ids], axis=0)\n                        U, S, Vt = np.linalg.svd(X_el, full_matrices=False)\n                        # take first principal vector(s)\n                        nvec = min(2, Vt.shape[0])\n                        for j in range(nvec):\n                            pv = Vt[j]\n                            amp = (0.5 + self.rng.random() * 1.0) * (S[j] / (ne + 1.0))\n                            cand = cand + (amp * pv) * (gscale * 2.0) * span\n                        # small jitter\n                        cand = cand + self.rng.normal(0.0, 0.02, size=self.dim) * span\n                    except Exception:\n                        # fallback: small isotropic perturbation\n                        cand = cand + self.rng.normal(0.0, 1e-3, size=self.dim) * span\n\n                # DE-like donor mixing inside local branch sometimes\n                if donors is not None and self.rng.random() < 0.22:\n                    F_scale = 0.35 + 0.9 * self.rng.random()\n                    cand = cand + F_scale * (donors[0] - donors[1]) * (0.2 + 0.9 * self.rng.random())\n\n                # occasional tempered Cauchy jump inside local move\n                if self.rng.random() < 0.04:\n                    u = self.rng.uniform(0.0, 1.0, size=self.dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    cauch = np.clip(cauch, -6.0, 6.0)\n                    cand = cand + 0.5 * gscale * cauch * span\n\n            # safety: if any non-finite values, replace them with uniform samples\n            if not np.all(np.isfinite(cand)):\n                nan_mask = ~np.isfinite(cand)\n                cand[nan_mask] = self._uniform_array(lb[nan_mask], ub[nan_mask])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            try:\n                f_cand = safe_eval(cand)\n            except StopIteration:\n                break\n\n            # append to archive\n            Xf = np.vstack([Xf, cand.reshape(1, -1)]) if Xf.size else np.array([cand])\n            F_arr = np.concatenate([F_arr, np.array([f_cand])])\n\n            # update best trackers and stagnation\n            improved_global = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                improved_global = True\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            finite_vals = F_arr[np.isfinite(F_arr)] if F_arr.size > 0 else np.array([])\n            placed_top = False\n            if finite_vals.size > 0 and np.isfinite(f_cand):\n                thr = np.percentile(finite_vals, 25.0)\n                placed_top = (f_cand <= thr)\n            elif np.isfinite(f_cand):\n                placed_top = True\n\n            success = improved_global or placed_top\n            success_window.append(1 if success else 0)\n            if len(success_window) > 100:\n                success_window.pop(0)\n\n            # adapt gscale every adapt_every iterations\n            if it % adapt_every == 0:\n                srate = np.mean(success_window) if len(success_window) > 0 else 0.0\n                if srate > 0.3:\n                    # many recent successes, slightly increase exploration to find broader improvement\n                    gscale *= (1.02 + 0.03 * self.rng.random())\n                elif srate < 0.05:\n                    # stagnating, increase scale to escape\n                    gscale *= (1.12 + 0.6 * self.rng.random())\n                else:\n                    # otherwise slowly decay to refine\n                    gscale *= 0.98\n                # keep gscale within sensible bounds\n                gscale = np.clip(gscale, 1e-5, 2.5)\n\n            # prune archive to maintain memory budget\n            if Xf.shape[0] > self.archive_max:\n                keep_idx = []\n                fin_idx = np.where(np.isfinite(F_arr))[0]\n                # keep best fraction\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort(F_arr[fin_idx])]\n                    n_keep_best = max(3, int(0.2 * self.archive_max))\n                    keep_idx.extend(sorted_fin[:n_keep_best].tolist())\n                # keep some random rest (including some infinities)\n                remaining = [i for i in range(Xf.shape[0]) if i not in keep_idx]\n                to_add = min(self.archive_max - len(keep_idx), len(remaining))\n                if to_add > 0:\n                    add = list(self.rng.choice(remaining, size=to_add, replace=False))\n                    keep_idx.extend(add)\n                keep_idx = sorted(set(keep_idx))\n                Xf = Xf[keep_idx]\n                F_arr = F_arr[keep_idx]\n\n            # stagnation detection & micro-restarts\n            if no_improve >= stagnation_limit:\n                # perform micro-restarts: inject a few diversified points\n                n_inject = min(6, max(3, int(2 + np.sqrt(self.dim))))\n                for _ in range(n_inject):\n                    if self.x_opt is not None and self.rng.random() < 0.85:\n                        # around best (local diversified)\n                        scale = gscale * (1.2 + self.rng.random() * 2.0)\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * (scale * span)\n                        x_new = self.x_opt + perturb\n                    else:\n                        # global diversified uniform\n                        x_new = self._uniform_array(lb, ub)\n                    x_new = self._reflect_bounds(x_new, lb, ub)\n                    try:\n                        f_new = safe_eval(x_new)\n                    except StopIteration:\n                        break\n                    Xf = np.vstack([Xf, x_new.reshape(1, -1)])\n                    F_arr = np.concatenate([F_arr, np.array([f_new])])\n                    if np.isfinite(f_new) and f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = x_new.copy()\n                        no_improve = 0\n                # encourage exploration after restart\n                gscale *= (1.06 + 0.4 * self.rng.random())\n                gscale = np.clip(gscale, 1e-5, 2.5)\n                no_improve = 0  # reset\n\n        # final safety: pick best finite point from archive if x_opt still None or to ensure best\n        finite_idx = np.where(np.isfinite(F_arr))[0]\n        if finite_idx.size > 0:\n            best_idx = finite_idx[np.argmin(F_arr[finite_idx])]\n            if self.x_opt is None or F_arr[best_idx] < self.f_opt:\n                self.f_opt = float(F_arr[best_idx])\n                self.x_opt = Xf[best_idx].copy()\n        # ensure x_opt defined\n        if self.x_opt is None:\n            self.x_opt = center.copy()\n            try:\n                self.f_opt = float(func(self.x_opt))\n            except Exception:\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1018bdcd-0858-40a4-a413-a70c238a4945", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        \"\"\"\n        budget: number of func evaluations allowed\n        dim: dimension of problem\n        seed: RNG seed\n        init_samples: number of initial LHS samples (default: 2*dim or 12 min)\n        archive_max: maximum archive size to keep\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.archive_max = int(archive_max)\n        if init_samples is None:\n            self.init_samples = max(12, 2 * self.dim)\n        else:\n            self.init_samples = int(init_samples)\n\n    # ---------------- utils ----------------\n    def _get_bounds(self, func):\n        # try several common locations for bounds, then default to [-5,5]\n        lb = None\n        ub = None\n\n        # try func.bounds.lb / .ub (common in BBOB wrappers)\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            lb = getattr(b, \"lb\", None)\n            ub = getattr(b, \"ub\", None)\n\n        # try direct attributes\n        if lb is None:\n            lb = getattr(func, \"lb\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None)\n\n        # if still None, try callable metadata dicts\n        if lb is None or ub is None:\n            md = getattr(func, \"metadata\", None)\n            if isinstance(md, dict):\n                lb = lb or md.get(\"lb\") or md.get(\"lower\")\n                ub = ub or md.get(\"ub\") or md.get(\"upper\")\n\n        if lb is None or ub is None:\n            # fallback to scalar [-5,5]\n            lb = -5.0\n            ub = 5.0\n\n        # convert to numpy arrays broadcast to dim\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        elif lb.size < self.dim:\n            # pad or repeat\n            lb = np.resize(lb, self.dim)\n\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        elif ub.size < self.dim:\n            ub = np.resize(ub, self.dim)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect overflow coordinates (bounce-back) iteratively up to few times\n        x = np.array(x, dtype=float)\n        for _ in range(3):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin Hypercube in [0,1]^dim\n        rng = self.rng\n        result = np.empty((n, dim), dtype=float)\n        intervals = np.linspace(0.0, 1.0, n + 1)\n        for j in range(dim):\n            perm = rng.permutation(n)\n            # sample uniformly inside each stratum\n            u = rng.random(n)\n            result[:, j] = intervals[perm] + u * (1.0 / n)\n        return result\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size) if n > 1 else (lb.size,)\n        u = self.rng.random(shape)\n        return lb + (ub - lb) * u\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # Safe evaluator that enforces budget and skips invalid calls\n        self.func = func\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n\n        calls_left = [self.budget]  # mutable capture\n\n        def safe_eval(x):\n            # x should be 1D array-like\n            if calls_left[0] <= 0:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            # ensure correct shape\n            if x.ndim != 1 or x.size != dim:\n                raise ValueError(\"x must be 1-D of length dim\")\n            calls_left[0] -= 1\n            try:\n                fx = func(x)\n            except Exception:\n                # return +inf on error\n                return np.inf\n            # convert to scalar float if possible\n            try:\n                fxv = float(np.asarray(fx).item())\n            except Exception:\n                fxv = np.inf\n            if not np.isfinite(fxv):\n                return np.inf\n            return fxv\n\n        # trackers\n        archive_X = []  # list of np arrays\n        archive_F = []  # list of floats\n        best_f = np.inf\n        best_x = None\n\n        # initialization: center + LHS + one uniform\n        center = (lb + ub) / 2.0\n\n        # evaluate center first if possible\n        try:\n            f_center = safe_eval(center)\n        except RuntimeError:\n            # budget 0\n            return np.inf, center\n        archive_X.append(np.array(center, dtype=float))\n        archive_F.append(f_center)\n        if np.isfinite(f_center) and f_center < best_f:\n            best_f = f_center\n            best_x = center.copy()\n\n        # LHS\n        nlhs = max(0, min(self.init_samples, calls_left[0]))\n        if nlhs > 0:\n            u = self._lhs01(nlhs, dim)\n            samples = lb + (ub - lb) * u\n            for i in range(nlhs):\n                x = samples[i]\n                # skip duplicates near center\n                if np.linalg.norm(x - center) < 1e-12:\n                    continue\n                if calls_left[0] <= 0:\n                    break\n                f = safe_eval(x)\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if np.isfinite(f) and f < best_f:\n                    best_f = f\n                    best_x = x.copy()\n\n        # ensure at least one random\n        if calls_left[0] > 0:\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive_X.append(np.array(x, dtype=float))\n            archive_F.append(f)\n            if np.isfinite(f) and f < best_f:\n                best_f = f\n                best_x = np.array(x, dtype=float)\n\n        # convert archives to arrays for convenience\n        def _to_arrays():\n            if len(archive_X) == 0:\n                return np.zeros((0, dim)), np.array([], dtype=float)\n            X = np.vstack([np.asarray(x, dtype=float) for x in archive_X])\n            F = np.array(archive_F, dtype=float)\n            return X, F\n\n        # short-term adaptation variables\n        gscale = 0.2 * np.linalg.norm(ub - lb)  # global scale for proposals\n        min_gscale = 1e-8\n        max_gscale = np.linalg.norm(ub - lb) * 2.0\n        adapt_window = 20\n        recent_successes = []\n        iter_count = 0\n        stagnation_counter = 0\n\n        # main loop\n        while calls_left[0] > 0:\n            iter_count += 1\n            X, F = _to_arrays()\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                finite_F = F[finite_mask]\n                finite_X = X[finite_mask, :]\n                # robust center: median of finite points\n                center_robust = np.median(finite_X, axis=0)\n                # per-dim scale: interquartile range (75-25 percentile), ensure nonzero\n                p75 = np.percentile(finite_X, 75, axis=0)\n                p25 = np.percentile(finite_X, 25, axis=0)\n                iq = p75 - p25\n                local_scale = np.maximum(iq, 1e-9)  # (dim,)\n            else:\n                center_robust = (lb + ub) / 2.0\n                local_scale = np.maximum(ub - lb, 1e-9)\n\n            # choose base biased toward better archive items using softmax on negative F\n            # higher probability to smaller F\n            logits = np.zeros(len(F))\n            if len(F) > 0 and np.any(np.isfinite(F)):\n                # replace inf by large positive number so negative-of-it is very negative\n                finite_idx = np.where(np.isfinite(F))[0]\n                subF = F[finite_idx]\n                # stabilize logits by subtracting min\n                inv = -subF\n                inv = inv - np.min(inv)\n                # small temperature scaling\n                temp = 0.5\n                probs_sub = np.exp(inv / (temp + 1e-12))\n                probs_sub = probs_sub / (np.sum(probs_sub) + 1e-12)\n                logits[finite_idx] = probs_sub\n                # ensure tiny probability for infinities\n                inf_idx = np.where(~np.isfinite(F))[0]\n                if inf_idx.size > 0:\n                    logits[inf_idx] = 1e-6\n                # sample base index\n                base_idx = self.rng.choice(len(F), p=logits / logits.sum())\n                base = np.array(X[base_idx], dtype=float)\n            else:\n                base = center_robust.copy()\n\n            # donor pool for DE-like moves: pick two donors biased to best\n            donors = None\n            if len(F) >= 3 and np.any(np.isfinite(F)):\n                kpool = max(2, min(8, len(F)))\n                # sample kpool indices biased to best\n                # reuse logits distribution to sample with replacement (but not the base index)\n                probs = np.maximum(logits, 0.0)\n                if probs.sum() <= 0:\n                    probs = None\n                else:\n                    probs = probs / probs.sum()\n                # select donors ensuring distinct indices\n                try:\n                    choices = self.rng.choice(len(F), size=min(kpool, len(F)), replace=False, p=probs)\n                except Exception:\n                    choices = self.rng.choice(len(F), size=min(kpool, len(F)), replace=False)\n                # choose two distinct donors from choices, not equal to base_idx if possible\n                choices = [int(c) for c in choices if (len(F) <= 3 or c != base_idx)]\n                if len(choices) >= 2:\n                    d1, d2 = self.rng.choice(choices, size=2, replace=False)\n                    donors = (np.array(X[d1], dtype=float), np.array(X[d2], dtype=float))\n\n            # decide move type\n            # mix probabilities: local (0.5), global (0.35), pca (0.10), random (0.05)\n            r = self.rng.random()\n            candidate = None\n            if r < 0.5:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                attract_prob = 0.2\n                if self.rng.random() < attract_prob:\n                    anchor = center_robust\n                else:\n                    anchor = base\n                # anisotropic gaussian step: scale per-dim by local_scale and gscale\n                scales = (local_scale / (np.median(local_scale) + 1e-12)) * gscale\n                # ensure min spread\n                scales = np.maximum(scales, 1e-12)\n                step = self.rng.normal(loc=0.0, scale=1.0, size=dim) * scales\n                if donors is not None and self.rng.random() < 0.15:\n                    # incorporate DE-like donor difference into local move\n                    dstep = donors[0] - donors[1]\n                    ffac = 0.5 * self.rng.random()\n                    step += ffac * dstep\n                candidate = anchor + step\n\n            elif r < 0.85:\n                # GLOBAL move: DE-like + tempered Cauchy jumps anchored on an elite or uniform\n                # choose elite among top half biased to best\n                cand_base = base.copy()\n                if donors is not None:\n                    Fd = 0.8 * (self.rng.random() + 0.2)\n                    candidate = cand_base + Fd * (donors[0] - donors[1])\n                else:\n                    candidate = cand_base.copy()\n                # tempered Cauchy perturbation (heavy-tail but tempered)\n                if self.rng.random() < 0.6:\n                    # sample Cauchy then shrink\n                    u = self.rng.standard_cauchy(size=dim)\n                    # temper by logistic to prevent huge moves\n                    temper = 1.0 / (1.0 + np.abs(u) / 3.0)\n                    candidate += (gscale * 0.8) * u * temper\n                # occasional uniform injection of one coordinate\n                if self.rng.random() < 0.2:\n                    idx = self.rng.integers(0, dim)\n                    candidate[idx] = lb[idx] + self.rng.random() * (ub[idx] - lb[idx])\n\n            elif r < 0.95:\n                # PCA-guided elite perturbation\n                # pick top-k elites (25% best or at least 3)\n                if np.any(finite_mask):\n                    k = max(3, int(max(3, 0.25 * finite_F.size)))\n                    # indices of finite sorted by F\n                    order = np.argsort(finite_F)\n                    sel = finite_X[order[:k], :]\n                    # perform PCA via SVD on centered sel\n                    C = sel - np.mean(sel, axis=0, keepdims=True)\n                    try:\n                        U, S, Vt = np.linalg.svd(C, full_matrices=False)\n                        principal = Vt[0]\n                        # move along principal with jitter\n                        mag = gscale * (1.0 + 0.5 * self.rng.random())\n                        jitter = self.rng.normal(scale=0.05, size=dim) * (ub - lb) * 0.02\n                        candidate = center_robust + mag * principal + jitter\n                    except Exception:\n                        # fallback to random local gaussian\n                        candidate = base + self.rng.normal(scale=0.5 * gscale, size=dim)\n                else:\n                    candidate = self._uniform_array(lb, ub, n=1)\n\n            else:\n                # pure random injection\n                candidate = self._uniform_array(lb, ub, n=1)\n\n            # occasional extra tempered Cauchy escape\n            if self.rng.random() < 0.02:\n                candidate += (gscale * 2.0) * (self.rng.standard_cauchy(size=dim) / (1.0 + np.abs(self.rng.standard_cauchy(size=dim))))\n\n            # safety: replace NaNs or inf with uniform samples\n            candidate = np.asarray(candidate, dtype=float)\n            naninf = ~np.isfinite(candidate)\n            if np.any(naninf):\n                candidate[naninf] = self._uniform_array(lb[naninf], ub[naninf], n=1)\n\n            # reflect into bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate\n            try:\n                f_c = safe_eval(candidate)\n            except RuntimeError:\n                break  # budget exhausted\n            archive_X.append(candidate.copy())\n            archive_F.append(f_c)\n\n            # update best trackers and stagnation\n            improved = False\n            if np.isfinite(f_c):\n                if f_c < best_f:\n                    best_f = f_c\n                    best_x = candidate.copy()\n                    improved = True\n\n            # success definition: improved global best OR placed among top 25% of finite vals\n            success = False\n            finiteFvals = F[np.isfinite(F)] if len(F) > 0 else np.array([])\n            if np.isfinite(f_c):\n                if f_c < best_f:  # already covered but keep\n                    success = True\n                elif finiteFvals.size > 0:\n                    threshold = np.percentile(finiteFvals, 25)\n                    if f_c <= threshold:\n                        success = True\n                else:\n                    # no earlier finite; any finite is success\n                    success = True\n\n            recent_successes.append(1 if success else 0)\n            if len(recent_successes) > adapt_window:\n                recent_successes.pop(0)\n\n            # adapt gscale every adapt_window iterations\n            if iter_count % adapt_window == 0:\n                succ_rate = np.mean(recent_successes) if recent_successes else 0.0\n                if succ_rate > 0.4:\n                    # more successes: modestly increase exploratory scale\n                    gscale *= 1.10 + 0.05 * (succ_rate - 0.4)\n                elif succ_rate < 0.05:\n                    # stagnating: increase scale to escape\n                    gscale *= 1.25\n                else:\n                    # otherwise decay slowly to refine\n                    gscale *= 0.98\n                # clamp gscale\n                gscale = float(np.clip(gscale, min_gscale, max_gscale))\n\n            # prune archive to maintain memory budget fairly (keep best and some random)\n            if len(archive_X) > self.archive_max:\n                Xcur, Fcur = _to_arrays()\n                # keep best kbest, plus some randoms\n                kbest = max(5, int(0.05 * self.archive_max))\n                order_all = np.argsort(Fcur)\n                keep_idx = set(order_all[:kbest].tolist())\n                # add some random indices to keep diversity\n                rest_idx = [i for i in range(len(Fcur)) if i not in keep_idx]\n                self.rng.shuffle(rest_idx)\n                add = min(self.archive_max - len(keep_idx), len(rest_idx))\n                keep_idx.update(rest_idx[:add])\n                # reconstruct archives\n                newX = [archive_X[i] for i in sorted(list(keep_idx))]\n                newF = [archive_F[i] for i in sorted(list(keep_idx))]\n                archive_X = newX\n                archive_F = newF\n\n            # stagnation detection & micro-restarts\n            if len(recent_successes) >= adapt_window:\n                if sum(recent_successes) == 0:\n                    stagnation_counter += 1\n                else:\n                    stagnation_counter = 0\n            else:\n                stagnation_counter = 0\n\n            if stagnation_counter >= 3:\n                # micro-restart: inject diversified points\n                stagnation_counter = 0\n                recent_successes = []\n                # around the best: create 4 local perturbations\n                n_local = min(4, calls_left[0])\n                for _ in range(n_local):\n                    pert = best_x + self.rng.normal(scale=gscale * 0.5, size=dim) * (local_scale / (np.median(local_scale) + 1e-12))\n                    pert = self._reflect_bounds(pert, lb, ub)\n                    if calls_left[0] <= 0:\n                        break\n                    f = safe_eval(pert)\n                    archive_X.append(pert.copy())\n                    archive_F.append(f)\n                    if np.isfinite(f) and f < best_f:\n                        best_f = f\n                        best_x = pert.copy()\n                # random injections\n                n_rand = min(6, calls_left[0])\n                for _ in range(n_rand):\n                    if calls_left[0] <= 0:\n                        break\n                    rx = self._uniform_array(lb, ub, n=1)\n                    try:\n                        f = safe_eval(rx)\n                    except RuntimeError:\n                        break\n                    archive_X.append(np.array(rx, dtype=float))\n                    archive_F.append(f)\n                    if np.isfinite(f) and f < best_f:\n                        best_f = f\n                        best_x = np.array(rx, dtype=float)\n                # slightly bump gscale to explore\n                gscale = min(max_gscale, gscale * 1.4)\n\n        # final safety: pick best finite point from archive if needed\n        Xfin, Ffin = _to_arrays()\n        if Xfin.shape[0] > 0:\n            finite_mask = np.isfinite(Ffin)\n            if np.any(finite_mask):\n                idx = int(np.argmin(Ffin[finite_mask]))\n                # map to full index\n                fin_indices = np.where(finite_mask)[0]\n                best_idx = fin_indices[idx]\n                best_x_final = Xfin[best_idx].copy()\n                best_f_final = float(Ffin[best_idx])\n            else:\n                # no finite found, fallback to center\n                best_x_final = center.copy()\n                best_f_final = np.inf\n        else:\n            best_x_final = center.copy()\n            best_f_final = np.inf\n\n        # ensure best_x defined\n        if best_x is None or not np.isfinite(best_f):\n            best_x = best_x_final\n            best_f = best_f_final\n\n        self.f_opt = float(best_f)\n        self.x_opt = np.array(best_x, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 887, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities contain NaN", "error": "In the code, line 887, in numpy.random._generator.Generator.choice, the following error occurred:\nValueError: Probabilities contain NaN", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "756e1fdf-f9f1-4985-8019-4efd1dd1b34c", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves,", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(archive_max)\n        # short-term adaptation scale (relative to per-dim interquartile range)\n        self.gscale = 0.5\n        self.rng = np.random.default_rng(seed)\n        # trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        lb = None\n        ub = None\n        # common attribute names used by some benchmarks\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # if it's a pair (lb, ub)\n            if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n            else:\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n        # try name variants directly on func\n        if lb is None:\n            for name in (\"lb\", \"lower\", \"xmin\", \"xlower\"):\n                if hasattr(func, name):\n                    lb = np.asarray(getattr(func, name), dtype=float)\n                    break\n        if ub is None:\n            for name in (\"ub\", \"upper\", \"xmax\", \"xupper\"):\n                if hasattr(func, name):\n                    ub = np.asarray(getattr(func, name), dtype=float)\n                    break\n        # final fallback\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n        lb = np.atleast_1d(np.array(lb, dtype=float))\n        ub = np.atleast_1d(np.array(ub, dtype=float))\n        # broadcast to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n        # pad/truncate\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        if lb.size > self.dim:\n            lb = lb[:self.dim]\n        if ub.size > self.dim:\n            ub = ub[:self.dim]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds, do a few iterations to settle\n        x = x.copy()\n        for _ in range(8):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            x[low] = 2 * lb[low] - x[low]\n            x[high] = 2 * ub[high] - x[high]\n        # final clamp to handle any remaining numerical issues\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        dim = int(dim)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.uniform(size=(n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            idx = self.rng.permutation(n)\n            lhs[:, j] = cut[idx] + u[:, j] * (1.0 / n)\n        lhs = np.clip(lhs, 0.0, 1.0)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        n = int(n)\n        if n == 1:\n            # sample single vector between lb and ub\n            return self.rng.uniform(0.0, 1.0, size=self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.uniform(0.0, 1.0, size=(n, self.dim)) * (ub - lb) + lb\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n\n        def safe_eval(x):\n            # x: array-like of length dim\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            try:\n                fv = func(x)\n            except Exception:\n                fv = np.inf\n            try:\n                fv = float(fv)\n            except Exception:\n                fv = np.inf\n            self.evals += 1\n            return fv\n\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        center = 0.5 * (lb + ub)\n\n        # initialization: center + LHS + one uniform\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        lhs01 = self._lhs01(n0, self.dim)\n        init_points = lb + lhs01 * range_vec  # shape (n0, dim)\n        uni = self._uniform_array(lb, ub, n=1)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = float(f_center)\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(init_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = init_points[i]\n            # skip duplicates near center\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.evals < self.budget:\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = float(f_uni)\n                self.x_opt = uni.copy()\n\n        # short-term adaptation and control variables\n        gscale = float(self.gscale)\n        success_window = deque(maxlen=30)\n\n        stagn_count = 0\n        iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X_arr = np.array(archive_X) if len(archive_X) > 0 else np.empty((0, self.dim))\n            F_arr = np.array(archive_F) if len(archive_F) > 0 else np.empty((0,))\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n\n            # robust center (median of finite points) and per-dim scale (75-25 percentile)\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                median = np.median(Xf, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-12, q75 - q25)\n                mean_scale = float(np.mean(per_dim_scale) + 1e-12)\n            else:\n                Xf = np.empty((0, self.dim))\n                Ff = np.array([])\n                median = center.copy()\n                per_dim_scale = np.maximum(1e-8, 0.1 * range_vec)\n                mean_scale = float(np.mean(per_dim_scale))\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_idx.size > 0:\n                stdF = np.std(Ff)\n                stdF = stdF + 1e-9\n                # normalized negative fitness as logits (better = larger)\n                logits = -(Ff - np.min(Ff)) / stdF\n                logits = logits - np.max(logits)  # numeric stability\n                exp_logits = np.exp(logits)\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = self.rng.choice(len(Ff), p=probs)\n                base = Xf[pick_idx].copy()\n            else:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if finite_idx.size >= 3:\n                sorted_idx = np.argsort(Ff)\n                pool_size = max(3, min(30, len(sorted_idx)))\n                pick_pool = sorted_idx[:pool_size]\n                # choose kdon in [2,4] depending on pool size\n                kdon = min(4, max(2, max(2, len(pick_pool) // 5)))\n                # sample donor indices without replacement from the pool\n                chosen = self.rng.choice(pick_pool, size=min(kdon, len(pick_pool)), replace=False)\n                donors = [Xf[int(i)].copy() for i in chosen]\n            elif len(archive_X) >= 2:\n                idxs = self.rng.choice(len(archive_X), size=min(2, len(archive_X)), replace=False)\n                donors = [np.asarray(archive_X[int(i)]).copy() for i in idxs]\n\n            # decide move type\n            p_global = 0.16  # probability of global jump\n            move_rand = self.rng.random()\n            c = self.rng.standard_cauchy(self.dim)  # tempered cauchy vector (unscaled)\n\n            if move_rand < p_global:\n                # GLOBAL move: DE-like + tempered Cauchy jumps anchored on an elite or uniform\n                if finite_idx.size > 0 and self.rng.random() < 0.9:\n                    n_elite = max(1, Xf.shape[0] // 2)\n                    elite_idx = np.argsort(Ff)[:n_elite]\n                    anchor = Xf[self.rng.choice(elite_idx)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n\n                cand = anchor.copy()\n                # if we have donors, do DE-like difference\n                if len(donors) >= 2:\n                    dmove = donors[0] - donors[1]\n                    F_scale = 0.8 * (0.3 + 0.7 * self.rng.random())\n                    cand = cand + F_scale * dmove\n                # tempered Cauchy perturbation\n                scale = gscale * (0.6 + 0.9 * self.rng.random()) * mean_scale\n                cand = cand + scale * c\n                # occasional uniform injection of one coordinate\n                if self.rng.random() < 0.12:\n                    d = int(self.rng.integers(0, self.dim))\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                alpha = 0.0\n                if self.rng.random() < 0.35:\n                    alpha = 0.1 * (self.rng.random() + 0.5)  # small attraction factor\n\n                base_attracted = (1 - alpha) * base + alpha * median\n                # anisotropic gaussian step: per-dim sigma scaled by gscale and per_dim_scale\n                sigma = gscale * (0.6 + 0.8 * self.rng.random()) * per_dim_scale\n                # to avoid too small sigma, ensure min\n                sigma = np.maximum(sigma, 1e-12)\n                cand = base_attracted + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 6 and self.rng.random() < 0.20:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elites_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elites_idx]\n                    E = elites - np.mean(elites, axis=0, keepdims=True)\n                    try:\n                        # PCA via SVD of E\n                        u, svals, vt = np.linalg.svd(E, full_matrices=False)\n                        pc_dir = vt[0]\n                        pc_norm = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                        # scale along principal direction based on singular value and gscale\n                        pc_scale = (svals[0] + 1e-12) * gscale\n                        cand2 = elites.mean(axis=0) + pc_norm * pc_scale\n                        if self.rng.random() < 0.5:\n                            # use PCA candidate with small jitter\n                            cand = cand2 + self.rng.normal(0, 0.1 * pc_scale, size=self.dim)\n                    except Exception:\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.3:\n                    dvec = donors[0] - donors[1]\n                    F_scale = 0.4 + 0.8 * self.rng.random()\n                    cand = cand + F_scale * dvec * (0.3 + 0.7 * self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.12:\n                    scale = gscale * (0.8 + 1.2 * self.rng.random()) * mean_scale\n                    cand = cand + scale * c\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                # cand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)  # doesn't handle masked bounds easily\n                # do per-dim replacement\n                for j in np.where(bad)[0]:\n                    cand[j] = self.rng.uniform(lb[j], ub[j])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved_global = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            finite_vals = F_arr[np.isfinite(F_arr)] if F_arr.size > 0 else np.array([])\n            placed_top = False\n            if finite_vals.size > 0 and np.isfinite(f_cand):\n                sorted_vals = np.sort(np.concatenate([finite_vals, [f_cand]]))\n                rank = np.searchsorted(sorted_vals, f_cand, side='left') + 1\n                pct = rank / float(sorted_vals.size)\n                if pct <= 0.25:\n                    placed_top = True\n            elif np.isfinite(f_cand):\n                # if no earlier finite, any finite is considered success\n                placed_top = True\n\n            success = improved_global or placed_top\n            success_window.append(1 if success else 0)\n\n            # compute success rate\n            srate = (sum(success_window) / len(success_window)) if len(success_window) > 0 else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                # if many successes, modestly increase exploratory scale (move farther)\n                if srate > 0.25:\n                    gscale = min(10.0, gscale * (1.06 + 0.12 * self.rng.random()))\n                # if stagnating, increase scale to escape local minima\n                if srate < 0.04:\n                    gscale = min(10.0, gscale * (1.04 + 0.05 * self.rng.random()))\n                # otherwise slowly decay to refine\n                if 0.04 <= srate <= 0.25:\n                    gscale = max(1e-6, gscale * (0.98 + 0.03 * self.rng.random()))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max * 2:\n                F_arr2 = np.array(archive_F)\n                all_idx = np.arange(len(F_arr2))\n                fin_idx = np.where(np.isfinite(F_arr2))[0]\n                keep_idx = []\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort(F_arr2[fin_idx])]\n                    take = int(self.archive_max * 0.8)\n                    take = max(1, min(take, sorted_fin.size))\n                    keep_idx.extend(sorted_fin[:take].tolist())\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx.tolist()) - set(keep_idx))\n                if len(remaining) > 0:\n                    add = self.rng.choice(remaining, size=min(self.archive_max - len(keep_idx), len(remaining)), replace=False)\n                    keep_idx.extend([int(i) for i in add])\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(40, 6 * self.dim) or (iters > 100 and srate < 0.02):\n                # perform micro-restarts: inject a few diversified points\n                n_inject = min(6, max(2, 2 + self.dim // 5))\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.85:\n                        # around best (local diversified)\n                        scale = gscale * (1.2 + self.rng.random() * 2.0)\n                        candr = self.x_opt + self.rng.normal(0, scale, size=self.dim) * (0.5 * range_vec)\n                        candr = self._reflect_bounds(candr, lb, ub)\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.x_opt = candr.copy()\n                        self.f_opt = float(fr)\n                # encourage exploration after restart\n                success_window.clear()\n                stagn_count = 0\n                # slightly bump gscale to explore\n                gscale = min(10.0, gscale * (1.1 + 0.2 * self.rng.random()))\n\n        # final safety: pick best finite point from archive if x_opt still None or to ensure best\n        F_arr = np.array(archive_F)\n        if F_arr.size > 0:\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                if self.x_opt is None or F_arr[best_i] < self.f_opt:\n                    self.x_opt = np.array(archive_X[int(best_i)]).copy()\n                    self.f_opt = float(F_arr[best_i])\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            # fallback to center\n            self.x_opt = center.copy()\n            self.f_opt = safe_eval(self.x_opt)\n\n        return float(self.f_opt), np.asarray(self.x_opt).copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 37, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "error": "In the code, line 37, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "67c1c6da-eca1-4e19-a720-49857613d835", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian local refinement, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(max(50, archive_max))\n        # short-term adaptation scale (relative to per-dim interquartile range)\n        self.gscale = 0.5\n        self.rng = np.random.default_rng(seed)\n        # trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        default_lb = -5.0\n        default_ub = 5.0\n\n        lb = None\n        ub = None\n\n        # Common patterns: func.bounds = (lb, ub) or object with lb/ub or lower/upper\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            try:\n                # tuple/list pair\n                if isinstance(b, (list, tuple)) and len(b) >= 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # if it's an object with attributes\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or lb\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or ub\n            except Exception:\n                lb = lb\n                ub = ub\n\n        # direct attributes on func\n        for name in (\"lb\", \"lbounds\", \"lower\", \"xlower\", \"xmin\"):\n            if lb is None and hasattr(func, name):\n                try:\n                    lb = np.asarray(getattr(func, name), dtype=float)\n                except Exception:\n                    pass\n        for name in (\"ub\", \"ubounds\", \"upper\", \"xupper\", \"xmax\"):\n            if ub is None and hasattr(func, name):\n                try:\n                    ub = np.asarray(getattr(func, name), dtype=float)\n                except Exception:\n                    pass\n\n        if lb is None:\n            lb = np.full(self.dim, default_lb, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, default_ub, dtype=float)\n\n        lb = np.atleast_1d(np.array(lb, dtype=float))\n        ub = np.atleast_1d(np.array(ub, dtype=float))\n\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n\n        # pad/truncate\n        if lb.size < self.dim:\n            lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n        if ub.size < self.dim:\n            ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        if lb.size > self.dim:\n            lb = lb[:self.dim]\n        if ub.size > self.dim:\n            ub = ub[:self.dim]\n\n        # ensure feasible\n        ub = np.maximum(ub, lb + 1e-12)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds, do a few iterations to settle\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(8):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            x[low] = 2 * lb[low] - x[low]\n            x[high] = 2 * ub[high] - x[high]\n        # final clamp to handle any remaining numerical issues\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(max(1, n))\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.uniform(size=(n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            order = self.rng.permutation(n)\n            lhs[:, j] = cut[:-1] + u[order, j] * (cut[1] - cut[0])\n        lhs = np.clip(lhs, 0.0, 1.0)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        n = int(n)\n        if n == 1:\n            return self.rng.uniform(0.0, 1.0, size=self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.uniform(0.0, 1.0, size=(n, self.dim)) * (ub - lb) + lb\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            try:\n                fv = func(x)\n            except Exception:\n                fv = np.inf\n            try:\n                fv = float(fv)\n            except Exception:\n                fv = np.inf\n            self.evals += 1\n            return fv\n\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        center = 0.5 * (lb + ub)\n\n        # archive structures\n        archive_X = []\n        archive_F = []\n\n        # initialization: center + LHS + one uniform\n        n0 = max(2, min(self.init_samples, self.budget))\n        lhs01 = self._lhs01(n0, self.dim)\n        init_points = lb + lhs01 * range_vec  # shape (n0, dim)\n        uni = self._uniform_array(lb, ub, n=1)\n\n        stagn_count = 0\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = float(f_center)\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(init_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = init_points[i]\n            # skip duplicates near center\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.evals < self.budget:\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = float(f_uni)\n                self.x_opt = uni.copy()\n\n        # short-term adaptation and control variables\n        gscale = float(self.gscale)\n        success_window = deque(maxlen=30)\n\n        iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X_arr = np.array(archive_X) if len(archive_X) > 0 else np.empty((0, self.dim))\n            F_arr = np.array(archive_F) if len(archive_F) > 0 else np.empty((0,))\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n\n            # robust center (median of finite points) and per-dim scale (75-25 percentile)\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                median = np.median(Xf, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                per_dim_scale = np.maximum(1e-12, 0.5 * (q75 - q25))\n                mean_scale = float(np.mean(per_dim_scale) + 1e-12)\n            else:\n                Xf = np.empty((0, self.dim))\n                Ff = np.array([])\n                median = center.copy()\n                per_dim_scale = np.maximum(1e-8, 0.1 * range_vec)\n                mean_scale = float(np.mean(per_dim_scale))\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_idx.size > 0:\n                logits = - (Ff - np.min(Ff))\n                # prevent overflow\n                logits = logits - np.max(logits)\n                exp_logits = np.exp(np.clip(logits, -700, 700))\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = int(self.rng.choice(len(Ff), p=probs))\n                base = Xf[pick_idx].copy()\n            else:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if finite_idx.size >= 3:\n                sorted_idx = np.argsort(Ff)\n                pool_size = max(3, min(30, len(sorted_idx)))\n                pick_pool = sorted_idx[:pool_size]\n                # choose kdon in [2,4] depending on pool size\n                kdon = int(min(4, max(2, max(2, len(pick_pool) // 5))))\n                chosen = self.rng.choice(pick_pool, size=min(len(pick_pool), kdon + 2), replace=False)\n                donors = [Xf[int(i)].copy() for i in chosen]\n            elif len(archive_X) >= 2:\n                # fallback to random picks from archive\n                idxs = self.rng.choice(len(archive_X), size=min(len(archive_X), 3), replace=False)\n                donors = [np.asarray(archive_X[int(i)]).copy() for i in idxs]\n\n            # precompute some donor differences if available\n            dvec = np.zeros(self.dim)\n            dmove = np.zeros(self.dim)\n            if len(donors) >= 2:\n                # pick two distinct donors and one extra for 3-difference if available\n                i1, i2 = self.rng.choice(len(donors), size=2, replace=False)\n                dvec = donors[i1] - donors[i2]\n                if len(donors) >= 3:\n                    i3 = self.rng.choice(len(donors), size=1)[0]\n                    dmove = donors[i1] - donors[i3]\n                else:\n                    dmove = dvec.copy()\n                F_scale = 0.7 * (0.3 + 0.7 * self.rng.random())\n            else:\n                F_scale = 0.6 * (0.5 + 0.5 * self.rng.random())\n\n            # decide move type\n            p_global = 0.16  # probability of global jump\n            move_rand = self.rng.random()\n            c = self.rng.standard_cauchy(self.dim)  # tempered cauchy vector (unscaled)\n\n            if move_rand < p_global:\n                # GLOBAL move: DE-like + tempered Cauchy jumps anchored on an elite or uniform\n                if finite_idx.size > 0 and self.rng.random() < 0.9:\n                    n_elite = max(1, Xf.shape[0] // 2)\n                    elite_idx = np.argsort(Ff)[:n_elite]\n                    anchor = Xf[self.rng.choice(elite_idx)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n                cand = anchor.copy()\n                # if we have donors, do DE-like difference\n                if len(donors) >= 2:\n                    cand = cand + F_scale * dmove\n                # tempered Cauchy perturbation\n                scale = gscale * (0.6 + 0.9 * self.rng.random()) * mean_scale\n                cand = cand + scale * c\n                # occasional uniform injection of one coordinate\n                if self.rng.random() < 0.12:\n                    d = int(self.rng.integers(0, self.dim))\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                alpha = 0.0\n                if self.rng.random() < 0.35:\n                    alpha = 0.1 * (self.rng.random() + 0.5)  # small attraction factor\n                base_attracted = (1 - alpha) * base + alpha * median\n                # anisotropic gaussian step: per-dim sigma scaled by gscale and per_dim_scale\n                sigma = gscale * (0.6 + 0.8 * self.rng.random()) * per_dim_scale\n                sigma = np.maximum(sigma, 1e-12)\n                cand = base_attracted + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 6 and self.rng.random() < 0.20:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elites_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elites_idx]\n                    # center and SVD\n                    E = elites - np.mean(elites, axis=0)\n                    try:\n                        u, svals, vt = np.linalg.svd(E, full_matrices=False)\n                        pc_dir = vt[0]\n                        pc_norm = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                        # scale along principal direction based on singular value and gscale\n                        pc_scale = (svals[0] + 1e-12) * gscale\n                        cand2 = elites.mean(axis=0) + pc_norm * pc_scale\n                        # use PCA candidate with small jitter\n                        cand = cand2 + self.rng.normal(0, 0.1 * pc_scale, size=self.dim)\n                    except Exception:\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.3:\n                    cand = cand + F_scale * dvec * (0.3 + 0.7 * self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.12:\n                    scale = gscale * (0.8 + 1.2 * self.rng.random()) * mean_scale\n                    cand = cand + scale * c\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                for j in np.where(bad)[0]:\n                    cand[j] = self.rng.uniform(lb[j], ub[j])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved_global = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            finite_vals = F_arr[np.isfinite(F_arr)] if F_arr.size > 0 else np.array([])\n            placed_top = False\n            if finite_vals.size > 0 and np.isfinite(f_cand):\n                sorted_vals = np.sort(np.concatenate([finite_vals, [f_cand]]))\n                rank = np.searchsorted(sorted_vals, f_cand, side='left') + 1\n                pct = rank / float(sorted_vals.size)\n                if pct <= 0.25:\n                    placed_top = True\n            elif np.isfinite(f_cand) and finite_vals.size == 0:\n                # first finite value is a success\n                placed_top = True\n\n            success = improved_global or placed_top\n            success_window.append(1 if success else 0)\n\n            # compute success rate\n            srate = (sum(success_window) / len(success_window)) if len(success_window) > 0 else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                # if many successes, modestly increase exploratory scale (move farther)\n                if srate > 0.25:\n                    gscale = min(10.0, gscale * (1.04 + 0.05 * self.rng.random()))\n                # otherwise slowly decay to refine\n                if 0.04 <= srate <= 0.25:\n                    gscale = max(1e-6, gscale * (0.98 + 0.03 * self.rng.random()))\n                # if very low success, increase scale to escape\n                if srate < 0.04:\n                    gscale = min(20.0, gscale * (1.02 + 0.12 * self.rng.random()))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max * 2:\n                F_arr2 = np.array(archive_F)\n                all_idx = np.arange(len(F_arr2))\n                fin_idx2 = np.where(np.isfinite(F_arr2))[0]\n                keep_idx = []\n                if fin_idx2.size > 0:\n                    sorted_fin = fin_idx2[np.argsort(F_arr2[fin_idx2])]\n                    take = int(self.archive_max * 0.8)\n                    take = max(1, min(take, sorted_fin.size))\n                    keep_idx.extend(sorted_fin[:take].tolist())\n                remaining = [int(i) for i in all_idx if i not in keep_idx]\n                if len(remaining) > 0:\n                    add = self.rng.choice(remaining, size=min(self.archive_max - len(keep_idx), len(remaining)), replace=False)\n                    keep_idx.extend([int(i) for i in add])\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(40, 6 * self.dim) or (iters > 100 and srate < 0.02):\n                # perform micro-restarts: inject a few diversified points\n                n_inject = min(6, max(2, 2 + self.dim // 5))\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.85:\n                        # around best (local diversified)\n                        scale = gscale * (1.2 + self.rng.random() * 2.0)\n                        candr = self.x_opt + self.rng.normal(0, scale, size=self.dim) * (0.5 * range_vec)\n                        candr = self._reflect_bounds(candr, lb, ub)\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.x_opt = candr.copy()\n                        self.f_opt = float(fr)\n                # encourage exploration after restart\n                stagn_count = 0\n                gscale = min(10.0, gscale * (1.1 + 0.2 * self.rng.random()))\n\n        # final safety: pick best finite point from archive if x_opt still None or to ensure best\n        F_arr = np.array(archive_F)\n        if F_arr.size > 0:\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            if finite_idxs.size > 0:\n                best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n                if self.x_opt is None or F_arr[best_i] < self.f_opt:\n                    self.x_opt = np.array(archive_X[int(best_i)]).copy()\n                    self.f_opt = float(F_arr[best_i])\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            # fallback to center\n            self.x_opt = center.copy()\n            self.f_opt = safe_eval(self.x_opt)\n\n        return float(self.f_opt), np.asarray(self.x_opt).copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 197, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=30)", "error": "In the code, line 197, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=30)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7d235c30-b79b-4b5f-8fbb-ad28a6592e50", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian local search, tempered Cauchy escapes, short-term scale adaptation and micro-restarts — designed for robust black-box continuous optimization.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(archive_max)\n        # short-term adaptation scale (relative to per-dim interquartile range)\n        self.gscale = 0.5\n        self.rng = np.random.default_rng(seed)\n        # trackers\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Many BBOB functions use [-5,5], so default to that.\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n        # If func exposes bounds as attributes, try to use them (best-effort).\n        try:\n            if hasattr(func, \"bounds\"):\n                b = getattr(func, \"bounds\")\n                if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                    lb_tmp = np.asarray(b[0], dtype=float)\n                    ub_tmp = np.asarray(b[1], dtype=float)\n                    if lb_tmp.size in (1, self.dim) and ub_tmp.size in (1, self.dim):\n                        if lb_tmp.size == 1:\n                            lb = np.full(self.dim, float(lb_tmp.item()), dtype=float)\n                        else:\n                            lb[:lb_tmp.size] = lb_tmp[:self.dim]\n                        if ub_tmp.size == 1:\n                            ub = np.full(self.dim, float(ub_tmp.item()), dtype=float)\n                        else:\n                            ub[:ub_tmp.size] = ub_tmp[:self.dim]\n            # try direct attributes\n            for name in (\"lower\", \"lb\", \"xmin\", \"x_lower\"):\n                if hasattr(func, name):\n                    val = np.asarray(getattr(func, name), dtype=float)\n                    if val.size == 1:\n                        lb = np.full(self.dim, float(val.item()), dtype=float)\n                    else:\n                        lb[:min(val.size, self.dim)] = val[:self.dim]\n            for name in (\"upper\", \"ub\", \"xmax\", \"x_upper\"):\n                if hasattr(func, name):\n                    val = np.asarray(getattr(func, name), dtype=float)\n                    if val.size == 1:\n                        ub = np.full(self.dim, float(val.item()), dtype=float)\n                    else:\n                        ub[:min(val.size, self.dim)] = val[:self.dim]\n        except Exception:\n            # fallback to defaults\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # ensure ordering\n        ub = np.maximum(ub, lb + 1e-12)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds, do a few iterations to settle\n        x = np.array(x, dtype=float, copy=True)\n        for _ in range(4):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            x[low] = 2.0 * lb[low] - x[low]\n            x[high] = 2.0 * ub[high] - x[high]\n        # final clamp to handle any remaining numerical issues\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        dim = int(dim)\n        if n <= 0:\n            return np.empty((0, dim))\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.uniform(size=(n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            idx = self.rng.permutation(n)\n            lhs[:, j] = cut[idx] + u[:, j] * (1.0 / n)\n        lhs = np.clip(lhs, 0.0, 1.0 - 1e-12)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        n = int(n)\n        if n == 1:\n            return self.rng.uniform(0.0, 1.0, size=self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.uniform(0.0, 1.0, size=(n, self.dim)) * (ub - lb) + lb\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n\n        def safe_eval(x):\n            # x: array-like of length dim\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                fv = func(np.asarray(x, dtype=float))\n            except Exception:\n                fv = np.inf\n            self.evals += 1\n            return float(fv) if np.isfinite(fv) else np.inf\n\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n\n        # initialization: center + LHS + one uniform\n        n0 = max(1, self.init_samples)\n        lhs01 = self._lhs01(n0, self.dim)\n        init_points = lb + lhs01 * range_vec  # shape (n0, dim)\n        uni = self._uniform_array(lb, ub, n=1)\n        center = 0.5 * (lb + ub)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = float(f_center)\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(init_points.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = init_points[i].copy()\n            # skip exact duplicate to center\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.evals < self.budget:\n            f_uni = safe_eval(uni)\n            archive_X.append(uni.copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = float(f_uni)\n                self.x_opt = uni.copy()\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=30)\n\n        stagn_count = 0\n        iters = 0\n\n        # control hyper-parameters (tunable)\n        p_global = 0.28  # probability of global (bigger) moves\n        pool_size = max(6, int(0.25 * max(6, len(archive_X))))\n        kdon = 3  # donor count for DE-like moves\n        n_inject = max(4, int(0.2 * self.dim))\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X_arr = np.array(archive_X) if len(archive_X) > 0 else np.empty((0, self.dim))\n            F_arr = np.array(archive_F) if len(archive_F) > 0 else np.empty((0,))\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            finite_vals = F_arr[finite_idx] if finite_idx.size > 0 else np.array([])\n\n            # robust center (median of finite points) and per-dim scale (75-25 percentile)\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                median = np.median(Xf, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-12, q75 - q25)\n                mean_scale = float(np.mean(per_dim_scale) + 1e-12)\n            else:\n                Xf = np.empty((0, self.dim))\n                Ff = np.array([])\n                median = center.copy()\n                per_dim_scale = np.maximum(1e-8, 0.1 * range_vec)\n                mean_scale = float(np.mean(per_dim_scale))\n\n            # choose a base (elite-biased)\n            base = median.copy()\n            if finite_idx.size > 0:\n                # softmax on negative F (better = larger logits)\n                stdF = float(np.std(Ff))\n                if stdF < 1e-12:\n                    stdF = 1.0\n                logits = -(Ff - np.min(Ff)) / stdF\n                # stabilize\n                logits = logits - np.max(logits)\n                exp_logits = np.exp(logits)\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = self.rng.choice(len(Ff), p=probs)\n                base = Xf[int(pick_idx)].copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if finite_idx.size >= 3:\n                sorted_idx = np.argsort(Ff)\n                pick_pool = sorted_idx[:min(pool_size, sorted_idx.size)]\n                k = min(kdon, pick_pool.size)\n                # sample without replacement\n                chosen = self.rng.choice(pick_pool, size=k, replace=False)\n                donors = [Xf[int(i)].copy() for i in chosen]\n\n            # decide move type\n            move_rand = self.rng.random()\n\n            # start candidate from base\n            cand = base.copy()\n\n            # tempered Cauchy helper\n            def tempered_cauchy(scale):\n                # generator's standard_cauchy may produce huge tails; temper by logistic scaling\n                c = self.rng.standard_cauchy(size=self.dim)\n                # damp extreme values\n                c = np.tanh(0.8 * c)\n                return scale * c\n\n            # compute per-dim sigma for anisotropic gaussian moves\n            sigma = (self.gscale * (0.6 + 0.8 * self.rng.random()) * per_dim_scale)\n            sigma = np.maximum(sigma, 1e-12 * (range_vec + 1e-12))\n\n            if move_rand < p_global:\n                # GLOBAL move: anchored on an elite or a uniform (diversify)\n                if finite_idx.size > 0 and self.rng.random() < 0.75:\n                    # pick an elite anchor\n                    n_elite = max(1, Xf.shape[0] // 3)\n                    elite_idx = np.argsort(Ff)[:n_elite]\n                    anchor = Xf[self.rng.choice(elite_idx)]\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n                cand = anchor.copy()\n\n                # if we have donors, do DE-like difference\n                if len(donors) >= 2:\n                    dmove = np.zeros(self.dim)\n                    # combine differences\n                    for j in range(len(donors) - 1):\n                        dmove += donors[j] - donors[j + 1]\n                    dmove = dmove / max(1, (len(donors) - 1))\n                    F_scale = 0.8 * (0.3 + 0.7 * self.rng.random())\n                    cand = cand + F_scale * dmove\n\n                # tempered Cauchy perturbation for escape\n                c = tempered_cauchy(self.gscale * (0.6 + 0.9 * self.rng.random()) * mean_scale)\n                cand = cand + c\n\n                # occasional uniform injection of one coordinate\n                if self.rng.random() < 0.12:\n                    d = int(self.rng.integers(0, self.dim))\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n\n                # small Gaussian jitter\n                cand = cand + self.rng.normal(0.0, 0.05 * mean_scale, size=self.dim)\n\n            else:\n                # LOCAL move: anisotropic gaussian around base attracted to median sometimes\n                if self.rng.random() < 0.35:\n                    # small attraction towards median to exploit\n                    alpha = 0.1 * (self.rng.random() + 0.5)\n                    cand = cand + alpha * (median - cand)\n\n                # anisotropic gaussian step\n                anis = self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                cand = cand + anis\n\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 6 and self.rng.random() < 0.20:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elite_idx = np.argsort(Ff)[:num_elite]\n                    elites = Xf[elite_idx, :]\n                    E = elites - np.mean(elites, axis=0, keepdims=True)\n                    try:\n                        u, svals, vt = np.linalg.svd(E, full_matrices=False)\n                        # principal direction\n                        pc_dir = vt[0, :]\n                        pc_norm = pc_dir / (np.linalg.norm(pc_dir) + 1e-12)\n                        pc_scale = (svals[0] / (np.sum(svals) + 1e-12)) * mean_scale\n                        # make a candidate along principal axis with jitter\n                        cand_pca = np.mean(elites, axis=0) + pc_norm * (self.rng.normal(0.0, 0.8) * pc_scale * self.gscale)\n                        # blend with cand\n                        blend = 0.4 + 0.6 * self.rng.random()\n                        cand = blend * cand + (1.0 - blend) * cand_pca\n                        # small jitter orthogonal to PC\n                        cand = cand + self.rng.normal(0.0, 0.03 * mean_scale, size=self.dim)\n                    except Exception:\n                        pass\n\n                # DE-like donor mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.42:\n                    dvec = donors[0] - donors[1]\n                    F_scale = 0.4 + 0.8 * self.rng.random()\n                    cand = cand + F_scale * dvec * (0.3 + 0.7 * self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.12:\n                    c = tempered_cauchy(self.gscale * (0.8 + 1.2 * self.rng.random()) * mean_scale)\n                    cand = cand + c\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                for j in np.where(bad)[0]:\n                    cand[j] = self.rng.uniform(lb[j], ub[j])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved_global = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if finite_vals.size > 0 and np.isfinite(f_cand):\n                combined = np.concatenate([finite_vals, np.array([f_cand])])\n                sorted_vals = np.sort(combined)\n                rank = np.searchsorted(sorted_vals, f_cand, side='left') + 1\n                pct = rank / float(sorted_vals.size)\n                if pct <= 0.25:\n                    success = True\n            elif np.isfinite(f_cand):\n                # if no earlier finite, any finite is considered success\n                success = True\n\n            success_window.append(1 if success else 0)\n\n            # compute success rate\n            srate = (sum(success_window) / len(success_window)) if len(success_window) > 0 else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                if srate > 0.25:\n                    self.gscale = min(10.0, self.gscale * (1.06 + 0.12 * self.rng.random()))\n                if srate < 0.04:\n                    self.gscale = min(10.0, self.gscale * (1.04 + 0.05 * self.rng.random()))\n                if 0.04 <= srate <= 0.25:\n                    self.gscale = max(1e-6, self.gscale * (0.98 + 0.03 * self.rng.random()))\n\n            # prune archive to maintain memory budget occasionally\n            if len(archive_X) > int(1.5 * self.archive_max):\n                # keep best fraction and some random others\n                F_arr2 = np.array(archive_F)\n                all_idx = np.arange(len(archive_F))\n                fin_idx2 = np.where(np.isfinite(F_arr2))[0]\n                keep_idx = []\n                if fin_idx2.size > 0:\n                    sorted_fin = fin_idx2[np.argsort(F_arr2[fin_idx2])]\n                    take = int(self.archive_max * 0.75)\n                    take = max(1, min(take, sorted_fin.size))\n                    keep_idx.extend(sorted_fin[:take].tolist())\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx.tolist()) - set(keep_idx))\n                if len(remaining) > 0:\n                    add = self.rng.choice(remaining, size=min(self.archive_max - len(keep_idx), len(remaining)), replace=False)\n                    keep_idx.extend([int(i) for i in add])\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(40, 6 * self.dim) or (iters > 100 and srate < 0.02):\n                # perform micro-restarts: inject a few diversified points\n                for _ in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.6:\n                        # around best (local diversified)\n                        scale_local = 0.5 * range_vec * (0.05 + 0.5 * self.rng.random())\n                        candr = self.x_opt + self.rng.normal(0, 1.0, size=self.dim) * scale_local\n                        candr = self._reflect_bounds(candr, lb, ub)\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.x_opt = candr.copy()\n                        self.f_opt = float(fr)\n                # encourage exploration after restart\n                success_window.clear()\n                stagn_count = 0\n                self.gscale = min(10.0, self.gscale * (1.1 + 0.2 * self.rng.random()))\n\n        # final safety: pick best finite point from archive if x_opt still None or to ensure best\n        F_arr = np.array(archive_F) if len(archive_F) > 0 else np.array([])\n        X_arr = np.array(archive_X) if len(archive_X) > 0 else np.empty((0, self.dim))\n        finite_idxs = np.where(np.isfinite(F_arr))[0]\n        if F_arr.size > 0 and finite_idxs.size > 0:\n            best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n            if self.x_opt is None or F_arr[best_i] < self.f_opt:\n                self.f_opt = float(F_arr[best_i])\n                self.x_opt = X_arr[best_i].copy()\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            # fallback: sample best center-like point within budget remaining\n            cand = center.copy()\n            f_c = safe_eval(cand)\n            self.x_opt = cand.copy()\n            self.f_opt = float(f_c)\n\n        return float(self.f_opt), np.asarray(self.x_opt).copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 169, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=30)", "error": "In the code, line 169, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=30)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "893aed77-d636-4229-a8b9-625cd3d8cef3", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian local search, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(max(4, init_samples))\n        self.archive_max = int(archive_max)\n        # short-term adaptation scale (relative to per-dim interquartile range)\n        self.gscale = 0.5\n        # RNG\n        self.rng = np.random.default_rng(seed)\n        # trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        lb = None\n        ub = None\n        # Common patterns\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # if it's a pair (lb, ub)\n            if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n            else:\n                # object with .lb/.ub or .lower/.upper\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"xmin\", None) or getattr(b, \"xlower\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"xmax\", None) or getattr(b, \"xupper\", None)\n        # try name variants directly on func\n        if lb is None:\n            for name in (\"lb\", \"lower\", \"xmin\", \"xlower\"):\n                if hasattr(func, name):\n                    lb = np.asarray(getattr(func, name), dtype=float)\n                    break\n        if ub is None:\n            for name in (\"ub\", \"upper\", \"xmax\", \"xupper\"):\n                if hasattr(func, name):\n                    ub = np.asarray(getattr(func, name), dtype=float)\n                    break\n        # final fallback\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n        lb = np.atleast_1d(np.array(lb, dtype=float))\n        ub = np.atleast_1d(np.array(ub, dtype=float))\n        # Broadcast scalars to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n        # Truncate or pad if sizes differ\n        if lb.size > self.dim:\n            lb = lb[:self.dim]\n        elif lb.size < self.dim:\n            tmp = np.full(self.dim, lb[-1], dtype=float)\n            tmp[:lb.size] = lb\n            lb = tmp\n        if ub.size > self.dim:\n            ub = ub[:self.dim]\n        elif ub.size < self.dim:\n            tmp = np.full(self.dim, ub[-1], dtype=float)\n            tmp[:ub.size] = ub\n            ub = tmp\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds, do a few iterations to settle\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(8):\n            low = x < lb\n            high = x > ub\n            if not (np.any(low) or np.any(high)):\n                break\n            x[low] = 2 * lb[low] - x[low]\n            x[high] = 2 * ub[high] - x[high]\n        # final clamp to handle any remaining numerical issues\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.uniform(size=(n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            order = self.rng.permutation(n)\n            lhs[:, j] = cut[:n] + u[:, j] * (cut[1] - cut[0])\n            lhs[:, j] = np.clip(lhs[:, j], 0.0, 1.0)\n            # shuffle the column\n            lhs[:, j] = lhs[order, j]\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        n = int(n)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            out = np.empty((n, self.dim), dtype=float)\n            for i in range(n):\n                out[i, :] = self.rng.uniform(lb, ub)\n            return out\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n\n        def safe_eval(x):\n            # x: array-like of length dim\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                fv = float(func(x))\n            except Exception:\n                fv = np.inf\n            # count the evaluation attempt (successful or not)\n            self.evals += 1\n            return fv\n\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        center = (lb + ub) / 2.0\n\n        # initialization: center + LHS + one uniform\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        lhs01 = self._lhs01(n0, self.dim)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = float(f_center)\n                self.x_opt = center.copy()\n\n        # evaluate LHS points (map to bounds)\n        for i in range(lhs01.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = lb + lhs01[i] * (ub - lb)\n            # skip duplicates near center\n            if np.allclose(x, center, atol=1e-12):\n                continue\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.evals < self.budget:\n            uni = self._uniform_array(lb, ub, n=1)\n            f_uni = safe_eval(uni)\n            archive_X.append(np.asarray(uni).copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = float(f_uni)\n                self.x_opt = np.asarray(uni).copy()\n\n        # short-term adaptation and control variables\n        gscale = float(self.gscale)\n        iters = 0\n        stagn_count = 0\n        success_window = deque(maxlen=50)\n\n        # helper to compute per-dim IQR-based scale and robust median\n        def compute_stats():\n            X_arr = np.array(archive_X) if len(archive_X) > 0 else np.empty((0, self.dim))\n            F_arr = np.array(archive_F) if len(archive_F) > 0 else np.empty((0,))\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = np.maximum(1e-12, (q75 - q25))\n                median = np.median(Xf, axis=0)\n            else:\n                per_dim_scale = np.maximum(1e-12, range_vec / 6.0)\n                median = center.copy()\n            mean_scale = float(np.mean(per_dim_scale))\n            return X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median\n\n        # main loop\n        while self.evals < self.budget:\n            iters += 1\n            X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median = compute_stats()\n            finite_count = finite_idx.size\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_count > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx].astype(float)\n                # compute softmax over negative fitness (better = larger logit)\n                logits = -(Ff - np.nanmax(Ff))\n                exp_logits = np.exp(logits - np.max(logits))\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = self.rng.choice(np.arange(Xf.shape[0]), p=probs)\n                base = Xf[int(pick_idx)].copy()\n            else:\n                base = center.copy()\n                Xf = np.empty((0, self.dim))\n                Ff = np.array([])\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if finite_count >= 3:\n                sorted_idx = np.argsort(Ff)\n                pool_size = max(3, min(12, int(0.25 * finite_count) + 3))\n                pick_pool = sorted_idx[:pool_size]\n                kdon = min(4, max(2, len(pick_pool) // 3))\n                chosen = self.rng.choice(pick_pool, size=min(kdon, len(pick_pool)), replace=False)\n                donors = [Xf[int(i)].copy() for i in chosen]\n            elif len(archive_X) >= 2:\n                # fallback: random two from archive\n                idxs = self.rng.choice(len(archive_X), size=min(2, len(archive_X)), replace=False)\n                donors = [np.asarray(archive_X[int(i)]).copy() for i in idxs]\n\n            # choose move type\n            p_global = 0.28  # probability of a global move\n            is_global = self.rng.random() < p_global\n\n            # prepare tempered cauchy and scaling\n            c = self.rng.standard_cauchy(self.dim)\n            F_scale = (1.0 + self.rng.random()) * (0.6 * mean_scale + 0.05 * np.mean(range_vec))\n            cand = base.copy()\n\n            if is_global:\n                # GLOBAL move: anchor on elite or uniformly sample anchor\n                if finite_count >= 3 and self.rng.random() < 0.65:\n                    n_elite = max(1, int(0.07 * finite_count) + 1)\n                    elite_idx = np.argsort(Ff)[:n_elite]\n                    anchor = Xf[int(self.rng.choice(elite_idx))].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n                cand = anchor.copy()\n\n                # if we have donors, do DE-like difference from donors\n                if len(donors) >= 2:\n                    dmove = donors[0] - donors[1]\n                    cand = cand + F_scale * dmove * (0.8 + 0.6 * self.rng.random())\n\n                # tempered Cauchy perturbation to occasionally perform long jumps\n                scale = gscale * (0.8 + 1.2 * self.rng.random()) * mean_scale\n                cand = cand + scale * (c / (1.0 + np.abs(c)))  # tempered\n\n                # occasional coordinate injection\n                if self.rng.random() < 0.12:\n                    d = self.rng.integers(0, self.dim)\n                    cand[d] = self.rng.uniform(lb[d], ub[d])\n\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                alpha = 0.0\n                if self.rng.random() < 0.35:\n                    alpha = 0.1 * (self.rng.random() + 0.5)  # small attraction factor\n                cand = (1.0 - alpha) * base + alpha * median\n\n                # anisotropic gaussian step: per-dim sigma scaled by gscale and per_dim_scale\n                sigma = gscale * (0.6 + 0.8 * self.rng.random()) * per_dim_scale\n                # ensure not too small\n                sigma = np.maximum(sigma, 1e-12 * (ub - lb + 1.0))\n\n                cand = cand + self.rng.normal(0.0, sigma)\n\n                # PCA-guided elite perturbation occasionally\n                if Xf.shape[0] >= 6 and self.rng.random() < 0.22:\n                    num_elite = max(3, int(0.25 * Xf.shape[0]))\n                    elites_idx = np.argsort(Ff)[:num_elite]\n                    E = Xf[elites_idx, :]\n                    try:\n                        # center E\n                        E0 = E - np.mean(E, axis=0)\n                        u, svals, vt = np.linalg.svd(E0, full_matrices=False)\n                        pc_dir = vt[0]\n                        pc_scale = svals[0] / (np.sum(svals) + 1e-12) * mean_scale * (1.0 + self.rng.random())\n                        # move along principal component\n                        if self.rng.random() < 0.6:\n                            cand = cand + pc_scale * pc_dir * (0.6 + 1.2 * self.rng.random())\n                        else:\n                            # opposite direction sometimes\n                            cand = cand - 0.5 * pc_scale * pc_dir * (0.6 + 1.2 * self.rng.random())\n                    except Exception:\n                        pass\n\n                # small DE-like mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.28:\n                    dvec = donors[0] - donors[1]\n                    cand = cand + 0.4 * F_scale * dvec * (0.3 + 0.7 * self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.10:\n                    scale = gscale * (0.4 + 1.0 * self.rng.random()) * mean_scale\n                    cand = cand + scale * (c / (1.0 + np.abs(c)))\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                for j in np.where(bad)[0]:\n                    cand[j] = self.rng.uniform(lb[j], ub[j])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best trackers and stagnation\n            improved_global = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.x_opt = cand.copy()\n                self.f_opt = float(f_cand)\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            finite_vals = F_arr[np.isfinite(F_arr)] if F_arr.size > 0 else np.array([])\n            if np.isfinite(f_cand):\n                if improved_global:\n                    success = True\n                elif finite_vals.size > 0:\n                    # determine rank\n                    combined = np.concatenate([finite_vals, [f_cand]])\n                    rank = np.searchsorted(np.sort(combined), f_cand, side='left')\n                    # placed among top 25% (1-based)\n                    if rank < max(1, int(0.25 * combined.size)):\n                        success = True\n                else:\n                    success = True\n\n            success_window.append(1 if success else 0)\n\n            # compute success rate\n            srate = (sum(success_window) / len(success_window)) if len(success_window) > 0 else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 5 == 0:\n                if srate > 0.35:\n                    # many successes, increase exploratory scale\n                    gscale = min(20.0, gscale * (1.04 + 0.08 * self.rng.random()))\n                elif srate < 0.06:\n                    # too few successes -> increase to escape\n                    gscale = min(20.0, gscale * (1.06 + 0.12 * self.rng.random()))\n                # otherwise slowly decay to refine\n                if 0.06 <= srate <= 0.25:\n                    gscale = max(1e-8, gscale * (0.98 + 0.03 * self.rng.random()))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max * 2:\n                F_arr2 = np.array(archive_F)\n                all_idx = np.arange(len(F_arr2))\n                fin_idx = np.where(np.isfinite(F_arr2))[0]\n                keep_idx = []\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort(F_arr2[fin_idx])]\n                    take = max(1, min(self.archive_max // 2, sorted_fin.size))\n                    keep_idx.extend(sorted_fin[:take].tolist())\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx) - set(keep_idx))\n                if len(remaining) > 0:\n                    add = self.rng.choice(remaining, size=min(self.archive_max - len(keep_idx), len(remaining)), replace=False)\n                    keep_idx.extend(list(add))\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(40, 6 * self.dim) or (iters > 100 and srate < 0.02):\n                # micro-restarts: inject a few diversified points around best and random\n                n_restart = max(3, min(12, 2 + self.dim // 2))\n                for k in range(n_restart):\n                    if self.x_opt is not None and self.rng.random() < 0.7:\n                        # local diversified around best\n                        jitter = self.rng.normal(0.0, gscale * 0.6 * mean_scale, size=self.dim)\n                        candr = self.x_opt + jitter\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1)\n                    candr = self._reflect_bounds(candr, lb, ub)\n                    if self.evals >= self.budget:\n                        break\n                    fr = safe_eval(candr)\n                    archive_X.append(np.asarray(candr).copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and (self.x_opt is None or fr < self.f_opt):\n                        self.x_opt = np.asarray(candr).copy()\n                        self.f_opt = float(fr)\n                # encourage exploration after restart\n                success_window.clear()\n                stagn_count = 0\n                gscale = min(20.0, gscale * (1.1 + 0.2 * self.rng.random()))\n\n        # final safety: pick best finite point from archive if needed\n        F_arr = np.array(archive_F)\n        finite_idxs = np.where(np.isfinite(F_arr))[0]\n        if finite_idxs.size > 0:\n            best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n            if self.x_opt is None or F_arr[best_i] < self.f_opt:\n                self.x_opt = np.array(archive_X[int(best_i)]).copy()\n                self.f_opt = float(F_arr[best_i])\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            # fallback to center and evaluate if possible\n            self.x_opt = center.copy()\n            if self.evals < self.budget:\n                self.f_opt = safe_eval(self.x_opt)\n            else:\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt).copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 39, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"xmin\", None) or getattr(b, \"xlower\", None)", "error": "In the code, line 39, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"xmin\", None) or getattr(b, \"xlower\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5e6da7b8-e800-4041-bb39-093daf370085", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian local search, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n        # short-term adaptation scale (relative to per-dim robust scale)\n        self.gscale = 1.0\n        # internal trackers filled in __call__\n        self.evals = 0\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        lb = None\n        ub = None\n        # Common pattern: func.bounds -> (lb, ub) or object with .lb/.ub\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # tuple/list/np.ndarray of length 2\n            if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                lb_cand, ub_cand = b\n                try:\n                    lb = np.asarray(lb_cand, dtype=float)\n                    ub = np.asarray(ub_cand, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n        # try attributes directly\n        if lb is None:\n            for name in (\"lb\", \"lower\", \"xmin\", \"xlower\"):\n                v = getattr(func, name, None)\n                if v is not None:\n                    lb = np.asarray(v, dtype=float)\n                    break\n        if ub is None:\n            for name in (\"ub\", \"upper\", \"xmax\", \"xupper\"):\n                v = getattr(func, name, None)\n                if v is not None:\n                    ub = np.asarray(v, dtype=float)\n                    break\n        # fallback to scalar bounds [-5,5]\n        if lb is None: lb = -5.0\n        if ub is None: ub = 5.0\n        lb = np.atleast_1d(np.array(lb, dtype=float))\n        ub = np.atleast_1d(np.array(ub, dtype=float))\n        # Broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n        # Ensure correct length\n        if lb.size < self.dim:\n            tmp = np.full(self.dim, lb[-1], dtype=float)\n            tmp[:lb.size] = lb\n            lb = tmp\n        if ub.size < self.dim:\n            tmp = np.full(self.dim, ub[-1], dtype=float)\n            tmp[:ub.size] = ub\n            ub = tmp\n        # final safety\n        lb = lb[:self.dim].astype(float)\n        ub = ub[:self.dim].astype(float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds: repeatedly reflect until within bounds\n        x = np.asarray(x, dtype=float)\n        for _ in range(8):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            x[low] = 2 * lb[low] - x[low]\n            x[high] = 2 * ub[high] - x[high]\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = max(1, int(n))\n        dim = int(dim)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.random((n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            order = self.rng.permutation(n)\n            for i in range(n):\n                a = cut[i]\n                b = cut[i + 1]\n                lhs[i, j] = a + u[i, j] * (b - a)\n            lhs[:, j] = lhs[order, j]\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            out = np.empty((int(n), self.dim), dtype=float)\n            for i in range(int(n)):\n                out[i, :] = self.rng.uniform(lb, ub)\n            return out\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def safe_eval(x):\n            # x: array-like of length dim\n            if self.evals >= self.budget:\n                return np.inf\n            self.evals += 1\n            try:\n                fv = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                fv = np.inf\n            return fv\n\n        lb, ub = self._get_bounds(func)\n        center = (lb + ub) / 2.0\n\n        # initialization: center + LHS + one uniform\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        lhs01 = self._lhs01(n0, self.dim)\n        uni = self._uniform_array(lb, ub, n=1)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points (map to bounds)\n        for i in range(lhs01.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = lb + lhs01[i] * (ub - lb)\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.evals < self.budget:\n            u = uni if uni.ndim == 1 else uni[0]\n            f_uni = safe_eval(u)\n            archive_X.append(np.asarray(u).copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = f_uni\n                self.x_opt = np.asarray(u).copy()\n\n        # short-term adaptation and control variables\n        gscale = float(self.gscale)\n        success_window = deque(maxlen=50)\n        stagn_count = 0\n        stagn_limit = max(50, 10 * self.dim)\n\n        # helper to compute per-dim robust scale and median\n        def compute_stats():\n            F_arr = np.array(archive_F, dtype=float) if len(archive_F) > 0 else np.empty((0,))\n            X_arr = np.array(archive_X, dtype=float) if len(archive_X) > 0 else np.empty((0, self.dim))\n            finite_idx = np.where(np.isfinite(F_arr))[0] if F_arr.size > 0 else np.array([], dtype=int)\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                median = np.median(Xf, axis=0)\n                # robust scale: IQR -> approximate sigma = IQR / 1.349; but avoid zeros\n                per_dim_scale = np.maximum(1e-12, (q75 - q25) / 1.349)\n                mean_scale = float(np.mean(per_dim_scale))\n            else:\n                # fallback to full search-range based scale\n                per_dim_scale = np.maximum(1e-12, (ub - lb) / 6.0)\n                median = center.copy()\n                mean_scale = float(np.mean(per_dim_scale))\n            return X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median\n\n        # main loop\n        while self.evals < self.budget:\n            X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median = compute_stats()\n            finite_count = finite_idx.size if finite_idx is not None else 0\n\n            # choose base biased towards better archive items using softmax on negative F\n            base = None\n            if finite_count > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                logits = -Ff\n                logits = logits - np.max(logits)  # stabilize\n                exp_logits = np.exp(logits)\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = self.rng.choice(np.arange(Xf.shape[0]), p=probs)\n                base = Xf[int(pick_idx)].copy()\n            else:\n                base = self._uniform_array(lb, ub, n=1).copy()\n\n            # small chance to use center instead\n            if self.rng.random() < 0.08:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if finite_count >= 3:\n                # choose 3 distinct donors among best half-of-finite\n                sorted_idx = np.argsort(Ff)\n                topk = max(2, int(0.5 * sorted_idx.size))\n                pool = sorted_idx[:topk]\n                if pool.size >= 2:\n                    choices = self.rng.choice(pool, size=min(3, pool.size), replace=False)\n                    donors = [Xf[int(i)].copy() for i in choices]\n            elif len(archive_X) >= 2:\n                # fallback: random two from archive\n                idxs = self.rng.choice(len(archive_X), size=min(2, len(archive_X)), replace=False)\n                donors = [np.asarray(archive_X[int(i)]).copy() for i in idxs]\n\n            # choose move type: global vs local\n            p_global = 0.18 + 0.02 * min(1.0, max(0.0, (self.f_opt - np.min(F_arr[np.isfinite(F_arr)])) / (1.0 + abs(self.f_opt)) if F_arr.size>0 else 0.0))\n            is_global = self.rng.random() < p_global\n\n            # prepare tempered cauchy and scaling\n            c = self.rng.standard_cauchy(self.dim)\n            # temper extremes\n            tau = 6.0 + 2.0 * self.rng.random()\n            c = c / (1.0 + np.abs(c) / tau)\n\n            if is_global:\n                # GLOBAL move: anchor on elite or uniformly sample anchor\n                if finite_count > 0 and self.rng.random() < 0.7:\n                    # pick one elite anchor from best 10% (or top 1 if few)\n                    Xf = X_arr[finite_idx, :]\n                    Ff = F_arr[finite_idx]\n                    order = np.argsort(Ff)\n                    take = max(1, int(0.10 * order.size))\n                    pick = order[self.rng.integers(0, take)]\n                    anchor = Xf[int(pick)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1).copy()\n                # make a cauchy jump scaled by per-dim scale and gscale\n                scale_vec = per_dim_scale * (0.9 + 1.6 * self.rng.random(self.dim))\n                cand = anchor + gscale * (c * scale_vec)\n                # occasional DE-like differential from donors\n                if len(donors) >= 2 and self.rng.random() < 0.4:\n                    dvec = donors[0] - donors[-1]\n                    F_scale = 0.6 * (1.0 + self.rng.random())\n                    cand = cand + F_scale * dvec * (0.4 + 0.8 * self.rng.random())\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.18:\n                    mask = self.rng.random(self.dim) < 0.18\n                    if mask.any():\n                        u = self._uniform_array(lb, ub, n=1)\n                        cand[mask] = u[mask]\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                alpha = 0.0\n                if self.rng.random() < 0.35:\n                    alpha = 0.12 * (self.rng.random() + 0.5)  # small attraction factor\n                cand = base.copy()\n                # anisotropic gaussian step: per-dim sigma scaled by gscale and per_dim_scale\n                sig = gscale * per_dim_scale * (0.6 + 1.2 * self.rng.random(self.dim))\n                step = self.rng.normal(0.0, 1.0, size=self.dim) * sig\n                cand = cand + step\n                # attract toward median occasionally\n                if alpha > 0.0:\n                    cand = cand * (1.0 - alpha) + median * alpha\n                # PCA-guided elite perturbation occasionally\n                if finite_count >= 4 and self.rng.random() < 0.22:\n                    # form elite set (top 8 or half)\n                    Xf = X_arr[finite_idx, :]\n                    Ff = F_arr[finite_idx]\n                    order = np.argsort(Ff)\n                    take = max(3, min(order.size, 8))\n                    elites_idx = order[:take]\n                    E = Xf[elites_idx, :]\n                    # center E\n                    E0 = E - np.mean(E, axis=0)\n                    try:\n                        # SVD to get principal directions\n                        u, svals, vt = np.linalg.svd(E0, full_matrices=False)\n                        pc_dir = vt[0]\n                        # scale along pc is proportional to largest singular value\n                        pc_scale = (svals[0] / (np.sum(svals) + 1e-12)) * mean_scale * (0.6 + 1.4 * self.rng.random())\n                        # move along principal component (both directions randomly)\n                        if self.rng.random() < 0.6:\n                            cand = cand + pc_scale * pc_dir * (0.6 + 1.25 * self.rng.random())\n                        else:\n                            cand = cand - 0.6 * pc_scale * pc_dir * (0.6 + 1.25 * self.rng.random())\n                    except Exception:\n                        pass\n                # small DE-like mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.28:\n                    dvec = donors[0] - donors[-1]\n                    F_scale = 0.4 * (1.0 + 0.8 * self.rng.random())\n                    cand = cand + 0.4 * F_scale * dvec * (0.3 + 0.7 * self.rng.random())\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.10:\n                    c_small = self.rng.standard_cauchy(self.dim)\n                    c_small = c_small / (1.0 + np.abs(c_small) / (4.0 + 2.0 * self.rng.random()))\n                    cand = cand + 0.6 * gscale * c_small * per_dim_scale * (0.5 + self.rng.random(self.dim))\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                for j in np.where(bad)[0]:\n                    cand[j] = self.rng.uniform(lb[j], ub[j])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            improved_global = False\n            if np.isfinite(f_cand) and (self.x_opt is None or f_cand < self.f_opt):\n                self.x_opt = cand.copy()\n                self.f_opt = f_cand\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            finite_vals = F_arr[np.isfinite(F_arr)] if F_arr.size > 0 else np.array([])\n            if improved_global:\n                success = True\n            elif np.isfinite(f_cand) and finite_vals.size > 0:\n                combined = np.concatenate([finite_vals, np.array([f_cand])])\n                rank = np.searchsorted(np.sort(combined), f_cand, side='left')\n                if rank < max(1, int(0.25 * combined.size)):\n                    success = True\n            else:\n                # consider finite unknowns as mild success\n                if np.isfinite(f_cand):\n                    success = True\n\n            success_window.append(1 if success else 0)\n\n            # compute success rate\n            srate = (sum(success_window) / len(success_window)) if len(success_window) > 0 else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if len(success_window) == success_window.maxlen:\n                if srate > 0.45:\n                    # many successes, increase exploratory scale\n                    gscale = min(50.0, gscale * (1.03 + 0.06 * self.rng.random()))\n                elif srate < 0.06:\n                    # too few successes -> increase to escape\n                    gscale = min(50.0, gscale * (1.05 + 0.10 * self.rng.random()))\n                else:\n                    # otherwise slowly decay to refine\n                    gscale = max(1e-8, gscale * (0.985 + 0.03 * self.rng.random()))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max * 2:\n                F_arr2 = np.array(archive_F, dtype=float)\n                all_idx = np.arange(len(F_arr2))\n                fin_idx = np.where(np.isfinite(F_arr2))[0]\n                keep_idx = []\n                # keep best half of finite\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort(F_arr2[fin_idx])]\n                    take = max(1, min(self.archive_max // 2, sorted_fin.size))\n                    keep_idx.extend(list(sorted_fin[:take]))\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx) - set(keep_idx))\n                if len(remaining) > 0:\n                    add = self.rng.choice(remaining, size=min(self.archive_max - len(keep_idx), len(remaining)), replace=False)\n                    keep_idx.extend(list(add))\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > stagn_limit:\n                # micro-restarts: inject a few diversified points around best and random\n                n_restart = max(3, min(12, 2 + self.dim // 2))\n                for k in range(n_restart):\n                    if self.x_opt is not None and self.rng.random() < 0.75:\n                        # local diversified around best\n                        jitter = self.rng.normal(0.0, gscale * 0.8 * per_dim_scale, size=self.dim)\n                        candr = self.x_opt + jitter\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1).copy()\n                    candr = self._reflect_bounds(candr, lb, ub)\n                    if self.evals >= self.budget:\n                        break\n                    fr = safe_eval(candr)\n                    archive_X.append(np.asarray(candr).copy())\n                    archive_F.append(fr)\n                    if np.isfinite(fr) and (self.x_opt is None or fr < self.f_opt):\n                        self.x_opt = np.asarray(candr).copy()\n                        self.f_opt = fr\n                # encourage exploration after restart\n                success_window.clear()\n                stagn_count = 0\n                gscale = min(50.0, gscale * (1.08 + 0.18 * self.rng.random()))\n\n        # final safety: pick best finite point from archive if needed\n        F_arr = np.array(archive_F, dtype=float)\n        finite_idxs = np.where(np.isfinite(F_arr))[0]\n        if finite_idxs.size > 0:\n            best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n            if self.x_opt is None or F_arr[best_i] < self.f_opt:\n                self.x_opt = np.array(archive_X[int(best_i)]).copy()\n                self.f_opt = float(F_arr[best_i])\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            # fallback to center and evaluate if possible\n            if self.evals < self.budget:\n                f_center = safe_eval(center)\n                self.x_opt = center.copy()\n                self.f_opt = f_center\n            else:\n                self.f_opt = np.inf\n                self.x_opt = self._uniform_array(lb, ub, n=1).copy()\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 62, in _get_bounds, the following error occurred:\nTypeError: only 0-dimensional arrays can be converted to Python scalars\nOn line: lb = np.full(self.dim, float(lb), dtype=float)", "error": "In the code, line 62, in _get_bounds, the following error occurred:\nTypeError: only 0-dimensional arrays can be converted to Python scalars\nOn line: lb = np.full(self.dim, float(lb), dtype=float)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8b3b4ea5-5869-4e03-b651-17f2f4fb57d5", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian local search, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # short-term adaptation scale (relative to per-dim robust scale)\n        self.gscale = 1.0\n        # internal\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]\n        lb = None\n        ub = None\n        # common patterns\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # could be (lb, ub) or list of (lb, ub)\n            try:\n                if isinstance(b, tuple) or isinstance(b, list):\n                    if len(b) == 2 and (np.shape(b[0]) == () or np.shape(b[0]) == (self.dim,)):\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n        # try attributes\n        if lb is None:\n            for name in (\"lb\", \"lower\", \"xmin\", \"xlower\"):\n                v = getattr(func, name, None)\n                if v is not None:\n                    lb = np.asarray(v, dtype=float)\n                    break\n            for name in (\"ub\", \"upper\", \"xmax\", \"xupper\"):\n                v = getattr(func, name, None)\n                if v is not None:\n                    ub = np.asarray(v, dtype=float)\n                    break\n        # if single scalars given, broadcast\n        if lb is None and ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n            return lb, ub\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n        if ub is None:\n            ub = np.full(self.dim, 5.0, dtype=float)\n        lb = np.atleast_1d(np.asarray(lb, dtype=float))\n        ub = np.atleast_1d(np.asarray(ub, dtype=float))\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n        # if length mismatch, try to broadcast or clip\n        if lb.size < self.dim:\n            lb = np.concatenate([lb, np.full(self.dim - lb.size, lb[-1])])\n        if ub.size < self.dim:\n            ub = np.concatenate([ub, np.full(self.dim - ub.size, ub[-1])])\n        lb = lb[:self.dim].astype(float)\n        ub = ub[:self.dim].astype(float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.asarray(x, dtype=float)\n        # reflect coordinates that go beyond bounds: repeatedly reflect until within bounds\n        for _ in range(8):\n            high = x > ub\n            if not np.any(high):\n                break\n            x[high] = 2 * ub[high] - x[high]\n            low = x < lb\n            if np.any(low):\n                x[low] = 2 * lb[low] - x[low]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = max(1, int(n))\n        dim = int(dim)\n        cut = np.linspace(0, 1, n + 1)\n        u = self.rng.rand(n, dim)\n        lhs = np.zeros((n, dim))\n        for j in range(dim):\n            order = self.rng.permutation(n)\n            for i in range(n):\n                a = cut[i]\n                b = cut[i + 1]\n                lhs[order[i], j] = a + (b - a) * u[i, j]\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return (lb + self.rng.rand(self.dim) * (ub - lb)).astype(float)\n        out = lb + self.rng.rand(int(n), self.dim) * (ub - lb)\n        return out\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        archive_X = []\n        archive_F = []\n\n        def safe_eval(x):\n            # x: array-like of length dim\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                xv = np.asarray(x, dtype=float)\n                fval = float(func(xv))\n            except Exception:\n                fval = np.inf\n            self.evals += 1\n            # keep in archive (caller may append duplicate - it's fine)\n            return fval\n\n        # initialization: center + LHS + one uniform\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # LHS samples\n        lhs01 = self._lhs01(n0, self.dim)\n        for i in range(n0):\n            if self.evals >= self.budget:\n                break\n            x = lb + lhs01[i] * (ub - lb)\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.evals < self.budget:\n            u = self._uniform_array(lb, ub, n=1)\n            f_uni = safe_eval(u)\n            archive_X.append(np.asarray(u).copy())\n            archive_F.append(f_uni)\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.f_opt = f_uni\n                self.x_opt = np.asarray(u).copy()\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=50)\n        stagn_count = 0\n\n        # helper to compute per-dim robust scale and median\n        def compute_stats():\n            if len(archive_X) == 0:\n                X_arr = np.empty((0, self.dim))\n            else:\n                X_arr = np.array(archive_X, dtype=float)\n            if len(archive_F) == 0:\n                F_arr = np.array([], dtype=float)\n            else:\n                F_arr = np.array(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0] if F_arr.size > 0 else np.array([], dtype=int)\n            # robust per-dim spread from finite samples\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                per_dim_scale = np.maximum(1e-12, (q75 - q25) / 1.349)\n                mean_scale = float(np.mean(per_dim_scale))\n                median = np.median(Xf, axis=0)\n            else:\n                # fallback to full search-range based scale\n                per_dim_scale = np.maximum(1e-12, (ub - lb) / 6.0)\n                mean_scale = float(np.mean(per_dim_scale))\n                median = 0.5 * (lb + ub)\n            # safety: if scale zeros, use fraction of range\n            per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-3)\n            return X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median\n\n        # main loop\n        while self.evals < self.budget:\n            X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median = compute_stats()\n            finite_count = finite_idx.size if finite_idx is not None else 0\n\n            # choose base biased towards better archive items using softmax on negative F\n            base = None\n            if finite_count > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                # temperature adaptively from mean_scale (bigger scale -> more exploration)\n                beta = max(1e-6, 1.0 / (1.0 + mean_scale))\n                # stabilize\n                minF = np.min(Ff)\n                logits = -beta * (Ff - minF)\n                ex = np.exp(logits - np.max(logits))\n                probs = ex / np.sum(ex)\n                pick_idx = self.rng.choice(np.arange(Xf.shape[0]), p=probs)\n                base = Xf[int(pick_idx)].copy()\n            else:\n                base = center.copy()\n\n            # small chance to use center instead\n            if self.rng.random() < 0.08:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = None\n            if finite_count >= 3:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                sorted_idx = np.argsort(Ff)\n                topk = max(3, int(0.5 * sorted_idx.size))\n                pool = sorted_idx[:topk]\n                if pool.size >= 3:\n                    choices = self.rng.choice(pool, size=3, replace=False)\n                    donors = [Xf[int(i)].copy() for i in choices]\n            # fallback: pick random archive members (may include infinities)\n            if donors is None and len(archive_X) >= 3:\n                idxs = self.rng.choice(len(archive_X), size=3, replace=False)\n                donors = [np.asarray(archive_X[int(i)], dtype=float).copy() for i in idxs]\n\n            # choose move type: global vs local\n            p_global = 0.28 + 0.12 * (self.rng.rand())  # slightly variable\n            is_global = self.rng.random() < p_global\n\n            # prepare candidate\n            cand = base.copy()\n\n            # occasionally perform a DE-like differential perturbation based on donors\n            if donors is not None and self.rng.random() < 0.22:\n                dvec = donors[0] - donors[1]\n                F_scale = 0.8 * (0.5 + self.rng.random())\n                cand = cand + 0.6 * F_scale * dvec * (0.4 + 0.7 * self.rng.random())\n\n            if is_global:\n                # GLOBAL move: anchor on elite or uniformly sample anchor then tempered cauchy jump\n                if finite_count > 0 and self.rng.random() < 0.7:\n                    Ff = F_arr[finite_idx]\n                    order = np.argsort(Ff)\n                    take = max(1, int(0.10 * order.size))\n                    pick = order[self.rng.randint(0, take)]\n                    anchor = Xf[int(pick)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n                # tempered Cauchy: scale per-dim and temper extremes\n                scale_vec = per_dim_scale * self.gscale * (0.9 + 1.6 * self.rng.rand(self.dim))\n                # sample cauchy but temper by multiplying with a bounded factor\n                cauch = self.rng.standard_cauchy(self.dim)\n                # temper extremes by tanh-like bound\n                temper = np.tanh(cauch / (1.0 + mean_scale))\n                cand = anchor + scale_vec * temper * (1.0 + 2.0 * self.rng.rand(self.dim))\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.12:\n                    mask = self.rng.rand(self.dim) < 0.12\n                    if mask.any():\n                        u = self._uniform_array(lb, ub, n=1)\n                        cand[mask] = u[mask]\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                alpha = 0.0\n                if self.rng.random() < 0.35:\n                    alpha = 0.12 * (self.rng.random() + 0.5)  # small attraction factor\n                # anisotropic gaussian step: per-dim sigma scaled by gscale and per_dim_scale\n                sig = per_dim_scale * self.gscale * (0.6 + 1.2 * self.rng.rand(self.dim))\n                step = self.rng.normal(0.0, 1.0, size=self.dim) * sig\n                cand = base.copy() + step\n                # attract toward median occasionally\n                if alpha > 0.0:\n                    cand = cand * (1.0 - alpha) + median * alpha\n                # PCA-guided elite perturbation occasionally\n                if finite_count >= 4 and self.rng.random() < 0.22:\n                    Xf = X_arr[finite_idx, :]\n                    Ff = F_arr[finite_idx]\n                    order = np.argsort(Ff)\n                    take = min(8, max(4, int(0.5 * order.size)))\n                    elite_idx = order[:take]\n                    E = Xf[elite_idx, :]\n                    E0 = E - np.mean(E, axis=0, keepdims=True)\n                    try:\n                        u, svals, vt = np.linalg.svd(E0, full_matrices=False)\n                        pc_dir = vt[0]\n                        pc_scale = svals[0] if svals.size > 0 else mean_scale\n                        # move along principal component (randomly directioned)\n                        dir_sign = 1 if self.rng.rand() < 0.5 else -1\n                        mag = self.gscale * pc_scale * (0.4 + 1.8 * self.rng.rand())\n                        cand = cand + dir_sign * mag * pc_dir\n                    except Exception:\n                        pass\n                # small DE-like mixing inside local branch sometimes\n                if donors is not None and self.rng.random() < 0.18:\n                    dvec = donors[0] - donors[2]\n                    F_scale = 0.4 * (1.0 + 0.8 * self.rng.random())\n                    cand = cand + 0.4 * F_scale * dvec * (0.3 + 0.7 * self.rng.random())\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.06:\n                    cauch = self.rng.standard_cauchy(self.dim)\n                    cand = cand + per_dim_scale * self.gscale * np.tanh(cauch) * (1.0 + self.rng.rand(self.dim))\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                for j in np.where(bad)[0]:\n                    cand[j] = self._uniform_array(lb[j:j+1], ub[j:j+1], n=1)[0]\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            improved_global = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved_global = True\n                stagn_count = 0\n            else:\n                stagn_count += 1\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            finite_vals = np.array([archive_F[i] for i in range(len(archive_F)) if np.isfinite(archive_F[i])], dtype=float)\n            if np.isfinite(f_cand) and finite_vals.size > 0:\n                combined = np.concatenate([finite_vals, np.array([f_cand])])\n                rank = np.sum(combined <= f_cand) - 1\n                if rank < max(1, int(0.25 * combined.size)):\n                    success = True\n            elif np.isfinite(f_cand):\n                success = True  # mild success when few finite exist\n\n            success_window.append(1 if success else 0)\n\n            # compute success rate\n            srate = (sum(success_window) / len(success_window)) if len(success_window) > 0 else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if len(success_window) == success_window.maxlen:\n                if srate > 0.45:\n                    # many successes, increase exploratory scale moderately\n                    self.gscale = min(8.0, self.gscale * (1.05 + 0.15 * self.rng.rand()))\n                elif srate < 0.12:\n                    # too few successes -> increase to escape\n                    self.gscale = min(12.0, self.gscale * (1.15 + 0.3 * self.rng.rand()))\n                else:\n                    # otherwise slowly decay to refine\n                    self.gscale = max(0.08, self.gscale * (0.92 + 0.12 * self.rng.rand()))\n\n            # prune archive to maintain memory budget\n            if len(archive_F) > self.archive_max:\n                F_arr2 = np.array(archive_F, dtype=float)\n                all_idx = np.arange(len(F_arr2))\n                fin_idx = np.where(np.isfinite(F_arr2))[0]\n                keep_idx = []\n                # keep best half of finite\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort(F_arr2[fin_idx])]\n                    take = max(1, int(0.5 * sorted_fin.size))\n                    keep_idx.extend(list(sorted_fin[:take]))\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx.tolist()) - set(keep_idx))\n                if len(remaining) > 0:\n                    add = self.rng.choice(remaining, size=min(self.archive_max - len(keep_idx), len(remaining)), replace=False)\n                    keep_idx.extend(list(add))\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if stagn_count > max(30, int(40 + 30 * (1.0 / (1.0 + mean_scale)))):\n                # micro-restarts: inject a few diversified points around best and random\n                n_restart = min(12, max(4, int(6 + self.dim / 2)))\n                for k in range(n_restart):\n                    if self.evals >= self.budget:\n                        break\n                    if (self.x_opt is not None) and (self.rng.random() < 0.7):\n                        # local diversified around best\n                        jitter = self.rng.normal(0.0, self.gscale * 0.8 * per_dim_scale, size=self.dim)\n                        candr = self.x_opt + jitter\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1).copy()\n                    candr = self._reflect_bounds(candr, lb, ub)\n                    f_r = safe_eval(candr)\n                    archive_X.append(candr.copy())\n                    archive_F.append(f_r)\n                    if np.isfinite(f_r) and f_r < self.f_opt:\n                        self.f_opt = float(f_r)\n                        self.x_opt = candr.copy()\n                # encourage exploration after restart\n                stagn_count = 0\n                self.gscale = max(0.5, self.gscale * 1.8)\n\n        # final safety: pick best finite point from archive if needed\n        F_arr = np.array(archive_F, dtype=float) if len(archive_F) > 0 else np.array([], dtype=float)\n        if F_arr.size > 0 and np.any(np.isfinite(F_arr)):\n            finite_idxs = np.where(np.isfinite(F_arr))[0]\n            best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n            self.x_opt = np.array(archive_X[int(best_i)]).copy()\n            self.f_opt = float(F_arr[best_i])\n        else:\n            # ensure x_opt is defined: fallback to center and evaluate if possible\n            if self.x_opt is None and self.evals < self.budget:\n                f_center = safe_eval(center)\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n            elif self.x_opt is None:\n                # give some uniform point (no budget to evaluate) but keep consistent types\n                self.x_opt = np.asarray(center, dtype=float).copy()\n                self.f_opt = float(self.f_opt if np.isfinite(self.f_opt) else np.inf)\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 178, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "error": "In the code, line 178, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "27a94933-dda9-46f0-a760-98423d6633a6", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian local refinements and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # internal trackers\n        self.evals = 0\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to extract lower and upper bounds from func. Fallback to [-5,5]^dim.\n        Accepts common patterns: func.bounds -> (lb, ub), or attributes lb/ub/xmin/xmax/...\n        Scalar bounds are broadcast to dimension.\n        Returns (lb, ub) arrays of shape (dim,)\n        \"\"\"\n        lb = None\n        ub = None\n\n        # common container attribute 'bounds'\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # handle tuple/list/array of length 2: (low, high)\n                if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                    lb, ub = b\n        except Exception:\n            lb, ub = None, None\n\n        # try direct attributes names\n        if lb is None:\n            for name in (\"lb\", \"lower\", \"xmin\", \"x_lower\", \"x_min\"):\n                v = getattr(func, name, None)\n                if v is not None:\n                    lb = v\n                    break\n        if ub is None:\n            for name in (\"ub\", \"upper\", \"xmax\", \"x_upper\", \"x_max\"):\n                v = getattr(func, name, None)\n                if v is not None:\n                    ub = v\n                    break\n\n        # fallback to [-5,5]\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # Broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # If arrays shorter than dim, repeat last entry\n        if lb.size < self.dim:\n            tmp = np.full(self.dim, lb[-1], dtype=float)\n            tmp[:lb.size] = lb\n            lb = tmp\n        if ub.size < self.dim:\n            tmp = np.full(self.dim, ub[-1], dtype=float)\n            tmp[:ub.size] = ub\n            ub = tmp\n\n        lb = lb[: self.dim].astype(float)\n        ub = ub[: self.dim].astype(float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        \"\"\"\n        Reflect any coordinates that are outside bounds back into the box repeatedly,\n        then clamp to ensure numerical safety.\n        \"\"\"\n        x = np.asarray(x, dtype=float).copy()\n        for j in range(self.dim):\n            if not np.isfinite(x[j]):\n                x[j] = self.rng.uniform(lb[j], ub[j])\n                continue\n            # reflect until within bounds or until a few iterations\n            it = 0\n            while (x[j] < lb[j] or x[j] > ub[j]) and it < 10:\n                if x[j] < lb[j]:\n                    x[j] = lb[j] + (lb[j] - x[j])\n                elif x[j] > ub[j]:\n                    x[j] = ub[j] - (x[j] - ub[j])\n                it += 1\n            # final clamp\n            if x[j] < lb[j]:\n                x[j] = lb[j]\n            if x[j] > ub[j]:\n                x[j] = ub[j]\n        return x\n\n    def _lhs01(self, n, dim):\n        \"\"\"\n        Simple Latin hypercube in [0,1]^dim -> returns (n,dim)\n        \"\"\"\n        n = int(max(1, n))\n        out = np.empty((n, dim), dtype=float)\n        # strata in each dimension\n        for d in range(dim):\n            perm = self.rng.permutation(n)\n            # sample uniformly within each stratum\n            u = (perm + self.rng.random(n)) / float(n)\n            out[:, d] = u\n        return out\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if int(n) == 1:\n            return self.rng.uniform(lb, ub)\n        out = np.empty((int(n), self.dim), dtype=float)\n        for i in range(int(n)):\n            out[i, :] = self.rng.uniform(lb, ub)\n        return out\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        def safe_eval(x):\n            # x: array-like of length dim\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                # ensure float array\n                xv = np.asarray(x, dtype=float)\n                f = float(func(xv))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # initialization: center + LHS + one uniform\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        lhs01 = self._lhs01(n0, self.dim)\n        uni = self._uniform_array(lb, ub, n=1)\n\n        archive_X = []\n        archive_F = []\n\n        # evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(np.asarray(center).copy())\n            archive_F.append(float(f_center))\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.x_opt = np.asarray(center).copy()\n                self.f_opt = float(f_center)\n\n        # evaluate LHS points (map to bounds)\n        for i in range(lhs01.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = lb + (ub - lb) * lhs01[i, :]\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.x_opt = x.copy()\n                self.f_opt = float(f)\n\n        # ensure at least one uniform sample\n        if self.evals < self.budget:\n            u = uni.copy().reshape(self.dim)\n            f_uni = safe_eval(u)\n            archive_X.append(u.copy())\n            archive_F.append(float(f_uni))\n            if np.isfinite(f_uni) and f_uni < self.f_opt:\n                self.x_opt = u.copy()\n                self.f_opt = float(f_uni)\n\n        # short-term adaptation and control variables\n        gscale = 0.6  # global multiplier for step sizes (adapted)\n        per_dim_scale = np.maximum((ub - lb) / 10.0, 1e-6)  # baseline per-dim scale\n        adapt_window = max(10, min(200, self.budget // 50))\n        recent_successes = []\n        iter_since_best = 0\n\n        # helper to compute per-dim robust scale and median\n        def compute_stats():\n            X_arr = np.array(archive_X, dtype=float)\n            F_arr = np.array(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                Xf = X_arr[finite_idx, :]\n                median = np.median(Xf, axis=0)\n                # robust scale via IQR\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                iqr = np.maximum(q75 - q25, 1e-12)\n                # convert to approx sigma: IQR / 1.349\n                sigma = np.maximum(iqr / 1.349, 1e-12)\n                # also factor in range fallback to avoid zeros\n                sigma = np.maximum(sigma, (ub - lb) / 100.0)\n                return median, sigma\n            else:\n                # fallback to center and broad scale\n                return center.copy(), np.maximum((ub - lb) / 8.0, 1e-6)\n\n        # main loop\n        while self.evals < self.budget:\n            # prepare arrays\n            X_arr = np.array(archive_X, dtype=float)\n            F_arr = np.array(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            finite_count = finite_idx.size\n\n            # compute median and per-dim scale\n            median, scale_vec = compute_stats()\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_count > 0:\n                Xf = X_arr[finite_idx, :]\n                Ff = F_arr[finite_idx]\n                # logits negative of F (smaller F => larger probability)\n                logits = -Ff.copy().astype(float)\n                logits = logits - np.max(logits)\n                exp_logits = np.exp(logits)\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                pick_idx = self.rng.choice(len(probs), p=probs)\n                base = Xf[int(pick_idx)].copy()\n            else:\n                base = self._uniform_array(lb, ub, n=1).copy()\n\n            # small chance to use center instead\n            if self.rng.random() < 0.05:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick from some of the best finite points\n            donors = []\n            if finite_count >= 3:\n                # take best half\n                order = np.argsort(F_arr[finite_idx])\n                pool = finite_idx[order[: max(1, finite_count // 2)]]\n                k = min(3, pool.size)\n                if pool.size >= k:\n                    choices = self.rng.choice(pool, size=k, replace=False)\n                    donors = [X_arr[int(i)].copy() for i in choices]\n            if len(donors) < 2 and len(archive_X) >= 2:\n                idxs = self.rng.choice(len(archive_X), size=min(2, len(archive_X)), replace=False)\n                donors = [np.asarray(archive_X[int(i)]).copy() for i in idxs]\n\n            # choose move type: global vs local\n            p_global = 0.18 + 0.02 * min(1.0, max(0.0, (self.f_opt - (np.min(F_arr[np.isfinite(F_arr)]) if finite_count>0 else self.f_opt)) / (1.0 + abs(self.f_opt)))) \n            is_global = (self.rng.random() < p_global)\n\n            # prepare tempered cauchy and scaling\n            c = self.rng.standard_cauchy(self.dim)\n            # temper extremes with tau\n            tau = 2.5\n            c = c / (1.0 + np.abs(c) / tau)\n\n            if is_global:\n                # GLOBAL move: anchor on elite or uniformly sample anchor\n                if finite_count >= 3 and self.rng.random() < 0.85:\n                    Xf = X_arr[finite_idx, :]\n                    order = np.argsort(F_arr[finite_idx])\n                    take = max(1, int(0.10 * order.size))\n                    pick = order[self.rng.integers(0, take)]\n                    anchor = Xf[int(pick)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1).copy()\n                # make a cauchy jump scaled by per-dim scale and gscale\n                cand = anchor + gscale * (c * scale_vec)\n                # occasional DE-like differential from donors\n                if len(donors) >= 2 and self.rng.random() < 0.4:\n                    F_scale = 0.6 * (1.0 + self.rng.random())\n                    diff = donors[0] - donors[1]\n                    cand = cand + F_scale * diff\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.18:\n                    mask = (self.rng.rand(self.dim) < 0.2)\n                    if mask.any():\n                        u = self._uniform_array(lb, ub, n=1)\n                        cand[mask] = u[mask]\n                # tempered Cauchy escape heavy jump occasionally\n                if self.rng.random() < 0.05:\n                    heavy = self.rng.standard_cauchy(self.dim)\n                    heavy = heavy / (1.0 + np.abs(heavy) / (tau * 2.0))\n                    cand = cand + 1.5 * gscale * heavy * scale_vec\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                alpha = 0.0\n                # anisotropic gaussian step: per-dim sigma scaled by gscale and per_dim_scale\n                sigma = gscale * scale_vec * (0.5 + self.rng.random() * 1.5)\n                gauss = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + gauss * sigma\n                # attract toward median occasionally\n                if self.rng.random() < 0.25 and finite_count > 0:\n                    attract = (median - base) * (0.1 + 0.8 * self.rng.random())\n                    cand = cand + attract\n                # PCA-guided elite perturbation occasionally\n                if finite_count >= 4 and self.rng.random() < 0.22:\n                    # form elite set (top 8 or half)\n                    Xf = X_arr[finite_idx, :]\n                    ranks = np.argsort(F_arr[finite_idx])\n                    ecount = min(8, max(2, finite_count // 2))\n                    elites_idx = ranks[:ecount]\n                    E = Xf[elites_idx, :]\n                    try:\n                        E0 = E - np.mean(E, axis=0)\n                        # SVD to get principal directions\n                        u_s, svals, vt = np.linalg.svd(E0, full_matrices=False)\n                        pc_dir = vt[0, :]\n                        # scale along pc is proportional to largest singular value\n                        mean_scale = np.mean(scale_vec)\n                        pc_scale = (svals[0] / (np.sum(svals) + 1e-12)) * mean_scale * (0.6 + 1.4 * self.rng.random())\n                        if self.rng.random() < 0.6:\n                            cand = cand - 0.6 * pc_scale * pc_dir * (0.6 + 1.25 * self.rng.random())\n                        else:\n                            cand = cand + 0.5 * pc_scale * pc_dir * (0.5 + 1.0 * self.rng.random())\n                    except Exception:\n                        pass\n                # small DE-like mixing inside local branch sometimes\n                if len(donors) >= 2 and self.rng.random() < 0.18:\n                    cand = cand + 0.5 * (donors[0] - donors[1]) * (0.2 + self.rng.random())\n\n            # safety: replace any non-finite entries with uniform samples (per-dim)\n            for j in range(self.dim):\n                if not np.isfinite(cand[j]):\n                    cand[j] = self.rng.uniform(lb[j], ub[j])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(float(f_cand))\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.x_opt = np.asarray(cand).copy()\n                self.f_opt = float(f_cand)\n                success = True\n                iter_since_best = 0\n            else:\n                iter_since_best += 1\n\n            # compute membership among finite values\n            F_arr2 = np.array(archive_F, dtype=float)\n            finite_idx2 = np.where(np.isfinite(F_arr2))[0]\n            if np.isfinite(f_cand) and finite_idx2.size > 0:\n                rank = np.sum(F_arr2[finite_idx2] <= f_cand)\n                if rank <= max(1, int(0.25 * finite_idx2.size)):\n                    success = True\n\n            # record recent success\n            recent_successes.append(1 if success else 0)\n            if len(recent_successes) > adapt_window:\n                recent_successes.pop(0)\n            srate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # adapt gscale every few iterations (short-term adaptation)\n            # if many successes, increase exploratory scale; if too few, increase to escape; otherwise slowly decay\n            if len(recent_successes) >= adapt_window:\n                if srate > 0.45:\n                    gscale = gscale * (1.0 + 0.25 * (srate - 0.45))\n                elif srate < 0.12:\n                    # very low success -> increase to try larger moves (escape)\n                    gscale = gscale * (1.0 + 0.5 * (0.12 - srate) + 0.2)\n                else:\n                    # moderate, decay slowly to refine\n                    gscale = max(1e-6, gscale * 0.98)\n                # clamp gscale\n                gscale = float(np.clip(gscale, 1e-4, 8.0))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max:\n                F_arr2 = np.array(archive_F, dtype=float)\n                all_idx = np.arange(len(F_arr2))\n                finite_idx = np.where(np.isfinite(F_arr2))[0]\n                keep_idx = []\n                # keep best half of finite\n                if finite_idx.size > 0:\n                    order = np.argsort(F_arr2[finite_idx])\n                    bestk = max(1, finite_idx.size // 2)\n                    keep_idx.extend(list(finite_idx[order[:bestk]]))\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx) - set(keep_idx))\n                if len(remaining) > 0:\n                    add = self.rng.choice(remaining, size=min(self.archive_max - len(keep_idx), len(remaining)), replace=False)\n                    keep_idx.extend(list(add))\n                # rebuild archives\n                keep_idx = sorted(set(keep_idx))\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if iter_since_best > max(150, self.budget // 40) or (gscale < 1e-3 and iter_since_best > 50):\n                # micro-restart: inject a few diversified points around best and random\n                n_inject = min(10, max(3, self.dim))\n                best_center = self.x_opt.copy() if self.x_opt is not None else center.copy()\n                for k in range(n_inject):\n                    if self.evals >= self.budget:\n                        break\n                    if k < n_inject // 2:\n                        # local diversified around best\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim) * (0.5 + self.rng.random() * 1.5) * scale_vec\n                        candr = best_center + jitter\n                    else:\n                        # random injection\n                        candr = self._uniform_array(lb, ub, n=1)\n                    candr = self._reflect_bounds(candr, lb, ub)\n                    fr = safe_eval(candr)\n                    archive_X.append(np.asarray(candr).copy())\n                    archive_F.append(float(fr))\n                    if np.isfinite(fr) and fr < self.f_opt:\n                        self.x_opt = np.asarray(candr).copy()\n                        self.f_opt = float(fr)\n                # encourage exploration after restart\n                gscale = min(2.0, gscale * (1.0 + 0.6 * self.rng.random()))\n                recent_successes = []\n                iter_since_best = 0\n\n        # final safety: pick best finite point from archive if needed\n        F_arr = np.array(archive_F, dtype=float)\n        finite_idxs = np.where(np.isfinite(F_arr))[0]\n        if finite_idxs.size > 0:\n            best_i = finite_idxs[np.argmin(F_arr[finite_idxs])]\n            if self.x_opt is None or F_arr[best_i] < self.f_opt:\n                self.x_opt = np.asarray(archive_X[int(best_i)]).copy()\n                self.f_opt = float(F_arr[best_i])\n\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            # fallback to center and evaluate if possible\n            self.x_opt = np.asarray(center).copy()\n            if self.evals < self.budget:\n                f_center = safe_eval(self.x_opt)\n                if np.isfinite(f_center):\n                    self.f_opt = float(f_center)\n                else:\n                    self.f_opt = np.inf\n            else:\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 285, in __call__, the following error occurred:\nAttributeError: 'numpy.random.mtrand.RandomState' object has no attribute 'integers'\nOn line: pick = order[self.rng.integers(0, take)]", "error": "In the code, line 285, in __call__, the following error occurred:\nAttributeError: 'numpy.random.mtrand.RandomState' object has no attribute 'integers'\nOn line: pick = order[self.rng.integers(0, take)]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "2d4d5f45-9779-4c66-8268-d3ee54f181f2", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves,", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # short-term adaptation scale (relative to per-dim robust scale)\n        self.gscale = 1.0\n        # internal trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bound-like attributes from func, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # Common patterns to check:\n        if hasattr(func, 'bounds'):\n            b = func.bounds\n            # b can be tuple (lb, ub) or an object with .lb/.ub\n            if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                lb_cand, ub_cand = b\n                lb = np.asarray(lb_cand, dtype=float)\n                ub = np.asarray(ub_cand, dtype=float)\n            else:\n                try:\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n        # try attributes lower/upper\n        if lb is None:\n            if hasattr(func, 'lb'):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                except Exception:\n                    lb = None\n            if hasattr(func, 'ub'):\n                try:\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    ub = None\n        # fallback to scalars or [-5,5]\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n        lb = np.atleast_1d(np.array(lb, dtype=float))\n        ub = np.atleast_1d(np.array(ub, dtype=float))\n\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # ensure correct length\n        if lb.size < self.dim:\n            tmp = np.empty(self.dim, dtype=float)\n            tmp[:] = lb.ravel()[-1] if lb.size > 0 else -5.0\n            lb = tmp\n        if ub.size < self.dim:\n            tmp = np.empty(self.dim, dtype=float)\n            tmp[:] = ub.ravel()[-1] if ub.size > 0 else 5.0\n            ub = tmp\n\n        lb = lb[:self.dim].astype(float)\n        ub = ub[:self.dim].astype(float)\n        # final safety\n        mask = ub <= lb\n        if np.any(mask):\n            # enforce small positive width\n            ub = np.where(mask, lb + 1e-6 + np.abs(lb) * 1e-6 + 1.0, ub)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates that go beyond bounds: repeatedly reflect until within bounds\n        x = np.asarray(x, dtype=float).copy()\n        span = ub - lb\n        # Avoid divide by zero: if span=0, clamp to lb\n        for _ in range(4):  # up to a few reflections\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        H = np.zeros((n, dim), dtype=float)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        for j in range(dim):\n            # random permutation of intervals\n            u = self.rng.rand(n)\n            H[:, j] = cut[:n] + u * (1.0 / n)\n            order = self.rng.permutation(n)\n            H[:, j] = H[order, j]\n        return H\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        archive_X = []\n        archive_F = []\n\n        # safe evaluator that respects budget and catches exceptions\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return np.inf\n            self.evals += 1\n            try:\n                f = func(x)\n                # Ensure float\n                f = float(f)\n            except Exception:\n                f = np.inf\n            # update best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # initialization: center + LHS + one uniform\n        n0 = min(self.init_samples, max(1, self.budget // 20))\n        center = 0.5 * (lb + ub)\n        # Evaluate center first (if budget allows)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(np.asarray(center).copy())\n            archive_F.append(f_center)\n\n        # LHS samples\n        lhs01 = self._lhs01(max(1, n0 - 1), self.dim)\n        for i in range(lhs01.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = lb + lhs01[i] * (ub - lb)\n            f = safe_eval(x)\n            archive_X.append(np.asarray(x).copy())\n            archive_F.append(f)\n\n        # ensure at least one uniform sample\n        if self.evals < self.budget:\n            u = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(u)\n            archive_X.append(np.asarray(u).copy())\n            archive_F.append(f)\n\n        # short-term adaptation and control variables\n        stagn_count = 0\n        success_window = []\n        max_window = 40\n        iter_count = 0\n\n        def compute_stats():\n            X_arr = np.array(archive_X, dtype=float)\n            F_arr = np.array(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size == 0:\n                # no finite evaluations yet\n                per_dim_scale = np.maximum(1e-12, (ub - lb) / 6.0)\n                mean_scale = np.mean(per_dim_scale)\n                median = 0.5 * (lb + ub)\n                return X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median\n            Xf = X_arr[finite_idx]\n            # robust scale from IQR\n            try:\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                iqr = np.maximum(1e-12, q75 - q25)\n                per_dim_scale = iqr / 1.349  # approximate sigma\n            except Exception:\n                per_dim_scale = np.maximum(1e-12, (ub - lb) / 6.0)\n            # fallback to full-range-based scale for tiny values\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12 + (ub - lb) * 1e-6)\n            mean_scale = float(np.mean(per_dim_scale))\n            median = np.median(Xf, axis=0)\n            return X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median\n\n        # main loop\n        while self.evals < self.budget:\n            iter_count += 1\n            X_arr, F_arr, finite_idx, per_dim_scale, mean_scale, median = compute_stats()\n            finite_count = finite_idx.size\n\n            # choose base biased towards better archive items using softmax on negative F\n            if finite_count > 0:\n                Ff = F_arr[finite_idx]\n                # stabilize and scale\n                Fmin = np.min(Ff)\n                scale_fac = np.std(Ff) if np.std(Ff) > 1e-8 else (abs(Fmin) + 1.0)\n                logits = -(Ff - Fmin) / (scale_fac + 1e-12)\n                exp_logits = np.exp(logits - np.max(logits))\n                probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n                idx_choice = finite_idx[np.searchsorted(np.cumsum(probs), self.rng.rand(), side='right')]\n                base = X_arr[idx_choice].copy()\n            else:\n                base = center.copy()\n\n            # small chance to use center instead\n            if self.rng.rand() < 0.03:\n                base = center.copy()\n\n            # donor pool for DE-like moves\n            donors = []\n            if finite_count >= 3:\n                best_half = np.argsort(F_arr[finite_idx])[:max(2, finite_count // 2)]\n                donor_indices = finite_idx[best_half]\n                # choose up to 3 donors distinct\n                if donor_indices.size >= 2:\n                    pick = self.rng.choice(donor_indices, size=min(3, donor_indices.size), replace=False)\n                    donors = [X_arr[p] for p in pick]\n            elif len(archive_X) >= 2:\n                idxs = self.rng.choice(len(archive_X), size=min(2, len(archive_X)), replace=False)\n                donors = [archive_X[i] for i in idxs]\n\n            # choose move type\n            p_global = 0.25 + 0.15 * (1.0 - (iter_count / (1.0 + max(1, self.budget // 100))))\n            is_global = (self.rng.rand() < p_global)\n            cand = None\n\n            # prepare tempered cauchy and scaling\n            # temper extremes by clipping large tails\n            if is_global:\n                # GLOBAL move: anchor on elite or uniformly sample anchor\n                if finite_count >= 3 and self.rng.rand() < 0.7:\n                    # pick one elite anchor from best 10% (or top 1 if few)\n                    take_top = max(1, int(max(1, finite_count) * 0.10))\n                    top_idx = np.argsort(F_arr[finite_idx])[:take_top]\n                    anchor = X_arr[finite_idx[top_idx[self.rng.randint(len(top_idx))]]].copy()\n                else:\n                    # uniform anchor\n                    anchor = self._uniform_array(lb, ub, n=1)\n                # tempered cauchy jump\n                # sample independent cauchy components\n                u = self.rng.rand(self.dim)\n                # standard cauchy via tan(pi*(u-0.5))\n                r = np.tan(np.pi * (u - 0.5))\n                # temper extremes by logistic damping\n                damp = 1.0 / (1.0 + np.abs(r) / 10.0)\n                r *= damp\n                # scale by per-dim scale and gscale, randomized factor\n                scale_vec = per_dim_scale * self.gscale * (0.8 + 1.6 * self.rng.rand(self.dim))\n                cand = anchor + r * scale_vec\n\n                # occasional DE-like differential added\n                if len(donors) >= 2 and self.rng.rand() < 0.45:\n                    dvec = donors[0] - donors[-1]\n                    # scale differential by a random factor related to mean_scale\n                    cand = cand + 0.6 * (0.6 + self.rng.rand()) * dvec * (self.gscale * 0.5)\n\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.rand() < 0.08:\n                    k = max(1, int(0.08 * self.dim))\n                    idxs = self.rng.choice(self.dim, size=k, replace=False)\n                    ucoords = self._uniform_array(lb, ub, n=1)\n                    cand[idxs] = ucoords[idxs]\n\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                sigma = per_dim_scale * self.gscale * (0.25 + 1.2 * self.rng.rand(self.dim))\n                noise = self.rng.randn(self.dim) * sigma\n                cand = base + noise\n                # attract toward median occasionally\n                if finite_count > 0 and self.rng.rand() < 0.28:\n                    alpha = 0.05 + 0.25 * self.rng.rand()\n                    cand = (1.0 - alpha) * cand + alpha * median\n                # PCA-guided elite perturbation occasionally\n                if finite_count >= 4 and self.rng.rand() < 0.22:\n                    # form elite set (top 8 or half)\n                    k = min(8, max(2, finite_count // 2))\n                    elite_idx = np.argsort(F_arr[finite_idx])[:k]\n                    E = X_arr[finite_idx][elite_idx]\n                    E0 = E - np.mean(E, axis=0)\n                    try:\n                        uvec, svals, vt = np.linalg.svd(E0, full_matrices=False)\n                        pc = vt[0]\n                        pc_scale = (svals[0] / (np.sum(svals) + 1e-12)) * mean_scale * (0.6 + 1.4 * self.rng.rand())\n                        # choose sign and magnitude\n                        mag = pc_scale * ( -1.0 if self.rng.rand() < 0.5 else 1.0 )\n                        cand = cand + mag * pc\n                        # small DE-like mixing inside local branch sometimes\n                        if len(donors) >= 2 and self.rng.rand() < 0.4:\n                            F_scale = 0.4 * (1.0 + 0.8 * self.rng.rand())\n                            cand = cand + F_scale * (donors[0] - donors[-1])\n                    except Exception:\n                        pass\n                # occasional small coordinate injection\n                if self.rng.rand() < 0.06:\n                    idxs = self.rng.choice(self.dim, size=max(1, int(0.03 * self.dim)), replace=False)\n                    ucoords = self._uniform_array(lb, ub, n=1)\n                    cand[idxs] = ucoords[idxs]\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            if not np.all(np.isfinite(cand)):\n                nan_idx = ~np.isfinite(cand)\n                cand[nan_idx] = self._uniform_array(lb, ub, n=1)[nan_idx]\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_cand = safe_eval(cand)\n            archive_X.append(np.asarray(cand).copy())\n            archive_F.append(f_cand)\n\n            # define success: improved global best OR placed among top 25% of finite vals\n            finite_vals = np.array([v for v in archive_F if np.isfinite(v)], dtype=float)\n            if finite_vals.size == 0:\n                success = np.isfinite(f_cand)\n            else:\n                combined = np.concatenate([finite_vals, np.array([f_cand])])\n                rank = np.searchsorted(np.sort(combined), f_cand, side='left')\n                success = (f_cand < self.f_opt) or (rank < max(1, int(0.25 * combined.size)))\n            # mild success if finite but not terrific\n            success_window.append(1 if success else 0)\n            if len(success_window) > max_window:\n                success_window.pop(0)\n\n            # compute success rate and adapt gscale every few iterations\n            if iter_count % 8 == 0 and len(success_window) > 0:\n                srate = float(np.mean(success_window))\n                if srate > 0.45:\n                    # many successes, increase exploratory scale moderately\n                    self.gscale = min(1e6, self.gscale * (1.06 + 0.08 * self.rng.rand()))\n                elif srate < 0.08:\n                    # too few successes -> increase to escape\n                    self.gscale = min(1e6, self.gscale * (1.10 + 0.25 * self.rng.rand()))\n                else:\n                    # otherwise slowly decay to refine\n                    self.gscale = max(1e-8, self.gscale * (0.985 + 0.03 * self.rng.rand()))\n\n            # prune archive to maintain memory budget\n            if len(archive_X) > self.archive_max:\n                F_arr2 = np.array(archive_F, dtype=float)\n                all_idx = list(range(len(archive_X)))\n                fin_idx = np.where(np.isfinite(F_arr2))[0]\n                keep_idx = []\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort(F_arr2[fin_idx])]\n                    take = max(1, min(self.archive_max // 2, sorted_fin.size))\n                    keep_idx.extend(list(sorted_fin[:take]))\n                # keep some random rest (including some infinities)\n                remaining = list(set(all_idx) - set(keep_idx))\n                if len(remaining) > 0:\n                    add_n = min(self.archive_max - len(keep_idx), len(remaining))\n                    add = list(self.rng.choice(remaining, size=add_n, replace=False))\n                    keep_idx.extend(add)\n                # rebuild archive\n                newX = [archive_X[i] for i in keep_idx]\n                newF = [archive_F[i] for i in keep_idx]\n                archive_X = newX\n                archive_F = newF\n\n            # stagnation detection & micro-restarts\n            # micro-restarts triggered by long time with no global improvement\n            if np.isfinite(self.f_opt):\n                # compute how long since last improvement by scanning tail for lower than current opt\n                # we maintain stagn_count as consecutive iterations with no improvement\n                if f_cand < self.f_opt:\n                    stagn_count = 0\n                else:\n                    stagn_count += 1\n            else:\n                stagn_count += 1\n\n            if stagn_count > max(30, int(0.5 * self.dim)):\n                # perform micro-restart: inject diversified points\n                n_local = min(8, max(2, self.dim))\n                best_x = self.x_opt if (self.x_opt is not None) else center\n                # generate local diversified around best\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    local_noise = (self.rng.randn(self.dim) * per_dim_scale * (0.8 + 1.6 * self.rng.rand(self.dim)) *\n                                   (1.0 + 0.5 * self.rng.rand()))\n                    xr = self._reflect_bounds(best_x + local_noise, lb, ub)\n                    fr = safe_eval(xr)\n                    archive_X.append(np.asarray(xr).copy())\n                    archive_F.append(fr)\n                # a few uniform injections\n                for _ in range(min(6, max(1, self.budget - self.evals))):\n                    if self.evals >= self.budget:\n                        break\n                    ur = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(ur)\n                    archive_X.append(np.asarray(ur).copy())\n                    archive_F.append(fr)\n                # encourage exploration after restart\n                self.gscale = max(self.gscale, 1.0 + self.rng.rand() * 3.0)\n                success_window.clear()\n                stagn_count = 0\n\n        # final safety: pick best finite point from archive if needed\n        F_arr = np.array(archive_F, dtype=float) if len(archive_F) > 0 else np.array([], dtype=float)\n        X_arr = np.array(archive_X, dtype=float) if len(archive_X) > 0 else np.array([], dtype=float)\n        finite_idxs = np.where(np.isfinite(F_arr))[0] if F_arr.size > 0 else np.array([], dtype=int)\n        if finite_idxs.size > 0:\n            best_idx = finite_idxs[np.argmin(F_arr[finite_idxs])]\n            if self.x_opt is None or F_arr[best_idx] < self.f_opt:\n                self.f_opt = float(F_arr[best_idx])\n                self.x_opt = X_arr[best_idx].copy()\n        else:\n            # fallback to center and evaluate if possible\n            if self.x_opt is None and self.evals < self.budget:\n                f_center = safe_eval(center)\n                archive_X.append(np.asarray(center).copy())\n                archive_F.append(f_center)\n                if np.isfinite(f_center):\n                    self.f_opt = f_center\n                    self.x_opt = center.copy()\n\n        # final return\n        return float(self.f_opt), (None if self.x_opt is None else np.asarray(self.x_opt, dtype=float))", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b88904ab-d5b7-4a9f-9b54-dde084ba5628", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian local search, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = int(archive_max)\n        # internal trackers:\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Default bounds [-5,5] for all dimensions\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        # try to extract attributes from func if present\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # accept tuple/list/np.ndarray of length 2 or object with lb/ub\n                if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                    lbo = np.asarray(b[0], dtype=float)\n                    ubo = np.asarray(b[1], dtype=float)\n                    # if scalars, broadcast\n                    if lbo.size == 1:\n                        lbo = np.full(self.dim, float(lbo))\n                    if ubo.size == 1:\n                        ubo = np.full(self.dim, float(ubo))\n                    if lbo.size == self.dim and ubo.size == self.dim:\n                        lb, ub = lbo, ubo\n                else:\n                    # maybe an object with .lb and .ub\n                    if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                        lbo = np.asarray(b.lb, dtype=float)\n                        ubo = np.asarray(b.ub, dtype=float)\n                        if lbo.size == 1:\n                            lbo = np.full(self.dim, float(lbo))\n                        if ubo.size == 1:\n                            ubo = np.full(self.dim, float(ubo))\n                        if lbo.size == self.dim and ubo.size == self.dim:\n                            lb, ub = lbo, ubo\n            else:\n                # check func has direct attributes lb/ub\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lbo = np.asarray(func.lb, dtype=float)\n                    ubo = np.asarray(func.ub, dtype=float)\n                    if lbo.size == 1:\n                        lbo = np.full(self.dim, float(lbo))\n                    if ubo.size == 1:\n                        ubo = np.full(self.dim, float(ubo))\n                    if lbo.size == self.dim and ubo.size == self.dim:\n                        lb, ub = lbo, ubo\n        except Exception:\n            # ignore and use defaults\n            pass\n        # safety: if any ub <= lb, replace with defaults for those coords\n        bad = ub <= lb\n        if np.any(bad):\n            ub[bad] = 5.0\n            lb[bad] = -5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coords repeatedly until within bounds, fallback to clamp\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect: x = lb + (lb - x) for below; x = ub - (x - ub) for above\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        u = self.rng.random((n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            steps = (perm + u[:, j]) / n\n            lhs[:, j] = steps\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            r = self.rng.random((n, self.dim))\n            return lb + r * (ub - lb)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                return None  # no budget left\n            try:\n                xv = np.asarray(x, dtype=float)\n                fv = float(func(xv))\n            except Exception:\n                fv = np.inf\n            self.eval_count += 1\n            return fv\n\n        # initialize archive\n        archive_X = []\n        archive_F = []\n\n        # initial center at midpoint of bounds\n        center = (lb + ub) / 2.0\n        # initial LHS in [0,1] mapped to bounds and one uniform\n        lhs01 = self._lhs01(self.init_samples, self.dim)\n        lhs_pts = lb + lhs01 * (ub - lb)\n        uni = self._uniform_array(lb, ub, n=1)\n\n        # Evaluate center first if budget allows\n        f_center = safe_eval(center)\n        if f_center is not None:\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(lhs_pts.shape[0]):\n            if self.eval_count >= self.budget:\n                break\n            x = lhs_pts[i]\n            f = safe_eval(x)\n            if f is None:\n                break\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.eval_count < self.budget:\n            u = uni[0] if uni.ndim > 1 else uni\n            f = safe_eval(u)\n            if f is not None:\n                archive_X.append(u.copy())\n                archive_F.append(f)\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = u.copy()\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=50)\n        gscale = 0.5  # global scale multiplier (adapted)\n        iter_since_improve = 0\n        iters = 0\n\n        def compute_stats():\n            # compute per-dim robust scale and median from finite archive\n            X = np.asarray(archive_X, dtype=float)\n            F = np.asarray(archive_F, dtype=float)\n            finite_mask = np.isfinite(F)\n            if finite_mask.sum() == 0:\n                # fallback: full range\n                median = center.copy()\n                per_dim_scale = (ub - lb) / 4.0\n            else:\n                Xf = X[finite_mask]\n                median = np.median(Xf, axis=0)\n                # IQR per dimension\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr / 1.349  # approximate sigma\n                # fallback for near-zero\n                small = per_dim_scale <= 1e-8\n                per_dim_scale[small] = 0.5 * (ub[small] - lb[small]) / 6.0\n            # ensure min reasonable scale\n            per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-6)\n            return median, per_dim_scale\n\n        # main loop\n        while self.eval_count < self.budget:\n            iters += 1\n            # prune small archive excess occasionally\n            if len(archive_X) > max(2 * self.archive_max, 4):\n                # keep best half and some random rest\n                F_arr = np.asarray(archive_F)\n                finite_idx = np.where(np.isfinite(F_arr))[0]\n                order = np.argsort(F_arr[finite_idx])\n                keep_best = finite_idx[order[: max(1, len(order) // 2)]].tolist()\n                rest_idx = [i for i in range(len(archive_X)) if i not in keep_best]\n                rng_keep = self.rng.choice(rest_idx, size=min(len(rest_idx), self.archive_max // 4), replace=False).tolist() if rest_idx else []\n                keep = set(keep_best + rng_keep)\n                archive_X = [archive_X[i] for i in range(len(archive_X)) if i in keep]\n                archive_F = [archive_F[i] for i in range(len(archive_F)) if i in keep]\n\n            # compute stats\n            median, per_dim_scale = compute_stats()\n\n            # prepare sampling base: bias towards better archive items via softmax on -F\n            F_arr = np.asarray(archive_F, dtype=float)\n            X_arr = np.asarray(archive_X, dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n            if finite_idx.size == 0:\n                # random base\n                base = self._uniform_array(lb, ub, n=1)\n            else:\n                # softmax on negatives to bias to better (smaller) F\n                Ff = F_arr[finite_idx]\n                # numerical stabilization\n                s = np.clip((np.max(Ff) - Ff), 0, None)\n                temp = 1.0 + 5.0 * self.rng.random()  # temperature randomness\n                logits = s / (np.std(s) + 1e-9)\n                probs = np.exp(logits / temp)\n                probs = probs / probs.sum()\n                pick = self.rng.choice(finite_idx, p=probs)\n                base = X_arr[pick].copy()\n\n            # small chance to use center instead\n            if self.rng.random() < 0.03 and self.x_opt is not None:\n                base = self.x_opt.copy()\n\n            # donor pool for DE-like moves\n            donor = None\n            if finite_idx.size >= 3 and len(archive_X) >= 3:\n                pool = finite_idx\n                # pick three distinct donors\n                a, b, c = self.rng.choice(pool, size=3, replace=False)\n                donor = (X_arr[a] - X_arr[b]) + (X_arr[c] - base)\n            elif len(archive_X) >= 3:\n                # fallback random from whole archive\n                idxs = self.rng.choice(len(archive_X), size=3, replace=False)\n                donor = (X_arr[idxs[0]] - X_arr[idxs[1]]) + (X_arr[idxs[2]] - base)\n\n            # choose move type: global vs local\n            move_rand = self.rng.random()\n            is_global = move_rand < (0.25 + 0.1 * (self.f_opt > 1e-3))  # more global if poor\n            # prepare tempered cauchy\n            cauchy = self.rng.standard_cauchy(self.dim)\n            # temper extremes\n            c_small = np.tanh(0.1 * cauchy)  # bounded small jump factor\n\n            # produce candidate\n            if is_global:\n                # GLOBAL move: anchor on elite or uniformly sample anchor\n                cand = base.copy()\n                if finite_idx.size >= max(1, int(max(1, len(finite_idx) * 0.1))):\n                    # pick an elite anchor among top 10% (or top1)\n                    top_k = max(1, int(max(1, len(finite_idx) * 0.1)))\n                    order = np.argsort(F_arr[finite_idx])\n                    elite_idx = finite_idx[order[:top_k]]\n                    anchor = X_arr[self.rng.choice(elite_idx)]\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n\n                # make a cauchy jump scaled by per-dim scale and gscale\n                scale_vec = per_dim_scale * (0.8 + 1.6 * self.rng.random(self.dim))\n                F_scale = 0.8 * (1.0 + self.rng.random())\n                cand = anchor + F_scale * gscale * c_small * scale_vec\n\n                # occasional DE-like differential from donors\n                if donor is not None and self.rng.random() < 0.25:\n                    cand = cand + 0.6 * gscale * donor * (0.5 + self.rng.random())\n\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.12:\n                    mask = self.rng.random(self.dim) < 0.15\n                    cand[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                local_sigma = per_dim_scale * (0.05 + 0.9 * self.rng.random(self.dim)) * gscale\n                cand = base + self.rng.normal(size=self.dim) * local_sigma\n                # attract toward median occasionally\n                if self.rng.random() < 0.18:\n                    alpha = 0.1 + 0.45 * self.rng.random()\n                    cand = cand * (1.0 - alpha) + median * alpha\n                # PCA-guided elite perturbation occasionally\n                if finite_idx.size >= 4 and self.rng.random() < 0.22:\n                    # form elite set (top up to 8 or half)\n                    k = min(8, max(4, finite_idx.size // 2))\n                    elite_inds = finite_idx[np.argsort(F_arr[finite_idx])[:k]]\n                    E = X_arr[elite_inds] - np.mean(X_arr[elite_inds], axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        pc = Vt[0]\n                        # scale along principal direction proportional to spread\n                        pc_scale = S[0] / max(1e-12, np.mean(S))\n                        step = (self.rng.normal() * (0.5 + self.rng.random()) * pc_scale) * per_dim_scale * gscale\n                        if self.rng.random() < 0.6:\n                            cand = cand + step * pc\n                        else:\n                            cand = cand - step * pc\n                    except Exception:\n                        pass\n                # small DE-like mixing inside local branch sometimes\n                if donor is not None and self.rng.random() < 0.12:\n                    cand = cand + 0.4 * gscale * donor * (0.4 + self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.05:\n                    cand = cand + 0.6 * gscale * c_small * per_dim_scale * (0.6 + self.rng.random(self.dim))\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.count_nonzero(bad)) * (ub[bad] - lb[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            if self.eval_count >= self.budget:\n                break\n            f_cand = safe_eval(cand)\n            if f_cand is None:\n                break\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # determine success: improved global best OR placed among top 25% of finite vals\n            prev_best = self.f_opt\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                success = True\n                iter_since_improve = 0\n            else:\n                # rank among finite\n                F_arr = np.asarray(archive_F, dtype=float)\n                finite_mask = np.isfinite(F_arr)\n                if finite_mask.sum() > 0:\n                    combined = np.sort(F_arr[finite_mask])\n                    rank = np.searchsorted(combined, f_cand, side='left')\n                    success = (rank < max(1, len(combined) // 4))\n                else:\n                    success = False\n                iter_since_improve += 1\n\n            success_window.append(1 if success else 0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if len(success_window) == success_window.maxlen:\n                srate = sum(success_window) / len(success_window)\n                # If unusually many successes, become slightly more exploratory to capitalize\n                if srate > 0.4:\n                    gscale = min(5.0, gscale * (1.0 + 0.2 * (srate - 0.4)))\n                # If too few successes -> increase to escape (but not blow up)\n                elif srate < 0.08:\n                    gscale = min(10.0, gscale * (1.0 + 2.0 * (0.08 - srate)))\n                else:\n                    # slowly decay to refine\n                    gscale = max(1e-3, gscale * 0.995)\n\n            # prune archive to maintain memory budget occasionally\n            if len(archive_X) > self.archive_max:\n                # keep best half of finite and some random rest (including some infinities)\n                F_arr = np.asarray(archive_F)\n                finite_idx = np.where(np.isfinite(F_arr))[0]\n                keep = set()\n                if finite_idx.size > 0:\n                    order = np.argsort(F_arr[finite_idx])\n                    keep_best = finite_idx[order[: max(1, finite_idx.size // 2)]].tolist()\n                    keep.update(keep_best)\n                # keep some random others\n                rest = [i for i in range(len(archive_X)) if i not in keep]\n                keep_rand = self.rng.choice(rest, size=min(len(rest), self.archive_max - len(keep)), replace=False).tolist() if rest else []\n                keep.update(keep_rand)\n                archive_X = [archive_X[i] for i in range(len(archive_X)) if i in keep]\n                archive_F = [archive_F[i] for i in range(len(archive_F)) if i in keep]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 2 * self.dim):\n                # micro-restart: inject diversified points around best and uniformly\n                iter_since_improve = 0\n                num_inject = min(10 + self.dim, self.budget - self.eval_count)\n                for k in range(num_inject):\n                    if self.eval_count >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.75:\n                        # local diversified around best\n                        perturb = self.rng.normal(scale=per_dim_scale * (0.2 + 0.8 * self.rng.random(self.dim)))\n                        newx = self.x_opt + perturb\n                    else:\n                        # global uniform injection\n                        newx = self._uniform_array(lb, ub, n=1)\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    fnew = safe_eval(newx)\n                    if fnew is None:\n                        break\n                    archive_X.append(newx.copy())\n                    archive_F.append(fnew)\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.f_opt = fnew\n                        self.x_opt = newx.copy()\n                # encourage exploration after restart\n                gscale = max(gscale, 1.0 + 0.5 * self.rng.random())\n\n            # safety small pause - continue until budget exhausted\n        # final safety: pick best finite point from archive if needed\n        if self.x_opt is None and len(archive_X) > 0:\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best = finite_idx[np.argmin(F_arr[finite_idx])]\n                self.x_opt = np.asarray(archive_X[best]).copy()\n                self.f_opt = float(archive_F[best])\n            else:\n                # fallback: evaluate center if possible\n                if self.eval_count < self.budget:\n                    f_center = safe_eval(center)\n                    if f_center is not None:\n                        self.x_opt = center.copy()\n                        self.f_opt = f_center\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 172, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "error": "In the code, line 172, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1eeb4d53-c7bb-4bfb-8cab-30ff2d1b709d", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian local search, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nimport math\nimport time\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = max(2, int(init_samples))\n        self.archive_max = max(50, int(archive_max))\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n\n        # internal state to return later\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Default bounds [-5,5] for all dimensions\n        lb = None\n        ub = None\n\n        # common possible attributes: func.bounds.lb / func.bounds.ub (arrays),\n        # or func.lb / func.ub, or func.lower_bounds / func.upper_bounds\n        def _as_array(x):\n            if x is None:\n                return None\n            a = np.asarray(x, dtype=float)\n            if a.ndim == 0:\n                a = np.full(self.dim, float(a))\n            if a.size == 2 and a.shape == (2,):\n                # ambiguous: treat as [lb, ub] if length matches 2 and dim==1 - otherwise ignore\n                if self.dim == 1:\n                    return np.array([a[0]]), np.array([a[1]])\n            if a.shape == (self.dim,):\n                return a\n            # if scalar given\n            if a.size == 1:\n                return np.full(self.dim, float(a))\n            return None\n\n        # try multiple attributes\n        cand_pairs = []\n        if hasattr(func, \"bounds\"):\n            bnds = getattr(func, \"bounds\")\n            try:\n                lb_c = getattr(bnds, \"lb\", None)\n                ub_c = getattr(bnds, \"ub\", None)\n                if lb_c is not None and ub_c is not None:\n                    cand_pairs.append((lb_c, ub_c))\n            except Exception:\n                pass\n        for name in (\"lb\", \"ub\", \"lower_bounds\", \"upper_bounds\", \"lower\", \"upper\"):\n            if hasattr(func, name):\n                # pack pairs later\n                pass\n        # check direct attributes\n        if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n            cand_pairs.append((getattr(func, \"lb\"), getattr(func, \"ub\")))\n        if hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n            cand_pairs.append((getattr(func, \"lower_bounds\"), getattr(func, \"upper_bounds\")))\n        if hasattr(func, \"bounds\") and isinstance(getattr(func, \"bounds\"), (tuple, list)) and len(getattr(func, \"bounds\")) == 2:\n            cand_pairs.append(tuple(getattr(func, \"bounds\")))\n\n        # fallback try attributes individually if they exist\n        for name_l, name_u in ((\"lower\", \"upper\"), (\"min_bounds\", \"max_bounds\"), (\"min\", \"max\")):\n            if hasattr(func, name_l) and hasattr(func, name_u):\n                cand_pairs.append((getattr(func, name_l), getattr(func, name_u)))\n\n        # attempt to coerce candidates\n        for a, b in cand_pairs:\n            la = _as_array(a)\n            ub_ = _as_array(b)\n            if la is None or ub_ is None:\n                continue\n            # if returned two arrays for scalar [lb, ub] case\n            if isinstance(la, tuple) and len(la) == 2 and isinstance(ub_, tuple) and len(ub_) == 2:\n                pass\n            if la.shape == (self.dim,) and ub_.shape == (self.dim,):\n                lb = la\n                ub = ub_\n                break\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # safety: ensure ub > lb everywhere\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or ub[i] <= lb[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates repeatedly until within bounds, fallback to clamp\n        xx = x.copy()\n        for i in range(len(xx)):\n            if not np.isfinite(xx[i]):\n                continue\n            low = lb[i]; high = ub[i]\n            if low >= high:\n                xx[i] = np.clip(xx[i], low, high)\n                continue\n            # reflect while out of bounds but limit iterations\n            it = 0\n            while (xx[i] < low or xx[i] > high) and it < 8:\n                if xx[i] < low:\n                    xx[i] = low + (low - xx[i])\n                elif xx[i] > high:\n                    xx[i] = high - (xx[i] - high)\n                it += 1\n            if xx[i] < low or xx[i] > high or not np.isfinite(xx[i]):\n                xx[i] = np.clip(xx[i], low, high)\n        return xx\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        rng = self.rng\n        seg = np.linspace(0.0, 1.0, n + 1)\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            # random point in each interval\n            u = rng.uniform(size=n) * (seg[1] - seg[0]) + seg[:-1]\n            rng.shuffle(u)\n            points[:, j] = u\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            return self.rng.uniform(0.0, 1.0, size=(n, self.dim)) * (ub - lb) + lb\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        evals = 0\n        start_time = time.time()\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            try:\n                val = float(func(x))\n            except Exception:\n                val = np.inf\n            evals += 1\n            return val\n\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span[span <= 0] = 10.0  # safety\n\n        # initial center and samples\n        center = 0.5 * (lb + ub)\n        archive_X = []\n        archive_F = []\n\n        # Evaluate center first if budget allows\n        if evals < self.budget:\n            f_center = safe_eval(center)\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # LHS seeds and at least one uniform sample\n        remaining_initial = max(0, min(self.init_samples - len(archive_X), self.budget - evals - 1))\n        if remaining_initial > 0:\n            lhs = self._lhs01(remaining_initial, self.dim)\n            for i in range(remaining_initial):\n                x = lb + lhs[i] * (ub - lb)\n                f = safe_eval(x)\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if evals < self.budget:\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # Convert to numpy arrays for convenience\n        X = np.asarray(archive_X, dtype=float)\n        F = np.asarray(archive_F, dtype=float)\n\n        # short-term adaptation and control variables\n        gscale = 0.2  # global scale multiplier for cauchy jumps (relative to per-dim scales)\n        local_scale = 0.05  # anisotropic local gaussian base multiplier\n        min_scale_frac = 1e-6\n        success_window = []\n        adapt_window = 30\n        stagnation_counter = 0\n        best_since_restart = self.f_opt\n\n        def compute_stats():\n            # robust per-dim scale (IQR-based) and median\n            nonlocal X, F\n            finite_idx = np.isfinite(F)\n            if not np.any(finite_idx):\n                # fallback to full range\n                med = 0.5 * (lb + ub)\n                scl = np.maximum(1e-3 * span, 0.1 * span)\n                return med, scl\n            Xf = X[finite_idx]\n            if Xf.shape[0] == 0:\n                med = 0.5 * (lb + ub)\n                scl = np.maximum(1e-3 * span, 0.1 * span)\n                return med, scl\n            med = np.median(Xf, axis=0)\n            q75 = np.percentile(Xf, 75, axis=0)\n            q25 = np.percentile(Xf, 25, axis=0)\n            iqr = q75 - q25\n            # convert IQR to approximate std: std ~ IQR/1.349\n            scl = iqr / 1.349\n            # fallback for small scales to MAD-like\n            if np.any(scl <= 0):\n                mad = np.median(np.abs(Xf - med), axis=0)\n                scl = np.where(scl <= 0, mad * 1.4826, scl)\n            # fallback to a fraction of span\n            scl = np.where(np.isfinite(scl) & (scl > 0), scl, 0.1 * span)\n            min_s = np.maximum(min_scale_frac * span, 1e-8)\n            scl = np.maximum(scl, min_s)\n            return med, scl\n\n        # main loop\n        it = 0\n        last_improve_iter = 0\n        while evals < self.budget:\n            it += 1\n            # occasionally prune tiny archive excess early (keep best half and some random)\n            if X.shape[0] > 3 * self.archive_max:\n                finite_idx = np.isfinite(F)\n                order = np.argsort(F[finite_idx])\n                keep_best = int(max(2, np.sum(finite_idx) // 2))\n                keep_idx = np.nonzero(finite_idx)[0][order[:keep_best]].tolist()\n                # add some random others\n                others = list(set(range(X.shape[0])) - set(keep_idx))\n                self.rng.shuffle(others)\n                keep_idx += others[:self.archive_max - len(keep_idx)]\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            med, scales = compute_stats()\n\n            # prepare sampling base: bias towards better archive items via softmax on -F\n            finite_idx = np.isfinite(F)\n            weights = None\n            if np.any(finite_idx):\n                vals = F[finite_idx]\n                # numerical stabilization\n                vmin = np.min(vals)\n                z = - (vals - vmin)\n                # temperature adapts to spread\n                temp = max(1e-3, np.std(vals) if vals.size > 1 else 1.0)\n                z = z / (temp + 1e-12)\n                maxz = np.max(z)\n                z = z - maxz\n                w = np.exp(z)\n                if np.sum(w) <= 0:\n                    w = np.ones_like(w)\n                w = w / np.sum(w)\n                idxs = np.nonzero(finite_idx)[0]\n                weights = (idxs, w)\n            else:\n                weights = (np.arange(X.shape[0]), np.ones(X.shape[0]) / max(1, X.shape[0]))\n\n            # random base vs biased base\n            use_random_base_prob = 0.08\n            if self.rng.rand() < use_random_base_prob or X.shape[0] == 0:\n                base = lb + self.rng.rand(self.dim) * (ub - lb)\n            else:\n                idxs, w = weights\n                choose = self.rng.choice(len(idxs), p=w)\n                base = X[idxs[choose]].copy()\n\n            # small chance to use center instead\n            if self.rng.rand() < 0.02:\n                base = center.copy()\n\n            # donor pool for DE-like moves: pick three distinct donors\n            def pick_donors(k=3):\n                if X.shape[0] >= k:\n                    idxs_all = np.arange(X.shape[0])\n                    # prefer finite ones\n                    idxs_fin = np.nonzero(np.isfinite(F))[0]\n                    if idxs_fin.size >= k:\n                        sel = self.rng.choice(idxs_fin, size=k, replace=False)\n                        return X[sel]\n                    else:\n                        sel = self.rng.choice(idxs_all, size=k, replace=False)\n                        return X[sel]\n                else:\n                    # not enough archived points: sample from uniform\n                    return lb + self.rng.rand(k, self.dim) * (ub - lb)\n\n            donors = pick_donors(3)\n\n            # choose move type: global vs local\n            p_global = 0.18 + 0.02 * (1.0 if (np.isfinite(self.f_opt) and self.f_opt > 1e-1) else 0.0)\n            is_global = self.rng.rand() < p_global\n\n            # tempered cauchy helper\n            def tempered_cauchy(scale_vec, gscale_local):\n                # sample standard Cauchy via tan(pi*(u-0.5))\n                u = self.rng.rand(self.dim)\n                c = np.tan(np.pi * (u - 0.5))\n                # temper extremes: clip quantiles\n                q_low = np.quantile(c, 0.005)\n                q_high = np.quantile(c, 0.995)\n                c = np.clip(c, q_low, q_high)\n                return c * scale_vec * gscale_local\n\n            # produce candidate\n            if is_global:\n                # anchor on elite or uniform anchor\n                elite_count = max(1, int(max(1, 0.1 * np.sum(np.isfinite(F)))))\n                if np.sum(np.isfinite(F)) >= elite_count:\n                    fin_idx = np.nonzero(np.isfinite(F))[0]\n                    order = np.argsort(F[fin_idx])\n                    elites_idx = fin_idx[order[:elite_count]]\n                    anchor = X[self.rng.choice(elites_idx)]\n                else:\n                    anchor = lb + self.rng.rand(self.dim) * (ub - lb)\n\n                # cauchy jump\n                jump = tempered_cauchy(scales, gscale)\n                cand = anchor + jump\n\n                # occasional DE-like differential from donors\n                if self.rng.rand() < 0.38:\n                    beta = 0.6 + 0.6 * self.rng.rand()\n                    cand += beta * (donors[0] - donors[1])\n\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.rand() < 0.14:\n                    n_inject = max(1, int(0.05 * self.dim) + self.rng.randint(0, max(1, int(0.2 * self.dim))))\n                    idxs_inj = self.rng.choice(self.dim, size=n_inject, replace=False)\n                    for ii in idxs_inj:\n                        cand[ii] = lb[ii] + self.rng.rand() * (ub[ii] - lb[ii])\n\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                anis = self.rng.randn(self.dim) * (scales * local_scale)\n                cand = base + anis\n\n                # attract toward median occasionally\n                if self.rng.rand() < 0.28:\n                    alpha = 0.05 + 0.4 * self.rng.rand()\n                    cand = cand * (1 - alpha) + med * alpha\n\n                # PCA-guided elite perturbation occasionally\n                if self.rng.rand() < 0.12 and np.sum(np.isfinite(F)) >= 3:\n                    fin_idx = np.nonzero(np.isfinite(F))[0]\n                    order = np.argsort(F[fin_idx])\n                    k = min(8, max(2, len(order) // 2))\n                    elites_idx = fin_idx[order[:k]]\n                    elites = X[elites_idx]\n                    # center and PCA\n                    C = elites - np.mean(elites, axis=0)\n                    # SVD for principal direction\n                    try:\n                        U, S, Vt = np.linalg.svd(C, full_matrices=False)\n                        principal = Vt[0]\n                        spread = np.sqrt(np.sum((C @ principal) ** 2) / max(1, C.shape[0] - 1))\n                        magnitude = 0.5 + 2.0 * (self.rng.rand())\n                        cand += principal * spread * magnitude * (0.5 + self.rng.rand())\n                    except Exception:\n                        # fallback small gaussian\n                        cand += (self.rng.randn(self.dim) * (0.5 * scales * local_scale))\n\n                # small DE-like mixing inside local branch sometimes\n                if self.rng.rand() < 0.18:\n                    gamma = 0.2 * (0.5 + self.rng.rand())\n                    cand += gamma * (donors[0] - donors[2])\n\n                # occasional tempered Cauchy jump for escape\n                if self.rng.rand() < 0.03:\n                    cand += tempered_cauchy(scales, 2.0 * gscale)\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.rand(np.sum(bad)) * (ub[bad] - lb[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate (respecting budget)\n            f_cand = safe_eval(cand)\n\n            # insert into archive\n            X = np.vstack([X, cand.reshape(1, -1)])\n            F = np.concatenate([F, np.array([f_cand])])\n\n            # determine success: improved global best OR placed among top 25% of finite vals\n            inserted_success = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                inserted_success = True\n                last_improve_iter = it\n                stagnation_counter = 0\n            else:\n                # rank among finite\n                fin_mask = np.isfinite(F)\n                if np.sum(fin_mask) > 0:\n                    rank = np.sum(F[fin_mask] <= f_cand)\n                    if rank <= max(1, int(0.25 * np.sum(fin_mask))):\n                        inserted_success = True\n\n            # update success window\n            success_window.append(1 if inserted_success else 0)\n            if len(success_window) > adapt_window:\n                success_window.pop(0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if it % 7 == 0:\n                recent = np.sum(success_window) / max(1, len(success_window))\n                # If unusually many successes, become slightly more exploratory to capitalize\n                if recent > 0.35:\n                    gscale = max(0.05, gscale * 1.08)\n                # If too few successes -> increase to escape (but not blow up)\n                elif recent < 0.08:\n                    gscale = min(10.0, gscale * 1.18)\n                else:\n                    # slowly decay to refine\n                    gscale = max(1e-5, gscale * 0.985)\n\n                # keep local_scale bounded\n                local_scale = np.clip(local_scale * (0.99 + 0.02 * (0.5 - recent)), 1e-6, 0.5)\n\n            # prune archive to maintain memory budget occasionally\n            if X.shape[0] > self.archive_max * 1.2 or (it % 50 == 0 and X.shape[0] > self.archive_max):\n                finite_idx = np.isfinite(F)\n                fin_indices = np.nonzero(finite_idx)[0]\n                if fin_indices.size > 0:\n                    order = np.argsort(F[fin_indices])\n                    keep_best = int(max(2, fin_indices.size // 2))\n                    keep_idx = fin_indices[order[:keep_best]].tolist()\n                else:\n                    keep_idx = []\n\n                # keep some random others and some infinities to preserve diversity\n                others = list(set(range(X.shape[0])) - set(keep_idx))\n                self.rng.shuffle(others)\n                need = max(0, self.archive_max - len(keep_idx))\n                keep_idx += others[:need]\n                X = X[keep_idx]\n                F = F[keep_idx]\n\n            # stagnation detection & micro-restarts\n            if self.f_opt < np.inf:\n                if self.f_opt < best_since_restart - 1e-12:\n                    best_since_restart = self.f_opt\n            if (it - last_improve_iter) > max(100, 5 * self.dim) and (evals < self.budget):\n                # micro-restart: inject diversified points around best and some uniforms\n                last_improve_iter = it\n                stagnation_counter += 1\n                num_local = min(6 + self.dim // 3, max(3, int(0.05 * self.budget)))\n                # local diversified around best\n                if self.x_opt is not None:\n                    for _ in range(num_local):\n                        perturb = self.rng.randn(self.dim) * (scales * (0.5 + self.rng.rand()))\n                        xnew = self.x_opt + perturb\n                        xnew = self._reflect_bounds(xnew, lb, ub)\n                        if evals < self.budget:\n                            fnew = safe_eval(xnew)\n                            X = np.vstack([X, xnew.reshape(1, -1)])\n                            F = np.concatenate([F, np.array([fnew])])\n                            if np.isfinite(fnew) and fnew < self.f_opt:\n                                self.f_opt = fnew\n                                self.x_opt = xnew.copy()\n                # global uniform injection\n                num_uniform = min(10 + self.dim, max(3, int(0.02 * self.budget)))\n                for _ in range(num_uniform):\n                    if evals >= self.budget:\n                        break\n                    xnew = lb + self.rng.rand(self.dim) * (ub - lb)\n                    fnew = safe_eval(xnew)\n                    X = np.vstack([X, xnew.reshape(1, -1)])\n                    F = np.concatenate([F, np.array([fnew])])\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.f_opt = fnew\n                        self.x_opt = xnew.copy()\n                # encourage exploration after restart\n                gscale = min(5.0, gscale * (1.6 + 0.2 * self.rng.rand()))\n                local_scale = min(0.5, local_scale * (1.4 + 0.2 * self.rng.rand()))\n\n            # safety small pause - continue until budget exhausted\n            # (no sleeping, just loop)\n\n        # final safety: pick best finite point from archive if needed\n        fin_mask = np.isfinite(F)\n        if np.any(fin_mask):\n            idx = np.argmin(F[fin_mask])\n            # map to global index\n            fin_idx = np.nonzero(fin_mask)[0][idx]\n            best_x = X[fin_idx].copy()\n            best_f = float(F[fin_idx])\n            self.f_opt = best_f\n            self.x_opt = best_x\n        else:\n            # fallback: evaluate center if possible\n            if evals < self.budget:\n                f_center = safe_eval(center)\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n            else:\n                # give a uniform sample\n                self.x_opt = lb + self.rng.rand(self.dim) * (ub - lb)\n                self.f_opt = safe_eval(self.x_opt) if evals < self.budget else np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in __call__, the following error occurred:\nNameError: name 'time' is not defined\nOn line: start_time = time.time()", "error": "In the code, line 148, in __call__, the following error occurred:\nNameError: name 'time' is not defined\nOn line: start_time = time.time()", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ca25f5d8-d503-44bb-ad33-785e320de894", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style recombination, PCA-guided elite perturbations, anisotropic Gaussian local search, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = int(archive_max)\n\n        # internal trackers for results\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Default bounds [-5,5] for all dimensions\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        try:\n            # func.bounds can be (lb, ub) or object with .lb/.ub\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if isinstance(b, (tuple, list, np.ndarray)) and len(b) == 2:\n                    lbo = np.asarray(b[0], dtype=float)\n                    ubo = np.asarray(b[1], dtype=float)\n                    if lbo.size == 1:\n                        lbo = np.full(self.dim, float(lbo))\n                    if ubo.size == 1:\n                        ubo = np.full(self.dim, float(ubo))\n                    if lbo.size == self.dim and ubo.size == self.dim:\n                        lb, ub = lbo, ubo\n                else:\n                    if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                        lbo = np.asarray(b.lb, dtype=float)\n                        ubo = np.asarray(b.ub, dtype=float)\n                        if lbo.size == 1:\n                            lbo = np.full(self.dim, float(lbo))\n                        if ubo.size == 1:\n                            ubo = np.full(self.dim, float(ubo))\n                        if lbo.size == self.dim and ubo.size == self.dim:\n                            lb, ub = lbo, ubo\n            else:\n                # maybe func has direct lb/ub\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lbo = np.asarray(func.lb, dtype=float)\n                    ubo = np.asarray(func.ub, dtype=float)\n                    if lbo.size == 1:\n                        lbo = np.full(self.dim, float(lbo))\n                    if ubo.size == 1:\n                        ubo = np.full(self.dim, float(ubo))\n                    if lbo.size == self.dim and ubo.size == self.dim:\n                        lb, ub = lbo, ubo\n        except Exception:\n            # ignore and use defaults\n            pass\n\n        # safety: ensure ub > lb elementwise\n        bad = ub <= lb\n        if np.any(bad):\n            ub[bad] = 5.0\n            lb[bad] = -5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coords repeatedly until within bounds, fallback to clamp\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        u = self.rng.random((n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            steps = (perm + u[:, j]) / n\n            lhs[:, j] = steps\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            r = self.rng.random((n, self.dim))\n            return lb + r * (ub - lb)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # x should be 1D array-like\n            if self.eval_count >= self.budget:\n                return None  # no budget left\n            try:\n                xv = np.asarray(x, dtype=float)\n                fv = float(func(xv))\n            except Exception:\n                # treat exception as a bad (infinite) evaluation but consume budget\n                fv = float(np.inf)\n            self.eval_count += 1\n            return fv\n\n        # initialize archive\n        archive_X = []\n        archive_F = []\n\n        # initial center at midpoint of bounds\n        center = (lb + ub) / 2.0\n\n        # initial LHS in [0,1] mapped to bounds\n        lhs01 = self._lhs01(self.init_samples, self.dim)\n        lhs_pts = lb + lhs01 * (ub - lb)\n\n        # add an additional uniform sample\n        uni = self._uniform_array(lb, ub, n=1)\n\n        # Evaluate center first if budget allows\n        f_center = safe_eval(center)\n        if f_center is not None:\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(lhs_pts.shape[0]):\n            if self.eval_count >= self.budget:\n                break\n            x = lhs_pts[i]\n            f = safe_eval(x)\n            if f is None:\n                break\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.eval_count < self.budget:\n            u = uni[0] if np.ndim(uni) > 1 else uni\n            f = safe_eval(u)\n            if f is not None:\n                archive_X.append(np.asarray(u).copy())\n                archive_F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = np.asarray(u).copy()\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=50)\n        gscale = 0.5  # global scale multiplier (adapted)\n        iter_since_improve = 0\n        iters = 0\n\n        def compute_stats():\n            # compute per-dim robust scale and median from finite archive\n            X = np.asarray(archive_X, dtype=float)\n            F = np.asarray(archive_F, dtype=float)\n            finite_mask = np.isfinite(F)\n            if finite_mask.sum() == 0:\n                median = center.copy()\n                per_dim_scale = (ub - lb) / 4.0\n            else:\n                Xf = X[finite_mask]\n                # median per-dim\n                median = np.median(Xf, axis=0)\n                # IQR per dimension -> approx sigma\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr / 1.349\n                # fallbacks if iqr is zero/small\n                small = per_dim_scale <= 1e-8\n                per_dim_scale[small] = 0.5 * (ub[small] - lb[small]) / 6.0\n            per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-6)\n            return median, per_dim_scale\n\n        # main loop\n        while self.eval_count < self.budget:\n            iters += 1\n\n            # compute stats\n            median, per_dim_scale = compute_stats()\n\n            # prepare sampling base: bias towards better archive items via softmax on -F\n            F_arr = np.asarray(archive_F, dtype=float)\n            X_arr = np.asarray(archive_X, dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n            if finite_idx.size == 0:\n                # random base\n                base = self._uniform_array(lb, ub, n=1)\n            else:\n                # softmax on negatives to bias to better (smaller) F\n                Ff = F_arr[finite_idx]\n                s = -(Ff - np.min(Ff))\n                temp = 0.8 + 2.0 * self.rng.random()  # temperature randomness\n                # normalize s\n                s = s - np.mean(s)\n                logits = s / (np.std(s) + 1e-9)\n                probs = np.exp(logits / temp)\n                probs = probs / (probs.sum() + 1e-12)\n                pick = self.rng.choice(finite_idx, p=probs)\n                base = X_arr[pick].copy()\n\n            # small chance to use current best instead\n            if self.rng.random() < 0.03 and self.x_opt is not None:\n                base = np.asarray(self.x_opt).copy()\n\n            # donor pool for DE-like moves (try archive first, fallback to uniform)\n            donor = None\n            if len(archive_X) >= 4:\n                # pick three distinct donors (indices)\n                idxs = self.rng.choice(len(archive_X), size=3, replace=False)\n                donor = (X_arr[idxs[0]] - X_arr[idxs[1]]) + (X_arr[idxs[2]] - base)\n\n            # choose move type: global vs local\n            move_rand = self.rng.random()\n            is_global = move_rand < (0.25 + 0.1 * (self.f_opt > 1e-3))  # more global if poor\n\n            # prepare tempered cauchy\n            cauchy_raw = self.rng.standard_cauchy(self.dim)\n            # temper extremes by sigmoid-like squashing\n            c_small = np.tanh(np.clip(cauchy_raw, -10.0, 10.0))  # in (-1,1)\n            # scale for local gaussian\n            local_sigma = 0.6 * per_dim_scale * (0.5 + self.rng.random(self.dim))\n\n            # produce candidate\n            if is_global:\n                # GLOBAL move: anchor on an elite or uniformly sample anchor\n                if finite_idx.size > 0:\n                    top_k = max(1, int(max(1, finite_idx.size * 0.1)))\n                    order = np.argsort(F_arr[finite_idx])\n                    elite_idx = finite_idx[order[:top_k]]\n                    anchor = X_arr[self.rng.choice(elite_idx)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n\n                # make a tempered cauchy jump scaled by per-dim scale and gscale\n                scale_vec = per_dim_scale * (0.6 + 1.2 * self.rng.random(self.dim))\n                F_scale = 0.6 * (1.0 + self.rng.random())\n                cand = anchor + F_scale * gscale * c_small * scale_vec\n\n                # occasional DE-like differential from donors\n                if donor is not None and self.rng.random() < 0.28:\n                    cand = cand + (0.5 + self.rng.random() * 0.8) * gscale * donor * (0.5 + self.rng.random())\n\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.12:\n                    mask = self.rng.random(self.dim) < 0.15\n                    if mask.any():\n                        cand[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n\n                # small extra global jitter\n                if self.rng.random() < 0.08:\n                    cand = cand + self.rng.normal(scale=0.2 * per_dim_scale * (1.0 + self.rng.random(self.dim)))\n\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                cand = base + self.rng.normal(scale=local_sigma)\n\n                # attract toward median occasionally\n                if self.rng.random() < 0.18:\n                    alpha = 0.1 + 0.45 * self.rng.random()\n                    cand = cand * (1.0 - alpha) + median * alpha\n\n                # PCA-guided elite perturbation occasionally\n                if finite_idx.size >= 4 and self.rng.random() < 0.38:\n                    k = min(8, max(4, finite_idx.size // 2))\n                    elite_inds = finite_idx[np.argsort(F_arr[finite_idx])[:k]]\n                    E = X_arr[elite_inds] - np.mean(X_arr[elite_inds], axis=0)\n                    try:\n                        # compute principal components via SVD\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        pc = Vt[0]\n                        pc_spread = S[0] / max(1e-12, np.mean(S))\n                        step_mag = (0.4 + 1.2 * self.rng.random()) * pc_spread * per_dim_scale * gscale\n                        direction = 1.0 if self.rng.random() < 0.6 else -1.0\n                        cand = cand + direction * pc * step_mag\n                    except Exception:\n                        pass\n\n                # small DE-like mixing inside local branch sometimes\n                if donor is not None and self.rng.random() < 0.12:\n                    cand = cand + 0.35 * gscale * donor * (0.4 + self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.05:\n                    cand = cand + 0.7 * gscale * c_small * per_dim_scale * (0.6 + self.rng.random(self.dim))\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.count_nonzero(bad)) * (ub[bad] - lb[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            if f_cand is None:\n                break\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # determine success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                success = True\n                iter_since_improve = 0\n            else:\n                # rank among finite\n                F_arr = np.asarray(archive_F, dtype=float)\n                finite_mask = np.isfinite(F_arr)\n                if finite_mask.sum() > 0:\n                    combined = np.sort(F_arr[finite_mask])\n                    rank = np.searchsorted(combined, f_cand, side='left')\n                    success = (rank < max(1, len(combined) // 4))\n                else:\n                    success = False\n                iter_since_improve += 1\n\n            success_window.append(1 if success else 0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if len(success_window) == success_window.maxlen:\n                srate = sum(success_window) / len(success_window)\n                # If unusually many successes, become slightly more exploratory to capitalize\n                if srate > 0.55:\n                    gscale = min(5.0, gscale * (1.0 + 0.25 * (srate - 0.5)))\n                # If too few successes -> increase to escape (but not blow up)\n                elif srate < 0.08:\n                    gscale = min(10.0, gscale * (1.0 + 2.0 * (0.08 - srate)))\n                else:\n                    # slowly decay to refine\n                    gscale = max(1e-3, gscale * 0.995)\n\n            # prune archive to maintain memory budget occasionally\n            if len(archive_X) > self.archive_max:\n                F_arr = np.asarray(archive_F)\n                finite_idx = np.where(np.isfinite(F_arr))[0]\n                keep = set()\n                if finite_idx.size > 0:\n                    order = np.argsort(F_arr[finite_idx])\n                    keep_best = finite_idx[order[: max(1, finite_idx.size // 2)]].tolist()\n                    keep.update(keep_best)\n                # keep some random others to maintain diversity (including infinities)\n                rest = [i for i in range(len(archive_X)) if i not in keep]\n                if rest:\n                    keep_rand = self.rng.choice(rest, size=min(len(rest), self.archive_max - len(keep)), replace=False).tolist()\n                    keep.update(keep_rand)\n                # rebuild archives\n                archive_X = [archive_X[i] for i in range(len(archive_X)) if i in keep]\n                archive_F = [archive_F[i] for i in range(len(archive_F)) if i in keep]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 2 * self.dim):\n                # micro-restart: inject diversified points around best and uniformly\n                iter_since_improve = 0\n                num_inject = min(10 + self.dim, self.budget - self.eval_count)\n                for k in range(num_inject):\n                    if self.eval_count >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.75:\n                        # local diversified around best\n                        perturb = self.rng.normal(scale=per_dim_scale * (0.2 + 0.8 * self.rng.random(self.dim)))\n                        newx = self.x_opt + perturb\n                    else:\n                        # global uniform injection\n                        newx = self._uniform_array(lb, ub, n=1)\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    fnew = safe_eval(newx)\n                    if fnew is None:\n                        break\n                    archive_X.append(newx.copy())\n                    archive_F.append(fnew)\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.f_opt = fnew\n                        self.x_opt = newx.copy()\n                # encourage exploration after restart\n                gscale = max(gscale, 1.0 + 0.5 * self.rng.random())\n\n        # final safety: pick best finite point from archive if needed\n        if (self.x_opt is None or not np.isfinite(self.f_opt)) and len(archive_X) > 0:\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best = finite_idx[np.argmin(F_arr[finite_idx])]\n                self.x_opt = np.asarray(archive_X[best]).copy()\n                self.f_opt = float(archive_F[best])\n            else:\n                # fallback: evaluate center if possible\n                if self.eval_count < self.budget:\n                    f_center = safe_eval(center)\n                    if f_center is not None and np.isfinite(f_center):\n                        self.x_opt = center.copy()\n                        self.f_opt = f_center\n\n        # final fallback: if still no finite solution, return best known (maybe inf)\n        if self.x_opt is None:\n            # create a random vector within bounds as output\n            self.x_opt = self._uniform_array(lb, ub, n=1)\n            self.f_opt = float(np.inf)\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 175, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "error": "In the code, line 175, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "985f9f06-91b6-4a7b-89fb-b6a89bac1e90", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = max(10, int(archive_max))\n\n        # runtime trackers\n        self.eval_count = 0\n        self.x_opt = None\n        self.f_opt = float(np.inf)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # default bounds [-5,5]\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # case (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lbo = np.asarray(b[0], dtype=float)\n                    ubo = np.asarray(b[1], dtype=float)\n                    if lbo.size == 1:\n                        lbo = np.full(self.dim, float(lbo))\n                    if ubo.size == 1:\n                        ubo = np.full(self.dim, float(ubo))\n                    if lbo.size == self.dim and ubo.size == self.dim:\n                        lb, ub = lbo, ubo\n                else:\n                    # case object with .lb and .ub\n                    if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                        lbo = np.asarray(b.lb, dtype=float)\n                        ubo = np.asarray(b.ub, dtype=float)\n                        if lbo.size == 1:\n                            lbo = np.full(self.dim, float(lbo))\n                        if ubo.size == 1:\n                            ubo = np.full(self.dim, float(ubo))\n                        if lbo.size == self.dim and ubo.size == self.dim:\n                            lb, ub = lbo, ubo\n            else:\n                # maybe direct attributes func.lb/func.ub\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lbo = np.asarray(func.lb, dtype=float)\n                    ubo = np.asarray(func.ub, dtype=float)\n                    if lbo.size == 1:\n                        lbo = np.full(self.dim, float(lbo))\n                    if ubo.size == 1:\n                        ubo = np.full(self.dim, float(ubo))\n                    if lbo.size == self.dim and ubo.size == self.dim:\n                        lb, ub = lbo, ubo\n        except Exception:\n            # ignore and use defaults\n            pass\n\n        # safety: ensure ub > lb elementwise\n        bad = ub <= lb\n        if np.any(bad):\n            ub = ub.copy()\n            lb = lb.copy()\n            ub[bad] = 5.0\n            lb[bad] = -5.0\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coords repeatedly until within bounds, fallback to clamp\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        dim = int(dim)\n        # create strata positions\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.random((n, dim))\n        lhs = np.zeros((n, dim), dtype=float)\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            # sample uniformly inside each stratum\n            lhs[:, j] = cut[:-1][perm] + u[:, j] * (1.0 / n)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            r = self.rng.random(self.dim)\n            return lb + r * (ub - lb)\n        else:\n            r = self.rng.random((int(n), self.dim))\n            return lb + r * (ub - lb)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                return None\n            try:\n                xv = np.asarray(x, dtype=float)\n                # ensure 1D\n                xv = xv.reshape(self.dim)\n                fv = float(func(xv))\n            except Exception:\n                fv = float(np.inf)\n            self.eval_count += 1\n            return fv\n\n        # initialize\n        lb, ub = self._get_bounds(func)\n        self.eval_count = 0\n        self.x_opt = None\n        self.f_opt = float(np.inf)\n\n        archive_X = []\n        archive_F = []\n\n        # initial center at midpoint of bounds\n        center = (lb + ub) / 2.0\n\n        # initial LHS in [0,1] mapped to bounds\n        lhs01 = self._lhs01(self.init_samples, self.dim)\n        lhs_pts = lb + lhs01 * (ub - lb)\n\n        # add an additional uniform sample\n        uni = self._uniform_array(lb, ub, n=5)  # a few to pick from\n\n        # Evaluate center first if budget allows\n        f_center = safe_eval(center)\n        if f_center is not None:\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(lhs_pts.shape[0]):\n            if self.eval_count >= self.budget:\n                break\n            x = lhs_pts[i]\n            f = safe_eval(x)\n            if f is None:\n                break\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.eval_count < self.budget:\n            u = uni[self.rng.integers(len(uni))]\n            f = safe_eval(u)\n            if f is not None:\n                archive_X.append(np.asarray(u).copy())\n                archive_F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = np.asarray(u).copy()\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=50)\n        gscale = 0.5  # global scale multiplier (adapted)\n        iter_since_improve = 0\n        iters = 0\n\n        def compute_stats(X_arr, F_arr):\n            finite_mask = np.isfinite(F_arr)\n            if finite_mask.sum() == 0:\n                median = center.copy()\n                per_dim_scale = (ub - lb) / 4.0\n                return median, per_dim_scale\n            Xf = X_arr[finite_mask]\n            median = np.median(Xf, axis=0)\n            q25 = np.percentile(Xf, 25, axis=0)\n            q75 = np.percentile(Xf, 75, axis=0)\n            iqr = q75 - q25\n            per_dim_scale = iqr / 1.349  # approximate sigma from IQR\n            # fallbacks if iqr is zero/small\n            small = per_dim_scale <= 1e-8\n            per_dim_scale[small] = 0.5 * (ub[small] - lb[small]) / 6.0\n            per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-8)\n            return median, per_dim_scale\n\n        # main loop\n        while self.eval_count < self.budget:\n            iters += 1\n\n            # build arrays\n            F_arr = np.asarray(archive_F, dtype=float) if archive_F else np.array([], dtype=float)\n            X_arr = np.asarray(archive_X, dtype=float) if archive_X else np.zeros((0, self.dim), dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n\n            # compute statistics for sampling scales\n            median, per_dim_scale = compute_stats(X_arr, F_arr)\n\n            # prepare sampling base: bias towards better archive items via softmax on -F\n            if finite_idx.size == 0:\n                base = self._uniform_array(lb, ub, n=1)\n            else:\n                Ff = F_arr[finite_idx]\n                # create scores: smaller F => larger score\n                s = -(Ff - np.min(Ff))\n                temp = 0.6 + 1.6 * self.rng.random()  # temperature randomness\n                s = s - np.mean(s)\n                denom = (np.std(s) + 1e-9)\n                logits = s / denom\n                probs = np.exp(logits / temp)\n                probs = probs / (probs.sum() + 1e-12)\n                pick = finite_idx[self.rng.choice(len(finite_idx), p=probs)]\n                base = X_arr[pick].copy()\n\n            # small chance to use current best instead\n            if (self.x_opt is not None) and (self.rng.random() < 0.12):\n                base = np.asarray(self.x_opt).copy()\n\n            # donor pool for DE-like moves (try archive first, fallback to uniform)\n            donor = None\n            if len(archive_X) >= 4:\n                # pick three distinct donors (indices)\n                idxs = self.rng.choice(len(archive_X), size=3, replace=False)\n                donor = (X_arr[idxs[0]] - X_arr[idxs[1]]) + (X_arr[idxs[2]] - base)\n\n            # choose move type: global vs local\n            move_rand = self.rng.random()\n            is_global = move_rand < (0.25 + 0.1 * float(self.f_opt > 1e-3))  # more global if poor\n\n            # prepare tempered cauchy\n            cauchy_raw = self.rng.standard_cauchy(self.dim)\n            # temper extremes by tanh squashing\n            c_small = np.tanh(np.clip(cauchy_raw, -10.0, 10.0))  # in (-1,1)\n\n            # scale for local gaussian\n            local_sigma = 0.6 * per_dim_scale * (0.5 + self.rng.random(self.dim))\n\n            # produce candidate\n            if is_global:\n                # GLOBAL move: anchor on an elite or uniformly sample anchor\n                if finite_idx.size >= 4 and self.rng.random() < 0.82:\n                    # pick among top-k elites\n                    k = max(1, int(max(1, finite_idx.size * 0.12)))\n                    order = np.argsort(F_arr[finite_idx])\n                    elite_idx = finite_idx[order[:k]]\n                    anchor = X_arr[self.rng.choice(elite_idx)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n\n                # make a tempered cauchy jump scaled by per-dim scale and gscale\n                scale_vec = per_dim_scale * (0.6 + self.rng.random(self.dim) * 1.2)\n                F_scale = 0.6 * (1.0 + self.rng.random())\n                cand = anchor + F_scale * gscale * c_small * scale_vec\n\n                # occasional DE-like differential from donors\n                if donor is not None and self.rng.random() < 0.32:\n                    cand = cand + (0.5 + self.rng.random() * 0.8) * gscale * donor * (0.5 + self.rng.random())\n\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.12:\n                    mask = self.rng.random(self.dim) < 0.15\n                    if mask.any():\n                        cand[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n\n                # small extra global jitter\n                if self.rng.random() < 0.10:\n                    cand = cand + self.rng.normal(scale=0.25 * per_dim_scale * (1.0 + self.rng.random(self.dim)))\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                cand = base + self.rng.normal(scale=local_sigma)\n\n                # attract toward median occasionally\n                if self.rng.random() < 0.18 and finite_idx.size > 0:\n                    alpha = 0.1 + 0.45 * self.rng.random()\n                    cand = cand * (1.0 - alpha) + median * alpha\n\n                # PCA-guided elite perturbation occasionally\n                if finite_idx.size >= 4 and self.rng.random() < 0.38:\n                    k = min(8, max(4, finite_idx.size // 2))\n                    elite_inds = finite_idx[np.argsort(F_arr[finite_idx])[:k]]\n                    E = X_arr[elite_inds] - np.mean(X_arr[elite_inds], axis=0)\n                    try:\n                        # compute principal components via SVD\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        pc = Vt[0]\n                        # scale along PC by its spread and per-dim scale\n                        pc_spread = (S[0] / max(1e-12, np.mean(S))) if S.size > 0 else 1.0\n                        step_mag = (0.4 + 1.2 * self.rng.random()) * pc_spread * per_dim_scale * gscale\n                        direction = 1.0 if self.rng.random() < 0.6 else -1.0\n                        cand = cand + direction * pc * step_mag\n                    except Exception:\n                        pass\n\n                # small DE-like mixing inside local branch sometimes\n                if donor is not None and self.rng.random() < 0.12:\n                    cand = cand + 0.35 * gscale * donor * (0.4 + self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.05:\n                    scale_vec = per_dim_scale * (0.6 + self.rng.random(self.dim))\n                    cand = cand + 0.9 * gscale * c_small * scale_vec\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.count_nonzero(bad)) * (ub[bad] - lb[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            if f_cand is None:\n                break\n\n            # append to archive\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # determine success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                success = True\n                iter_since_improve = 0\n            else:\n                # rank among finite\n                F_arr = np.asarray(archive_F, dtype=float)\n                finite_mask = np.isfinite(F_arr)\n                nf = finite_mask.sum()\n                if nf > 0 and np.isfinite(f_cand):\n                    combined = np.sort(F_arr[finite_mask])\n                    rank = np.searchsorted(combined, f_cand, side='left')\n                    if rank < max(1, int(0.25 * nf)):\n                        success = True\n                iter_since_improve += 1\n\n            success_window.append(1 if success else 0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if len(success_window) == success_window.maxlen:\n                srate = sum(success_window) / len(success_window)\n                # If unusually many successes, become slightly more exploratory to capitalize\n                if srate > 0.55:\n                    gscale = min(5.0, gscale * (1.0 + 0.25 * (srate - 0.5)))\n                # If too few successes -> slowly decay to refine\n                else:\n                    gscale = max(1e-3, gscale * 0.995)\n\n            # prune archive to maintain memory budget occasionally\n            if len(archive_X) > self.archive_max * 1.2:\n                F_arr = np.asarray(archive_F, dtype=float)\n                finite_mask = np.isfinite(F_arr)\n                finite_idx = np.where(finite_mask)[0]\n                keep = set()\n                if finite_idx.size > 0:\n                    order = np.argsort(F_arr[finite_idx])\n                    keep_best = finite_idx[order[: max(1, finite_idx.size // 2)]].tolist()\n                    keep.update(keep_best)\n                # keep some random others to maintain diversity (including infinities)\n                rest = [i for i in range(len(archive_X)) if i not in keep]\n                num_keep_rand = max(0, self.archive_max - len(keep))\n                if rest and num_keep_rand > 0:\n                    sampled = list(self.rng.choice(rest, size=min(num_keep_rand, len(rest)), replace=False))\n                    keep.update(sampled)\n                # rebuild archives\n                keep_list = sorted(list(keep))\n                archive_X = [archive_X[i] for i in keep_list]\n                archive_F = [archive_F[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 4 * self.dim):\n                # micro-restart: inject diversified points around best and uniformly\n                iter_since_improve = 0\n                num_inject = min(12 + self.dim, max(1, self.budget - self.eval_count))\n                for k in range(num_inject):\n                    if self.eval_count >= self.budget:\n                        break\n                    if (self.x_opt is not None) and (self.rng.random() < 0.75):\n                        # local diversified around best\n                        perturb = self.rng.normal(scale=per_dim_scale * (0.2 + 0.8 * self.rng.random(self.dim)))\n                        newx = self.x_opt + perturb\n                        # occasionally mix some uniform coordinates\n                        if self.rng.random() < 0.18:\n                            mask = self.rng.random(self.dim) < 0.12\n                            if mask.any():\n                                newx[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n                    else:\n                        # global uniform injection\n                        newx = self._uniform_array(lb, ub, n=1)\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    fnew = safe_eval(newx)\n                    if fnew is None:\n                        break\n                    archive_X.append(newx.copy())\n                    archive_F.append(fnew)\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.f_opt = fnew\n                        self.x_opt = newx.copy()\n                # encourage exploration after restart\n                gscale = max(gscale, 1.0 + 0.5 * self.rng.random())\n\n        # final safety: pick best finite point from archive if needed\n        if (self.x_opt is None or not np.isfinite(self.f_opt)) and len(archive_X) > 0:\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best = finite_idx[np.argmin(F_arr[finite_idx])]\n                self.x_opt = np.asarray(archive_X[best]).copy()\n                self.f_opt = float(F_arr[best])\n            else:\n                # fallback: evaluate center if possible\n                if self.eval_count < self.budget:\n                    f_center = safe_eval(center)\n                    if f_center is not None and np.isfinite(f_center):\n                        self.x_opt = center.copy()\n                        self.f_opt = f_center\n\n        # final fallback: if still no finite solution, return random within bounds\n        if self.x_opt is None:\n            self.x_opt = self._uniform_array(lb, ub, n=1)\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 186, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "error": "In the code, line 186, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "845ea3e6-0f0a-4c43-8195-8ff85788b9e2", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = max(10, int(archive_max))\n        self.init_samples = max(4, int(init_samples))\n        self.seed = seed\n\n        # runtime trackers\n        self.eval_count = 0\n        self.x_opt = None\n        self.f_opt = float(np.inf)\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # default bounds [-5,5]\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # case (lb, ub) as tuples/lists/arrays\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lbo = np.asarray(b[0], dtype=float)\n                    ubo = np.asarray(b[1], dtype=float)\n                    if lbo.size == 1:\n                        lb = np.full(self.dim, float(lbo))\n                    elif lbo.size == self.dim:\n                        lb = lbo.copy()\n                    if ubo.size == 1:\n                        ub = np.full(self.dim, float(ubo))\n                    elif ubo.size == self.dim:\n                        ub = ubo.copy()\n                else:\n                    # case object with .lb and .ub\n                    if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                        lbo = np.asarray(b.lb, dtype=float)\n                        ubo = np.asarray(b.ub, dtype=float)\n                        if lbo.size == 1:\n                            lb = np.full(self.dim, float(lbo))\n                        elif lbo.size == self.dim:\n                            lb = lbo.copy()\n                        if ubo.size == 1:\n                            ub = np.full(self.dim, float(ubo))\n                        elif ubo.size == self.dim:\n                            ub = ubo.copy()\n            else:\n                # maybe direct attributes func.lb/func.ub\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lbo = np.asarray(func.lb, dtype=float)\n                    ubo = np.asarray(func.ub, dtype=float)\n                    if lbo.size == 1:\n                        lb = np.full(self.dim, float(lbo))\n                    elif lbo.size == self.dim:\n                        lb = lbo.copy()\n                    if ubo.size == 1:\n                        ub = np.full(self.dim, float(ubo))\n                    elif ubo.size == self.dim:\n                        ub = ubo.copy()\n        except Exception:\n            # ignore and use defaults\n            pass\n\n        # safety: ensure ub > lb elementwise\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape != (self.dim,):\n            lb = np.full(self.dim, -5.0, dtype=float)\n        if ub.shape != (self.dim,):\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # if any invalid, fix\n        bad = ~(np.isfinite(lb) & np.isfinite(ub) & (ub > lb))\n        if np.any(bad):\n            ub = ub.copy()\n            lb = lb.copy()\n            lb[bad] = -5.0\n            ub[bad] = 5.0\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coords repeatedly until within bounds, fallback to clamp\n        x = np.asarray(x, dtype=float).copy()\n        # iterative reflection up to a few times\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = 2*lb - x ; reflect above: x = 2*ub - x\n            x[below] = 2.0 * lb[below] - x[below]\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        dim = int(dim)\n        if n <= 0:\n            return np.zeros((0, dim), dtype=float)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        lhs = np.zeros((n, dim), dtype=float)\n        u = self.rng.random((n, dim))\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            lhs[:, j] = cut[:-1][perm] + u[:, j] * (1.0 / n)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.random((n, self.dim)) * (ub - lb)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                return None\n            try:\n                xv = np.asarray(x, dtype=float).reshape(self.dim)\n                fv = float(func(xv))\n            except Exception:\n                fv = float(np.inf)\n            self.eval_count += 1\n            return fv\n\n        # initialize\n        lb, ub = self._get_bounds(func)\n        self.eval_count = 0\n        self.x_opt = None\n        self.f_opt = float(np.inf)\n\n        archive_X = []\n        archive_F = []\n\n        # initial center at midpoint of bounds\n        center = (lb + ub) / 2.0\n\n        # initial LHS in [0,1] mapped to bounds\n        n_lhs = max(self.init_samples, min(2 * self.dim, self.budget))\n        lhs01 = self._lhs01(n_lhs, self.dim)\n        lhs_pts = lb + lhs01 * (ub - lb)\n\n        # add an additional uniform sample set\n        uni = self._uniform_array(lb, ub, n=min(8, max(4, self.init_samples)))\n\n        # Evaluate center first if budget allows\n        f_center = safe_eval(center)\n        if f_center is not None:\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(lhs_pts.shape[0]):\n            if self.eval_count >= self.budget:\n                break\n            x = lhs_pts[i]\n            f = safe_eval(x)\n            if f is None:\n                break\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.eval_count < self.budget:\n            u = uni[self.rng.integers(len(uni))]\n            f = safe_eval(u)\n            if f is not None:\n                archive_X.append(u.copy())\n                archive_F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = u.copy()\n\n        # short-term adaptation and control variables\n        gscale = 0.6  # global scale multiplier (adapted)\n        iter_since_improve = 0\n        iters = 0\n\n        # success window for adaptation\n        success_window = deque(maxlen=48)\n\n        def compute_stats(X_arr, F_arr):\n            # returns median and per-dim scale estimate (sigma-like)\n            if X_arr.shape[0] == 0 or F_arr.size == 0:\n                median = (lb + ub) / 2.0\n                per_dim_scale = (ub - lb) / 6.0\n                per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-8)\n                return median, per_dim_scale\n\n            finite_mask = np.isfinite(F_arr)\n            if finite_mask.sum() == 0:\n                median = (lb + ub) / 2.0\n                per_dim_scale = (ub - lb) / 6.0\n                per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-8)\n                return median, per_dim_scale\n\n            Xf = X_arr[finite_mask]\n            median = np.median(Xf, axis=0)\n            q25 = np.percentile(Xf, 25, axis=0)\n            q75 = np.percentile(Xf, 75, axis=0)\n            iqr = q75 - q25\n            per_dim_scale = iqr / 1.349  # approximate sigma from IQR\n            # fallbacks if iqr is zero/small\n            small = per_dim_scale <= 1e-8\n            per_dim_scale[small] = 0.5 * (ub[small] - lb[small]) / 6.0\n            per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-8)\n            return median, per_dim_scale\n\n        # main loop\n        while self.eval_count < self.budget:\n            iters += 1\n\n            # build arrays\n            F_arr = np.asarray(archive_F, dtype=float) if archive_F else np.array([], dtype=float)\n            X_arr = np.asarray(archive_X, dtype=float) if archive_X else np.zeros((0, self.dim), dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n\n            # compute statistics for sampling scales\n            median, per_dim_scale = compute_stats(X_arr, F_arr)\n\n            # prepare sampling base: bias towards better archive items via softmax on -F\n            base = None\n            donor = None\n            if finite_idx.size == 0:\n                # no finite info, sample uniform\n                base = self._uniform_array(lb, ub, n=1)\n            else:\n                Ff = F_arr[finite_idx]\n                # convert to positive scores: lower F => higher score\n                scores = np.max(Ff) - Ff + 1e-12\n                # temperature randomness to diversify\n                temp = 0.5 + self.rng.random() * 1.5\n                ex = np.exp(scores / (np.mean(scores + 1e-12) * temp + 1e-12))\n                probs = ex / (np.sum(ex) + 1e-12)\n                pick = finite_idx[self.rng.choice(len(finite_idx), p=probs)]\n                base = X_arr[pick].copy()\n\n            # small chance to use current best instead\n            if (self.x_opt is not None) and (self.rng.random() < 0.12):\n                base = np.asarray(self.x_opt).copy()\n\n            # donor pool for DE-like moves (try archive first, fallback to uniform)\n            if len(archive_X) >= 4:\n                try:\n                    idxs = self.rng.choice(len(archive_X), size=3, replace=False)\n                    donor = (X_arr[idxs[0]] - X_arr[idxs[1]]) + (X_arr[idxs[2]] - base)\n                except Exception:\n                    donor = None\n            else:\n                donor = None\n\n            # choose move type: global vs local\n            # adapt p_global based on gscale (more exploration when gscale large)\n            p_global = 0.22 + 0.5 * min(1.0, gscale / 1.5)\n            is_global = self.rng.random() < p_global\n\n            # prepare tempered cauchy\n            c_small = np.tanh(self.rng.standard_cauchy(self.dim) * 0.8)\n\n            # scale for local gaussian\n            local_sigma = 0.6 * per_dim_scale * (0.4 + self.rng.random(self.dim) * 1.2)\n\n            # produce candidate\n            cand = None\n            if is_global:\n                # GLOBAL move: anchor on an elite or uniformly sample anchor\n                if finite_idx.size >= 4 and self.rng.random() < 0.82:\n                    # pick among top-k elites by fitness\n                    k = max(1, int(max(1, finite_idx.size * 0.12)))\n                    order = np.argsort(F_arr[finite_idx])\n                    elite_idx = finite_idx[order[:max(1, k)]]\n                    anchor = X_arr[self.rng.choice(elite_idx)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n\n                # make a tempered cauchy jump scaled by per-dim scale and gscale\n                scale_vec = per_dim_scale * (0.6 + self.rng.random(self.dim) * 1.2)\n                F_scale = 0.6 * (0.8 + self.rng.random() * 1.6)\n                cand = anchor + F_scale * gscale * c_small * scale_vec\n\n                # occasional DE-like differential from donors\n                if donor is not None and self.rng.random() < 0.32:\n                    cand = cand + (0.4 + self.rng.random() * 0.9) * gscale * donor * (0.5 + self.rng.random())\n\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.12:\n                    mask = self.rng.random(self.dim) < 0.15\n                    if mask.any():\n                        cand[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n\n                # small extra global jitter\n                if self.rng.random() < 0.18:\n                    cand = cand + 0.08 * gscale * self.rng.normal(scale=per_dim_scale)\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                cand = base + self.rng.normal(scale=local_sigma)\n\n                # attract toward median occasionally\n                if self.rng.random() < 0.18 and finite_idx.size > 0:\n                    alpha = 0.12 + self.rng.random() * 0.28\n                    cand = cand * (1.0 - alpha) + median * alpha\n\n                # PCA-guided elite perturbation occasionally\n                if finite_idx.size >= 4 and self.rng.random() < 0.38:\n                    k = max(2, min(8, int(max(2, finite_idx.size * 0.12))))\n                    elite_inds = finite_idx[np.argsort(F_arr[finite_idx])[:k]]\n                    E = X_arr[elite_inds] - np.mean(X_arr[elite_inds], axis=0)\n                    try:\n                        # compute principal components via SVD\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        pc = Vt[0]  # principal direction\n                        pc_spread = max(1e-8, S[0] / max(1, E.shape[0] - 1))\n                        step_mag = (0.4 + 1.2 * self.rng.random()) * pc_spread * per_dim_scale.mean() * gscale\n                        direction = 1.0 if self.rng.random() < 0.6 else -1.0\n                        cand = cand + direction * step_mag * pc\n                    except Exception:\n                        pass\n\n                # small DE-like mixing inside local branch sometimes\n                if donor is not None and self.rng.random() < 0.28:\n                    cand = cand + 0.35 * gscale * donor * (0.4 + self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.05:\n                    scale_vec = per_dim_scale * (0.8 + self.rng.random(self.dim) * 0.9)\n                    cand = cand + 0.9 * gscale * c_small * scale_vec\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.count_nonzero(bad)) * (ub[bad] - lb[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            if f_cand is None:\n                break\n\n            # append to archive\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # determine success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                success = True\n                iter_since_improve = 0\n            else:\n                # rank among finite\n                finite_mask = np.isfinite(F_arr)\n                if finite_mask.sum() > 0 and np.isfinite(f_cand):\n                    combined = np.sort(F_arr[finite_mask])\n                    rank = np.searchsorted(combined, f_cand, side='left')\n                    if rank < max(1, int(0.25 * combined.size)):\n                        success = True\n                iter_since_improve += 1\n\n            success_window.append(1 if success else 0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if len(success_window) == success_window.maxlen:\n                srate = sum(success_window) / len(success_window)\n                # If unusually many successes, become slightly more exploratory to capitalize\n                if srate > 0.55:\n                    gscale = min(6.0, gscale * (1.0 + 0.25 * (srate - 0.5)))\n                # If too few successes -> slowly decay to refine\n                elif srate < 0.08:\n                    gscale = max(0.06, gscale * (0.9 - 0.5 * (0.08 - srate)))\n                else:\n                    # gentle relaxation toward 1.0\n                    gscale = 0.98 * gscale + 0.02 * 1.0\n\n            # prune archive to maintain memory budget occasionally\n            if len(archive_X) > self.archive_max * 1.2:\n                F_arr = np.asarray(archive_F, dtype=float)\n                finite_mask = np.isfinite(F_arr)\n                finite_idx = np.where(finite_mask)[0]\n                keep = set()\n                num_keep_best = max(3, int(self.archive_max * 0.4))\n                if finite_idx.size > 0:\n                    order = np.argsort(F_arr[finite_idx])\n                    best_idx = finite_idx[order[:min(len(order), num_keep_best)]]\n                    keep.update(best_idx.tolist())\n                # keep some random others to maintain diversity (including infinities)\n                rest = [i for i in range(len(archive_X)) if i not in keep]\n                num_keep_rand = max(0, self.archive_max - len(keep))\n                if rest and num_keep_rand > 0:\n                    sampled = list(self.rng.choice(rest, size=min(num_keep_rand, len(rest)), replace=False))\n                    keep.update(sampled)\n                keep_list = sorted(list(keep))\n                archive_X = [archive_X[i] for i in keep_list]\n                archive_F = [archive_F[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(60, 6 * self.dim) and self.eval_count < self.budget:\n                # micro-restart: inject diversified points around best and uniformly\n                iter_since_improve = 0\n                num_inject = min(12 + self.dim, max(1, self.budget - self.eval_count))\n                for _ in range(num_inject):\n                    if self.eval_count >= self.budget:\n                        break\n                    if (self.x_opt is not None) and (self.rng.random() < 0.75):\n                        # local diversified around best\n                        perturb = self.rng.normal(scale=per_dim_scale * (0.8 + self.rng.random() * 1.6))\n                        newx = self.x_opt + perturb\n                        # occasionally mix some uniform coordinates\n                        if self.rng.random() < 0.22:\n                            mask = self.rng.random(self.dim) < 0.12\n                            if mask.any():\n                                newx[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n                    else:\n                        # global uniform injection\n                        newx = self._uniform_array(lb, ub, n=1)\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    fnew = safe_eval(newx)\n                    if fnew is None:\n                        break\n                    archive_X.append(newx.copy())\n                    archive_F.append(fnew)\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.f_opt = fnew\n                        self.x_opt = newx.copy()\n                # encourage exploration after restart\n                gscale = max(gscale, 1.0 + 0.5 * self.rng.random())\n\n        # final safety: pick best finite point from archive if needed\n        if (self.x_opt is None or not np.isfinite(self.f_opt)) and len(archive_X) > 0:\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best = finite_idx[np.argmin(F_arr[finite_idx])]\n                self.x_opt = np.asarray(archive_X[best]).copy()\n                self.f_opt = float(F_arr[best])\n            else:\n                # fallback: evaluate center if possible\n                if self.eval_count < self.budget:\n                    f_center = safe_eval(center)\n                    if f_center is not None and np.isfinite(f_center):\n                        self.x_opt = center.copy()\n                        self.f_opt = f_center\n\n        # final fallback: if still no finite solution, return random within bounds\n        if self.x_opt is None:\n            self.x_opt = self._uniform_array(lb, ub, n=1)\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 205, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=48)", "error": "In the code, line 205, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=48)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "13e15322-f4d7-46ec-ac19-f82a04a1058b", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = max(10, int(archive_max))\n\n        # runtime trackers\n        self.eval_count = 0\n        self.x_opt = None\n        self.f_opt = float(np.inf)\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # default bounds [-5,5]\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n\n        # try func.bounds as (lb, ub) or object with .lb and .ub,\n        # then try func.lb / func.ub attributes\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lbo = np.asarray(b[0], dtype=float)\n                    ubo = np.asarray(b[1], dtype=float)\n                    if lbo.size == 1:\n                        lbo = np.full(self.dim, float(lbo))\n                    if ubo.size == 1:\n                        ubo = np.full(self.dim, float(ubo))\n                    if lbo.size == self.dim and ubo.size == self.dim:\n                        lb, ub = lbo.copy(), ubo.copy()\n                else:\n                    # maybe object with .lb and .ub\n                    if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                        lbo = np.asarray(b.lb, dtype=float)\n                        ubo = np.asarray(b.ub, dtype=float)\n                        if lbo.size == 1:\n                            lbo = np.full(self.dim, float(lbo))\n                        if ubo.size == 1:\n                            ubo = np.full(self.dim, float(ubo))\n                        if lbo.size == self.dim and ubo.size == self.dim:\n                            lb, ub = lbo.copy(), ubo.copy()\n            # maybe direct attributes func.lb/func.ub\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                lbo = np.asarray(func.lb, dtype=float)\n                ubo = np.asarray(func.ub, dtype=float)\n                if lbo.size == 1:\n                    lbo = np.full(self.dim, float(lbo))\n                if ubo.size == 1:\n                    ubo = np.full(self.dim, float(ubo))\n                if lbo.size == self.dim and ubo.size == self.dim:\n                    lb, ub = lbo.copy(), ubo.copy()\n        except Exception:\n            # ignore and use defaults\n            pass\n\n        # safety: ensure ub > lb elementwise\n        bad = ~(np.isfinite(lb) & np.isfinite(ub) & (ub > lb))\n        if np.any(bad):\n            ub = ub.copy()\n            lb = lb.copy()\n            ub[bad] = 5.0\n            lb[bad] = -5.0\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coords repeatedly until within bounds, fallback to clamp\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: lb + (lb - val)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: ub - (val - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(max(1, n))\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.random((n, dim))\n        lhs = np.empty((n, dim), dtype=float)\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            # positions inside strata\n            lhs[:, j] = cut[:-1] + (cut[1:] - cut[:-1]) * u[perm, j]\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            arr = lb + self.rng.random((int(n), self.dim)) * (ub - lb)\n            return arr\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget and catches exceptions\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                return None\n            try:\n                xv = np.asarray(x, dtype=float).reshape(self.dim)\n                fv = float(func(xv))\n            except Exception:\n                # treat as infinite (bad) but still consume budget\n                self.eval_count += 1\n                return np.inf\n            self.eval_count += 1\n            return fv\n\n        # initialize\n        self.eval_count = 0\n        self.x_opt = None\n        self.f_opt = float(np.inf)\n\n        lb, ub = self._get_bounds(func)\n        center = (lb + ub) / 2.0\n\n        # initial LHS in [0,1] mapped to bounds\n        lhs01 = self._lhs01(self.init_samples, self.dim)\n        lhs_pts = lb + lhs01 * (ub - lb)\n\n        # small uniform pool\n        uni_pool = self._uniform_array(lb, ub, n=8)\n\n        archive_X = []\n        archive_F = []\n\n        # Evaluate center first if budget allows\n        f_center = safe_eval(center)\n        if f_center is not None:\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # evaluate LHS points\n        for i in range(len(lhs_pts)):\n            if self.eval_count >= self.budget:\n                break\n            x = lhs_pts[i]\n            f = safe_eval(x)\n            if f is None:\n                break\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least some uniform samples\n        while self.eval_count < self.budget and len(archive_X) < max(8, self.init_samples + 4):\n            u = uni_pool[self.rng.integers(len(uni_pool))]\n            f = safe_eval(u)\n            if f is None:\n                break\n            archive_X.append(u.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = u.copy()\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=50)\n        gscale = 0.6  # global scale multiplier (adapted)\n        iter_since_improve = 0\n        iters = 0\n\n        # main loop\n        while self.eval_count < self.budget:\n            iters += 1\n\n            # build arrays\n            X_arr = np.asarray(archive_X, dtype=float) if archive_X else np.zeros((0, self.dim), dtype=float)\n            F_arr = np.asarray(archive_F, dtype=float) if archive_F else np.full((0,), np.inf)\n\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n            nf = finite_idx.size\n\n            # compute statistics for sampling scales\n            if nf > 0:\n                Xf = X_arr[finite_mask]\n                # per-dim IQR-based sigma approximation\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr / 1.349\n                # fallbacks if iqr is zero/small\n                small = per_dim_scale <= 1e-8\n                per_dim_scale[small] = 0.5 * (ub[small] - lb[small]) / 6.0\n                median = np.median(Xf, axis=0)\n            else:\n                # fallback\n                median = center.copy()\n                per_dim_scale = 0.5 * (ub - lb) / 6.0\n\n            # prepare sampling base: bias towards better archive items via softmax on -F\n            base = self._uniform_array(lb, ub, n=1)\n            if nf > 0:\n                Ff = F_arr[finite_idx]\n                # create scores: smaller F => larger score\n                s = -Ff\n                # normalize s\n                s = s - np.mean(s)\n                denom = np.std(s) if np.std(s) > 1e-12 else (np.max(np.abs(s)) + 1e-12)\n                logits = s / denom\n                temp = 0.6 + 1.6 * self.rng.random()  # temperature randomness\n                probs = np.exp(logits / temp)\n                probs = probs / (probs.sum() + 1e-12)\n                pick = finite_idx[self.rng.choice(len(finite_idx), p=probs)]\n                base = X_arr[pick].copy()\n\n            # small chance to use current best instead\n            if (self.x_opt is not None) and (self.rng.random() < 0.12):\n                base = self.x_opt.copy()\n\n            # donor pool for DE-like moves (try archive first, fallback to uniform)\n            donor = None\n            if nf >= 3:\n                # pick three distinct donors (indices)\n                idxs = self.rng.choice(finite_idx, size=3, replace=False)\n                donor = (X_arr[idxs[0]] - X_arr[idxs[1]]) + (X_arr[idxs[2]] - base)\n            else:\n                # small random differential\n                donor = self.rng.normal(scale=0.5 * per_dim_scale, size=self.dim)\n\n            # choose move type: global vs local\n            move_rand = self.rng.random()\n            is_global = move_rand < (0.25 + 0.1 * float(self.f_opt > 1e-3))  # more global if poor\n\n            # prepare tempered cauchy\n            cauchy_raw = self.rng.standard_cauchy(self.dim)\n            # temper extremes by tanh squashing to avoid huge jumps\n            cauchy_tempered = np.tanh(cauchy_raw)  # in (-1,1)\n\n            # scale for local gaussian\n            local_sigma = 0.6 * per_dim_scale * (0.4 + 0.6 * self.rng.random(self.dim))\n\n            # produce candidate\n            if is_global:\n                # GLOBAL move: anchor on an elite or uniformly sample anchor\n                if nf >= 4 and self.rng.random() < 0.65:\n                    # pick among top-k elites\n                    k = max(2, min(10, nf // 2))\n                    order = np.argsort(F_arr[finite_idx])\n                    elites = finite_idx[order[:k]]\n                    anchor = X_arr[self.rng.choice(elites)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n\n                # make a tempered cauchy jump scaled by per-dim scale and gscale\n                scale_vec = per_dim_scale * (0.6 + self.rng.random(self.dim) * 1.2) * gscale\n                cand = anchor + cauchy_tempered * scale_vec\n\n                # occasional DE-like differential from donors\n                if donor is not None and self.rng.random() < 0.38:\n                    cand = cand + (0.4 + 0.8 * self.rng.random()) * donor * gscale\n\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.12:\n                    mask = self.rng.random(self.dim) < 0.18\n                    if mask.any():\n                        uvals = self.rng.random(np.count_nonzero(mask))\n                        cand[mask] = lb[mask] + uvals * (ub[mask] - lb[mask])\n\n                # small extra global jitter\n                if self.rng.random() < 0.10:\n                    cand = cand + self.rng.normal(scale=0.25 * per_dim_scale * (1.0 + self.rng.random(self.dim)))\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                anis = local_sigma * (0.5 + self.rng.random(self.dim))\n                cand = base + self.rng.normal(scale=anis)\n\n                # attract toward median occasionally\n                if self.rng.random() < 0.18 and nf > 0:\n                    alpha = 0.1 + 0.45 * self.rng.random()\n                    cand = cand * (1.0 - alpha) + median * alpha\n\n                # PCA-guided elite perturbation occasionally\n                if nf >= 4 and self.rng.random() < 0.38:\n                    # pick a few elites\n                    k = min(8, max(4, nf // 3))\n                    order = np.argsort(F_arr[finite_idx])\n                    elite_inds = finite_idx[order[:k]]\n                    E = X_arr[elite_inds] - np.mean(X_arr[elite_inds], axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        # principal direction\n                        pc = Vt[0]\n                        # scale by spread proportionally to first singular value\n                        s0 = S[0] if S.size > 0 else 0.0\n                        spread = s0 / max(1e-12, np.mean(S) if S.size > 0 else (s0 + 1e-12))\n                        # perturb along pc\n                        strength = (0.6 + self.rng.random() * 1.6) * per_dim_scale.mean() * (0.5 + 0.6 * spread)\n                        sign = 1.0 if self.rng.random() < 0.6 else -1.0\n                        cand = cand + sign * pc * strength * self.rng.random()\n                    except Exception:\n                        pass\n\n                # small DE-like mixing inside local branch sometimes\n                if donor is not None and self.rng.random() < 0.12:\n                    cand = cand + 0.35 * gscale * donor * (0.4 + self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.05:\n                    scale_vec = per_dim_scale * (0.6 + self.rng.random(self.dim))\n                    cand = cand + cauchy_tempered * scale_vec * (1.0 + gscale * 0.6)\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            bad_mask = ~np.isfinite(cand)\n            if bad_mask.any():\n                uvals = self.rng.random(np.count_nonzero(bad_mask))\n                cand[bad_mask] = lb[bad_mask] + uvals * (ub[bad_mask] - lb[bad_mask])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            if f_cand is None:\n                break\n\n            # append to archive\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # determine success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = f_cand\n                self.x_opt = cand.copy()\n                success = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # rank among finite\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            nf = finite_mask.sum()\n            if nf > 0 and np.isfinite(f_cand):\n                combined = np.sort(F_arr[finite_mask])\n                rank = np.searchsorted(combined, f_cand, side='left')\n                if rank < max(1, int(0.25 * nf)):\n                    success = True\n\n            success_window.append(1 if success else 0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if len(success_window) == success_window.maxlen:\n                srate = sum(success_window) / float(success_window.maxlen)\n                # If unusually many successes, become slightly more exploratory to capitalize\n                if srate > 0.55:\n                    gscale = min(8.0, gscale * (1.0 + 0.25 * (srate - 0.5)))\n                # If too few successes -> slowly decay to refine\n                elif srate < 0.18:\n                    gscale = max(0.04, gscale * (1.0 - 0.18 * (0.2 + (0.18 - srate))))\n                # small random jitter\n                gscale = max(0.03, min(10.0, gscale * (1.0 + 0.08 * (self.rng.random() - 0.5))))\n\n            # prune archive to maintain memory budget occasionally\n            if len(archive_X) > int(self.archive_max * 1.2) and len(archive_X) > 20:\n                F_arr = np.asarray(archive_F, dtype=float)\n                finite_mask = np.isfinite(F_arr)\n                finite_idx = np.where(finite_mask)[0]\n                keep = set()\n                if finite_idx.size > 0:\n                    order = np.argsort(F_arr[finite_idx])\n                    # keep best 40% of finite\n                    keep_best = finite_idx[order[:max(1, int(0.4 * finite_idx.size))]]\n                    keep.update(int(i) for i in keep_best)\n                # keep some random others for diversity (including infinite)\n                rem = [i for i in range(len(archive_X)) if i not in keep]\n                sample_n = max(0, min(len(rem), int(self.archive_max - len(keep))))\n                if sample_n > 0 and len(rem) > 0:\n                    sampled = set(self.rng.choice(rem, size=sample_n, replace=False))\n                    keep.update(int(i) for i in sampled)\n                # ensure we keep something\n                if len(keep) == 0:\n                    keep.update(range(min(len(archive_X), self.archive_max)))\n                keep_list = sorted(list(keep))\n                archive_X = [archive_X[i] for i in keep_list]\n                archive_F = [archive_F[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(30, 6 * self.dim):\n                # micro-restart: inject diversified points around best and uniformly\n                num_inject = min(6 + self.dim // 3, int(0.5 * (self.archive_max)))\n                for k in range(num_inject):\n                    if self.eval_count >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.7:\n                        # local diversified around best\n                        perturb = self.rng.normal(scale=per_dim_scale * (0.2 + 0.9 * self.rng.random(self.dim)))\n                        newx = self.x_opt + perturb\n                        # occasionally mix some uniform coordinates\n                        if self.rng.random() < 0.18:\n                            mask = self.rng.random(self.dim) < 0.12\n                            if mask.any():\n                                newx[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n                    else:\n                        # global uniform injection\n                        newx = self._uniform_array(lb, ub, n=1)\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    fnew = safe_eval(newx)\n                    if fnew is None:\n                        break\n                    archive_X.append(newx.copy())\n                    archive_F.append(fnew)\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.f_opt = fnew\n                        self.x_opt = newx.copy()\n                # encourage exploration after restart\n                gscale = max(gscale, 1.0 + 0.5 * self.rng.random())\n                iter_since_improve = 0\n                success_window.clear()\n\n        # final safety: pick best finite point from archive if needed\n        if archive_F:\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best = finite_idx[np.argmin(F_arr[finite_idx])]\n                self.f_opt = float(F_arr[best])\n                self.x_opt = np.asarray(archive_X[best]).copy()\n            else:\n                # fallback: evaluate center if possible\n                f_center = safe_eval(center)\n                if f_center is not None and np.isfinite(f_center):\n                    self.f_opt = f_center\n                    self.x_opt = center.copy()\n\n        # final fallback: if still no finite solution, return random within bounds\n        if self.x_opt is None:\n            self.x_opt = self._uniform_array(lb, ub, n=1)\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 189, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "error": "In the code, line 189, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e91d99d7-7061-4586-9ce6-6ccd30427182", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.dim = int(dim)\n        self.budget = int(budget)\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = int(archive_max)\n        self.seed = seed\n\n        # runtime trackers (populated in __call__)\n        self.rng = None\n        self.eval_count = 0\n        self.x_opt = None\n        self.f_opt = float(np.inf)\n\n    # ---------- small utilities ----------\n    def _get_bounds(self, func):\n        # default bounds [-5,5] per dimension\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n\n        # try to infer if func exposes bounds in common ways\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                # case: object with .lb and .ub (arrays or scalars)\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb_try = np.asarray(b.lb, dtype=float)\n                    ub_try = np.asarray(b.ub, dtype=float)\n                    if lb_try.size == 1:\n                        lb_try = np.full(self.dim, float(lb_try))\n                    if ub_try.size == 1:\n                        ub_try = np.full(self.dim, float(ub_try))\n                    if lb_try.size == self.dim and ub_try.size == self.dim:\n                        lb, ub = lb_try, ub_try\n                # case: tuple (lb, ub)\n                elif isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb_try = np.asarray(b[0], dtype=float)\n                    ub_try = np.asarray(b[1], dtype=float)\n                    if lb_try.size == 1:\n                        lb_try = np.full(self.dim, float(lb_try))\n                    if ub_try.size == 1:\n                        ub_try = np.full(self.dim, float(ub_try))\n                    if lb_try.size == self.dim and ub_try.size == self.dim:\n                        lb, ub = lb_try, ub_try\n\n            # maybe direct attributes func.lb/func.ub\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                lbo = np.asarray(func.lb, dtype=float)\n                ubo = np.asarray(func.ub, dtype=float)\n                if lbo.size == 1:\n                    lbo = np.full(self.dim, float(lbo))\n                if ubo.size == 1:\n                    ubo = np.full(self.dim, float(ubo))\n                if lbo.size == self.dim and ubo.size == self.dim:\n                    lb, ub = lbo, ubo\n        except Exception:\n            # ignore and use defaults\n            pass\n\n        # safety: ensure ub > lb elementwise\n        bad = ub <= lb\n        if np.any(bad):\n            ub = ub.copy()\n            lb = lb.copy()\n            ub[bad] = 5.0\n            lb[bad] = -5.0\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates repeatedly until within bounds (max iterations), fallback to clamp\n        x = np.asarray(x, dtype=float)\n        for _ in range(5):\n            below = x < lb\n            above = x > ub\n            if not np.any(below) and not np.any(above):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin Hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        dim = int(dim)\n        rng = self.rng\n        cut = np.linspace(0.0, 1.0, n + 1)\n        lhs = np.zeros((n, dim), dtype=float)\n        u = rng.random((n, dim))\n        for j in range(dim):\n            perm = rng.permutation(n)\n            lhs[:, j] = cut[:-1][perm] + u[:, j] * (1.0 / n)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        n = int(n)\n        shape = (n, self.dim)\n        r = self.rng.random(shape)\n        return lb + r * (ub - lb)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # initialize RNG\n        self.rng = np.random.default_rng(self.seed)\n\n        # safe evaluator that respects budget and catches exceptions\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                return None\n            try:\n                xv = np.asarray(x, dtype=float).reshape(self.dim)\n                fv = float(func(xv))\n            except Exception:\n                # treat as invalid (infinite)\n                fv = float(\"inf\")\n            self.eval_count += 1\n            return fv\n\n        # init trackers\n        self.eval_count = 0\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        archive_X = []\n        archive_F = []\n\n        # Evaluate center first\n        f_center = safe_eval(center)\n        if f_center is not None:\n            archive_X.append(center.copy())\n            archive_F.append(f_center)\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = center.copy()\n\n        # initial LHS in [0,1] mapped to bounds\n        lhs01 = self._lhs01(self.init_samples, self.dim)\n        lhs_pts = lb + lhs01 * (ub - lb)\n\n        # add an additional uniform sample set to choose from\n        uni = self._uniform_array(lb, ub, n=10)\n\n        # Evaluate LHS points\n        for i in range(lhs_pts.shape[0]):\n            if self.eval_count >= self.budget:\n                break\n            x = lhs_pts[i]\n            f = safe_eval(x)\n            if f is None:\n                break\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # ensure at least one uniform sample\n        if self.eval_count < self.budget:\n            u = uni[self.rng.integers(len(uni))]\n            f = safe_eval(u)\n            if f is not None:\n                archive_X.append(u.copy())\n                archive_F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = np.asarray(u).copy()\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=50)\n        gscale = 0.5  # global scale multiplier (adapted)\n        iter_since_improve = 0\n        iters = 0\n\n        # main loop\n        while self.eval_count < self.budget:\n            iters += 1\n\n            # build arrays\n            X_arr = np.asarray(archive_X, dtype=float) if archive_X else np.zeros((0, self.dim))\n            F_arr = np.asarray(archive_F, dtype=float) if archive_F else np.array([], dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n            nf = finite_idx.size\n\n            # compute statistics for sampling scales using finite archive points\n            if nf > 0:\n                Xf = X_arr[finite_idx]\n                median = np.median(Xf, axis=0)\n                # use robust spread (IQR) or std fallback\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approx to std\n                # fallback to std if IQR vanishes\n                small = per_dim_scale <= 1e-12\n                if np.any(small):\n                    per_dim_scale[small] = np.std(Xf[:, small], axis=0) if Xf.shape[0] > 1 else (ub - lb)[small] * 0.2\n                # ensure non-zero\n                per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-8)\n            else:\n                median = 0.5 * (lb + ub)\n                per_dim_scale = 0.5 * (ub - lb)\n\n            # prepare sampling base: bias towards better archive items via softmax on -F\n            if nf > 0:\n                Ff = F_arr[finite_idx]\n                # numerical stabilization\n                s = -(Ff - np.min(Ff))\n                # add small temperature randomness\n                temp = 0.6 + 1.6 * self.rng.random()\n                logits = s / (np.std(s) + 1e-9)\n                probs = np.exp(logits / temp - np.max(logits / temp))\n                probs = probs / (np.sum(probs) + 1e-12)\n                # sample base index\n                if self.rng.random() < 0.85:\n                    sel = finite_idx[self.rng.choice(len(finite_idx), p=probs)]\n                    base = X_arr[sel].copy()\n                else:\n                    base = self._uniform_array(lb, ub, n=1)[0]\n            else:\n                base = self._uniform_array(lb, ub, n=1)[0]\n\n            # small chance to use current best instead\n            if (self.x_opt is not None) and (self.rng.random() < 0.12):\n                base = np.asarray(self.x_opt).copy()\n\n            # donor pool for DE-like moves (try archive first, fallback to uniform)\n            if len(archive_X) >= 3 and self.rng.random() < 0.5:\n                idxs = self.rng.choice(len(archive_X), size=3, replace=False)\n                donor = archive_X[idxs[0]] - archive_X[idxs[1]] + 0.5 * (archive_X[idxs[2]] - archive_X[idxs[0]])\n            else:\n                # uniform donor direction\n                donor = self._uniform_array(lb, ub, n=1)[0] - base\n\n            # choose move type: global vs local\n            move_rand = self.rng.random()\n\n            # prepare tempered cauchy\n            cauchy_raw = self.rng.standard_cauchy(self.dim)\n            c_small = np.tanh(np.clip(cauchy_raw, -10.0, 10.0))  # in (-1,1)\n\n            # scale for local gaussian\n            local_sigma = 0.6 * per_dim_scale * (0.5 + self.rng.random(self.dim))\n\n            # produce candidate\n            if move_rand < 0.38:\n                # GLOBAL move: anchor on an elite or uniformly sample anchor\n                if nf > 0 and self.rng.random() < 0.8:\n                    # pick among top-k elites\n                    k = max(1, min(8, nf // 4 + 1))\n                    order = np.argsort(F_arr[finite_idx])\n                    elite_idx = finite_idx[order[:k]]\n                    anchor = X_arr[self.rng.choice(elite_idx)]\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)[0]\n\n                # make a tempered cauchy jump scaled by per-dim scale and gscale\n                F_scale = 0.6 * (1.0 + self.rng.random())\n                cand = anchor + gscale * (per_dim_scale * F_scale) * c_small\n\n                # occasional DE-like differential from donors\n                if self.rng.random() < 0.37:\n                    cand = cand + (0.4 + self.rng.random() * 0.9) * gscale * donor * (0.5 + self.rng.random())\n\n                # coordinate injection: replace some coords by uniform occasionally\n                mask = self.rng.random(self.dim) < 0.12\n                if mask.any():\n                    cand[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n\n                # small extra global jitter\n                if self.rng.random() < 0.10:\n                    cand += (self._uniform_array(lb, ub, n=1)[0] - 0.5 * (lb + ub)) * 0.08 * (1.0 + self.rng.random())\n\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                cand = base.copy()\n                if self.rng.random() < 0.18 and nf > 0:\n                    alpha = 0.1 + 0.45 * self.rng.random()\n                    cand = (1 - alpha) * cand + alpha * median\n\n                # anisotropic gaussian step\n                step = self.rng.normal(scale=local_sigma)\n                cand = cand + step * (0.5 + 0.5 * self.rng.random(self.dim)) * gscale\n\n                # PCA-guided elite perturbation occasionally\n                if nf >= min(4, self.dim) and self.rng.random() < 0.22:\n                    # choose a subset of elites\n                    k = min(max(3, nf // 6 + 3), nf)\n                    order = np.argsort(F_arr[finite_idx])\n                    elite_inds = finite_idx[order[:k]]\n                    Xsub = X_arr[elite_inds]\n                    Xc = Xsub - Xsub.mean(axis=0)\n                    try:\n                        # SVD to get principal components\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        pc = Vt[0]  # top principal direction\n                        pc_spread = S[0] / max(1.0, np.sqrt(k))  # normalized\n                        direction = np.sign(self.rng.normal())\n                        step_mag = (0.4 + 1.2 * self.rng.random()) * pc_spread * gscale\n                        cand = cand + direction * pc * step_mag\n                    except Exception:\n                        pass\n\n                # small DE-like mixing inside local branch sometimes\n                if self.rng.random() < 0.15:\n                    cand = cand + 0.6 * gscale * donor * (0.3 + self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape\n                if self.rng.random() < 0.05:\n                    cand = cand + 0.9 * gscale * c_small * per_dim_scale\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = lb[bad] + self.rng.random(np.count_nonzero(bad)) * (ub[bad] - lb[bad])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            if f_cand is None:\n                break\n\n            # append to archive\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # determine success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.x_opt = cand.copy()\n                self.f_opt = float(f_cand)\n                success = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # rank among finite (for relative success)\n            if np.isfinite(f_cand) and nf > 0:\n                combined = np.sort(F_arr[finite_mask])\n                # compute rank position\n                rank = np.searchsorted(combined, f_cand, side='left')\n                if rank < max(1, int(0.25 * max(1, nf))):\n                    success = True\n\n            success_window.append(1 if success else 0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 8 == 0 and len(success_window) == success_window.maxlen:\n                srate = sum(success_window) / len(success_window)\n                if srate > 0.28:\n                    # many successes: increase exploration a bit\n                    gscale = min(8.0, gscale * (1.03 + 0.02 * self.rng.random()))\n                elif srate < 0.06:\n                    # decay to refine search\n                    gscale = max(1e-3, gscale * (0.95 - 0.03 * self.rng.random()))\n                else:\n                    # slight stabilization\n                    gscale = max(1e-3, gscale * (0.995 + 0.005 * self.rng.random()))\n\n            # prune archive to maintain memory budget occasionally\n            if len(archive_X) > max(2 * self.archive_max, 2 * self.dim) and (iters % 11 == 0):\n                # keep best half and some random others\n                F_arr = np.asarray(archive_F, dtype=float)\n                finite_mask = np.isfinite(F_arr)\n                finite_idx = np.where(finite_mask)[0]\n                keep = set()\n                if finite_idx.size > 0:\n                    order = np.argsort(F_arr[finite_idx])\n                    best_half = finite_idx[order[: max(1, finite_idx.size // 2)]].tolist()\n                    keep.update(best_half)\n                # keep some random others to maintain diversity (including infinities)\n                rest = [i for i in range(len(archive_X)) if i not in keep]\n                num_keep_rand = min(len(rest), max(1, self.archive_max - len(keep)))\n                if rest and num_keep_rand > 0:\n                    sampled = list(self.rng.choice(rest, size=num_keep_rand, replace=False))\n                    keep.update(sampled)\n                # rebuild archives\n                keep_list = sorted(list(keep))\n                if keep_list:\n                    archive_X = [archive_X[i] for i in keep_list]\n                    archive_F = [archive_F[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(60, 6 * self.dim):\n                # micro-restart: inject diversified points around best and uniformly\n                iter_since_improve = 0\n                num_inject = min(12 + self.dim, max(1, self.budget - self.eval_count))\n                for j in range(num_inject):\n                    if self.eval_count >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.7:\n                        perturb = self.rng.normal(scale=per_dim_scale * (0.2 + 0.8 * self.rng.random(self.dim)))\n                        newx = self.x_opt + perturb\n                        # occasionally mix some uniform coordinates\n                        mask = self.rng.random(self.dim) < 0.12\n                        if mask.any():\n                            newx[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n                    else:\n                        newx = self._uniform_array(lb, ub, n=1)[0]\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    fnew = safe_eval(newx)\n                    if fnew is None:\n                        break\n                    archive_X.append(newx.copy())\n                    archive_F.append(fnew)\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.x_opt = newx.copy()\n                        self.f_opt = float(fnew)\n                # encourage exploration after restart\n                gscale = max(gscale, 1.0 + 0.5 * self.rng.random())\n\n        # final safety: pick best finite point from archive if needed\n        if (self.x_opt is None or not np.isfinite(self.f_opt)) and len(archive_X) > 0:\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best = finite_idx[np.argmin(F_arr[finite_idx])]\n                self.f_opt = float(F_arr[best])\n                self.x_opt = np.asarray(archive_X[best]).copy()\n            else:\n                # fallback: random\n                self.x_opt = self._uniform_array(lb, ub, n=1)[0]\n                self.f_opt = float(\"inf\")\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 185, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "error": "In the code, line 185, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "187c0e0a-61a6-4f34-8cc8-45ceee4378a6", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = int(archive_max)\n\n        # runtime trackers (populated in __call__)\n        self.rng = None\n        self.x_opt = None\n        self.f_opt = float(np.inf)\n\n    # ---------- small utilities ----------\n    def _get_bounds(self, func):\n        # default bounds [-5,5] per dimension\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n\n        # try common protocols on func to get bounds\n        # 1) func.bounds as (lb, ub) or object with .lb/.ub scalar/array\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # if it's a tuple/list (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb_try = np.asarray(b[0], dtype=float)\n                    ub_try = np.asarray(b[1], dtype=float)\n                    if lb_try.size == 1:\n                        lb = np.full(self.dim, float(lb_try))\n                    elif lb_try.size == self.dim:\n                        lb = lb_try.astype(float)\n                    if ub_try.size == 1:\n                        ub = np.full(self.dim, float(ub_try))\n                    elif ub_try.size == self.dim:\n                        ub = ub_try.astype(float)\n                else:\n                    # maybe object with .lb and .ub\n                    lb_try = getattr(b, \"lb\", None)\n                    ub_try = getattr(b, \"ub\", None)\n                    if lb_try is not None and ub_try is not None:\n                        lb_try = np.asarray(lb_try, dtype=float)\n                        ub_try = np.asarray(ub_try, dtype=float)\n                        if lb_try.size == 1:\n                            lb = np.full(self.dim, float(lb_try))\n                        elif lb_try.size == self.dim:\n                            lb = lb_try.astype(float)\n                        if ub_try.size == 1:\n                            ub = np.full(self.dim, float(ub_try))\n                        elif ub_try.size == self.dim:\n                            ub = ub_try.astype(float)\n        except Exception:\n            pass\n\n        # 2) direct attributes func.lb/func.ub\n        try:\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                lb_try = np.asarray(getattr(func, \"lb\"), dtype=float)\n                ub_try = np.asarray(getattr(func, \"ub\"), dtype=float)\n                if lb_try.size == 1:\n                    lb = np.full(self.dim, float(lb_try))\n                elif lb_try.size == self.dim:\n                    lb = lb_try.astype(float)\n                if ub_try.size == 1:\n                    ub = np.full(self.dim, float(ub_try))\n                elif ub_try.size == self.dim:\n                    ub = ub_try.astype(float)\n        except Exception:\n            pass\n\n        # safety: ensure ub > lb elementwise, otherwise reset defaults for offending dims\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        bad = ub <= lb\n        if np.any(bad):\n            ub = ub.copy()\n            lb = lb.copy()\n            ub[bad] = 5.0\n            lb[bad] = -5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect coordinates repeatedly until within bounds (max iterations), fallback to clamp\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(6):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = lb + (lb - x) = 2*lb - x\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above: x = ub - (x - ub) = 2*ub - x\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin Hypercube in [0,1]^dim -> returns (n, dim)\n        n = int(n)\n        dim = int(dim)\n        rng = self.rng\n        lhs = np.zeros((n, dim), dtype=float)\n        # for each dim create permuted strata\n        for d in range(dim):\n            perm = rng.permutation(n)\n            # random offset within each stratum\n            r = rng.random(n)\n            lhs[:, d] = (perm + r) / float(n)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (int(n), lb.size) if int(n) > 1 else (1, lb.size)\n        r = self.rng.random(shape)\n        return lb + r * (ub - lb)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # initialize RNG\n        self.rng = np.random.default_rng(self.seed)\n\n        # bounds\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator that respects budget and catches exceptions\n        self.eval_count = 0\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                return None\n            try:\n                xv = np.asarray(x, dtype=float).reshape(self.dim)\n            except Exception:\n                # malformed x counts as a call but returns inf\n                self.eval_count += 1\n                return float(\"inf\")\n            try:\n                self.eval_count += 1\n                fv = float(func(xv))\n                # treat NaN as inf\n                if not np.isfinite(fv):\n                    return float(\"inf\")\n                return fv\n            except Exception:\n                # any exception considered as invalid but consumes budget\n                return float(\"inf\")\n\n        # init trackers\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # archive lists\n        archive_X = []\n        archive_F = []\n\n        # center point evaluation\n        center = 0.5 * (lb + ub)\n        f_center = safe_eval(center)\n        if f_center is not None:\n            archive_X.append(np.asarray(center, dtype=float).copy())\n            archive_F.append(float(f_center))\n            if np.isfinite(f_center) and f_center < self.f_opt:\n                self.f_opt = f_center\n                self.x_opt = np.asarray(center, dtype=float).copy()\n\n        # initial LHS samples mapped to bounds\n        n_init = max(1, min(self.init_samples, max(1, self.budget - self.eval_count)))\n        if n_init > 0:\n            lhs = self._lhs01(n_init, self.dim)\n            samples = lb + lhs * (ub - lb)\n            for i in range(samples.shape[0]):\n                if self.eval_count >= self.budget:\n                    break\n                x = samples[i]\n                f = safe_eval(x)\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n        # add some extra uniform exploration (small)\n        extra = min(6, max(0, self.budget - self.eval_count))\n        for _ in range(extra):\n            if self.eval_count >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)[0]\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # short-term adaptation and control variables\n        success_window = deque(maxlen=50)\n        gscale = 0.6  # global scale multiplier (adapted)\n        iter_since_improve = 0\n        iters = 0\n\n        # main loop\n        while self.eval_count < self.budget:\n            iters += 1\n\n            # arrays for finite archive\n            X_arr = np.asarray(archive_X, dtype=float) if len(archive_X) > 0 else np.zeros((0, self.dim))\n            F_arr = np.asarray(archive_F, dtype=float) if len(archive_F) > 0 else np.array([], dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n            nf = finite_idx.size\n\n            # compute per-dimension robust scale from finite archive points\n            if nf >= 2:\n                Xf = X_arr[finite_mask]\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approximate robust std\n                # fallback to std where IQR is zero\n                small = per_dim_scale <= 0\n                if np.any(small):\n                    per_dim_scale[small] = np.std(Xf[:, small], axis=0) if Xf.shape[0] > 1 else (ub - lb)[small] * 0.2\n            elif nf == 1:\n                per_dim_scale = np.full(self.dim, 0.2) * (ub - lb)\n            else:\n                per_dim_scale = 0.25 * (ub - lb)\n\n            per_dim_scale = np.maximum(per_dim_scale, (ub - lb) * 1e-8)\n\n            # prepare sampling base: bias towards better archive items via softmax on -F\n            base = None\n            if nf > 0:\n                logits = -(F_arr[finite_mask] - np.min(F_arr[finite_mask]))\n                # temperature randomization\n                temp = 0.6 + 1.6 * self.rng.random()\n                scaled = logits / (temp + 1e-12)\n                # numeric stable softmax\n                scaled = scaled - np.max(scaled)\n                probs = np.exp(scaled)\n                probs = probs / (np.sum(probs) + 1e-12)\n                # sample an elite index\n                pick = np.where(finite_mask)[0][self.rng.choice(nf, p=probs)]\n                base = X_arr[pick].copy()\n            else:\n                base = self._uniform_array(lb, ub, n=1)[0]\n\n            # small chance to use current best instead\n            if (self.x_opt is not None) and (self.rng.random() < 0.12):\n                base = np.asarray(self.x_opt).copy()\n\n            # donor pool for DE-like moves (try archive first, fallback to uniform)\n            donor = np.zeros(self.dim, dtype=float)\n            if len(archive_X) >= 3 and self.rng.random() < 0.5:\n                idxs = self.rng.choice(len(archive_X), size=3, replace=False)\n                d0 = archive_X[idxs[0]]\n                d1 = archive_X[idxs[1]]\n                d2 = archive_X[idxs[2]]\n                donor = (np.asarray(d0, dtype=float) - np.asarray(d1, dtype=float)) + (np.asarray(d2, dtype=float) - base)\n            else:\n                donor = self.rng.normal(0.0, 1.0, size=self.dim)\n\n            # choose move type: global vs local\n            is_global = self.rng.random() < 0.28  # global escapes less frequent\n\n            # prepare tempered cauchy scalar (heavy-tailed for escapes)\n            u = self.rng.random()\n            # standard Cauchy via tan(pi*(u-0.5)) but moderate it\n            cauchy = np.tan(np.pi * (u - 0.5))\n            cauchy = np.sign(cauchy) * np.minimum(10.0, np.abs(cauchy))\n\n            # scale for local gaussian\n            local_sigma = 0.6 * per_dim_scale * (0.3 + 0.7 * self.rng.random(self.dim))\n\n            # produce candidate\n            cand = base.copy()\n\n            if is_global:\n                # GLOBAL move: anchor on an elite or uniformly sample anchor\n                if nf > 0 and self.rng.random() < 0.85:\n                    # pick among top-k elites\n                    k = max(1, min(8, max(1, nf // 4 + 1)))\n                    order = np.argsort(F_arr[finite_mask])\n                    top_idx = np.where(finite_mask)[0][order[:k]]\n                    anchor = X_arr[self.rng.choice(top_idx)]\n                    cand = np.asarray(anchor, dtype=float).copy()\n                else:\n                    cand = self._uniform_array(lb, ub, n=1)[0]\n\n                # tempered cauchy jump scaled by per-dim scale and gscale\n                F_scale = 0.6 * (1.0 + self.rng.random())\n                cand = cand + (cauchy * F_scale * gscale) * (per_dim_scale / (np.mean(per_dim_scale) + 1e-12))\n\n                # occasional DE-like differential from donors\n                if self.rng.random() < 0.37:\n                    factor = (0.4 + self.rng.random() * 0.9) * gscale * (0.5 + self.rng.random())\n                    cand = cand + factor * donor\n\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.14:\n                    mask = self.rng.random(self.dim) < 0.28\n                    if mask.any():\n                        cand[mask] = lb[mask] + self.rng.random(np.count_nonzero(mask)) * (ub[mask] - lb[mask])\n\n                # small extra global jitter\n                if self.rng.random() < 0.10:\n                    cand = cand + 0.6 * (self.rng.normal(size=self.dim) * per_dim_scale) * (0.7 + 0.6 * self.rng.random())\n\n            else:\n                # LOCAL move: anisotropic gaussian around base sometimes attracted to median\n                if nf > 0 and self.rng.random() < 0.38:\n                    Xf = X_arr[finite_mask]\n                    median = np.median(Xf, axis=0)\n                    alpha = 0.1 + 0.45 * self.rng.random()\n                    cand = (1 - alpha) * cand + alpha * median\n\n                # anisotropic gaussian step with per-dim sigma\n                step = local_sigma * self.rng.normal(size=self.dim)\n                cand = cand + step * (0.5 + 0.5 * self.rng.random(self.dim)) * gscale\n\n                # PCA-guided elite perturbation occasionally\n                if nf >= 3 and self.rng.random() < 0.18:\n                    # choose a subset of elites\n                    k = max(2, min(6, nf // 2))\n                    order = np.argsort(F_arr[finite_mask])\n                    elite_inds = np.where(finite_mask)[0][order[:k]]\n                    Xsub = X_arr[elite_inds]\n                    # center and SVD to get principal components\n                    Xm = Xsub - np.mean(Xsub, axis=0, keepdims=True)\n                    try:\n                        U, S, Vt = np.linalg.svd(Xm, full_matrices=False)\n                        pc = Vt[0]  # leading principal direction\n                        pc_spread = S[0] / max(1.0, np.sqrt(k))\n                        direction = np.sign(self.rng.normal())\n                        step_mag = (0.4 + 1.2 * self.rng.random()) * pc_spread * gscale\n                        cand = cand + direction * step_mag * pc\n                    except Exception:\n                        # fallback small gaussian\n                        cand = cand + 0.2 * per_dim_scale * self.rng.normal(size=self.dim)\n\n                # small DE-like mixing inside local branch sometimes\n                if self.rng.random() < 0.15:\n                    cand = cand + 0.6 * gscale * donor * (0.3 + self.rng.random())\n\n                # tempered Cauchy jump occasionally for escape even in local mode\n                if self.rng.random() < 0.06:\n                    cand = cand + (cauchy * 0.8 * gscale) * (per_dim_scale / (np.mean(per_dim_scale) + 1e-12))\n\n            # safety: if any non-finite values, replace them with uniform samples (per-dim)\n            cand = np.asarray(cand, dtype=float)\n            bad_coords = ~np.isfinite(cand)\n            if bad_coords.any():\n                cand[bad_coords] = lb[bad_coords] + self.rng.random(np.count_nonzero(bad_coords)) * (ub[bad_coords] - lb[bad_coords])\n\n            # reflect to bounds and final clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            if self.eval_count >= self.budget:\n                break\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(float(f_cand))\n\n            # determine success: improved global best OR placed among top 25% of finite vals\n            success = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                success = True\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # rank among finite (for relative success)\n            if np.isfinite(f_cand) and nf > 0:\n                combined = np.sort(F_arr[finite_mask])\n                rank = np.searchsorted(combined, f_cand, side='left')\n                # if within top 25% of current finite archive -> consider success\n                if rank < max(1, int(0.25 * len(combined))):\n                    success = True\n\n            success_window.append(1 if success else 0)\n\n            # adapt gscale every few iterations (short-term adaptation)\n            if iters % 7 == 0 and len(success_window) > 3:\n                srate = float(sum(success_window)) / len(success_window)\n                if srate > 0.28:\n                    gscale = min(3.0, gscale * (1.06 + 0.02 * self.rng.random()))\n                elif srate < 0.06:\n                    gscale = max(0.06, gscale * (0.86 - 0.04 * self.rng.random()))\n                else:\n                    # slight random jitter to avoid lock\n                    gscale = max(0.02, gscale * (0.98 + 0.06 * (self.rng.random() - 0.5)))\n\n            # prune archive to maintain memory budget occasionally\n            if len(archive_X) > max(2 * self.archive_max, 2 * self.dim) and (iters % 11 == 0):\n                F_arr = np.asarray(archive_F, dtype=float)\n                finite_mask = np.isfinite(F_arr)\n                finite_idx = np.where(finite_mask)[0]\n                # keep best half and some random others\n                keep = set()\n                if finite_idx.size > 0:\n                    n_keep_best = max(2, int(0.5 * len(finite_idx)))\n                    best_idx = finite_idx[np.argsort(F_arr[finite_idx])[:n_keep_best]]\n                    keep.update(best_idx.tolist())\n                # sample some others\n                rest = [i for i in range(len(archive_X)) if i not in keep]\n                num_keep_rand = max(0, min(len(rest), int(0.2 * len(archive_X))))\n                if len(rest) > 0 and num_keep_rand > 0:\n                    sampled = list(self.rng.choice(rest, size=num_keep_rand, replace=False))\n                    keep.update(sampled)\n                # always keep current best if present\n                if self.x_opt is not None:\n                    # find nearest occurrence of x_opt in archive (if any) by min distance\n                    dists = np.sum((np.asarray(archive_X) - self.x_opt) ** 2, axis=1)\n                    argmin = int(np.argmin(dists))\n                    keep.add(argmin)\n                # rebuild archives\n                keep_list = sorted(list(keep))\n                archive_X = [archive_X[i] for i in keep_list]\n                archive_F = [archive_F[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(60, 6 * self.dim):\n                # micro-restart: inject diversified points around best and uniformly\n                iter_since_improve = 0\n                num_inject = min(12 + self.dim, max(1, self.budget - self.eval_count))\n                for _ in range(num_inject):\n                    if self.eval_count >= self.budget:\n                        break\n                    if self.x_opt is not None and self.rng.random() < 0.7:\n                        # inject around current best with anisotropic gaussian\n                        mask = self.rng.random(self.dim) < 0.6\n                        newx = np.asarray(self.x_opt).copy()\n                        if mask.any():\n                            newx[mask] = newx[mask] + (self.rng.normal(size=np.count_nonzero(mask)) * (per_dim_scale[mask] * (0.6 + self.rng.random())))\n                        # mix some uniform coordinates sometimes\n                        if self.rng.random() < 0.4:\n                            mixmask = self.rng.random(self.dim) < 0.18\n                            if mixmask.any():\n                                newx[mixmask] = lb[mixmask] + self.rng.random(np.count_nonzero(mixmask)) * (ub[mixmask] - lb[mixmask])\n                    else:\n                        newx = self._uniform_array(lb, ub, n=1)[0]\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    fnew = safe_eval(newx)\n                    archive_X.append(newx.copy())\n                    archive_F.append(float(fnew))\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.x_opt = newx.copy()\n                        self.f_opt = float(fnew)\n                # encourage exploration after restart\n                gscale = max(0.2, gscale * (0.8 + 0.6 * self.rng.random()))\n                success_window.clear()\n\n            # ensure archive does not grow unboundedly beyond archive_max by occasional trimming\n            if len(archive_X) > 2 * self.archive_max and (iters % 5 == 0):\n                F_arr = np.asarray(archive_F, dtype=float)\n                order = np.argsort(F_arr)\n                keep_n = self.archive_max\n                keep_idx = order[:keep_n].tolist()\n                # keep some random extras to maintain diversity\n                rest = [i for i in range(len(archive_X)) if i not in keep_idx]\n                add_rand = min(len(rest), int(0.05 * self.archive_max))\n                if add_rand > 0:\n                    keep_idx += list(self.rng.choice(rest, size=add_rand, replace=False))\n                archive_X = [archive_X[i] for i in sorted(keep_idx)]\n                archive_F = [archive_F[i] for i in sorted(keep_idx)]\n\n        # final safety: pick best finite point from archive if needed\n        if (self.x_opt is None or not np.isfinite(self.f_opt)) and len(archive_X) > 0:\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best = finite_idx[np.argmin(F_arr[finite_idx])]\n                self.f_opt = float(F_arr[best])\n                self.x_opt = np.asarray(archive_X[best], dtype=float).copy()\n            else:\n                # fallback: random\n                self.x_opt = self._uniform_array(lb, ub, n=1)[0]\n                self.f_opt = float(\"inf\")\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 207, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "error": "In the code, line 207, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "92064f5f-c88f-432f-b0ec-d9799e56c864", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_samples=12, archive_max=300, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = int(archive_max)\n        self.verbose = verbose\n\n        # runtime\n        self.rng = np.random.default_rng(seed)\n        self.eval_count = 0\n        self.f_opt = float('inf')\n        self.x_opt = None\n\n    # ------------ Utilities ------------\n    def _get_bounds(self, func):\n        # try to infer bounds but fallback to [-5,5] per dimension\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # case: object with .lb/.ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb_try = np.asarray(b.lb, dtype=float)\n                    ub_try = np.asarray(b.ub, dtype=float)\n                    if lb_try.size == 1:\n                        lb_try = np.full(self.dim, float(lb_try))\n                    if ub_try.size == 1:\n                        ub_try = np.full(self.dim, float(ub_try))\n                    if lb_try.shape[0] == self.dim and ub_try.shape[0] == self.dim:\n                        lb, ub = lb_try, ub_try\n                # case: tuple/list (lb, ub)\n                elif isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb_try = np.asarray(b[0], dtype=float)\n                    ub_try = np.asarray(b[1], dtype=float)\n                    if lb_try.size == 1:\n                        lb_try = np.full(self.dim, float(lb_try))\n                    if ub_try.size == 1:\n                        ub_try = np.full(self.dim, float(ub_try))\n                    if lb_try.shape[0] == self.dim and ub_try.shape[0] == self.dim:\n                        lb, ub = lb_try, ub_try\n            else:\n                # try func.lb / func.ub\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb_try = np.asarray(func.lb, dtype=float)\n                    ub_try = np.asarray(func.ub, dtype=float)\n                    if lb_try.size == 1:\n                        lb_try = np.full(self.dim, float(lb_try))\n                    if ub_try.size == 1:\n                        ub_try = np.full(self.dim, float(ub_try))\n                    if lb_try.shape[0] == self.dim and ub_try.shape[0] == self.dim:\n                        lb, ub = lb_try, ub_try\n        except Exception:\n            pass\n\n        # safety: ensure ub > lb elementwise\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or ub[i] <= lb[i]:\n                lb[i], ub[i] = -5.0, 5.0\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(5):\n            below = x < lb\n            above = x > ub\n            if not np.any(below) and not np.any(above):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple LHS in [0,1]^dim\n        rng = self.rng\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = rng.random((n, dim))\n        lhs = np.empty((n, dim))\n        for j in range(dim):\n            perm = rng.permutation(n)\n            lhs[:, j] = cut[:-1][perm] + u[:, j] * (1.0 / n)\n        return lhs\n\n    def _uniform_array(self, lb, ub, n=1):\n        r = self.rng.random((n, self.dim))\n        shape = (n, self.dim)\n        return lb + r * (ub - lb)\n\n    # ------------ Main call ------------\n    def __call__(self, func):\n        # init\n        self.eval_count = 0\n        self.f_opt = float('inf')\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        archive_X = []\n        archive_F = []\n\n        # safe evaluator respecting budget\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                return None\n            try:\n                xv = np.asarray(x, dtype=float).reshape(self.dim)\n            except Exception:\n                self.eval_count += 1\n                return float('inf')\n            try:\n                f = func(xv)\n                # allow None to indicate exceeded budget by func, but we'll treat as fail\n                if f is None or (isinstance(f, float) and not np.isfinite(f)):\n                    f = float('inf')\n            except Exception:\n                f = float('inf')\n            self.eval_count += 1\n            return float(f)\n\n        # evaluate center first (cheap anchor)\n        if self.eval_count < self.budget:\n            f_center = safe_eval(center)\n            if f_center is not None:\n                archive_X.append(center.copy())\n                archive_F.append(f_center)\n                if np.isfinite(f_center) and f_center < self.f_opt:\n                    self.f_opt = float(f_center)\n                    self.x_opt = center.copy()\n\n        # initial Latin Hypercube samples\n        n_init = min(self.init_samples, max(1, self.budget - self.eval_count))\n        lhs01 = self._lhs01(n_init, self.dim)\n        lhs_pts = lb + lhs01 * (ub - lb)\n        for i in range(lhs_pts.shape[0]):\n            if self.eval_count >= self.budget:\n                break\n            x = lhs_pts[i]\n            f = safe_eval(x)\n            if f is not None:\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n\n        # ensure at least one random if budget remains and no finite yet\n        if self.eval_count < self.budget and (self.x_opt is None or not np.isfinite(self.f_opt)):\n            u = self._uniform_array(lb, ub, n=1)[0]\n            f = safe_eval(u)\n            if f is not None:\n                archive_X.append(u.copy())\n                archive_F.append(f)\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = u.copy()\n\n        # short-term adaptation variables\n        gscale = 0.5  # global multiplier for step sizes\n        success_window = deque(maxlen=50)\n        iter_since_improve = 0\n        iter_total = 0\n\n        # main loop\n        while self.eval_count < self.budget:\n            iter_total += 1\n\n            # prepare arrays\n            if archive_F:\n                F_arr = np.asarray(archive_F, dtype=float)\n                X_arr = np.asarray(archive_X, dtype=float)\n            else:\n                F_arr = np.array([], dtype=float)\n                X_arr = np.zeros((0, self.dim), dtype=float)\n\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n            nf = finite_idx.size\n\n            # per-dimension scale from archive: use IQR or std fallback\n            if nf >= 2:\n                Xf = X_arr[finite_idx]\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approx convert IQR to std\n                # fallback to std\n                std = Xf.std(axis=0)\n                mask_zero = (per_dim_scale <= 1e-8)\n                per_dim_scale[mask_zero] = std[mask_zero]\n                per_dim_scale = np.maximum(per_dim_scale, 1e-6 * (ub - lb))\n            else:\n                per_dim_scale = 0.5 * (ub - lb)\n\n            # sampling base: bias to elites by softmax(-F)\n            base = center.copy()\n            if nf > 0:\n                s = -F_arr[finite_idx].copy()\n                s = s - s.mean()\n                temp = 0.2 + 0.8 * (0.5 + 0.5 * self.rng.random())  # random temperature\n                logits = s / (np.std(s) + 1e-9) / temp\n                exp = np.exp(logits - logits.max())\n                probs = exp / exp.sum()\n                sel = self.rng.choice(finite_idx, p=probs)\n                base = X_arr[sel].copy()\n            else:\n                base = self._uniform_array(lb, ub, n=1)[0]\n\n            # small chance to use global best\n            if self.x_opt is not None and np.isfinite(self.f_opt) and self.rng.random() < 0.08:\n                base = np.asarray(self.x_opt).copy()\n\n            # donor pool for DE-like moves: pick 2 donors\n            if len(archive_X) >= 3:\n                idxs = self.rng.choice(len(archive_X), size=2, replace=False)\n                donor = X_arr[idxs[0]] - X_arr[idxs[1]]\n            else:\n                donor = self._uniform_array(lb, ub, n=1)[0] - base\n\n            # decide move type\n            r = self.rng.random()\n            cand = base.copy()\n\n            # tempered cauchy raw sample per-dim\n            u = self.rng.random(self.dim)\n            # standard cauchy: tan(pi*(u-0.5)) but temper with tanh for stability\n            cauchy_raw = np.tan(np.pi * (u - 0.5))\n            c_small = np.tanh(np.clip(cauchy_raw, -10.0, 10.0))  # (-1,1)\n\n            # scaled per-dim factor\n            F_scale = per_dim_scale\n            # LOCAL move: anisotropic gaussian with attraction to median\n            if r < 0.50:\n                # anisotropic gaussian\n                jitter = self.rng.normal(scale=1.0, size=self.dim)\n                local_sigma = 0.6 * per_dim_scale * (0.5 + self.rng.random(self.dim))\n                # occasionally pull towards median of finite archive\n                if nf > 0 and self.rng.random() < 0.18:\n                    median = np.median(X_arr[finite_idx], axis=0)\n                    cand = base + 0.4 * (median - base) + local_sigma * jitter * gscale\n                else:\n                    cand = base + local_sigma * jitter * gscale\n\n                # small DE-like mixing\n                if self.rng.random() < 0.35:\n                    cand = cand + 0.4 * gscale * donor * (0.3 + 0.7 * self.rng.random())\n\n                # occasional PCA-guided elite perturbation\n                if nf >= 3 and self.rng.random() < 0.12:\n                    k = min(max(2, nf // 4), nf)\n                    elite_inds = finite_idx[np.argsort(F_arr[finite_idx])[:k]]\n                    Xsub = X_arr[elite_inds]\n                    Xc = Xsub - Xsub.mean(axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        pc_vec = Vt[0]\n                        pc_spread = S[0] / max(1.0, np.sqrt(k))\n                        direction = np.sign(self.rng.normal())\n                        step_mag = (0.4 + 1.2 * self.rng.random()) * pc_spread * gscale\n                        cand = cand + direction * step_mag * pc_vec\n                    except Exception:\n                        pass\n\n            # GLOBAL move: tempered Cauchy jump anchored on elite or uniform\n            elif r < 0.90:\n                # choose anchor among elites or uniform\n                if nf > 0 and self.rng.random() < 0.85:\n                    k = max(1, min(nf, 5))\n                    elite_idx = np.argsort(F_arr[finite_idx])[:k]\n                    anchor = X_arr[finite_idx[ self.rng.choice(elite_idx) ]]\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)[0]\n                # tempered Cauchy jump scaled\n                cand = anchor + gscale * (F_scale * c_small)\n                # occasional DE donor add\n                if self.rng.random() < 0.20:\n                    cand = cand + 0.5 * gscale * donor\n                # coordinate injection: replace some coords by uniform occasionally\n                if self.rng.random() < 0.12:\n                    mask = self.rng.random(self.dim) < 0.15\n                    if mask.any():\n                        cand[mask] = lb[mask] + self.rng.random(mask.sum()) * (ub[mask] - lb[mask])\n\n            # rare big escape\n            else:\n                anchor = self._uniform_array(lb, ub, n=1)[0]\n                cand = anchor + 3.0 * gscale * (F_scale * c_small)\n                # small global jitter\n                if self.rng.random() < 0.3:\n                    cand = cand + 0.5 * self.rng.normal(size=self.dim) * per_dim_scale\n\n            # small extra jitter for exploration\n            if self.rng.random() < 0.10:\n                cand = cand + 0.05 * (ub - lb) * (self.rng.random(self.dim) - 0.5)\n\n            # safety: replace non-finite coords by uniform\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = lb[bad] + self.rng.random(bad.sum()) * (ub[bad] - lb[bad])\n\n            # reflect and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            if self.eval_count >= self.budget:\n                break\n            f_cand = safe_eval(cand)\n            if f_cand is None:\n                break\n\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n\n            # update best\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # determine relative success: among top 25% of finite values\n            rel_success = False\n            if nf + 1 >= 3:\n                F_all = np.asarray([f for f in archive_F if np.isfinite(f)])\n                if F_all.size > 0:\n                    thresh = np.percentile(F_all, 25)\n                    if np.isfinite(f_cand) and f_cand <= thresh:\n                        rel_success = True\n\n            success_window.append(1 if (improved or rel_success) else 0)\n\n            # adapt gscale every few iterations based on recent success rate\n            if len(success_window) >= 8 and (iter_total % 6 == 0):\n                srate = np.mean(success_window)\n                if srate > 0.28:\n                    # increase exploration a bit then decay\n                    gscale = min(5.0, gscale * (1.05 + 0.03 * self.rng.random()))\n                elif srate < 0.08:\n                    # reduce step-size to refine\n                    gscale = max(1e-4, gscale * (0.90 + 0.05 * self.rng.random()))\n                else:\n                    # slight stabilization\n                    gscale = max(1e-4, gscale * (0.98 + 0.03 * self.rng.random()))\n\n            # prune archive occasionally\n            if len(archive_X) > max(2 * self.archive_max, self.archive_max + 50):\n                # keep best half and some random others\n                F_arr2 = np.asarray(archive_F, dtype=float)\n                finite_idx2 = np.where(np.isfinite(F_arr2))[0]\n                keep = set()\n                if finite_idx2.size > 0:\n                    order = np.argsort(F_arr2[finite_idx2])\n                    best_half = finite_idx2[order[: max(1, finite_idx2.size // 2)]].tolist()\n                    keep.update(best_half)\n                # sample some random ones\n                rest = [i for i in range(len(archive_X)) if i not in keep]\n                if len(rest) > 0:\n                    num_keep_rand = min(len(rest), max(0, self.archive_max - len(keep)))\n                    sampled = list(self.rng.choice(rest, size=num_keep_rand, replace=False))\n                    keep.update(sampled)\n                keep_list = sorted(list(keep))\n                archive_X = [archive_X[i] for i in keep_list]\n                archive_F = [archive_F[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts: if too long without improvement\n            if iter_since_improve > max(60, 6 * self.dim):\n                iter_since_improve = 0\n                # inject diversified points around best and uniformly\n                n_inject = min(10 + self.dim, max(5, self.archive_max // 10))\n                for j in range(n_inject):\n                    if self.eval_count >= self.budget:\n                        break\n                    if self.x_opt is not None and np.isfinite(self.f_opt) and self.rng.random() < 0.7:\n                        # gaussian around best\n                        newx = self.x_opt + 0.8 * per_dim_scale * self.rng.normal(size=self.dim) * (1.0 + 0.5 * self.rng.random())\n                    else:\n                        # uniform\n                        newx = self._uniform_array(lb, ub, n=1)[0]\n                    # occasional coordinate mixing\n                    if self.rng.random() < 0.35 and self.x_opt is not None:\n                        mask = self.rng.random(self.dim) < 0.25\n                        newx[mask] = self.x_opt[mask]\n                    newx = self._reflect_bounds(newx, lb, ub)\n                    fnew = safe_eval(newx)\n                    if fnew is None:\n                        break\n                    archive_X.append(newx.copy())\n                    archive_F.append(fnew)\n                    if np.isfinite(fnew) and fnew < self.f_opt:\n                        self.f_opt = float(fnew)\n                        self.x_opt = newx.copy()\n\n                # encourage exploration by increasing gscale a bit\n                gscale = min(3.0, gscale * (1.2 + 0.2 * self.rng.random()))\n\n        # final safety: pick best finite point from archive if needed\n        if (self.x_opt is None or not np.isfinite(self.f_opt)) and len(archive_X) > 0:\n            F_arr = np.asarray(archive_F, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best = finite_idx[np.argmin(F_arr[finite_idx])]\n                self.f_opt = float(F_arr[best])\n                self.x_opt = np.asarray(archive_X[best]).copy()\n            else:\n                # fallback random\n                rnd = self._uniform_array(lb, ub, n=1)[0]\n                self.x_opt = rnd.copy()\n                self.f_opt = float('inf')\n\n        if self.verbose:\n            print(\"MG-ADS finished: evals\", self.eval_count, \"best f\", self.f_opt)\n\n        return self.f_opt, np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 175, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "error": "In the code, line 175, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "da2277d3-cfd2-4050-bce7-9096317ce3bb", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.init_samples = int(init_samples) if init_samples is not None else max(12, 4 * self.dim)\n        self.archive_max = int(archive_max)\n        # runtime trackers\n        self.eval_count = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    # -------- utilities ----------\n    def _get_bounds(self, func):\n        # Try to get bounds from func object if present in several common forms\n        lb = None\n        ub = None\n        # Common patterns\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            # tuple (lb, ub) or object with lb/ub attributes\n            if isinstance(b, tuple) or isinstance(b, list):\n                try:\n                    lb_try = np.asarray(b[0], dtype=float)\n                    ub_try = np.asarray(b[1], dtype=float)\n                    if lb_try.size in (1, self.dim) and ub_try.size in (1, self.dim):\n                        lb = np.broadcast_to(lb_try, (self.dim,)).astype(float)\n                        ub = np.broadcast_to(ub_try, (self.dim,)).astype(float)\n                except Exception:\n                    pass\n            else:\n                # maybe object with lb/ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    try:\n                        lb_try = np.asarray(b.lb, dtype=float)\n                        ub_try = np.asarray(b.ub, dtype=float)\n                        lb = np.broadcast_to(lb_try, (self.dim,)).astype(float)\n                        ub = np.broadcast_to(ub_try, (self.dim,)).astype(float)\n                    except Exception:\n                        pass\n        # direct attributes\n        if lb is None:\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                try:\n                    lb_try = np.asarray(func.lb, dtype=float)\n                    ub_try = np.asarray(func.ub, dtype=float)\n                    lb = np.broadcast_to(lb_try, (self.dim,)).astype(float)\n                    ub = np.broadcast_to(ub_try, (self.dim,)).astype(float)\n                except Exception:\n                    pass\n        # fallback to default [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # sanity: ensure ub > lb\n        mask_bad = ub <= lb\n        if np.any(mask_bad):\n            ub = ub.copy()\n            lb = lb.copy()\n            ub[mask_bad] = lb[mask_bad] + 1.0\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect repeatedly to bring x within [lb,ub]\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(4):\n            below = x < lb\n            above = x > ub\n            if not np.any(below) and not np.any(above):\n                break\n            # reflect outside parts\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        n = int(n)\n        dim = int(dim)\n        # Simple Latin Hypercube: divide [0,1] into n strata per dimension and permute\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.random((n, dim))\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            points[:, j] = cut[:-1] + (u[:, j] + perm) / n  # stratified but permuted\n            # fix to [0,1)\n            points[:, j] = np.minimum(points[:, j], np.nextafter(1.0, 0.0))\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        n = int(n)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return lb + (ub - lb) * self.rng.random((n, self.dim))\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # reset trackers\n        self.eval_count = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # Don't call func if budget already exhausted\n            if self.eval_count >= self.budget:\n                return float(\"inf\")\n            try:\n                val = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                val = float(\"inf\")\n            self.eval_count += 1\n            return val\n\n        # Archive: lists for X and F (X: 1D arrays)\n        archive_X = []\n        archive_F = []\n\n        # Evaluate center first\n        center = 0.5 * (lb + ub)\n        f_center = safe_eval(center)\n        archive_X.append(center.copy())\n        archive_F.append(f_center)\n        if np.isfinite(f_center) and f_center < self.f_opt:\n            self.f_opt = float(f_center)\n            self.x_opt = center.copy()\n\n        # Initial Latin Hypercube samples (mapped to bounds)\n        lhs01 = self._lhs01(self.init_samples, self.dim)\n        lhs_pts = lb + lhs01 * (ub - lb)\n        for p in lhs_pts:\n            if self.eval_count >= self.budget:\n                break\n            f = safe_eval(p)\n            archive_X.append(p.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = p.copy()\n\n        # Add some additional uniform samples to diversify\n        add_uniform = max(0, min(self.budget - self.eval_count, self.dim * 2))\n        uni_pts = self._uniform_array(lb, ub, n=add_uniform)\n        for p in uni_pts:\n            if self.eval_count >= self.budget:\n                break\n            f = safe_eval(p)\n            archive_X.append(p.copy())\n            archive_F.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = p.copy()\n\n        # control variables\n        gscale = 0.5  # global multiplier for step sizes\n        success_window = deque(maxlen=40)  # track recent successes (True/False)\n        last_improve_count = 0\n\n        # Main loop\n        while self.eval_count < self.budget:\n            # prepare archived arrays\n            X_arr = np.asarray(archive_X, dtype=float)\n            F_arr = np.asarray(archive_F, dtype=float)\n\n            # identify finite entries\n            finite_mask = np.isfinite(F_arr)\n            if np.any(finite_mask):\n                Xf = X_arr[finite_mask]\n                Ff = F_arr[finite_mask]\n            else:\n                # no valid evaluations -> sample uniform\n                cand = self._uniform_array(lb, ub, n=1)[0]\n                f_c = safe_eval(cand)\n                archive_X.append(cand.copy())\n                archive_F.append(f_c)\n                if np.isfinite(f_c) and f_c < self.f_opt:\n                    self.f_opt = float(f_c)\n                    self.x_opt = cand.copy()\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            q25 = np.percentile(Xf, 25, axis=0)\n            q75 = np.percentile(Xf, 75, axis=0)\n            iqr = (q75 - q25) / 1.349  # approx std from IQR\n            std = np.std(Xf, axis=0, ddof=1)\n            per_dim_scale = np.where(iqr > 1e-9, iqr, std * 0.8)\n            # ensure not zero\n            per_dim_scale = np.maximum(per_dim_scale, 1e-8 * (ub - lb))\n\n            # choose base index biased toward better F via softmax on -F\n            Frel = Ff - np.min(Ff)\n            s = -Frel / (np.std(Frel) + 1e-9)\n            # small temperature noise\n            s = s + 0.1 * self.rng.normal(size=s.shape)\n            # numerical stabilization\n            smax = np.max(s)\n            probs = np.exp(s - smax)\n            probs = probs / (np.sum(probs) + 1e-12)\n            base_idx = self.rng.choice(len(Xf), p=probs)\n            base = Xf[base_idx].copy()\n\n            # small chance to use global best directly\n            if self.rng.random() < 0.08 and self.x_opt is not None:\n                base = self.x_opt.copy()\n\n            # donor pool selection (for DE-like moves)\n            nf = len(Xf)\n            if nf >= 3:\n                d_idxs = self.rng.choice(nf, size=2, replace=False)\n                donor1 = Xf[d_idxs[0]].copy()\n                donor2 = Xf[d_idxs[1]].copy()\n                donor_vec = donor1 - donor2\n            else:\n                # fallback random direction\n                donor_vec = self.rng.normal(size=self.dim)\n                donor_vec = donor_vec / (np.linalg.norm(donor_vec) + 1e-9)\n\n            # choose move type\n            p_global = 0.35  # probability of global jump\n            is_global = self.rng.random() < p_global\n\n            # tempered Cauchy sampler\n            def tempered_cauchy(scale_vec, temp=1.0):\n                # sample independent cauchy and temper with gaussian for stability\n                c = self.rng.standard_cauchy(size=self.dim)\n                g = self.rng.normal(scale=0.2, size=self.dim)\n                return (c * temp + g) * scale_vec\n\n            # scale modifiers\n            per_dim_scale_eff = per_dim_scale * (1.0 + 0.5 * self.rng.random(self.dim))\n            # produce candidate\n            if is_global:\n                # GLOBAL move: anchor on elite or uniform\n                if self.rng.random() < 0.7 and len(Ff) >= 3:\n                    # pick among top-k elites\n                    k = max(1, min(10, len(Ff) // 6 + 2))\n                    order = np.argsort(Ff)\n                    elite_idx = order[self.rng.integers(0, min(k, len(order)))]\n                    anchor = Xf[elite_idx].copy()\n                else:\n                    # uniform anchor\n                    anchor = self._uniform_array(lb, ub, n=1)[0]\n                # tempered cauchy jump\n                c_temp = 0.9 + 2.1 * self.rng.random()\n                cand = anchor + tempered_cauchy(per_dim_scale_eff * gscale, temp=c_temp)\n                # occasional DE-like differential injection\n                if self.rng.random() < 0.37:\n                    factor = 0.6 * self.rng.random()\n                    cand = cand + factor * donor_vec * gscale\n                # coordinate injection\n                mask = self.rng.random(self.dim) < 0.08\n                if mask.any():\n                    cand[mask] = lb[mask] + (ub[mask] - lb[mask]) * self.rng.random(np.count_nonzero(mask))\n                # small extra jitter\n                cand = cand + 0.03 * gscale * self.rng.normal(size=self.dim)\n            else:\n                # LOCAL move: anisotropic gaussian around base, sometimes attracted to median\n                median = np.median(Xf, axis=0)\n                if self.rng.random() < 0.25:\n                    attract = 0.25 * (median - base)\n                else:\n                    attract = 0.0\n                alpha = 0.1 + 0.45 * self.rng.random()\n                gauss = self.rng.normal(size=self.dim) * per_dim_scale_eff * (0.6 + alpha) * gscale\n                cand = base + attract + gauss\n                # PCA-guided elite perturbation occasionally\n                if self.rng.random() < 0.13 and Xf.shape[0] >= 3:\n                    # choose subset of elites\n                    nf_el = min(max(3, nf // 6 + 3), nf)\n                    idxs = np.argsort(Ff)[:nf_el]\n                    subset = Xf[idxs]\n                    # do SVD on centered data (PCA)\n                    try:\n                        m = np.mean(subset, axis=0)\n                        U, Svals, Vt = np.linalg.svd((subset - m), full_matrices=False)\n                        # principal direction perturbation\n                        pc = Vt[0]\n                        pc_spread = Svals[0] / max(1.0, np.linalg.norm(pc))\n                        step_mag = (0.4 + 1.2 * self.rng.random()) * pc_spread * gscale\n                        cand = cand + step_mag * pc * (1.0 + 0.3 * self.rng.normal())\n                    except Exception:\n                        pass\n                # small DE-like mixing inside local branch sometimes\n                if self.rng.random() < 0.18:\n                    cand = cand + 0.5 * self.rng.random() * donor_vec * per_dim_scale_eff\n                # tempered cauchy tiny escape occasionally\n                if self.rng.random() < 0.03:\n                    c_small = tempered_cauchy(per_dim_scale_eff * 0.5, temp=0.5)\n                    cand = cand + 0.9 * gscale * c_small\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            cand = np.asarray(cand, dtype=float)\n            bad_coords = ~np.isfinite(cand)\n            if bad_coords.any():\n                cand[bad_coords] = lb[bad_coords] + (ub[bad_coords] - lb[bad_coords]) * self.rng.random(np.count_nonzero(bad_coords))\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(f_cand)\n            # update best\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved = True\n                last_improve_count = self.eval_count\n\n            # rank-based relative success: placed among top 25% of finite values\n            rel_success = False\n            if np.any(np.isfinite(F_arr)):\n                F_all = np.asarray(archive_F, dtype=float)\n                finite_idxs = np.where(np.isfinite(F_all))[0]\n                if finite_idxs.size > 0:\n                    # recompute ranks among finite\n                    fin_vals = F_all[finite_idxs]\n                    rank = np.sum(fin_vals <= f_cand)  # how many <= f_cand\n                    # success if in best quarter\n                    if rank <= max(1, int(0.25 * fin_vals.size)):\n                        rel_success = True\n\n            success = improved or rel_success\n            success_window.append(1 if success else 0)\n\n            # short-term adaptation of gscale every few iterations\n            if len(success_window) == success_window.maxlen:\n                srate = sum(success_window) / len(success_window)\n                if srate > 0.35:\n                    gscale = min(3.0, gscale * (1.05 + 0.2 * self.rng.random()))\n                elif srate < 0.08:\n                    gscale = max(0.03, gscale * (0.85 - 0.15 * self.rng.random()))\n                else:\n                    # slight drift toward exploitation\n                    gscale = max(0.04, gscale * (0.95 + 0.1 * (0.5 - self.rng.random())))\n\n            # prune archive if too large\n            if len(archive_X) > max(100, self.archive_max):\n                F_arr2 = np.asarray(archive_F, dtype=float)\n                finite_idx = np.where(np.isfinite(F_arr2))[0]\n                # keep best half of finite and some random others\n                keep = set()\n                if finite_idx.size > 0:\n                    order = finite_idx[np.argsort(F_arr2[finite_idx])]\n                    keep_best = order[: max(1, finite_idx.size // 2)].tolist()\n                    for k in keep_best:\n                        keep.add(k)\n                # keep some random indices (including inf) for diversity\n                all_idx = list(range(len(archive_X)))\n                self.rng.shuffle(all_idx)\n                for k in all_idx[: max(1, int(0.2 * len(archive_X)))]:\n                    keep.add(k)\n                keep_list = sorted(list(keep))\n                if len(keep_list) < len(archive_X):\n                    archive_X = [archive_X[i] for i in keep_list]\n                    archive_F = [archive_F[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            # if no improvement for many evaluations, inject micro-restart\n            if (self.x_opt is not None) and (self.eval_count - last_improve_count > max(50, 6 * self.dim)):\n                # perform micro-restart: explore around best and some uniform points\n                nr = min(6 + self.dim // 2, self.budget - self.eval_count)\n                if nr > 0:\n                    for _ in range(nr):\n                        if self.eval_count >= self.budget:\n                            break\n                        # perturb around best using per-dim scale\n                        perturb = self.rng.normal(scale=1.0, size=self.dim) * per_dim_scale * (1.8 + 1.2 * self.rng.random())\n                        newx = self.x_opt + perturb\n                        # mix some uniform coordinates\n                        mask = self.rng.random(self.dim) < 0.12\n                        if mask.any():\n                            newx[mask] = lb[mask] + (ub[mask] - lb[mask]) * self.rng.random(np.count_nonzero(mask))\n                        newx = self._reflect_bounds(newx, lb, ub)\n                        fnew = safe_eval(newx)\n                        archive_X.append(newx.copy())\n                        archive_F.append(fnew)\n                        if np.isfinite(fnew) and fnew < self.f_opt:\n                            self.f_opt = float(fnew)\n                            self.x_opt = newx.copy()\n                            last_improve_count = self.eval_count\n                # encourage exploration a bit\n                gscale = min(2.5, gscale * (1.2 + 0.2 * self.rng.random()))\n\n        # final best: choose best finite from archive\n        F_arr_final = np.asarray(archive_F, dtype=float)\n        finite_idxs = np.where(np.isfinite(F_arr_final))[0]\n        if finite_idxs.size > 0:\n            best_idx = finite_idxs[np.argmin(F_arr_final[finite_idxs])]\n            self.f_opt = float(F_arr_final[best_idx])\n            self.x_opt = np.asarray(archive_X[best_idx], dtype=float)\n        else:\n            # fallback: random point\n            xr = self._uniform_array(lb, ub, n=1)[0]\n            self.x_opt = xr.copy()\n            self.f_opt = float(\"inf\")\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 170, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=40)  # track recent successes (True/False)", "error": "In the code, line 170, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_window = deque(maxlen=40)  # track recent successes (True/False)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "349c0965-c340-4ac3-be2c-1c4e7e34b74b", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.archive_max = int(archive_max)\n        # number of initial samples (besides center)\n        self.init_samples = init_samples if init_samples is not None else max(20, 4 * self.dim)\n\n        # runtime trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # -------- utilities ----------\n    def _get_bounds(self, func):\n        # Try several common patterns: func.bounds as object with lb/ub, or tuple/list\n        lb = None\n        ub = None\n        # func.bounds.lb / func.bounds.ub\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                try:\n                    lb_try = np.asarray(b.lb, dtype=float)\n                    ub_try = np.asarray(b.ub, dtype=float)\n                    if lb_try.size == self.dim and ub_try.size == self.dim:\n                        lb, ub = lb_try, ub_try\n                except Exception:\n                    pass\n            # maybe tuple (lb, ub)\n            if lb is None:\n                try:\n                    if isinstance(b, (tuple, list)) and len(b) == 2:\n                        lb_try = np.asarray(b[0], dtype=float)\n                        ub_try = np.asarray(b[1], dtype=float)\n                        if lb_try.size == self.dim and ub_try.size == self.dim:\n                            lb, ub = lb_try, ub_try\n                except Exception:\n                    pass\n        # direct attributes\n        if lb is None and hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n            try:\n                lb_try = np.asarray(func.lb, dtype=float)\n                ub_try = np.asarray(func.ub, dtype=float)\n                if lb_try.size == self.dim and ub_try.size == self.dim:\n                    lb, ub = lb_try, ub_try\n            except Exception:\n                pass\n        # fallback to default [-5,5]\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # sanity: ensure ub > lb per-dim\n        ub = np.maximum(ub, lb + 1e-12)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect repeatedly to bring x within [lb,ub]\n        x = np.asarray(x, dtype=float)\n        # vectorized reflection: reflect components outside bounds until they are within.\n        # Use while loops with modulo-like reflection\n        # for stability, cap number of reflections\n        for _ in range(5):\n            below = x < lb\n            if not np.any(below) and not np.any(x > ub):\n                break\n            # reflect below\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above\n            above = x > ub\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube: divide [0,1] into n strata per dimension and permute\n        points = np.zeros((n, dim))\n        rng = self.rng\n        for d in range(dim):\n            perm = rng.permutation(n)\n            # sample within strata uniformly\n            u = (perm + rng.random(n)) / n\n            points[:, d] = u\n        # slight jitter to avoid exact 0/1\n        points = np.clip(points, 1e-12, 1 - 1e-12)\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            # return shape (n, dim)\n            return self.rng.uniform(low=lb, high=ub, size=(n, self.dim))\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # reset trackers\n        self.rng = np.random.default_rng(self.seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n\n        budget_left = int(self.budget)\n\n        def safe_eval(x):\n            nonlocal budget_left\n            if budget_left <= 0:\n                return np.inf\n            # ensure array type\n            x = np.asarray(x, dtype=float)\n            # call\n            try:\n                f = func(x)\n            except Exception:\n                # if function errors, treat as infinite cost\n                f = np.inf\n            budget_left -= 1\n            return f\n\n        # Archive: lists for X and F (X: 1D arrays)\n        archive_X = []\n        archive_F = []\n\n        # Evaluate center first\n        center = 0.5 * (lb + ub)\n        f_center = safe_eval(center)\n        archive_X.append(center.copy())\n        archive_F.append(float(f_center))\n        if np.isfinite(f_center) and f_center < self.f_opt:\n            self.f_opt = float(f_center)\n            self.x_opt = center.copy()\n\n        # Initial Latin Hypercube samples (mapped to bounds)\n        n_init = min(max(2, self.init_samples), budget_left)\n        if n_init > 0:\n            lhs = self._lhs01(n_init, self.dim)\n            samples = lb + lhs * (ub - lb)\n            for i in range(n_init):\n                if budget_left <= 0:\n                    break\n                x = samples[i]\n                f = safe_eval(x)\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n\n        # Add some additional uniform samples to diversify\n        extra = min(2 * self.dim, budget_left)\n        for _ in range(extra):\n            if budget_left <= 0:\n                break\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive_X.append(x.copy())\n            archive_F.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # control variables\n        gscale = 0.5  # global scale multiplier\n        adapt_interval = max(20, int(10 + 2 * self.dim))\n        success_window = []\n        iter_since_improve = 0\n        total_iters = 0\n\n        # Main loop\n        while budget_left > 0:\n            total_iters += 1\n\n            # prepare archived arrays\n            X_arr = np.asarray(archive_X, dtype=float) if len(archive_X) > 0 else np.zeros((0, self.dim))\n            F_arr = np.asarray(archive_F, dtype=float) if len(archive_F) > 0 else np.array([], dtype=float)\n\n            # identify finite entries\n            finite_mask = np.isfinite(F_arr)\n            finite_idxs = np.where(finite_mask)[0]\n            # use only finite for statistics; if none, sample uniformly\n            if finite_idxs.size == 0:\n                # purely random guess\n                cand = self._uniform_array(lb, ub, n=1)\n                f_cand = safe_eval(cand)\n                archive_X.append(cand.copy())\n                archive_F.append(float(f_cand))\n                if np.isfinite(f_cand) and f_cand < self.f_opt:\n                    self.f_opt = float(f_cand)\n                    self.x_opt = cand.copy()\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n                # update success window\n                success_window.append(False)\n                if len(success_window) > adapt_interval:\n                    success_window.pop(0)\n                # continue\n                continue\n\n            Xf = X_arr[finite_idxs]\n            Ff = F_arr[finite_idxs]\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            q25 = np.percentile(Xf, 25, axis=0)\n            q75 = np.percentile(Xf, 75, axis=0)\n            iqr = (q75 - q25) / 1.349  # approx std from IQR\n            std = np.std(Xf, axis=0, ddof=1)\n            per_dim_scale = np.maximum(iqr, 0.5 * std)\n            # ensure not zero\n            per_dim_scale = np.where(per_dim_scale <= 0, (ub - lb) * 0.02 + 1e-12, per_dim_scale)\n            # base scale vector (will be modulated by gscale)\n            scale_vec = per_dim_scale * gscale\n\n            # choose base index biased toward better F via softmax on -F\n            # small temperature noise\n            temp = 0.05 + 0.2 * self.rng.random()\n            # numerical stabilization\n            vals = -Ff\n            vals = vals - np.max(vals)\n            probs = np.exp(vals / (np.std(vals) + 1e-6) / (1.0 + temp))\n            if np.any(probs <= 0) or np.sum(probs) == 0:\n                probs = np.ones_like(probs)\n            probs = probs / np.sum(probs)\n            base_idx = finite_idxs[self.rng.choice(len(finite_idxs), p=probs)]\n\n            # small chance to use global best directly\n            if np.isfinite(self.f_opt) and self.rng.random() < 0.07:\n                base = self.x_opt.copy()\n            else:\n                base = X_arr[base_idx].copy()\n\n            # donor pool selection (for DE-like moves)\n            # select two distinct indices from finite archive different from base_idx\n            pool = list(set(finite_idxs.tolist()))\n            if base_idx in pool and len(pool) > 1:\n                pool.remove(base_idx)\n            if len(pool) >= 2:\n                a, b = self.rng.choice(pool, size=2, replace=False)\n                donor_vec = X_arr[a] - X_arr[b]\n            elif len(pool) == 1:\n                donor_vec = X_arr[pool[0]] - base\n            else:\n                # fallback random direction\n                donor_vec = self.rng.normal(0, 1.0, size=self.dim)\n\n            # choose move type\n            p = self.rng.random()\n            # tempered Cauchy sampler\n            def tempered_cauchy(scale_vec_local, temp_local=1.0):\n                # sample independent cauchy and temper with gaussian for stability\n                c = self.rng.standard_cauchy(size=self.dim)\n                g = self.rng.normal(0.0, 0.6, size=self.dim)\n                return (c * temp_local + g) * scale_vec_local\n\n            # produce candidate\n            cand = None\n            if p < 0.18:\n                # GLOBAL heavy move: tempered cauchy from an anchor (elite or uniform)\n                # pick among top-k elites\n                k = max(2, int(0.08 * len(finite_idxs)))\n                order = np.argsort(Ff)  # ascending\n                pick_idx = finite_idxs[order[self.rng.integers(0, min(k, len(order)))]] if len(order) > 0 else base_idx\n                if self.rng.random() < 0.6:\n                    anchor = X_arr[pick_idx].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub, n=1)\n                temp_local = 0.6 + 0.8 * self.rng.random()\n                c_jump = tempered_cauchy(scale_vec * (1.5 + 1.5 * self.rng.random(self.dim)), temp_local=temp_local)\n                cand = anchor + c_jump\n                # occasional DE-like differential injection\n                if self.rng.random() < 0.3:\n                    factor = 0.6 + 0.8 * self.rng.random()\n                    cand = cand + factor * donor_vec * (0.5 + self.rng.random() * 1.0)\n                # coordinate injection\n                if self.rng.random() < 0.12:\n                    mask = self.rng.random(self.dim) < 0.12\n                    cand[mask] = self._uniform_array(lb[mask], ub[mask], n=1)\n                # small extra jitter\n                cand = cand + self.rng.normal(0, scale_vec * 0.05)\n            else:\n                # LOCAL move: anisotropic gaussian around base, sometimes attracted to median\n                attract = 0.0\n                if self.rng.random() < 0.2:\n                    med = np.median(Xf, axis=0)\n                    attract = 0.25 * (med - base)  # small pulling toward median\n                alpha = 0.1 + 0.45 * self.rng.random()\n                local_noise = self.rng.normal(0.0, 1.0, size=self.dim) * (scale_vec * (0.6 + self.rng.random(self.dim) * 0.8))\n                cand = base + alpha * attract + local_noise\n                # PCA-guided elite perturbation occasionally\n                if self.rng.random() < 0.18:\n                    # choose subset of elites\n                    k = max(3, min(12, int(0.12 * len(finite_idxs))))\n                    order = np.argsort(Ff)\n                    sel_idx = finite_idxs[order[:k]]\n                    subset = X_arr[sel_idx]\n                    if subset.shape[0] >= 2:\n                        # do SVD on centered data (PCA)\n                        m = np.mean(subset, axis=0)\n                        C = (subset - m)\n                        try:\n                            U, S, VT = np.linalg.svd(C, full_matrices=False)\n                            # principal direction perturbation\n                            ncomp = min(3, VT.shape[0])\n                            coeffs = self.rng.normal(0.0, 1.0, size=ncomp) * (S[:ncomp] / (1.0 + S[0]))\n                            pca_move = np.dot(coeffs, VT[:ncomp, :])\n                            cand = cand + 0.75 * gscale * pca_move\n                        except Exception:\n                            pass\n                # small DE-like mixing inside local branch sometimes\n                if self.rng.random() < 0.18:\n                    fac = 0.3 + 0.9 * self.rng.random()\n                    cand = cand + fac * donor_vec * (0.2 + 0.8 * self.rng.random())\n                # tempered cauchy tiny escape occasionally\n                if self.rng.random() < 0.03:\n                    cand = cand + tempered_cauchy(scale_vec * 0.4, temp=0.4)\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            archive_X.append(cand.copy())\n            archive_F.append(float(f_cand))\n            improved = False\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                improved = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # rank-based relative success: placed among top 25% of finite values\n            # recompute among updated finite values\n            F_all = np.asarray(archive_F, dtype=float)\n            finite_idxs2 = np.where(np.isfinite(F_all))[0]\n            success = False\n            if finite_idxs2.size > 0:\n                fin_vals = F_all[finite_idxs2]\n                rank = np.sum(fin_vals < f_cand) if np.isfinite(f_cand) else fin_vals.size\n                # better means lower; success if in best quarter\n                if np.isfinite(f_cand) and rank <= max(1, int(0.25 * fin_vals.size)):\n                    success = True\n            success_window.append(1 if success else 0)\n            if len(success_window) > adapt_interval:\n                success_window.pop(0)\n\n            # short-term adaptation of gscale every few iterations\n            if (total_iters % adapt_interval) == 0:\n                if len(success_window) > 0:\n                    rate = float(np.sum(success_window)) / len(success_window)\n                    if rate > 0.18:\n                        # successful -> slightly reduce exploration (narrow)\n                        gscale = max(0.03, gscale * (0.85 - 0.15 * self.rng.random()))\n                    else:\n                        # unsuccessful -> increase exploration\n                        gscale = min(4.0, gscale * (1.08 + 0.5 * self.rng.random()))\n                # slight drift toward exploitation\n                gscale = min(5.0, max(0.02, gscale * (1.0 - 0.02 * self.rng.random())))\n\n            # prune archive if too large\n            if len(archive_X) > self.archive_max:\n                # keep best half of finite and some random others\n                F_arr_pr = np.asarray(archive_F, dtype=float)\n                finite_idxs_pr = np.where(np.isfinite(F_arr_pr))[0]\n                keep = set()\n                if finite_idxs_pr.size > 0:\n                    order = finite_idxs_pr[np.argsort(F_arr_pr[finite_idxs_pr])]\n                    nkeep_best = max(2, int(0.5 * finite_idxs_pr.size))\n                    keep.update(order[:nkeep_best].tolist())\n                    # keep some random finite\n                    remain = order[nkeep_best:]\n                    if remain.size > 0:\n                        pick = self.rng.choice(remain, size=min(len(remain), int(self.archive_max * 0.1)), replace=False)\n                        keep.update(pick.tolist())\n                # always keep current best if exists\n                if self.x_opt is not None:\n                    # find the index of best in archive (first match)\n                    for idx, x in enumerate(archive_X):\n                        if np.allclose(x, self.x_opt, atol=1e-12):\n                            keep.add(idx)\n                            break\n                # keep some random indices to preserve diversity (including infinities)\n                all_idx = list(range(len(archive_X)))\n                rem = set(all_idx) - keep\n                if len(rem) > 0:\n                    nrand = max(0, self.archive_max - len(keep))\n                    pick_rand = list(self.rng.choice(list(rem), size=min(len(rem), nrand), replace=False))\n                    keep.update(pick_rand)\n                keep_list = sorted(list(keep))\n                # prune\n                archive_X = [archive_X[i] for i in keep_list]\n                archive_F = [archive_F[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            stagnation_threshold = max(40, 6 * self.dim)\n            if iter_since_improve >= stagnation_threshold and budget_left > 0:\n                # perform micro-restart: explore around best and some uniform points\n                iter_since_improve = 0\n                # cluster size depends on dim and remaining budget\n                cluster = min(12 + self.dim, budget_left, 40)\n                # define local radii using per-dim scales\n                local_radii = per_dim_scale * (1.0 + 1.5 * self.rng.random(self.dim))\n                # generate cluster around best (if exists), otherwise around median\n                if self.x_opt is not None:\n                    center_restart = self.x_opt.copy()\n                else:\n                    center_restart = np.median(Xf, axis=0)\n                for k in range(cluster):\n                    if budget_left <= 0:\n                        break\n                    sigma = local_radii * (0.3 + 0.9 * (k / max(1, cluster)))\n                    x = center_restart + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                    # mix some uniform coordinates\n                    if self.rng.random() < 0.4:\n                        mask = self.rng.random(self.dim) < 0.15\n                        if np.any(mask):\n                            x[mask] = self._uniform_array(lb[mask], ub[mask], n=1)\n                    x = self._reflect_bounds(x, lb, ub)\n                    f = safe_eval(x)\n                    archive_X.append(x.copy())\n                    archive_F.append(float(f))\n                    if np.isfinite(f) and f < self.f_opt:\n                        self.f_opt = float(f)\n                        self.x_opt = x.copy()\n                # encourage exploration a bit\n                gscale = min(3.5, gscale * (1.2 + 0.35 * self.rng.random()))\n\n        # final best: choose best finite from archive\n        F_arr_final = np.asarray(archive_F, dtype=float)\n        finite_idxs_final = np.where(np.isfinite(F_arr_final))[0]\n        if finite_idxs_final.size > 0:\n            best_idx = finite_idxs_final[np.argmin(F_arr_final[finite_idxs_final])]\n            best_x = archive_X[best_idx].copy()\n            best_f = float(archive_F[best_idx])\n            self.f_opt = best_f\n            self.x_opt = best_x\n        else:\n            # fallback: random point\n            x = self._uniform_array(lb, ub, n=1)\n            self.x_opt = x.copy()\n            self.f_opt = float(np.inf)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 329, in __call__, the following error occurred:\nTypeError: MemoryGuidedADS.__call__.<locals>.tempered_cauchy() got an unexpected keyword argument 'temp'\nOn line: cand = cand + tempered_cauchy(scale_vec * 0.4, temp=0.4)", "error": "In the code, line 329, in __call__, the following error occurred:\nTypeError: MemoryGuidedADS.__call__.<locals>.tempered_cauchy() got an unexpected keyword argument 'temp'\nOn line: cand = cand + tempered_cauchy(scale_vec * 0.4, temp=0.4)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9b09fb12-4ed0-403b-8d21-492b99677ad5", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        self.init_samples = init_samples if init_samples is not None else max(20, 4 * self.dim)\n        self.archive_max = int(archive_max)\n        # internal trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # -------- utilities ----------\n    def _get_bounds(self, func):\n        # Attempt to extract bounds from common patterns; fallback to [-5,5]\n        lb = None\n        ub = None\n        if hasattr(func, 'bounds'):\n            b = func.bounds\n            if hasattr(b, 'lb') and hasattr(b, 'ub'):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n        if lb is None and hasattr(func, 'lb') and hasattr(func, 'ub'):\n            lb = np.asarray(func.lb, dtype=float)\n            ub = np.asarray(func.ub, dtype=float)\n        if lb is None:\n            try:\n                # sometimes func has attribute 'dimension' and implicit [-5,5]\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n            except Exception:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        # ensure shapes\n        lb = np.asarray(lb).reshape(-1)\n        ub = np.asarray(ub).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            # try to broadcast or fallback\n            lb = np.resize(lb, self.dim)\n            ub = np.resize(ub, self.dim)\n        # sanity clamp\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1e-6\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect repeatedly to bring x within [lb,ub], then clamp if numerical drift\n        x = np.asarray(x, dtype=float)\n        for _ in range(4):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (np.any(below) or np.any(above)):\n                break\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = rng.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        pts = a[:, None] + (b - a)[:, None] * u\n        # permute each column\n        for j in range(dim):\n            rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            samples = self.rng.rand(n, self.dim) * (ub - lb) + lb\n            return samples\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # ensure not to call func more than budget\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            try:\n                f = float(func(x))\n            except Exception:\n                # if function errors, penalize\n                f = np.inf\n            self.evals += 1\n            # bookkeep best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # seed center at middle of bounds\n        center = 0.5 * (lb + ub)\n        f_center = safe_eval(center)\n        X_archive.append(center.copy()); F_archive.append(f_center)\n\n        # initial LHS samples\n        n_init = min(self.init_samples, max(1, self.budget - self.evals))\n        if n_init > 0:\n            lhs = self._lhs01(n_init, self.dim)\n            for i in range(n_init):\n                x = lb + lhs[i] * (ub - lb)\n                f = safe_eval(x)\n                X_archive.append(x.copy())\n                F_archive.append(f)\n                if self.evals >= self.budget:\n                    break\n\n        # add a few purely uniform random points if budget remains\n        while self.evals < self.budget and len(X_archive) < min(self.init_samples + 10, 2 * self.dim + 30):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        archive_max = max(50, min(self.archive_max, 2000))\n        gscale = 0.2 * (ub - lb)  # global per-dim scale\n        success_count = 0\n        iter_since_improve = 0\n        total_iters = 0\n\n        # tempered cauchy\n        def tempered_cauchy(scale_vec, temp=1.0):\n            # sample independent Cauchy components with gaussian tempering for numerical stability\n            z = self.rng.standard_cauchy(size=self.dim) * scale_vec\n            g = self.rng.randn(self.dim) * (scale_vec * 0.1) / max(1.0, temp)\n            return z * (1.0 / (1.0 + np.abs(g)))\n\n        # main loop: continue until budget used\n        while self.evals < self.budget:\n            total_iters += 1\n            # prepare archived arrays\n            X = np.array(X_archive) if X_archive else np.zeros((0, self.dim))\n            F = np.array(F_archive) if F_archive else np.array([])\n\n            finite_mask = np.isfinite(F)\n            if not np.any(finite_mask):\n                # no valid evaluations -> sample uniform\n                cand = self._uniform_array(lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            X_fin = X[finite_mask]\n            q75 = np.percentile(X_fin, 75, axis=0)\n            q25 = np.percentile(X_fin, 25, axis=0)\n            iqr = (q75 - q25) / 1.349  # approximate std\n            std = np.std(X_fin, axis=0)\n            scale = np.where(np.isfinite(iqr) & (iqr > 1e-12), iqr, std)\n            scale = np.where(scale <= 1e-12, 0.2 * (ub - lb), scale)\n            # clamp scale to reasonable portion of domain\n            scale = np.minimum(scale, 0.5 * (ub - lb))\n\n            # choose a base index biased toward better F via softmax on -F\n            finite_F = F.copy()\n            finite_F[~finite_mask] = np.max(F[finite_mask]) + 1.0\n            # numerical stability\n            fmin = np.min(finite_F)\n            scores = - (finite_F - fmin) / (np.std(finite_F) + 1e-9)\n            # small temperature noise\n            probs = np.exp(scores - np.max(scores))\n            probs = probs / np.sum(probs)\n            # pick base\n            base_idx = self.rng.choice(len(X), p=probs)\n            base = X[base_idx].copy()\n\n            # small chance to use global best directly\n            if self.rng.rand() < 0.02 and self.x_opt is not None:\n                base = self.x_opt.copy()\n\n            # donor pool selection (for DE-like moves)\n            idxs = np.arange(len(X))\n            if len(X) >= 3:\n                # pick 3 distinct others\n                others = idxs[idxs != base_idx]\n                a, b, c = self.rng.choice(others, size=3, replace=False)\n                donor = X[a] + 0.8 * (X[b] - X[c])\n            else:\n                donor = base + self.rng.randn(self.dim) * scale\n\n            # choose move type\n            r = self.rng.rand()\n            move_type = 'LOCAL' if r < 0.6 else 'GLOBAL'\n            if r < 0.02:\n                move_type = 'PCA'\n\n            # scale modifiers\n            g_modifier = 1.0 + 0.2 * (self.rng.rand() - 0.5)\n            # produce candidate\n            if move_type == 'GLOBAL':\n                # anchor on either elite or uniform\n                if self.rng.rand() < 0.7:\n                    # pick among top-k elites\n                    k = max(1, min(10, len(X_fin) // 4))\n                    elite_idx = np.argsort(F[finite_mask])[:k]\n                    # map back to original indices\n                    orig_indices = np.where(finite_mask)[0][elite_idx]\n                    anchor = X[self.rng.choice(orig_indices)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n                # tempered cauchy jump\n                t = 1.0 + 0.5 * self.rng.rand()\n                jump = tempered_cauchy(scale * (g_modifier * (0.5 + self.rng.rand())), temp=t)\n                cand = anchor + jump\n                # occasional DE-like differential injection\n                if self.rng.rand() < 0.3:\n                    cand = cand + 0.5 * (donor - base)\n                # coordinate injection\n                if self.rng.rand() < 0.2:\n                    mask = self.rng.rand(self.dim) < 0.2\n                    cand[mask] = base[mask] + np.random.randn(mask.sum()) * scale[mask]\n                # small extra jitter\n                cand = cand + 0.01 * (ub - lb) * self.rng.randn(self.dim)\n            elif move_type == 'PCA':\n                # choose subset of elites and do PCA\n                k = max(3, min(20, len(X_fin) // 5 + 3))\n                idxs_elite = np.argsort(F[finite_mask])[:k]\n                X_el = X_fin[idxs_elite]\n                # center\n                Xc = X_el - np.mean(X_el, axis=0)\n                try:\n                    u, s, vh = np.linalg.svd(Xc, full_matrices=False)\n                    pcs = vh\n                    # perturb along principal components with decaying amplitude\n                    scales = (s / (s[0] + 1e-12)) * scale\n                    weights = self.rng.randn(pcs.shape[0])\n                    perturb = (weights[:, None] * pcs).T.dot(scales)\n                    cand = base + perturb * (0.5 + self.rng.rand() * 1.0)\n                except Exception:\n                    cand = base + self.rng.randn(self.dim) * scale * 0.5\n            else:  # LOCAL\n                anisotropic = self.rng.randn(self.dim) * (scale * (0.2 + 0.8 * self.rng.rand()))\n                # sometimes attracted to median\n                if self.rng.rand() < 0.3:\n                    median = np.median(X_fin, axis=0)\n                    cand = base + 0.5 * (median - base) + anisotropic\n                else:\n                    cand = base + anisotropic\n                # small DE-like mixing inside local branch sometimes\n                if self.rng.rand() < 0.15:\n                    cand = cand + 0.2 * (donor - base)\n\n            # tempered cauchy tiny escape occasionally\n            if self.rng.rand() < 0.01:\n                cand = cand + tempered_cauchy(0.05 * (ub - lb), temp=2.0)\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            if not np.all(np.isfinite(cand)):\n                mask = ~np.isfinite(cand)\n                cand[mask] = self._uniform_array(lb, ub, n=1)[mask]\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n            # update success metrics\n            finite_inds = np.where(np.isfinite(F_archive))[0]\n            if finite_inds.size > 0:\n                # recompute ranks among finite\n                finite_vals = np.array(F_archive)[finite_inds]\n                rank_pos = np.sum(finite_vals <= f_cand)\n                # success if in best quarter\n                if rank_pos <= max(1, int(0.25 * max(1, finite_vals.size))):\n                    success = True\n                    success_count += 1\n                    iter_since_improve = 0\n                else:\n                    success = False\n                    iter_since_improve += 1\n            else:\n                success = False\n                iter_since_improve += 1\n\n            # short-term adaptation of gscale every few iterations\n            if total_iters % 10 == 0:\n                if success_count > 3:\n                    # encourage smaller scales (exploitation)\n                    gscale = gscale * (0.95 - 0.02 * self.rng.rand())\n                else:\n                    # encourage exploration\n                    gscale = gscale * (1.02 + 0.02 * self.rng.rand())\n                # keep within domain fractions\n                gscale = np.minimum(np.maximum(gscale, 1e-6 * (ub - lb)), 0.6 * (ub - lb))\n                success_count = 0\n\n            # prune archive if too large\n            if len(X_archive) > archive_max:\n                # keep best half of finite and some random others + some inf entries\n                F_arr = np.array(F_archive)\n                finite_idx = np.where(np.isfinite(F_arr))[0]\n                inf_idx = np.where(~np.isfinite(F_arr))[0]\n                keep = set()\n                if finite_idx.size > 0:\n                    nkeep_best = max(2, int(0.5 * finite_idx.size))\n                    best_fin = finite_idx[np.argsort(F_arr[finite_idx])[:nkeep_best]]\n                    for ii in best_fin:\n                        keep.add(ii)\n                    # keep some random finite\n                    rem_fin = list(set(finite_idx) - set(best_fin))\n                    self.rng.shuffle(rem_fin)\n                    for ii in rem_fin[:int(0.2 * len(rem_fin))]:\n                        keep.add(ii)\n                # add some infs for diversity\n                self.rng.shuffle(inf_idx)\n                for ii in inf_idx[:min(10, len(inf_idx))]:\n                    keep.add(ii)\n                # if too few kept, add random ones\n                all_idx = np.arange(len(X_archive)).tolist()\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= archive_max:\n                        break\n                    keep.add(ii)\n                keep = sorted(list(keep))\n                X_archive = [X_archive[i] for i in keep]\n                F_archive = [F_archive[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(30, 6 * self.dim):\n                # micro-restart: generate around best and add random uniform points\n                iter_since_improve = 0\n                # pick current best\n                if self.x_opt is None:\n                    base_best = self._uniform_array(lb, ub)\n                else:\n                    base_best = self.x_opt.copy()\n                nlocal = min(10 + self.dim, max(5, self.budget - self.evals - 1))\n                for i in range(nlocal):\n                    # perturb around best using per-dim scale and some coordinate mixing\n                    pert = base_best + self.rng.randn(self.dim) * (0.5 * scale * (0.5 + self.rng.rand(self.dim)))\n                    # mix some coords uniform\n                    if self.rng.rand() < 0.4:\n                        mask = self.rng.rand(self.dim) < 0.2\n                        uu = self._uniform_array(lb, ub)\n                        pert[mask] = uu[mask]\n                    pert = self._reflect_bounds(pert, lb, ub)\n                    if self.evals >= self.budget:\n                        break\n                    f = safe_eval(pert)\n                    X_archive.append(pert.copy()); F_archive.append(f)\n                # add some pure randoms to diversify\n                nrand = min(5 + self.dim // 2, max(1, self.budget - self.evals))\n                for i in range(nrand):\n                    if self.evals >= self.budget:\n                        break\n                    xx = self._uniform_array(lb, ub)\n                    f = safe_eval(xx)\n                    X_archive.append(xx.copy()); F_archive.append(f)\n\n        # final best: choose best finite from archive\n        F_arr = np.array(F_archive)\n        X_arr = np.array(X_archive)\n        finite_idx = np.where(np.isfinite(F_arr))[0]\n        if finite_idx.size > 0:\n            best_i = finite_idx[np.argmin(F_arr[finite_idx])]\n            return float(F_arr[best_i]), X_arr[best_i].copy()\n        else:\n            # fallback: random point\n            x = self._uniform_array(lb, ub)\n            return float(func(x)), x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4e8d96d6-3f52-4c81-9071-4922acfd0529", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=10, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        self.archive_max = int(archive_max)\n        self.init_samples = int(init_samples)\n\n        # internal trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # -------- utilities ----------\n    def _get_bounds(self, func):\n        # Attempt to extract bounds from common patterns; fallback to [-5,5]\n        lb = None\n        ub = None\n        if hasattr(func, 'bounds'):\n            b = func.bounds\n            if hasattr(b, 'lb') and hasattr(b, 'ub'):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n        if lb is None and hasattr(func, 'lb') and hasattr(func, 'ub'):\n            lb = np.asarray(func.lb, dtype=float)\n            ub = np.asarray(func.ub, dtype=float)\n        if lb is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # ensure shapes\n        lb = np.asarray(lb).reshape(-1)\n        ub = np.asarray(ub).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n        # sanity clamp and ensure ub>lb\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1e-6\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect repeatedly to bring x within [lb,ub], then clamp if numerical drift\n        x = np.asarray(x, dtype=float)\n        for _ in range(6):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (np.any(below) or np.any(above)):\n                break\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = rng.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        pts = a[:, None] + (b - a)[:, None] * u\n        # permute each column\n        for j in range(dim):\n            rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            samples = self.rng.rand(n, self.dim) * (ub - lb) + lb\n            return samples\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # sample independent Cauchy components with light gaussian tempering for numerical stability\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        # draw standard cauchy then temper with gaussian noise\n        z = self.rng.standard_cauchy(size=self.dim) * scale_vec\n        g = self.rng.randn(self.dim) * (np.maximum(scale_vec, 1e-12) * 0.1) / max(1.0, temp)\n        # divide by (1+|g|) to temper heavy tails slightly\n        return z * (1.0 / (1.0 + np.abs(g)))\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            # increment after attempting\n            self.evals += 1\n            # bookkeep best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # evaluate center\n        f_center = safe_eval(center)\n        X_archive.append(center.copy()); F_archive.append(f_center)\n\n        # initial LHS samples\n        n_init = min(self.init_samples, max(0, self.budget - self.evals))\n        if n_init > 0:\n            lhs = self._lhs01(n_init, self.dim)\n            for i in range(n_init):\n                if self.evals >= self.budget:\n                    break\n                x = lb + lhs[i] * (ub - lb)\n                f = safe_eval(x)\n                X_archive.append(x.copy())\n                F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        for _ in range(2):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        archive_max = max(50, min(self.archive_max, 2000))\n        success_count = 0\n        iter_since_improve = 0\n        total_iters = 0\n\n        # global gscale (per-dimension)\n        gscale = 0.1 * (ub - lb)\n\n        # main loop: continue until budget used\n        while self.evals < self.budget:\n            total_iters += 1\n\n            # prepare archived arrays\n            X = np.array(X_archive) if X_archive else np.zeros((0, self.dim))\n            F = np.array(F_archive) if F_archive else np.array([])\n\n            finite_mask = np.isfinite(F)\n            if not np.any(finite_mask):\n                # no valid evaluations -> sample uniform\n                cand = self._uniform_array(lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            X_fin = X[finite_mask]\n            if X_fin.shape[0] < 2:\n                # not enough points => sample near center/uniform\n                cand = center + self.rng.randn(self.dim) * (0.1 * (ub - lb))\n                cand = self._reflect_bounds(cand, lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            q75 = np.percentile(X_fin, 75, axis=0)\n            q25 = np.percentile(X_fin, 25, axis=0)\n            iqr = (q75 - q25) / 1.349  # approximate std\n            std = np.std(X_fin, axis=0)\n            scale = np.where(np.isfinite(iqr) & (iqr > 1e-12), iqr, std)\n            scale = np.where(scale <= 1e-12, 0.2 * (ub - lb), scale)\n            # clamp scale to reasonable portion of domain\n            scale = np.minimum(scale, 0.5 * (ub - lb))\n\n            # choose a base index biased toward better F via softmax on -F\n            finite_F = F.copy()\n            # give a large penalty to non-finite so they are unlikely base\n            finite_F[~finite_mask] = np.max(F[finite_mask]) + 1.0\n            fmin = np.min(finite_F)\n            denom = np.std(finite_F) + 1e-9\n            scores = - (finite_F - fmin) / denom\n            # small temperature noise for exploration\n            scores = scores + 0.2 * (self.rng.rand(*scores.shape) - 0.5)\n            probs = np.exp(scores - np.max(scores))\n            probs = probs / np.sum(probs)\n            base_idx = self.rng.choice(len(X), p=probs)\n            base = X[base_idx].copy()\n\n            # small chance to use global best directly\n            if self.rng.rand() < 0.02 and self.x_opt is not None:\n                base = self.x_opt.copy()\n\n            # donor pool selection (for DE-like moves)\n            if len(X) >= 4:\n                idxs = np.arange(len(X))\n                others = idxs[idxs != base_idx]\n                a, b, c = self.rng.choice(others, size=3, replace=False)\n                donor = X[a] + 0.8 * (X[b] - X[c])\n            else:\n                donor = base + self.rng.randn(self.dim) * scale\n\n            # choose move type\n            r = self.rng.rand()\n            move_type = 'LOCAL' if r < 0.6 else 'GLOBAL'\n            if r < 0.02:\n                move_type = 'PCA'\n\n            # scale modifiers\n            g_modifier = 1.0 + 0.2 * (self.rng.rand() - 0.5)\n\n            # produce candidate\n            cand = base.copy()\n            if move_type == 'GLOBAL':\n                # anchor on either elite or uniform\n                if self.rng.rand() < 0.7:\n                    # pick among top-k elites\n                    # build indices of finite sorted by F\n                    fin_idx_list = np.where(finite_mask)[0]\n                    k = max(1, min(10, len(fin_idx_list) // 4))\n                    elite_idx = np.argsort(F[fin_idx_list])[:k]\n                    orig_indices = fin_idx_list[elite_idx]\n                    anchor = X[self.rng.choice(orig_indices)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n                # tempered cauchy jump\n                t = 1.0 + 0.5 * self.rng.rand()\n                jump = self._tempered_cauchy(gscale * (0.5 + self.rng.rand(self.dim)*0.5), temp=t)\n                cand = anchor + jump\n                # occasional DE-like differential injection\n                if self.rng.rand() < 0.3:\n                    cand = cand + 0.5 * (donor - base)\n                # coordinate injection\n                if self.rng.rand() < 0.2:\n                    mask = (self.rng.rand(self.dim) < 0.2)\n                    if mask.any():\n                        # use local gaussian per-dim\n                        cand[mask] = base[mask] + self.rng.randn(mask.sum()) * scale[mask]\n                # small extra jitter\n                cand = cand + 0.01 * (ub - lb) * self.rng.randn(self.dim)\n\n            elif move_type == 'PCA':\n                # choose subset of elites and do PCA\n                fin_idx_list = np.where(finite_mask)[0]\n                k = max(3, min(20, max(3, len(fin_idx_list) // 5 + 3)))\n                k = min(k, X_fin.shape[0])\n                idxs_elite = np.argsort(F[finite_mask])[:k]\n                X_el = X_fin[idxs_elite]\n                # center\n                Xc = X_el - np.mean(X_el, axis=0)\n                try:\n                    u, s, vh = np.linalg.svd(Xc, full_matrices=False)\n                    pcs = vh  # principal directions (rows)\n                    # build perturbation using top components with random weights\n                    top = min(pcs.shape[0], max(1, int(1 + self.rng.rand() * (pcs.shape[0]-1))))\n                    weights = self.rng.randn(top)\n                    scales_pc = (s[:top] / (s[0] + 1e-12)) * (scale)\n                    perturb = (weights[:, None] * pcs[:top]).T.dot(scales_pc)\n                    cand = base + perturb * (0.5 + self.rng.rand() * 1.0)\n                except Exception:\n                    cand = base + self.rng.randn(self.dim) * scale * 0.5\n\n            else:  # LOCAL\n                anisotropic = self.rng.randn(self.dim) * (scale * (0.2 + 0.8 * self.rng.rand()))\n                # sometimes attracted to median\n                if self.rng.rand() < 0.3:\n                    median = np.median(X_fin, axis=0)\n                    cand = base + 0.5 * (median - base) + anisotropic\n                else:\n                    cand = base + anisotropic\n                # small DE-like mixing inside local branch sometimes\n                if self.rng.rand() < 0.15:\n                    cand = cand + 0.2 * (donor - base)\n\n            # tempered cauchy tiny escape occasionally\n            if self.rng.rand() < 0.01:\n                cand = cand + self._tempered_cauchy(0.05 * (ub - lb), temp=2.0)\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            if not np.all(np.isfinite(cand)):\n                mask = ~np.isfinite(cand)\n                cand[mask] = self._uniform_array(lb, ub)[mask]\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n            # update success metrics\n            finite_inds = np.where(np.isfinite(F_archive))[0]\n            if finite_inds.size > 0:\n                finite_vals = np.array(F_archive)[finite_inds]\n                # rank of new candidate among finite (smaller is better)\n                rank_pos = np.sum(finite_vals <= f_cand)\n                # success if in best quarter (rank small)\n                if rank_pos <= max(1, int(0.25 * max(1, finite_vals.size))):\n                    success_count += 1\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n            else:\n                iter_since_improve += 1\n\n            # short-term adaptation of gscale every few iterations\n            if total_iters % 10 == 0:\n                if success_count > 3:\n                    # encourage smaller scales (exploitation)\n                    gscale = gscale * (0.95 - 0.02 * self.rng.rand())\n                else:\n                    # encourage exploration\n                    gscale = gscale * (1.02 + 0.02 * self.rng.rand())\n                # keep within domain fractions (per-dim)\n                gscale = np.minimum(np.maximum(gscale, 1e-6 * (ub - lb)), 0.6 * (ub - lb))\n                success_count = 0\n\n            # prune archive if too large\n            if len(X_archive) > archive_max:\n                F_arr = np.array(F_archive)\n                finite_idx = np.where(np.isfinite(F_arr))[0].tolist()\n                inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                keep = set()\n                if len(finite_idx) > 0:\n                    nkeep_best = max(2, int(0.5 * len(finite_idx)))\n                    best_fin = np.array(finite_idx)[np.argsort(F_arr[finite_idx])[:nkeep_best]].tolist()\n                    for ii in best_fin:\n                        keep.add(ii)\n                    # keep some random finite others\n                    rem_fin = [ii for ii in finite_idx if ii not in best_fin]\n                    self.rng.shuffle(rem_fin)\n                    for ii in rem_fin[:max(0, int(0.2 * len(rem_fin)))]:\n                        keep.add(ii)\n                # add some infs for diversity\n                self.rng.shuffle(inf_idx)\n                for ii in inf_idx[:min(10, len(inf_idx))]:\n                    keep.add(ii)\n                # if too few kept, add random ones\n                all_idx = list(range(len(X_archive)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= archive_max:\n                        break\n                    keep.add(ii)\n                keep = sorted(list(keep))[:archive_max]\n                X_archive = [X_archive[i] for i in keep]\n                F_archive = [F_archive[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(30, 6 * self.dim):\n                # micro-restart: generate around best and add random uniform points\n                iter_since_improve = 0\n                if self.x_opt is None:\n                    base_best = self._uniform_array(lb, ub)\n                else:\n                    base_best = self.x_opt.copy()\n                nlocal = min(10 + self.dim, max(5, self.budget - self.evals - 1))\n                for i in range(nlocal):\n                    if self.evals >= self.budget:\n                        break\n                    # perturb around best using per-dim scale and some coordinate mixing\n                    pert = base_best + self.rng.randn(self.dim) * (0.5 * scale * (0.5 + self.rng.rand(self.dim)))\n                    # mix some coords uniform with small prob\n                    mask = (self.rng.rand(self.dim) < 0.1)\n                    if mask.any():\n                        uu = self._uniform_array(lb, ub)\n                        pert[mask] = uu[mask]\n                    pert = self._reflect_bounds(pert, lb, ub)\n                    f = safe_eval(pert)\n                    X_archive.append(pert.copy()); F_archive.append(f)\n                # add some pure randoms to diversify\n                nrand = min(5 + self.dim // 2, max(1, self.budget - self.evals))\n                for _ in range(nrand):\n                    if self.evals >= self.budget:\n                        break\n                    xx = self._uniform_array(lb, ub)\n                    f = safe_eval(xx)\n                    X_archive.append(xx.copy()); F_archive.append(f)\n\n        # final best: choose best finite from archive\n        F_arr = np.array(F_archive)\n        X_arr = np.array(X_archive)\n        finite_idx = np.where(np.isfinite(F_arr))[0]\n        if finite_idx.size > 0:\n            best_i = finite_idx[np.argmin(F_arr[finite_idx])]\n            return float(F_arr[best_i]), X_arr[best_i].copy()\n        else:\n            # fallback: random point\n            x = self._uniform_array(lb, ub)\n            return float(func(x)), x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d868f783-262c-4ff9-b402-4344c011a5bf", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite subspace perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=10, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        self.archive_max = int(archive_max)\n        self.init_samples = int(init_samples)\n\n        # internal trackers (filled on call)\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # -------- utilities ----------\n    def _get_bounds(self, func):\n        # Attempt to extract bounds from common patterns; fallback to [-5,5]\n        lb = None\n        ub = None\n        # common pattern: func.bounds.lb / ub or func.lb / func.ub\n        if hasattr(func, 'bounds'):\n            b = func.bounds\n            if hasattr(b, 'lb') and hasattr(b, 'ub'):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n        if lb is None and hasattr(func, 'lb') and hasattr(func, 'ub'):\n            lb = np.asarray(func.lb, dtype=float)\n            ub = np.asarray(func.ub, dtype=float)\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb).reshape(-1)\n            ub = np.asarray(ub).reshape(-1)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n            # sanity clamp and ensure ub>lb\n            lb = lb.astype(float)\n            ub = ub.astype(float)\n            for i in range(self.dim):\n                if not np.isfinite(lb[i]):\n                    lb[i] = -5.0\n                if not np.isfinite(ub[i]):\n                    ub[i] = 5.0\n                if ub[i] <= lb[i]:\n                    ub[i] = lb[i] + 1e-6\n        return lb.copy(), ub.copy()\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect repeatedly to bring x within [lb,ub], then clamp if numerical drift\n        x = np.asarray(x, dtype=float)\n        for _ in range(8):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (np.any(below) or np.any(above)):\n                break\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        pts = a[:, None] + (b - a)[:, None] * u\n        # permute each column\n        for j in range(dim):\n            self.rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # sample independent Cauchy components with light gaussian tempering for numerical stability\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        z = self.rng.standard_cauchy(size=self.dim) * scale_vec\n        g = self.rng.randn(self.dim) * (np.maximum(scale_vec, 1e-12) * 0.12) / max(1.0, temp)\n        # divide by (1+|g|) to temper heavy tails slightly\n        return z * (1.0 / (1.0 + np.abs(g)))\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # never call func if budget exhausted\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            # increment after attempting\n            self.evals += 1\n            # bookkeep best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # evaluate center (good starting anchor)\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            X_archive.append(center.copy()); F_archive.append(f_center)\n\n        # initial LHS samples\n        n_init = min(self.init_samples, max(0, self.budget - self.evals))\n        if n_init > 0:\n            lhs = self._lhs01(n_init, self.dim)\n            for i in range(n_init):\n                if self.evals >= self.budget:\n                    break\n                x = lb + lhs[i] * (ub - lb)\n                f = safe_eval(x)\n                X_archive.append(x.copy()); F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        for _ in range(2):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        archive_max = max(50, min(self.archive_max, 2000))\n        success_count = 0\n        iter_since_improve = 0\n        total_iters = 0\n\n        # global gscale (per-dimension) initial fraction of domain\n        gscale = 0.12 * (ub - lb)\n\n        # main loop: continue until budget used\n        while self.evals < self.budget:\n            total_iters += 1\n\n            # prepare archived arrays\n            X = np.array(X_archive) if len(X_archive) > 0 else np.zeros((0, self.dim))\n            F = np.array(F_archive) if len(F_archive) > 0 else np.array([])\n\n            finite_mask = np.isfinite(F) if F.size > 0 else np.zeros(0, dtype=bool)\n\n            # if no valid evaluations -> sample uniform\n            if not np.any(finite_mask):\n                cand = self._uniform_array(lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            X_fin = X[finite_mask]\n            if X_fin.shape[0] < 3:\n                # not enough points => sample near center/uniform\n                cand = center + self.rng.randn(self.dim) * (0.12 * (ub - lb))\n                cand = self._reflect_bounds(cand, lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            q75 = np.percentile(X_fin, 75, axis=0)\n            q25 = np.percentile(X_fin, 25, axis=0)\n            iqr = (q75 - q25) / 1.349  # approximate std\n            std = np.std(X_fin, axis=0)\n            scale = np.where(np.isfinite(iqr) & (iqr > 1e-12), iqr, std)\n            scale = np.where(scale <= 1e-12, 0.2 * (ub - lb), scale)\n            # clamp scale to reasonable portion of domain\n            scale = np.minimum(scale, 0.6 * (ub - lb))\n\n            # choose a base index biased toward better F via softmax on -F\n            finite_F = F.copy()\n            fmin = np.min(F[finite_mask])\n            fmax = np.max(F[finite_mask])\n            denom = max(1e-12, fmax - fmin)\n            # give a large penalty to non-finite so they are unlikely base\n            finite_F[~finite_mask] = fmax + 1.0\n            scores = - (finite_F - fmin) / denom\n            # small temperature noise for exploration\n            scores = scores + 0.2 * (self.rng.rand(*scores.shape) - 0.5)\n            # stabilize and exponentiate\n            exp_scores = np.exp(scores - np.max(scores))\n            probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n            base_idx = int(self.rng.choice(len(X), p=probs))\n            base = X[base_idx].copy()\n\n            # small chance to use global best directly\n            if self.rng.rand() < 0.02 and self.x_opt is not None:\n                base = self.x_opt.copy()\n\n            # donor pool selection (for DE-like moves)\n            donor = None\n            if len(X) >= 4:\n                idxs = np.arange(len(X))\n                others = idxs[idxs != base_idx]\n                a, b, c = self.rng.choice(others, size=3, replace=False)\n                donor = X[a].copy()\n                donor_b = X[b].copy()\n                donor_c = X[c].copy()\n            else:\n                # if not enough, pick random uniform point as donor\n                donor = self._uniform_array(lb, ub)\n\n            # choose move type\n            r = self.rng.rand()\n            # LOCAL more probable, GLOBAL occasional\n            move_type = 'LOCAL' if r < 0.65 else 'GLOBAL'\n            if r < 0.015:\n                move_type = 'ESCAPE'  # rare escape\n\n            # scale modifiers\n            g_modifier = 1.0 + 0.5 * (self.rng.rand() - 0.5)\n\n            # produce candidate\n            cand = base.copy()\n            if move_type == 'GLOBAL':\n                # anchor on either elite or uniform\n                if self.rng.rand() < 0.7 and X_fin.shape[0] >= 3:\n                    # pick among top-k elites\n                    fin_idx_list = np.where(finite_mask)[0]\n                    k = max(1, min(10, max(1, len(fin_idx_list) // 4)))\n                    sorted_fin = np.argsort(F[fin_idx_list])\n                    elite_idxs = fin_idx_list[sorted_fin[:k]]\n                    anchor = X[self.rng.choice(elite_idxs)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n                # tempered cauchy jump anchored at elite/uniform\n                t = 1.0 + 1.5 * self.rng.rand()\n                jump = self._tempered_cauchy(gscale * (0.5 + self.rng.rand(self.dim) * 0.7) * g_modifier, temp=t)\n                cand = anchor + jump\n                # occasional DE-like differential injection (mix donors)\n                if self.rng.rand() < 0.35 and donor is not None:\n                    cand = cand + 0.5 * (donor - base)\n                # coordinate injection: replace subset with local gaussian\n                if self.rng.rand() < 0.22:\n                    mask = (self.rng.rand(self.dim) < 0.25)\n                    if mask.any():\n                        gauss = self.rng.randn(mask.sum()) * scale[mask]\n                        cand[mask] = base[mask] + gauss\n                # small extra jitter\n                cand = cand + 0.02 * (ub - lb) * self.rng.randn(self.dim)\n\n                # PCA-guided elite perturbation sometimes\n                if self.rng.rand() < 0.5 and X_fin.shape[0] >= 4:\n                    # choose subset of elites and do PCA\n                    fin_idx_list = np.where(finite_mask)[0]\n                    k = max(3, min(20, max(3, len(fin_idx_list) // 5 + 3)))\n                    sorted_fin = np.argsort(F[fin_idx_list])\n                    idxs_elite = sorted_fin[:k]\n                    X_el = X_fin[idxs_elite]\n                    # center\n                    Xc = X_el - np.mean(X_el, axis=0)\n                    try:\n                        u, s, vh = np.linalg.svd(Xc, full_matrices=False)\n                        pcs = vh  # principal directions (rows)\n                        # build perturbation using top components with random weights\n                        top = min(pcs.shape[0], max(1, int(1 + self.rng.rand() * (pcs.shape[0] - 1))))\n                        weights = self.rng.randn(top) * (0.5 + self.rng.rand(top))\n                        # scale per PC proportional to singular values and global scale\n                        scales_pc = (s[:top] / (np.maximum(np.mean(s[:top]), 1e-12))) * (0.3 * np.mean(scale))\n                        perturb = (pcs[:top].T @ (weights * scales_pc))\n                        cand = cand + perturb\n                    except Exception:\n                        pass\n\n            elif move_type == 'ESCAPE':\n                # a strong but rare tempered Cauchy escape from random anchor\n                anchor = self._uniform_array(lb, ub) if self.rng.rand() < 0.5 else base\n                jump = self._tempered_cauchy(1.2 * (ub - lb) * (0.4 + self.rng.rand(self.dim) * 0.6),\n                                             temp=0.7 + self.rng.rand())\n                cand = anchor + jump\n                # small smoothing\n                cand = cand + 0.03 * (ub - lb) * self.rng.randn(self.dim)\n\n            else:  # LOCAL\n                # anisotropic gaussian local search around base\n                anisotropic = self.rng.randn(self.dim) * (scale * (0.08 + 0.92 * self.rng.rand(self.dim)))\n                # sometimes attracted to median of archive\n                if self.rng.rand() < 0.18:\n                    median = np.median(X_fin, axis=0)\n                    cand = median + anisotropic * 0.5\n                else:\n                    cand = base + anisotropic\n                # small DE-like mixing inside local branch sometimes\n                if self.rng.rand() < 0.18 and donor is not None:\n                    cand = cand + 0.18 * (donor - base)\n                # tempered cauchy tiny escape occasionally\n                if self.rng.rand() < 0.05:\n                    cand = cand + self._tempered_cauchy(0.04 * (ub - lb), temp=2.0)\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            if not np.all(np.isfinite(cand)):\n                mask = ~np.isfinite(cand)\n                replacement = self._uniform_array(lb, ub)\n                cand[mask] = replacement[mask]\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n            # update success metrics\n            finite_vals = np.array(F_archive)[np.isfinite(F_archive)]\n            if finite_vals.size > 0:\n                # rank of new candidate among finite (smaller is better)\n                rank_pos = int(np.sum(finite_vals <= f_cand))\n                if rank_pos <= max(1, int(0.25 * max(1, finite_vals.size))):\n                    success_count += 1\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n            else:\n                iter_since_improve += 1\n\n            # short-term adaptation of gscale every few iterations\n            if total_iters % 10 == 0:\n                if success_count > 3:\n                    # encourage smaller scales (exploitation)\n                    gscale = gscale * (0.95 - 0.02 * self.rng.rand())\n                else:\n                    # encourage exploration\n                    gscale = gscale * (1.02 + 0.06 * self.rng.rand())\n                # keep within domain fractions (per-dim)\n                gscale = np.minimum(np.maximum(gscale, 1e-6 * (ub - lb)), 0.6 * (ub - lb))\n                success_count = 0\n\n            # prune archive if too large\n            if len(X_archive) > archive_max:\n                F_arr = np.array(F_archive)\n                inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                finite_idx = np.where(np.isfinite(F_arr))[0].tolist()\n                keep = set()\n                # keep a number of best finite\n                if len(finite_idx) > 0:\n                    nkeep_best = max(2, int(0.5 * len(finite_idx)))\n                    best_fin = np.array(finite_idx)[np.argsort(F_arr[finite_idx])[:nkeep_best]].tolist()\n                    for ii in best_fin:\n                        keep.add(ii)\n                    # keep some random finite others\n                    rem_fin = [ii for ii in finite_idx if ii not in best_fin]\n                    self.rng.shuffle(rem_fin)\n                    for ii in rem_fin[:max(0, int(0.2 * len(rem_fin)))]:\n                        keep.add(ii)\n                # add some infs for diversity\n                self.rng.shuffle(inf_idx)\n                for ii in inf_idx[:min(10, len(inf_idx))]:\n                    keep.add(ii)\n                # if too few kept, add random ones\n                all_idx = list(range(len(X_archive)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= archive_max:\n                        break\n                    keep.add(ii)\n                keep = sorted(list(keep))[:archive_max]\n                X_archive = [X_archive[i] for i in keep]\n                F_archive = [F_archive[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(30, 6 * self.dim):\n                # micro-restart: generate around best and add random uniform points\n                iter_since_improve = 0\n                if self.x_opt is None:\n                    base_best = center.copy()\n                else:\n                    base_best = self.x_opt.copy()\n                # small local search around best\n                nlocal = min(10 + self.dim, max(5, self.budget - self.evals - 1))\n                for i in range(nlocal):\n                    if self.evals >= self.budget:\n                        break\n                    pert = base_best + self.rng.randn(self.dim) * (0.5 * scale * (0.5 + self.rng.rand(self.dim)))\n                    # mix some coords uniform with small prob\n                    mask = (self.rng.rand(self.dim) < 0.08)\n                    if mask.any():\n                        uu = self._uniform_array(lb, ub)\n                        pert[mask] = uu[mask]\n                    pert = self._reflect_bounds(pert, lb, ub)\n                    f = safe_eval(pert)\n                    X_archive.append(pert.copy()); F_archive.append(f)\n                # add some pure randoms to diversify\n                nrand = min(5 + self.dim // 2, max(1, self.budget - self.evals))\n                for _ in range(nrand):\n                    if self.evals >= self.budget:\n                        break\n                    xx = self._uniform_array(lb, ub)\n                    f = safe_eval(xx)\n                    X_archive.append(xx.copy()); F_archive.append(f)\n\n        # final best: choose best finite from archive\n        if len(F_archive) == 0:\n            # fallback: random point\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return float(f), x\n        F_arr = np.array(F_archive)\n        X_arr = np.array(X_archive)\n        finite_idx = np.where(np.isfinite(F_arr))[0]\n        if finite_idx.size > 0:\n            best_i = int(finite_idx[np.argmin(F_arr[finite_idx])])\n            return float(F_arr[best_i]), X_arr[best_i].copy()\n        else:\n            # fallback: random point\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return float(f), x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e6437bed-5f06-40f5-8878-b923a8b59481", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=10, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n\n        # internal trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # -------- utilities ----------\n    def _get_bounds(self, func):\n        # Try to extract bounds from passed func; fallback to [-5,5]\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, 'bounds'):\n                b = func.bounds\n                if hasattr(b, 'lb') and hasattr(b, 'ub'):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n        except Exception:\n            lb = ub = None\n\n        # fallback attempt: some testbeds expose func.lb/func.ub or func.ub attr\n        if lb is None or ub is None:\n            if hasattr(func, 'lb') and hasattr(func, 'ub'):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    lb = ub = None\n        if lb is None or ub is None:\n            if hasattr(func, 'ub'):\n                try:\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    ub = None\n            if hasattr(func, 'lb'):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                except Exception:\n                    lb = None\n\n        if lb is None:\n            lb = np.full(self.dim, -5.0)\n        if ub is None:\n            ub = np.full(self.dim, 5.0)\n\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        # if single numbers provided\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # sanitize\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1e-6\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.asarray(x, dtype=float).copy()\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        # reflect repeatedly\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin Hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        pts = a[:, None] + (b - a)[:, None] * u\n        # permute each column\n        for j in range(dim):\n            self.rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        # standard cauchy with slight gaussian tempering\n        z = self.rng.standard_cauchy(size=self.dim) * scale_vec\n        g = 0.5 * self.rng.randn(self.dim)  # small gaussian temper\n        return z * (1.0 / (1.0 + np.abs(g) * temp))\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            # bookkeep best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # control variables\n        success_count = 0\n        iter_since_improve = 0\n        total_iters = 0\n\n        # global gscale (per-dimension)\n        gscale = 0.1 * (ub - lb)\n\n        # evaluate center if budget allows\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            X_archive.append(center.copy()); F_archive.append(f_center)\n\n        # initial LHS samples\n        n_init = min(self.init_samples, max(0, self.budget - self.evals))\n        if n_init > 0:\n            lhs = self._lhs01(n_init, self.dim)\n            for i in range(n_init):\n                if self.evals >= self.budget:\n                    break\n                x = lb + lhs[i] * (ub - lb)\n                f = safe_eval(x)\n                X_archive.append(x.copy()); F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        for _ in range(2):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # main loop: continue until budget used\n        while self.evals < self.budget:\n            total_iters += 1\n\n            # prepare archived arrays\n            X = np.array(X_archive) if X_archive else np.zeros((0, self.dim))\n            F = np.array(F_archive) if F_archive else np.array([])\n\n            finite_mask = np.isfinite(F)\n            X_fin = X[finite_mask] if X.size and F.size else np.zeros((0, self.dim))\n            F_fin = F[finite_mask] if F.size else np.array([])\n\n            if X_fin.shape[0] == 0:\n                # no valid evaluations -> sample uniform\n                cand = self._uniform_array(lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            if X_fin.shape[0] < 2:\n                # not enough points => sample near center/uniform\n                cand = center + self.rng.randn(self.dim) * (0.1 * (ub - lb))\n                cand = self._reflect_bounds(cand, lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            q75 = np.percentile(X_fin, 75, axis=0)\n            q25 = np.percentile(X_fin, 25, axis=0)\n            iqr = (q75 - q25) / 1.349  # approximate std\n            std = np.std(X_fin, axis=0)\n            scale = np.where(np.isfinite(iqr) & (iqr > 1e-12), iqr, std)\n            # ensure minimum scale fraction\n            domain = (ub - lb)\n            min_scale = 1e-6 * domain\n            scale = np.maximum(scale, min_scale)\n            # clamp scale to reasonable portion of domain\n            scale = np.minimum(scale, 0.5 * domain)\n            # combine with global gscale\n            scale = np.maximum(scale, gscale * 0.5)\n\n            # choose a base index biased toward better F via softmax on -F\n            finite_F = F.copy()\n            # give a large penalty to non-finite so they are unlikely base\n            if finite_mask.any():\n                finite_F[~finite_mask] = np.max(F[finite_mask]) + 1.0\n            else:\n                finite_F[:] = 0.0\n            fmin = np.min(finite_F)\n            denom = np.std(finite_F) + 1e-9\n            scores = - (finite_F - fmin) / denom\n            # small temperature noise for exploration\n            scores = scores + 0.2 * (self.rng.rand(*scores.shape) - 0.5)\n            probs = np.exp(scores - np.max(scores))\n            probs = probs / np.sum(probs)\n            base_idx = self.rng.choice(len(X), p=probs)\n            base = X[base_idx].copy()\n\n            # small chance to use global best directly\n            if self.rng.rand() < 0.02 and self.x_opt is not None:\n                base = self.x_opt.copy()\n\n            # donor pool selection (for DE-like moves)\n            if len(X) >= 4:\n                idxs = np.arange(len(X))\n                others = idxs[idxs != base_idx]\n                if others.size >= 3:\n                    a, b, c = self.rng.choice(others, size=3, replace=False)\n                    donor = X[a] + 0.8 * (X[b] - X[c])\n                else:\n                    donor = base + self.rng.randn(self.dim) * scale\n            else:\n                donor = base + self.rng.randn(self.dim) * scale\n\n            # choose move type\n            r = self.rng.rand()\n            move_type = 'LOCAL' if r < 0.6 else 'GLOBAL'\n            if r < 0.02:\n                move_type = 'PCA'\n\n            # scale modifiers\n            g_modifier = 1.0 + 0.2 * (self.rng.rand() - 0.5)\n\n            # produce candidate\n            cand = None\n            if move_type == 'GLOBAL':\n                # anchor on either elite or uniform\n                if self.rng.rand() < 0.7 and F_fin.size > 0:\n                    fin_idx_list = np.where(finite_mask)[0]\n                    k = max(1, min(10, len(fin_idx_list) // 4))\n                    sorted_fin = fin_idx_list[np.argsort(F[fin_idx_list])]\n                    k = max(1, min(k, len(sorted_fin)))\n                    elite_subset = sorted_fin[:k]\n                    anchor = X[self.rng.choice(elite_subset)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n\n                # tempered cauchy jump\n                t = 1.0 + 0.5 * self.rng.rand()\n                jump_scale = gscale * (0.5 + self.rng.rand(self.dim) * 0.5)\n                jump = self._tempered_cauchy(jump_scale, temp=t)\n                cand = anchor + jump\n\n                # occasional DE-like coordinate injection\n                if self.rng.rand() < 0.2:\n                    mask = (self.rng.rand(self.dim) < 0.2)\n                    if mask.any():\n                        cand[mask] = base[mask] + self.rng.randn(mask.sum()) * scale[mask]\n                # small extra jitter\n                cand = cand + 0.01 * (ub - lb) * self.rng.randn(self.dim)\n\n            elif move_type == 'PCA':\n                # choose a subset of elites and do PCA\n                # pick up to min(20, number of finite)\n                n_el = min(max(3, X_fin.shape[0] // 3), X_fin.shape[0], 20)\n                # indices of best n_el\n                idxs_sorted = np.argsort(F_fin)\n                elite_idx_local = idxs_sorted[:n_el]\n                X_el = X_fin[elite_idx_local, :]\n                Xc = X_el - X_el.mean(axis=0, keepdims=True)\n                try:\n                    u, s, vh = np.linalg.svd(Xc, full_matrices=False)\n                    top = min( max(1, int(min(self.dim, 3 + self.rng.randint(0, 3)))), vh.shape[0])\n                    pcs = vh[:top, :]  # top principal components (top x dim)\n                    # sample weights along PCs\n                    weights = self.rng.randn(top) * (0.5 + self.rng.rand() * 0.5)\n                    scales_pc = (s[:top] / (np.mean(s[:top]) + 1e-12)) * (gscale.mean() * (0.5 + self.rng.rand()))\n                    perturb = (weights * scales_pc)[:, None] * pcs\n                    perturb = perturb.sum(axis=0)\n                    cand = base + perturb + 0.2 * (self.rng.randn(self.dim) * scale * (0.2 + self.rng.rand(self.dim)))\n                except Exception:\n                    cand = base + self.rng.randn(self.dim) * scale * 0.5\n\n            else:  # LOCAL\n                anisotropic = self.rng.randn(self.dim) * (scale * (0.2 + 0.8 * self.rng.rand(self.dim)))\n                if self.rng.rand() < 0.3 and F_fin.size > 0:\n                    median = np.median(X_fin, axis=0)\n                    cand = base + 0.5 * (median - base) + anisotropic\n                else:\n                    cand = base + anisotropic\n                    # small DE-like mixing inside local branch sometimes\n                    if self.rng.rand() < 0.3:\n                        cand = cand + 0.2 * (donor - base)\n\n            # tempered cauchy tiny escape occasionally\n            if self.rng.rand() < 0.01:\n                cand = cand + self._tempered_cauchy(0.05 * (ub - lb), temp=2.0)\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            cand = np.asarray(cand, dtype=float)\n            bad_coords = ~np.isfinite(cand)\n            if bad_coords.any():\n                uu = self._uniform_array(lb, ub)\n                cand[bad_coords] = uu[bad_coords]\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n            # update success metrics\n            finite_inds = np.where(np.isfinite(np.array(F_archive)))[0]\n            if finite_inds.size > 0:\n                finite_vals = np.array(F_archive)[finite_inds]\n                # rank of new candidate among finite (smaller is better)\n                rank_pos = np.sum(finite_vals <= f_cand)\n                # success if in best quarter (rank small)\n                if rank_pos <= max(1, int(0.25 * max(1, finite_vals.size))):\n                    success_count += 1\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n            else:\n                iter_since_improve += 1\n\n            # short-term adaptation of gscale every few iterations\n            if total_iters % 10 == 0:\n                if success_count > 0 and self.rng.rand() < 0.7:\n                    # encourage smaller scales (exploitation)\n                    gscale = gscale * (0.95 - 0.02 * self.rng.rand())\n                else:\n                    # encourage exploration\n                    gscale = gscale * (1.02 + 0.02 * self.rng.rand())\n                # keep within domain fractions (per-dim)\n                gscale = np.minimum(np.maximum(gscale, 1e-6 * (ub - lb)), 0.6 * (ub - lb))\n                success_count = 0  # reset short-term\n\n            # prune archive if too large\n            if len(X_archive) > self.archive_max:\n                F_arr = np.array(F_archive)\n                finite_idx = np.where(np.isfinite(F_arr))[0].tolist()\n                inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                keep = set()\n                # keep best finite half (at least 2)\n                if len(finite_idx) > 0:\n                    nkeep_best = max(2, int(0.5 * len(finite_idx)))\n                    best_fin = np.array(finite_idx)[np.argsort(F_arr[finite_idx])[:nkeep_best]].tolist()\n                    for ii in best_fin:\n                        keep.add(ii)\n                    # keep some random finite others\n                    rem_fin = [ii for ii in finite_idx if ii not in best_fin]\n                    self.rng.shuffle(rem_fin)\n                    for ii in rem_fin[:max(0, int(0.2 * len(rem_fin)))]:\n                        keep.add(ii)\n                # add some infs for diversity\n                self.rng.shuffle(inf_idx)\n                for ii in inf_idx[:min(len(inf_idx), 5)]:\n                    keep.add(ii)\n                # if too few kept, add random ones\n                all_idx = list(range(len(X_archive)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= self.archive_max:\n                        break\n                    keep.add(ii)\n                keep = sorted(list(keep))[:self.archive_max]\n                X_archive = [X_archive[i] for i in keep]\n                F_archive = [F_archive[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(50, 10 * self.dim):\n                # micro-restart: generate around best and add random uniform points\n                iter_since_improve = 0\n                nrand = min(10, max(2, self.dim // 2))\n                if self.x_opt is None:\n                    base_best = self._uniform_array(lb, ub)\n                else:\n                    base_best = self.x_opt.copy()\n                # perturb around best using per-dim scale and some coordinate mixing\n                pert = base_best + self.rng.randn(self.dim) * (0.5 * scale * (0.5 + self.rng.rand(self.dim)))\n                # with small prob mix some coordinates completely uniform\n                mask = (self.rng.rand(self.dim) < 0.1)\n                if mask.any():\n                    uu = self._uniform_array(lb, ub)\n                    pert[mask] = uu[mask]\n                f = safe_eval(pert)\n                X_archive.append(pert.copy()); F_archive.append(f)\n                # add some pure randoms to diversify\n                for _ in range(nrand):\n                    if self.evals >= self.budget:\n                        break\n                    xx = self._uniform_array(lb, ub)\n                    f = safe_eval(xx)\n                    X_archive.append(xx.copy()); F_archive.append(f)\n\n        # final best: choose best finite from archive\n        F_arr = np.array(F_archive)\n        X_arr = np.array(X_archive)\n        finite_idx = np.where(np.isfinite(F_arr))[0]\n        if finite_idx.size > 0:\n            best_i = finite_idx[np.argmin(F_arr[finite_idx])]\n            return float(F_arr[best_i]), X_arr[best_i].copy()\n        else:\n            # fallback: return best seen tracked or random\n            if self.x_opt is not None:\n                return float(self.f_opt), self.x_opt.copy()\n            x = self._uniform_array(lb, ub)\n            return float(func(x)), x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "bb2ad59a-c51a-4814-8e6e-bbc3d016b765", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term per-dimension scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=10, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.archive_max = int(archive_max)\n        self.init_samples = int(init_samples)\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n        # internal trackers\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # -------- utilities ----------\n    def _get_bounds(self, func):\n        # Attempt to extract bounds from common patterns; fallback to [-5,5]\n        lb = None\n        ub = None\n        # Pattern: func.bounds is (lb, ub) or object with lb/ub, or func.lb/ub attributes\n        if hasattr(func, 'bounds'):\n            b = func.bounds\n            if isinstance(b, tuple) or isinstance(b, list):\n                try:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n            else:\n                # object with lb/ub attributes?\n                if hasattr(b, 'lb') and hasattr(b, 'ub'):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n        if lb is None and hasattr(func, 'lb') and hasattr(func, 'ub'):\n            try:\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.asarray(lb).reshape(-1)\n            ub = np.asarray(ub).reshape(-1)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb.item()))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub.item()))\n\n        # ensure shapes and finite values\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                # fallback\n                lb[i] = -5.0\n                ub[i] = 5.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect repeatedly to bring x within [lb,ub], then clamp if numerical drift\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect symmetrical\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        pts = a[:, None] + (b - a)[:, None] * u\n        # permute each column\n        for j in range(dim):\n            self.rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            samples = self.rng.rand(n, self.dim) * (ub - lb) + lb\n            return samples\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # draw standard Cauchy then temper with gaussian noise; divide by (1+|g|)\n        u = self.rng.rand(self.dim)\n        # standard cauchy: tan(pi*(u-0.5))\n        c = np.tan(np.pi * (u - 0.5))\n        g = self.rng.randn(self.dim) * np.sqrt(max(1e-12, float(temp)))\n        tempered = c / (1.0 + np.abs(g))\n        return tempered * np.asarray(scale_vec, dtype=float)\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n        center = 0.5 * (lb + ub)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # returns f or np.inf if budget exhausted; updates internal best when evaluation occurs\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                # if evaluation errors, treat as non-finite\n                f = np.inf\n            self.evals += 1\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # evaluate center\n        if self.evals < self.budget:\n            f_center = safe_eval(center)\n            X_archive.append(center.copy()); F_archive.append(f_center)\n        else:\n            return float(self.f_opt), (self.x_opt.copy() if self.x_opt is not None else self._uniform_array(lb, ub))\n\n        # initial LHS samples\n        n_init = min(self.init_samples, max(0, self.budget - self.evals))\n        if n_init > 0:\n            lhs = self._lhs01(n_init, self.dim)\n            for i in range(n_init):\n                if self.evals >= self.budget:\n                    break\n                x = lb + lhs[i] * (ub - lb)\n                f = safe_eval(x)\n                X_archive.append(x.copy()); F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        for _ in range(2):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        success_count = 0\n        total_iters = 0\n        iter_since_improve = 0\n\n        # global gscale (per-dimension) initial\n        gscale = 0.1 * (ub - lb)\n\n        # main loop: continue until budget used\n        while self.evals < self.budget:\n            total_iters += 1\n\n            # prepare archived arrays\n            X = np.array(X_archive) if len(X_archive) > 0 else np.zeros((0, self.dim))\n            F = np.array(F_archive) if len(F_archive) > 0 else np.array([])\n\n            finite_mask = np.isfinite(F)\n            if not np.any(finite_mask):\n                # no valid evaluations -> sample uniform\n                cand = self._uniform_array(lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            X_fin = X[finite_mask]\n            F_fin = F[finite_mask]\n            if X_fin.shape[0] < 2:\n                # not enough points => sample near center/uniform\n                cand = center + self.rng.randn(self.dim) * (0.1 * (ub - lb))\n                cand = self._reflect_bounds(cand, lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            q75 = np.percentile(X_fin, 75, axis=0)\n            q25 = np.percentile(X_fin, 25, axis=0)\n            iqr = q75 - q25\n            std = np.std(X_fin, axis=0)\n            scale = np.where(iqr > 1e-12, 0.74 * iqr, std)\n            scale = np.where(scale <= 1e-12, 0.2 * (ub - lb), scale)\n            # clamp scale to reasonable portion of domain\n            scale = np.minimum(scale, 0.5 * (ub - lb))\n\n            # choose a base index biased toward better F via softmax on -F\n            finite_F = F.copy()\n            # give a large penalty to non-finite so they are unlikely base\n            max_finite = np.max(F[finite_mask]) if np.any(finite_mask) else 1.0\n            finite_F[~finite_mask] = max_finite + 1.0\n            fmin = np.min(finite_F)\n            denom = np.std(finite_F) + 1e-9\n            scores = np.exp(-(finite_F - fmin) / (denom + 1e-12))\n            # small temperature noise for exploration\n            scores = scores + 0.2 * (self.rng.rand(*scores.shape) - 0.5)\n            # ensure non-negative\n            scores = np.maximum(scores, 0.0)\n            probs = scores / (np.sum(scores) + 1e-12)\n            # pick base index among all (prefer finite ones)\n            base_idx = int(self.rng.choice(len(F), p=probs))\n            base = X[base_idx].copy()\n\n            # small chance to use global best directly\n            if (self.x_opt is not None) and (self.rng.rand() < 0.05):\n                base = self.x_opt.copy()\n                # attach index approx (if exists in archive)\n                # (not critical)\n\n            # donor pool selection (for DE-like moves)\n            donor = None\n            if len(X) >= 4:\n                idxs = np.arange(len(X))\n                others = idxs[idxs != base_idx]\n                # pick 3 distinct others\n                if others.size >= 3:\n                    sel = self.rng.choice(others, size=3, replace=False)\n                    x1, x2, x3 = X[sel[0]], X[sel[1]], X[sel[2]]\n                    Fdiff = (x2 - x3)\n                    # scale factor random in DE range\n                    Fscale = 0.6 + 0.8 * self.rng.rand()\n                    donor = x1 + Fscale * Fdiff + self.rng.randn(self.dim) * (0.02 * (ub - lb))\n\n            # choose move type\n            r = self.rng.rand()\n            move_type = 'GLOBAL' if r < 0.35 else 'PCA' if r < 0.6 else 'LOCAL'\n\n            # scale modifiers\n            g_modifier = 1.0 + 0.2 * (self.rng.rand() - 0.5)\n\n            # produce candidate\n            cand = base.copy()\n            f_cand = np.inf\n            if move_type == 'GLOBAL':\n                # anchor on either elite or uniform\n                if self.rng.rand() < 0.7 and X_fin.shape[0] >= 3:\n                    # pick among top-k elites\n                    k = max(1, min(10, X_fin.shape[0] // 4))\n                    elite_idx = np.argsort(F_fin)[:k]\n                    anchor = X_fin[self.rng.choice(elite_idx)]\n                else:\n                    anchor = self._uniform_array(lb, ub)\n\n                # tempered cauchy jump\n                t = 1.0 + 0.5 * self.rng.rand()\n                jump_scale = gscale * (0.5 + self.rng.rand(self.dim) * 0.5)\n                jump = self._tempered_cauchy(jump_scale, temp=t)\n                cand = anchor + jump\n\n                # occasional DE-like differential injection (coordinate-wise)\n                if donor is not None and (self.rng.rand() < 0.25):\n                    mask = self.rng.rand(self.dim) < 0.3\n                    if mask.any():\n                        cand[mask] = donor[mask]\n\n                # small extra jitter\n                cand += self.rng.randn(self.dim) * (0.01 * (ub - lb))\n\n            elif move_type == 'PCA':\n                # choose subset of elites and do PCA\n                k = min(10, max(3, X_fin.shape[0] // 4))\n                idxs_elite = np.argsort(F_fin)[:k]\n                X_el = X_fin[idxs_elite]\n                # center\n                Xc = X_el - np.mean(X_el, axis=0)\n                try:\n                    u, s, vh = np.linalg.svd(Xc, full_matrices=False)\n                    # build perturbation using top components with random weights\n                    # choose top components count\n                    top = max(1, min(self.dim, int(1 + self.rng.rand() * min(self.dim, 3))))\n                    pcs = vh[:top]  # top principal directions\n                    weights = (self.rng.randn(top) * (0.5 + self.rng.rand(top)))\n                    scales_pc = (s[:top] / (np.sqrt(max(1, X_el.shape[0])) + 1e-12)) * (0.5 + self.rng.rand(top)[:, None]).flatten()\n                    perturb = np.zeros(self.dim)\n                    for j in range(top):\n                        perturb += weights[j] * pcs[j] * scales_pc[j]\n                    cand = base + perturb + self.rng.randn(self.dim) * (0.02 * scale)\n                except Exception:\n                    # fallback to scaled gaussian\n                    cand = base + self.rng.randn(self.dim) * (0.5 * scale)\n\n            else:  # LOCAL\n                anisotropic = self.rng.randn(self.dim) * (scale * (0.2 + 0.8 * self.rng.rand()))\n                if self.rng.rand() < 0.4:\n                    # sometimes attracted to median\n                    median = np.median(X_fin, axis=0)\n                    cand = base + 0.5 * (median - base) + anisotropic\n                else:\n                    cand = base + anisotropic\n\n                # small DE-like mixing inside local branch sometimes\n                if donor is not None and (self.rng.rand() < 0.15):\n                    mix_mask = self.rng.rand(self.dim) < 0.2\n                    if mix_mask.any():\n                        cand[mix_mask] = base[mix_mask] + 0.8 * (donor[mix_mask] - base[mix_mask])\n\n            # tempered cauchy tiny escape occasionally\n            if self.rng.rand() < 0.01:\n                tiny = self._tempered_cauchy(0.05 * (ub - lb), temp=0.5)\n                cand += tiny\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            if not np.all(np.isfinite(cand)):\n                mask = ~np.isfinite(cand)\n                cand[mask] = self._uniform_array(lb, ub)[mask]\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals < self.budget:\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n            else:\n                break\n\n            # update success metrics\n            finite_inds = np.where(np.isfinite(np.array(F_archive)))[0]\n            if f_cand is not None and np.isfinite(f_cand):\n                finite_vals = np.array(F_archive)[finite_inds]\n                # rank of new candidate among finite (smaller is better)\n                rank_pos = int(np.sum(finite_vals <= f_cand))\n                # success if in best quarter (rank small)\n                if rank_pos <= max(1, int(0.25 * max(1, finite_vals.size))):\n                    success_count += 1\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n            else:\n                iter_since_improve += 1\n\n            # short-term adaptation of gscale every few iterations\n            if total_iters % 4 == 0:\n                if success_count > 0 and (success_count / max(1, total_iters)) > 0.15:\n                    # encourage smaller scales (exploitation)\n                    gscale = gscale * (0.96 - 0.02 * self.rng.rand())\n                else:\n                    # encourage exploration\n                    gscale = gscale * (1.02 + 0.02 * self.rng.rand())\n                # small random nudges\n                gscale = gscale * (1.0 + 0.05 * (self.rng.rand(self.dim) - 0.5))\n                # keep within domain fractions (per-dim)\n                gscale = np.minimum(np.maximum(gscale, 1e-6 * (ub - lb)), 0.6 * (ub - lb))\n\n            # prune archive if too large\n            if len(F_archive) > 2 * self.archive_max:\n                F_arr = np.array(F_archive)\n                finite_idx = np.where(np.isfinite(F_arr))[0].tolist()\n                inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                keep = set()\n                # keep best finite ones\n                if len(finite_idx) > 0:\n                    best_fin = np.argsort(F_arr[finite_idx])  # indices into finite_idx\n                    # convert to original indices and keep top chunk\n                    for ii in finite_idx:\n                        # will handle below\n                        pass\n                    # keep top up to archive_max//2\n                    nkeep = min(len(finite_idx), max(1, self.archive_max // 2))\n                    best_global = np.array(finite_idx)[np.argsort(F_arr[finite_idx])[:nkeep]].tolist()\n                    for ii in best_global:\n                        keep.add(int(ii))\n                    # keep some random finite others\n                    rem_fin = [ii for ii in finite_idx if ii not in keep]\n                    self.rng.shuffle(rem_fin)\n                    for ii in rem_fin[:min(30, len(rem_fin))]:\n                        keep.add(int(ii))\n                # add some infs for diversity\n                self.rng.shuffle(inf_idx)\n                for ii in inf_idx[:min(20, len(inf_idx))]:\n                    keep.add(int(ii))\n                # if too few kept, add random ones\n                all_idx = list(range(len(F_arr)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= self.archive_max:\n                        break\n                    keep.add(int(ii))\n                keep_list = sorted(list(keep))\n                X_archive = [X_archive[i] for i in keep_list]\n                F_archive = [F_archive[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(30, 6 * self.dim):\n                # micro-restart: generate around best and add random uniform points\n                iter_since_improve = 0\n                if self.x_opt is None:\n                    base_best = self._uniform_array(lb, ub)\n                else:\n                    base_best = self.x_opt.copy()\n                nlocal = min(10 + self.dim, max(5, self.budget - self.evals - 1))\n                for i in range(nlocal):\n                    if self.evals >= self.budget:\n                        break\n                    # perturb around best using per-dim scale and some coordinate mixing\n                    pert = base_best + self.rng.randn(self.dim) * (0.2 * gscale)\n                    # mix some coords uniform with small prob\n                    mask = (self.rng.rand(self.dim) < 0.1)\n                    if mask.any():\n                        uu = self._uniform_array(lb, ub)\n                        pert[mask] = uu[mask]\n                    pert = self._reflect_bounds(pert, lb, ub)\n                    f = safe_eval(pert)\n                    X_archive.append(pert.copy()); F_archive.append(f)\n                # add some pure randoms to diversify\n                nrand = min(5 + self.dim // 2, max(1, self.budget - self.evals))\n                for _ in range(nrand):\n                    if self.evals >= self.budget:\n                        break\n                    xx = self._uniform_array(lb, ub)\n                    f = safe_eval(xx)\n                    X_archive.append(xx.copy()); F_archive.append(f)\n\n        # final best: choose best finite from archive\n        F_arr = np.array(F_archive)\n        X_arr = np.array(X_archive)\n        finite_idx = np.where(np.isfinite(F_arr))[0]\n        if finite_idx.size > 0:\n            best_i = int(finite_idx[np.argmin(F_arr[finite_idx])])\n            return float(F_arr[best_i]), X_arr[best_i].copy()\n        else:\n            # fallback: random point (but must respect budget: no evals left)\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                return float(f), x\n            else:\n                # return best tracked or random\n                if self.x_opt is not None:\n                    return float(self.f_opt), self.x_opt.copy()\n                else:\n                    return float(np.inf), self._uniform_array(lb, ub)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4b673c88-5c23-4ae8-8bcd-376052a88237", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line idea: maintain a compact archive of evaluated points and mix several proposal kernels\n    (anisotropic local Gaussians, PCA-guided elite perturbations, DE-style donor recombination,\n    tempered Cauchy global jumps) with short-term adaptation of per-dimension scales and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        # initial population size\n        if init_samples is None:\n            self.init_samples = max(8, min(40, int(5 * np.log1p(self.dim))))\n        else:\n            self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # try common patterns, fallback to [-5,5]\n        try:\n            if hasattr(func, 'bounds') and func.bounds is not None:\n                b = func.bounds\n                if hasattr(b, 'lb') and hasattr(b, 'ub'):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                    return lb.reshape(-1), ub.reshape(-1)\n            if hasattr(func, 'lb') and hasattr(func, 'ub'):\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n                return lb.reshape(-1), ub.reshape(-1)\n        except Exception:\n            pass\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect repeatedly to bring x within [lb,ub], then clamp\n        x = np.asarray(x, dtype=float)\n        for _ in range(8):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (np.any(below) or np.any(above)):\n                break\n        # final clamp to avoid tiny numerical drift\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        pts = np.zeros((n, dim))\n        for j in range(dim):\n            idx = np.arange(n)\n            self.rng.shuffle(idx)\n            pts[:, j] = cut[:n] + u[:, j] * (cut[1] - cut[0])\n            # permute rows by idx\n            pts[:, j] = pts[idx, j]\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # sample independent Cauchy components with light gaussian tempering for numerical stability\n        # draw standard cauchy then temper by dividing by (1 + temp*|g2|)\n        dim = scale_vec.size\n        g1 = self.rng.standard_cauchy(size=dim)\n        g2 = self.rng.randn(dim)\n        tempered = g1 / (1.0 + temp * np.abs(g2))\n        # cap extremes to avoid numerical overflow\n        cap = 1e2\n        tempered = np.clip(tempered, -cap, cap)\n        return tempered * scale_vec\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare bounds and center\n        lb, ub = self._get_bounds(func)\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n            ub = np.full(self.dim, ub.item())\n        lb = np.asarray(lb).reshape(-1)[:self.dim]\n        ub = np.asarray(ub).reshape(-1)[:self.dim]\n        center = 0.5 * (lb + ub)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            # bookkeep best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # initialize archive\n        X_archive = []\n        F_archive = []\n\n        # initial LHS around domain\n        n0 = min(self.init_samples, max(1, self.budget // 10))\n        pts01 = self._lhs01(n0, self.dim)\n        for i in range(n0):\n            if self.evals >= self.budget:\n                break\n            x0 = lb + pts01[i] * (ub - lb)\n            x0 = self._reflect_bounds(x0, lb, ub)\n            f0 = safe_eval(x0)\n            X_archive.append(x0.copy()); F_archive.append(f0)\n\n        # add a few pure random points if budget allows\n        for _ in range(min(5, max(0, (self.budget - self.evals)//2))):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        success_count = 0\n        iter_since_improve = 0\n        total_iters = 0\n\n        # global gscale (per-dimension) initial\n        gscale = 0.1 * (ub - lb)\n\n        # main loop\n        while self.evals < self.budget:\n            total_iters += 1\n\n            # form arrays\n            if X_archive:\n                X = np.array(X_archive)\n                F = np.array(F_archive)\n            else:\n                X = np.zeros((0, self.dim))\n                F = np.array([])\n\n            finite_mask = np.isfinite(F)\n            if finite_mask.any():\n                # robust stats among finite\n                X_fin = X[finite_mask]\n                F_fin = F[finite_mask]\n                median = np.median(X_fin, axis=0)\n                q25 = np.percentile(X_fin, 25, axis=0)\n                q75 = np.percentile(X_fin, 75, axis=0)\n                iqr = q75 - q25\n                std = np.std(X_fin, axis=0)\n                scale = np.where(iqr > 1e-12, iqr, std)\n                # fallback to domain fraction\n                scale = np.where(scale <= 1e-12, 0.2 * (ub - lb), scale)\n            else:\n                # no finite evals yet\n                median = center.copy()\n                scale = 0.2 * (ub - lb)\n\n            # pick base biased toward better F (smaller better)\n            base = center.copy()\n            if finite_mask.any():\n                # softmax on -F (smaller F -> larger weight)\n                scores = -F_fin\n                scores = (scores - scores.max())\n                w = np.exp(scores / (max(1e-3, np.std(scores)) + 1e-6))\n                w = np.maximum(w, 1e-8)\n                w = w / w.sum()\n                idx_in_fin = self.rng.choice(len(X_fin), p=w)\n                base = X_fin[idx_in_fin].copy()\n            else:\n                # sample uniform base if no finite points\n                base = self._uniform_array(lb, ub)\n\n            # occasionally use global best directly\n            if (self.x_opt is not None) and (self.rng.rand() < 0.02):\n                base = self.x_opt.copy()\n\n            # donor selection for DE-like moves\n            donor = None\n            if len(X_archive) >= 4:\n                others = list(range(len(X_archive)))\n                # try to avoid picking the same as base; map base to an index if base in archive approx\n                try:\n                    base_idx = min(range(len(X_archive)), key=lambda i: np.linalg.norm(X_archive[i]-base))\n                except Exception:\n                    base_idx = None\n                if base_idx is not None and len(others) > 1:\n                    others.remove(base_idx)\n                a, b, c = self.rng.choice(others, size=3, replace=False)\n                donor = X_archive[a] + 0.8 * (X_archive[b] - X_archive[c])\n\n            # choose move type\n            r = self.rng.rand()\n            if r < 0.50:\n                move_type = 'LOCAL'\n            elif r < 0.70:\n                move_type = 'PCA'\n            elif r < 0.9:\n                move_type = 'DE'\n            else:\n                move_type = 'GLOBAL'\n\n            # produce anisotropic gaussian perturbation\n            anisotropic = self.rng.randn(self.dim) * (gscale * (0.8 + 0.4 * self.rng.rand(self.dim)))\n\n            # produce candidate\n            cand = base.copy()\n            if move_type == 'LOCAL':\n                if self.rng.rand() < 0.3 and finite_mask.any():\n                    cand = base + 0.5 * (median - base) + anisotropic\n                else:\n                    cand = base + anisotropic\n                # small DE-like mixing inside local branch sometimes\n                if (donor is not None) and (self.rng.rand() < 0.15):\n                    cand = cand + 0.15 * (donor - base)\n\n            elif move_type == 'PCA':\n                # use PCA on top-k elites\n                if finite_mask.sum() >= max(3, min(20, len(X_fin))):\n                    k = max(3, min(20, int(0.2 * len(X_fin))))\n                    elite_idx = np.argsort(F_fin)[:k]\n                    X_elite = X_fin[elite_idx] - np.mean(X_fin[elite_idx], axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(X_elite, full_matrices=False)\n                        # sample in top subspace (random number of top comps)\n                        top = max(1, min(self.dim, 1 + int(self.rng.rand() * min(self.dim, 3))))\n                        weights = self.rng.randn(top)\n                        # scale along pcs by singular values and global scale\n                        pc_vec = (Vt[:top].T @ (weights * (S[:top] / (S[0] + 1e-12))))\n                        pc_vec = pc_vec * (0.6 + 0.8 * self.rng.rand()) * (scale / (np.std(X_elite, axis=0) + 1e-12))\n                        cand = base + pc_vec\n                    except Exception:\n                        cand = base + anisotropic * 0.7\n                else:\n                    cand = base + anisotropic * 0.8\n\n            elif move_type == 'DE' and donor is not None:\n                # anchor DE injection, mix donor with base\n                cand = base + 0.5 * (donor - base) + 0.5 * anisotropic\n                # sometimes coordinate-wise crossover\n                if self.rng.rand() < 0.25:\n                    mask = self.rng.rand(self.dim) < 0.2\n                    if mask.any():\n                        uu = self._uniform_array(lb, ub)\n                        cand[mask] = uu[mask]\n\n            else:  # GLOBAL\n                # anchor on either a random elite or uniform\n                if (finite_mask.any() and self.rng.rand() < 0.7):\n                    k = max(1, min(10, int(len(X_fin) * 0.1) if len(X_fin) > 0 else 1))\n                    idxs = np.argsort(F_fin)[:k]\n                    anchor = X_fin[self.rng.choice(idxs)].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n                # tempered cauchy jump scaled by domain and scale\n                temp = 1.0 + 0.5 * self.rng.rand()\n                scale_vec = 0.3 * (ub - lb) * (0.5 + self.rng.rand(self.dim))\n                jump = self._tempered_cauchy(scale_vec, temp=temp)\n                cand = anchor + jump\n                # occasional DE-like differential injection\n                if (donor is not None) and (self.rng.rand() < 0.2):\n                    cand = cand + 0.4 * (donor - base)\n                # small coordinate jitter\n                if self.rng.rand() < 0.2:\n                    jitter_mask = self.rng.rand(self.dim) < 0.15\n                    if jitter_mask.any():\n                        cand[jitter_mask] += 0.5 * self.rng.randn(jitter_mask.sum()) * scale[jitter_mask]\n\n            # small occasional tiny tempered cauchy escape\n            if self.rng.rand() < 0.01:\n                cand += self._tempered_cauchy(0.05 * (ub - lb), temp=2.0)\n\n            # handle non-finite coords by replacing with uniform values\n            cand = np.asarray(cand, dtype=float)\n            if not np.all(np.isfinite(cand)):\n                mask = ~np.isfinite(cand)\n                cand[mask] = self._uniform_array(lb, ub)[mask]\n\n            # reflect to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n            # update success metrics\n            finite_inds = [i for i, fv in enumerate(F_archive) if np.isfinite(fv)]\n            if finite_inds:\n                finite_vals = np.array([F_archive[i] for i in finite_inds])\n                rank_pos = np.sum(finite_vals <= f_cand)\n                # success if in best quarter of finite\n                if rank_pos <= max(1, int(0.25 * len(finite_vals))):\n                    success_count += 1\n                    iter_since_improve = 0\n                else:\n                    iter_since_improve += 1\n            else:\n                iter_since_improve += 1\n\n            # short-term adaptation of gscale every few iterations\n            if total_iters % 6 == 0:\n                # if we've had many successes, shrink for exploitation; else expand slightly\n                if success_count > 2:\n                    gscale = gscale * (0.85 + 0.1 * self.rng.rand())\n                else:\n                    gscale = gscale * (1.01 + 0.05 * self.rng.rand())\n                # keep within domain fractions\n                gscale = np.minimum(np.maximum(gscale, 1e-6 * (ub - lb)), 0.6 * (ub - lb))\n                # decay success counter slowly\n                success_count = max(0, success_count - 1)\n\n            # prune archive if too large\n            if len(X_archive) > max(2 * self.archive_max, self.archive_max + 20):\n                F_arr = np.array(F_archive)\n                finite_idx = np.where(np.isfinite(F_arr))[0].tolist()\n                inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                keep = set()\n                if finite_idx:\n                    nkeep_best = max(2, int(0.5 * len(finite_idx)))\n                    best_fin = np.array(finite_idx)[np.argsort(F_arr[finite_idx])[:nkeep_best]].tolist()\n                    for ii in best_fin:\n                        keep.add(ii)\n                    # keep some random finite others\n                    rem_fin = [ii for ii in finite_idx if ii not in best_fin]\n                    self.rng.shuffle(rem_fin)\n                    for ii in rem_fin[:max(0, int(0.1 * len(finite_idx)))]:\n                        keep.add(ii)\n                # add some infs for diversity\n                for ii in inf_idx[:min(10, len(inf_idx))]:\n                    keep.add(ii)\n                # if too few kept, add random ones\n                all_idx = list(range(len(X_archive)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= self.archive_max:\n                        break\n                    keep.add(ii)\n                keep = sorted(list(keep))[:self.archive_max]\n                X_archive = [X_archive[i] for i in keep]\n                F_archive = [F_archive[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if iter_since_improve > max(30, 6 * self.dim):\n                iter_since_improve = 0\n                # create around-best perturbations and some uniform points\n                if self.x_opt is not None and np.isfinite(self.f_opt):\n                    base_best = self.x_opt.copy()\n                else:\n                    base_best = self._uniform_array(lb, ub)\n                # generate several local perturbs\n                n_local = min(8, max(3, self.dim))\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    pert = base_best + self.rng.randn(self.dim) * (0.5 * scale * (0.5 + self.rng.rand(self.dim)))\n                    # mix random coords with small prob\n                    mask = (self.rng.rand(self.dim) < 0.1)\n                    if mask.any():\n                        uu = self._uniform_array(lb, ub)\n                        pert[mask] = uu[mask]\n                    pert = self._reflect_bounds(pert, lb, ub)\n                    f = safe_eval(pert)\n                    X_archive.append(pert.copy()); F_archive.append(f)\n                # also add pure uniform diversifiers\n                for _ in range(min(6, max(1, self.dim // 2))):\n                    if self.evals >= self.budget:\n                        break\n                    xx = self._uniform_array(lb, ub)\n                    f = safe_eval(xx)\n                    X_archive.append(xx.copy()); F_archive.append(f)\n\n        # final best: choose best finite from archive\n        F_arr = np.array(F_archive) if F_archive else np.array([])\n        X_arr = np.array(X_archive) if X_archive else np.zeros((0, self.dim))\n        if F_arr.size > 0 and np.isfinite(F_arr).any():\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            best_i = finite_idx[np.argmin(F_arr[finite_idx])]\n            return float(F_arr[best_i]), X_arr[best_i].copy()\n        else:\n            # fallback: random point (should rarely happen)\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return float(f), x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8f4e87ee-785f-440b-8cf3-ef1e54e162d0", "fitness": 0.32893205929650293, "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=10, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = int(archive_max)\n        self.init_samples = int(init_samples)\n        self.seed = seed\n\n        # state\n        self.rng = np.random.RandomState(seed)\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Try common patterns for bounds then default to [-5,5]\n        dim = self.dim\n        lb = None\n        ub = None\n        # Try attribute patterns used in many wrappers\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None:\n            try:\n                lb = np.asarray(func.bounds.lb, dtype=float)\n                ub = np.asarray(func.bounds.ub, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n        if lb is None:\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n        if lb is None:\n            # fallback to common BBOB bounds\n            lb = np.full(dim, -5.0, dtype=float)\n            ub = np.full(dim, 5.0, dtype=float)\n\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(dim, float(ub.item()))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        # sanity clamp and ensure ub > lb\n        for i in range(dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1e-6\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect repeatedly to bring x within [lb,ub], then clamp if numerical drift\n        x = np.asarray(x, dtype=float).copy()\n        # vectorized reflection: reflect until within bounds (but cap iterations)\n        for _ in range(4):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # clamp to ensure numerical safety\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0.0, 1.0, n + 1)\n        pts = np.zeros((n, dim))\n        for j in range(dim):\n            a = cut[:n]\n            b = cut[1:n+1]\n            u = rng.rand(n)\n            pts[:, j] = a + u * (b - a)\n            rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.rand(n, self.dim) * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # sample independent Cauchy components with light gaussian tempering for numerical stability\n        # draw standard cauchy then temper with gaussian noise\n        z = self.rng.standard_cauchy(size=self.dim) * np.asarray(scale_vec, dtype=float)\n        g = self.rng.randn(self.dim) * float(temp)\n        z = z / (1.0 + np.abs(g))\n        # clamp large values to reasonable multiples of scale\n        maxv = 20.0 * np.maximum(scale_vec, 1e-12)\n        z = np.clip(z, -maxv, maxv)\n        return z\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        self.evals = 0\n        self.rng = np.random.RandomState(self.seed if self.seed is not None else None)\n\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # x: array-like, returns float, increments evals if called\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                # ensure numpy array\n                x_a = np.asarray(x, dtype=float).copy()\n                f = func(x_a)\n                # If function returns array, try to extract scalar\n                if isinstance(f, (list, tuple, np.ndarray)):\n                    f = float(np.asarray(f).ravel()[0])\n                elif not np.isfinite(f):\n                    f = np.inf\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            # update best bookkeeping\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_a.copy()\n            return f\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # initial LHS samples (spread)\n        n_init = max(2, min(self.init_samples, self.budget // 10))\n        pts = self._lhs01(n_init, dim)\n        pts = pts * (ub - lb) + lb\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = pts[i]\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        n_unif = min(5, max(0, (self.budget - self.evals) // 50 + 1))\n        for _ in range(n_unif):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        archive_max = max(50, min(self.archive_max, 2000))\n        total_iters = 0\n        gscale = 0.05 * (ub - lb)  # per-dim global scale\n        success_count = 0\n        recent_best = self.f_opt\n        stagnation_counter = 0\n\n        # main loop: continue until budget used\n        while self.evals < self.budget:\n            total_iters += 1\n\n            # prepare archived arrays\n            if len(F_archive) == 0:\n                # no archived evaluations -> sample uniform\n                cand = self._uniform_array(lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            X = np.vstack([np.asarray(x, dtype=float) for x in X_archive])\n            F = np.asarray(F_archive, dtype=float)\n            finite_mask = np.isfinite(F)\n\n            # if too few finite points => sample more broadly\n            if finite_mask.sum() < 2:\n                cand = self._uniform_array(lb, ub)\n                cand = self._reflect_bounds(cand, lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            X_fin = X[finite_mask]\n            if X_fin.shape[0] < 2:\n                cand = self._uniform_array(lb, ub)\n                cand = self._reflect_bounds(cand, lb, ub)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n                continue\n            q75 = np.percentile(X_fin, 75, axis=0)\n            q25 = np.percentile(X_fin, 25, axis=0)\n            iqr = q75 - q25\n            std = np.std(X_fin, axis=0)\n            scale = np.where(np.isfinite(iqr) & (iqr > 1e-12), iqr, std)\n            # clamp scale to reasonable portion of domain\n            domain = ub - lb\n            scale = np.minimum(np.maximum(scale, 1e-12 * domain), 0.8 * domain)\n\n            # choose a base index biased toward better F via softmax on -F\n            finite_F = F.copy()\n            # give a large penalty to non-finite so they are unlikely base\n            if finite_mask.any():\n                maxf = np.max(F[finite_mask])\n            else:\n                maxf = 0.0\n            finite_F[~finite_mask] = maxf + 1.0\n            # softmax on negative F (lower is better)\n            scores = np.exp(- (finite_F - finite_F.min()) / (1e-6 + (finite_F.std() if finite_F.std() > 0 else 1.0)))\n            probs = scores / (scores.sum() + 1e-12)\n            base_idx = self.rng.choice(len(F), p=probs)\n            base = X[base_idx].copy()\n\n            # small chance to use global best directly\n            if (self.x_opt is not None) and (self.rng.rand() < 0.05):\n                base = self.x_opt.copy()\n\n            # donor pool selection (for DE-like moves)\n            others = [i for i in range(len(F)) if i != base_idx and np.isfinite(F[i])]\n            if len(others) >= 3:\n                a_idx, b_idx, c_idx = self.rng.choice(others, size=3, replace=False)\n                a = X[a_idx]; b = X[b_idx]; c = X[c_idx]\n            else:\n                # if insufficient donors sample from elites/random\n                a = self._uniform_array(lb, ub)\n                b = self._uniform_array(lb, ub)\n                c = self._uniform_array(lb, ub)\n\n            # choose move type\n            r = self.rng.rand()\n            move_type = 'LOCAL' if r < 0.6 else 'GLOBAL'\n            if r < 0.02:\n                move_type = 'PCA'\n\n            # scale modifiers\n            t = 0.8 + 0.4 * self.rng.rand()  # temp parameter for tempered cauchy\n\n            # produce candidate\n            cand = base.copy()\n            if move_type == 'GLOBAL':\n                # anchor on either elite or uniform\n                if self.rng.rand() < 0.7 and finite_mask.any():\n                    # pick among top-k elites\n                    fin_idx_list = np.where(finite_mask)[0]\n                    k = max(3, min(20, max(3, len(fin_idx_list) // 5 + 3)))\n                    sorted_fin = fin_idx_list[np.argsort(F[fin_idx_list])]\n                    chosen_idx = sorted_fin[self.rng.randint(0, min(k, len(sorted_fin)))]\n                    anchor = X[chosen_idx].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n                # tempered cauchy jump around anchor\n                jump_scale = gscale * (0.5 + self.rng.rand(dim) * 0.5)\n                jump = self._tempered_cauchy(jump_scale, temp=t)\n                cand = anchor + jump\n                # occasional DE-like differential injection\n                if self.rng.rand() < 0.15:\n                    cand = cand + 0.5 * (a - b)\n                # coordinate-wise local gaussian injection on some coords\n                if self.rng.rand() < 0.3:\n                    mask = self.rng.rand(dim) < 0.3\n                    cand[mask] = base[mask] + self.rng.randn(mask.sum()) * (0.5 * scale[mask])\n                # small extra jitter\n                cand = cand + 1e-3 * self.rng.randn(dim) * domain\n\n            elif move_type == 'PCA':\n                # choose subset of elites and do PCA\n                fin_idx_list = np.where(finite_mask)[0]\n                k = max(3, min(30, max(3, len(fin_idx_list) // 6 + 3)))\n                idxs_elite = fin_idx_list[np.argsort(F[fin_idx_list])[:k]]\n                X_elite = X[idxs_elite]\n                center = np.median(X_elite, axis=0)\n                # PCA via SVD of centered elites\n                try:\n                    Xc = X_elite - center\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    # choose number of components\n                    ncomp = max(1, min(dim, int(np.clip((S > (S.max() * 1e-3)).sum(), 1, dim))))\n                    comps = Vt[:ncomp]\n                    # build perturbation in PCA subspace\n                    weights = (0.2 + 0.8 * self.rng.rand(ncomp)) * S[:ncomp]\n                    coeffs = self.rng.randn(ncomp) * weights\n                    pca_pert = np.dot(coeffs, comps)\n                except Exception:\n                    pca_pert = self.rng.randn(dim) * (0.1 * scale)\n                cand = center + pca_pert\n                # sometimes attracted to best with small gaussian\n                if self.rng.rand() < 0.4 and (self.x_opt is not None):\n                    cand = cand * (0.7 + 0.3 * self.rng.rand()) + self.x_opt * (0.3 * self.rng.rand())\n\n            else:  # LOCAL\n                # anisotropic Gaussian local moves around base, coordinate mixing, occasional DE step\n                anisotropic = self.rng.randn(dim) * (scale * (0.2 + 0.8 * self.rng.rand()))\n                cand = base + anisotropic\n                if self.rng.rand() < 0.12:\n                    # inject differential vector (DE-like)\n                    cand = cand + 0.6 * (a - b)\n                # mix some coordinates uniformly to escape ridges\n                mask = self.rng.rand(dim) < 0.05\n                if mask.any():\n                    cand[mask] = self._uniform_array(lb, ub)[mask]\n\n            # tempered cauchy tiny escape occasionally\n            if self.rng.rand() < 0.01:\n                cand = cand + self._tempered_cauchy(0.2 * domain, temp=0.5)\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            bad_coords = ~np.isfinite(cand)\n            if bad_coords.any():\n                cand[bad_coords] = self._uniform_array(lb, ub)[bad_coords]\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n            # update success metrics\n            finite_inds = np.where(np.isfinite(F_archive))[0]\n            if finite_inds.size > 0:\n                finite_vals = np.array(F_archive)[finite_inds]\n                # rank of new candidate among finite (smaller is better)\n                rank_pos = np.sum(finite_vals < f_cand) + 1  # 1-based rank\n                if rank_pos <= max(1, int(0.25 * max(1, finite_vals.size))):\n                    success_count += 1\n\n            # short-term adaptation of gscale every few iterations\n            if total_iters % 10 == 0:\n                # adjust based on recent success rate\n                if success_count > 0:\n                    # encourage smaller scales (exploitation)\n                    gscale = gscale * (0.98 - 0.01 * self.rng.rand(dim))\n                else:\n                    # encourage exploration\n                    gscale = gscale * (1.02 + 0.02 * self.rng.rand(dim))\n                # keep within domain fractions (per-dim)\n                gscale = np.minimum(np.maximum(gscale, 1e-6 * (ub - lb)), 0.6 * (ub - lb))\n                # reset short-term success\n                success_count = 0\n\n            # prune archive if too large\n            if len(F_archive) > archive_max:\n                F_arr = np.array(F_archive)\n                finite_idx = np.where(np.isfinite(F_arr))[0].tolist()\n                inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                keep = set()\n                # keep best half of finite\n                if len(finite_idx) > 0:\n                    sorted_fin = np.array(finite_idx)[np.argsort(F_arr[finite_idx])]\n                    nkeep_best = max(2, int(0.5 * len(sorted_fin)))\n                    for ii in sorted_fin[:nkeep_best]:\n                        keep.add(int(ii))\n                    # keep some random finite others\n                    rem_fin = list(sorted_fin[nkeep_best:])\n                    self.rng.shuffle(rem_fin)\n                    for ii in rem_fin[:max(0, int(0.2 * len(rem_fin)))]:\n                        keep.add(int(ii))\n                # add some infs for diversity\n                self.rng.shuffle(inf_idx)\n                for ii in inf_idx[:min(len(inf_idx), 3)]:\n                    keep.add(int(ii))\n                # fill with random picks if too few\n                all_idx = list(range(len(F_arr)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= archive_max:\n                        break\n                    keep.add(int(ii))\n                keep = sorted(list(keep))[:archive_max]\n                F_archive = [F_archive[i] for i in keep]\n                X_archive = [X_archive[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if np.isfinite(self.f_opt):\n                if self.f_opt < recent_best - 1e-12:\n                    recent_best = self.f_opt\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n            else:\n                stagnation_counter += 1\n\n            if stagnation_counter > max(20, 10 + dim):\n                # micro-restart: generate around best and add random uniform points\n                stagnation_counter = 0\n                # generate a few local perturbations around best\n                if self.x_opt is not None:\n                    nlocal = min(10 + dim, max(5, self.budget - self.evals - 1))\n                    for _ in range(nlocal):\n                        if self.evals >= self.budget:\n                            break\n                        pert = self.x_opt + self.rng.randn(dim) * (0.1 * scale) + 0.01 * self.rng.randn(dim) * domain\n                        pert = self._reflect_bounds(pert, lb, ub)\n                        f = safe_eval(pert)\n                        X_archive.append(pert.copy()); F_archive.append(f)\n                # add some pure randoms to diversify\n                nrand = min(5 + dim // 2, max(1, self.budget - self.evals))\n                for _ in range(nrand):\n                    if self.evals >= self.budget:\n                        break\n                    xx = self._uniform_array(lb, ub)\n                    f = safe_eval(xx)\n                    X_archive.append(xx.copy()); F_archive.append(f)\n\n        # final best: choose best finite from archive\n        if self.x_opt is not None and np.isfinite(self.f_opt):\n            return float(self.f_opt), self.x_opt.copy()\n        # fallback: choose best in archive if available\n        if len(F_archive) > 0:\n            F_arr = np.array(F_archive)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best_i = finite_idx[np.argmin(F_arr[finite_idx])]\n                return float(F_arr[best_i]), X_archive[best_i].copy()\n        # final fallback: return random point (no function call)\n        x = self._uniform_array(lb, ub)\n        return float(np.inf), x", "configspace": "", "generation": 0, "feedback": "The algorithm MemoryGuidedADS scored 0.329 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14948860379250895, 0.30459756469746946, 0.40901161454385515, 0.5407458992350116, 0.32615890297976513, 0.4290783438480815, 0.262876378252749, 0.352388376954176, 0.33673523586543397, 0.1782396727959783]}, "task_prompt": ""}
{"id": "8a792b24-ac88-49d4-97cf-f69d63d16d90", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = int(init_samples) if init_samples is not None else max(10, min(50, self.budget // 20))\n        self.archive_max = int(archive_max)\n\n        # state\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Try common wrappers, fall back to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = getattr(b, \"lb\", None)\n                ub = getattr(b, \"ub\", None)\n            if lb is None:\n                lb = getattr(func, \"lower_bounds\", None)\n            if ub is None:\n                ub = getattr(func, \"upper_bounds\", None)\n        except Exception:\n            lb = ub = None\n\n        if lb is None or ub is None:\n            # generic fallback\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub), dtype=float)\n            # if dims mismatch, fallback\n            if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n                lb = np.full(self.dim, -5.0, dtype=float)\n                ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure ub > lb\n        ub = np.maximum(ub, lb + 1e-12)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.asarray(x, dtype=float).copy()\n        # reflect repeatedly a few times to bring x into [lb,ub]\n        for _ in range(4):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        rng = self.rng\n        cut = np.linspace(0.0, 1.0, n + 1)\n        pts = np.empty((n, dim), dtype=float)\n        for j in range(dim):\n            a = cut[:-1]\n            b = cut[1:]\n            u = rng.rand(n)\n            pts[:, j] = a + u * (b - a)\n            rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        # returns shape (dim,) if n==1, else (n,dim)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.rand(n, self.dim) * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # sample independent Cauchy components with light gaussian tempering\n        # use standard Cauchy: tan(pi*(u-0.5))\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper: multiply by gaussian factor to reduce extremely heavy tails occasionally\n        g = self.rng.normal(scale=0.5 * temp, size=self.dim)\n        z = c / (1.0 + np.abs(g))\n        # scale by scale_vec\n        z = z * scale_vec\n        # clamp large values to reasonable multiples of scale\n        maxv = 20.0 * np.maximum(scale_vec, 1e-12)\n        z = np.clip(z, -maxv, maxv)\n        return z\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed if self.seed is not None else None)\n        self.rng = rng\n\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # reflect & clamp before evaluation\n            x_a = self._reflect_bounds(x, lb, ub)\n            if self.evals >= self.budget:\n                # budget exhausted, return inf\n                return np.inf, x_a\n            # call func and convert to scalar\n            try:\n                f = func(x_a)\n                if isinstance(f, np.ndarray):\n                    f = float(f.ravel()[0])\n                else:\n                    f = float(f)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_a.copy()\n            return f, x_a\n\n        # Archives\n        X_archive = []\n        F_archive = []\n\n        # initial LHS spread\n        n_init = max(2, min(self.init_samples, max(2, self.budget // 10)))\n        pts01 = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = lb + pts01[i] * domain_width\n            f, xa = safe_eval(x)\n            X_archive.append(xa.copy())\n            F_archive.append(f)\n\n        # add a couple of pure random points if budget remains\n        for _ in range(min(5, self.budget - self.evals)):\n            x = self._uniform_array(lb, ub)\n            f, xa = safe_eval(x)\n            X_archive.append(xa.copy()); F_archive.append(f)\n\n        # control variables\n        gscale = 0.08 * domain_width  # per-dim baseline gaussian scale (fraction of domain)\n        short_window = 40\n        recent_success = []\n        stagnation_counter = 0\n        recent_best = self.f_opt if np.isfinite(self.f_opt) else np.inf\n\n        total_iters = 0\n\n        # main loop\n        while self.evals < self.budget:\n            total_iters += 1\n\n            # prepare archive arrays\n            X = np.asarray(X_archive, dtype=float) if len(X_archive) > 0 else np.empty((0, self.dim))\n            F = np.asarray(F_archive, dtype=float) if len(F_archive) > 0 else np.empty((0,))\n\n            finite_mask = np.isfinite(F)\n            finite_inds = np.where(finite_mask)[0]\n            finite_vals = F[finite_mask] if finite_inds.size > 0 else np.array([])\n\n            # If too few archived finite points, sample more broadly\n            if finite_inds.size < 2:\n                cand = self._uniform_array(lb, ub)\n                f_cand, x_cand = safe_eval(cand)\n                X_archive.append(x_cand.copy()); F_archive.append(f_cand)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            try:\n                IQR = np.subtract(*np.percentile(X[finite_inds], [75, 25], axis=0))\n                scale_per_dim = np.where(IQR > 1e-12, IQR / 1.349, np.std(X[finite_inds], axis=0))\n            except Exception:\n                scale_per_dim = np.std(X[finite_inds], axis=0)\n            # clamp scale to reasonable portion of domain\n            scale_per_dim = np.clip(scale_per_dim, 1e-8 * domain_width, 0.6 * domain_width)\n\n            # choose a base index biased toward better F via softmax on -F\n            finite_F = F.copy()\n            # scores: lower F => higher score\n            if finite_inds.size > 0:\n                best_val = np.min(finite_F[finite_mask])\n                worst_val = np.max(finite_F[finite_mask])\n                rng_shift = 1e-6\n                scores = np.exp(-(finite_F - best_val) / (max(worst_val - best_val, 1e-9) + rng_shift))\n                scores[~finite_mask] = 1e-9\n                probs = scores / (scores.sum() + 1e-12)\n                base_idx = rng.choice(len(F), p=probs)\n            else:\n                base_idx = rng.randint(0, max(1, len(F)))\n\n            # small chance to use global best directly\n            if self.x_opt is not None and rng.rand() < 0.05:\n                base_idx = np.argmin(F)\n\n            # donor pool selection (for DE-like moves)\n            idxs = list(range(len(F)))\n            if len(idxs) >= 4:\n                choices = [i for i in idxs if i != base_idx]\n                a_idx, b_idx, c_idx = rng.choice(choices, size=3, replace=False)\n                a = X[a_idx]; b = X[b_idx]; c = X[c_idx]\n            else:\n                a = self._uniform_array(lb, ub)\n                b = self._uniform_array(lb, ub)\n                c = self._uniform_array(lb, ub)\n\n            base = X[base_idx].copy()\n\n            # choose move type\n            r = rng.rand()\n            if r < 0.28:\n                move_type = 'GLOBAL'\n            elif r < 0.7:\n                move_type = 'PCA'\n            else:\n                move_type = 'LOCAL'\n\n            # scale modifiers and temperature\n            t = 0.8 + 0.4 * rng.rand()\n            gamma = 0.6 * (0.5 + rng.rand())  # DE scaling\n\n            # produce candidate\n            cand = base.copy()\n            if move_type == 'GLOBAL':\n                # anchor on either elite or uniform\n                fin_sorted_idx = finite_inds[np.argsort(F[finite_inds])]\n                k = max(3, min(20, max(3, len(fin_sorted_idx) // 5 + 3)))\n                k = min(k, len(fin_sorted_idx))\n                chosen_idx = fin_sorted_idx[rng.randint(0, k)]\n                anchor = X[chosen_idx].copy() if rng.rand() < 0.8 else self._uniform_array(lb, ub)\n\n                # tempered cauchy jump around anchor\n                scale_vec = np.maximum(0.6 * scale_per_dim, 0.02 * domain_width)\n                cand = anchor + self._tempered_cauchy(scale_vec, temp=t)\n\n                # occasional DE-like differential injection\n                if rng.rand() < 0.18:\n                    cand = cand + gamma * (a - b)\n\n                # coordinate-wise local gaussian injection on some coords\n                mask = rng.rand(self.dim) < 0.25\n                cand[mask] += rng.normal(scale=0.02 * domain_width, size=mask.sum())\n\n                # small extra jitter\n                cand += rng.normal(scale=0.002 * domain_width, size=self.dim) * (rng.rand(self.dim) < 0.1)\n\n            elif move_type == 'PCA':\n                # choose subset of elites and do PCA\n                ne = max(3, min(20, len(finite_inds)))\n                elite_idx = finite_inds[np.argsort(F[finite_inds])[:ne]]\n                X_elite = X[elite_idx]\n                center = np.mean(X_elite, axis=0)\n                Xc = X_elite - center\n                try:\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    # choose number of components\n                    var_exp = S**2\n                    var_exp = var_exp / (var_exp.sum() + 1e-12)\n                    cum = np.cumsum(var_exp)\n                    ncomp = max(1, np.searchsorted(cum, 0.85) + 1)\n                    ncomp = min(ncomp, Vt.shape[0])\n                    # build perturbation in PCA subspace\n                    coeffs = (0.2 + 0.8 * rng.rand(ncomp)) * S[:ncomp]\n                    pca_dir = (Vt[:ncomp].T * coeffs).sum(axis=1)\n                    pca_pert = pca_dir * (0.5 + rng.rand())\n                    cand = center + pca_pert\n                    # sometimes attracted to best with small gaussian\n                    if self.x_opt is not None and rng.rand() < 0.12:\n                        cand = cand * (0.6 + 0.4 * rng.rand()) + self.x_opt * (0.4 * rng.rand())\n                except Exception:\n                    # fallback to global small gaussian\n                    cand = base + rng.normal(scale=0.04 * domain_width, size=self.dim)\n\n            else:  # LOCAL\n                # anisotropic Gaussian local moves around base, coordinate mixing, occasional DE step\n                per_dim_scale = gscale * (0.6 + 0.8 * rng.rand(self.dim))\n                cand = base + rng.normal(scale=per_dim_scale, size=self.dim)\n\n                # occasional DE injection to help cross ridges\n                if rng.rand() < 0.12:\n                    cand = cand + gamma * (a - b)\n\n                # mix some coordinates uniformly to escape ridges\n                mask = rng.rand(self.dim) < 0.08\n                if mask.any():\n                    cand[mask] = self._uniform_array(lb, ub)[mask]\n\n            # tempered cauchy tiny escape occasionally\n            if rng.rand() < 0.04:\n                tiny_scale = np.maximum(1e-3 * domain_width, 0.02 * scale_per_dim)\n                cand += self._tempered_cauchy(tiny_scale, temp=0.6)\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            bad_coords = ~np.isfinite(cand)\n            if bad_coords.any():\n                cand[bad_coords] = self._uniform_array(lb, ub)[bad_coords]\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand, x_cand = safe_eval(cand)\n            X_archive.append(x_cand.copy()); F_archive.append(f_cand)\n\n            # update success metrics\n            success = False\n            if np.isfinite(f_cand):\n                # rank of new candidate among finite (smaller is better)\n                finite_vals = np.asarray([v for v in F_archive if np.isfinite(v)], dtype=float)\n                rank_pos = np.sum(finite_vals < f_cand) + 1\n                if rank_pos <= max(1, int(0.25 * max(1, finite_vals.size))):\n                    success = True\n\n            recent_success.append(1 if success else 0)\n            if len(recent_success) > short_window:\n                recent_success.pop(0)\n\n            # short-term adaptation of gscale every few iterations\n            if total_iters % 8 == 0:\n                succ_rate = np.mean(recent_success) if recent_success else 0.0\n                if succ_rate > 0.25:\n                    # encourage smaller scales (exploitation)\n                    gscale = gscale * (0.85 - 0.05 * rng.rand(self.dim))\n                elif succ_rate < 0.08:\n                    # encourage exploration\n                    gscale = gscale * (1.08 + 0.07 * rng.rand(self.dim))\n                # keep within domain fractions (per-dim)\n                gscale = np.clip(gscale, 1e-8 * domain_width, 0.6 * domain_width)\n\n            # prune archive if too large\n            if len(F_archive) > self.archive_max:\n                F_arr = np.asarray(F_archive, dtype=float)\n                finite_idx = np.where(np.isfinite(F_arr))[0]\n                keep = set()\n                if finite_idx.size > 0:\n                    sorted_fin = finite_idx[np.argsort(F_arr[finite_idx])]\n                    nkeep_best = max(2, len(sorted_fin) // 2)\n                    for ii in sorted_fin[:nkeep_best]:\n                        keep.add(int(ii))\n                    # keep some random finite others\n                    others = [ii for ii in finite_idx if ii not in keep]\n                    rng.shuffle(others)\n                    for ii in others[:min(len(others), max(5, len(sorted_fin)//6))]:\n                        keep.add(int(ii))\n                # keep a few infs/diverse\n                nonfin = [i for i in range(len(F_arr)) if not np.isfinite(F_arr[i])]\n                rng.shuffle(nonfin)\n                for ii in nonfin[:5]:\n                    keep.add(int(ii))\n                # if too few kept, randomly add some\n                all_idx = list(range(len(F_arr)))\n                rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= self.archive_max:\n                        break\n                    keep.add(int(ii))\n                keep = sorted(list(keep))[:self.archive_max]\n                X_archive = [X_archive[i] for i in keep]\n                F_archive = [F_archive[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if np.isfinite(self.f_opt) and self.f_opt < recent_best - 1e-12:\n                recent_best = self.f_opt\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            if stagnation_counter > max(80, self.dim * 10):\n                # micro-restart: generate around best and add random uniform points\n                stagnation_counter = 0\n                # perturbations around best\n                if self.x_opt is not None and self.evals < self.budget:\n                    n_local = min(6 + self.dim // 3, max(3, self.budget - self.evals))\n                    for _ in range(n_local):\n                        if self.evals >= self.budget:\n                            break\n                        scale = 0.02 * domain_width * (1.0 + rng.rand(self.dim))\n                        pert = self.x_opt + rng.normal(scale=scale)\n                        f, xa = safe_eval(pert)\n                        X_archive.append(xa.copy()); F_archive.append(f)\n                # add some pure randoms to diversify\n                nrand = min(6 + self.dim // 2, max(1, self.budget - self.evals))\n                for _ in range(nrand):\n                    if self.evals >= self.budget:\n                        break\n                    x = self._uniform_array(lb, ub)\n                    f, xa = safe_eval(x)\n                    X_archive.append(xa.copy()); F_archive.append(f)\n\n        # final best: choose best finite from archive\n        if len(F_archive) > 0:\n            F_arr = np.asarray(F_archive, dtype=float)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best_idx = finite_idx[np.argmin(F_arr[finite_idx])]\n            else:\n                best_idx = int(np.nanargmin(F_arr)) if F_arr.size > 0 else None\n            if best_idx is not None:\n                return float(F_archive[best_idx]), np.asarray(X_archive[best_idx], dtype=float)\n        # final fallback\n        if self.x_opt is not None:\n            return self.f_opt, self.x_opt\n        # as last resort, random point\n        x = np.random.uniform(lb, ub)\n        f = func(x)\n        return float(f), np.asarray(x, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 247, in numpy.random._common.validate_output_shape, the following error occurred:\nValueError: Output size (1,) is not compatible with broadcast dimensions of inputs (2,).\nOn line: chosen_idx = fin_sorted_idx[rng.randint(0, k)]", "error": "In the code, line 247, in numpy.random._common.validate_output_shape, the following error occurred:\nValueError: Output size (1,) is not compatible with broadcast dimensions of inputs (2,).\nOn line: chosen_idx = fin_sorted_idx[rng.randint(0, k)]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "cf50ac2a-b56b-4600-ab91-9d695a9773a0", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=10, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n\n        self.rng = np.random.RandomState(self.seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # state counters\n        self.evals = 0\n\n        # short-term adaptation variables\n        self.gscale = np.ones(self.dim) * 0.2 * 10.0 / max(1.0, self.dim)  # initial per-dim fraction of domain\n        self.short_success = 0\n        self.short_trials = 0\n        self.adapt_interval = 50  # adapt every this many iterations\n        self.iter_since_adapt = 0\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Try to fetch bounds from common wrappers, otherwise default [-5,5]^dim\n        lb = None\n        ub = None\n        # Try attributes used by common wrappers\n        for attr in (\"bounds\", \"search_space\", \"domain\"):\n            if hasattr(func, attr):\n                b = getattr(func, attr)\n                # If it's an object with lb/ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                    break\n                # If it's tuple/list like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                    break\n        # fallback to func.bounds if available\n        if lb is None and hasattr(func, \"bounds\"):\n            b = func.bounds\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            elif isinstance(b, (tuple, list)) and len(b) == 2:\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n\n        # fallback default box [-5,5]\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure shapes\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # sanity clamp and ensure ub > lb\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect repeatedly to bring x within [lb,ub], then clamp if numerical drift\n        x = np.asarray(x, dtype=float).copy()\n        # vectorized reflection with cap iterations\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = lb + (lb - x) => x = 2*lb - x\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above: x = ub - (x - ub) => x = 2*ub - x\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        pts = a[:, None] + (b - a)[:, None] * u\n        # shuffle each column\n        for j in range(dim):\n            self.rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.rand(n, self.dim) * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # sample independent Cauchy components with light gaussian tempering for numerical stability\n        # draw standard cauchy then temper with gaussian noise and clamp large values\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        g = self.rng.randn(self.dim) * 0.5  # gaussian temper\n        tempered = c / (1.0 + np.abs(g) * temp)\n        step = tempered * np.asarray(scale_vec, dtype=float)\n        # clamp to reasonable multiples\n        max_mul = 10.0\n        cap = max_mul * np.asarray(scale_vec, dtype=float)\n        step = np.maximum(np.minimum(step, cap), -cap)\n        return step\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        self.evals = 0\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            x_a = np.asarray(x, dtype=float).reshape(self.dim)\n            try:\n                f = func(x_a)\n            except Exception:\n                # try calling with list\n                f = func(list(x_a))\n            # if returns array, try to flatten to scalar\n            if isinstance(f, (list, tuple, np.ndarray)):\n                f = np.asarray(f)\n                if f.size == 0:\n                    f = np.inf\n                else:\n                    f = float(np.ravel(f)[0])\n            # update counters and best bookkeeping\n            self.evals += 1\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_a.copy()\n            return float(f)\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # initial LHS samples (spread)\n        n_init = max(2, min(self.init_samples, max(2, self.budget // 20)))\n        n_init = min(n_init, self.budget)\n        pts = self._lhs01(n_init, self.dim)\n        pts = pts * (ub - lb) + lb\n        for i in range(n_init):\n            x = pts[i]\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        for _ in range(3):\n            if self.evals >= self.budget: break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        success_count = 0\n        stagnation_counter = 0\n        iter_count = 0\n\n        # main loop: continue until budget used\n        try:\n            while self.evals < self.budget:\n                iter_count += 1\n                self.iter_since_adapt += 1\n\n                # prepare archived arrays\n                if len(X_archive) == 0:\n                    cand = self._uniform_array(lb, ub)\n                    f_cand = safe_eval(cand)\n                    X_archive.append(cand.copy()); F_archive.append(f_cand)\n                    continue\n\n                X = np.asarray(X_archive, dtype=float)\n                F = np.asarray(F_archive, dtype=float)\n                finite_mask = np.isfinite(F)\n                finite_idx = np.where(finite_mask)[0]\n                nonfinite_idx = np.where(~finite_mask)[0]\n\n                # if too few finite points => sample more broadly\n                if finite_idx.size < 3:\n                    cand = self._uniform_array(lb, ub)\n                    f_cand = safe_eval(cand)\n                    X_archive.append(cand.copy()); F_archive.append(f_cand)\n                    continue\n\n                # compute per-dimension robust scale (IQR) fallback to std\n                iqr = np.subtract(*np.percentile(X[finite_idx], [75, 25], axis=0))\n                std = np.std(X[finite_idx], axis=0)\n                scale = np.where(np.isfinite(iqr) & (iqr > 1e-12), iqr, std)\n                # clamp scale to reasonable portion of domain\n                scale = np.maximum(scale, 1e-8)\n                scale = np.minimum(scale, 0.5 * domain)\n\n                # choose a base index biased toward better F via softmax on -F\n                Finf = F.copy()\n                Finf[~finite_mask] = np.nanmax(F[finite_mask]) + 10.0\n                # compute softmax weights on negative F (lower is better)\n                recent = Finf[finite_mask]\n                # numerical safe softmax\n                vals = -recent\n                vals = vals - np.max(vals)\n                weights = np.exp(vals / (1.0 + np.std(vals)))\n                weights = weights / np.sum(weights)\n                # map pick\n                base_idx = np.random.choice(finite_idx, p=weights)\n                base = X[base_idx].copy()\n\n                # small chance to use global best directly\n                if (self.x_opt is not None) and (self.rng.rand() < 0.05):\n                    base = self.x_opt.copy()\n\n                # donor pool selection (for DE-like moves)\n                donors_idx = self.rng.choice(finite_idx, size=min(6, finite_idx.size), replace=False)\n\n                # choose move type\n                r = self.rng.rand()\n                move_type = 'LOCAL' if r < 0.6 else 'GLOBAL'\n\n                cand = base.copy()\n\n                # scale modifiers\n                global_scale = np.median(scale)\n                gscale_vec = self.gscale * domain  # convert fraction to absolute\n\n                if move_type == 'GLOBAL':\n                    # anchor on either elite or uniform\n                    if (self.rng.rand() < 0.7) and (finite_idx.size > 0):\n                        # pick among top-k elites\n                        k = max(3, min(30, max(3, finite_idx.size // 6 + 3)))\n                        elite_idx = np.argsort(F[finite_idx])[:k]\n                        chosen = finite_idx[self.rng.choice(elite_idx)]\n                        anchor = X[chosen].copy()\n                    else:\n                        anchor = self._uniform_array(lb, ub)\n\n                    cand = anchor.copy()\n                    # tempered cauchy jump around anchor\n                    cand += self._tempered_cauchy(0.4 * scale + 1e-12, temp=1.0)\n\n                    # occasional DE-like differential injection\n                    if finite_idx.size >= 2 and self.rng.rand() < 0.45:\n                        d1, d2 = self.rng.choice(finite_idx, size=2, replace=False)\n                        diff = X[d1] - X[d2]\n                        Fdiff = self.rng.randn(self.dim) * 0.2\n                        cand += 0.6 * diff * (0.5 + 0.5 * self.rng.rand()) + Fdiff * scale\n\n                    # coordinate-wise local gaussian injection on some coords\n                    mask = self.rng.rand(self.dim) < 0.5\n                    if mask.any():\n                        noise = self.rng.randn(self.dim) * (0.5 * scale)\n                        cand[mask] += noise[mask]\n\n                    # PCA-guided elite perturbations sometimes\n                    if (finite_idx.size >= 5) and (self.rng.rand() < 0.35):\n                        fin_idx_list = finite_idx\n                        k = max(3, min(30, max(3, len(fin_idx_list) // 6 + 3)))\n                        # pick elites on actual F\n                        sorted_fin = fin_idx_list[np.argsort(F[fin_idx_list])]\n                        elites = X[sorted_fin[:k]]\n                        # center and do SVD\n                        Xc = elites - np.mean(elites, axis=0)\n                        try:\n                            U, Svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                            ncomp = max(1, min(self.dim, int(1 + np.sum(Svals > (0.1 * Svals[0])))))\n                            comps = Vt[:ncomp]\n                            # sample in PCA subspace\n                            coeffs = self.rng.randn(ncomp) * (0.3 * np.mean(scale))\n                            pca_pert = np.zeros(self.dim)\n                            for j in range(ncomp):\n                                pca_pert += coeffs[j] * comps[j]\n                            cand += pca_pert\n                        except Exception:\n                            # fallback small gaussian\n                            cand += self.rng.randn(self.dim) * (0.05 * domain)\n\n                    # small extra jitter\n                    cand += self.rng.randn(self.dim) * (0.02 * domain)\n\n                else:  # LOCAL\n                    # anisotropic Gaussian local moves around base, coordinate mixing, occasional DE step\n                    anis_noise = self.rng.randn(self.dim) * (0.5 * gscale_vec)\n                    cand += anis_noise\n\n                    # occasional differential injection (DE-like)\n                    if self.rng.rand() < 0.12 and finite_idx.size >= 3:\n                        d = self.rng.choice(finite_idx, size=2, replace=False)\n                        cand += 0.8 * (X[d[0]] - X[d[1]]) * self.rng.rand()\n\n                    # mix some coordinates uniformly to escape ridges\n                    mask = self.rng.rand(self.dim) < 0.08\n                    if mask.any():\n                        uni = self._uniform_array(lb, ub)\n                        cand[mask] = uni[mask]\n\n                    # tempered small cauchy tiny escape occasionally\n                    if self.rng.rand() < 0.08:\n                        cand += self._tempered_cauchy(0.05 * domain, temp=0.5)\n\n                # safety: nan-inf handling, replace non-finite coords with uniform\n                bad_coords = ~np.isfinite(cand)\n                if bad_coords.any():\n                    cand[bad_coords] = self._uniform_array(lb, ub)[bad_coords]\n\n                # reflect to bounds and clamp\n                cand = self._reflect_bounds(cand, lb, ub)\n\n                # evaluate candidate\n                try:\n                    f_cand = safe_eval(cand)\n                except StopIteration:\n                    break\n\n                # add to archive\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n                # update success metrics\n                finite_vals = F[np.isfinite(F)]\n                rank_pos = 1 + np.sum((finite_vals < f_cand))\n                if np.isfinite(f_cand) and rank_pos <= max(1, int(0.25 * max(1, finite_vals.size))):\n                    success = True\n                else:\n                    success = False\n\n                if success:\n                    success_count += 1\n                    self.short_success += 1\n                self.short_trials += 1\n\n                # short-term adaptation of gscale every few iterations\n                if self.iter_since_adapt >= self.adapt_interval:\n                    # adjust based on recent success rate\n                    sr = (self.short_success / max(1, self.short_trials))\n                    # encourage smaller scales if success high, otherwise increase\n                    if sr > 0.2:\n                        self.gscale *= 0.9\n                    elif sr < 0.05:\n                        self.gscale *= 1.15\n                    # keep within domain fractions (per-dim)\n                    self.gscale = np.clip(self.gscale, 1e-4, 0.8)\n                    # reset short-term success\n                    self.short_success = 0\n                    self.short_trials = 0\n                    self.iter_since_adapt = 0\n\n                # prune archive if too large\n                if len(F_archive) > self.archive_max:\n                    F_arr = np.array(F_archive)\n                    inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                    finite_idx2 = np.where(np.isfinite(F_arr))[0]\n                    sorted_fin = np.argsort(F_arr[finite_idx2])  # indices into finite_idx2\n                    keep = set()\n                    # keep best half of finite\n                    if len(finite_idx2) > 0:\n                        nkeep_best = max(2, int(0.5 * len(sorted_fin)))\n                        best_fin_indices = finite_idx2[sorted_fin[:nkeep_best]]\n                        for ii in best_fin_indices:\n                            keep.add(int(ii))\n                    # keep some random finite others\n                    others = [int(ii) for ii in finite_idx2 if ii not in keep]\n                    self.rng.shuffle(others)\n                    for ii in others[:max(0, int(0.2 * self.archive_max))]:\n                        keep.add(int(ii))\n                    # add some infs for diversity\n                    for ii in inf_idx[:min(len(inf_idx), 5)]:\n                        keep.add(int(ii))\n                    # fill with random picks if too few\n                    all_idx = list(range(len(F_archive)))\n                    self.rng.shuffle(all_idx)\n                    for ii in all_idx:\n                        if len(keep) >= self.archive_max:\n                            break\n                        keep.add(int(ii))\n                    keep_list = sorted(list(keep))\n                    X_archive = [X_archive[i] for i in keep_list]\n                    F_archive = [F_archive[i] for i in keep_list]\n\n                # stagnation detection & micro-restarts\n                # measure progress against best\n                if np.isfinite(self.f_opt):\n                    if f_cand < self.f_opt - 1e-12:\n                        recent_best = self.f_opt\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    stagnation_counter += 1\n\n                # if stagnation observed, do micro-restart to diversify\n                if stagnation_counter > max(50, int(0.05 * self.budget)):\n                    stagnation_counter = 0\n                    # generate a few local perturbations around best\n                    if self.x_opt is not None:\n                        for _ in range(min(6, max(1, self.dim // 2))):\n                            xx = self.x_opt + self.rng.randn(self.dim) * (0.05 * domain)\n                            xx = self._reflect_bounds(xx, lb, ub)\n                            try:\n                                f = safe_eval(xx)\n                            except StopIteration:\n                                break\n                            X_archive.append(xx.copy()); F_archive.append(f)\n                    # add some pure randoms to diversify\n                    for _ in range(10):\n                        if self.evals >= self.budget:\n                            break\n                        xx = self._uniform_array(lb, ub)\n                        try:\n                            f = safe_eval(xx)\n                        except StopIteration:\n                            break\n                        X_archive.append(xx.copy()); F_archive.append(f)\n\n        except StopIteration:\n            # budget exhausted, fall through to return\n            pass\n\n        # final best: choose best finite from archive\n        if len(F_archive) > 0:\n            F_arr = np.array(F_archive)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best_i = finite_idx[np.argmin(F_arr[finite_idx])]\n                return float(F_arr[best_i]), np.asarray(X_archive[best_i], dtype=float)\n            else:\n                # all non-finite, return random archived x without evaluating\n                return np.inf, np.asarray(X_archive[0], dtype=float)\n        else:\n            # final fallback: return a random point (no function call)\n            x = self._uniform_array(lb, ub)\n            return np.inf, x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "f60bfdc2-ee88-4a52-ad30-f42de01cebf7", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.init_samples = int(max(2, init_samples))\n        self.archive_max = int(archive_max)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = np.zeros(self.dim, dtype=float)\n\n        # short-term adaptation variables\n        self.short_window = max(10, int(0.02 * self.budget))\n        self.short_attempts = 0\n        self.short_success = 0\n        # per-dimension scale (start as fraction of domain)\n        self.gscale = np.full(self.dim, 0.2)  # fraction of domain range (later multiplied by domain size)\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Default bounds [-5,5]^dim\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Try common wrappers/attributes - robust, non-failing\n        try:\n            if hasattr(func, \"bounds\"):\n                b = getattr(func, \"bounds\")\n                # cases: object with lb/ub, or tuple/list (lb,ub)\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb_try = np.asarray(b.lb, dtype=float)\n                    ub_try = np.asarray(b.ub, dtype=float)\n                    if lb_try.size == self.dim and ub_try.size == self.dim:\n                        lb, ub = lb_try, ub_try\n                elif isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb_try = np.asarray(b[0], dtype=float)\n                    ub_try = np.asarray(b[1], dtype=float)\n                    if lb_try.size == self.dim and ub_try.size == self.dim:\n                        lb, ub = lb_try, ub_try\n        except Exception:\n            pass\n\n        # some benchmark functions expose lb/ub directly\n        try:\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                lb_try = np.asarray(func.lb, dtype=float)\n                ub_try = np.asarray(func.ub, dtype=float)\n                if lb_try.size == self.dim and ub_try.size == self.dim:\n                    lb, ub = lb_try, ub_try\n        except Exception:\n            pass\n\n        # Ensure ub > lb\n        ub = np.asarray(ub, dtype=float)\n        lb = np.asarray(lb, dtype=float)\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # clamp\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or ub[i] <= lb[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.asarray(x, dtype=float).copy()\n        # reflect repeatedly until inside with safe cap\n        for _ in range(12):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect: x = 2*lb - x for below; x = 2*ub - x for above\n            x[below] = 2.0 * lb[below] - x[below]\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp for numerical drift\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        rng = self.rng\n        rng_state = rng.get_state()\n        perm = np.zeros((n, dim))\n        for j in range(dim):\n            perm[:, j] = (rng.permutation(n) + rng.rand(n)) / n\n        rng.set_state(rng_state)\n        return perm\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            # return n x dim\n            return self.rng.uniform(lb, ub, size=(n, self.dim))\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # sample independent Cauchy components with light tempering/clamping\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # temper/clamp large tails smoothly\n        cap = 50.0\n        tempered = c / (1.0 + np.abs(c) / cap)\n        step = tempered * np.asarray(scale_vec, dtype=float) * temp\n        # final clamp to avoid NaNs\n        step = np.maximum(np.minimum(step, cap), -cap)\n        return step\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n        # ensure domain positive\n        domain = np.maximum(domain, 1e-8)\n        # adapt gscale to absolute domain now\n        self.gscale = np.clip(self.gscale * domain, 1e-8, domain * 0.5)\n\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            try:\n                # call function (ensure shape flat list)\n                out = func(x)\n            except Exception:\n                # sometimes the function expects list\n                try:\n                    out = func(list(x))\n                except Exception:\n                    evals += 1\n                    return np.inf\n            evals += 1\n            # handle array outputs\n            if isinstance(out, (list, tuple, np.ndarray)):\n                try:\n                    f = float(np.asarray(out).ravel()[0])\n                except Exception:\n                    f = np.inf\n            else:\n                try:\n                    f = float(out)\n                except Exception:\n                    f = np.inf\n            # update best\n            if np.isfinite(f) and f < getattr(self, \"f_opt\", np.inf):\n                self.f_opt = f\n                self.x_opt = np.asarray(x, dtype=float).copy()\n            return f\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # initial LHS samples (spread)\n        n_init = min(self.init_samples, max(2, self.budget // 20))\n        lhs = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            x = lb + lhs[i] * domain\n            f = safe_eval(x)\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if evals >= self.budget:\n                break\n\n        # add a couple purely uniform random points if budget remains\n        extra_uniform = min(6, max(0, self.budget - evals))\n        for _ in range(extra_uniform):\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if evals >= self.budget:\n                break\n\n        # control variables\n        no_improve_count = 0\n        best_since = self.f_opt if np.isfinite(self.f_opt) else np.inf\n        stagnation_thresh = max(50, int(0.05 * self.budget))\n        iters_since_micro = 0\n\n        # main loop\n        while evals < self.budget:\n            # prepare archived arrays\n            X_arr = np.asarray(X_archive, dtype=float)\n            F_arr = np.asarray(F_archive, dtype=float)\n            finite_mask = np.isfinite(F_arr)\n            finite_idx = np.where(finite_mask)[0]\n            finite_vals = F_arr[finite_mask] if finite_mask.any() else np.array([])\n\n            # if too few finite points => sample more broadly\n            if finite_vals.size < 3:\n                cand = self._uniform_array(lb, ub, n=1)\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy())\n                F_archive.append(f_cand)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            if finite_idx.size >= 4:\n                samples = X_arr[finite_idx]\n                q75 = np.percentile(samples, 75, axis=0)\n                q25 = np.percentile(samples, 25, axis=0)\n                scales = np.maximum(q75 - q25, 1e-8)\n            else:\n                scales = np.std(X_arr[finite_idx], axis=0)\n                scales = np.maximum(scales, 1e-8)\n            # clamp scale to reasonable portion of domain\n            scales = np.minimum(scales, 0.5 * domain)\n            scales = np.maximum(scales, 1e-8)\n\n            # choose a base index biased toward better F via softmax on -F\n            recent = F_arr.copy()\n            # convert infinities to large positive values (worse)\n            recent[~np.isfinite(recent)] = recent[np.isfinite(recent)].max() + 1.0 if np.any(np.isfinite(recent)) else 1e6\n            vals = -recent\n            # numerical safe softmax\n            vals = vals - np.max(vals)\n            w = np.exp(vals / (np.std(vals) + 1e-8))\n            probs = w / np.sum(w)\n\n            # pick base from archive\n            base_idx = self.rng.choice(len(X_archive), p=probs)\n            base = X_arr[base_idx].copy()\n\n            # small chance to use global best directly\n            if (self.x_opt is not None) and (self.rng.rand() < 0.15):\n                base = self.x_opt.copy()\n\n            # donor pool selection (for DE-like moves)\n            pool_idx = list(range(len(X_archive)))\n            if len(pool_idx) >= 3:\n                a, b, c = self.rng.choice(pool_idx, size=3, replace=False)\n            else:\n                # fallback to random uniform\n                a = b = c = base_idx\n\n            # choose move type\n            r = self.rng.rand()\n            cand = base.copy()\n\n            # scale modifiers (mix short-term gscale and per-dim scales)\n            mixed_scale = 0.6 * self.gscale + 0.4 * scales\n\n            if r < 0.22:\n                # tempered cauchy around anchor -> global jumps\n                step = self._tempered_cauchy(mixed_scale, temp=1.0)\n                cand = base + step\n            elif r < 0.45:\n                # DE-like differential injection\n                diff = X_arr[b] - X_arr[c]\n                F_factor = 0.8 + 0.4 * self.rng.rand()\n                cand = base + F_factor * diff * (self.rng.rand(self.dim) * 0.7 + 0.3)\n                # small gaussian for jitter\n                cand += self.rng.randn(self.dim) * (0.05 * mixed_scale)\n            elif r < 0.65:\n                # anisotropic Gaussian local moves on some coords\n                mask = self.rng.rand(self.dim) < 0.6\n                noise = self.rng.randn(self.dim) * (0.6 * mixed_scale)\n                cand[mask] += noise[mask]\n            elif r < 0.82:\n                # PCA-guided elite perturbations\n                # pick top-k elites on actual F\n                k = max(3, min(30, max(3, finite_vals.size // 6 + 3)))\n                elite_idx = finite_idx[np.argsort(F_arr[finite_idx])[:k]]\n                elites = X_arr[elite_idx]\n                center = np.mean(elites, axis=0)\n                A = elites - center\n                try:\n                    # SVD/PCA\n                    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n                    # select subspace dim (50% energy)\n                    energy = np.cumsum(S**2) / np.sum(S**2)\n                    subdim = int(np.searchsorted(energy, 0.5) + 1)\n                    subdim = max(1, min(self.dim, subdim))\n                    # sample in PCA subspace\n                    coeffs = self.rng.randn(subdim) * (0.5 * S[:subdim])\n                    delta = np.dot(coeffs, Vt[:subdim, :])\n                    cand = center + delta + self.rng.randn(self.dim) * (0.02 * mixed_scale)\n                except Exception:\n                    # fallback small gaussian\n                    cand = base + self.rng.randn(self.dim) * (0.1 * mixed_scale)\n            else:\n                # mixed move: coordinate-wise mixing with uniform injection\n                mask = self.rng.rand(self.dim) < 0.25\n                cand[mask] = self._uniform_array(lb[mask], ub[mask], n=1)\n                cand += self.rng.randn(self.dim) * (0.03 * mixed_scale)\n\n            # occasional small tempered-cauchy escape\n            if self.rng.rand() < 0.04:\n                cand += self._tempered_cauchy(0.5 * mixed_scale, temp=0.6)\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect to bounds and clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n\n            # add to archive\n            X_archive.append(cand.copy())\n            F_archive.append(f_cand)\n\n            # update success metrics\n            # define rank among finite values (1 is best)\n            if np.isfinite(f_cand):\n                combined_fin = np.array([v for v in F_archive if np.isfinite(v)])\n                rank_pos = np.sum(combined_fin < f_cand) + 1\n                if rank_pos <= max(1, int(0.25 * max(1, combined_fin.size))):\n                    success = True\n                else:\n                    success = False\n            else:\n                success = False\n\n            self.short_attempts += 1\n            if success:\n                self.short_success += 1\n                no_improve_count = 0\n                iters_since_micro = 0\n            else:\n                no_improve_count += 1\n                iters_since_micro += 1\n\n            # short-term adaptation of gscale every few iterations\n            if self.short_attempts >= min(self.short_window, max(5, int(0.01 * self.budget))):\n                rate = self.short_success / max(1, self.short_attempts)\n                # if success high -> shrink scales slightly, else increase\n                factor = 0.85 if rate > 0.25 else (1.12 if rate < 0.08 else 1.0)\n                # apply small per-dim stochastic adjustments respecting domain\n                noise = 1.0 + 0.05 * (self.rng.randn(self.dim))\n                self.gscale = np.clip(self.gscale * factor * noise, 1e-8, domain * 0.6)\n                # reset short-term counters\n                self.short_attempts = 0\n                self.short_success = 0\n\n            # prune archive if too large\n            if len(X_archive) > self.archive_max:\n                # keep best half of finite\n                keep = set()\n                finite_idx = np.where(np.isfinite(F_arr))[0]\n                if finite_idx.size > 0:\n                    sorted_finite = finite_idx[np.argsort(F_arr[finite_idx])]\n                    for ii in sorted_finite[: max(1, finite_idx.size // 2)]:\n                        keep.add(int(ii))\n                # keep some random finite others for diversity\n                others = [i for i in range(len(X_archive)) if i not in keep]\n                if len(others) > 0:\n                    picks = self.rng.choice(others, size=min(len(others), self.archive_max - len(keep)), replace=False)\n                    for ii in picks:\n                        keep.add(int(ii))\n                # rebuild small archive\n                X_archive = [X_archive[i] for i in sorted(list(keep))]\n                F_archive = [F_archive[i] for i in sorted(list(keep))]\n\n            # stagnation detection & micro-restarts\n            if (self.f_opt < best_since):\n                best_since = self.f_opt\n                no_improve_count = 0\n            else:\n                no_improve_count += 0  # already updated above; keep consistent\n\n            # if stagnation observed, do micro-restart to diversify\n            if iters_since_micro >= stagnation_thresh:\n                # generate a few local perturbations around best and some randoms\n                n_local = min(8, max(2, self.dim))\n                if self.x_opt is None:\n                    # sample randoms\n                    for _ in range(n_local):\n                        if evals >= self.budget:\n                            break\n                        x = self._uniform_array(lb, ub, n=1)\n                        f = safe_eval(x)\n                        X_archive.append(x.copy()); F_archive.append(f)\n                else:\n                    for k in range(n_local):\n                        if evals >= self.budget:\n                            break\n                        scale_k = (0.05 + 0.2 * (k / max(1, n_local))) * domain\n                        x = self.x_opt + self.rng.randn(self.dim) * scale_k\n                        x = self._reflect_bounds(x, lb, ub)\n                        f = safe_eval(x)\n                        X_archive.append(x.copy()); F_archive.append(f)\n                    # add some pure randoms for diversity\n                    for _ in range(min(6, max(1, self.dim // 2))):\n                        if evals >= self.budget:\n                            break\n                        x = self._uniform_array(lb, ub, n=1)\n                        f = safe_eval(x)\n                        X_archive.append(x.copy()); F_archive.append(f)\n                # slightly increase gscale to encourage exploration\n                self.gscale = np.minimum(self.gscale * 1.6, domain * 0.8)\n                iters_since_micro = 0\n\n            # loop continues until budget exhausted\n\n        # final best: choose best finite from archive\n        if len(F_archive) == 0:\n            # no evals (!) fallback random point\n            x = self._uniform_array(lb, ub, n=1)\n            return np.inf, x\n        F_arr = np.asarray(F_archive, dtype=float)\n        X_arr = np.asarray(X_archive, dtype=float)\n        if np.any(np.isfinite(F_arr)):\n            idx = int(np.nanargmin(F_arr))\n            return float(F_arr[idx]), X_arr[idx].copy()\n        else:\n            # all non-finite, return random archived x without evaluating\n            idx = self.rng.randint(0, len(X_archive))\n            return np.inf, X_arr[idx].copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "64e5db02-56ab-489e-a26e-9e4aa84b42d0", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=20, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.init_samples = max(4, int(init_samples))\n        self.archive_max = int(archive_max)\n\n        # bookkeeping\n        self.nfev = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Returns lb, ub arrays of shape (dim,)\n        dim = self.dim\n        # Try common attributes\n        if hasattr(func, 'bounds'):\n            b = getattr(func, 'bounds')\n            # Accept (lb, ub) tuple/list\n            if isinstance(b, (tuple, list)) and len(b) == 2:\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n                if lb.shape == ():  # scalars\n                    lb = np.full(dim, float(lb))\n                    ub = np.full(dim, float(ub))\n                return lb.reshape(dim), ub.reshape(dim)\n        # try lb/ub attributes\n        for a in ('lb', 'ub', 'lower_bounds', 'upper_bounds', 'lower', 'upper'):\n            if hasattr(func, a):\n                try:\n                    lb = np.asarray(getattr(func, 'lb' if a == 'lb' else ('lower_bounds' if a == 'lower_bounds' else getattr(func, a))), dtype=float)\n                except Exception:\n                    pass\n        # If nothing found, default [-5,5]\n        lb = np.full(dim, -5.0, dtype=float)\n        ub = np.full(dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflective boundary handling: reflect repeatedly up to a few times then clip\n        x = x.copy()\n        it = 0\n        maxit = 6\n        while it < maxit and (np.any(x < lb) or np.any(x > ub)):\n            below = x < lb\n            above = x > ub\n            if np.any(below):\n                x[below] = 2 * lb[below] - x[below]\n            if np.any(above):\n                x[above] = 2 * ub[above] - x[above]\n            it += 1\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # LHS in [0,1)\n        u = np.zeros((n, dim), dtype=float)\n        rng = self.rng\n        for j in range(dim):\n            perm = rng.permutation(n)\n            u[:, j] = (perm + rng.rand(n)) / n\n        return u\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + (ub - lb) * self.rng.rand(self.dim)\n        else:\n            return lb + (ub - lb) * self.rng.rand(n, self.dim)\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # tempered cauchy: draw independent standard cauchy, temper with gaussian, clamp to multiples\n        rng = self.rng\n        # sample standard cauchy via inverse CDF\n        u = rng.rand(self.dim) - 0.5\n        c = np.tan(np.pi * u)\n        # temper with gaussian noise\n        g = rng.normal(scale=0.2, size=self.dim)\n        t = c * np.exp(-0.5 * (g**2))  # damp extreme tails slightly\n        # scale\n        step = t * scale_vec * temp\n        # clamp to avoid extremely large leaps: max 20 * scale_vec\n        max_mult = 20.0\n        cap = max_mult * np.maximum(1e-12, scale_vec)\n        step = np.maximum(-cap, np.minimum(cap, step))\n        return step\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        dim = self.dim\n        lb, ub = self._get_bounds(func)\n        lb = lb.reshape(dim)\n        ub = ub.reshape(dim)\n        domain_range = ub - lb\n        domain_range[domain_range <= 0] = 10.0  # fallback\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # x should be 1D array-like\n            if self.nfev >= self.budget:\n                return np.inf\n            try:\n                # ensure list or array\n                xin = np.asarray(x, dtype=float).reshape(dim)\n            except Exception:\n                xin = np.asarray(x, dtype=float)\n            try:\n                # try list first for wrappers that prefer python lists\n                f = func(xin.tolist())\n            except Exception:\n                try:\n                    f = func(xin)\n                except Exception:\n                    f = np.inf\n            # If returns array, try to reduce to scalar\n            try:\n                if isinstance(f, (list, tuple, np.ndarray)):\n                    f = float(np.asarray(f).ravel()[0])\n                else:\n                    f = float(f)\n            except Exception:\n                f = np.inf\n            self.nfev += 1\n            # update best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = xin.copy()\n            return f\n\n        # Archive: store points and function values\n        X_archive = []\n        F_archive = []\n\n        # initial LHS samples (spread)\n        init_n = min(self.init_samples, max(1, self.budget // 10))\n        lhs01 = self._lhs01(init_n, dim)\n        X0 = lb + (ub - lb) * lhs01\n        for i in range(init_n):\n            if self.nfev >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            X_archive.append(x.copy())\n            F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        extra_uniform = min(4, max(0, self.budget - self.nfev))\n        for i in range(extra_uniform):\n            if self.nfev >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy())\n            F_archive.append(f)\n\n        # control variables\n        rng = self.rng\n        # per-dimension global-scale (gscale) will be adapted\n        # initialize as fraction of domain\n        gscale = np.maximum(domain_range * 0.08, 1e-8)  # default ~8% of domain\n        gscale0 = gscale.copy()\n        # short-term adaptation trackers\n        recent_success = []\n        adapt_interval = 40\n        adapt_target = 0.2  # desired success rate\n        adapt_up = 1.2\n        adapt_down = 0.85\n\n        # move probabilities\n        p_cauchy = 0.12\n        p_de = 0.30\n        p_pca = 0.15\n        p_local = 0.35\n        p_randmix = 0.08\n\n        iter_since_best = 0\n        stagnation_limit = max(200, 5 * dim)\n        micro_restart_count = 0\n\n        # main loop: continue until budget used\n        while self.nfev < self.budget:\n            # prepare archived arrays\n            X_arr = np.array(X_archive) if len(X_archive) > 0 else np.empty((0, dim))\n            F_arr = np.array(F_archive) if len(F_archive) > 0 else np.empty((0,))\n\n            finite_mask = np.isfinite(F_arr)\n            n_finite = int(np.sum(finite_mask))\n\n            # if too few finite points => sample more broadly\n            if n_finite < 4:\n                # random uniform exploration\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                X_archive.append(x.copy()); F_archive.append(f)\n                continue\n\n            # compute per-dimension robust scale (IQR) fallback to std\n            X_finite = X_arr[finite_mask]\n            if X_finite.shape[0] >= 3:\n                q75 = np.percentile(X_finite, 75, axis=0)\n                q25 = np.percentile(X_finite, 25, axis=0)\n                pscale = (q75 - q25) / 1.349  # approx std\n                # fallback to std where zero\n                stds = np.std(X_finite, axis=0)\n                pscale[pscale <= 0] = stds[pscale <= 0]\n            else:\n                pscale = np.std(X_finite, axis=0) if X_finite.size > 0 else domain_range * 0.2\n            # clamp scale to domain fraction\n            pscale = np.maximum(pscale, domain_range * 1e-6)\n            pscale = np.minimum(pscale, domain_range * 1.2)\n\n            # choose a base index biased toward better F via softmax on -F\n            Ff = F_arr.copy()\n            Ffinite = Ff[finite_mask]\n            bestF = np.min(Ffinite) if Ffinite.size > 0 else np.inf\n            # temperature scaled by spread of F\n            Ft = Ffinite - bestF\n            T = np.median(np.maximum(1e-8, Ft)) + 1e-8\n            # safe softmax\n            scores = np.exp(- (Ff - bestF) / (T + 1e-12))\n            scores[~finite_mask] = 0.0\n            ssum = np.sum(scores)\n            if ssum <= 0:\n                probs = np.ones(len(scores)) / len(scores)\n            else:\n                probs = scores / ssum\n            # pick base index\n            base_idx = rng.choice(len(X_arr), p=probs)\n            base_x = X_arr[base_idx].copy()\n            base_f = F_arr[base_idx]\n\n            # small chance to use global best directly\n            if rng.rand() < 0.02 and n_finite > 0:\n                best_idx = int(np.argmin(F_arr))\n                base_idx = best_idx\n                base_x = X_arr[best_idx].copy()\n                base_f = F_arr[best_idx]\n\n            # donor pool selection (for DE-like moves)\n            idxs = list(range(len(X_arr)))\n            # ensure we have enough donors\n            donors = []\n            if len(idxs) >= 4:\n                # choose distinct donors excluding base\n                pool = [i for i in idxs if i != base_idx]\n                donors = rng.choice(pool, size=min(4, len(pool)), replace=False)\n            else:\n                donors = rng.choice(idxs, size=min(2, len(idxs)), replace=True)\n\n            # choose move type\n            rmove = rng.rand()\n            xnew = base_x.copy()\n\n            # scale modifiers\n            # anchor either elite or uniform\n            if rng.rand() < 0.6 and n_finite >= 3:\n                # pick among top-k elites\n                k = max(2, int(0.15 * n_finite))\n                elite_idx_sorted = np.argsort(F_arr[finite_mask])[:k]\n                # map to original indices\n                finite_indices = np.nonzero(finite_mask)[0]\n                pick_idx = finite_indices[elite_idx_sorted[rng.randint(len(elite_idx_sorted))]]\n                anchor = X_arr[pick_idx].copy()\n            else:\n                # pick uniform archive member\n                anchor = X_arr[rng.randint(len(X_arr))].copy()\n\n            # operate by move type probabilities\n            if rmove < p_cauchy:\n                # tempered cauchy jump around anchor\n                step = self._tempered_cauchy(pscale, temp=1.0)\n                xnew = anchor + step\n            elif rmove < p_cauchy + p_de:\n                # DE-like differential injection (current-to-rand/cur-to-best hybrids)\n                Ffactor = 0.4 + 0.6 * rng.rand()\n                if len(donors) >= 2:\n                    d1 = X_arr[donors[0]]\n                    d2 = X_arr[donors[1]]\n                    de_step = Ffactor * (d1 - d2)\n                else:\n                    de_step = Ffactor * (self._uniform_array(lb, ub) - anchor)\n                # occasionally inject a scaled best-direction\n                if rng.rand() < 0.25 and n_finite > 0:\n                    best_idx = int(np.argmin(F_arr))\n                    best_dir = X_arr[best_idx] - anchor\n                    de_step = de_step + 0.5 * Ffactor * best_dir\n                xnew = anchor + de_step\n                # small gaussian jitter\n                xnew += rng.normal(scale=0.06, size=dim) * pscale\n            elif rmove < p_cauchy + p_de + p_pca:\n                # PCA-guided elite perturbations sometimes\n                k = max(3, min( max(3, int(0.2 * n_finite)), n_finite))\n                # pick elites by actual F\n                elite_indices = np.argsort(F_arr)[:k]\n                eliteX = X_arr[elite_indices]\n                # center and SVD\n                cen = np.mean(eliteX, axis=0)\n                Xc = eliteX - cen\n                try:\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    # sample in top subspace\n                    subdim = max(1, min(dim, int(np.sum(S > 1e-12))))\n                    coeffs = rng.normal(scale=0.6, size=subdim) * (S[:subdim] / (k**0.5))\n                    perturb = (coeffs @ Vt[:subdim, :])\n                    xnew = cen + perturb\n                except Exception:\n                    # fallback small gaussian\n                    xnew = anchor + rng.normal(scale=0.2, size=dim) * pscale\n            else:\n                # anisotropic Gaussian local moves around anchor, coordinate mixing\n                mask = rng.rand(dim) < (0.15 + 0.6 * rng.rand())\n                local_step = rng.normal(scale=1.0, size=dim) * (gscale * (rng.rand(dim) * 0.8 + 0.2))\n                xnew = anchor + local_step * mask\n                # occasional small cauchy escape on leftovers\n                if rng.rand() < 0.06:\n                    xnew += self._tempered_cauchy(gscale * 0.5, temp=0.6)\n\n            # occasional differential injection (small)\n            if rng.rand() < 0.05 and len(donors) >= 2:\n                d1 = X_arr[donors[0]]\n                d2 = X_arr[donors[1]]\n                xnew += 0.15 * (d1 - d2)\n\n            # mix some coordinates uniformly to escape ridges\n            if rng.rand() < 0.03:\n                mix_mask = rng.rand(dim) < 0.08\n                xnew[mix_mask] = self._uniform_array(lb, ub)[mix_mask]\n\n            # small extra jitter\n            xnew += rng.normal(scale=0.01, size=dim) * domain_range\n\n            # safety: nan-inf handling, replace non-finite coords with uniform\n            bad = ~np.isfinite(xnew)\n            if np.any(bad):\n                xnew[bad] = self._uniform_array(lb, ub)[bad]\n\n            # reflect to bounds and clamp\n            xnew = self._reflect_bounds(xnew, lb, ub)\n\n            # evaluate candidate (stop if budget exhausted)\n            if self.nfev >= self.budget:\n                break\n            fnew = safe_eval(xnew)\n            X_archive.append(xnew.copy()); F_archive.append(fnew)\n\n            # update success metrics: success if better than base or improved global best\n            succ = False\n            if np.isfinite(fnew) and np.isfinite(base_f) and fnew < base_f:\n                succ = True\n            if np.isfinite(fnew) and fnew < self.f_opt:\n                succ = True\n            recent_success.append(1 if succ else 0)\n            if len(recent_success) > 200:\n                recent_success.pop(0)\n\n            # short-term adaptation of gscale every few iterations\n            if (len(recent_success) >= adapt_interval) and (len(recent_success) % adapt_interval == 0):\n                srate = float(np.sum(recent_success[-adapt_interval:]) / max(1, adapt_interval))\n                if srate > adapt_target:\n                    # make gscale a bit smaller (finer search)\n                    gscale = np.maximum(gscale * adapt_down, domain_range * 1e-8)\n                else:\n                    # enlarge to explore more\n                    gscale = np.minimum(gscale * adapt_up, domain_range * 0.5)\n                # reset recent successes partially\n                recent_success = recent_success[-adapt_interval:]\n\n            # prune archive if too large\n            if len(X_archive) > self.archive_max:\n                # keep best half of finite, keep some random others, add a few infs for diversity\n                X_arr = np.array(X_archive)\n                F_arr = np.array(F_archive)\n                finite_mask = np.isfinite(F_arr)\n                finite_indices = np.where(finite_mask)[0]\n                inf_indices = np.where(~finite_mask)[0]\n                keep = []\n                if finite_indices.size > 0:\n                    k = max(1, finite_indices.size // 2)\n                    best_inds = finite_indices[np.argsort(F_arr[finite_indices])[:k]].tolist()\n                    keep.extend(best_inds)\n                # keep some random of the remaining finite\n                remaining = [i for i in range(len(X_archive)) if i not in keep]\n                rng.shuffle(remaining)\n                keep.extend(remaining[: max(0, self.archive_max - len(keep))])\n                # rebuild archives\n                X_archive = [X_archive[i] for i in keep]\n                F_archive = [F_archive[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if np.isfinite(self.f_opt):\n                # measure iterations since last best (approx using last added best)\n                # we keep iter_since_best as number of evaluations since last improvement\n                # update iter_since_best\n                # We approximate by checking most recent archive entries for improvement\n                # Simpler: track iter_since_best via last reset when f_opt changed.\n                pass  # iter_since_best handled below\n\n            # update iter_since_best\n            if len(F_arr) > 0:\n                current_best = np.min(F_arr[finite_mask]) if finite_mask.any() else np.inf\n                if current_best < self.f_opt:\n                    iter_since_best = 0\n                else:\n                    iter_since_best += 1\n\n            # detect stagnation\n            if iter_since_best > stagnation_limit or (len(recent_success) >= stagnation_limit and sum(recent_success[-stagnation_limit:]) == 0):\n                # micro-restart to diversify\n                micro_restart_count += 1\n                iter_since_best = 0\n                # generate a few local perturbations around best and some pure randoms\n                if n_finite > 0:\n                    best_idx = int(np.argmin(F_arr))\n                    best_x = X_arr[best_idx].copy()\n                else:\n                    best_x = self._uniform_array(lb, ub)\n                # create around-best perturbations\n                for k in range(min(8, self.budget - self.nfev)):\n                    if self.nfev >= self.budget:\n                        break\n                    alpha = 0.02 + 0.2 * rng.rand()\n                    x = best_x + rng.normal(scale=alpha, size=dim) * domain_range\n                    x = self._reflect_bounds(x, lb, ub)\n                    f = safe_eval(x)\n                    X_archive.append(x.copy()); F_archive.append(f)\n                # add several pure uniform samples\n                for k in range(min(8, self.budget - self.nfev)):\n                    if self.nfev >= self.budget:\n                        break\n                    x = self._uniform_array(lb, ub)\n                    f = safe_eval(x)\n                    X_archive.append(x.copy()); F_archive.append(f)\n                # relax gscale back towards initial\n                gscale = (gscale + gscale0) / 2.0\n                recent_success = []\n\n        # budget exhausted, fall through to return\n        # final best: choose best finite from archive\n        if len(F_archive) == 0:\n            # no evaluations? return center\n            x_return = lb + 0.5 * (ub - lb)\n            return np.inf, x_return\n        F_arr = np.array(F_archive)\n        X_arr = np.array(X_archive)\n        finite_mask = np.isfinite(F_arr)\n        if np.any(finite_mask):\n            best_idx = int(np.argmin(F_arr[finite_mask]))\n            finite_indices = np.nonzero(finite_mask)[0]\n            best_global_idx = finite_indices[best_idx]\n            return float(F_arr[best_global_idx]), X_arr[best_global_idx].copy()\n        else:\n            # all non-finite: return a random archived x without evaluating\n            idx = rng.randint(len(X_arr))\n            return np.inf, X_arr[idx].copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e2ae20ce-e003-4301-85ae-7ca53c07aef1", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n\n        self.rng = np.random.RandomState(self.seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # state counters\n        self.evals = 0\n\n        # short-term adaptation variables (fractions of domain)\n        self.gscale = np.ones(self.dim) * (0.2 * 10.0 / max(1.0, self.dim))\n        self.adapt_interval = 50  # adapt every this many iterations\n        self.iter_since_adapt = 0\n        self.short_success = 0\n        self.short_trials = 0\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Try to fetch bounds from common wrappers, otherwise default [-5,5]^dim\n        lb = None\n        ub = None\n        # Try attribute names that some wrappers provide\n        for attr in (\"bounds\", \"search_space\", \"domain\"):\n            if hasattr(func, attr):\n                b = getattr(func, attr)\n                # If it's an object with lb/ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                    break\n                # If it's tuple/list like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                    break\n        # try func.bounds fallback\n        if lb is None and hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            elif isinstance(b, (tuple, list)) and len(b) == 2:\n                lb = np.asarray(b[0], dtype=float)\n                ub = np.asarray(b[1], dtype=float)\n\n        # fallback default box [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure shapes\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # sanity clamp and ensure ub > lb\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect repeatedly to bring x within [lb,ub], then clamp if numerical drift\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = 2*lb - x\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above: x = 2*ub - x\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        pts = a[:, None] + (b - a)[:, None] * u\n        # shuffle each column\n        for j in range(dim):\n            self.rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.rand(n, self.dim) * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0, max_mul=4.0):\n        # sample independent Cauchy components with light gaussian tempering for numerical stability\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        # draw standard cauchy\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # temper with small gaussian noise to avoid extreme huge steps\n        g = self.rng.randn(self.dim) * 0.5\n        tempered = c / (1.0 + np.abs(g) * temp)\n        step = tempered * scale_vec\n        # clamp to reasonable multiples\n        cap = max_mul * np.maximum(scale_vec, 1e-12)\n        step = np.maximum(np.minimum(step, cap), -cap)\n        return step\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            x_a = np.asarray(x, dtype=float).ravel().copy()\n            try:\n                f_raw = func(x_a)\n            except Exception:\n                # try calling with list\n                try:\n                    f_raw = func(list(x_a))\n                except Exception:\n                    f_raw = np.inf\n            # if returns array, try to flatten to scalar\n            if isinstance(f_raw, (list, tuple, np.ndarray)):\n                try:\n                    f_val = float(np.ravel(f_raw)[0])\n                except Exception:\n                    f_val = np.inf\n            else:\n                try:\n                    f_val = float(f_raw)\n                except Exception:\n                    f_val = np.inf\n            # count this evaluation\n            self.evals += 1\n            # update best bookkeeping\n            if np.isfinite(f_val) and f_val < self.f_opt:\n                self.f_opt = float(f_val)\n                self.x_opt = x_a.copy()\n            return float(f_val)\n\n        # Archive: flexible lists\n        X_archive = []\n        F_archive = []\n\n        # initial LHS samples (spread) mapped to bounds\n        n_init = max(2, min(self.init_samples, max(2, self.budget // 20)))\n        n_init = min(n_init, max(2, self.budget))\n        pts = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            x = lb + pts[i] * domain\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        for _ in range(3):\n            if self.evals >= self.budget: break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        success_count = 0\n        stagnation_counter = 0\n        iter_count = 0\n\n        # main loop: continue until budget used\n        try:\n            while self.evals < self.budget:\n                iter_count += 1\n                self.iter_since_adapt += 1\n\n                # prepare archived arrays\n                if len(X_archive) == 0:\n                    cand = self._uniform_array(lb, ub)\n                    try:\n                        f = safe_eval(cand)\n                    except StopIteration:\n                        break\n                    X_archive.append(cand.copy()); F_archive.append(f)\n                    continue\n\n                X = np.asarray(X_archive, dtype=float)\n                F = np.asarray(F_archive, dtype=float)\n                finite_mask = np.isfinite(F)\n                finite_idx = np.where(finite_mask)[0]\n\n                # if too few finite points => sample more broadly\n                if finite_idx.size < 3:\n                    cand = self._uniform_array(lb, ub)\n                    try:\n                        f_cand = safe_eval(cand)\n                    except StopIteration:\n                        break\n                    X_archive.append(cand.copy()); F_archive.append(f_cand)\n                    continue\n\n                # compute per-dimension robust scale (IQR) fallback to std\n                try:\n                    iqr = np.subtract(*np.percentile(X[finite_idx], [75, 25], axis=0))\n                except Exception:\n                    iqr = np.zeros(self.dim)\n                std = np.std(X[finite_idx], axis=0)\n                scale = np.where((iqr > 1e-12), iqr, std)\n                # clamp scale to reasonable portion of domain\n                scale = np.maximum(scale, 1e-8)\n                scale = np.minimum(scale, 0.5 * domain)\n\n                # choose a base index biased toward better F via softmax on -F\n                Finf = F.copy()\n                if finite_idx.size > 0:\n                    Finf[~finite_mask] = np.nanmax(F[finite_mask]) + 10.0\n                # compute weights on finite entries only\n                recent_vals = F[finite_idx]\n                vals = -recent_vals\n                vals = vals - np.max(vals)\n                denom = (1.0 + np.std(vals)) if np.isfinite(np.std(vals)) and np.std(vals) > 1e-12 else 1.0\n                weights = np.exp(vals / denom)\n                weights = weights / np.sum(weights)\n                # map pick to global indices\n                try:\n                    pick_local = self.rng.choice(len(finite_idx), p=weights)\n                except Exception:\n                    pick_local = self.rng.randint(len(finite_idx))\n                base_idx = int(finite_idx[pick_local])\n                base = X[base_idx].copy()\n\n                # small chance to use global best directly\n                if (self.x_opt is not None) and (self.rng.rand() < 0.05):\n                    base = self.x_opt.copy()\n\n                # choose move type\n                r = self.rng.rand()\n                move_type = 'LOCAL' if r < 0.6 else 'GLOBAL'\n\n                cand = base.copy()\n\n                # scale modifiers\n                gscale_vec = self.gscale * domain  # convert fraction to absolute\n\n                if move_type == 'GLOBAL':\n                    # anchor on either elite or uniform\n                    if (self.rng.rand() < 0.7) and (finite_idx.size > 0):\n                        # pick among top-k elites\n                        k = max(3, min(30, max(3, finite_idx.size // 6 + 3)))\n                        sorted_fin = finite_idx[np.argsort(F[finite_idx])]\n                        elite_idx = sorted_fin[:min(len(sorted_fin), k)]\n                        if elite_idx.size > 0:\n                            chosen = int(self.rng.choice(elite_idx))\n                            anchor = X[chosen].copy()\n                        else:\n                            anchor = self._uniform_array(lb, ub)\n                    else:\n                        anchor = self._uniform_array(lb, ub)\n\n                    cand = anchor.copy()\n                    # tempered cauchy jump around anchor (global heavy jump)\n                    cand += self._tempered_cauchy(0.4 * (scale + 1e-12), temp=1.0, max_mul=6.0)\n\n                    # occasional DE-like differential injection\n                    if finite_idx.size >= 2 and self.rng.rand() < 0.45:\n                        dsel = self.rng.choice(finite_idx, size=min(2, finite_idx.size), replace=False)\n                        if dsel.size == 2:\n                            d1, d2 = int(dsel[0]), int(dsel[1])\n                            diff = X[d1] - X[d2]\n                            Fdiff = self.rng.randn(self.dim) * 0.2\n                            cand += 0.6 * diff * (0.5 + 0.5 * self.rng.rand()) + Fdiff * scale\n\n                    # coordinate-wise local gaussian injection on some coords\n                    mask = self.rng.rand(self.dim) < 0.5\n                    if mask.any():\n                        noise = self.rng.randn(self.dim) * (0.5 * scale)\n                        cand[mask] += noise[mask]\n\n                    # PCA-guided elite perturbations sometimes\n                    if (finite_idx.size >= 5) and (self.rng.rand() < 0.35):\n                        fin_idx_list = finite_idx\n                        k2 = max(3, min(30, max(3, len(fin_idx_list) // 6 + 3)))\n                        sorted_fin = fin_idx_list[np.argsort(F[fin_idx_list])]\n                        elites = X[sorted_fin[:k2]]\n                        # center and do SVD\n                        Xc = elites - np.mean(elites, axis=0)\n                        try:\n                            U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                            # choose number of components\n                            ncomp = min(self.dim, max(1, int(np.sum(svals > (0.1 * np.max(svals))))))\n                            comps = Vt[:ncomp]\n                            # sample in PCA subspace\n                            coeffs = self.rng.randn(ncomp) * (0.3 * np.mean(np.maximum(scale, 1e-12)))\n                            pca_pert = np.zeros(self.dim)\n                            for j in range(ncomp):\n                                pca_pert += coeffs[j] * comps[j]\n                            cand += pca_pert\n                        except Exception:\n                            # fallback small gaussian\n                            cand += self.rng.randn(self.dim) * (0.05 * domain)\n\n                    # small extra jitter\n                    cand += self.rng.randn(self.dim) * (0.02 * domain)\n\n                else:  # LOCAL\n                    # anisotropic Gaussian local moves around base, coordinate mixing, occasional DE step\n                    anis_noise = self.rng.randn(self.dim) * (0.5 * gscale_vec)\n                    cand += anis_noise\n\n                    # occasional differential injection (DE-like)\n                    if self.rng.rand() < 0.12 and finite_idx.size >= 3:\n                        d = self.rng.choice(finite_idx, size=2, replace=False)\n                        cand += 0.8 * (X[int(d[0])] - X[int(d[1])]) * self.rng.rand()\n\n                    # mix some coordinates uniformly to escape ridges\n                    mask = self.rng.rand(self.dim) < 0.08\n                    if mask.any():\n                        uni = self._uniform_array(lb, ub)\n                        cand[mask] = uni[mask]\n\n                    # tempered small cauchy tiny escape occasionally\n                    if self.rng.rand() < 0.06:\n                        cand += self._tempered_cauchy(0.05 * domain, temp=0.5, max_mul=2.0)\n\n                # safety: nan-inf handling, replace non-finite coords with uniform\n                bad_coords = ~np.isfinite(cand)\n                if bad_coords.any():\n                    u = self._uniform_array(lb, ub)\n                    cand[bad_coords] = u[bad_coords]\n\n                # reflect to bounds and clamp\n                cand = self._reflect_bounds(cand, lb, ub)\n\n                # evaluate candidate\n                try:\n                    f_cand = safe_eval(cand)\n                except StopIteration:\n                    break\n\n                # add to archive\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n                # update success metrics\n                finite_vals = F[np.isfinite(F)]\n                if finite_vals.size == 0:\n                    rank_pos = np.inf\n                else:\n                    rank_pos = 1 + np.sum((finite_vals < f_cand))\n                if np.isfinite(f_cand) and (rank_pos <= max(1, int(0.25 * max(1, finite_vals.size)))):\n                    success = True\n                else:\n                    success = False\n\n                if success:\n                    success_count += 1\n                    self.short_success += 1\n                self.short_trials += 1\n\n                # short-term adaptation of gscale every few iterations\n                if self.iter_since_adapt >= self.adapt_interval:\n                    sr = (self.short_success / max(1.0, self.short_trials))\n                    # encourage smaller scales if success high, otherwise increase\n                    if sr > 0.2:\n                        self.gscale *= 0.9\n                    elif sr < 0.05:\n                        self.gscale *= 1.15\n                    # keep within domain fractions (per-dim)\n                    self.gscale = np.clip(self.gscale, 1e-4, 0.8)\n                    # reset short-term success\n                    self.short_success = 0\n                    self.short_trials = 0\n                    self.iter_since_adapt = 0\n\n                # prune archive if too large\n                if len(F_archive) > self.archive_max:\n                    F_arr = np.array(F_archive)\n                    inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                    finite_idx2 = np.where(np.isfinite(F_arr))[0]\n                    # sort finite by value\n                    sorted_fin_idx = finite_idx2[np.argsort(F_arr[finite_idx2])]\n                    keep = set()\n                    # keep best half of finite\n                    if len(sorted_fin_idx) > 0:\n                        nkeep_best = max(2, int(0.5 * len(sorted_fin_idx)))\n                        best_fin_indices = sorted_fin_idx[:nkeep_best]\n                        for ii in best_fin_indices:\n                            keep.add(int(ii))\n                    # keep some random finite others for diversity\n                    others = [int(ii) for ii in sorted_fin_idx if ii not in keep]\n                    self.rng.shuffle(others)\n                    for ii in others[:max(0, int(0.2 * self.archive_max))]:\n                        keep.add(int(ii))\n                    # add some infs for diversity (if present)\n                    for ii in inf_idx[:min(len(inf_idx), 5)]:\n                        keep.add(int(ii))\n                    # fill with random picks if still too few\n                    all_idx = list(range(len(F_archive)))\n                    self.rng.shuffle(all_idx)\n                    idx_iter = iter(all_idx)\n                    while len(keep) < self.archive_max:\n                        try:\n                            ii = next(idx_iter)\n                        except StopIteration:\n                            break\n                        keep.add(int(ii))\n                    keep_list = sorted(list(keep))[:self.archive_max]\n                    X_archive = [X_archive[i] for i in keep_list]\n                    F_archive = [F_archive[i] for i in keep_list]\n\n                # stagnation detection & micro-restarts\n                if np.isfinite(self.f_opt):\n                    # if we improved, reset; else increment\n                    if f_cand < self.f_opt - 1e-12:\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    stagnation_counter += 1\n\n                # if stagnation observed, do micro-restart to diversify\n                if stagnation_counter > max(50, int(0.05 * self.budget)):\n                    stagnation_counter = 0\n                    # generate a few local perturbations around best\n                    if self.x_opt is not None:\n                        for _ in range(min(6, max(1, self.dim // 2))):\n                            xx = self.x_opt + self.rng.randn(self.dim) * (0.05 * domain)\n                            xx = self._reflect_bounds(xx, lb, ub)\n                            try:\n                                f = safe_eval(xx)\n                            except StopIteration:\n                                break\n                            X_archive.append(xx.copy()); F_archive.append(f)\n                    # add some pure randoms to diversify\n                    for _ in range(10):\n                        if self.evals >= self.budget:\n                            break\n                        xx = self._uniform_array(lb, ub)\n                        try:\n                            f = safe_eval(xx)\n                        except StopIteration:\n                            break\n                        X_archive.append(xx.copy()); F_archive.append(f)\n\n        except StopIteration:\n            # budget exhausted, fall through to return\n            pass\n\n        # final best: choose best finite from archive or tracked best\n        if len(F_archive) > 0:\n            F_arr = np.array(F_archive)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best_local = int(finite_idx[np.argmin(F_arr[finite_idx])])\n                return float(F_arr[best_local]), np.asarray(X_archive[best_local], dtype=float)\n            else:\n                # all non-finite, return any archived x (first)\n                return np.inf, np.asarray(X_archive[0], dtype=float)\n        else:\n            # final fallback: return a random point (no function call)\n            x = self._uniform_array(lb, ub)\n            return np.inf, x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7c188856-d8e6-4d12-bf43-5f122ba6d606", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Memory-guided Adaptive Directional Sampling (MG-ADS) — an archive-driven hybrid sampler that mixes DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # initial sampling count\n        if init_samples is None:\n            self.init_samples = max(6, min(40, self.budget // 30))\n        else:\n            self.init_samples = int(init_samples)\n\n        # archive and bookkeeping\n        self.archive_max = int(archive_max)\n        self.X_archive = []  # list of np.array vectors\n        self.F_archive = []  # list of floats\n\n        # best found\n        self.x_opt = None\n        self.f_opt = np.inf\n\n        # adaptation controls\n        self.gscale = np.full(self.dim, 0.12)  # per-dim fraction of domain (will multiply by domain)\n        self.short_success = 0\n        self.short_trials = 0\n        self.adapt_interval = 50\n        self.iter_since_adapt = 0\n\n        # other counters\n        self.evals = 0\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Try to fetch bounds common patterns: func.bounds as tuple or object with lb/ub, else default [-5,5]^dim\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            if b is None:\n                pass\n            elif hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                try:\n                    lb = np.asarray(b.lb, dtype=float).reshape(-1)\n                    ub = np.asarray(b.ub, dtype=float).reshape(-1)\n                except Exception:\n                    pass\n            elif isinstance(b, (tuple, list)) and len(b) == 2:\n                try:\n                    lb = np.asarray(b[0], dtype=float).reshape(-1)\n                    ub = np.asarray(b[1], dtype=float).reshape(-1)\n                except Exception:\n                    pass\n\n        # fallback single lb/ub attributes sometimes provided\n        if lb is None and hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n            try:\n                lb = np.asarray(func.lb, dtype=float).reshape(-1)\n                ub = np.asarray(func.ub, dtype=float).reshape(-1)\n            except Exception:\n                lb = None\n\n        # default\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Normalize shapes\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # sanitize infinities or invalid ranges\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflects=8):\n        # reflect repeatedly to bring x within [lb,ub], then clamp\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_reflects):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = 2.0 * lb[below] - x[below]\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        pts = np.zeros((n, dim))\n        for j in range(dim):\n            a = cut[:n]\n            b = cut[1:n + 1]\n            u = self.rng.rand(n)\n            pts[:, j] = a + u * (b - a)\n            self.rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.rand(n, self.dim) * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # sample independent tempered Cauchy components\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        g = self.rng.randn(self.dim) * 0.6  # gaussian temper\n        tempered = c / (1.0 + np.abs(g) * temp)\n        step = tempered * np.asarray(scale_vec, dtype=float)\n        # clamp to reasonable multiples\n        max_mul = 10.0\n        cap = max_mul * np.asarray(scale_vec, dtype=float)\n        step = np.maximum(np.minimum(step, cap), -cap)\n        return step\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # prepare\n        self.evals = 0\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n\n        # safe evaluator that respects budget and handles shapes\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            x_a = np.asarray(x, dtype=float).reshape(self.dim)\n            # try passing numpy array first, if fails try list\n            try:\n                f = func(x_a)\n            except Exception:\n                try:\n                    f = func(x_a.tolist())\n                except Exception:\n                    # as last resort, return inf\n                    f = np.inf\n            # if returns array-like, try to reduce to scalar\n            if isinstance(f, (list, tuple, np.ndarray)):\n                f = np.asarray(f)\n                if f.size == 1:\n                    f = float(f.ravel()[0])\n                else:\n                    # if multi-dim, take first element\n                    f = float(np.ravel(f)[0])\n            try:\n                f = float(f)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # initial LHS samples\n        n_init = max(2, min(self.init_samples, max(2, self.budget // 20)))\n        n_init = min(n_init, max(2, self.budget))\n        pts = self._lhs01(n_init, self.dim)\n        pts = pts * (ub - lb) + lb\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = pts[i]\n            f = safe_eval(x)\n            self.X_archive.append(x.copy()); self.F_archive.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f; self.x_opt = x.copy()\n\n        # a few pure randoms for diversity\n        for _ in range(3):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.X_archive.append(x.copy()); self.F_archive.append(f)\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f; self.x_opt = x.copy()\n\n        # control variables\n        success_count = 0\n        stagnation_counter = 0\n        iter_count = 0\n\n        try:\n            while self.evals < self.budget:\n                iter_count += 1\n                self.iter_since_adapt += 1\n\n                # prepare arrays\n                if len(self.X_archive) == 0:\n                    cand = self._uniform_array(lb, ub)\n                    f_cand = safe_eval(cand)\n                    self.X_archive.append(cand.copy()); self.F_archive.append(f_cand)\n                    if np.isfinite(f_cand) and f_cand < self.f_opt:\n                        self.f_opt = f_cand; self.x_opt = cand.copy()\n                    continue\n\n                X = np.asarray(self.X_archive, dtype=float)\n                F = np.asarray(self.F_archive, dtype=float)\n                finite_mask = np.isfinite(F)\n                finite_idx = np.where(finite_mask)[0]\n                nonfinite_idx = np.where(~finite_mask)[0]\n\n                # if too few finite points => sample uniformly\n                if finite_idx.size < 3:\n                    cand = self._uniform_array(lb, ub)\n                    f_cand = safe_eval(cand)\n                    self.X_archive.append(cand.copy()); self.F_archive.append(f_cand)\n                    if np.isfinite(f_cand) and f_cand < self.f_opt:\n                        self.f_opt = f_cand; self.x_opt = cand.copy()\n                    continue\n\n                # robust per-dim scale: IQR fallback to std\n                iqr = np.subtract(*np.percentile(X[finite_idx], [75, 25], axis=0))\n                std = np.std(X[finite_idx], axis=0)\n                scale = np.where(np.isfinite(iqr) & (iqr > 1e-12), iqr, std)\n                scale = np.maximum(scale, 1e-8)\n                scale = np.minimum(scale, 0.6 * domain)  # clamp\n\n                # choose base by softmax on -F (prefer better)\n                Finf = F.copy()\n                Finf[~finite_mask] = np.nanmax(F[finite_mask]) + 10.0\n                recent = Finf[finite_mask]\n                vals = -recent\n                vals = vals - np.max(vals)\n                denom = 1.0 + np.std(vals) if np.isfinite(np.std(vals)) and np.std(vals) > 1e-12 else 1.0\n                weights = np.exp(vals / denom)\n                if np.sum(weights) <= 0 or np.any(~np.isfinite(weights)):\n                    probs = None\n                else:\n                    probs = weights / np.sum(weights)\n                if probs is None:\n                    base_idx = self.rng.choice(finite_idx)\n                else:\n                    # map back probabilities to indices in finite_idx\n                    base_idx = self.rng.choice(finite_idx, p=probs)\n                base = X[base_idx].copy()\n\n                # occasionally use global best\n                if (self.x_opt is not None) and (self.rng.rand() < 0.06):\n                    base = self.x_opt.copy()\n\n                # donor pool for DE-like operations\n                donors_k = min(6, finite_idx.size)\n                donors_idx = self.rng.choice(finite_idx, size=donors_k, replace=False)\n\n                # decide move type\n                move_type = 'LOCAL' if self.rng.rand() < 0.7 else 'GLOBAL'\n\n                # convert fractional gscale to absolute per-dim\n                gscale_vec = self.gscale * domain\n\n                # create candidate\n                cand = base.copy()\n\n                if move_type == 'GLOBAL':\n                    # anchor: elite or uniform\n                    if (self.rng.rand() < 0.75) and (finite_idx.size >= 5):\n                        # pick among top-k elites\n                        k = max(3, min(30, finite_idx.size // 5))\n                        fin_sorted = np.argsort(F[finite_idx])\n                        chosen = finite_idx[fin_sorted[self.rng.randint(0, min(k, fin_sorted.size))]]\n                        anchor = X[chosen].copy()\n                    else:\n                        anchor = self._uniform_array(lb, ub)\n                    cand = anchor.copy()\n                    # tempered Cauchy jump scaled by median scale fraction\n                    cand += self._tempered_cauchy(0.45 * scale + 1e-12, temp=1.0)\n\n                    # occasional DE-like differential injection\n                    if finite_idx.size >= 2 and self.rng.rand() < 0.45:\n                        d1, d2 = self.rng.choice(finite_idx, size=2, replace=False)\n                        diff = X[d1] - X[d2]\n                        cand += diff * (0.2 + 0.8 * self.rng.rand())  # scaled differential\n\n                    # coordinate-wise local gaussian injection\n                    mask = self.rng.rand(self.dim) < 0.5\n                    if mask.any():\n                        noise = self.rng.randn(self.dim) * (0.5 * scale)\n                        cand[mask] += noise[mask]\n\n                    # PCA-guided elite perturbations sometimes\n                    if (finite_idx.size >= 6) and (self.rng.rand() < 0.30):\n                        fin_idx_list = finite_idx\n                        k = max(4, min(30, len(fin_idx_list) // 4))\n                        fin_sorted = np.argsort(F[fin_idx_list])\n                        elite_idx = fin_idx_list[fin_sorted[:k]]\n                        elites = X[elite_idx]\n                        # center and SVD\n                        try:\n                            Xc = elites - elites.mean(axis=0)\n                            U, Svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                            # choose number components capturing a fraction of variance\n                            total = np.sum(Svals)\n                            if total <= 0:\n                                raise np.linalg.LinAlgError\n                            cum = np.cumsum(Svals) / total\n                            ncomp = int(np.searchsorted(cum, 0.6)) + 1\n                            ncomp = max(1, min(self.dim, ncomp))\n                            comps = Vt[:ncomp]\n                            coeffs = self.rng.randn(ncomp) * (0.25 * np.mean(scale))\n                            pca_pert = np.dot(coeffs, comps)\n                            cand += pca_pert\n                        except Exception:\n                            cand += self.rng.randn(self.dim) * (0.03 * domain)\n\n                    # small jitter\n                    cand += self.rng.randn(self.dim) * (0.02 * domain)\n\n                else:  # LOCAL\n                    # anisotropic Gaussian local moves around base\n                    anis_noise = self.rng.randn(self.dim) * (0.5 * gscale_vec)\n                    cand += anis_noise\n\n                    # occasional DE injection\n                    if self.rng.rand() < 0.15 and finite_idx.size >= 3:\n                        d = self.rng.choice(finite_idx, size=2, replace=False)\n                        cand += 0.5 * (X[d[0]] - X[d[1]]) * (0.5 + 0.5 * self.rng.rand())\n\n                    # mix some coordinates uniformly to escape ridges\n                    mask = self.rng.rand(self.dim) < 0.06\n                    if mask.any():\n                        uni = self._uniform_array(lb, ub)\n                        cand[mask] = uni[mask]\n\n                    # tempered small cauchy tiny escape occasionally\n                    if self.rng.rand() < 0.08:\n                        cand += self._tempered_cauchy(0.04 * domain, temp=0.5)\n\n                # nan/inf or bad coords replacement\n                bad_coords = ~np.isfinite(cand)\n                if bad_coords.any():\n                    fallback = self._uniform_array(lb, ub)\n                    cand[bad_coords] = fallback[bad_coords]\n\n                # reflect and clamp to bounds\n                cand = self._reflect_bounds(cand, lb, ub)\n\n                # evaluate candidate\n                if self.evals >= self.budget:\n                    break\n                f_cand = safe_eval(cand)\n\n                # add to archive\n                self.X_archive.append(cand.copy()); self.F_archive.append(f_cand)\n\n                # update best bookkeeping\n                improved = False\n                if np.isfinite(f_cand) and f_cand < self.f_opt:\n                    improved = True\n                    self.f_opt = f_cand\n                    self.x_opt = cand.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # success metrics: consider improvement relative to base or top quartile\n                # success if candidate is in top 30% of finite archive or improves best\n                finite_vals = np.array(self.F_archive)[np.isfinite(self.F_archive)]\n                rank_pos = None\n                if np.isfinite(f_cand):\n                    rank_pos = 1 + np.sum(finite_vals < f_cand)\n                median_rank = len(finite_vals) // 2\n                success = improved or (rank_pos is not None and rank_pos <= max(1, int(0.3 * len(finite_vals))))\n                if success:\n                    success_count += 1\n                    self.short_success += 1\n                self.short_trials += 1\n\n                # short-term adaptation\n                if self.iter_since_adapt >= self.adapt_interval:\n                    sr = (self.short_success / max(1, self.short_trials))\n                    # if success rate high -> shrink, if too low -> expand\n                    if sr > 0.25:\n                        self.gscale *= 0.88\n                    elif sr < 0.06:\n                        self.gscale *= 1.14\n                    # clip gscale\n                    self.gscale = np.clip(self.gscale, 1e-4, 0.8)\n                    self.short_success = 0\n                    self.short_trials = 0\n                    self.iter_since_adapt = 0\n\n                # prune archive if too large\n                if len(self.F_archive) > self.archive_max:\n                    F_arr = np.array(self.F_archive)\n                    finite_idx2 = np.where(np.isfinite(F_arr))[0]\n                    inf_idx = np.where(~np.isfinite(F_arr))[0].tolist()\n                    keep = set()\n                    # keep best half of finite\n                    if len(finite_idx2) > 0:\n                        sorted_fin = np.argsort(F_arr[finite_idx2])\n                        nkeep_best = max(2, int(0.5 * len(sorted_fin)))\n                        best_fin_indices = finite_idx2[sorted_fin[:nkeep_best]]\n                        for ii in best_fin_indices:\n                            keep.add(int(ii))\n                    # keep some random other finite\n                    others = [int(ii) for ii in finite_idx2 if ii not in keep]\n                    self.rng.shuffle(others)\n                    for ii in others[:max(0, int(0.15 * self.archive_max))]:\n                        keep.add(int(ii))\n                    # keep a few inf for diversity\n                    for ii in inf_idx[:min(len(inf_idx), 5)]:\n                        keep.add(int(ii))\n                    # fill with some random picks to reach archive_max or at least some number\n                    all_idx = list(range(len(self.F_archive)))\n                    self.rng.shuffle(all_idx)\n                    for ii in all_idx:\n                        if len(keep) >= int(0.6 * self.archive_max):\n                            break\n                        keep.add(int(ii))\n                    keep_list = sorted(list(keep))\n                    self.X_archive = [self.X_archive[i] for i in keep_list]\n                    self.F_archive = [self.F_archive[i] for i in keep_list]\n\n                # stagnation detection & micro-restarts\n                if stagnation_counter > max(40, int(0.06 * self.budget)):\n                    stagnation_counter = 0\n                    # local perturbations around best\n                    if self.x_opt is not None and np.isfinite(self.f_opt):\n                        for _ in range(6):\n                            xx = self.x_opt + self.rng.randn(self.dim) * (0.06 * domain)\n                            xx = self._reflect_bounds(xx, lb, ub)\n                            if self.evals >= self.budget:\n                                break\n                            f = safe_eval(xx)\n                            self.X_archive.append(xx.copy()); self.F_archive.append(f)\n                            if np.isfinite(f) and f < self.f_opt:\n                                self.f_opt = f; self.x_opt = xx.copy()\n                    # plus randoms\n                    for _ in range(8):\n                        if self.evals >= self.budget:\n                            break\n                        xx = self._uniform_array(lb, ub)\n                        f = safe_eval(xx)\n                        self.X_archive.append(xx.copy()); self.F_archive.append(f)\n                        if np.isfinite(f) and f < self.f_opt:\n                            self.f_opt = f; self.x_opt = xx.copy()\n\n        except StopIteration:\n            # budget exhausted\n            pass\n\n        # final best: choose best finite from archive\n        if len(self.F_archive) == 0:\n            # fallback random\n            x = self._uniform_array(lb, ub)\n            return np.inf, x\n\n        F_arr = np.array(self.F_archive)\n        finite_idx = np.where(np.isfinite(F_arr))[0]\n        if finite_idx.size > 0:\n            best_i = finite_idx[np.argmin(F_arr[finite_idx])]\n            return float(F_arr[best_i]), np.asarray(self.X_archive[best_i], dtype=float)\n        else:\n            # no finite values, return random archive point\n            idx = self.rng.randint(0, len(self.X_archive))\n            return np.inf, np.asarray(self.X_archive[idx], dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "c48b853b-8ed3-43ac-b2a1-c43cee2d1683", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n\n        self.rng = np.random.RandomState(self.seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # evaluation counter\n        self.evals = 0\n\n        # short-term adaptation variables (fractions of domain)\n        # initialize to moderately large (will be scaled by domain)\n        self.gscale = np.ones(self.dim) * 0.2\n        self.short_success = 0\n        self.short_trials = 0\n        self.adapt_interval = 50  # adapt every this many iterations\n        self.iter_since_adapt = 0\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Try a few common attribute names, else default to [-5,5]^dim\n        lb = None\n        ub = None\n        # common wrappers might provide bounds as attributes\n        for attr in (\"bounds\", \"search_space\", \"domain\"):\n            if hasattr(func, attr):\n                b = getattr(func, attr)\n                # common patterns:\n                # - object with lb/ub attributes\n                try:\n                    if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                        lb = np.asarray(b.lb, dtype=float)\n                        ub = np.asarray(b.ub, dtype=float)\n                        break\n                except Exception:\n                    pass\n                # - tuple/list (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    try:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n                        break\n                    except Exception:\n                        pass\n        # fallback to direct attributes on func\n        if lb is None and (hasattr(func, \"lb\") and hasattr(func, \"ub\")):\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                lb = None\n\n        # final fallback to [-5,5]^dim\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # reshape/scalar handling\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # ensure shapes\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n        # numeric sanity: finite and ub > lb\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]):\n                ub[i] = 5.0\n            if ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(10):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = 2*lb - x\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above: x = 2*ub - x\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Latin Hypercube sampling in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        pts = np.empty((n, dim), dtype=float)\n        for j in range(dim):\n            # random in each interval\n            a = cut[:-1]\n            b = cut[1:]\n            u = self.rng.rand(n)\n            col = a + (b - a) * u\n            self.rng.shuffle(col)\n            pts[:, j] = col\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # scale_vec can be scalar or vector of length dim (absolute scale)\n        scale = np.asarray(scale_vec, dtype=float)\n        if scale.size == 1:\n            scale = np.full(self.dim, float(scale), dtype=float)\n        # standard cauchy via tan(pi*(u-0.5))\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper with gaussian noise so extremely large jumps are damped\n        g = self.rng.randn(self.dim) * 0.5\n        tempered = c / (1.0 + np.abs(g) * float(temp))\n        step = tempered * scale\n        # clamp to reasonable multiples of scale\n        max_mul = 10.0\n        cap = np.maximum(np.abs(scale) * max_mul, 1e-12)\n        step = np.maximum(np.minimum(step, cap), -cap)\n        return step\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n        # interpret gscale as fraction of domain\n        self.gscale = np.maximum(self.gscale, 1e-8)  # avoid zeros\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            x_a = np.asarray(x, dtype=float).reshape(self.dim)\n            try:\n                f = func(x_a)\n            except Exception:\n                try:\n                    f = func(list(x_a))\n                except Exception:\n                    f = np.inf\n            # try to coerce to scalar\n            try:\n                f = float(np.asarray(f).ravel()[0])\n            except Exception:\n                f = np.inf\n            # update counters and best bookkeeping\n            self.evals += 1\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_a.copy()\n            return float(f)\n\n        # Archive\n        X_archive = []\n        F_archive = []\n\n        # initial LHS samples (mapped to domain)\n        n_init = max(2, min(self.init_samples, max(2, self.budget // 20)))\n        n_init = min(n_init, self.budget)\n        pts = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            x = lb + pts[i] * domain\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        for _ in range(3):\n            if self.evals >= self.budget: break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # control variables\n        success_count = 0\n        stagnation_counter = 0\n        stagnation_limit = max(20, 5 * self.dim)  # allow more time in higher dims\n\n        try:\n            while self.evals < self.budget:\n                self.iter_since_adapt += 1\n\n                # prepare archived arrays\n                if len(X_archive) == 0:\n                    # fallback: sample uniformly\n                    cand = self._uniform_array(lb, ub)\n                    try:\n                        f_cand = safe_eval(cand)\n                    except StopIteration:\n                        break\n                    X_archive.append(cand.copy()); F_archive.append(f_cand)\n                    continue\n\n                X = np.array(X_archive, dtype=float)\n                F = np.array(F_archive, dtype=float)\n                finite_mask = np.isfinite(F)\n                finite_idx = np.where(finite_mask)[0]\n                n_finite = finite_idx.size\n\n                # if too few finite points => sample more broadly\n                if n_finite < 2:\n                    cand = self._uniform_array(lb, ub)\n                    try:\n                        f_cand = safe_eval(cand)\n                    except StopIteration:\n                        break\n                    X_archive.append(cand.copy()); F_archive.append(f_cand)\n                    continue\n\n                # compute per-dimension robust scale (IQR fallback to std)\n                try:\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    scale = (q75 - q25) / 1.349  # approx std from IQR\n                    # fallback to std if tiny or zero\n                    sstd = np.std(X[finite_idx], axis=0)\n                    mask_small = (~np.isfinite(scale)) | (scale < 1e-9)\n                    scale[mask_small] = sstd[mask_small]\n                except Exception:\n                    scale = np.std(X[finite_idx], axis=0)\n                # clamp scale to reasonable portion of domain\n                scale = np.maximum(scale, 1e-8)\n                scale = np.minimum(scale, 0.5 * domain)\n\n                # choose a base index biased toward better F via softmax on -F\n                Finf = F.copy()\n                if n_finite > 0:\n                    worst_f = np.nanmax(F[finite_mask])\n                else:\n                    worst_f = 1.0\n                Finf[~finite_mask] = worst_f + 10.0\n                recent = Finf[finite_mask]\n                vals = -recent\n                vals = vals - np.max(vals)\n                denom = 1.0 + np.std(vals) if np.isfinite(np.std(vals)) and np.std(vals) > 1e-12 else 1.0\n                weights = np.exp(vals / denom)\n                weights = weights / np.sum(weights)\n                base_idx = np.random.choice(finite_idx, p=weights)\n                base = X[base_idx].copy()\n\n                # small chance to use global best directly\n                if (self.x_opt is not None) and (self.rng.rand() < 0.05):\n                    base = self.x_opt.copy()\n\n                # donor pool selection (for DE-like moves)\n                k_donors = min(6, max(2, n_finite))\n                donors_idx = self.rng.choice(finite_idx, size=k_donors, replace=False)\n\n                # choose move type\n                r = self.rng.rand()\n                move_type = 'LOCAL' if r < 0.6 else 'GLOBAL'\n\n                cand = base.copy()\n\n                # scale modifiers\n                global_scale = np.median(scale)\n\n                if move_type == 'GLOBAL':\n                    # anchor on either elite or uniform\n                    use_elite_anchor = (self.rng.rand() < 0.7) and (n_finite > 0)\n                    if use_elite_anchor:\n                        # pick among top-k elites (k relative to finite size)\n                        k_elite = max(1, min(10, int(max(1, 0.2 * n_finite))))\n                        sorted_fin = finite_idx[np.argsort(F[finite_idx])]\n                        elite_idx = sorted_fin[:k_elite]\n                        # choose one elite as anchor\n                        chosen = self.rng.choice(elite_idx)\n                        anchor = X[chosen].copy()\n                    else:\n                        # uniform anchor\n                        anchor = self._uniform_array(lb, ub)\n\n                    cand = anchor.copy()\n\n                    # tempered cauchy jump around anchor\n                    cauch_scale = (0.5 * domain)  # absolute scale per-dim\n                    cand += self._tempered_cauchy(cauch_scale, temp=1.0) * (0.6 * self.rng.rand())\n\n                    # occasional DE-like differential injection\n                    if n_finite >= 2 and self.rng.rand() < 0.45:\n                        d1, d2 = self.rng.choice(finite_idx, size=2, replace=False)\n                        diff = X[d1] - X[d2]\n                        Fdiff = self.rng.randn(self.dim) * 0.2\n                        cand += 0.6 * diff * (0.5 + 0.5 * self.rng.rand()) + Fdiff * scale\n\n                    # coordinate-wise local gaussian injection on some coords\n                    mask = self.rng.rand(self.dim) < 0.3\n                    if mask.any():\n                        gauss = self.rng.randn(self.dim) * (0.5 * scale)\n                        cand[mask] += gauss[mask]\n\n                    # PCA-guided elite perturbations sometimes\n                    if (n_finite >= 5) and (self.rng.rand() < 0.35):\n                        # pick elites on actual F\n                        sorted_fin = finite_idx[np.argsort(F[finite_idx])]\n                        k = max(3, min(10, int(0.15 * n_finite)))\n                        elites = X[sorted_fin[:k]]\n                        # center and do SVD\n                        try:\n                            Xc = elites - np.mean(elites, axis=0)\n                            U, Svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                            if Svals.size == 0:\n                                raise np.linalg.LinAlgError\n                            ncomp = max(1, min(self.dim, int(1 + np.sum(Svals > (0.1 * Svals[0])))))\n                            coeffs = self.rng.randn(ncomp) * (0.3 * np.mean(scale))\n                            pca_pert = np.dot(coeffs, Vt[:ncomp, :])\n                            cand += pca_pert\n                        except Exception:\n                            # fallback small gaussian\n                            cand += self.rng.randn(self.dim) * (0.05 * domain)\n\n                    # small extra jitter\n                    cand += self.rng.randn(self.dim) * (0.02 * domain)\n\n                else:  # LOCAL\n                    # anisotropic Gaussian local moves around base, coordinate mixing, occasional DE step\n                    anis_noise = self.rng.randn(self.dim) * (0.7 * self.gscale * domain)\n                    cand += anis_noise\n\n                    # occasional differential injection (DE-like)\n                    if self.rng.rand() < 0.12 and n_finite >= 3:\n                        d = self.rng.choice(finite_idx, size=2, replace=False)\n                        cand += 0.8 * (X[d[0]] - X[d[1]]) * self.rng.rand()\n\n                    # mix some coordinates uniformly to escape ridges\n                    mask = self.rng.rand(self.dim) < 0.08\n                    if mask.any():\n                        uni = self._uniform_array(lb, ub)\n                        cand[mask] = uni[mask]\n\n                    # tempered small cauchy tiny escape occasionally\n                    if self.rng.rand() < 0.08:\n                        cand += self._tempered_cauchy(0.05 * domain, temp=0.5)\n\n                # safety: nan-inf handling, replace non-finite coords with uniform\n                bad_coords = ~np.isfinite(cand)\n                if bad_coords.any():\n                    uni = self._uniform_array(lb, ub)\n                    cand[bad_coords] = uni[bad_coords]\n\n                # reflect to bounds and clamp\n                cand = self._reflect_bounds(cand, lb, ub)\n\n                # evaluate candidate\n                try:\n                    f_cand = safe_eval(cand)\n                except StopIteration:\n                    break\n\n                # add to archive\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n                # update success metrics\n                finite_vals = F[np.isfinite(F)] if F.size > 0 else np.array([])\n                # rank position of new candidate among finite (1-based best)\n                if finite_vals.size > 0:\n                    # compute rank among extended set including new\n                    combined = np.r_[finite_vals, f_cand]\n                    rank_pos = int(np.argsort(np.argsort(combined))[-1])  # 0-based rank\n                    # success if candidate is among top 25% (lower is better)\n                    rank_threshold = max(0, int(0.25 * max(1, finite_vals.size)))\n                    success = (rank_pos <= rank_threshold)\n                else:\n                    success = np.isfinite(f_cand)\n\n                # update short-term trackers\n                self.short_trials += 1\n                if success:\n                    self.short_success += 1\n                    success_count += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # short-term adaptation of gscale every few iterations\n                if self.iter_since_adapt >= self.adapt_interval:\n                    sr = (self.short_success / max(1, self.short_trials))\n                    # encourage smaller scales if success high, otherwise increase\n                    if sr > 0.25:\n                        self.gscale *= 0.9\n                    elif sr < 0.05:\n                        self.gscale *= 1.12\n                    else:\n                        # slight random walk\n                        self.gscale *= (1.0 + (self.rng.rand(self.dim) - 0.5) * 0.06)\n                    # keep within domain fractions (per-dim)\n                    self.gscale = np.clip(self.gscale, 1e-4, 0.8)\n                    # reset short-term counters\n                    self.short_success = 0\n                    self.short_trials = 0\n                    self.iter_since_adapt = 0\n\n                # prune archive if too large\n                if len(F_archive) > self.archive_max:\n                    F_arr = np.array(F_archive)\n                    finite_idx2 = np.where(np.isfinite(F_arr))[0].tolist()\n                    inf_idx = [int(i) for i in range(len(F_arr)) if not np.isfinite(F_arr[i])]\n                    keep = set()\n                    # keep best half of finite (or up to archive_max//2)\n                    if len(finite_idx2) > 0:\n                        sorted_fin = np.array(finite_idx2)[np.argsort(F_arr[finite_idx2])]\n                        nkeep_best = max(1, min(len(sorted_fin), self.archive_max // 2))\n                        for ii in sorted_fin[:nkeep_best]:\n                            keep.add(int(ii))\n                    # keep some random finite others\n                    others = [int(ii) for ii in finite_idx2 if ii not in keep]\n                    self.rng.shuffle(others)\n                    for ii in others[:max(0, int(0.2 * self.archive_max))]:\n                        keep.add(int(ii))\n                    # add some infs for diversity\n                    self.rng.shuffle(inf_idx)\n                    for ii in inf_idx[:max(0, int(0.05 * self.archive_max))]:\n                        keep.add(int(ii))\n                    # fill with random picks from remaining until archive_max\n                    all_idx = list(range(len(F_arr)))\n                    rem = [ii for ii in all_idx if ii not in keep]\n                    self.rng.shuffle(rem)\n                    for ii in rem:\n                        if len(keep) >= self.archive_max:\n                            break\n                        keep.add(int(ii))\n                    keep_list = sorted(list(keep))[:self.archive_max]\n                    X_archive = [X_archive[i] for i in keep_list]\n                    F_archive = [F_archive[i] for i in keep_list]\n\n                # stagnation detection & micro-restarts\n                if np.isfinite(self.f_opt):\n                    # consider any strict improvement resetting stagnation\n                    if f_cand < self.f_opt - 1e-12:\n                        stagnation_counter = 0\n                    # else already incremented above\n                # if stagnation observed, do micro-restart to diversify\n                if stagnation_counter >= stagnation_limit:\n                    stagnation_counter = 0\n                    # generate a few local perturbations around best\n                    if self.x_opt is not None:\n                        for _ in range(min(8, max(1, self.dim // 2))):\n                            xx = self.x_opt + self.rng.randn(self.dim) * (0.03 * domain)\n                            xx = self._reflect_bounds(xx, lb, ub)\n                            try:\n                                ff = safe_eval(xx)\n                            except StopIteration:\n                                break\n                            X_archive.append(xx.copy()); F_archive.append(ff)\n                    # add some pure randoms to diversify\n                    for _ in range(10):\n                        if self.evals >= self.budget:\n                            break\n                        xx = self._uniform_array(lb, ub)\n                        try:\n                            fxx = safe_eval(xx)\n                        except StopIteration:\n                            break\n                        X_archive.append(xx.copy()); F_archive.append(fxx)\n\n        except StopIteration:\n            # budget exhausted, fall through to return\n            pass\n\n        # final best: choose best finite from archive\n        if len(F_archive) > 0:\n            F_arr = np.array(F_archive)\n            finite_idx = np.where(np.isfinite(F_arr))[0]\n            if finite_idx.size > 0:\n                best_i = finite_idx[np.argmin(F_arr[finite_idx])]\n                return float(F_arr[best_i]), np.asarray(X_archive[best_i], dtype=float)\n            else:\n                # all non-finite, return first archived x\n                return np.inf, np.asarray(X_archive[0], dtype=float)\n        else:\n            # final fallback: return a random point (no function call)\n            x = self._uniform_array(lb, ub)\n            return np.inf, x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6750e442-8d2d-4252-85ab-fb1df9cc7cac", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=12, archive_max=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n\n        self.rng = np.random.RandomState(self.seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # evaluation counter\n        self.evals = 0\n\n        # short-term adaptation variables (fractions of domain)\n        self.gscale = np.full(self.dim, 0.2)  # relative to domain (fraction)\n        self.short_success = 0\n        self.short_trials = 0\n        self.adapt_interval = 50\n        self.iter_since_adapt = 0\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Try several common patterns; final fallback [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\"):\n                b = getattr(func, \"bounds\")\n                # maybe tuple (lb, ub) or object with lb/ub\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                elif hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n            if lb is None and hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            if lb is None and hasattr(func, \"lower_bounds\") and hasattr(func, \"upper_bounds\"):\n                lb = np.asarray(getattr(func, \"lower_bounds\"), dtype=float)\n                ub = np.asarray(getattr(func, \"upper_bounds\"), dtype=float)\n        except Exception:\n            lb = None\n\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n\n        # handle scalar bounds\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # resize if needed\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        # ensure finite and ub > lb\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]) or ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1.0\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.asarray(x, dtype=float).copy()\n        below = x < lb\n        above = x > ub\n        if below.any():\n            x[below] = 2.0 * lb[below] - x[below]\n        if above.any():\n            x[above] = 2.0 * ub[above] - x[above]\n        # clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple LHS in [0,1)\n        pts = np.empty((n, dim), dtype=float)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        for j in range(dim):\n            a = cut[:-1]\n            b = cut[1:]\n            u = self.rng.rand(n)\n            col = a + (b - a) * u\n            self.rng.shuffle(col)\n            pts[:, j] = col\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0):\n        # tempered coordinate-wise Cauchy: sample cauchy and damp by gaussian factor\n        scale = np.asarray(scale_vec, dtype=float)\n        # avoid zero-scale\n        scale = np.maximum(scale, 1e-12)\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        g = self.rng.randn(self.dim) * 0.6\n        tempered = c / (1.0 + np.abs(g) * float(temp))\n        max_mul = 10.0\n        cap = np.maximum(np.abs(scale) * max_mul, 1e-12)\n        jump = np.sign(tempered) * np.minimum(np.abs(tempered) * scale, cap)\n        return jump\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                raise StopIteration()\n            x_a = np.asarray(x, dtype=float).reshape(self.dim)\n            try:\n                f = func(x_a.copy())\n            except Exception:\n                f = np.inf\n            # coerce to scalar\n            try:\n                f = float(np.asarray(f).ravel()[0])\n            except Exception:\n                f = np.inf\n            # update counters and best bookkeeping\n            self.evals += 1\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_a.copy()\n            return float(f)\n\n        X_archive = []\n        F_archive = []\n\n        # initial LHS samples\n        n_init = max(2, min(self.init_samples, max(2, self.budget // 20)))\n        n_init = min(n_init, self.budget)\n        pts = self._lhs01(n_init, self.dim)\n        try:\n            for i in range(n_init):\n                x = lb + pts[i] * domain\n                f = safe_eval(x)\n                X_archive.append(x.copy()); F_archive.append(f)\n        except StopIteration:\n            # budget exhausted during initialization\n            if len(F_archive) == 0:\n                return self.f_opt, self.x_opt\n            # return best so far\n            best_idx = int(np.argmin([f if np.isfinite(f) else np.inf for f in F_archive]))\n            return float(F_archive[best_idx]), np.asarray(X_archive[best_idx], dtype=float)\n\n        # add a couple purely uniform random points if budget remains\n        try:\n            if self.evals < self.budget:\n                for _ in range(min(3, self.budget - self.evals)):\n                    x = self._uniform_array(lb, ub)\n                    f = safe_eval(x)\n                    X_archive.append(x.copy()); F_archive.append(f)\n        except StopIteration:\n            pass\n\n        success_count = 0\n        stagnation_counter = 0\n        stagnation_limit = max(20, 5 * self.dim)\n\n        try:\n            while self.evals < self.budget:\n                self.iter_since_adapt += 1\n\n                # prepare arrays\n                X = np.array(X_archive, dtype=float) if len(X_archive) > 0 else np.zeros((0, self.dim))\n                F = np.array(F_archive, dtype=float) if len(F_archive) > 0 else np.array([])\n\n                finite_mask = np.isfinite(F)\n                finite_idx = np.where(finite_mask)[0].tolist()\n                n_finite = len(finite_idx)\n\n                # ensure we have at least some finite points (else sample uniform)\n                if n_finite < 2:\n                    cand = self._uniform_array(lb, ub)\n                    f_cand = safe_eval(cand)\n                    X_archive.append(cand.copy()); F_archive.append(f_cand)\n                    continue\n\n                # compute per-dim robust scale: IQR fallback to std\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    scale = (q75 - q25) / 1.349\n                    small = scale < 1e-8\n                    if small.any():\n                        sstd = np.std(X[finite_idx], axis=0)\n                        scale[small] = sstd[small]\n                    # final fallback if still tiny\n                    scale = np.maximum(scale, 1e-8)\n                except Exception:\n                    scale = np.std(X[finite_idx], axis=0)\n                    scale = np.maximum(scale, 1e-8)\n\n                # clamp scale to portion of domain\n                scale = np.minimum(scale, 0.5 * domain)\n\n                # choose a base index biased toward better F via softmax on -F\n                vals = -F.copy()\n                vals[~finite_mask] = np.min(vals[finite_mask]) if n_finite > 0 else 0.0\n                # stabilize\n                vals = vals - np.max(vals)\n                weights = np.exp(vals / (1.0 + np.std(vals)))\n                if np.sum(weights) == 0:\n                    weights = np.ones_like(weights)\n                weights = weights / np.sum(weights)\n                base_idx = self.rng.choice(len(weights), p=weights)\n                base = X[base_idx].copy()\n\n                # small chance to use global best directly\n                if (self.x_opt is not None) and (self.rng.rand() < 0.05):\n                    base = self.x_opt.copy()\n\n                # donor pool selection (for DE-like moves)\n                k_donors = min(6, max(2, n_finite))\n                donors_idx = self.rng.choice(finite_idx, size=k_donors, replace=False)\n\n                # choose move type\n                r = self.rng.rand()\n                move_type = 'LOCAL' if r < 0.6 else 'GLOBAL'\n\n                cand = base.copy()\n\n                # absolute scales\n                per_dim_scale = self.gscale * domain\n                global_scale = np.median(per_dim_scale)\n\n                if move_type == 'GLOBAL':\n                    # anchor: either elite or a uniform sample\n                    if self.rng.rand() < 0.7 and n_finite >= 5:\n                        k_elite = max(2, int(0.2 * n_finite))\n                        elite_idx = np.array(finite_idx)[np.argsort(F[finite_idx])][:k_elite]\n                        anchor = X[self.rng.choice(elite_idx)].copy()\n                    else:\n                        anchor = self._uniform_array(lb, ub)\n\n                    cand = anchor.copy()\n\n                    # tempered cauchy jump around anchor\n                    cauch_scale = np.maximum(0.4 * domain, per_dim_scale)\n                    cand += self._tempered_cauchy(cauch_scale, temp=1.0)\n\n                    # occasional DE-like injection\n                    if n_finite >= 3 and self.rng.rand() < 0.35:\n                        d_idx = self.rng.choice(finite_idx, size=2, replace=False)\n                        diff = X[d_idx[0]] - X[d_idx[1]]\n                        cand += 0.8 * diff * (0.5 + self.rng.rand() * 0.8)\n\n                    # small gaussian jitter\n                    cand += self.rng.randn(self.dim) * (0.05 * domain)\n\n                    # mix some coords with uniform to escape ridges\n                    mask = self.rng.rand(self.dim) < 0.15\n                    if mask.any():\n                        uni = self._uniform_array(lb, ub)\n                        cand[mask] = uni[mask]\n\n                else:  # LOCAL\n                    # anisotropic gaussian local noise\n                    anis = self.rng.randn(self.dim) * (0.6 * per_dim_scale)\n                    cand += anis\n\n                    # coordinate-wise local gaussian injection on some coords\n                    mask = self.rng.rand(self.dim) < 0.35\n                    if mask.any():\n                        cand[mask] += self.rng.randn(np.count_nonzero(mask)) * (0.5 * per_dim_scale[mask])\n\n                    # PCA-guided elite perturbations sometimes\n                    if n_finite >= 5 and self.rng.rand() < 0.35:\n                        sorted_fin = np.array(finite_idx)[np.argsort(F[finite_idx])]\n                        k_el = max(3, min(10, int(0.15 * n_finite)))\n                        elites = X[sorted_fin[:k_el]]\n                        try:\n                            Xc = elites - np.mean(elites, axis=0)\n                            U, Svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                            if Svals.size > 0:\n                                # choose number of components covering significant variance\n                                cum = np.cumsum(Svals) / np.sum(Svals)\n                                ncomp = max(1, int(np.searchsorted(cum, 0.7) + 1))\n                                coeffs = self.rng.randn(ncomp) * (0.25 * np.mean(per_dim_scale))\n                                perturb = np.dot(coeffs, Vt[:ncomp, :])\n                                cand += perturb\n                        except Exception:\n                            cand += self.rng.randn(self.dim) * (0.03 * domain)\n\n                    # occasional DE-style differential injection\n                    if n_finite >= 3 and self.rng.rand() < 0.18:\n                        d = self.rng.choice(finite_idx, size=2, replace=False)\n                        cand += 0.7 * (X[d[0]] - X[d[1]]) * self.rng.rand()\n\n                    # tiny tempered cauchy escape occasionally\n                    if self.rng.rand() < 0.07:\n                        cand += self._tempered_cauchy(0.05 * domain, temp=0.8)\n\n                # safety: replace NaN/inf coords with uniform random within bounds\n                bad_coords = ~np.isfinite(cand)\n                if bad_coords.any():\n                    uni = self._uniform_array(lb, ub)\n                    cand[bad_coords] = uni[bad_coords]\n\n                # reflect to bounds & clamp\n                cand = self._reflect_bounds(cand, lb, ub)\n\n                # evaluate candidate\n                f_cand = safe_eval(cand)\n                X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n                # update success metrics (candidate is \"success\" if among top 25% of finite)\n                finite_vals = F[np.isfinite(F)] if F.size > 0 else np.array([])\n                combined_vals = np.r_[finite_vals, f_cand]\n                # rank position (1 is best)\n                rank_pos = int(1 + np.sum(combined_vals[:-1] < combined_vals[-1]))\n                rank_threshold = max(1, int(0.25 * combined_vals.size))\n                success = rank_pos <= rank_threshold\n                self.short_trials += 1\n                if success:\n                    self.short_success += 1\n                    success_count += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # short-term adaptation of gscale:\n                if self.iter_since_adapt >= self.adapt_interval:\n                    sr = (self.short_success / max(1, self.short_trials))\n                    if sr > 0.25:\n                        self.gscale *= 0.9\n                    elif sr < 0.08:\n                        self.gscale *= 1.12\n                    # small random walk\n                    self.gscale *= (1.0 + (self.rng.rand(self.dim) - 0.5) * 0.06)\n                    # keep within domain fractions\n                    self.gscale = np.clip(self.gscale, 1e-4, 0.8)\n                    # reset counters\n                    self.short_success = 0\n                    self.short_trials = 0\n                    self.iter_since_adapt = 0\n\n                # prune archive if too large\n                if len(F_archive) > self.archive_max:\n                    F_arr = np.array(F_archive)\n                    finite_idx2 = np.where(np.isfinite(F_arr))[0].tolist()\n                    keep = set()\n                    # keep best half of finite\n                    if len(finite_idx2) > 0:\n                        sorted_fin = np.array(finite_idx2)[np.argsort(F_arr[finite_idx2])]\n                        nkeep_best = max(1, min(len(sorted_fin), self.archive_max // 2))\n                        for ii in sorted_fin[:nkeep_best]:\n                            keep.add(int(ii))\n                    # keep some random finite others\n                    others = [ii for ii in finite_idx2 if ii not in keep]\n                    self.rng.shuffle(others)\n                    for ii in others[:max(0, (self.archive_max // 4))]:\n                        keep.add(int(ii))\n                    # add some infs for diversity if space\n                    inf_idx = [int(i) for i in range(len(F_arr)) if not np.isfinite(F_arr[i])]\n                    self.rng.shuffle(inf_idx)\n                    for ii in inf_idx[:max(0, (self.archive_max - len(keep)) // 6)]:\n                        keep.add(int(ii))\n                    # fill with random remaining until archive_max\n                    rem = [i for i in range(len(F_arr)) if i not in keep]\n                    self.rng.shuffle(rem)\n                    for ii in rem:\n                        if len(keep) >= self.archive_max:\n                            break\n                        keep.add(int(ii))\n                    keep_list = sorted(list(keep))[:self.archive_max]\n                    F_archive = [F_archive[i] for i in keep_list]\n                    X_archive = [X_archive[i] for i in keep_list]\n\n                # stagnation detection & micro-restarts\n                if np.isfinite(self.f_opt):\n                    # reset stagnation counter whenever we strictly improved\n                    # (done above when success & update best)\n                    pass\n\n                if stagnation_counter >= stagnation_limit:\n                    # micro-restart: spread around best and random samples\n                    n_local = min(8, max(1, self.dim // 2))\n                    for _ in range(n_local):\n                        if self.x_opt is None:\n                            break\n                        xx = self.x_opt + self.rng.randn(self.dim) * (0.03 * domain)\n                        xx = self._reflect_bounds(xx, lb, ub)\n                        try:\n                            fxx = safe_eval(xx)\n                        except StopIteration:\n                            break\n                        X_archive.append(xx.copy()); F_archive.append(fxx)\n                    # add some randoms\n                    for _ in range(3):\n                        if self.evals >= self.budget:\n                            break\n                        xx = self._uniform_array(lb, ub)\n                        try:\n                            fxx = safe_eval(xx)\n                        except StopIteration:\n                            break\n                        X_archive.append(xx.copy()); F_archive.append(fxx)\n                    stagnation_counter = 0\n                    # slightly increase exploration scales\n                    self.gscale = np.minimum(0.6, self.gscale * (1.0 + 0.2 * self.rng.rand(self.dim)))\n\n        except StopIteration:\n            # budget exhausted, fall through to return\n            pass\n\n        # final best: choose best finite from archive\n        if len(F_archive) == 0:\n            # fallback random\n            x = self._uniform_array(lb, ub)\n            return np.inf, x\n        F_arr = np.array(F_archive)\n        finite_idx_final = np.where(np.isfinite(F_arr))[0]\n        if finite_idx_final.size > 0:\n            best_i = int(finite_idx_final[np.argmin(F_arr[finite_idx_final])])\n            return float(F_arr[best_i]), np.asarray(X_archive[best_i], dtype=float)\n        else:\n            return np.inf, np.asarray(X_archive[0], dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ad454e79-558f-4740-af3b-a793ad47d318", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # initial sample count (reasonable default)\n        if init_samples is None:\n            self.init_samples = min(max(12, 2 * self.dim), max(1, self.budget // 10))\n        else:\n            self.init_samples = int(init_samples)\n        self.archive_max = int(archive_max)\n\n        # state\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # short-term adaptation\n        self.short_success = 0\n        self.short_trials = 0\n        self.gscale = 0.2  # fraction of domain for global anisotropic moves\n        self.stagnation = 0\n\n        # bookkeeping for recent decisions (for adaptation)\n        self.recent_results = []\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bounds from func if available; otherwise default [-5,5]^dim\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # reshape/expand scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n        # ensure validity\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]) or ub[i] <= lb[i]:\n                ub[i] = lb[i] + 1.0\n        return lb, ub\n\n    def _reflect_and_clamp(self, x, lb, ub):\n        # single-step reflection for out-of-bounds, then clamp\n        below = x < lb\n        above = x > ub\n        x[below] = 2.0 * lb[below] - x[below]\n        x[above] = 2.0 * ub[above] - x[above]\n        # clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple LHS in [0,1)\n        pts = np.empty((n, dim), dtype=float)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        for j in range(dim):\n            a = cut[:-1]\n            b = cut[1:]\n            u = self.rng.rand(n)\n            pts[:, j] = a + (b - a) * u\n            self.rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_in_box(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, temp=1.0, max_mul=10.0):\n        # coordinate-wise tempered Cauchy: heavy tails damped by Gaussian factor\n        scale = np.asarray(scale_vec, dtype=float)\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        g = self.rng.randn(self.dim)\n        tempered = c / (1.0 + np.abs(g) * float(temp))\n        jump = tempered * scale\n        # limit extreme multipliers\n        clip_mul = np.minimum(np.maximum(jump / (scale + 1e-12), -max_mul), max_mul)\n        jump = clip_mul * scale\n        return jump\n\n    # ----------------- main call -----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n        # safety evaluator\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x_arr = np.asarray(x, dtype=float).reshape(self.dim)\n            try:\n                f = func(x_arr)\n            except Exception:\n                f = np.inf\n            # coerce to scalar\n            try:\n                f = float(np.asarray(f).ravel()[0])\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_arr.copy()\n            return f\n\n        # archives\n        X_archive = []\n        F_archive = []\n\n        # initial LHS samples\n        n_init = min(self.init_samples, max(1, self.budget))\n        pts = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = lb + pts[i] * domain\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # add a couple purely uniform random points if budget remains\n        while self.evals < self.budget and len(X_archive) < n_init + 3:\n            x = self._uniform_in_box(lb, ub)\n            f = safe_eval(x)\n            X_archive.append(x.copy()); F_archive.append(f)\n\n        # ensure we have some best\n        if self.x_opt is None and len(F_archive) > 0:\n            finite_idx = [i for i,f in enumerate(F_archive) if np.isfinite(f)]\n            if len(finite_idx) > 0:\n                best_i = int(finite_idx[np.argmin([F_archive[i] for i in finite_idx])])\n                self.x_opt = np.asarray(X_archive[best_i]).copy()\n                self.f_opt = float(F_archive[best_i])\n\n        # main loop\n        stagnation_threshold = max(20, 10 * self.dim)\n        max_iter_without_improve = stagnation_threshold\n        while self.evals < self.budget:\n            # prepare arrays\n            X = np.array(X_archive, dtype=float) if len(X_archive) > 0 else np.empty((0, self.dim))\n            F = np.array(F_archive, dtype=float) if len(F_archive) > 0 else np.empty((0,))\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0].tolist()\n            n_finite = len(finite_idx)\n\n            # compute robust per-dim scale from IQR or std\n            if n_finite >= 3:\n                Xf = X[finite_idx]\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                scale = (q75 - q25) / 1.349  # approximate std\n                small = scale < 1e-9\n                if np.any(small):\n                    sstd = np.std(Xf, axis=0)\n                    scale[small] = sstd[small]\n                # final fallback\n                if np.all(scale < 1e-9):\n                    scale = np.std(Xf, axis=0)\n            elif n_finite >= 1:\n                scale = np.std(X[finite_idx], axis=0)\n            else:\n                scale = 0.3 * domain\n\n            # clamp scale to portion of domain\n            per_dim_scale = np.minimum(np.maximum(scale, 1e-12), 0.5 * domain)\n\n            # choose anchor biased toward better F via softmax on -F\n            if n_finite >= 2:\n                vals = -F[finite_idx]\n                stab = 1.0 + np.std(vals) if np.std(vals) > 0 else 1.0\n                exps = np.exp((vals - np.max(vals)) / stab)\n                if np.sum(exps) == 0:\n                    probs = np.ones_like(exps) / exps.size\n                else:\n                    probs = exps / np.sum(exps)\n                base_choice = self.rng.choice(np.array(finite_idx), p=probs)\n                anchor = X[base_choice].copy()\n            elif len(X_archive) > 0:\n                anchor = X_archive[self.rng.randint(len(X_archive))].copy()\n            else:\n                anchor = lb + self.rng.rand(self.dim) * domain\n\n            # decide move type\n            p_global = 0.10\n            p_de = 0.25\n            move = self.rng.rand()\n            if move < p_global:\n                # GLOBAL: tempered Cauchy jump around best or anchor\n                if self.x_opt is not None and self.rng.rand() < 0.6:\n                    center = self.x_opt.copy()\n                else:\n                    center = anchor.copy()\n                jump = self._tempered_cauchy(self.gscale * domain, temp=0.9)\n                cand = center + jump\n                # occasional uniform mixing\n                if self.rng.rand() < 0.15:\n                    mask = self.rng.rand(self.dim) < 0.15\n                    cand[mask] = self._uniform_in_box(lb, ub)[mask]\n            elif move < p_global + p_de and n_finite >= 3:\n                # DE-like differential injection with jitter\n                idxs = list(range(len(X_archive)))\n                if len(idxs) >= 3:\n                    r = self.rng.choice(idxs, size=3, replace=False)\n                    x_r1 = X[r[0]].copy()\n                    x_r2 = X[r[1]].copy()\n                    anchor_r = X[r[2]].copy()\n                else:\n                    # fall back\n                    x_r1 = anchor.copy(); x_r2 = lb + self.rng.rand(self.dim) * domain; anchor_r = anchor.copy()\n                Fscale = 0.8 * self.gscale\n                cand = anchor_r + Fscale * (x_r1 - x_r2)\n                # small anisotropic gaussian jitter\n                cand += self.rng.randn(self.dim) * (0.12 * per_dim_scale)\n                # mix some coords with uniform to escape ridges\n                if self.rng.rand() < 0.3:\n                    mask = self.rng.rand(self.dim) < 0.12\n                    uni = self._uniform_in_box(lb, ub)\n                    cand[mask] = uni[mask]\n            else:\n                # LOCAL: anisotropic gaussian local moves + PCA-guided elite perturbation\n                cand = anchor.copy()\n                anis = self.rng.randn(self.dim) * (0.6 * per_dim_scale)\n                cand += anis\n                # coordinate-wise local gaussian injection on subset\n                mask = self.rng.rand(self.dim) < 0.35\n                if np.any(mask):\n                    cand[mask] += self.rng.randn(np.count_nonzero(mask)) * (0.5 * per_dim_scale[mask])\n                # sometimes do PCA-guided perturbation on elites\n                if n_finite >= 5 and self.rng.rand() < 0.45:\n                    k_el = max(3, min(15, int(0.15 * n_finite)))\n                    sorted_fin = np.array(finite_idx)[np.argsort(F[finite_idx])]\n                    elite_idx = sorted_fin[:k_el]\n                    elites = X[elite_idx]\n                    Xc = elites - np.mean(elites, axis=0)\n                    try:\n                        U, Svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        if Svals.size > 0 and np.sum(Svals) > 0:\n                            cum = np.cumsum(Svals) / np.sum(Svals)\n                            ncomp = max(1, int(np.searchsorted(cum, 0.7) + 1))\n                            ncomp = min(ncomp, Vt.shape[0])\n                            coeffs = self.rng.randn(ncomp) * (0.25 * np.mean(per_dim_scale))\n                            perturb = np.dot(coeffs, Vt[:ncomp, :])\n                            cand += perturb\n                    except Exception:\n                        cand += self.rng.randn(self.dim) * (0.03 * domain)\n                # occasional tiny tempered cauchy escape\n                if self.rng.rand() < 0.07:\n                    cand += self._tempered_cauchy(0.05 * domain, temp=0.8)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad_coords = ~np.isfinite(cand)\n            if np.any(bad_coords):\n                uni = self._uniform_in_box(lb, ub)\n                cand[bad_coords] = uni[bad_coords]\n\n            # reflect to bounds & clamp\n            cand = self._reflect_and_clamp(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            X_archive.append(cand.copy()); F_archive.append(f_cand)\n\n            # update short-term success metrics: success if among top 25% of finite\n            if len(F_archive) > 1:\n                finite_vals = np.array([v for v in F_archive if np.isfinite(v)])\n                if finite_vals.size > 0 and np.isfinite(f_cand):\n                    threshold = np.percentile(finite_vals, 25)\n                    success = f_cand <= threshold\n                else:\n                    success = False\n            else:\n                success = False\n            self.recent_results.append(bool(success))\n            if len(self.recent_results) > max(20, 5 * self.dim):\n                self.recent_results.pop(0)\n            # adapt gscale every few trials based on recent success fraction\n            self.short_trials += 1\n            if success:\n                self.short_success += 1\n                self.stagnation = 0\n            else:\n                self.stagnation += 1\n            if self.short_trials >= max(8, self.dim):\n                sr = self.short_success / float(self.short_trials)\n                if sr > 0.30:\n                    self.gscale = max(0.02, self.gscale * 0.90)\n                elif sr < 0.08:\n                    self.gscale = min(1.0, self.gscale * 1.12)\n                # small random walk\n                if self.rng.rand() < 0.2:\n                    self.gscale = np.clip(self.gscale * (1.0 + 0.04 * (self.rng.randn())), 1e-3, 2.0)\n                # reset\n                self.short_success = 0\n                self.short_trials = 0\n\n            # prune archive if too large\n            if len(F_archive) > self.archive_max:\n                F_arr = np.array(F_archive)\n                keep = set()\n                finite_idx2 = [i for i in range(len(F_arr)) if np.isfinite(F_arr[i])]\n                # keep best half of finite\n                if len(finite_idx2) > 0:\n                    sorted_fin = np.array(finite_idx2)[np.argsort(F_arr[finite_idx2])]\n                    nkeep_best = max(1, min(len(sorted_fin), self.archive_max // 2))\n                    for ii in sorted_fin[:nkeep_best]:\n                        keep.add(int(ii))\n                # keep random finite others\n                other_fin = [int(i) for i in finite_idx2 if i not in keep]\n                self.rng.shuffle(other_fin)\n                for ii in other_fin[:max(0, (self.archive_max // 4))]:\n                    keep.add(int(ii))\n                # add some infs for diversity if space\n                inf_idx = [int(i) for i in range(len(F_arr)) if not np.isfinite(F_arr[i])]\n                self.rng.shuffle(inf_idx)\n                for ii in inf_idx[:max(0, (self.archive_max - len(keep)) // 6)]:\n                    keep.add(int(ii))\n                # fill with random remaining until archive_max\n                rem = [i for i in range(len(F_arr)) if i not in keep]\n                self.rng.shuffle(rem)\n                for ii in rem:\n                    if len(keep) >= self.archive_max:\n                        break\n                    keep.add(int(ii))\n                keep_list = sorted(list(keep))[:self.archive_max]\n                F_archive = [F_archive[i] for i in keep_list]\n                X_archive = [X_archive[i] for i in keep_list]\n\n            # stagnation detection & micro-restart\n            if self.stagnation >= max_iter_without_improve:\n                # micro-restart: sample around best and add random points\n                self.stagnation = 0\n                if self.x_opt is None:\n                    # fallback uniform seeding\n                    n_local = min(8, max(1, self.dim))\n                    for _ in range(n_local):\n                        if self.evals >= self.budget:\n                            break\n                        xx = self._uniform_in_box(lb, ub)\n                        fxx = safe_eval(xx)\n                        X_archive.append(xx.copy()); F_archive.append(fxx)\n                else:\n                    n_local = min(10, max(1, 3 * self.dim // 4))\n                    for _ in range(n_local):\n                        if self.evals >= self.budget:\n                            break\n                        # localized spread around best with anisotropic sizes\n                        spread = (0.05 + 0.02 * self.rng.rand(self.dim)) * domain * (1.0 + 0.5 * self.rng.randn())\n                        xx = self.x_opt + self.rng.randn(self.dim) * spread\n                        xx = self._reflect_and_clamp(xx, lb, ub)\n                        fxx = safe_eval(xx)\n                        X_archive.append(xx.copy()); F_archive.append(fxx)\n                    # add some purely randoms\n                    for _ in range(3):\n                        if self.evals >= self.budget:\n                            break\n                        xx = self._uniform_in_box(lb, ub)\n                        fxx = safe_eval(xx)\n                        X_archive.append(xx.copy()); F_archive.append(fxx)\n                # slightly increase exploration scales to escape\n                self.gscale = min(1.5, self.gscale * 1.15)\n                self.recent_results = []\n\n        # final best: choose best finite from archive\n        if len(F_archive) == 0:\n            return np.inf, None\n        F_arr = np.array(F_archive)\n        finite_idx_final = np.where(np.isfinite(F_arr))[0]\n        if finite_idx_final.size > 0:\n            best_i = int(finite_idx_final[np.argmin(F_arr[finite_idx_final])])\n            return float(F_arr[best_i]), np.asarray(X_archive[best_i], dtype=float)\n        else:\n            # no finite evaluations, return first archived (shouldn't happen)\n            return np.inf, np.asarray(X_archive[0], dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "53db9be9-4d32-4df2-b2e9-c407c385a6db", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # initial LHS size default: small fraction of budget but at least 4\n        if init_samples is None:\n            self.init_samples = max(4, min(20, self.budget // 50))\n        else:\n            self.init_samples = int(init_samples)\n\n        # archive and bookkeeping\n        self.archive_max = int(archive_max)\n        self.X_archive = []   # list of np.array points\n        self.F_archive = []   # list of floats\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # adaptation variables\n        # per-dim scale expressed as fraction of domain width\n        self.gscale = np.full(self.dim, 0.08)  # start with modest step relative to domain\n        self.iter_since_adapt = 0\n        self.success_window = []   # recent success flags (1/0) for short-term adaptation\n        self.success_window_len = 40\n\n        # stagnation detection / microrestart\n        self.best_since_restart = np.inf\n        self.stagnation_counter = 0\n        self.stagnation_limit = max(30, self.dim * 6)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Attempt to read bounds; default [-5,5]^dim\n        lb = None\n        ub = None\n\n        # common patterns: func.bounds as (lb,ub) or object with .lb/.ub; also func.bounds may be list/tuple\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            try:\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                elif hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(getattr(b, \"lb\"), dtype=float)\n                    ub = np.asarray(getattr(b, \"ub\"), dtype=float)\n            except Exception:\n                lb = None\n\n        # fallback to func.lb / func.ub attributes\n        if lb is None:\n            if hasattr(func, \"lb\"):\n                try:\n                    lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n                except Exception:\n                    lb = None\n        if ub is None:\n            if hasattr(func, \"ub\"):\n                try:\n                    ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n                except Exception:\n                    ub = None\n\n        # final fallback to [-5,5]\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n        else:\n            lb = np.resize(lb, self.dim).astype(float)\n        if ub is None:\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            ub = np.resize(ub, self.dim).astype(float)\n\n        # ensure ub > lb, finite\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]) or ub[i] <= lb[i]:\n                ub[i] = lb[i] + 10.0  # fallback width 10 -> [-5,5] style\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect technique: reflect any coordinate outside [lb,ub] back into the interval\n        x = np.asarray(x, dtype=float).copy()\n        for i in range(self.dim):\n            lo = lb[i]; hi = ub[i]\n            if np.isfinite(x[i]):\n                width = hi - lo\n                if width <= 0:\n                    x[i] = lo\n                    continue\n                # reflect repeatedly until inside\n                while x[i] < lo or x[i] > hi:\n                    if x[i] < lo:\n                        x[i] = lo + (lo - x[i])\n                    if x[i] > hi:\n                        x[i] = hi - (x[i] - hi)\n                # clamp minor floating error\n                if x[i] < lo: x[i] = lo\n                if x[i] > hi: x[i] = hi\n            else:\n                x[i] = self.rng.uniform(lo, hi)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        pts = np.empty((n, dim), dtype=float)\n        for j in range(dim):\n            a = cut[:-1]\n            u = self.rng.rand(n)\n            pts[:, j] = a + u * (1.0 / n)\n            self.rng.shuffle(pts[:, j])\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise tempered Cauchy: draw standard cauchy and damp large tails\n        scale = np.asarray(scale_vec, dtype=float)\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # temper with gaussian damping to avoid extreme outliers\n        damp = np.exp(-0.5 * (np.abs(c) / 4.0)**2)\n        raw = c * damp\n        cap = cap_multiplier * np.maximum(scale, 1e-12)\n        jump = np.sign(raw) * np.minimum(np.abs(raw) * scale * 2.0, cap)\n        return jump\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n        # scale gscale to absolute domain fraction\n        self.gscale = np.clip(self.gscale, 1e-6, 1.0) * 1.0  # fraction, multiply by domain when used\n\n        # safe evaluator that respects budget\n        self.evals = 0\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                raise StopIteration()\n            try:\n                x = np.asarray(x, dtype=float)\n            except Exception:\n                x = np.asarray(x)\n            f = float(func(x))\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        n_init = max(2, min(self.init_samples, max(2, self.budget // 20)))\n        lhs = self._lhs01(n_init, self.dim)\n        # scale to bounds\n        for i in range(n_init):\n            x = lb + lhs[i] * domain\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            self.X_archive.append(x.copy())\n            self.F_archive.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # add a couple of pure uniform draws (diversity)\n        while self.evals < min(self.budget, n_init + 4):\n            x = self._uniform_array(lb, ub)\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            self.X_archive.append(x.copy()); self.F_archive.append(f)\n            if f < self.f_opt:\n                self.f_opt = f; self.x_opt = x.copy()\n\n        # main loop\n        try:\n            while True:\n                if self.evals >= self.budget:\n                    break\n\n                # convert archive to arrays for computation convenience\n                if len(self.X_archive) > 0:\n                    X = np.array(self.X_archive, dtype=float)\n                    F = np.array(self.F_archive, dtype=float)\n                else:\n                    X = np.zeros((0, self.dim))\n                    F = np.array([])\n\n                finite_idx = [i for i, fv in enumerate(F) if np.isfinite(fv)]\n                n_finite = len(finite_idx)\n\n                # compute robust per-dim scale using IQR if possible, else std, else domain-based\n                if n_finite >= 4:\n                    Xf = X[finite_idx]\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    iqr = np.maximum(q75 - q25, 1e-9)\n                    scale = iqr / 1.349  # approx std\n                    small = scale < 1e-8\n                    if small.any():\n                        std = np.std(Xf, axis=0)\n                        scale[small] = np.maximum(std[small], 1e-8)\n                elif n_finite >= 1:\n                    scale = np.std(X[finite_idx], axis=0)\n                    scale = np.maximum(scale, 1e-8)\n                else:\n                    scale = domain * 0.2  # broad if nothing yet\n\n                # clamp scale to fractions of domain (absolute)\n                per_dim_scale = np.maximum(scale, 1e-12)\n                per_dim_scale = np.minimum(per_dim_scale, domain * 0.5)\n                abs_gscale = np.maximum(self.gscale * domain, 1e-12)\n\n                # choose anchor: often a good elite, sometimes a random\n                if n_finite >= 5 and self.rng.rand() < 0.7:\n                    # select elite by sampling from top quantile biased\n                    order = np.argsort(F[finite_idx])\n                    elites = np.array(finite_idx)[order[:max(1, int(0.2 * n_finite))]]\n                    anchor_idx = self.rng.choice(elites)\n                    anchor = X[anchor_idx].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n\n                # propose a candidate via a mixture of strategies\n                cand = anchor.copy()\n\n                move_roll = self.rng.rand()\n                # move probabilities:\n                # 0.00-0.18 DE-injection (if archive has enough)\n                # 0.18-0.45 PCA elite perturbation (if enough)\n                # 0.45-0.85 Local anisotropic Gaussian\n                # 0.85-1.00 Tempered Cauchy global escape\n                if move_roll < 0.18 and n_finite >= 3:\n                    # DE-style: pick two random distinct points from finite archive\n                    idxs = self.rng.choice(finite_idx, size=2, replace=False)\n                    diff = X[idxs[0]] - X[idxs[1]]\n                    scale_factor = 0.6 * (0.4 + self.rng.rand() * 1.2)\n                    cand = cand + scale_factor * diff * (0.5 + self.rng.rand(self.dim) * 0.6)\n                    # slight gaussian jitter\n                    cand += self.rng.randn(self.dim) * (0.03 * domain)\n\n                elif move_roll < 0.45 and n_finite >= 6:\n                    # PCA-guided elite perturbation\n                    k_el = max(4, min(12, int(0.12 * n_finite)))\n                    idxs = np.argsort(F[finite_idx])[:k_el]\n                    Xel = X[np.array(finite_idx)[idxs]]\n                    # center\n                    center = np.mean(Xel, axis=0)\n                    Xc = Xel - center\n                    try:\n                        # PCA via SVD\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        # choose number of components that explain > 60% variance or up to 3\n                        var = S**2\n                        cum = np.cumsum(var) / np.sum(var) if var.sum() > 0 else np.zeros_like(var)\n                        ncomp = int(np.searchsorted(cum, 0.6) + 1)\n                        ncomp = max(1, min(ncomp, min(5, Vt.shape[0])))\n                        # perturb along top components with random coefficients\n                        coeffs = self.rng.randn(ncomp) * (0.12 * np.median(abs_gscale))\n                        perturb = (coeffs @ Vt[:ncomp, :])\n                        cand = center + perturb\n                        # small extra anisotropy\n                        cand += self.rng.randn(self.dim) * (0.02 * domain)\n                    except Exception:\n                        # fallback small gaussian\n                        cand += self.rng.randn(self.dim) * (0.05 * domain)\n\n                elif move_roll < 0.85:\n                    # Local anisotropic gaussian: scale per-dim by learned gscale and archive spread\n                    anis = self.rng.randn(self.dim) * (0.8 * abs_gscale + 0.5 * per_dim_scale)\n                    cand += anis\n                    # coordinate-wise local gaussian injection on some coords\n                    mask = self.rng.rand(self.dim) < 0.35\n                    cand[mask] += self.rng.randn(np.sum(mask)) * (0.35 * abs_gscale[mask])\n                    # sometimes mix some coords uniformly to escape ridges\n                    if self.rng.rand() < 0.12:\n                        uni = self._uniform_array(lb, ub)\n                        mix_mask = self.rng.rand(self.dim) < 0.12\n                        cand[mix_mask] = uni[mix_mask]\n\n                else:\n                    # tempered Cauchy global escape around anchor\n                    jump = self._tempered_cauchy(abs_gscale, cap_multiplier=8.0)\n                    cand += jump\n                    # small chance to add differential jump\n                    if n_finite >= 2 and self.rng.rand() < 0.25:\n                        idxs = self.rng.choice(finite_idx, size=2, replace=False)\n                        cand += 0.4 * (X[idxs[0]] - X[idxs[1]]) * (0.4 + 0.8 * self.rng.rand())\n\n                # occasional direct jump to current best to refine (small probability)\n                if (self.x_opt is not None) and (self.rng.rand() < 0.05):\n                    cand = self.x_opt.copy()\n                    cand += self.rng.randn(self.dim) * (0.06 * domain)\n\n                # safety: replace NaN/inf coords with uniform random within bounds\n                bad_coords = ~np.isfinite(cand)\n                if bad_coords.any():\n                    cand[bad_coords] = self._uniform_array(lb, ub)[bad_coords]\n\n                # reflect to bounds & clamp\n                cand = self._reflect_bounds(cand, lb, ub)\n\n                # evaluate candidate\n                try:\n                    f_cand = safe_eval(cand)\n                except StopIteration:\n                    break\n\n                # record\n                self.X_archive.append(cand.copy()); self.F_archive.append(f_cand)\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = cand.copy()\n\n                # assess success: among top 25% of finite scores in archive\n                finite_vals = np.array([v for v in self.F_archive if np.isfinite(v)])\n                if finite_vals.size > 0:\n                    rank_pos = (finite_vals < f_cand).sum() + 1  # 1 is best\n                    rank_threshold = max(1, int(0.25 * len(finite_vals)))\n                    success = 1 if rank_pos <= rank_threshold else 0\n                else:\n                    success = 0\n                self.success_window.append(success)\n                if len(self.success_window) > self.success_window_len:\n                    self.success_window.pop(0)\n\n                # short-term adaptation of gscale every few iterations\n                self.iter_since_adapt += 1\n                if self.iter_since_adapt >= 6:\n                    win = np.array(self.success_window, dtype=float)\n                    if win.size > 4:\n                        rate = win.mean()\n                        # if too many successes, slightly decrease step sizes to refine\n                        if rate > 0.35:\n                            self.gscale *= (0.9 - 0.06 * self.rng.rand(self.dim))\n                        # if too few successes, increase exploration slightly\n                        elif rate < 0.08:\n                            self.gscale *= (1.08 + 0.1 * self.rng.rand(self.dim))\n                        else:\n                            # small random walk to maintain diversity\n                            self.gscale *= (1.0 + (self.rng.rand(self.dim) - 0.5) * 0.06)\n                        # clamp to reasonable fractions\n                        self.gscale = np.clip(self.gscale, 1e-5, 0.6)\n                    self.iter_since_adapt = 0\n\n                # prune archive if too large: keep best half + some randoms + a few infs\n                if len(self.F_archive) > self.archive_max:\n                    F_arr = np.array(self.F_archive)\n                    # indices of finite sorted by fitness\n                    fin_idx = [i for i, v in enumerate(F_arr) if np.isfinite(v)]\n                    keep = set()\n                    if len(fin_idx) > 0:\n                        sorted_fin = sorted(fin_idx, key=lambda i: F_arr[i])\n                        nkeep_best = max(2, int(0.5 * len(sorted_fin)))\n                        for ii in sorted_fin[:nkeep_best]:\n                            keep.add(int(ii))\n                        # keep some random finite others\n                        others = [int(i) for i in sorted_fin[nkeep_best:]]\n                        self.rng.shuffle(others)\n                        for ii in others[:max(0, (self.archive_max - len(keep))//2)]:\n                            keep.add(int(ii))\n                    # keep some infinite entries if present (diversity)\n                    inf_idx = [i for i, v in enumerate(F_arr) if not np.isfinite(v)]\n                    for ii in inf_idx[:max(0, self.archive_max - len(keep))]:\n                        keep.add(int(ii))\n                    # fill rest with random remaining entries until archive_max\n                    all_idx = list(range(len(F_arr)))\n                    rem = [i for i in all_idx if i not in keep]\n                    self.rng.shuffle(rem)\n                    for ii in rem:\n                        if len(keep) >= self.archive_max:\n                            break\n                        keep.add(int(ii))\n                    # rebuild archives\n                    keep_list = sorted(list(keep))\n                    self.X_archive = [self.X_archive[i] for i in keep_list]\n                    self.F_archive = [self.F_archive[i] for i in keep_list]\n\n                # stagnation detection & micro-restarts\n                if self.f_opt < self.best_since_restart - 1e-12:\n                    # improvement observed\n                    self.best_since_restart = self.f_opt\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if self.stagnation_counter >= self.stagnation_limit:\n                    # micro-restart: generate a small local cloud around best and some randoms\n                    if self.x_opt is not None:\n                        n_local = min(10, max(3, self.dim // 2))\n                        for _ in range(n_local):\n                            perturb = self.rng.randn(self.dim) * (0.08 * domain)\n                            xx = self.x_opt + perturb\n                            xx = self._reflect_bounds(xx, lb, ub)\n                            try:\n                                fxx = safe_eval(xx)\n                            except StopIteration:\n                                break\n                            self.X_archive.append(xx.copy()); self.F_archive.append(fxx)\n                            if fxx < self.f_opt:\n                                self.f_opt = fxx; self.x_opt = xx.copy()\n                    # add a few global randoms\n                    for _ in range(3):\n                        if self.evals >= self.budget: break\n                        xx = self._uniform_array(lb, ub)\n                        xx = self._reflect_bounds(xx, lb, ub)\n                        try:\n                            fxx = safe_eval(xx)\n                        except StopIteration:\n                            break\n                        self.X_archive.append(xx.copy()); self.F_archive.append(fxx)\n                        if fxx < self.f_opt:\n                            self.f_opt = fxx; self.x_opt = xx.copy()\n                    # slightly increase exploration scales after restart\n                    self.gscale = np.minimum(0.6, self.gscale * (1.0 + 0.2 * self.rng.rand(self.dim)))\n                    self.stagnation_counter = 0\n                    # reset success window to encourage exploration\n                    self.success_window = []\n\n        except StopIteration:\n            # budget exhausted\n            pass\n\n        # final best: choose best finite from archive\n        if len(self.F_archive) == 0:\n            # fallback: random point\n            x = self._uniform_array(lb, ub)\n            return np.inf, x\n        F_arr = np.array(self.F_archive, dtype=float)\n        finite_indices = np.array([i for i, v in enumerate(F_arr) if np.isfinite(v)], dtype=int)\n        if finite_indices.size > 0:\n            best_i = int(finite_indices[np.argmin(F_arr[finite_indices])])\n            return float(F_arr[best_i]), np.asarray(self.X_archive[best_i], dtype=float)\n        else:\n            # no finite values: return best recorded (may be inf)\n            best_i = int(np.argmin(F_arr))\n            return float(F_arr[best_i]), np.asarray(self.X_archive[best_i], dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5aa2fa2d-2a1c-47a6-9d3e-5cf4bd0cfba2", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = int(archive_max)\n        self.rng = np.random.default_rng(seed)\n        # algorithm state\n        self.gscale = np.full(self.dim, 0.08)  # fraction of domain (will be multiplied by domain widths)\n        self.success_window_len = 40\n        self.success_window = []\n        self.stagnation_limit = max(30, self.dim * 6)\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        # archives\n        self.X_archive = []  # list of np.array dim\n        self.F_archive = []  # list of floats\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to read bounds from func in common formats; default [-5,5]^dim\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            if b is not None:\n                # common types: tuple (lb,ub), object with lb/ub, or simple list\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb_try, ub_try = b\n                    try:\n                        lb = np.asarray(lb_try, dtype=float)\n                        ub = np.asarray(ub_try, dtype=float)\n                    except Exception:\n                        lb = None; ub = None\n                else:\n                    # object with .lb and .ub\n                    try:\n                        lb = np.asarray(getattr(b, \"lb\"), dtype=float)\n                        ub = np.asarray(getattr(b, \"ub\"), dtype=float)\n                    except Exception:\n                        lb = None; ub = None\n        # fallback to attributes func.lb / func.ub\n        if lb is None or ub is None:\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                lb = None; ub = None\n        # final fallback: scalar or vector default [-5,5]\n        if lb is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n        else:\n            lb = np.resize(lb, self.dim).astype(float)\n        if ub is None:\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            ub = np.resize(ub, self.dim).astype(float)\n        # Ensure ub > lb and finite; adjust invalid dims to default width 10\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]):\n                lb[i] = -5.0\n            if not np.isfinite(ub[i]) or ub[i] <= lb[i]:\n                ub[i] = lb[i] + 10.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect technique: reflect any coordinate outside [lb,ub] back into the interval\n        x = np.asarray(x, dtype=float).copy()\n        for i in range(self.dim):\n            lo = lb[i]; hi = ub[i]\n            if not np.isfinite(x[i]):\n                x[i] = self.rng.uniform(lo, hi)\n                continue\n            # reflect repeatedly until inside\n            # use reflection: if over -> hi - (val - hi); if under -> lo + (lo - val)\n            # loop safeguard: do modulo to limit growth\n            cnt = 0\n            while (x[i] < lo or x[i] > hi) and cnt < 10:\n                if x[i] < lo:\n                    x[i] = lo + (lo - x[i])\n                elif x[i] > hi:\n                    x[i] = hi - (x[i] - hi)\n                cnt += 1\n            # if still outside (extreme), sample uniformly\n            if x[i] < lo or x[i] > hi or not np.isfinite(x[i]):\n                x[i] = self.rng.uniform(lo, hi)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        pts = np.empty((n, dim), dtype=float)\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            # sample in each interval\n            u = self.rng.random(n) * (1.0 / n)\n            pts[:, j] = cut[:-1] + u\n            pts[:, j] = pts[perm, j]  # shuffle rows\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            out = self.rng.random((n, self.dim)) * (ub - lb) + lb\n            return out\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise tempered Cauchy: draw standard cauchy and damp large tails\n        raw = self.rng.standard_cauchy(self.dim)\n        # temper with gaussian damping to avoid extreme outliers (damp tails)\n        damp = np.exp(-0.5 * (np.abs(raw) / 4.0) ** 2)\n        temp = raw * damp\n        cap = cap_multiplier\n        jump = np.sign(temp) * np.minimum(np.abs(temp) * scale_vec * cap, cap * scale_vec)\n        return jump\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n        # scale gscale to absolute domain width (clamp)\n        self.gscale = np.clip(self.gscale, 1e-6, 1.0) * domain\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # checks budget before calling\n            if self.evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            f = func(x)\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        n_init = max(4, min(self.budget // 8, max(4, self.dim * 2)))\n        n_init = min(n_init, self.budget)\n        lhs = self._lhs01(n_init, self.dim)\n        # scale to bounds and evaluate\n        try:\n            for i in range(n_init):\n                x = lb + lhs[i] * domain\n                x = self._reflect_bounds(x, lb, ub)\n                f = safe_eval(x)\n                self.X_archive.append(x.copy())\n                self.F_archive.append(float(f))\n                if f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n        except StopIteration:\n            # budget exhausted during init\n            if self.x_opt is None:\n                # return best we have or random\n                if len(self.F_archive) > 0:\n                    best_i = int(np.argmin(np.array(self.F_archive, dtype=float)))\n                    return float(self.F_archive[best_i]), np.asarray(self.X_archive[best_i], dtype=float)\n                else:\n                    x = self._uniform_array(lb, ub)\n                    return float(np.inf), x\n\n        # add a couple of pure uniform draws for diversity\n        try:\n            while self.evals < min(self.budget, n_init + 4):\n                x = self._uniform_array(lb, ub)\n                x = self._reflect_bounds(x, lb, ub)\n                f = safe_eval(x)\n                self.X_archive.append(x.copy())\n                self.F_archive.append(float(f))\n                if f < self.f_opt:\n                    self.f_opt = float(f); self.x_opt = x.copy()\n        except StopIteration:\n            pass\n\n        # main loop: generate candidates until budget exhausted\n        best_since_restart = self.f_opt\n        stagnation_counter = 0\n        iter_count = 0\n\n        try:\n            while True:\n                iter_count += 1\n\n                # convert archive to arrays for computation convenience\n                if len(self.X_archive) > 0:\n                    X_arr = np.vstack(self.X_archive)\n                    F_arr = np.asarray(self.F_archive, dtype=float)\n                else:\n                    X_arr = np.empty((0, self.dim)); F_arr = np.array([])\n\n                finite_idx = np.isfinite(F_arr)\n                n_finite = int(np.sum(finite_idx))\n\n                # compute robust per-dim scale using IQR if possible, else std, else domain-based\n                if n_finite >= 4:\n                    finite_X = X_arr[finite_idx]\n                    iqr = np.subtract(*np.percentile(finite_X, [75, 25], axis=0))\n                    # convert iqr to approx std\n                    scale = iqr / 1.349\n                    small = (scale <= 1e-8)\n                    if small.any():\n                        std = np.std(finite_X, axis=0)\n                        scale[small] = std[small]\n                    # if still tiny, use broad domain fallback\n                    if np.all(scale <= 1e-12):\n                        scale = domain * 0.2\n                elif len(self.X_archive) > 0:\n                    # use archive spread or default\n                    Xm = np.asarray(self.X_archive)\n                    scale = np.maximum(np.std(Xm, axis=0), 1e-8)\n                    scale = np.minimum(scale, domain * 0.5)\n                else:\n                    scale = domain * 0.2\n\n                per_dim_scale = np.maximum(scale, 1e-12)\n\n                # choose anchor: often a good elite, sometimes random one\n                anchor = None\n                if n_finite >= 2 and self.rng.random() < 0.85:\n                    # bias toward elites: sample from top quantile\n                    q = max(1, int(0.25 * n_finite))\n                    idxs_sorted = np.argsort(F_arr[finite_idx])\n                    # map back global indices\n                    finite_indices = np.nonzero(finite_idx)[0]\n                    elites = finite_indices[idxs_sorted[:q]]\n                    anchor_idx = self.rng.choice(elites)\n                    anchor = X_arr[anchor_idx].copy()\n                elif self.X_archive:\n                    anchor = X_arr[self.rng.integers(len(self.X_archive))].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n\n                # propose a candidate via a mixture of strategies\n                cand = anchor.copy()\n                move_roll = self.rng.random()\n\n                # Move probabilities:\n                # 0.00-0.18 DE-injection (if archive has enough)\n                # 0.18-0.45 PCA elite perturbation (if enough)\n                # 0.45-0.85 Local anisotropic Gaussian\n                # 0.85-1.00 Tempered Cauchy global escape\n                if move_roll < 0.18 and n_finite >= 4:\n                    # DE-style: pick two random distinct points from finite archive, mutate anchor\n                    idxs = np.nonzero(finite_idx)[0]\n                    i, j = self.rng.choice(idxs, size=2, replace=False)\n                    donor = X_arr[i] - X_arr[j]\n                    scale_factor = 0.6 * (0.4 + self.rng.random() * 1.2)\n                    diff = donor * scale_factor\n                    # slight gaussian jitter\n                    jitter = self.rng.normal(0.0, 0.02, self.dim) * domain\n                    cand = anchor + diff + jitter\n\n                elif move_roll < 0.45 and n_finite >= 6:\n                    # PCA-guided elite perturbation\n                    # pick top k_el elites\n                    k_el = min(max(4, int(0.15 * n_finite)), n_finite)\n                    elite_idx = np.argsort(F_arr[finite_idx])[:k_el]\n                    finite_indices = np.nonzero(finite_idx)[0]\n                    elites_global = finite_indices[elite_idx]\n                    X_elite = X_arr[elites_global]\n                    # center\n                    mean_e = np.mean(X_elite, axis=0)\n                    try:\n                        Xc = X_elite - mean_e\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        var = S ** 2\n                        cumvar = np.cumsum(var) / (np.sum(var) + 1e-30)\n                        ncomp = int(np.searchsorted(cumvar, 0.6) + 1)\n                        ncomp = max(1, min(ncomp, min(3, Vt.shape[0])))\n                        # coefficients along components\n                        coeffs = self.rng.normal(0.0, 1.0, size=(ncomp,))\n                        # scale coefficients by singular values and by per-dim scale\n                        comp_dirs = Vt[:ncomp, :]\n                        perturb = coeffs @ comp_dirs\n                        # scale perturbation to per-dim scales\n                        alpha = 0.25 + 0.75 * self.rng.random()\n                        cand = mean_e + perturb * (per_dim_scale * alpha)\n                        # small gaussian to add anisotropy\n                        cand += self.rng.normal(0.0, 0.02, self.dim) * per_dim_scale\n                    except Exception:\n                        # fallback small gaussian\n                        cand = anchor + self.rng.normal(0.0, 1.0, self.dim) * (per_dim_scale * 0.3)\n\n                elif move_roll < 0.85:\n                    # Local anisotropic gaussian: scale per-dim by learned gscale and archive spread\n                    # coordinate-wise local gaussian injection on some coords\n                    mask = self.rng.random(self.dim) < 0.35\n                    noise = self.rng.normal(0.0, 1.0, self.dim) * (self.gscale * 0.9 + per_dim_scale * 0.2)\n                    # sometimes mix some coords uniformly to escape ridges\n                    if self.rng.random() < 0.08:\n                        mix_idx = self.rng.choice(self.dim, size=max(1, self.dim // 6), replace=False)\n                        noise[mix_idx] = (self.rng.random(len(mix_idx)) - 0.5) * domain[mix_idx] * 0.6\n                    cand = anchor + noise * mask + self.rng.normal(0.0, 1e-6, self.dim)\n\n                else:\n                    # tempered Cauchy global escape around anchor\n                    scale_vec = np.maximum(self.gscale, per_dim_scale)\n                    jump = self._tempered_cauchy(scale_vec, cap_multiplier=6.0)\n                    # small chance to add differential jump\n                    if n_finite >= 2 and self.rng.random() < 0.12:\n                        idxs = np.nonzero(finite_idx)[0]\n                        i, j = self.rng.choice(idxs, size=2, replace=False)\n                        jump += 0.6 * (X_arr[i] - X_arr[j])\n                    cand = anchor + jump\n\n                # occasional direct small jump to current best to refine\n                if self.x_opt is not None and self.rng.random() < 0.03:\n                    cand = self.x_opt + self.rng.normal(0.0, 0.02, self.dim) * (self.gscale * 0.3)\n\n                # safety: replace NaN/inf coords with uniform random within bounds\n                bad_coords = ~np.isfinite(cand)\n                if np.any(bad_coords):\n                    cand[bad_coords] = self._uniform_array(lb, ub)[bad_coords]\n\n                # reflect to bounds & clamp\n                cand = self._reflect_bounds(cand, lb, ub)\n\n                # evaluate candidate\n                f_cand = safe_eval(cand)\n\n                # record\n                self.X_archive.append(cand.copy()); self.F_archive.append(float(f_cand))\n                if f_cand < self.f_opt:\n                    self.f_opt = float(f_cand); self.x_opt = cand.copy()\n\n                # assess success: success if candidate is among top 25% of finite scores\n                finite_vals = F_arr[finite_idx] if n_finite > 0 else np.array([])\n                success = False\n                if n_finite > 0:\n                    cutoff = np.percentile(np.concatenate([finite_vals, [f_cand]]), 25)\n                    if f_cand <= cutoff:\n                        success = True\n                else:\n                    # first successes\n                    success = True\n\n                # update success window and stagnation counter\n                self.success_window.append(1 if success else 0)\n                if len(self.success_window) > self.success_window_len:\n                    self.success_window.pop(0)\n                if f_cand + 1e-12 < best_since_restart:\n                    best_since_restart = f_cand\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # short-term adaptation of gscale every few iterations\n                if (iter_count % max(3, int(self.success_window_len / 5))) == 0 and len(self.success_window) > 4:\n                    rate = float(np.sum(self.success_window)) / len(self.success_window)\n                    # if too many successes, slightly decrease step sizes to refine\n                    if rate > 0.35:\n                        self.gscale *= 0.92\n                    # if too few successes, increase exploration slightly\n                    elif rate < 0.15:\n                        self.gscale *= 1.10\n                    else:\n                        # small random walk to maintain diversity\n                        self.gscale *= 1.0 + (self.rng.normal(0.0, 0.02, size=self.dim))\n                    # clamp to reasonable fractions of domain\n                    self.gscale = np.clip(self.gscale, domain * 1e-6, domain * 0.6)\n\n                # prune archive if too large: keep best half + some randoms + a few infs\n                if len(self.X_archive) > self.archive_max:\n                    F_arr2 = np.asarray(self.F_archive, dtype=float)\n                    all_idx = list(range(len(F_arr2)))\n                    fin_idx = [i for i, v in enumerate(F_arr2) if np.isfinite(v)]\n                    inf_idx = [i for i, v in enumerate(F_arr2) if not np.isfinite(v)]\n                    keep = set()\n                    # keep best half of finite\n                    if len(fin_idx) > 0:\n                        sorted_fin = sorted(fin_idx, key=lambda i: F_arr2[i])\n                        nkeep_best = max(2, int(0.5 * len(sorted_fin)))\n                        for ii in sorted_fin[:nkeep_best]:\n                            keep.add(int(ii))\n                        # keep some random finite others\n                        remaining_fin = [i for i in sorted_fin[nkeep_best:]]\n                        nrand_keep = max(0, min(len(remaining_fin), int(0.2 * self.archive_max)))\n                        if nrand_keep > 0:\n                            sel = self.rng.choice(remaining_fin, size=nrand_keep, replace=False)\n                            for ii in sel:\n                                keep.add(int(ii))\n                    # keep some infinite entries if present (diversity)\n                    for ii in inf_idx[:max(0, self.archive_max - len(keep))]:\n                        keep.add(int(ii))\n                    # fill rest with random remaining entries until archive_max\n                    rem = [i for i in all_idx if i not in keep]\n                    self.rng.shuffle(rem)\n                    for ii in rem[:max(0, self.archive_max - len(keep))]:\n                        keep.add(int(ii))\n                    keep_list = sorted(list(keep))\n                    # rebuild archives\n                    self.X_archive = [self.X_archive[i] for i in keep_list]\n                    self.F_archive = [self.F_archive[i] for i in keep_list]\n\n                # stagnation detection & micro-restarts\n                if stagnation_counter >= self.stagnation_limit:\n                    # micro-restart: generate a small local cloud around best and some randoms\n                    stagnation_counter = 0\n                    self.success_window = []\n                    best_since_restart = self.f_opt\n                    # local cloud\n                    n_local = min(20, max(6, int(4 + 2 * self.dim)))\n                    for _ in range(n_local):\n                        if self.x_opt is None:\n                            xx = self._uniform_array(lb, ub)\n                        else:\n                            perturb = self.rng.normal(0.0, 1.0, self.dim) * (self.gscale * (0.6 + self.rng.random()))\n                            xx = self.x_opt + perturb\n                            xx = self._reflect_bounds(xx, lb, ub)\n                        try:\n                            fxx = safe_eval(xx)\n                        except StopIteration:\n                            break\n                        self.X_archive.append(xx.copy()); self.F_archive.append(float(fxx))\n                        if fxx < self.f_opt:\n                            self.f_opt = float(fxx); self.x_opt = xx.copy()\n                    # add a few global randoms\n                    n_global = min(8, max(2, int(self.dim)))\n                    for _ in range(n_global):\n                        xx = self._uniform_array(lb, ub)\n                        try:\n                            fxx = safe_eval(xx)\n                        except StopIteration:\n                            break\n                        self.X_archive.append(xx.copy()); self.F_archive.append(float(fxx))\n                        if fxx < self.f_opt:\n                            self.f_opt = float(fxx); self.x_opt = xx.copy()\n                    # slightly increase exploration scales after restart\n                    self.gscale = np.minimum(self.gscale * 1.2, domain * 0.6)\n\n        except StopIteration:\n            # budget exhausted, proceed to return best found\n            pass\n\n        # final best: choose best finite from archive\n        if len(self.F_archive) == 0:\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n        F_arr = np.asarray(self.F_archive, dtype=float)\n        finite_indices = np.nonzero(np.isfinite(F_arr))[0]\n        if finite_indices.size > 0:\n            best_i = int(finite_indices[np.argmin(F_arr[finite_indices])])\n            return float(F_arr[best_i]), np.asarray(self.X_archive[best_i], dtype=float)\n        else:\n            # no finite values recorded (unlikely) - return best recorded (may be inf)\n            best_i = int(np.argmin(F_arr))\n            return float(F_arr[best_i]), np.asarray(self.X_archive[best_i], dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "dd3c3098-415d-43d1-b25d-6a9680e1aac9", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.archive_max = int(archive_max)\n\n        # archives\n        self.X_archive = []  # list of np.array (dim,)\n        self.F_archive = []  # list of floats (may be np.inf)\n\n        # adaptive scales (fraction of domain width)\n        self.gscale = np.ones(self.dim) * 0.05\n\n        # bookkeeping\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # short term success window for adaptation\n        self.success_window = deque(maxlen=50)\n        # stagnation detection\n        self.iter_since_improve = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try multiple common conventions\n        lb = None; ub = None\n        try:\n            lb = np.asarray(func.lb, dtype=float)\n            ub = np.asarray(func.ub, dtype=float)\n        except Exception:\n            pass\n        if lb is None or ub is None:\n            # try .bounds object\n            try:\n                b = func.bounds\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            except Exception:\n                pass\n        # fallback to [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # ensure shapes\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        # ensure lengths\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # check finite and ub>lb\n        for i in range(self.dim):\n            if not np.isfinite(lb[i]) or not np.isfinite(ub[i]) or ub[i] <= lb[i]:\n                lb[i] = -5.0\n                ub[i] = 5.0\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = np.asarray(x, dtype=float).copy()\n        # reflect repeatedly until inside; but keep limited iterations\n        for _ in range(4):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect\n            x[above] = ub[above] - (x[above] - ub[above])\n            x[below] = lb[below] + (lb[below] - x[below])\n        # final clamp if still outside\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # basic Latin hypercube in [0,1)\n        # generate permutations for each dim\n        cut = np.linspace(0, 1, n + 1)\n        u = self.rng.uniform(size=(n, dim))\n        result = np.zeros((n, dim))\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            result[:, j] = cut[:-1][perm] + u[:, j] * (1.0 / n)\n        return result\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb).reshape(-1)\n        ub = np.asarray(ub).reshape(-1)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            out = self.rng.uniform(size=(n, self.dim)) * (ub - lb) + lb\n            return out\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # tempered coordinate-wise Cauchy: draw standard cauchy but damp extreme tails\n        # scale_vec: absolute per-dim scales\n        cap = float(cap_multiplier)\n        z = self.rng.standard_cauchy(self.dim)\n        # temper: shrink extreme values using tanh and a soft cap\n        # small factor to keep effect moderate relative to scale_vec\n        tempered = np.tanh(z / cap) * (cap * scale_vec)\n        # occasionally allow a small extra factor\n        if self.rng.random() < 0.05:\n            tempered *= (1.0 + 0.5 * self.rng.random())\n        return tempered\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain = ub - lb\n        # clamp gscale to domain fraction\n        self.gscale = np.clip(self.gscale, domain * 1e-6, domain * 0.6)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                raise StopIteration()\n            # ensure 1d array\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            f = func(x)\n            self.eval_count += 1\n            return float(f)\n\n        # ---- initialization: LHS samples ----\n        # allocate a small initial population: a modest fraction of budget\n        n_init = max(8, min(10 * self.dim, int(self.budget * 0.08)))\n        n_init = min(n_init, self.budget)\n        try:\n            samples01 = self._lhs01(n_init, self.dim)\n            for s in range(n_init):\n                x = lb + samples01[s] * (ub - lb)\n                f = safe_eval(x)\n                # record\n                self.X_archive.append(x.copy())\n                self.F_archive.append(float(f))\n                if np.isfinite(f) and f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n        except StopIteration:\n            # budget exhausted during init\n            if self.x_opt is None:\n                # choose best from what we have or sample a fallback\n                if len(self.F_archive) > 0:\n                    idx = int(np.argmin(self.F_archive))\n                    return float(self.F_archive[idx]), np.array(self.X_archive[idx])\n                else:\n                    # random fallback\n                    x = self._uniform_array(lb, ub)\n                    return float(func(x)), x\n\n        # add a couple of pure uniform draws for diversity\n        for _ in range(min(5, max(1, int(self.budget * 0.002)))):\n            if self.eval_count >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            self.X_archive.append(x.copy()); self.F_archive.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f); self.x_opt = x.copy()\n\n        # main loop: generate candidates until budget exhausted\n        stagnation_limit = max(150, 25 * self.dim)\n        iter_count = 0\n\n        while self.eval_count < self.budget:\n            iter_count += 1\n            # convert archive to arrays for convenience\n            X_arr = np.array(self.X_archive) if len(self.X_archive) > 0 else np.zeros((0, self.dim))\n            F_arr = np.array(self.F_archive) if len(self.F_archive) > 0 else np.array([])\n\n            finite_idx = np.isfinite(F_arr)\n            finite_vals = F_arr[finite_idx] if finite_idx.any() else np.array([])\n            n_finite = int(np.sum(finite_idx))\n            # robust per-dim scale using IQR of archive if possible\n            if X_arr.shape[0] >= 4 and n_finite >= 2:\n                try:\n                    # use only finite samples for spread estimation\n                    X_fin = X_arr[finite_idx, :]\n                    q75 = np.percentile(X_fin, 75, axis=0)\n                    q25 = np.percentile(X_fin, 25, axis=0)\n                    iqr = (q75 - q25)\n                    scale = iqr / 1.349  # approx convert IQR to std\n                    # fallback to std if iqr zero\n                    zero_mask = scale <= 0\n                    if zero_mask.any():\n                        std = np.std(X_fin, axis=0)\n                        scale[zero_mask] = std[zero_mask]\n                    # final fallback to domain fraction\n                    small_mask = scale <= 0\n                    scale[small_mask] = domain[small_mask] * 0.25\n                except Exception:\n                    scale = domain * 0.25\n            else:\n                scale = domain * 0.25\n            per_dim_scale = np.maximum(scale, 1e-12)\n\n            # choose anchor: bias to elites\n            anchor = None\n            if n_finite >= 4:\n                # choose from top quantile with bias\n                sorted_idx = np.argsort(F_arr[finite_idx])\n                fin_indices = np.nonzero(finite_idx)[0]\n                topk = max(2, int(max(3, len(sorted_idx) * 0.25)))\n                top_indices = fin_indices[sorted_idx[:topk]]\n                if self.rng.random() < 0.7:\n                    anchor_idx = int(self.rng.choice(top_indices))\n                else:\n                    anchor_idx = int(self.rng.integers(len(self.X_archive)))\n                anchor = np.array(self.X_archive[anchor_idx]).copy()\n            elif len(self.X_archive) > 0:\n                anchor = np.array(self.X_archive[self.rng.integers(len(self.X_archive))]).copy()\n            else:\n                anchor = self._uniform_array(lb, ub)\n\n            # propose a candidate via a mixture of strategies\n            r = self.rng.random()\n            cand = anchor.copy()\n            try:\n                if r < 0.18 and len(self.X_archive) >= 3:\n                    # DE-style: mutate anchor with difference of two others\n                    idxs = list(range(len(self.X_archive)))\n                    a_idx = None\n                    # pick two distinct\n                    p = self.rng.choice(idxs, size=2, replace=False)\n                    donor = np.array(self.X_archive[p[0]]) - np.array(self.X_archive[p[1]])\n                    F = 0.6 + 0.8 * self.rng.random()  # scale factor\n                    jitter = self.rng.normal(0.0, 0.02, size=self.dim) * per_dim_scale\n                    cand = anchor + F * donor + jitter\n                elif r < 0.45 and n_finite >= min(4, self.dim):\n                    # PCA-guided elite perturbation\n                    k_elite = max(3, min(30, n_finite // 2 + 1))\n                    # pick elites\n                    fin_idx_list = np.nonzero(finite_idx)[0]\n                    elite_idx = np.array(fin_idx_list)[np.argsort(finite_vals)[:k_elite]]\n                    X_elite = X_arr[elite_idx, :]\n                    mean_e = np.mean(X_elite, axis=0)\n                    Xc = X_elite - mean_e\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        var_explained = np.cumsum(S**2) / np.sum(S**2)\n                        ncomp = int(np.searchsorted(var_explained, 0.6) + 1)\n                        ncomp = max(1, min(ncomp, min(3, Vt.shape[0])))\n                        # sample coefficients along principal axes\n                        coeffs = self.rng.normal(0.0, 0.5, size=ncomp) * S[:ncomp]\n                        perturb = coeffs @ Vt[:ncomp, :]\n                        # scale perturbation to per-dim_scale and gscale\n                        alpha = 0.25 + 0.75 * self.rng.random()\n                        cand = mean_e + alpha * (perturb * (per_dim_scale / (np.std(X_elite, axis=0) + 1e-12)))\n                    except Exception:\n                        # fallback gaussian\n                        cand = anchor + self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale * 0.1\n                elif r < 0.85:\n                    # Local anisotropic gaussian\n                    # pick a subset of coordinates to perturb\n                    frac = 0.15 + 0.6 * self.rng.random()\n                    mask = self.rng.random(self.dim) < frac\n                    # anisotropic per-dim noise scaled by learned gscale and archive spread\n                    noise = self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_scale * self.gscale)\n                    cand = anchor + noise * mask\n                    # occasionally mix some coords uniform to escape ridges\n                    if self.rng.random() < 0.05:\n                        mix_idx = self.rng.choice(self.dim, size=max(1, int(0.05 * self.dim)), replace=False)\n                        cand[mix_idx] = lb[mix_idx] + self.rng.random(len(mix_idx)) * (ub[mix_idx] - lb[mix_idx])\n                else:\n                    # tempered Cauchy global escape around anchor\n                    scale_vec = np.maximum(per_dim_scale * 0.6, domain * 0.02)\n                    jump = self._tempered_cauchy(scale_vec, cap_multiplier=6.0)\n                    cand = anchor + jump\n                    # sometimes add a small differential jump for diversity\n                    if len(self.X_archive) >= 2 and self.rng.random() < 0.12:\n                        i, j = self.rng.choice(len(self.X_archive), size=2, replace=False)\n                        diff = np.array(self.X_archive[i]) - np.array(self.X_archive[j])\n                        cand += 0.2 * diff * self.rng.random()\n                # occasional direct small jump to current best to refine\n                if self.x_opt is not None and self.rng.random() < 0.03:\n                    cand = self.x_opt + self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_scale * 0.2)\n            except Exception:\n                # fallback robust candidate\n                cand = anchor + self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_scale * 0.2)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            if not np.all(np.isfinite(cand)):\n                bad = ~np.isfinite(cand)\n                cand[bad] = self._uniform_array(lb[bad], ub[bad]) if cand[bad].size == 1 else self.rng.uniform(lb[bad], ub[bad])\n            # reflect to bounds & clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            try:\n                f_cand = safe_eval(cand)\n            except StopIteration:\n                break\n\n            # record\n            self.X_archive.append(cand.copy()); self.F_archive.append(float(f_cand))\n            if np.isfinite(f_cand) and f_cand < self.f_opt:\n                self.f_opt = float(f_cand)\n                self.x_opt = cand.copy()\n                self.iter_since_improve = 0\n                self.success_window.append(1)\n            else:\n                self.iter_since_improve += 1\n                # success if candidate among top 25% of finite scores\n                try:\n                    combined = np.concatenate([finite_vals, [f_cand]])\n                    cutoff = np.percentile(combined, 25)\n                    self.success_window.append(1 if f_cand <= cutoff else 0)\n                except Exception:\n                    self.success_window.append(0)\n\n            # adaptation of gscale every so often\n            if iter_count % 8 == 0 and len(self.success_window) >= 6:\n                rate = float(sum(self.success_window)) / len(self.success_window)\n                if rate > 0.35:\n                    # too many successes -> refine\n                    self.gscale *= 0.92 * (0.95 + 0.1 * self.rng.random())\n                elif rate < 0.12:\n                    # too few successes -> increase exploration\n                    self.gscale *= 1.08 * (0.95 + 0.1 * self.rng.random())\n                else:\n                    # small random walk to maintain diversity\n                    self.gscale *= (1.0 + 0.03 * (self.rng.random(self.dim) - 0.5))\n                # clamp\n                self.gscale = np.clip(self.gscale, domain * 1e-6, domain * 0.6)\n\n            # prune archive if too large: keep best half + some randoms + a few infs\n            if len(self.X_archive) > self.archive_max:\n                F_arr2 = np.array(self.F_archive)\n                finite_mask2 = np.isfinite(F_arr2)\n                fin_idx2 = np.nonzero(finite_mask2)[0]\n                inf_idx = np.nonzero(~finite_mask2)[0].tolist()\n                keep = set()\n                if fin_idx2.size > 0:\n                    sorted_fin = fin_idx2[np.argsort(F_arr2[fin_idx2])]\n                    n_best = max(1, int(0.5 * len(sorted_fin)))\n                    keep.update(sorted_fin[:n_best].tolist())\n                    # keep some random others\n                    remaining_fin = sorted_fin[n_best:].tolist()\n                    nrand_keep = min(len(remaining_fin), max(0, int(self.archive_max * 0.1)))\n                    if nrand_keep > 0 and len(remaining_fin) > 0:\n                        sel = list(self.rng.choice(remaining_fin, size=nrand_keep, replace=False))\n                        keep.update(sel)\n                # keep some infinities as diversity\n                for ii in inf_idx[:max(0, self.archive_max - len(keep))]:\n                    keep.add(int(ii))\n                # fill rest with randoms if needed\n                all_idx = list(range(len(F_arr2)))\n                remain = list(set(all_idx) - keep)\n                while len(keep) < self.archive_max and len(remain) > 0:\n                    pick = int(self.rng.choice(remain))\n                    keep.add(pick)\n                    remain.remove(pick)\n                keep_list = sorted(list(keep))\n                self.X_archive = [self.X_archive[i] for i in keep_list]\n                self.F_archive = [self.F_archive[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if self.iter_since_improve >= stagnation_limit:\n                # micro-restart: generate a small local cloud around best and some randoms\n                best_since_restart = self.f_opt\n                n_local = min(20, max(5, int(10 + self.dim)))\n                n_global = min(8, max(2, int(5 + self.dim // 2)))\n                # create local samples around best (tight)\n                for _ in range(n_local):\n                    if self.eval_count >= self.budget:\n                        break\n                    if self.x_opt is None:\n                        xx = self._uniform_array(lb, ub)\n                    else:\n                        perturb = self.rng.normal(0.0, 1.0, size=self.dim) * (per_dim_scale * 0.08 * (1.0 + self.rng.random()))\n                        xx = self.x_opt + perturb\n                        xx = self._reflect_bounds(xx, lb, ub)\n                    try:\n                        fxx = safe_eval(xx)\n                    except StopIteration:\n                        break\n                    self.X_archive.append(xx.copy()); self.F_archive.append(float(fxx))\n                    if np.isfinite(fxx) and fxx < self.f_opt:\n                        self.f_opt = float(fxx); self.x_opt = xx.copy()\n                # some global randoms\n                for _ in range(n_global):\n                    if self.eval_count >= self.budget:\n                        break\n                    xx = self._uniform_array(lb, ub)\n                    try:\n                        fxx = safe_eval(xx)\n                    except StopIteration:\n                        break\n                    self.X_archive.append(xx.copy()); self.F_archive.append(float(fxx))\n                    if np.isfinite(fxx) and fxx < self.f_opt:\n                        self.f_opt = float(fxx); self.x_opt = xx.copy()\n                # after restart, slightly increase exploration scales\n                self.gscale = np.clip(self.gscale * (1.0 + 0.18 * self.rng.random(self.dim)), domain * 1e-6, domain * 0.6)\n                self.iter_since_improve = 0\n                self.success_window.clear()\n\n        # final best: choose best finite from archive\n        if len(self.F_archive) > 0:\n            F_arr_final = np.array(self.F_archive)\n            finite_mask = np.isfinite(F_arr_final)\n            if finite_mask.any():\n                idx = int(np.argmin(F_arr_final[finite_mask]))\n                fin_idx_list = np.nonzero(finite_mask)[0]\n                best_idx = fin_idx_list[idx]\n                return float(self.F_archive[best_idx]), np.array(self.X_archive[best_idx])\n            else:\n                # all inf/unusable, return best recorded values\n                if self.x_opt is not None:\n                    return float(self.f_opt), np.array(self.x_opt)\n                else:\n                    # fallback random point\n                    x = self._uniform_array(lb, ub)\n                    return float(func(x)), x\n        else:\n            # no evaluations were recorded (unlikely)\n            x = self._uniform_array(lb, ub)\n            f = func(x)\n            return float(f), x", "configspace": "", "generation": 0, "feedback": "In the code, line 31, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=50)", "error": "In the code, line 31, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "534d8a71-4938-468c-989f-e95293700647", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy global escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = int(archive_max)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # archives\n        self.X_archive = []  # list of np.array (vectors)\n        self.F_archive = []  # list of floats (may be np.inf)\n\n        # adaptive scales (fraction of domain width)\n        self.gscale = 0.2  # global scale (fraction of domain)\n        self.per_dim_scale = np.full(self.dim, 0.2)  # per-dim relative scales\n\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # short term success window for adaptation\n        self.success_window = []\n        self.success_window_size = max(20, int(0.01 * self.budget))\n\n        # stagnation detection\n        self.iter_since_improve = 0\n        self.last_restart_at = 0\n\n        # domain (defaults — BBOB standard [-5, 5])\n        self.lb = np.full(self.dim, -5.0)\n        self.ub = np.full(self.dim, 5.0)\n\n    # ---------------- utilities ----------------\n    def _reflect_bounds(self, x, lb, ub, max_iter=10):\n        # reflect repeatedly until inside; clamp if still outside\n        x = x.copy()\n        for _ in range(max_iter):\n            below = x < lb\n            if not np.any(below) and not np.any(x > ub):\n                break\n            # reflect below\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above\n            above = x > ub\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp if still outside\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # basic Latin hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        u = rng.rand(n, dim)\n        a = cut[:n]\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            perm = rng.permutation(n)\n            points[:, j] = a + u[:, j] * (1.0 / n)\n            points[:, j] = points[perm, j]\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size) if n > 1 else (lb.size,)\n        return self.rng.uniform(lb, ub, size=shape)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # tempered coordinate-wise Cauchy: draw standard cauchy but damp extreme tails\n        # scale_vec: absolute per-dim scales\n        u = self.rng.rand(self.dim)\n        # standard cauchy\n        c = np.tan(np.pi * (u - 0.5))\n        # temper extreme tails\n        cap = cap_multiplier\n        small_factor = 0.6\n        c_tempered = np.tanh(c / cap) * cap * small_factor\n        # occasionally allow a larger excursion\n        if self.rng.rand() < 0.05:\n            extra = (1.0 + self.rng.rand() * 2.0)\n            c_tempered *= extra\n        return c_tempered * np.asarray(scale_vec, dtype=float)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # attempt to get bounds from func, but default to class defaults\n        try:\n            # common patterns\n            if hasattr(func, \"ub\") and hasattr(func, \"lb\"):\n                ub = np.asarray(func.ub, dtype=float)\n                lb = np.asarray(func.lb, dtype=float)\n            elif hasattr(func, \"bounds\"):\n                b = func.bounds\n                # try attributes\n                if hasattr(b, \"ub\") and hasattr(b, \"lb\"):\n                    ub = np.asarray(b.ub, dtype=float)\n                    lb = np.asarray(b.lb, dtype=float)\n                elif hasattr(b, \"high\") and hasattr(b, \"low\"):\n                    ub = np.asarray(b.high, dtype=float)\n                    lb = np.asarray(b.low, dtype=float)\n                else:\n                    ub = self.ub\n                    lb = self.lb\n            else:\n                ub = self.ub\n                lb = self.lb\n        except Exception:\n            ub = self.ub\n            lb = self.lb\n\n        # ensure shapes\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size != self.dim:\n            lb = np.full(self.dim, lb[0] if lb.size > 0 else -5.0)\n        if ub.size != self.dim:\n            ub = np.full(self.dim, ub[0] if ub.size > 0 else 5.0)\n\n        domain = ub - lb\n        # clamp gscale to domain fraction\n        self.gscale = float(np.clip(self.gscale, 1e-6, 2.0))\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float).reshape(-1)\n            if self.evals >= self.budget:\n                # out of budget\n                raise StopIteration()\n            f = float(func(x))\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(20, 4 * self.dim), max(1, self.budget // 10))\n        n_init = min(n_init, self.budget)\n        X0 = self._lhs01(n_init, self.dim) if n_init > 1 else self._lhs01(1, self.dim)\n        # scale to bounds\n        X0 = lb + X0 * (ub - lb)\n        for s in range(n_init):\n            x = X0[s] if n_init > 1 else X0[0]\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            self.X_archive.append(np.array(x, dtype=float))\n            self.F_archive.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n                self.iter_since_improve = 0\n            else:\n                self.iter_since_improve += 1\n\n        # budget exhausted during init\n        if self.x_opt is None and self.evals >= self.budget:\n            # pick best from what we have or random fallback\n            if len(self.F_archive) > 0:\n                idx = int(np.argmin(np.array(self.F_archive)))\n                return self.F_archive[idx], self.X_archive[idx]\n            else:\n                x = self._uniform_array(lb, ub)\n                try:\n                    f = safe_eval(x)\n                except StopIteration:\n                    f = np.inf\n                return f, x\n\n        # add a couple of pure uniform draws for diversity\n        extras = min(5, self.budget - self.evals)\n        for _ in range(extras):\n            x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                break\n            self.X_archive.append(np.array(x, dtype=float))\n            self.F_archive.append(float(f))\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n                self.iter_since_improve = 0\n            else:\n                self.iter_since_improve += 1\n\n        # main loop: generate candidates until budget exhausted\n        try:\n            while self.evals < self.budget:\n                # convert archive to arrays for convenience\n                X_arr = np.array(self.X_archive) if len(self.X_archive) > 0 else np.zeros((0, self.dim))\n                F_arr = np.array(self.F_archive) if len(self.F_archive) > 0 else np.array([])\n                finite_idx = np.isfinite(F_arr)\n                n_finite = int(np.sum(finite_idx))\n\n                # robust per-dim scale using IQR of archive if possible\n                per_dim_scale = np.maximum(self.per_dim_scale, 1e-12)\n                if n_finite >= 4:\n                    finite_X = X_arr[finite_idx, :]\n                    q25 = np.percentile(finite_X, 25, axis=0)\n                    q75 = np.percentile(finite_X, 75, axis=0)\n                    iqr = (q75 - q25)\n                    # fallback to std if iqr zero\n                    small_mask = iqr <= 1e-12\n                    std = np.std(finite_X, axis=0)\n                    iqr[small_mask] = std[small_mask]\n                    # final fallback to domain fraction\n                    iqr_mask = iqr <= 1e-12\n                    iqr[iqr_mask] = domain[iqr_mask] * 0.25\n                    per_dim_scale = iqr / np.maximum(domain, 1e-12)\n                    # smooth with stored per_dim_scale\n                    self.per_dim_scale = 0.6 * self.per_dim_scale + 0.4 * per_dim_scale\n                else:\n                    # not enough history, keep current per_dim_scale but ensure bounds\n                    self.per_dim_scale = np.minimum(np.maximum(self.per_dim_scale, 1e-6), 2.0)\n\n                # choose anchor: bias to elites\n                if n_finite >= 4:\n                    sorted_idx = np.argsort(F_arr[finite_idx])\n                    fin_indices = np.nonzero(finite_idx)[0]\n                    fin_idx_list = fin_indices[sorted_idx]\n                    q = max(1, int(0.25 * len(fin_idx_list)))\n                    # biased selection: choose from top q with higher probability\n                    if self.rng.rand() < 0.85:\n                        pick = self.rng.choice(fin_idx_list[:q])\n                    else:\n                        pick = self.rng.choice(fin_idx_list)\n                    anchor = X_arr[pick].copy()\n                elif len(self.X_archive) > 0:\n                    anchor = X_arr[self.rng.randint(len(self.X_archive))].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n\n                # propose a candidate via a mixture of strategies\n                r = self.rng.rand()\n                cand = None\n                if r < 0.25 and len(self.X_archive) >= 3:\n                    # DE-style: mutate anchor with difference of two others\n                    # pick two distinct from archive (not anchor index if possible)\n                    idxs = list(range(len(self.X_archive)))\n                    if len(idxs) >= 3:\n                        # attempt to avoid choosing the index of anchor (if present)\n                        # find anchor index\n                        try:\n                            aidx = next(i for i, x in enumerate(self.X_archive) if np.allclose(x, anchor))\n                        except StopIteration:\n                            aidx = None\n                        cand_idx_pool = [i for i in idxs if i != aidx]\n                        a, b = self.rng.choice(cand_idx_pool, size=2, replace=False)\n                        donor = self.X_archive[a] - self.X_archive[b]\n                        # adaptive F factor\n                        F = 0.6 + self.rng.rand() * 0.8\n                        jitter = self.rng.normal(0.0, 0.02, size=self.dim) * (self.per_dim_scale * self.gscale)\n                        cand = anchor + F * donor + jitter\n                    else:\n                        cand = anchor + self.rng.normal(0, 0.1, size=self.dim) * (self.per_dim_scale * self.gscale)\n                elif r < 0.50 and n_finite >= 4:\n                    # PCA-guided elite perturbation\n                    k_elite = max(3, min( max(3, int(0.1 * max(4, n_finite))), n_finite))\n                    fin_idx_list = np.nonzero(finite_idx)[0][np.argsort(F_arr[finite_idx])]\n                    elite_idx = np.array(fin_idx_list[:k_elite], dtype=int)\n                    X_elite = X_arr[elite_idx, :]\n                    # center and SVD\n                    Xc = X_elite - X_elite.mean(axis=0, keepdims=True)\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        # use top components\n                        ncomp = max(1, min(self.dim, int(np.sum(S > 1e-8))))\n                        coeffs = self.rng.normal(0.0, 0.5, size=ncomp) * S[:ncomp]\n                        perturb = np.zeros(self.dim)\n                        for j in range(ncomp):\n                            perturb += coeffs[j] * Vt[j, :]\n                        # scale perturbation relative to per-dim_scale and gscale\n                        cand = anchor + perturb * (self.gscale * 0.6)\n                    except Exception:\n                        # fallback gaussian\n                        cand = anchor + self.rng.normal(0.0, 0.1, size=self.dim) * (self.per_dim_scale * self.gscale)\n                elif r < 0.85:\n                    # Local anisotropic gaussian\n                    # pick a subset of coordinates to perturb\n                    p_active = max(1.0 / self.dim, min(0.5, 0.2 + 0.3 * self.rng.rand()))\n                    mask = self.rng.rand(self.dim) < p_active\n                    noise = self.rng.normal(0.0, 1.0, size=self.dim) * (self.per_dim_scale * self.gscale)\n                    # mix some coords uniform to escape ridges occasionally\n                    if self.rng.rand() < 0.03:\n                        unif_mask = self.rng.rand(self.dim) < 0.2\n                        noise[unif_mask] = (self.rng.rand(np.sum(unif_mask)) - 0.5) * 2.0 * domain[unif_mask]\n                    cand = anchor + noise * mask\n                    # sometimes add a small differential jump for diversity\n                    if len(self.X_archive) >= 2 and self.rng.rand() < 0.08:\n                        a, b = self.rng.choice(len(self.X_archive), size=2, replace=False)\n                        cand += 0.15 * (self.X_archive[a] - self.X_archive[b])\n                else:\n                    # tempered Cauchy global escape around anchor\n                    scale_vec = self.per_dim_scale * domain * self.gscale\n                    cand = anchor + self._tempered_cauchy(scale_vec)\n                    # sometimes add a small pull to best to help refinement\n                    if self.x_opt is not None and self.rng.rand() < 0.2:\n                        cand = 0.6 * cand + 0.4 * self.x_opt\n\n                # fallback robust candidate if something went wrong\n                if cand is None or cand.shape[0] != self.dim:\n                    cand = anchor + self.rng.normal(0.0, 0.1, size=self.dim) * (self.per_dim_scale * self.gscale)\n\n                # safety: replace NaN/inf coords with uniform random within bounds\n                bad = ~np.isfinite(cand)\n                if np.any(bad):\n                    cand[bad] = lb[bad] + self.rng.rand(np.sum(bad)) * (ub[bad] - lb[bad])\n\n                # reflect to bounds & clamp\n                cand = self._reflect_bounds(cand, lb, ub)\n\n                # evaluate candidate\n                try:\n                    f_cand = safe_eval(cand)\n                except StopIteration:\n                    break\n\n                # record\n                self.X_archive.append(np.array(cand, dtype=float))\n                self.F_archive.append(float(f_cand))\n\n                # update best\n                if np.isfinite(f_cand) and f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = np.array(cand, dtype=float)\n                    self.iter_since_improve = 0\n                    self.success_window.append(1)\n                else:\n                    self.iter_since_improve += 1\n                    self.success_window.append(0)\n\n                # maintain success window length\n                if len(self.success_window) > self.success_window_size:\n                    self.success_window.pop(0)\n\n                # adaptation of gscale every so often (short-term)\n                if len(self.success_window) >= 5 and (len(self.success_window) % 5 == 0):\n                    succ_rate = sum(self.success_window) / max(1, len(self.success_window))\n                    if succ_rate > 0.25:\n                        # too many successes -> refine\n                        self.gscale *= 0.85\n                    elif succ_rate < 0.05:\n                        # too few successes -> increase exploration\n                        self.gscale *= 1.15\n                    else:\n                        # small random walk to maintain diversity\n                        self.gscale *= (1.0 + (self.rng.rand() - 0.5) * 0.05)\n                    # clamp\n                    self.gscale = float(np.clip(self.gscale, 1e-6, 2.0))\n                    # also nudge per-dim scales slightly\n                    self.per_dim_scale *= (1.0 + (self.rng.rand(self.dim) - 0.5) * 0.05)\n                    self.per_dim_scale = np.minimum(np.maximum(self.per_dim_scale, 1e-6), 2.0)\n\n                # prune archive if too large: keep best half + some randoms + a few infs\n                if len(self.F_archive) > 2 * self.archive_max:\n                    F_arr = np.array(self.F_archive)\n                    finite_mask2 = np.isfinite(F_arr)\n                    fin_idx2 = np.nonzero(finite_mask2)[0]\n                    # keep best half of finite\n                    keep = set()\n                    if fin_idx2.size > 0:\n                        sorted_fin = fin_idx2[np.argsort(F_arr[fin_idx2])]\n                        n_keep_best = max(1, len(sorted_fin) // 2)\n                        keep.update(sorted_fin[:n_keep_best].tolist())\n                        # keep some random others\n                        others = sorted_fin[n_keep_best:]\n                        if len(others) > 0:\n                            n_rand_keep = min(len(others), max(1, len(others) // 4))\n                            keep.update(self.rng.choice(others, size=n_rand_keep, replace=False).tolist())\n                    # keep some infinities as diversity\n                    inf_idx = np.nonzero(~finite_mask2)[0]\n                    if len(inf_idx) > 0:\n                        n_inf_keep = min(len(inf_idx), 5)\n                        keep.update(self.rng.choice(inf_idx, size=n_inf_keep, replace=False).tolist())\n                    # fill rest from random if needed\n                    all_idx = set(range(len(self.F_archive)))\n                    remaining = list(all_idx - keep)\n                    # randomly sample to reduce size\n                    to_keep = list(keep) + self.rng.choice(remaining, size=min(len(remaining), self.archive_max - len(keep)), replace=False).tolist()\n                    # rebuild archives\n                    self.X_archive = [self.X_archive[i] for i in to_keep]\n                    self.F_archive = [self.F_archive[i] for i in to_keep]\n\n                # stagnation detection & micro-restarts\n                if (self.iter_since_improve > max(50, int(0.02 * self.budget))\n                    and (self.evals - self.last_restart_at) > max(50, int(0.02 * self.budget))):\n                    # micro-restart: generate a small local cloud around best and some randoms\n                    self.last_restart_at = self.evals\n                    self.gscale = min(2.0, self.gscale * 1.2)  # slightly increase exploration\n                    n_local = min(12, max(4, self.dim * 2))\n                    local_sigma = self.per_dim_scale * domain * max(0.01, 0.05 * (1.0 + self.rng.rand()))\n                    for _ in range(n_local):\n                        if self.x_opt is not None and np.isfinite(self.f_opt):\n                            x = self.x_opt + self.rng.normal(0.0, 1.0, size=self.dim) * local_sigma\n                        else:\n                            x = self._uniform_array(lb, ub)\n                        x = self._reflect_bounds(x, lb, ub)\n                        try:\n                            f = safe_eval(x)\n                        except StopIteration:\n                            break\n                        self.X_archive.append(np.array(x, dtype=float))\n                        self.F_archive.append(float(f))\n                        if np.isfinite(f) and f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = np.array(x, dtype=float)\n                            self.iter_since_improve = 0\n                    # some global randoms\n                    for _ in range(6):\n                        x = self._uniform_array(lb, ub)\n                        try:\n                            f = safe_eval(x)\n                        except StopIteration:\n                            break\n                        self.X_archive.append(np.array(x, dtype=float))\n                        self.F_archive.append(float(f))\n\n                # enforce archive size limit\n                if len(self.F_archive) > self.archive_max:\n                    # keep best archive_max items (by F if finite, otherwise random)\n                    F_arr = np.array(self.F_archive)\n                    fin_mask = np.isfinite(F_arr)\n                    if np.any(fin_mask):\n                        fin_idx = np.where(fin_mask)[0]\n                        best_fin_idx = fin_idx[np.argsort(F_arr[fin_idx])][:self.archive_max]\n                        # if not enough finite, fill with some random from remaining\n                        if len(best_fin_idx) < self.archive_max:\n                            remaining = list(set(range(len(self.F_archive))) - set(best_fin_idx))\n                            extra = self.rng.choice(remaining, size=min(len(remaining), self.archive_max - len(best_fin_idx)), replace=False)\n                            keep_idx = np.concatenate([best_fin_idx, extra])\n                        else:\n                            keep_idx = best_fin_idx\n                    else:\n                        # all are inf: keep random subset\n                        keep_idx = self.rng.choice(range(len(self.F_archive)), size=self.archive_max, replace=False)\n                    self.X_archive = [self.X_archive[i] for i in keep_idx]\n                    self.F_archive = [self.F_archive[i] for i in keep_idx]\n\n            # main loop ends when budget exhausted\n        except StopIteration:\n            pass\n\n        # final best: choose best finite from archive\n        if len(self.F_archive) > 0:\n            F_arr = np.array(self.F_archive)\n            finite_mask = np.isfinite(F_arr)\n            if np.any(finite_mask):\n                fin_idx_list = np.nonzero(finite_mask)[0]\n                idx = int(np.argmin(F_arr[fin_idx_list]))\n                best_idx = fin_idx_list[idx]\n                return float(self.F_archive[best_idx]), np.array(self.X_archive[best_idx])\n            else:\n                # all inf/unusable, return best recorded values if any\n                if self.x_opt is not None:\n                    return float(self.f_opt), np.array(self.x_opt)\n                else:\n                    # fallback random point\n                    x = self._uniform_array(lb, ub)\n                    try:\n                        f = safe_eval(x)\n                    except StopIteration:\n                        f = np.inf\n                    return f, x\n        else:\n            # no evaluations were recorded (unlikely)\n            x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except StopIteration:\n                f = np.inf\n            return f, x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6e6d90c1-a39a-4fc6-9768-57f9ee614825", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # adaptive global scale (fraction of domain)\n        self.gscale = 0.2  # will be clamped to sensible domain fraction later\n\n        # bookkeeping\n        self.evals = 0\n        self.archive = []  # list of tuples (x (np.array), f (float))\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows\n        self.success_window = []\n        self.window_len = max(10, min(200, self.budget // 20))\n        self.last_improve_at = 0\n        self.stagnation_thresh = max(50, self.budget // 20)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try multiple common conventions\n        # 1) func.bounds as object with lb/ub or array-like\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # common case: object with lb, ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    arr = np.asarray(b)\n                    if arr.shape == (2,):\n                        lb = np.full(self.dim, float(arr[0]))\n                        ub = np.full(self.dim, float(arr[1]))\n                    elif arr.shape == (2, self.dim):\n                        lb = np.asarray(arr[0], dtype=float)\n                        ub = np.asarray(arr[1], dtype=float)\n                    else:\n                        raise ValueError\n                if lb.size == 1:\n                    lb = np.full(self.dim, lb.item())\n                if ub.size == 1:\n                    ub = np.full(self.dim, ub.item())\n                return lb, ub\n        except Exception:\n            pass\n\n        # 2) try attributes like lower_bounds, upper_bounds\n        for lattr, uattr in ((\"lower_bounds\", \"upper_bounds\"), (\"lb\", \"ub\"), (\"xmin\", \"xmax\")):\n            try:\n                lb = getattr(func, lattr, None)\n                ub = getattr(func, uattr, None)\n                if lb is not None and ub is not None:\n                    lb = np.asarray(lb, dtype=float)\n                    ub = np.asarray(ub, dtype=float)\n                    if lb.size == 1:\n                        lb = np.full(self.dim, lb.item())\n                    if ub.size == 1:\n                        ub = np.full(self.dim, ub.item())\n                    return lb, ub\n            except Exception:\n                pass\n\n        # fallback to [-5,5]\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=3):\n        # reflect repeatedly until inside; then clamp final\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        u = rng.rand(n, dim)\n        points = np.empty((n, dim))\n        for j in range(dim):\n            points[:, j] = cut[:-1] + u[:, j] * (cut[1] - cut[0])\n            rng.shuffle(points[:, j])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        shape = (n, lb.size)\n        return self.rng.uniform(lb, ub, size=shape)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # tempered coordinate-wise Cauchy\n        # draw uniform then map to cauchy\n        u = self.rng.rand(self.dim)\n        # standard Cauchy via tan for u in (0,1)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper extreme tails with tanh scaling\n        tempered = np.tanh(c / cap_multiplier) * cap_multiplier\n        # occasionally allow a full heavy tail multiplication\n        if self.rng.rand() < 0.05:\n            tempered = c  # full heavy tail rarely\n        # return scaled by scale_vec (abs)\n        return tempered * (np.abs(scale_vec) + 1e-12)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        # clamp global scale to domain fraction\n        domain_width = ub - lb\n        self.gscale = float(np.clip(self.gscale, 1e-6, 1.0))\n        self.gscale = float(np.clip(self.gscale, 1e-6, 1.0))\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # ensure 1d array\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            if self.evals >= self.budget:\n                return np.inf\n            # call function\n            fx = func(x)\n            # allow only finite numbers to be accepted in archive; but record inf too\n            self.evals += 1\n            return float(fx)\n\n        # ---- initialization: LHS samples ----\n        remaining = self.budget - self.evals\n        n_init = min(max(10, 6 * self.dim), max(1, remaining // 8))\n        init_pts = min(n_init, remaining)\n        if init_pts <= 0:\n            # nothing to evaluate\n            if self.best_x is None:\n                x0 = self._uniform_array(lb, ub)\n                return safe_eval(x0), x0\n            return self.best_f, self.best_x\n\n        lhs = self._lhs01(init_pts, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(X0.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n        # add a couple pure uniforms\n        for _ in range(min(3, self.budget - self.evals)):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if not enough initial samples, fall back to random draws until small pool\n        if len(self.archive) == 0:\n            x0 = self._uniform_array(lb, ub)\n            f0 = safe_eval(x0)\n            self.archive.append((x0.copy(), f0))\n            self.best_f = f0\n            self.best_x = x0.copy()\n\n        # main loop: generate candidates until budget exhausted\n        while self.evals < self.budget:\n            remaining = self.budget - self.evals\n\n            # convert archive to arrays\n            arr_X = np.array([x for x, f in self.archive])\n            arr_f = np.array([f for x, f in self.archive])\n\n            # robust per-dim scale using IQR or std\n            per_dim_scale = np.zeros(self.dim)\n            if arr_X.shape[0] >= 4:\n                q75 = np.percentile(arr_X, 75, axis=0)\n                q25 = np.percentile(arr_X, 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr / (1.349)  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n                per_dim_scale = np.where(per_dim_scale <= 0, domain_width * 0.05, per_dim_scale)\n            else:\n                per_dim_scale = domain_width * 0.2\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            if np.any(finite_mask):\n                finite_idxs = np.where(finite_mask)[0]\n                sorted_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])]\n            else:\n                sorted_idxs = np.arange(arr_X.shape[0])\n            topk = max(1, int(max(1, 0.15 * len(sorted_idxs))))\n            if len(sorted_idxs) == 0:\n                anchor = self._uniform_array(lb, ub)\n            else:\n                # biased sampling: choose among topk with exponential bias\n                if self.rng.rand() < 0.8 and len(sorted_idxs) > 0:\n                    candidates = sorted_idxs[:max(1, topk)]\n                    weights = np.exp(-np.arange(len(candidates)) / max(1, len(candidates)/3.0))\n                    weights = weights / weights.sum()\n                    idx = self.rng.choice(candidates, p=weights)\n                else:\n                    idx = self.rng.choice(sorted_idxs)\n                anchor = arr_X[idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            x_new = None\n\n            # Strategy selection probabilities adaptive-ish\n            p = self.rng.rand()\n            # DE-style differential injection\n            if p < 0.25 and arr_X.shape[0] >= 3:\n                # pick two distinct others\n                idxs = list(range(arr_X.shape[0]))\n                if len(idxs) >= 3:\n                    a = self.rng.choice(idxs)\n                    b = self.rng.choice([i for i in idxs if i != a])\n                    c = self.rng.choice([i for i in idxs if i != a and i != b])\n                    xa = arr_X[a]\n                    xb = arr_X[b]\n                    xc = arr_X[c]\n                    F = 0.5 + 0.5 * self.rng.rand()  # differential factor [0.5,1.0]\n                    candidate = anchor + F * (xb - xc)\n                    # small gaussian jitter\n                    candidate += self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.2)\n                    x_new = candidate\n            # PCA-guided elite perturbation\n            elif p < 0.55 and arr_X.shape[0] >= max(3, 2*self.dim):\n                elite_count = max(3, int(0.2 * arr_X.shape[0]))\n                finite_idxs = np.where(np.isfinite(arr_f))[0]\n                if len(finite_idxs) >= elite_count:\n                    elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])[:elite_count]]\n                else:\n                    elite_idxs = np.arange(min(elite_count, arr_X.shape[0]))\n                elites = arr_X[elite_idxs]\n                # center elites\n                mu = np.mean(elites, axis=0)\n                cov = np.cov((elites - mu).T) + np.eye(self.dim) * 1e-8\n                try:\n                    vals, vecs = np.linalg.eigh(cov)\n                    # sample coefficients along principal axes with variance proportional to eigenvalues\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(np.maximum(vals, 1e-12))\n                    scale = self.gscale * (0.5 + self.rng.rand() * 1.5)\n                    perturb = vecs.dot(coeffs) * scale\n                    candidate = anchor + perturb * (per_dim_scale / (domain_width + 1e-12))\n                    x_new = candidate\n                except Exception:\n                    x_new = anchor + self.rng.randn(self.dim) * per_dim_scale * self.gscale * 0.5\n            # Local anisotropic gaussian\n            elif p < 0.85:\n                # pick subset of coordinates to perturb (sparsity)\n                prob_coord = 0.3 + 0.5 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not np.any(coords):\n                    coords[self.rng.randint(0, self.dim)] = True\n                noise = np.zeros(self.dim)\n                # anisotropic noise scaled by per-dim scale\n                noise[coords] = self.rng.randn(np.count_nonzero(coords)) * (per_dim_scale[coords] * self.gscale)\n                # occasionally mix some dims uniform\n                if self.rng.rand() < 0.12:\n                    mix_coords = self.rng.rand(self.dim) < 0.06\n                    if np.any(mix_coords):\n                        noise[mix_coords] = (self._uniform_array(lb[mix_coords], ub[mix_coords]) - anchor[mix_coords])\n                candidate = anchor + noise\n                x_new = candidate\n            # tempered Cauchy global escape\n            else:\n                scale_vec = domain_width * (self.gscale * (0.8 + 1.2 * self.rng.rand()))\n                candidate = anchor + self._tempered_cauchy(scale_vec)\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if self.rng.rand() < 0.08 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.1 * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.07:\n                tiny = (self.rng.randn(self.dim) * (domain_width * 1e-3 * max(1.0, (self.best_f if np.isfinite(self.best_f) else 1.0))))\n                x_new = self.best_x + tiny\n\n            # fallback robust candidate from heterogenous mix\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * per_dim_scale * self.gscale\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad = ~np.isfinite(x_new)\n            if np.any(bad):\n                x_new[bad] = self._uniform_array(lb[bad], ub[bad])\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate\n            f_new = safe_eval(x_new)\n            self.archive.append((x_new.copy(), f_new))\n\n            # record success if candidate among top 25% of finite scores in current pool\n            finite_fs = arr_f[np.isfinite(arr_f)] if arr_f.size > 0 else np.array([])\n            if np.isfinite(f_new):\n                if f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                # compute success relative to archive\n                if finite_fs.size > 0:\n                    threshold = np.percentile(finite_fs, 25)\n                    success = (f_new <= threshold)\n                else:\n                    success = True\n            else:\n                success = False\n\n            self.success_window.append(1 if success else 0)\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every so often\n            if len(self.success_window) == self.window_len:\n                succ_rate = np.mean(self.success_window)\n                # too many successes -> refine (decrease)\n                if succ_rate > 0.3:\n                    self.gscale *= 0.85\n                # too few successes -> increase exploration\n                elif succ_rate < 0.08:\n                    self.gscale *= 1.25\n                else:\n                    # small random walk to maintain diversity\n                    self.gscale *= (0.95 + 0.1 * self.rng.rand())\n                # clamp relative to domain\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1.5))\n                # reset window sometimes to avoid lock-in\n                if self.rng.rand() < 0.02:\n                    self.success_window = []\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                # keep best half plus some randoms and a few infinities\n                arr_X = np.array([x for x, f in self.archive])\n                arr_f = np.array([f for x, f in self.archive])\n                idxs = np.argsort(arr_f, kind='mergesort')\n                keep_best = max(1, len(idxs) // 2)\n                kept = set(int(i) for i in idxs[:keep_best])\n                # keep some random others\n                others = [i for i in range(len(idxs)) if i not in kept]\n                self.rng.shuffle(others)\n                keep_rand = min(len(others), max(1, self.archive_max - keep_best - 3))\n                kept.update(others[:keep_rand])\n                # keep up to 3 infinities for diversity\n                inf_idxs = [i for i, f in enumerate(arr_f) if not np.isfinite(f)]\n                for ii in inf_idxs[:3]:\n                    kept.add(int(ii))\n                new_archive = [self.archive[i] for i in sorted(kept)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_thresh and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min( max(3, self.dim), self.budget - self.evals)\n                base = self.best_x if self.best_x is not None else self._uniform_array(lb, ub)\n                for _ in range(n_local):\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * max(0.02, self.gscale))\n                    cand = base + jitter\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # a few global randoms to regain diversity\n                n_global = min(5, self.budget - self.evals)\n                for _ in range(n_global):\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = min(1.0, self.gscale * (1.2 + 0.3 * self.rng.rand()))\n                # reset stagnation threshold a bit\n                self.last_improve_at = self.evals\n\n        # final best: choose best finite from archive\n        if self.best_x is None:\n            # pick best finite in archive\n            finite = [(x, f) for x, f in self.archive if np.isfinite(f)]\n            if len(finite) > 0:\n                best = min(finite, key=lambda t: t[1])\n                self.best_x, self.best_f = best[0].copy(), best[1]\n            else:\n                # all inf/unusable, return a random point\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.best_x = x.copy()\n                self.best_f = f\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "21b75cdc-c7df-4f61-af69-e0b202ce0b8d", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven adaptive hybrid sampler combining DE-style differential injections, PCA-guided elite moves, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # adaptive global scale (fraction of domain)\n        self.gscale = 0.2  # will be clamped to sensible domain fraction later\n\n        # bookkeeping\n        self.evals = 0\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows\n        self.success_window = []\n        self.window_len = max(10, min(200, max(10, self.budget // 20)))\n        self.last_improve_at = 0\n        self.stagnation_thresh = max(50, max(10, self.budget // 20))\n\n        # archive\n        self.archive = []\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try multiple common conventions; fallback to [-5,5]\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # object with lb/ub or 2xD array\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    arr = np.asarray(b)\n                    if arr.ndim == 1 and arr.size == 2:\n                        lb = np.full(self.dim, float(arr[0]))\n                        ub = np.full(self.dim, float(arr[1]))\n                    elif arr.shape == (2, self.dim):\n                        lb = np.asarray(arr[0], dtype=float)\n                        ub = np.asarray(arr[1], dtype=float)\n                    else:\n                        raise ValueError\n                if lb.size == 1:\n                    lb = np.full(self.dim, lb.item())\n                if ub.size == 1:\n                    ub = np.full(self.dim, ub.item())\n                return lb, ub\n        except Exception:\n            pass\n\n        for lattr, uattr in ((\"lower_bounds\", \"upper_bounds\"), (\"lb\", \"ub\"), (\"xmin\", \"xmax\")):\n            try:\n                lb = getattr(func, lattr, None)\n                ub = getattr(func, uattr, None)\n                if lb is not None and ub is not None:\n                    lb = np.asarray(lb, dtype=float)\n                    ub = np.asarray(ub, dtype=float)\n                    if lb.size == 1:\n                        lb = np.full(self.dim, lb.item())\n                    if ub.size == 1:\n                        ub = np.full(self.dim, ub.item())\n                    return lb, ub\n            except Exception:\n                pass\n\n        # fallback to [-5,5]\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=3):\n        # reflect repeatedly until inside; then clamp final\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        u = rng.rand(n, dim)\n        points = np.empty((n, dim))\n        for j in range(dim):\n            points[:, j] = cut[:-1] + u[:, j] * (cut[1] - cut[0])\n        # shuffle per column\n        for j in range(dim):\n            rng.shuffle(points[:, j])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        shape = (n, lb.size)\n        return self.rng.uniform(lb, ub, size=shape)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # tempered coordinate-wise Cauchy\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        tempered = np.tanh(c / cap_multiplier) * cap_multiplier\n        # occasionally allow a full heavy tail multiplication\n        if self.rng.rand() < 0.05:\n            tempered = c  # full heavy tail rarely\n        return tempered * (np.abs(scale_vec) + 1e-12)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        domain_width = ub - lb\n        # clamp global scale to domain fraction\n        self.gscale = float(np.clip(self.gscale, 1e-6, 1.0))\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                val = float(func(x))\n            except Exception:\n                val = np.inf\n            self.evals += 1\n            return val\n\n        # ---- initialization: LHS samples ----\n        remaining = max(0, self.budget - self.evals)\n        n_init = min(max(10, 6 * self.dim), max(1, remaining // 8))\n        init_pts = min(n_init, remaining)\n        if init_pts <= 0:\n            x0 = self._uniform_array(lb, ub)\n            f0 = safe_eval(x0)\n            return f0, x0\n\n        lhs = self._lhs01(init_pts, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(X0.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n        # add a couple pure uniforms\n        for _ in range(min(3, self.budget - self.evals)):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if archive empty, ensure at least one\n        if len(self.archive) == 0:\n            x0 = self._uniform_array(lb, ub)\n            f0 = safe_eval(x0)\n            self.archive.append((x0.copy(), f0))\n            if np.isfinite(f0):\n                self.best_x = x0.copy()\n                self.best_f = f0\n\n        # main loop: generate candidates until budget exhausted\n        while self.evals < self.budget:\n            remaining = self.budget - self.evals\n\n            # convert archive to arrays\n            arr_X = np.array([x for x, f in self.archive])\n            arr_f = np.array([f for x, f in self.archive])\n\n            # robust per-dim scale using IQR or std\n            if arr_X.shape[0] >= 4:\n                q25 = np.percentile(arr_X, 25, axis=0)\n                q75 = np.percentile(arr_X, 75, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr.copy()\n                small_mask = per_dim_scale <= 1e-12\n                if np.any(small_mask):\n                    per_dim_scale[small_mask] = np.std(arr_X[:, small_mask], axis=0) + 1e-12\n                per_dim_scale = np.where(per_dim_scale <= 0, domain_width * 0.05, per_dim_scale)\n            else:\n                per_dim_scale = domain_width * 0.2\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            if np.any(finite_mask):\n                finite_idxs = np.where(finite_mask)[0]\n                sorted_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])]\n            else:\n                sorted_idxs = np.arange(arr_X.shape[0])\n\n            topk = max(1, int(max(1, 0.15 * len(sorted_idxs))))\n            if len(sorted_idxs) == 0:\n                anchor = self._uniform_array(lb, ub)\n            else:\n                # biased sampling: choose among topk with exponential bias\n                if self.rng.rand() < 0.8 and len(sorted_idxs) > 0:\n                    candidates = sorted_idxs[:max(1, topk)]\n                    weights = np.exp(-np.arange(len(candidates)) / max(1, len(candidates) / 3.0))\n                    weights = weights / weights.sum()\n                    idx = int(self.rng.choice(candidates, p=weights))\n                else:\n                    idx = int(self.rng.choice(sorted_idxs))\n                anchor = arr_X[idx].copy()\n\n            x_new = None\n            # propose a candidate via a mixture of strategies\n            p = self.rng.rand()\n            # DE-style differential injection\n            if p < 0.25 and arr_X.shape[0] >= 3:\n                # pick three distinct others (not anchor index if possible)\n                idxs = list(range(arr_X.shape[0]))\n                if len(idxs) >= 3:\n                    choices = self.rng.choice(idxs, size=3, replace=False)\n                    xa = arr_X[choices[0]]\n                    xb = arr_X[choices[1]]\n                    xc = arr_X[choices[2]]\n                else:\n                    xa = xb = xc = anchor\n                F = 0.5 + 0.5 * self.rng.rand()  # differential factor [0.5,1.0]\n                candidate = anchor + F * (xb - xc)\n                # small gaussian jitter\n                candidate += self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.2)\n                x_new = candidate\n            # PCA-guided elite perturbation\n            elif p < 0.55 and arr_X.shape[0] >= max(3, 2 * self.dim):\n                elite_count = max(3, int(0.2 * arr_X.shape[0]))\n                finite_idxs = np.where(np.isfinite(arr_f))[0]\n                if len(finite_idxs) >= elite_count:\n                    elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])[:elite_count]]\n                else:\n                    elite_idxs = np.arange(min(elite_count, arr_X.shape[0]))\n                elites = arr_X[elite_idxs]\n                mu = np.mean(elites, axis=0)\n                cov = np.cov((elites - mu).T) + np.eye(self.dim) * 1e-8\n                try:\n                    vals, vecs = np.linalg.eigh(cov)\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(np.maximum(vals, 1e-12))\n                    scale = self.gscale * (0.5 + self.rng.rand() * 1.5)\n                    perturb = vecs.dot(coeffs) * scale\n                    candidate = anchor + perturb * (per_dim_scale / (domain_width + 1e-12))\n                    x_new = candidate\n                except Exception:\n                    x_new = anchor + self.rng.randn(self.dim) * per_dim_scale * self.gscale * 0.5\n            # Local anisotropic gaussian\n            elif p < 0.85:\n                prob_coord = 0.3 + 0.5 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not np.any(coords):\n                    coords[self.rng.randint(0, self.dim)] = True\n                noise = np.zeros(self.dim)\n                noise[coords] = self.rng.randn(np.count_nonzero(coords)) * (per_dim_scale[coords] * self.gscale)\n                # occasionally mix some dims uniform\n                if self.rng.rand() < 0.12:\n                    mix_coords = self.rng.rand(self.dim) < 0.06\n                    if np.any(mix_coords):\n                        u = self._uniform_array(lb[mix_coords], ub[mix_coords])\n                        noise[mix_coords] = (u - anchor[mix_coords])\n                x_new = anchor + noise\n            # tempered Cauchy global escape\n            else:\n                scale_vec = domain_width * (self.gscale * (0.8 + 1.2 * self.rng.rand()))\n                candidate = anchor + self._tempered_cauchy(scale_vec)\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if self.rng.rand() < 0.08 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.1 * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.07:\n                tiny = (self.rng.randn(self.dim) * (domain_width * 1e-3 * max(1.0, (self.best_f if np.isfinite(self.best_f) else 1.0))))\n                x_new = self.best_x + tiny\n\n            # fallback robust candidate from heterogenous mix\n            if x_new is None:\n                # mixture: combine local gaussian + one tempered coordinate\n                coords = self.rng.rand(self.dim) < 0.5\n                noise = np.zeros(self.dim)\n                noise[coords] = self.rng.randn(np.count_nonzero(coords)) * (per_dim_scale[coords] * self.gscale)\n                temp = self._tempered_cauchy(domain_width * self.gscale)\n                x_new = anchor + noise + 0.2 * temp\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad = ~np.isfinite(x_new)\n            if np.any(bad):\n                # per-coordinate uniform fills\n                for j in np.where(bad)[0]:\n                    x_new[j] = self.rng.uniform(lb[j], ub[j])\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate\n            f_new = safe_eval(x_new)\n            self.archive.append((x_new.copy(), f_new))\n\n            # record success if candidate among top 25% of finite scores in current pool\n            finite_fs = arr_f[np.isfinite(arr_f)] if arr_f.size > 0 else np.array([])\n            if np.isfinite(f_new):\n                if f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                # compute success relative to archive\n                if finite_fs.size > 0:\n                    threshold = np.percentile(finite_fs, 25)\n                    success = (f_new <= threshold)\n                else:\n                    success = True  # any finite when previously none is success\n            else:\n                success = False\n\n            self.success_window.append(1 if success else 0)\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every so often\n            if len(self.success_window) == self.window_len:\n                succ_rate = np.mean(self.success_window)\n                # too many successes -> refine (decrease)\n                if succ_rate > 0.3:\n                    self.gscale *= 0.85\n                # too few successes -> increase exploration\n                elif succ_rate < 0.15:\n                    self.gscale *= 1.25\n                else:\n                    # small random walk to maintain diversity\n                    self.gscale *= (0.95 + 0.1 * self.rng.rand())\n                # clamp relative to domain\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1.5))\n                # reset window sometimes to avoid lock-in\n                if self.rng.rand() < 0.02:\n                    self.success_window = []\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                arr_X = np.array([x for x, f in self.archive])\n                arr_f = np.array([f for x, f in self.archive])\n                idxs = np.argsort(arr_f, kind='mergesort')\n                keep_best = max(1, len(idxs) // 2)\n                kept = set(int(i) for i in idxs[:keep_best])\n                # keep some random others to maintain diversity\n                others = [i for i in range(len(idxs)) if i not in kept]\n                self.rng.shuffle(others)\n                keep_rand = min(len(others), max(1, self.archive_max - keep_best - 3))\n                kept.update(others[:keep_rand])\n                # keep up to 3 infinities for exploration signals\n                inf_idxs = [i for i, f in enumerate(arr_f) if not np.isfinite(f)]\n                for ii in inf_idxs[:3]:\n                    kept.add(int(ii))\n                new_archive = [self.archive[i] for i in sorted(kept)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_thresh and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(max(3, self.dim), self.budget - self.evals)\n                base = self.best_x if self.best_x is not None else self._uniform_array(lb, ub)\n                for _ in range(n_local):\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * max(0.02, self.gscale))\n                    cand = base + jitter\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # a few global randoms to regain diversity\n                n_global = min(5, self.budget - self.evals)\n                for _ in range(n_global):\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = min(1.0, self.gscale * (1.2 + 0.3 * self.rng.rand()))\n                # reset stagnation baseline\n                self.last_improve_at = self.evals\n\n        # final best: ensure best_x, best_f set from archive if needed\n        if self.best_x is None or not np.isfinite(self.best_f):\n            finite = [(x, f) for x, f in self.archive if np.isfinite(f)]\n            if len(finite) > 0:\n                best = min(finite, key=lambda t: t[1])\n                self.best_x, self.best_f = best[0].copy(), best[1]\n            else:\n                # all inf/unusable, return a random point\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.best_x = x.copy()\n                self.best_f = f\n\n        return float(self.best_f), self.best_x.copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "840a3af3-553d-4d21-a9de-8491b0141028", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS)", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # adaptive global scale (fraction of domain)\n        self.gscale = 0.2  # domain fraction, adapted during run\n\n        # bookkeeping\n        self.archive = []  # list of tuples (x (np.array), f (float))\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows\n        self.success_window = []\n        self.window_len = max(10, min(200, self.budget // 20))\n        self.last_improve_at = 0\n        self.stagnation_thresh = max(50, self.budget // 20)\n\n        # internal eval counter\n        self.evals = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try multiple common conventions; fallback to [-5,5]\n        # Many BBOB tasks use [-5,5] by default.\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # case: object with .lb and .ub\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                else:\n                    # maybe array-like\n                    arr = np.asarray(b, dtype=float)\n                    if arr.ndim == 1 and arr.size == 2:\n                        # scalar lower/upper\n                        lb = np.full(self.dim, float(arr[0]))\n                        ub = np.full(self.dim, float(arr[1]))\n                    elif arr.shape == (2, self.dim):\n                        lb = np.asarray(arr[0], dtype=float)\n                        ub = np.asarray(arr[1], dtype=float)\n                    else:\n                        raise ValueError(\"unknown bounds format\")\n                if lb.size == 1:\n                    lb = np.full(self.dim, lb.item())\n                if ub.size == 1:\n                    ub = np.full(self.dim, ub.item())\n                return lb, ub\n        except Exception:\n            pass\n\n        for lattr, uattr in ((\"lower_bounds\", \"upper_bounds\"), (\"lb\", \"ub\"), (\"xmin\", \"xmax\")):\n            try:\n                lb = getattr(func, lattr, None)\n                ub = getattr(func, uattr, None)\n                if lb is not None and ub is not None:\n                    lb = np.asarray(lb, dtype=float)\n                    ub = np.asarray(ub, dtype=float)\n                    if lb.size == 1:\n                        lb = np.full(self.dim, lb.item())\n                    if ub.size == 1:\n                        ub = np.full(self.dim, ub.item())\n                    return lb, ub\n            except Exception:\n                pass\n\n        # fallback\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=3):\n        x = np.array(x, dtype=float).copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        points = np.empty((n, dim))\n        for j in range(dim):\n            points[:, j] = cut[:-1] + u[:, j] * (cut[1] - cut[0])\n            self.rng.shuffle(points[:, j])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        shape = (n, lb.size)\n        return self.rng.uniform(lb, ub, size=shape)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # tempered coordinate-wise Cauchy\n        # draw uniform then map to cauchy\n        u = self.rng.rand(self.dim)\n        # standard Cauchy via tan(pi*(u-0.5))\n        c = np.tan(np.pi * (u - 0.5))\n        # temper extreme tails: shrink large values by tanh-scaling\n        tempered = np.tanh(c / cap_multiplier) * cap_multiplier\n        # occasionally allow heavy tail\n        if self.rng.rand() < 0.05:\n            tempered = c\n        return tempered * np.abs(scale_vec)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        domain_width = ub - lb\n        # clamp global scale to sensible fraction of domain\n        self.gscale = float(np.clip(self.gscale, 1e-6, 1.0))\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                # budget exhausted; return large penalty\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            self.evals += 1\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            return f\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(10, self.dim * 3), max(1, self.budget // 10))\n        remaining = self.budget - self.evals\n        init_pts = min(n_init, remaining)\n        if init_pts <= 0:\n            # nothing to evaluate\n            if self.best_x is None:\n                x0 = self._uniform_array(lb, ub)\n                f0 = safe_eval(x0)\n                self.best_x = x0.copy()\n                self.best_f = f0\n                self.archive.append((x0.copy(), f0))\n                return self.best_f, self.best_x\n            return self.best_f, self.best_x\n\n        lhs = self._lhs01(init_pts, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(X0.shape[0]):\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a couple pure uniforms for diversity\n        for _ in range(min(5, self.budget - self.evals)):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # main loop: generate candidates until budget exhausted\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample random if no archive\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                continue\n\n            arr_X = np.array([x for x, f in self.archive])\n            arr_f = np.array([f for x, f in self.archive])\n\n            # robust per-dim scale using IQR or std\n            per_dim_scale = np.zeros(self.dim)\n            if arr_X.shape[0] >= 4:\n                q75 = np.percentile(arr_X, 75, axis=0)\n                q25 = np.percentile(arr_X, 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n            else:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n            # ensure non-zero\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12 * domain_width)\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            if np.any(finite_mask):\n                finite_idxs = np.where(finite_mask)[0]\n                sorted_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])]\n            else:\n                sorted_idxs = np.arange(arr_X.shape[0])\n\n            topk = max(1, int(max(1, 0.15 * len(sorted_idxs)))) if len(sorted_idxs) > 0 else 1\n            if len(sorted_idxs) == 0:\n                anchor = self._uniform_array(lb, ub)\n                anchor_idx = None\n            else:\n                # biased sampling: choose among topk with exponential bias\n                if self.rng.rand() < 0.8 and len(sorted_idxs) > 0:\n                    candidates = sorted_idxs[:max(1, topk)]\n                    L = len(candidates)\n                    exps = np.exp(-np.arange(L) / max(1, L / 3.0))\n                    weights = exps / exps.sum()\n                    anchor_idx = int(self.rng.choice(candidates, p=weights))\n                else:\n                    anchor_idx = int(self.rng.choice(sorted_idxs))\n                anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            x_new = None\n\n            # Strategy selection probabilities adaptive-ish\n            p = self.rng.rand()\n            # DE-style differential injection\n            if p < 0.25 and arr_X.shape[0] >= 3:\n                idxs = list(range(arr_X.shape[0]))\n                # pick two distinct others not equal to anchor_idx if possible\n                others = [i for i in idxs if i != anchor_idx]\n                if len(others) >= 2:\n                    b, c = self.rng.choice(others, size=2, replace=False)\n                    xb = arr_X[b]\n                    xc = arr_X[c]\n                    F = 0.5 + 0.5 * self.rng.rand()  # differential factor [0.5,1.0]\n                    candidate = anchor + F * (xb - xc)\n                    # small gaussian jitter (anisotropic)\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * (self.gscale * 0.2))\n                    candidate = candidate + jitter\n                    x_new = candidate\n            # PCA-guided elite perturbation\n            elif p < 0.55 and arr_X.shape[0] >= max(3, 2 * self.dim):\n                elite_count = max(3, int(0.2 * arr_X.shape[0]))\n                if np.any(np.isfinite(arr_f)):\n                    finite_idxs = np.where(np.isfinite(arr_f))[0]\n                    if finite_idxs.size >= elite_count:\n                        elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])[:elite_count]]\n                    else:\n                        elite_idxs = np.arange(min(elite_count, arr_X.shape[0]))\n                else:\n                    elite_idxs = np.arange(min(elite_count, arr_X.shape[0]))\n                elites = arr_X[elite_idxs]\n                mu = np.mean(elites, axis=0)\n                cov = np.cov((elites - mu).T) + np.eye(self.dim) * 1e-8\n                try:\n                    vals, vecs = np.linalg.eigh(cov)\n                    # sample coefficients along principal axes with variance proportional to eigenvalues\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(np.maximum(vals, 1e-12))\n                    scale = (self.gscale * 0.8) * (np.mean(domain_width) / (np.mean(per_dim_scale) + 1e-12))\n                    perturb = vecs.dot(coeffs) * scale\n                    candidate = anchor + perturb * (per_dim_scale / (domain_width + 1e-12))\n                    x_new = candidate\n                except Exception:\n                    # fallback to small anisotropic gaussian\n                    noise = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.5)\n                    x_new = anchor + noise\n            # Local anisotropic gaussian\n            elif p < 0.85:\n                prob_coord = 0.3 + 0.5 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not np.any(coords):\n                    coords[self.rng.randint(0, self.dim)] = True\n                noise = np.zeros(self.dim)\n                noise[coords] = self.rng.randn(np.count_nonzero(coords)) * (per_dim_scale[coords] * self.gscale)\n                # occasionally mix some dims uniform\n                if self.rng.rand() < 0.12:\n                    mix_coords = self.rng.rand(self.dim) < 0.06\n                    if np.any(mix_coords):\n                        noise[mix_coords] = (self._uniform_array(lb[mix_coords], ub[mix_coords]) - anchor[mix_coords])\n                candidate = anchor + noise\n                x_new = candidate\n            # tempered Cauchy global escape\n            else:\n                scale_vec = domain_width * (self.gscale * (0.8 + 1.2 * self.rng.rand()))\n                candidate = anchor + self._tempered_cauchy(scale_vec)\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if x_new is not None and self.rng.rand() < 0.08 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.1 * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.07:\n                tiny = (self.rng.randn(self.dim) * (domain_width * 1e-3 * max(1.0, (abs(self.best_f) if np.isfinite(self.best_f) else 1.0))))\n                x_new = self.best_x + tiny\n\n            # fallback robust candidate from heterogenous mix if something went wrong\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * per_dim_scale * self.gscale\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if np.any(bad_coords):\n                x_new[bad_coords] = self._uniform_array(lb[bad_coords], ub[bad_coords])\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate\n            f_new = safe_eval(x_new)\n            self.archive.append((x_new.copy(), f_new))\n\n            # record success if candidate among top 25% of finite scores in current pool\n            finite_fs = arr_f[np.isfinite(arr_f)] if arr_f.size > 0 else np.array([])\n            success = False\n            if np.isfinite(f_new):\n                if f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                if finite_fs.size > 0:\n                    threshold = np.percentile(finite_fs, 25)\n                    success = (f_new <= threshold)\n                else:\n                    success = True\n            else:\n                success = False\n\n            self.success_window.append(1 if success else 0)\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every so often\n            if len(self.success_window) == self.window_len:\n                succ_rate = np.mean(self.success_window)\n                if succ_rate > 0.3:\n                    self.gscale *= 0.85\n                elif succ_rate < 0.08:\n                    self.gscale *= 1.25\n                else:\n                    self.gscale *= (0.95 + 0.1 * self.rng.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1.5))\n                if self.rng.rand() < 0.02:\n                    self.success_window = []\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                arr_X = np.array([x for x, f in self.archive])\n                arr_f = np.array([f for x, f in self.archive])\n                finite_idxs = np.where(np.isfinite(arr_f))[0]\n                if finite_idxs.size > 0:\n                    sorted_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                else:\n                    sorted_idxs = np.arange(len(arr_f))\n                keep_best = max(1, len(sorted_idxs) // 2)\n                kept = set(int(i) for i in sorted_idxs[:keep_best])\n                others = [i for i in range(len(arr_f)) if i not in kept]\n                self.rng.shuffle(others)\n                keep_rand = min(len(others), max(1, self.archive_max - keep_best - 3))\n                for i in others[:keep_rand]:\n                    kept.add(int(i))\n                # keep some infinities too for diversity\n                inf_idxs = [i for i, f in enumerate(arr_f) if not np.isfinite(f)]\n                for ii in inf_idxs[:3]:\n                    kept.add(int(ii))\n                new_archive = [self.archive[i] for i in sorted(kept)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_thresh and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(max(3, self.dim), self.budget - self.evals)\n                base = self.best_x if self.best_x is not None else self._uniform_array(lb, ub)\n                for _ in range(n_local):\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * (0.05 + 0.25 * self.rng.rand()) * self.gscale)\n                    cand = base + jitter\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                    if self.evals >= self.budget:\n                        break\n                # a few global randoms to regain diversity\n                n_global = min(5, self.budget - self.evals)\n                for _ in range(n_global):\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                    if self.evals >= self.budget:\n                        break\n                # after restart, slightly increase exploration scales\n                self.gscale = min(1.0, self.gscale * (1.2 + 0.3 * self.rng.rand()))\n                self.last_improve_at = self.evals\n                # reset small success window\n                self.success_window = []\n\n        # final best: choose best finite from archive\n        finite = [(x, f) for x, f in self.archive if np.isfinite(f)]\n        if finite:\n            best = min(finite, key=lambda t: t[1])\n            self.best_x, self.best_f = best[0].copy(), best[1]\n        else:\n            # all inf/unusable, return a random point (evaluate if budget permits)\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.best_x = x.copy()\n                self.best_f = f\n            else:\n                # no budget to evaluate, return previous best placeholders\n                if self.best_x is None:\n                    self.best_x = self._uniform_array(lb, ub)\n                    self.best_f = np.inf\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a7da6170-ac44-44ba-a145-d26b2f5ddd6b", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # adaptive global scale (fraction of domain)\n        self.gscale = 0.15  # initial global scale (fraction of domain width)\n\n        # bookkeeping\n        self.archive = []  # list of tuples (x (np.array), f (float))\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows\n        self.success_window = []  # list of booleans\n        self.window_len = max(10, min(200, max(1, self.budget // 20)))\n        self.last_improve_at = 0\n\n        # internal eval counter\n        self.evals = 0\n\n        # stagnation threshold to trigger micro-restart\n        self.stagnation_thresh = max(50, min(self.budget // 10, 200))\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try multiple common conventions; fallback to [-5,5]\n        try:\n            # obj with .bounds which may have lb/ub or array-like\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                    if lb.size == 1:\n                        lb = np.full(self.dim, lb.item())\n                    if ub.size == 1:\n                        ub = np.full(self.dim, ub.item())\n                    return lb, ub\n                else:\n                    arr = np.asarray(b, dtype=float)\n                    if arr.ndim == 1 and arr.size == 2:\n                        lb = np.full(self.dim, arr[0])\n                        ub = np.full(self.dim, arr[1])\n                        return lb, ub\n                    if arr.shape == (2, self.dim):\n                        lb = np.asarray(arr[0], dtype=float)\n                        ub = np.asarray(arr[1], dtype=float)\n                        return lb, ub\n        except Exception:\n            pass\n\n        # other attribute name pairs\n        for lattr, uattr in ((\"lower_bounds\", \"upper_bounds\"), (\"lb\", \"ub\"), (\"xmin\", \"xmax\")):\n            try:\n                lb = getattr(func, lattr, None)\n                ub = getattr(func, uattr, None)\n                if lb is not None and ub is not None:\n                    lb = np.asarray(lb, dtype=float)\n                    ub = np.asarray(ub, dtype=float)\n                    if lb.size == 1:\n                        lb = np.full(self.dim, lb.item())\n                    if ub.size == 1:\n                        ub = np.full(self.dim, ub.item())\n                    return lb, ub\n            except Exception:\n                continue\n\n        # fallback to typical BBOB bounds\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinate out-of-bounds values back into box (mirroring)\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            # if all in bounds, break\n            if not (np.any(x < lb) or np.any(x > ub)):\n                break\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        points = np.empty((n, dim))\n        for j in range(dim):\n            points[:, j] = cut[:-1] + u[:, j] * (cut[1] - cut[0])\n            self.rng.shuffle(points[:, j])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            # numpy broadcast uniform for vector bounds\n            return self.rng.uniform(0.0, 1.0, size=lb.shape) * (ub - lb) + lb\n        shape = (n, lb.size)\n        u = self.rng.uniform(0.0, 1.0, size=shape)\n        return u * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # tempered coordinate-wise Cauchy: produce heavier tails than gaussian but limit extremes\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard cauchy\n        # temper extreme tails: shrink very large values by tanh-scaling\n        tempered = np.tanh(c / cap_multiplier) * cap_multiplier\n        # with small probability allow raw cauchy to preserve heavy-tail jumps\n        if self.rng.rand() < 0.05:\n            out = c\n        else:\n            out = tempered\n        return out * np.abs(scale_vec)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        domain_width = ub - lb\n        # clamp global scale to sensible fraction of domain\n        self.gscale = float(np.clip(self.gscale, 1e-6, 1.0))\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # ---- initialization: a small LHS + a few randoms ----\n        remaining = self.budget - self.evals\n        if remaining <= 0:\n            # nothing to do\n            if self.best_x is None:\n                # return a random feasible point without evaluation\n                x = self._uniform_array(lb, ub)\n                return np.inf, x\n            return self.best_f, self.best_x\n\n        # choose number of initial points: scale with dim but not too many\n        n_init = min(max(2 * self.dim, 10), max(1, remaining // 8))\n        # generate LHS points in unit cube then map to bounds\n        lhs = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            x = lb + lhs[i] * (ub - lb)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniform randoms for diversity\n        for _ in range(min(5, self.budget - self.evals)):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # main loop: generate candidates until budget exhausted\n        while self.evals < self.budget:\n            # convert archive to arrays for statistics\n            if len(self.archive) == 0:\n                # sample random if no archive (shouldn't really happen)\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                continue\n\n            arr_X = np.vstack([x for x, f in self.archive])\n            arr_f = np.array([f for x, f in self.archive])\n\n            # robust per-dim scale using IQR or std\n            per_dim_scale = np.zeros(self.dim)\n            try:\n                q75 = np.percentile(arr_X, 75, axis=0)\n                q25 = np.percentile(arr_X, 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n            except Exception:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n\n            # ensure non-zero scales (fallback to fraction of domain)\n            min_scale = 1e-8 * (domain_width + 1.0)\n            per_dim_scale = np.maximum(per_dim_scale, min_scale)\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            if np.any(finite_mask):\n                finite_idxs = np.where(finite_mask)[0]\n                sorted_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])]\n            else:\n                sorted_idxs = np.arange(arr_X.shape[0])\n\n            # top fraction considered elites\n            elite_frac = 0.15\n            topk = max(1, int(np.ceil(elite_frac * len(sorted_idxs))))\n            elite_idxs = sorted_idxs[:topk] if len(sorted_idxs) > 0 else np.array([0], dtype=int)\n\n            # biased sampling: choose among topk with exponential bias or random\n            if len(elite_idxs) > 1 and self.rng.rand() < 0.85:\n                # exponential bias towards better elites\n                L = len(elite_idxs)\n                weights = np.exp(-np.arange(L) / max(1, L / 3.0))\n                weights = weights / weights.sum()\n                pick = self.rng.choice(np.arange(L), p=weights)\n                anchor_idx = int(elite_idxs[pick])\n            else:\n                anchor_idx = int(self.rng.choice(sorted_idxs))\n\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            p = self.rng.rand()\n            x_new = None\n\n            # DE-style differential injection\n            if p < 0.25 and arr_X.shape[0] >= 3:\n                # pick two distinct others not equal to anchor_idx\n                idxs = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                b, c = self.rng.choice(idxs, size=2, replace=False)\n                xb = arr_X[b]\n                xc = arr_X[c]\n                F = 0.5 + 0.5 * self.rng.rand()  # differential factor [0.5,1.0]\n                candidate = anchor + F * (xb - xc)\n                # small anisotropic gaussian jitter\n                jitter = self.rng.randn(self.dim) * (per_dim_scale * (self.gscale * 0.2))\n                candidate = candidate + jitter\n                x_new = candidate\n\n            # PCA-guided elite perturbation\n            elif p < 0.55 and arr_X.shape[0] >= max(3, 2 * self.dim):\n                # build from elites if enough\n                if elite_idxs.size >= min(3, self.dim):\n                    elites = arr_X[elite_idxs]\n                    mu = np.mean(elites, axis=0)\n                    cov = np.cov((elites - mu).T)\n                    # regularize covariance\n                    cov = cov + np.eye(self.dim) * 1e-8\n                    try:\n                        vals, vecs = np.linalg.eigh(cov)\n                        vals = np.maximum(vals, 1e-12)\n                        scale = self.gscale * np.mean(domain_width) * (0.5 + self.rng.rand())\n                        coeffs = self.rng.randn(self.dim) * np.sqrt(vals)\n                        perturb = vecs.dot(coeffs) * scale\n                        candidate = mu + perturb\n                        x_new = candidate\n                    except Exception:\n                        # fallback to small anisotropic gaussian\n                        noise = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.5)\n                        x_new = anchor + noise\n                else:\n                    noise = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.5)\n                    x_new = anchor + noise\n\n            # Local anisotropic gaussian\n            elif p < 0.85:\n                # coordinate-wise perturbation with probability to tweak each coord\n                prob_coord = 0.15 + 0.7 * self.rng.rand()  # fraction of coords changed\n                coords = self.rng.rand(self.dim) < prob_coord\n                noise = np.zeros(self.dim)\n                # anisotropic magnitudes\n                noise[coords] = self.rng.randn(np.count_nonzero(coords)) * (per_dim_scale[coords] * self.gscale * (0.8 * self.rng.rand() + 0.2))\n                # occasionally mix some dimensions uniformly across domain to escape plateaus\n                if self.rng.rand() < 0.04:\n                    mix_coords = self.rng.rand(self.dim) < 0.05\n                    if np.any(mix_coords):\n                        noise[mix_coords] = (self._uniform_array(lb[mix_coords], ub[mix_coords]) - anchor[mix_coords])\n                candidate = anchor + noise\n                x_new = candidate\n\n            # tempered Cauchy global escape\n            else:\n                scale_vec = domain_width * (self.gscale * (0.8 + 1.2 * self.rng.rand()))\n                candidate = anchor + self._tempered_cauchy(scale_vec)\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if x_new is not None and arr_X.shape[0] >= 2 and self.rng.rand() < 0.18:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.08 * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.07:\n                tiny = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.08)\n                x_new = self.best_x + tiny\n\n            # fallback robust candidate if something went wrong\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * per_dim_scale * self.gscale\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if np.any(bad_coords):\n                x_new[bad_coords] = self._uniform_array(lb[bad_coords], ub[bad_coords])\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate\n            f_new = safe_eval(x_new)\n            self.archive.append((x_new.copy(), f_new))\n\n            # record success if candidate among top 25% of finite scores in current pool\n            finite_fs = arr_f[np.isfinite(arr_f)] if arr_f.size > 0 else np.array([])\n            success = False\n            if np.isfinite(f_new):\n                if f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                if finite_fs.size > 0:\n                    threshold = np.percentile(finite_fs, 25)\n                    success = (f_new <= threshold)\n                else:\n                    success = True\n            else:\n                success = False\n\n            # update success window\n            self.success_window.append(1 if success else 0)\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every so often (based on recent success rate)\n            if len(self.success_window) >= max(5, min(50, self.window_len)):\n                succ_rate = float(np.mean(self.success_window))\n                # if too successful -> exploit (shrink), if failing -> explore more\n                if succ_rate > 0.33:\n                    self.gscale *= 0.9\n                elif succ_rate < 0.08:\n                    self.gscale *= 1.25\n                # slight jitter to avoid locked scale\n                self.gscale *= (1.0 + (self.rng.rand() - 0.5) * 0.02)\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1.5))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                arr_f = np.array([f for x, f in self.archive])\n                finite_idxs = np.where(np.isfinite(arr_f))[0]\n                if finite_idxs.size > 0:\n                    sorted_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                else:\n                    sorted_idxs = np.arange(len(arr_f))\n                keep_best = max(1, len(sorted_idxs) // 2)\n                kept = set(int(i) for i in sorted_idxs[:keep_best])\n                # keep some random others to preserve diversity\n                others = [i for i in range(len(arr_f)) if i not in kept]\n                if len(others) > 0:\n                    keep_rand = min(len(others), max(1, self.archive_max - keep_best))\n                    rand_choice = self.rng.choice(others, size=keep_rand, replace=False)\n                    for ii in rand_choice:\n                        kept.add(int(ii))\n                # keep some infinities too for diversity (if present)\n                inf_idxs = [i for i, f in enumerate(arr_f) if not np.isfinite(f)]\n                if len(inf_idxs) > 0:\n                    for ii in inf_idxs[:3]:\n                        kept.add(int(ii))\n                new_archive = [self.archive[i] for i in sorted(kept)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_thresh and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                base = self.best_x.copy() if self.best_x is not None else self._uniform_array(lb, ub)\n                n_local = min(8 + self.dim, max(3, self.budget - self.evals))\n                for _ in range(n_local):\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * (0.05 + 0.25 * self.rng.rand()) * self.gscale)\n                    cand = base + jitter\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_cand = safe_eval(cand)\n                    self.archive.append((cand.copy(), f_cand))\n                    if np.isfinite(f_cand) and f_cand < self.best_f:\n                        self.best_f = f_cand\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                    if self.evals >= self.budget:\n                        break\n                # a few global randoms to regain diversity\n                n_global = min(5, self.budget - self.evals)\n                for _ in range(n_global):\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                        break\n                # after restart, slightly increase exploration scales and reset small success window\n                self.gscale = float(np.clip(self.gscale * 1.08, 1e-6, 2.0))\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: choose best finite from archive\n        finite = [(x, f) for x, f in self.archive if np.isfinite(f)]\n        if finite:\n            finite_sorted = sorted(finite, key=lambda t: t[1])\n            self.best_x, self.best_f = finite_sorted[0][0].copy(), float(finite_sorted[0][1])\n        else:\n            # all inf/unusable, try to return a sampled point if budget permitted (but we exhausted budget)\n            if self.best_x is None:\n                self.best_x = self._uniform_array(lb, ub)\n                self.best_f = np.inf\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a87689a2-38e2-4c42-9c3d-b25d3a176142", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # adaptive global scale (fraction of domain)\n        self.gscale = 0.1\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows\n        self.last_improve_at = 0\n        # stagnation threshold (if no improvement for this many evals -> micro-restart)\n        self.stagnation_thresh = max(50, self.budget // 20)\n\n        # internal eval counter\n        self.evals = 0\n\n        # archive of (x,f)\n        self.archive = []\n\n        # sliding success window for scale adaptation\n        self.success_window = []\n        self.window_len = max(30, self.budget // 100)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Many BBOB problems use [-5,5], but be somewhat flexible:\n        try:\n            # if func has .bounds with .lb/.ub or tuple\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # tuple-like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # try attributes lb/ub\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n                    lb = np.asarray(lb, dtype=float)\n                    ub = np.asarray(ub, dtype=float)\n                if lb.size == 1:\n                    lb = np.full(self.dim, lb.item())\n                if ub.size == 1:\n                    ub = np.full(self.dim, ub.item())\n                return lb, ub\n        except Exception:\n            pass\n\n        # try direct attributes on func\n        for lattr, uattr in ((\"lb\", \"ub\"), (\"lower_bounds\", \"upper_bounds\"),\n                             (\"xmin\", \"xmax\"), (\"lower\", \"upper\")):\n            try:\n                lb = getattr(func, lattr, None)\n                ub = getattr(func, uattr, None)\n                if lb is not None and ub is not None:\n                    lb = np.asarray(lb, dtype=float)\n                    ub = np.asarray(ub, dtype=float)\n                    if lb.size == 1:\n                        lb = np.full(self.dim, lb.item())\n                    if ub.size == 1:\n                        ub = np.full(self.dim, ub.item())\n                    return lb, ub\n            except Exception:\n                pass\n\n        # fallback to [-5,5]\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            # if all inside break\n            if not (np.any(x < lb) or np.any(x > ub)):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        points = np.empty((n, dim))\n        for j in range(dim):\n            points[:, j] = cut[:-1] + u[:, j] * (cut[1] - cut[0])\n            self.rng.shuffle(points[:, j])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            shape = (n, lb.size)\n            return self.rng.uniform(lb, ub, size=shape)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # tempered coordinate-wise Cauchy\n        dim = int(scale_vec.size)\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper extreme tails: shrink large values by tanh-scaling\n        tempered = np.tanh(c / cap_multiplier) * cap_multiplier\n        # occasionally allow heavy tail\n        if self.rng.rand() < 0.05:\n            tempered = c\n        return tempered * np.abs(scale_vec)\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        domain_width = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            self.evals += 1\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            return f\n\n        # ---- initialization: LHS samples ----\n        n_init = int(min(max(10, self.dim * 3), max(1, self.budget // 10)))\n        n_init = max(2, n_init)\n        lhs = self._lhs01(n_init, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(n_init):\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a couple pure uniforms for diversity (bounded by budget)\n        adds = min(5, max(0, self.budget - self.evals))\n        for _ in range(adds):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, accept any finite from archive or set a random\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0:\n            # try one more random sample if budget allows\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.archive.append((x.copy(), f))\n                if np.isfinite(f):\n                    self.best_f = f\n                    self.best_x = x.copy()\n\n        # main loop: generate candidates until budget exhausted\n        adapt_every = max(20, self.window_len // 2)\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample random if no archive\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            arr_X = np.vstack([item[0] for item in self.archive])\n            arr_f = np.array([item[1] for item in self.archive], dtype=float)\n\n            # robust per-dim scale using IQR or std\n            try:\n                iqr = np.subtract(*np.percentile(arr_X, [75, 25], axis=0))\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if zero_mask.any():\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n            except Exception:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12)\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            if np.any(finite_mask):\n                finite_idxs = np.where(finite_mask)[0]\n                sorted_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])]\n            else:\n                sorted_idxs = np.arange(arr_X.shape[0])\n\n            # biased sampling among top-k\n            topk = max(1, min(len(sorted_idxs), 1 + len(sorted_idxs) // 5))\n            if len(sorted_idxs) > 0 and self.rng.rand() < 0.9:\n                candidates = sorted_idxs[:topk]\n                # exponential bias\n                exps = np.exp(-np.arange(len(candidates), dtype=float))\n                weights = exps / exps.sum()\n                anchor_idx = int(self.rng.choice(candidates, p=weights))\n            else:\n                anchor_idx = int(self.rng.randint(0, arr_X.shape[0]))\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            x_new = None\n\n            # Strategy selection probabilities adaptive-ish\n            r = self.rng.rand()\n            # DE-style differential injection (prob ~0.25)\n            if r < 0.25 and arr_X.shape[0] >= 3:\n                idxs = list(range(arr_X.shape[0]))\n                others = [i for i in idxs if i != anchor_idx]\n                b, c = self.rng.choice(others, size=2, replace=False)\n                xb = arr_X[b]\n                xc = arr_X[c]\n                F = 0.5 + 0.5 * self.rng.rand()  # differential factor [0.5,1.0]\n                candidate = anchor + F * (xb - xc)\n                # small anisotropic gaussian jitter\n                jitter = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.3)\n                x_new = candidate + jitter\n\n            # PCA-guided elite perturbation (prob ~0.25)\n            elif r < 0.5 and arr_X.shape[0] >= 5:\n                elite_count = max(3, int(0.2 * arr_X.shape[0]))\n                # pick best finite elites if possible\n                if np.any(finite_mask):\n                    finite_idxs = np.where(finite_mask)[0]\n                    if finite_idxs.size >= elite_count:\n                        elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])][:elite_count]\n                    else:\n                        elite_idxs = finite_idxs\n                else:\n                    elite_idxs = np.arange(arr_X.shape[0])[:elite_count]\n                elites = arr_X[elite_idxs]\n                mu = np.mean(elites, axis=0)\n                cov = np.cov(elites.T)\n                # stable eigen-decomposition\n                try:\n                    vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                    vals = np.maximum(vals, 1e-12)\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(vals)\n                    scale = (self.gscale * 0.8) * (np.mean(domain_width) / (np.mean(per_dim_scale) + 1e-12))\n                    perturb = vecs.dot(coeffs) * scale\n                    candidate = anchor + perturb * (per_dim_scale / (domain_width + 1e-12))\n                    x_new = candidate\n                except Exception:\n                    noise = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.5)\n                    x_new = anchor + noise\n\n            # Local anisotropic gaussian (prob ~0.25)\n            elif r < 0.75:\n                prob_coord = 0.3 + 0.6 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not coords.any():\n                    coords[self.rng.randint(0, self.dim)] = True\n                noise = np.zeros(self.dim)\n                noise[coords] = self.rng.randn(coords.sum()) * (per_dim_scale[coords] * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                # occasionally mix some dims uniform\n                if self.rng.rand() < 0.12:\n                    unif_coords = self.rng.rand(self.dim) < 0.08\n                    noise[unif_coords] = (self._uniform_array(lb[unif_coords], ub[unif_coords]) - anchor[unif_coords])\n                candidate = anchor + noise\n                x_new = candidate\n\n            # tempered Cauchy global escape (rest)\n            else:\n                scale_vec = np.maximum(per_dim_scale * (0.5 + 0.5 * self.rng.rand()), 1e-12) * (self.gscale * 2.0)\n                candidate = anchor + self._tempered_cauchy(scale_vec)\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if x_new is not None and self.rng.rand() < 0.08 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.1 * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.03:\n                tiny = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.05)\n                x_new = self.best_x + tiny\n\n            # fallback robust candidate\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * per_dim_scale * self.gscale\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if bad_coords.any():\n                x_new[bad_coords] = self._uniform_array(lb[bad_coords], ub[bad_coords])\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate\n            f_new = safe_eval(x_new)\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_x = x_new.copy()\n                self.best_f = f_new\n                self.last_improve_at = self.evals\n                self.success_window.append(1)\n            else:\n                # measure success relative to current archive: consider success if in top 25% finite\n                finite_fs = arr_f[np.isfinite(arr_f)] if arr_f.size > 0 else np.array([])\n                if np.isfinite(f_new) and finite_fs.size > 0:\n                    try:\n                        threshold = np.percentile(finite_fs, 25)\n                        self.success_window.append(1 if f_new <= threshold else 0)\n                    except Exception:\n                        self.success_window.append(0)\n                else:\n                    self.success_window.append(0)\n\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every now and then\n            if (self.evals % adapt_every) == 0 and len(self.success_window) > 0:\n                succ_rate = np.mean(self.success_window)\n                if succ_rate > 0.4:\n                    # too easy -> contract\n                    self.gscale *= 0.85\n                elif succ_rate < 0.08:\n                    # too hard -> expand\n                    self.gscale *= 1.25\n                else:\n                    # mild jitter\n                    self.gscale *= (0.95 + 0.1 * self.rng.rand())\n                # clamp\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1.0))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                # keep best ones and some randoms\n                arr_f = np.array([it[1] for it in self.archive])\n                finite_mask = np.isfinite(arr_f)\n                keep_best = max(5, int(0.2 * self.archive_max))\n                kept_ix = set()\n                if finite_mask.any():\n                    finite_idxs = np.where(finite_mask)[0]\n                    sorted_finite = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                    for i in sorted_finite[:keep_best]:\n                        kept_ix.add(int(i))\n                # keep some random ones for diversity\n                others = [i for i in range(len(arr_f)) if i not in kept_ix]\n                self.rng.shuffle(others)\n                keep_rand = min(len(others), max(1, self.archive_max - len(kept_ix)))\n                for i in others[:keep_rand]:\n                    kept_ix.add(int(i))\n                new_archive = [self.archive[i] for i in sorted(kept_ix)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_thresh:\n                # micro-restart: generate a small local cloud around best and some randoms\n                base = self.best_x if self.best_x is not None else self._uniform_array(lb, ub)\n                n_local = min(20, max(5, self.dim * 3))\n                for _ in range(n_local):\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * (0.05 + 0.25 * self.rng.rand()) * max(1.0, self.gscale))\n                    cand = base + jitter\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    if self.evals >= self.budget:\n                        break\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # a few global randoms to regain diversity\n                n_global = min(10, max(3, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * (1.2 + 0.3 * self.rng.rand()), 1e-6, 1.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive\n        finite_archive = [it for it in self.archive if np.isfinite(it[1])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda t: t[1])\n            self.best_x, self.best_f = best[0].copy(), float(best[1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.best_x = x.copy()\n                self.best_f = f\n            else:\n                if self.best_x is None:\n                    self.best_f = np.inf\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ab1753f5-0200-48b3-93e6-c234d50a0a38", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The code will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # internal adaptive global scale (multiplier for per-dim scales)\n        self.gscale = 0.12\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows & counters\n        self.evals = 0\n        self.last_improve_at = 0\n        self.success_window = []\n        self.window_len = max(30, self.budget // 100)\n        self.stagnation_threshold = max(200, self.budget // 10)\n\n        # archive of (x,f)\n        self.archive = []\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # if tuple-like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # object with lb/ub attributes\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n                    if lb is not None:\n                        lb = np.asarray(lb, dtype=float)\n                    if ub is not None:\n                        ub = np.asarray(ub, dtype=float)\n            # also try direct attributes\n            if lb is None:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            if ub is None:\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n            if lb is not None:\n                lb = np.asarray(lb, dtype=float)\n            if ub is not None:\n                ub = np.asarray(ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # Normalize scalars to vectors\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            # if all inside break\n            if not (np.any(x < lb) or np.any(x > ub)):\n                break\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        points = np.empty((n, dim), dtype=float)\n        for d in range(dim):\n            perm = self.rng.permutation(n)\n            # stratified samples in each cell\n            points[:, d] = (perm + self.rng.rand(n)) / n\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size)\n        samples = self.rng.rand(*shape) * (ub - lb) + lb\n        if n == 1:\n            return samples[0]\n        return samples\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        dim = scale_vec.size\n        # sample standard Cauchy via tan(pi*(u-0.5))\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper large values\n        # small prob to allow extreme heavy tail\n        if self.rng.rand() < 0.06:\n            factor = 1.0\n        else:\n            factor = np.tanh(np.abs(c)) / (np.abs(c) + 1e-12)\n        tempered = np.sign(c) * (np.abs(c) * factor)\n        # scale per-dim and cap\n        cap = cap_multiplier * scale_vec\n        res = np.clip(tempered * scale_vec, -cap, cap)\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n        if np.any(domain_width <= 0):\n            domain_width = np.maximum(domain_width, 1e-8)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        # allocate a small initial archive\n        n_init = min(max(12, self.dim * 4), max(1, self.budget // 20))\n        n_init = max(n_init, 6)\n        n_init = min(n_init, self.budget)\n        lhs = self._lhs01(n_init, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            x = self._reflect_bounds(x, lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        adds = min(6, max(0, self.budget - self.evals))\n        for _ in range(adds):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        adapt_every = max(10, min(200, self.window_len // 3))\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample random if no archive\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            try:\n                iqr = np.subtract(*np.percentile(arr_X, [75, 25], axis=0))\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n                # if still zeros fallback\n                per_dim_scale = np.where(per_dim_scale <= 1e-12, np.std(arr_X, axis=0) + 1e-12, per_dim_scale)\n            except Exception:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12)\n            scale_vec = per_dim_scale * domain_width / (domain_width + 1e-12)  # normalize to domain fraction\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            if np.any(finite_mask):\n                finite_idxs = np.where(finite_mask)[0]\n                sorted_finite = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                topk = max(2, int(0.15 * arr_X.shape[0]))\n                topk = min(topk, sorted_finite.size)\n                candidates = sorted_finite[:topk] if topk > 0 else sorted_finite\n                # exponential bias over candidate ranks\n                weights = np.exp(-np.arange(len(candidates), dtype=float))\n                weights = weights / weights.sum()\n                anchor_idx = self.rng.choice(candidates, p=weights)\n            else:\n                anchor_idx = self.rng.randint(arr_X.shape[0])\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            x_new = None\n            r = self.rng.rand()\n\n            # Strategy 1: DE-style differential injection (around 25%)\n            if r < 0.25 and arr_X.shape[0] >= 3:\n                others = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                b, c = self.rng.choice(others, size=2, replace=False)\n                xb = arr_X[b]\n                xc = arr_X[c]\n                F = 0.4 + 0.6 * self.rng.rand()  # differential factor [0.4,1.0]\n                diff = F * (xb - xc)\n                # anisotropic gaussian jitter\n                jitter = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                candidate = anchor + diff + jitter * domain_width\n                x_new = candidate\n\n            # Strategy 2: PCA-guided elite perturbation (~25%)\n            elif r < 0.5 and arr_X.shape[0] >= 5:\n                elite_count = max(3, min(arr_X.shape[0], int(0.18 * arr_X.shape[0])))\n                if np.any(finite_mask):\n                    if finite_idxs.size >= elite_count:\n                        elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])][:elite_count]\n                    else:\n                        elite_idxs = finite_idxs\n                else:\n                    elite_idxs = self.rng.choice(range(arr_X.shape[0]), size=elite_count, replace=False)\n                elites = arr_X[elite_idxs]\n                mu = np.mean(elites, axis=0)\n                cov = np.cov(elites.T)\n                # stable eigen-decomposition\n                try:\n                    vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                    # sample coefficients along principal axes, scaled by eigenvals and global scale\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(np.maximum(vals, 0.0))\n                    scale = self.gscale * (0.6 + 0.8 * self.rng.rand())\n                    perturb = vecs.dot(coeffs) * scale\n                    candidate = mu + perturb * domain_width / (domain_width + 1e-12)\n                    x_new = anchor + 0.6 * (candidate - anchor)\n                except Exception:\n                    # fallback to small local jitter\n                    x_new = anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale)\n\n            # Strategy 3: Local anisotropic gaussian (~25%)\n            elif r < 0.75:\n                prob_coord = 0.25 + 0.6 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not np.any(coords):\n                    coords[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise[coords] = self.rng.randn(coords.sum()) * (per_dim_scale[coords] * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                candidate = anchor + noise * domain_width\n                # occasionally mix in a few uniform coords\n                if self.rng.rand() < 0.12:\n                    mix_coords = self.rng.rand(self.dim) < 0.08\n                    candidate[mix_coords] = self._uniform_array(lb, ub)[mix_coords]\n                x_new = candidate\n\n            # Strategy 4: tempered Cauchy global escape (rest)\n            else:\n                candidate = anchor + self._tempered_cauchy(scale_vec * (1.0 + 4.0 * self.gscale))\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if x_new is not None and self.rng.rand() < 0.08 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.08 * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.035:\n                tiny = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.04)\n                x_new = self.best_x + tiny * domain_width\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if np.any(bad_coords):\n                x_new[bad_coords] = self._uniform_array(lb, ub)[bad_coords]\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                # success recorded\n                self.success_window.append(1)\n            else:\n                # measure success relative to current archive: consider success if in top 25% finite\n                finite_fs = arr_f[np.isfinite(arr_f)]\n                if np.isfinite(f_new) and finite_fs.size > 0:\n                    try:\n                        threshold = np.percentile(finite_fs, 25)\n                        self.success_window.append(1 if f_new <= threshold else 0)\n                    except Exception:\n                        self.success_window.append(0)\n                else:\n                    self.success_window.append(0)\n\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window = self.success_window[-self.window_len:]\n\n            # adaptation of gscale every now and then\n            if (self.evals % adapt_every) == 0 and len(self.success_window) > 0:\n                succ_rate = float(np.mean(self.success_window))\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    self.gscale *= 0.88\n                elif succ_rate < 0.12:\n                    self.gscale *= 1.14\n                else:\n                    # mild jitter to escape stagnation\n                    self.gscale *= (0.97 + 0.06 * self.rng.rand())\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1.5))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                # keep best ones and some randoms\n                arr_f_all = np.array([a[1] for a in self.archive], dtype=float)\n                finite_mask_all = np.isfinite(arr_f_all)\n                keep_best = max(6, int(0.2 * self.archive_max))\n                kept_ix = set()\n                if finite_mask_all.any():\n                    finite_idxs_all = np.where(finite_mask_all)[0]\n                    sorted_finite_all = finite_idxs_all[np.argsort(arr_f_all[finite_idxs_all])]\n                    for i in sorted_finite_all[:keep_best]:\n                        kept_ix.add(int(i))\n                # keep current best if present\n                if self.best_x is not None:\n                    # find index of best in archive (approx)\n                    diffs = np.linalg.norm(np.vstack([a[0] for a in self.archive]) - self.best_x, axis=1)\n                    best_idx = int(np.argmin(diffs))\n                    kept_ix.add(best_idx)\n                # keep some random ones for diversity\n                keep_rand = max(10, self.archive_max - len(kept_ix))\n                candidates = [i for i in range(len(self.archive)) if i not in kept_ix]\n                self.rng.shuffle(candidates)\n                for i in candidates[:keep_rand]:\n                    kept_ix.add(int(i))\n                new_archive = [self.archive[i] for i in sorted(kept_ix)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_threshold and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(30, max(6, self.dim * 4))\n                base = self.best_x.copy() if self.best_x is not None else self._uniform_array(lb, ub)\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * max(0.8, self.gscale) * 0.8)\n                    cand = self._reflect_bounds(base + jitter * domain_width, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.25, 1e-6, 2.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [t for t in self.archive if np.isfinite(t[1])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda t: t[1])\n            self.best_x, self.best_f = best[0].copy(), float(best[1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.best_x is None:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x) if self.evals < self.budget else np.inf\n                self.best_x = x.copy()\n                self.best_f = f\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7501679e-1c0b-4f3e-bdb1-3bb5e20b1118", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=42)\n        fbest, xbest = opt(func)\n    The algorithm will try to read func.bounds (tuple-like or object with lb/ub or lower/upper attributes).\n    If not found, defaults to [-5,5]^dim.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # internal adaptive global scale (multiplier for per-dim scales)\n        self.gscale = 0.12\n\n        # bookkeeping\n        self.evals = 0\n        self.archive = []  # list of (x, f)\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows & counters\n        self.last_improve_at = 0\n        self.window_len = 60\n        self.success_window = []\n        self.stagnation_threshold = max(150, int(self.budget * 0.08))\n\n        # adaptation frequency\n        self.adapt_every = max(20, int(self.budget * 0.002))\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # if tuple-like (lb, ub)\n            if isinstance(b, (tuple, list)) and len(b) == 2:\n                try:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n            else:\n                # object with lb/ub attributes\n                lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                ub_attr = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n                if lb_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                if ub_attr is not None:\n                    ub = np.asarray(ub_attr, dtype=float)\n        # also try direct attributes on func\n        if lb is None:\n            la = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            if la is not None:\n                lb = np.asarray(la, dtype=float)\n        if ub is None:\n            ua = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n            if ua is not None:\n                ub = np.asarray(ua, dtype=float)\n\n        # Normalize scalars to vectors\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast scalars to vectors if needed\n            if lb.ndim == 0:\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if ub.ndim == 0:\n                ub = np.full(self.dim, float(ub), dtype=float)\n            # if arrays but wrong length, try to broadcast or truncate/pad\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim).astype(float)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim).astype(float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # Mirror reflection until within bounds, then clamp as final safety\n        x = x.copy()\n        for _ in range(max_iter):\n            below = x < lb\n            above = x > ub\n            if not (np.any(below) or np.any(above)):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        base = (np.arange(n) + self.rng.rand(n, dim)) / float(n)\n        # permute within each dimension\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            base[:, j] = base[perm, j]\n        return base\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            rand = self.rng.rand(self.dim)\n            return lb + rand * (ub - lb)\n        else:\n            rand = self.rng.rand(n, self.dim)\n            return lb + rand * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with a soft cap; returns vector sized dim\n        dim = scale_vec.size\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # temper extremes softly: clip then apply a soft shrink for very large values\n        cap = cap_multiplier\n        abs_c = np.abs(c)\n        # small probability to allow extreme heavy tail (rarely allow raw larger values)\n        heavy_tail_mask = self.rng.rand(dim) < 0.02\n        capped = np.where(heavy_tail_mask, c, np.sign(c) * np.minimum(abs_c, cap))\n        # further smooth very small/large via tanh scaling to avoid numerical blow-up\n        factor = 1.0 + 0.5 * np.tanh(capped / (cap / 2.0))\n        tempered = capped * factor\n        # scale per-dim\n        res = tempered * scale_vec\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            # x should be 1D array\n            if self.evals >= self.budget:\n                return np.inf\n            # ensure numpy array copy\n            xv = np.asarray(x, dtype=float).copy()\n            f = func(xv)\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        # allocate a small initial archive\n        n_init = min(max(12, self.dim * 4), max(1, self.budget // 20))\n        n_init = min(n_init, self.budget)\n        X0 = lb + (ub - lb) * self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        for _ in range(min(6, max(0, self.budget - self.evals))):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample random if no archive\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            try:\n                q1 = np.nanpercentile(arr_X, 25, axis=0)\n                q3 = np.nanpercentile(arr_X, 75, axis=0)\n                iqr = q3 - q1\n                per_dim_scale = iqr / 1.349  # approximate std\n                # fallback small values to std\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n                per_dim_scale = np.where(per_dim_scale <= 1e-12, np.std(arr_X, axis=0) + 1e-12, per_dim_scale)\n            except Exception:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n            # ensure per_dim_scale reasonable relative to domain\n            per_dim_scale = np.maximum(per_dim_scale, domain_width * 1e-6)\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            finite_idxs = np.nonzero(finite_mask)[0]\n            anchor_idx = None\n            if finite_idxs.size > 0:\n                # choose from top-k with exponential bias\n                topk = max(2, min(12, int(len(finite_idxs) * 0.25)))\n                sorted_finite = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                topk = min(topk, sorted_finite.size)\n                candidates = sorted_finite[:topk]\n                ranks = np.arange(candidates.size)\n                weights = np.exp(-0.6 * ranks.astype(float))\n                weights = weights / weights.sum()\n                pick = self.rng.choice(candidates, p=weights)\n                anchor_idx = int(pick)\n            else:\n                anchor_idx = self.rng.randint(arr_X.shape[0])\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = self.rng.rand()\n            x_new = None\n\n            # Strategy 1: DE-style differential injection (~25%)\n            if r < 0.25 and arr_X.shape[0] >= 3:\n                # pick donors distinct from anchor\n                idxs = list(range(arr_X.shape[0]))\n                if anchor_idx in idxs:\n                    idxs.remove(anchor_idx)\n                self.rng.shuffle(idxs)\n                a_idx, b_idx = idxs[0], idxs[1]\n                xb = arr_X[a_idx]\n                xc = arr_X[b_idx]\n                # differential vector\n                diff = xb - xc\n                scale_diff = 0.8 * self.gscale\n                jitter = self.rng.randn(self.dim) * (per_dim_scale * 0.08)\n                candidate = anchor + scale_diff * diff + jitter\n                x_new = candidate\n\n            # Strategy 2: PCA-guided elite perturbation (~25%)\n            elif r < 0.5 and arr_X.shape[0] >= 5:\n                try:\n                    elite_count = min(arr_X.shape[0], max(4, int(arr_X.shape[0] * 0.2)))\n                    if finite_idxs.size > 0:\n                        elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])][:elite_count]\n                        X_elite = arr_X[elite_idxs]\n                    else:\n                        # fallback: pick random subset\n                        perm = self.rng.permutation(arr_X.shape[0])[:elite_count]\n                        X_elite = arr_X[perm]\n                    mu = X_elite.mean(axis=0)\n                    # covariance (regularized)\n                    C = np.cov((X_elite - mu).T)\n                    # stable eigen-decomposition\n                    eigvals, eigvecs = np.linalg.eigh(C + np.eye(self.dim) * 1e-12)\n                    # sample coefficients along principal axes, scaled by eigenvals and global scale\n                    coeffs = self.rng.randn(self.dim) * (np.sqrt(np.maximum(eigvals, 0.0)) + 1e-12) * (self.gscale * 4.0)\n                    perturb = eigvecs.dot(coeffs)\n                    # scale by domain ratio to avoid overshoot on tiny domains\n                    candidate = mu + perturb\n                    x_new = anchor + 0.6 * (candidate - anchor)\n                except Exception:\n                    # fallback to small local jitter\n                    x_new = anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.6)\n\n            # Strategy 3: Local anisotropic gaussian (~25%)\n            elif r < 0.75:\n                # pick subset of coords to perturb\n                coords = self.rng.rand(self.dim) < np.clip(0.6, 0.05, 0.9)\n                if not np.any(coords):\n                    coords[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise[coords] = self.rng.randn(np.count_nonzero(coords)) * (per_dim_scale[coords] * (self.gscale))\n                candidate = anchor + noise\n                # occasionally mix in a few uniform coords for larger moves\n                mix_coords = self.rng.rand(self.dim) < 0.08\n                if np.any(mix_coords):\n                    candidate[mix_coords] = self._uniform_array(lb, ub)[mix_coords]\n                x_new = candidate\n\n            # Strategy 4: tempered Cauchy global escape (rest)\n            else:\n                # allow larger jumps with per-dim heavy tail\n                scale_vec = per_dim_scale * (self.gscale * 6.0 + 1e-12)\n                jump = self._tempered_cauchy(scale_vec, cap_multiplier=8.0)\n                candidate = anchor + jump\n                # sometimes center around the median of archive for broader exploration\n                if self.rng.rand() < 0.12:\n                    median = np.median(arr_X, axis=0)\n                    candidate = 0.5 * (candidate + median)\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if x_new is not None and self.rng.rand() < 0.08 and arr_X.shape[0] >= 2:\n                a, b = self.rng.choice(arr_X.shape[0], size=2, replace=False)\n                dx = arr_X[a] - arr_X[b]\n                x_new += dx * (0.12 * self.gscale)\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.035:\n                tiny = 0.04 * (1.0 + self.rng.rand())\n                x_new = self.best_x + self.rng.randn(self.dim) * (per_dim_scale * tiny)\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad = (~np.isfinite(x_new)) | (x_new < lb - 1e6) | (x_new > ub + 1e6)\n            if np.any(bad):\n                x_new[bad] = self._uniform_array(lb, ub)[bad]\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                # success recorded\n                self.success_window.append(1)\n                self.last_improve_at = self.evals\n            else:\n                # measure success relative to current archive: consider success if in top 25% finite\n                finite_fs = np.array([a[1] for a in self.archive if np.isfinite(a[1])], dtype=float)\n                if np.isfinite(f_new) and finite_fs.size > 0:\n                    try:\n                        threshold = np.percentile(finite_fs, 25)\n                        self.success_window.append(1 if f_new <= threshold else 0)\n                    except Exception:\n                        self.success_window.append(0)\n                else:\n                    self.success_window.append(0)\n\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window = self.success_window[-self.window_len:]\n\n            # adaptation of gscale every now and then\n            if (self.evals % self.adapt_every) == 0 and len(self.success_window) > 0:\n                success_rate = float(np.sum(self.success_window)) / max(1, len(self.success_window))\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if success_rate > 0.35:\n                    self.gscale *= 0.88\n                elif success_rate < 0.10:\n                    self.gscale *= 1.14\n                else:\n                    # mild jitter to escape stagnation\n                    if self.rng.rand() < 0.08:\n                        self.gscale *= (1.0 + (self.rng.rand() - 0.5) * 0.12)\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-5, 6.0))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                arr_f_all = np.array([a[1] for a in self.archive], dtype=float)\n                finite_idxs_all = np.nonzero(np.isfinite(arr_f_all))[0]\n                kept_ix = set()\n                # keep best ones\n                if finite_idxs_all.size > 0:\n                    sorted_finite_all = finite_idxs_all[np.argsort(arr_f_all[finite_idxs_all])]\n                    keep_best = max(2, int(self.archive_max * 0.5))\n                    for i in sorted_finite_all[:keep_best]:\n                        kept_ix.add(int(i))\n                # keep some recent ones\n                start_recent = max(0, len(self.archive) - 20)\n                for i in range(start_recent, len(self.archive)):\n                    kept_ix.add(int(i))\n                # keep some randoms for diversity\n                candidates = list(set(range(len(self.archive))) - kept_ix)\n                self.rng.shuffle(candidates)\n                keep_rand = max(0, self.archive_max - len(kept_ix))\n                for i in candidates[:keep_rand]:\n                    kept_ix.add(int(i))\n                # rebuild archive\n                new_archive = [self.archive[i] for i in sorted(list(kept_ix))]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_threshold and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(30, max(6, self.dim * 4))\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    if self.best_x is None:\n                        cand = self._uniform_array(lb, ub)\n                    else:\n                        jitter = self.rng.randn(self.dim) * (per_dim_scale * max(0.9, self.gscale) * 0.8)\n                        cand = self.best_x + jitter\n                        cand = self._reflect_bounds(cand, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale *= 1.12\n                self.gscale = float(np.clip(self.gscale, 1e-5, 6.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [t for t in self.archive if np.isfinite(t[1])]\n        if len(finite_archive) > 0:\n            best_item = min(finite_archive, key=lambda p: p[1])\n            self.best_x = best_item[0].copy()\n            self.best_f = float(best_item[1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.best_x is None:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x) if self.evals < self.budget else np.inf\n                self.best_x = x.copy()\n                self.best_f = f\n\n        return float(self.best_f), np.asarray(self.best_x, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 58, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "error": "In the code, line 58, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb_attr = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "cc542ab6-76e6-44f7-9799-c9028f1b9c1a", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The code will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # adaptive global multiplier for per-dim scales\n        self.gscale = 0.15\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n        self.archive = []  # list of (x, f)\n        self.evals = 0\n\n        # adaptation windows & counters\n        self.success_window = deque(maxlen=120)\n        self.last_improve_at = 0\n        self.stagnation_threshold = max(60, min(500, self.budget // 10))\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"Try common patterns for bounds, otherwise return [-5,5]^dim\"\"\"\n        # default\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # If bounds is a tuple/list like (lb, ub)\n            if isinstance(b, (tuple, list)) and len(b) == 2:\n                try:\n                    _lb = np.asarray(b[0], dtype=float)\n                    _ub = np.asarray(b[1], dtype=float)\n                    if _lb.size in (1, self.dim) and _ub.size in (1, self.dim):\n                        if _lb.size == 1:\n                            _lb = np.full(self.dim, _lb.item(), dtype=float)\n                        if _ub.size == 1:\n                            _ub = np.full(self.dim, _ub.item(), dtype=float)\n                        lb, ub = _lb, _ub\n                except Exception:\n                    pass\n            else:\n                # object with attributes lb/ub or lower/upper\n                _lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                _ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n                try:\n                    if _lb is not None and _ub is not None:\n                        _lb = np.asarray(_lb, dtype=float)\n                        _ub = np.asarray(_ub, dtype=float)\n                        if _lb.size == 1:\n                            _lb = np.full(self.dim, _lb.item(), dtype=float)\n                        if _ub.size == 1:\n                            _ub = np.full(self.dim, _ub.item(), dtype=float)\n                        lb, ub = _lb, _ub\n                except Exception:\n                    pass\n\n        # Ensure shapes\n        if lb.shape != (self.dim,):\n            lb = np.full(self.dim, float(lb.flat[0]) if hasattr(lb, 'flat') else -5.0)\n        if ub.shape != (self.dim,):\n            ub = np.full(self.dim, float(ub.flat[0]) if hasattr(ub, 'flat') else 5.0)\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        \"\"\"Reflect out-of-bounds coordinates back into [lb,ub] by mirroring.\"\"\"\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (np.any(low_mask) or np.any(high_mask)):\n                break\n            # reflect low\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            # reflect high\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            # small random jitter if still problematic\n            x = np.where(np.isfinite(x), x, (lb + (ub - lb) * self.rng.rand(self.dim)))\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        \"\"\"Simple Latin hypercube in [0,1) returning shape (n,dim).\"\"\"\n        rng = self.rng\n        result = np.empty((n, dim), dtype=float)\n        for j in range(dim):\n            perm = rng.permutation(n)\n            # stratified samples within each cell\n            result[:, j] = (perm + rng.rand(n)) / float(n)\n        return result\n\n    def _uniform_array(self, lb, ub, n=1):\n        \"\"\"Uniform samples in [lb,ub]. If n==1 returns 1D array, else shape (n,dim).\"\"\"\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + (ub - lb) * self.rng.rand(self.dim)\n        else:\n            return lb + (ub - lb) * self.rng.rand(n, self.dim)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"Coordinate-wise Cauchy tempered with a cap and occasional heavy tail.\"\"\"\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        dim = scale_vec.size\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # occasional heavy tail multiplier\n        heavy = self.rng.rand(dim) < 0.03\n        mult = np.where(heavy, 10.0, 1.0)\n        c = c * mult\n        # temper extreme values smoothly and then scale\n        # use tanh to compress extremely large c while preserving variance\n        tempered = np.tanh(c / 6.0) * (np.abs(c) + 1e-12)\n        # scale per-dim and cap to avoid nonsense\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        step = np.sign(tempered) * np.minimum(np.abs(tempered) * scale_vec, cap)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        self.archive = []\n        self.evals = 0\n        self.best_f = np.inf\n        self.best_x = None\n        self.success_window.clear()\n        self.last_improve_at = 0\n\n        # choose initial batch size proportional to dim but limited by budget\n        n_init = min(max(12, 4 * self.dim), max(2, self.budget // 10))\n        n_init = min(n_init, self.budget)\n\n        X0 = lb + (ub - lb) * self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            x = X0[i].copy()\n            x = self._reflect_bounds(x, lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity if budget allows\n        adds = min(6, max(0, self.budget - self.evals))\n        for _ in range(adds):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        if (self.best_x is None or not np.isfinite(self.best_f)) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample random if no archive\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            iqr = np.subtract(*np.percentile(arr_X, [75, 25], axis=0))\n            per_dim_scale = iqr / 1.349  # approx std from iqr\n            zero_mask = per_dim_scale <= 1e-12\n            if np.any(zero_mask):\n                fallback = np.std(arr_X, axis=0) + 1e-12\n                per_dim_scale[zero_mask] = fallback[zero_mask]\n            # normalize to domain fraction to keep scales comparable across functions\n            scale_vec = per_dim_scale * (domain_width / (domain_width + 1e-12))\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            finite_idxs = np.where(finite_mask)[0]\n            if finite_idxs.size > 0:\n                sorted_finite = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                topk = max(1, min(len(sorted_finite), int(max(3, 0.2 * len(sorted_finite)))))\n                candidates = sorted_finite[:topk]\n                weights = np.exp(-np.arange(len(candidates), dtype=float))\n                probs = weights / np.sum(weights)\n                anchor_idx = int(self.rng.choice(candidates, p=probs))\n            else:\n                anchor_idx = int(self.rng.randint(0, arr_X.shape[0]))\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            x_new = None\n            strat = self.rng.rand()\n            # Strategy weights roughly: DE ~25%, PCA ~25%, Local ~25%, Cauchy ~25%\n            if strat < 0.25 and arr_X.shape[0] >= 3:\n                # Strategy 1: DE-style differential injection\n                others = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                r1, r2 = self.rng.choice(others, size=2, replace=False)\n                F = 0.4 + 0.6 * self.rng.rand()  # differential factor [0.4,1.0]\n                diff = F * (arr_X[r1] - arr_X[r2])\n                jitter = (per_dim_scale * self.gscale) * (0.5 + self.rng.rand(self.dim))\n                candidate = anchor + diff + jitter * self.rng.randn(self.dim)\n                x_new = candidate\n            elif strat < 0.50 and finite_idxs.size >= 2:\n                # Strategy 2: PCA-guided elite perturbation\n                elite_count = max(2, min(len(finite_idxs), int(max(2, 0.2 * len(finite_idxs)))))\n                elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])][:elite_count]\n                elite_X = arr_X[elite_idxs]\n                cov = np.cov(elite_X.T) if elite_X.shape[0] > 1 else np.atleast_2d(np.zeros(self.dim))\n                # stable eigen-decomposition\n                vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                vals = np.maximum(vals, 0.0)\n                coeffs = self.rng.randn(self.dim) * np.sqrt(vals) * (1.0 + 0.5 * self.rng.rand())\n                perturb = vecs.dot(coeffs) * (self.gscale * 1.2)\n                x_new = anchor + perturb\n                # small mix with local jitter sometimes\n                if self.rng.rand() < 0.25:\n                    x_new = 0.7 * anchor + 0.3 * x_new + (per_dim_scale * self.gscale) * 0.5 * self.rng.randn(self.dim)\n            elif strat < 0.75:\n                # Strategy 3: Local anisotropic gaussian\n                coords = self.rng.rand(self.dim) < (0.6 + 0.3 * self.rng.rand())\n                if not np.any(coords):\n                    coords[self.rng.randint(0, self.dim)] = True\n                noise = np.zeros(self.dim)\n                local_factors = (0.2 + 0.8 * self.rng.rand(self.dim))\n                noise[coords] = self.rng.randn(coords.sum()) * (per_dim_scale[coords] * self.gscale * local_factors[coords])\n                # occasionally mix in a few uniform coords\n                if self.rng.rand() < 0.08:\n                    uni_coords = self.rng.rand(self.dim) < 0.08\n                    noise[uni_coords] = (ub[uni_coords] - lb[uni_coords]) * (self.rng.rand(uni_coords.sum()) - 0.5) * 0.6\n                x_new = anchor + noise\n            else:\n                # Strategy 4: tempered Cauchy global escape\n                step = self._tempered_cauchy(scale_vec * max(0.5, self.gscale))\n                x_new = anchor + step\n                # sometimes shift anchor toward a random archive member for diversity\n                if self.rng.rand() < 0.4 and arr_X.shape[0] > 1:\n                    other = arr_X[self.rng.randint(0, arr_X.shape[0])]\n                    x_new = 0.6 * x_new + 0.4 * other\n\n            # sometimes add a small differential jump for diversity\n            if x_new is not None and self.rng.rand() < 0.08 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice([i for i in range(arr_X.shape[0]) if i != anchor_idx], size=2, replace=False)\n                small_diff = 0.25 * (arr_X[i1] - arr_X[i2]) * (0.2 + 0.8 * self.rng.rand())\n                x_new = x_new + small_diff\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.035:\n                x_new = 0.85 * x_new + 0.15 * self.best_x\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = self._uniform_array(lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad_coords = ~(np.isfinite(x_new))\n            if np.any(bad_coords):\n                x_new[bad_coords] = self._uniform_array(lb[bad_coords], ub[bad_coords]) if bad_coords.sum() > 1 else self._uniform_array(lb, ub)[bad_coords]\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(np.asarray(x_new, dtype=float), lb, ub)\n\n            # evaluate candidate if budget allows\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n\n            # update best if improved\n            improved = np.isfinite(f_new) and f_new < self.best_f\n            if improved:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            finite_vals = np.array([v for (_, v) in self.archive if np.isfinite(v)], dtype=float)\n            if finite_vals.size > 0 and np.isfinite(f_new):\n                threshold = np.percentile(finite_vals, 25)\n                self.success_window.append(1 if f_new <= threshold else 0)\n            else:\n                self.success_window.append(0)\n\n            # adaptation of gscale every now and then\n            if len(self.success_window) >= 20 and (self.evals % 20 == 0):\n                succ_rate = float(np.mean(self.success_window))\n                if succ_rate > 0.35:\n                    # too easy -> contract a bit\n                    self.gscale *= 0.92\n                elif succ_rate < 0.12:\n                    # too hard -> expand\n                    self.gscale *= 1.12\n                else:\n                    # mild jitter to escape stagnation\n                    self.gscale *= float(np.exp(self.rng.normal(0, 0.02)))\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-6, 2.0))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                # Keep best ones and some randoms to maintain diversity\n                arr_f_all = np.array([v for (_, v) in self.archive], dtype=float)\n                best_idxs = np.argsort(arr_f_all)[: max(1, int(self.archive_max * 0.5))]\n                remaining = [i for i in range(len(self.archive)) if i not in best_idxs]\n                keep_rand = max(0, self.archive_max - len(best_idxs))\n                if remaining and keep_rand > 0:\n                    rand_choice = self.rng.choice(remaining, size=min(len(remaining), keep_rand), replace=False)\n                    keep_idxs = list(best_idxs) + list(rand_choice)\n                else:\n                    keep_idxs = list(best_idxs)\n                # rebuild archive from kept indices (preserve order roughly)\n                self.archive = [self.archive[i] for i in keep_idxs]\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_threshold and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                if self.best_x is not None and np.isfinite(self.best_f):\n                    base = self.best_x.copy()\n                    jitter_scale = 0.06 + 0.12 * (1.0 - float(self.evals) / max(1.0, self.budget))\n                    n_local = min(18, max(6, 3 * self.dim))\n                    for _ in range(n_local):\n                        if self.evals >= self.budget:\n                            break\n                        jitter = (per_dim_scale * jitter_scale) * self.rng.randn(self.dim)\n                        cand = self._reflect_bounds(base + jitter * domain_width, lb, ub)\n                        f = safe_eval(cand)\n                        self.archive.append((cand.copy(), f))\n                        if np.isfinite(f) and f < self.best_f:\n                            self.best_f = f\n                            self.best_x = cand.copy()\n                            self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.25, 1e-6, 2.0))\n                # reset small success window and stagnation timer\n                self.success_window.clear()\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [t for t in self.archive if np.isfinite(t[1])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda t: t[1])\n            self.best_x = best[0].copy()\n            self.best_f = float(best[1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            x = self._uniform_array(lb, ub)\n            f = float(func(x)) if self.evals < self.budget else np.inf\n            self.best_x = x\n            self.best_f = f\n\n        return float(self.best_f), np.asarray(self.best_x, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=120)", "error": "In the code, line 33, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=120)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0b10cce8-fda5-4c4f-92fb-d381491b6f9b", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The code will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # adaptive global multiplier for proposals (gscale)\n        self.gscale = 0.6  # global exploration scale\n        self.gscale_min = 1e-4\n        self.gscale_max = 8.0\n\n        # bookkeeping\n        self.evals = 0\n        self.archive = []  # list of tuples (x, f)\n        self.best_x = None\n        self.best_f = np.inf\n        self.last_improve_at = 0\n\n        # adaptation windows\n        self.success_window = deque(maxlen=100)  # short-term success history\n        self.window_len = 100\n        self.stagnation_threshold = max(200, 20 * self.dim)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to extract bounds from func in common patterns; fallback to [-5,5]^dim\n        \"\"\"\n        try:\n            # pattern: func.bounds -> tuple (lb, ub) or object with lb/ub\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # tuple-like (lb, ub)\n                if isinstance(b, tuple) or isinstance(b, list):\n                    lb, ub = b\n                else:\n                    # object with attributes\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n            else:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        # Normalize scalars to vectors and fill defaults\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # final safety: ensure shapes match\n        if lb.shape != (self.dim,):\n            lb = np.resize(lb, (self.dim,))\n        if ub.shape != (self.dim,):\n            ub = np.resize(ub, (self.dim,))\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        \"\"\"\n        Reflect coordinates that are out of bounds back into the domain [lb, ub].\n        This does repeated reflection in case reflections still exceed bounds.\n        \"\"\"\n        x = x.copy()\n        for _ in range(max_iter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (np.any(below) or np.any(above)):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        \"\"\"\n        Simple Latin Hypercube Sampling in [0,1).\n        Returns (n,dim) array.\n        \"\"\"\n        result = np.zeros((n, dim))\n        for d in range(dim):\n            perm = self.rng.permutation(n)\n            # uniform in each subinterval\n            result[:, d] = (perm + self.rng.rand(n)) / float(n)\n        return result\n\n    def _uniform_array(self, lb, ub, n=1):\n        \"\"\"\n        Uniform sampling within bounds. If n==1 returns 1D array shape (dim,)\n        else returns shape (n,dim).\n        \"\"\"\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size)\n        samples = self.rng.rand(*shape) * (ub - lb) + lb\n        if n == 1:\n            return samples[0]\n        return samples\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy: sample standard Cauchy via tan(pi*(u-0.5)),\n        temper large values with tanh to avoid wild extremes, but allow occasional heavy tail.\n        Returns 1D array with same size as scale_vec.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper with tanh to shrink very large picks smoothly\n        factor = np.tanh(np.abs(c)) / (np.abs(c) + 1e-12)\n        tempered = np.sign(c) * (np.abs(c) * factor)\n        # occasional heavy tail (small prob) bypass tempering\n        mask_heavy = self.rng.rand(self.dim) < 0.02\n        if np.any(mask_heavy):\n            tempered[mask_heavy] = c[mask_heavy]\n        # scale per-dim and cap by cap_multiplier*scale_vec\n        scaled = tempered * scale_vec\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        scaled = np.clip(scaled, -cap, cap)\n        return scaled\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n        # safety\n        domain_width = np.maximum(domain_width, 1e-12)\n        if np.any(domain_width <= 0):\n            # degenerate domain, return one sample\n            x = np.maximum(np.minimum(np.zeros(self.dim), ub), lb)\n            f = func(x)\n            return f, x\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        # choose number of initial samples (small fraction of budget, but at least some)\n        n_init = int(min(max(20, 4 * self.dim), max(5, self.budget // 10)))\n        n_init = max(1, min(n_init, self.budget))\n        X0 = self._lhs01(n_init, self.dim) * domain_width + lb\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive.append((x.copy(), float(f)))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = float(f)\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniform samples for diversity (bounded by budget)\n        n_extra = min(10, max(0, self.budget - self.evals))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), float(f)))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = float(f)\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), float(f)))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = float(f)\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        adapt_every = max(10, min(200, self.window_len // 3))\n        # main loop\n        while self.evals < self.budget:\n            # convert archive to arrays where possible\n            if len(self.archive) == 0:\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), float(f_new)))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = float(f_new); self.best_x = x_new.copy()\n                continue\n\n            arr_X = np.array([a[0] for a in self.archive], dtype=float)\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR -> std approximation, fallback to std\n            if arr_X.shape[0] >= 4:\n                q75 = np.percentile(arr_X, 75, axis=0)\n                q25 = np.percentile(arr_X, 25, axis=0)\n                per_dim_iqr = q75 - q25\n                per_dim_scale = per_dim_iqr / 1.349  # approx convert to std\n                # handle zeros by fallback to std\n                per_dim_scale = np.where(per_dim_scale <= 1e-12, np.std(arr_X, axis=0) + 1e-12, per_dim_scale)\n            else:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n\n            # basic candidate selection strategy mixture\n            # choose anchor: biased toward elites\n            finite_mask = np.isfinite(arr_f)\n            if finite_mask.any():\n                finite_idxs = np.where(finite_mask)[0]\n                # elite fraction\n                n_elite = max(1, int(max(1, 0.2 * finite_idxs.size)))\n                sorted_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                elite_idxs = sorted_idxs[:n_elite]\n                # candidates pool is finite indices\n                candidates = finite_idxs\n                # exponential bias over candidate ranks\n                ranks = np.empty(candidates.size, dtype=float)\n                # compute ranks by sorted position among finite\n                rank_map = {idx: pos for pos, idx in enumerate(sorted_idxs)}\n                for j, idx in enumerate(candidates):\n                    ranks[j] = rank_map.get(idx, len(sorted_idxs))\n                weights = np.exp(-0.5 * ranks)\n                weights = weights / np.sum(weights)\n                anchor_idx = int(self.rng.choice(candidates, p=weights))\n                anchor = arr_X[anchor_idx].copy()\n            else:\n                # no finite values yet, choose randomly\n                anchor = self._uniform_array(lb, ub)\n\n            # mixture choice\n            r = self.rng.rand()\n            x_new = None\n            candidate = None\n\n            # Strategy 1: DE-style differential injection (~25%)\n            if r < 0.25 and arr_X.shape[0] >= 3:\n                # pick two other distinct archive members\n                idxs = list(range(arr_X.shape[0]))\n                # ensure anchor index removed if present\n                try:\n                    if anchor is not None:\n                        # find nearest anchor index in arr_X (approx)\n                        anchor_idx_in_arr = np.argmin(np.sum((arr_X - anchor) ** 2, axis=1))\n                        idxs.remove(int(anchor_idx_in_arr))\n                except Exception:\n                    pass\n                i1, i2 = self.rng.choice(idxs, size=2, replace=False)\n                xb = arr_X[i1].copy()\n                xc = arr_X[i2].copy()\n                F = 0.8 * self.gscale\n                diff = F * (xb - xc)\n                jitter = self.rng.normal(scale=0.03, size=self.dim) * per_dim_scale\n                candidate = anchor + diff + jitter\n                # occasionally mix a few uniform coordinates\n                if self.rng.rand() < 0.08:\n                    mix_coords = self.rng.rand(self.dim) < 0.08\n                    if np.any(mix_coords):\n                        candidate[mix_coords] = self._uniform_array(lb, ub)[mix_coords]\n                x_new = candidate\n\n            # Strategy 2: PCA-guided elite perturbation (~25%)\n            elif r < 0.50 and arr_X.shape[0] >= 3:\n                # build elites\n                if finite_mask.any():\n                    elites = arr_X[elite_idxs]\n                else:\n                    elites = arr_X\n                mu = np.mean(elites, axis=0)\n                # stable eigen-decomposition via SVD on centered data\n                Xc = elites - mu\n                # if too few points, fallback to isotropic gaussian\n                if Xc.shape[0] >= 2:\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        eigvals = (S ** 2) / max(1, Xc.shape[0] - 1)\n                        # sample coefficients along principal axes\n                        coeffs = self.rng.normal(scale=np.sqrt(np.maximum(eigvals, 0.0))) * (0.8 * self.gscale)\n                        # reconstruct perturbation\n                        perturb = (coeffs @ Vt)\n                        candidate = anchor + perturb\n                    except Exception:\n                        candidate = anchor + self.rng.normal(scale=per_dim_scale * 0.3) * self.gscale\n                else:\n                    candidate = anchor + self.rng.normal(scale=per_dim_scale * 0.3) * self.gscale\n\n                # move a bit towards candidate to maintain anchor locality\n                x_new = anchor + 0.6 * (candidate - anchor)\n                # small fallback jitter\n                if np.any(~np.isfinite(x_new)):\n                    x_new = anchor + self.rng.normal(scale=per_dim_scale * 0.05)\n\n            # Strategy 3: Local anisotropic gaussian (~25%)\n            elif r < 0.75:\n                prob_coord = 0.25 + 0.4 * self.gscale / (1.0 + self.gscale)\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not np.any(coords):\n                    coords[self.rng.randint(self.dim)] = True\n                candidate = anchor.copy()\n                # anisotropic gaussian per coordinate\n                candidate[coords] += self.rng.normal(scale=per_dim_scale[coords] * (0.4 * self.gscale))\n                # occasionally mix in a few uniform coords\n                if self.rng.rand() < 0.08:\n                    mix_coords = self.rng.rand(self.dim) < 0.06\n                    if np.any(mix_coords):\n                        candidate[mix_coords] = self._uniform_array(lb, ub)[mix_coords]\n                x_new = candidate\n\n            # Strategy 4: tempered Cauchy global escape (rest)\n            else:\n                scale_vec = np.maximum(domain_width * 0.5, per_dim_scale) * (1.0 * self.gscale)\n                jump = self._tempered_cauchy(scale_vec)\n                x_new = anchor + jump\n\n            # sometimes add a small differential jump for diversity\n            if self.rng.rand() < 0.06 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new += 0.1 * self.gscale * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.02:\n                # small jitter around best\n                x_new = self.best_x + self.rng.normal(scale=per_dim_scale * 0.15) * (0.5 * self.gscale)\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = self._uniform_array(lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad_coords = ~np.isfinite(x_new)\n            if np.any(bad_coords):\n                x_new[bad_coords] = self._uniform_array(lb, ub)[bad_coords]\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n            f_new = float(f_new)\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n\n            # update best\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                improved = True\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            finite_vals = [a[1] for a in self.archive if np.isfinite(a[1])]\n            if len(finite_vals) >= 4 and np.isfinite(f_new):\n                threshold = np.percentile(finite_vals, 25)\n                self.success_window.append(1 if f_new <= threshold else 0)\n            else:\n                # if we found any finite improvement count it as some success\n                if improved:\n                    self.success_window.append(1)\n                else:\n                    self.success_window.append(0)\n\n            # adaptation of gscale every now and then\n            if (self.evals % adapt_every) == 0 and len(self.success_window) > 0:\n                succ_rate = float(np.mean(self.success_window))\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    self.gscale *= 0.92\n                elif succ_rate < 0.15:\n                    self.gscale *= 1.12\n                # small jitter to escape stagnation\n                if self.rng.rand() < 0.05:\n                    self.gscale *= (1.0 + 0.1 * (self.rng.rand() - 0.5))\n                # clamp\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                arr_f_all = np.array([a[1] for a in self.archive], dtype=float)\n                finite_mask_all = np.isfinite(arr_f_all)\n                keep_best = int(self.archive_max * 0.6)\n                keep_rand = self.archive_max - keep_best\n                keep_idxs = []\n                if finite_mask_all.any():\n                    finite_idxs_all = np.where(finite_mask_all)[0]\n                    sorted_finite_all = finite_idxs_all[np.argsort(arr_f_all[finite_idxs_all])]\n                    for i in sorted_finite_all[:keep_best]:\n                        keep_idxs.append(int(i))\n                else:\n                    # if none finite, just pick random ones to keep\n                    keep_idxs = list(self.rng.choice(range(len(self.archive)), size=keep_best, replace=False))\n                # keep current best if present\n                if self.best_x is not None:\n                    # find closest index to best_x\n                    dists = np.sum((np.array([a[0] for a in self.archive]) - self.best_x) ** 2, axis=1)\n                    best_idx = int(np.argmin(dists))\n                    if best_idx not in keep_idxs:\n                        keep_idxs.append(best_idx)\n                # add random ones for diversity\n                all_idxs = list(range(len(self.archive)))\n                candidates = [i for i in all_idxs if i not in keep_idxs]\n                if len(candidates) > 0:\n                    self.rng.shuffle(candidates)\n                    for i in candidates[:keep_rand]:\n                        keep_idxs.append(int(i))\n                # rebuild archive\n                new_archive = [self.archive[i] for i in sorted(set(keep_idxs))]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_threshold and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(20, max(6, 6 * self.dim))\n                base = self.best_x.copy() if self.best_x is not None else self._uniform_array(lb, ub)\n                for _ in range(min(n_local, self.budget - self.evals)):\n                    jitter = self.rng.normal(scale=per_dim_scale * (0.2 + 0.2 * self.gscale))\n                    cand = self._reflect_bounds(base + jitter, lb, ub)\n                    f_cand = safe_eval(cand)\n                    self.archive.append((cand.copy(), float(f_cand)))\n                    if np.isfinite(f_cand) and f_cand < self.best_f:\n                        self.best_f = float(f_cand)\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_rand = min(6, max(2, self.dim))\n                for _ in range(n_rand):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_cand = safe_eval(cand)\n                    self.archive.append((cand.copy(), float(f_cand)))\n                    if np.isfinite(f_cand) and f_cand < self.best_f:\n                        self.best_f = float(f_cand)\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.15, self.gscale_min, self.gscale_max))\n                # reset small success window and stagnation timer\n                self.success_window.clear()\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [a for a in self.archive if np.isfinite(a[1])]\n        if len(finite_archive) > 0:\n            best_pair = min(finite_archive, key=lambda z: z[1])\n            return float(best_pair[1]), best_pair[0].copy()\n        else:\n            # no finite evaluation results (unlikely) -> return random feasible if possible\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                return float(f), x\n            # else return last seen archive entry or a uniform\n            if len(self.archive) > 0:\n                return float(self.archive[-1][1]), self.archive[-1][0].copy()\n            x = self._uniform_array(lb, ub)\n            return float(func(x)), x", "configspace": "", "generation": 0, "feedback": "In the code, line 36, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=100)  # short-term success history", "error": "In the code, line 36, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=100)  # short-term success history", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "bfbda84e-02ed-4faa-9053-8ef77cc5b04d", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The algorithm will try to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # global multiplier for per-dim scales (adaptive)\n        self.gscale = 1.0\n\n        # bookkeeping\n        self.evals = 0\n        self.archive = []  # list of (x (ndarray copy), f)\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows & counters\n        self.window_len = max(30, self.budget // 100)\n        self.success_window = []\n        self.last_improve_at = 0\n        self.restart_patience = max(200, self.budget // 20)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # Try func.bounds as object or tuple/list\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            try:\n                # tuple-like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # object with lb/ub attributes (vector or scalar)\n                    lb_attr = getattr(b, \"lb\", None)\n                    ub_attr = getattr(b, \"ub\", None)\n                    if lb_attr is not None and ub_attr is not None:\n                        lb = np.asarray(lb_attr, dtype=float)\n                        ub = np.asarray(ub_attr, dtype=float)\n            except Exception:\n                lb, ub = None, None\n        # also try direct attributes on func\n        if lb is None:\n            lb_attr = getattr(func, \"lb\", None)\n            ub_attr = getattr(func, \"ub\", None)\n            if lb_attr is not None and ub_attr is not None:\n                try:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                except Exception:\n                    lb, ub = None, None\n\n        # Normalize scalars to vectors\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            if lb.ndim == 0:\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if ub.ndim == 0:\n                ub = np.full(self.dim, float(ub), dtype=float)\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim).astype(float)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim).astype(float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # Mirror reflection: reflect any coordinate outside [lb,ub] back inside\n        x = x.astype(float).copy()\n        for _ in range(max_iter):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = lb + (lb - x)\n            x[below] = lb[below] + (lb[below] - x[below])\n            # reflect above: x = ub - (x - ub)\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        # stratified samples in each cell\n        if n <= 0:\n            return np.empty((0, dim))\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = self.rng.rand(n, dim)\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            idx = self.rng.permutation(n)\n            points[:, j] = cut[:n] + u[:, j] * (cut[1] - cut[0])\n            points[:, j] = np.clip(points[:, j], 0.0, 1.0 - 1e-12)\n            # shuffle to get permutation\n            points[:, j] = points[idx, j]\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size)\n        r = self.rng.rand(*shape)\n        return lb + r * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        d = scale_vec.size\n        u = self.rng.rand(d)\n        # standard Cauchy via tan(pi*(u-0.5))\n        c = np.tan(np.pi * (u - 0.5))\n        # temper large values with tanh factor, allow occasional heavy tail\n        abs_c = np.abs(c)\n        # factor compresses most samples, leaves occasional heavy tail\n        factor = np.tanh(abs_c / (1.5 + self.rng.rand() * 0.5)) + (self.rng.rand(d) < 0.02) * (1.0 + abs_c * 0.8)\n        tempered = np.sign(c) * (abs_c * factor)\n        cap = cap_multiplier * scale_vec\n        res = np.clip(tempered * scale_vec, -cap, cap)\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            # ensure 1d numpy array\n            x_arr = np.asarray(x, dtype=float).reshape(self.dim)\n            f = func(x_arr)\n            # coerce to float (some functions return other types)\n            try:\n                f = float(f)\n            except Exception:\n                pass\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        init_n = min(max(8, self.dim * 4), max(12, self.budget // 200))\n        init_n = min(init_n, max(4, self.budget // 20))\n        points01 = self._lhs01(init_n, self.dim)\n        xs = lb + points01 * (ub - lb)\n        for i in range(xs.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = self._reflect_bounds(xs[i], lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        extra_uniforms = min(6, max(0, self.budget - self.evals))\n        for _ in range(extra_uniforms):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)[0]\n            x = self._reflect_bounds(x, lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        if (not np.isfinite(self.best_f)) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub, n=1)[0]\n            x = self._reflect_bounds(x, lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample a random point\n                x_new = self._uniform_array(lb, ub, n=1)[0]\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            arr_X = np.array([a[0] for a in self.archive], dtype=float)\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            finite_mask = np.isfinite(arr_f)\n            finite_idxs = np.where(finite_mask)[0]\n            if finite_idxs.size == 0:\n                # fallback random exploration\n                x_new = self._uniform_array(lb, ub, n=1)[0]\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            if finite_idxs.size >= 4:\n                q75 = np.percentile(arr_X[finite_idxs], 75, axis=0)\n                q25 = np.percentile(arr_X[finite_idxs], 25, axis=0)\n                iqr = q75 - q25\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                fallback = np.std(arr_X[finite_idxs], axis=0)\n                per_dim_scale = np.where(per_dim_scale > 1e-12, per_dim_scale, fallback)\n            else:\n                per_dim_scale = np.std(arr_X[finite_idxs], axis=0)\n            # if still zeros fallback to (ub-lb)/6\n            small_mask = per_dim_scale < 1e-9\n            per_dim_scale[small_mask] = (ub - lb)[small_mask] / 6.0 + 1e-9\n\n            # choose anchor: bias to elites via exponential bias\n            sorted_idx = np.argsort(arr_f)  # ascending\n            ranks = np.empty_like(sorted_idx)\n            ranks[sorted_idx] = np.arange(len(sorted_idx))\n            # weight higher for better ranks: use exp decay\n            weights = np.exp(-1.5 * ranks / max(1, len(ranks)))\n            weights = weights / np.sum(weights)\n            anchor_idx = self.rng.choice(len(weights), p=weights)\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            u = self.rng.rand()\n            x_new = None\n            # Strategy 1: DE-style differential injection (~25%)\n            if u < 0.25 and arr_X.shape[0] >= 3:\n                # pick two random donors different from anchor\n                ids = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                a_idx, b_idx = self.rng.choice(ids, size=2, replace=False)\n                a = arr_X[a_idx]\n                b = arr_X[b_idx]\n                F = 0.8 * (0.8 + 0.4 * self.rng.rand()) * self.gscale  # differential weight\n                donor = anchor + F * (a - b)\n                # anisotropic gaussian jitter\n                jitter = self.rng.randn(self.dim) * (per_dim_scale * 0.05 * (0.8 + self.rng.rand()))\n                x_new = donor + jitter\n\n            # Strategy 2: PCA-guided elite perturbation (~25%)\n            elif u < 0.5 and finite_idxs.size >= 3:\n                elite_count = max(3, int(0.18 * arr_X.shape[0]))\n                # pick elites (best)\n                best_sorted = np.argsort(arr_f[finite_idxs])\n                chosen_elite_idxs = finite_idxs[best_sorted[:elite_count]]\n                elite = arr_X[chosen_elite_idxs]\n                # center\n                center = np.mean(elite, axis=0)\n                # compute covariance and eigen-decomposition (stable)\n                cov = np.cov(elite.T)\n                # regularize\n                cov += np.eye(self.dim) * 1e-9\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    eigvals = np.maximum(eigvals, 0.0)\n                    # sample coefficients along principal axes\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(eigvals + 1e-9)\n                    # scale by gscale and per-dim typical scale\n                    coeffs *= (0.8 + 0.6 * self.rng.rand()) * self.gscale\n                    delta = eigvecs.dot(coeffs)\n                    x_new = center + delta\n                except Exception:\n                    # fallback to small local jitter\n                    x_new = anchor + self.rng.randn(self.dim) * (per_dim_scale * 0.04)\n\n            # Strategy 3: Local anisotropic gaussian (~25%)\n            elif u < 0.75:\n                # choose a mask of coordinates to vary\n                mask_prob = 0.2 + 0.6 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < mask_prob\n                # ensure at least one changed\n                if not coords.any():\n                    coords[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise[coords] = self.rng.randn(coords.sum()) * (per_dim_scale[coords] * (0.6 + 0.8 * self.rng.rand()) * self.gscale)\n                # occasionally mix in a few uniform coords\n                if self.rng.rand() < 0.06:\n                    uidx = self.rng.choice(self.dim, size=max(1, int(0.05 * self.dim)), replace=False)\n                    noise[uidx] += (self.rng.rand(uidx.size) - 0.5) * (ub[uidx] - lb[uidx]) * 0.02\n                x_new = anchor + noise\n\n            # Strategy 4: tempered Cauchy global escape (rest)\n            else:\n                # tempered heavy tail jump\n                cap_mult = 6.0 + 4.0 * self.rng.rand()\n                jump = self._tempered_cauchy(per_dim_scale * (1.2 + 1.5 * self.rng.rand()), cap_multiplier=cap_mult)\n                # occasionally bias towards archive centroid or best\n                if self.rng.rand() < 0.35 and finite_idxs.size > 0:\n                    centroid = np.mean(arr_X[finite_idxs], axis=0)\n                    x_new = centroid + jump * (0.4 + 0.8 * self.rng.rand())\n                else:\n                    x_new = anchor + jump\n\n            # sometimes add a small differential jump for diversity\n            if self.rng.rand() < 0.12 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(arr_X.shape[0], size=2, replace=False)\n                smallF = 0.15 * (0.8 + self.rng.rand()) * self.gscale\n                x_new += smallF * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if (self.best_x is not None) and (self.rng.rand() < 0.07):\n                tiny = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.04)\n                x_new = self.best_x + tiny\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * (per_dim_scale * 0.1 * self.gscale)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad = (~np.isfinite(x_new)) | np.isnan(x_new)\n            if bad.any():\n                rnd = self._uniform_array(lb, ub, n=1)[0]\n                x_new[bad] = rnd[bad]\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n            self.archive.append((x_new.copy(), f_new))\n\n            # record improvement\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                improved = True\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            finite_fs = arr_f[finite_mask] if finite_mask.any() else np.array([])\n            if finite_fs.size > 0 and np.isfinite(f_new):\n                rank = (finite_fs <= f_new).sum()\n                frac = rank / max(1, finite_fs.size)\n                success = frac <= 0.25  # new point is among top 25%\n            else:\n                success = np.isfinite(f_new) and improved\n\n            self.success_window.append(1 if success else 0)\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every now and then\n            if (self.evals % max(10, self.window_len)) == 0:\n                if len(self.success_window) >= 4:\n                    success_rate = np.mean(self.success_window)\n                    # if too easy (high success) -> contract slightly, if too hard -> expand\n                    if success_rate > 0.4:\n                        # contract (focus)\n                        self.gscale *= (0.9 + 0.08 * self.rng.rand())\n                    elif success_rate < 0.15:\n                        # expand (explore)\n                        self.gscale *= (1.05 + 0.12 * self.rng.rand())\n                    else:\n                        # mild jitter to escape stagnation\n                        self.gscale *= (0.98 + 0.06 * self.rng.rand())\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e3))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                # keep best ones and some randoms\n                keep_best = max(6, int(0.2 * self.archive_max))\n                # sort by f\n                archive_fs = np.array([a[1] for a in self.archive], dtype=float)\n                sorted_idx = np.argsort(archive_fs)\n                kept_ix = set(int(i) for i in sorted_idx[:keep_best])\n                # keep current best if present\n                if self.best_x is not None:\n                    # find index of best approx\n                    dists = np.linalg.norm(np.array([a[0] for a in self.archive]) - self.best_x, axis=1)\n                    best_idx_approx = int(np.argmin(dists))\n                    kept_ix.add(best_idx_approx)\n                # keep some random ones for diversity\n                remaining = [i for i in range(len(self.archive)) if i not in kept_ix]\n                n_rand_keep = max(0, self.archive_max - len(kept_ix))\n                if remaining and n_rand_keep > 0:\n                    chosen = self.rng.choice(remaining, size=min(len(remaining), n_rand_keep), replace=False)\n                    for i in chosen:\n                        kept_ix.add(int(i))\n                # build new archive\n                new_archive = []\n                for i in kept_ix:\n                    new_archive.append((self.archive[i][0].copy(), float(self.archive[i][1])))\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.restart_patience and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                if self.best_x is None:\n                    # if no best, randomize\n                    for _ in range(min(10, self.budget - self.evals)):\n                        x = self._uniform_array(lb, ub, n=1)[0]\n                        f = safe_eval(x)\n                        self.archive.append((x.copy(), f))\n                        if np.isfinite(f) and f < self.best_f:\n                            self.best_f = f\n                            self.best_x = x.copy()\n                            self.last_improve_at = self.evals\n                else:\n                    # generate cluster around best\n                    n_local = min(20, max(6, self.dim * 2))\n                    for _ in range(n_local):\n                        if self.evals >= self.budget:\n                            break\n                        scale = 0.1 + 0.4 * self.rng.rand()\n                        x = self.best_x + self.rng.randn(self.dim) * (per_dim_scale * scale * (1.0 + 0.5 * self.rng.rand()))\n                        x = self._reflect_bounds(x, lb, ub)\n                        f = safe_eval(x)\n                        self.archive.append((x.copy(), f))\n                        if np.isfinite(f) and f < self.best_f:\n                            self.best_f = f\n                            self.best_x = x.copy()\n                            self.last_improve_at = self.evals\n                    # add some global randoms to regain diversity\n                    n_global = min(12, max(4, self.dim))\n                    for _ in range(n_global):\n                        if self.evals >= self.budget:\n                            break\n                        x = self._uniform_array(lb, ub, n=1)[0]\n                        f = safe_eval(x)\n                        self.archive.append((x.copy(), f))\n                        if np.isfinite(f) and f < self.best_f:\n                            self.best_f = f\n                            self.best_x = x.copy()\n                            self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale *= (1.08 + 0.12 * self.rng.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e3))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [(x, f) for (x, f) in self.archive if np.isfinite(f)]\n        if len(finite_archive) > 0:\n            finite_archive.sort(key=lambda t: t[1])\n            best = finite_archive[0]\n            self.best_x = best[0].copy()\n            self.best_f = float(best[1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub, n=1)[0]\n                f = safe_eval(x)\n                self.best_x = x.copy()\n                self.best_f = float(f)\n            else:\n                # fallback to the first archive item\n                if len(self.archive) > 0:\n                    self.best_x = self.archive[0][0].copy()\n                    self.best_f = float(self.archive[0][1])\n                else:\n                    self.best_x = np.full(self.dim, lb.mean())\n                    self.best_f = float(np.inf)\n\n        return float(self.best_f), self.best_x.copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "47acf13e-c0ee-4b0b-b8d1-46c5ca3e6f96", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The code will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # adaptive global scale multiplier\n        self.gscale = 0.2  # global exploration multiplier\n\n        # bookkeeping\n        self.evals = 0\n        self.archive = []  # list of (x, f)\n        self.best_f = np.inf\n        self.best_x = None\n\n        # adaptation windows & counters\n        self.success_window = deque(maxlen=50)  # recent success flags\n        self.adapt_every = max(5, dim // 2)\n        self.iter_since_adapt = 0\n        self.iter_since_improve = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to read bounds from func in a few common ways; fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            try:\n                # tuple/list (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb, ub = b\n                # object with lb/ub attributes\n                elif hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = b.lb; ub = b.ub\n            except Exception:\n                lb = ub = None\n        # also try direct attributes\n        if lb is None and hasattr(func, \"lb\"):\n            lb = getattr(func, \"lb\")\n        if ub is None and hasattr(func, \"ub\"):\n            ub = getattr(func, \"ub\")\n\n        # normalize scalars to vectors\n        if lb is None or ub is None:\n            lb = -5.0\n            ub = 5.0\n        lb = np.asarray(lb).astype(float)\n        ub = np.asarray(ub).astype(float)\n        if lb.size == 1:\n            lb = np.repeat(lb.item(), self.dim)\n        if ub.size == 1:\n            ub = np.repeat(ub.item(), self.dim)\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect outside points back into [lb, ub]\n        x = x.copy()\n        for _ in range(max_iter):\n            mask_low = x < lb\n            if not mask_low.any() and not (x > ub).any():\n                break\n            # reflect low: x = lb + (lb - x)\n            x[mask_low] = lb[mask_low] + (lb[mask_low] - x[mask_low])\n            mask_high = x > ub\n            x[mask_high] = ub[mask_high] - (x[mask_high] - ub[mask_high])\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        u = rng.rand(n, dim)\n        samples = np.zeros((n, dim))\n        for i in range(dim):\n            perms = rng.permutation(n)\n            samples[:, i] = cut[:-1] + (u[:, i] * (cut[1] - cut[0]))\n            samples[:, i] = np.clip(samples[perms, i], 0, 1 - 1e-12)\n        return samples\n\n    def _uniform_array(self, lb, ub, n=1):\n        n = int(n)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise tempered Cauchy\n        u = self.rng.rand(self.dim)\n        raw = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # temper extremes: soft-limit via tanh, but occasionally keep heavy tail\n        heavy_prob = 0.03\n        heavy = self.rng.rand(self.dim) < heavy_prob\n        temper = np.tanh(raw / 3.0) * 3.0  # tempered moderate tail\n        vals = np.where(heavy, raw * 3.0, temper)\n        # cap extreme values\n        cap = cap_multiplier\n        vals = np.clip(vals, -cap, cap)\n        return vals * scale_vec\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        width = ub - lb\n        if np.any(width <= 0):\n            raise ValueError(\"Invalid bounds: ub must be > lb elementwise\")\n\n        # safe evaluator\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: LHS samples ----\n        init_budget = max(8, min(self.budget // 10, 40 + 4 * self.dim))\n        n_init = int(init_budget)\n        lhs = self._lhs01(n_init, self.dim)  # shape (n_init, dim)\n        xs_init = lb + lhs * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = xs_init[i]\n            f = safe_eval(x)\n            if np.isfinite(f):\n                self.archive.append((x.copy(), f))\n                if f < self.best_f:\n                    self.best_f = f\n                    self.best_x = x.copy()\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        for _ in range(min(10, self.budget - self.evals)):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            if np.isfinite(f):\n                self.archive.append((x.copy(), f))\n                if f < self.best_f:\n                    self.best_f = f\n                    self.best_x = x.copy()\n\n        # If still no finite best, sample one more random if budget allows\n        if self.best_x is None and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            if np.isfinite(f):\n                self.archive.append((x.copy(), f))\n                self.best_f = f\n                self.best_x = x.copy()\n\n        # If still none (very unlikely), give up\n        if len(self.archive) == 0:\n            # return random\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x) if self.evals < self.budget else np.inf\n            return f, x\n\n        # Main optimization loop\n        while self.evals < self.budget:\n            # convert archive to arrays\n            X = np.array([a[0] for a in self.archive])\n            F = np.array([a[1] for a in self.archive])\n            idx_sorted = np.argsort(F)\n            X_sorted = X[idx_sorted]\n            F_sorted = F[idx_sorted]\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            q1 = np.percentile(X, 25, axis=0)\n            q3 = np.percentile(X, 75, axis=0)\n            iqr = q3 - q1\n            scale_est = iqr / 1.349  # approximate std from IQR\n            fallback_std = np.std(X, axis=0, ddof=0)\n            scale_vec = np.where(scale_est > 1e-12, scale_est, fallback_std)\n            # final fallback: tiny positive\n            scale_vec = np.where(scale_vec > 1e-12, scale_vec, (ub - lb) * 1e-3)\n\n            # choose anchor: bias to elites (exponential bias over candidate ranks)\n            n_archive = len(X_sorted)\n            ranks = np.arange(n_archive)\n            # bias factor: pick index with exp(-alpha * rank)\n            alpha = 6.0 / max(1, n_archive)  # adapt to archive size\n            probs = np.exp(-alpha * ranks)\n            probs = probs / probs.sum()\n            chosen_idx = self.rng.choice(n_archive, p=probs)\n            anchor = X_sorted[chosen_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            strat = self.rng.rand()\n            x_cand = None\n\n            # Strategy 1: DE-style differential injection (~25%)\n            if strat < 0.25 and n_archive >= 3:\n                # pick two donors at random distinct from anchor\n                i1, i2 = self.rng.choice(n_archive, size=2, replace=False)\n                donor_diff = X_sorted[i1] - X_sorted[i2]\n                jitter = self.rng.normal(0, 1.0, size=self.dim) * (scale_vec * 0.1)\n                F_scale = 0.8 + 0.8 * self.rng.rand()  # differential weight\n                x_cand = anchor + F_scale * donor_diff * (self.gscale)\n                # anisotropic gaussian jitter\n                x_cand += jitter * self.gscale\n            # Strategy 2: PCA-guided elite perturbation (~25%)\n            elif strat < 0.5:\n                n_elite = max(3, int(max(3, 0.15 * n_archive)))\n                elites = X_sorted[:n_elite]\n                # compute covariance with small regularization\n                cov = np.cov(elites.T) if elites.shape[0] > 1 else np.eye(self.dim) * 1e-6\n                cov += np.eye(self.dim) * 1e-8\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    eigvals = np.maximum(eigvals, 1e-12)\n                    # sample coefficients along principal axes\n                    coeffs = self.rng.normal(0, 1.0, size=self.dim) * np.sqrt(eigvals) * (self.gscale * 1.2)\n                    x_cand = anchor + eigvecs.dot(coeffs)\n                except Exception:\n                    # fallback to small local jitter\n                    x_cand = anchor + self.rng.normal(0, 1.0, size=self.dim) * (scale_vec * 0.1 * self.gscale)\n            # Strategy 3: Local anisotropic gaussian (~25%)\n            elif strat < 0.75:\n                anis = self.rng.normal(0, 1.0, size=self.dim) * (scale_vec * 0.6 * self.gscale)\n                # occasionally mix in a few uniform coords\n                if self.rng.rand() < 0.15:\n                    mask = self.rng.rand(self.dim) < 0.15\n                    anis[mask] = (self._uniform_array(lb, ub) - anchor)[mask]\n                x_cand = anchor + anis\n            # Strategy 4: tempered Cauchy global escape (rest)\n            else:\n                x_cand = anchor + self._tempered_cauchy(scale_vec * (self.gscale * 0.8), cap_multiplier=6.0)\n\n            # sometimes add a small differential jump for diversity\n            if self.rng.rand() < 0.10 and n_archive >= 2:\n                i1, i2 = self.rng.choice(n_archive, size=2, replace=False)\n                x_cand += 0.2 * (X_sorted[i1] - X_sorted[i2]) * self.gscale\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.08:\n                x_cand = self.best_x + self.rng.normal(0, 1.0, size=self.dim) * (scale_vec * 0.05 * self.gscale)\n\n            # ensure candidate exists\n            if x_cand is None:\n                x_cand = anchor + self.rng.normal(0, 1.0, size=self.dim) * (scale_vec * 0.1 * self.gscale)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            mask_bad = (~np.isfinite(x_cand)) | (np.abs(x_cand) > 1e300)\n            if mask_bad.any():\n                x_cand[mask_bad] = self._uniform_array(lb, ub)[mask_bad]\n\n            # reflect to bounds & clamp\n            x_cand = self._reflect_bounds(x_cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_cand = safe_eval(x_cand)\n\n            # record into archive\n            if np.isfinite(f_cand):\n                self.archive.append((x_cand.copy(), f_cand))\n                # update best\n                if f_cand < self.best_f:\n                    self.best_f = f_cand\n                    self.best_x = x_cand.copy()\n                    self.iter_since_improve = 0\n                    success_flag = True\n                else:\n                    self.iter_since_improve += 1\n                    success_flag = False\n                # measure success relative to current archive: consider success if in top 25% finite\n                finite_F = F_sorted if F_sorted.size > 0 else np.array([f_cand])\n                q25 = np.percentile(np.concatenate((finite_F, [f_cand])), 25)\n                is_success = f_cand <= q25\n                self.success_window.append(bool(is_success))\n            else:\n                # treat non-finite as failure\n                self.iter_since_improve += 1\n                self.success_window.append(False)\n\n            # adaptation of gscale every now and then\n            self.iter_since_adapt += 1\n            if self.iter_since_adapt >= self.adapt_every:\n                self.iter_since_adapt = 0\n                succ_rate = np.mean(self.success_window) if len(self.success_window) > 0 else 0.0\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    self.gscale *= 0.85\n                elif succ_rate < 0.12:\n                    self.gscale *= 1.12\n                # mild jitter to escape stagnation\n                if self.rng.rand() < 0.05:\n                    self.gscale *= (1.0 + (self.rng.rand() - 0.5) * 0.2)\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-6, 5.0))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                # keep best ones and some randoms and ensure best is kept\n                arch_arr = sorted(self.archive, key=lambda t: t[1])\n                keep_best = arch_arr[: max(10, self.archive_max // 5)]\n                rest = arch_arr[max(10, self.archive_max // 5):]\n                n_random_keep = max(0, self.archive_max - len(keep_best))\n                if len(rest) <= n_random_keep:\n                    kept = keep_best + rest\n                else:\n                    picks = self.rng.choice(len(rest), size=n_random_keep, replace=False)\n                    kept = keep_best + [rest[i] for i in picks]\n                # ensure global best present\n                if self.best_x is not None:\n                    kept.append((self.best_x.copy(), float(self.best_f)))\n                # deduplicate approx by converting to tuples of bytes (cheap hash)\n                seen = set()\n                new_archive = []\n                for x, f in kept:\n                    key = tuple(np.round(x, 12).tolist())\n                    if key not in seen:\n                        seen.add(key)\n                        new_archive.append((x.copy(), float(f)))\n                self.archive = new_archive[:self.archive_max]\n\n            # stagnation detection & micro-restarts\n            if self.iter_since_improve > max(50, 5 * self.dim):\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(20, max(8, 4 + self.dim))\n                added = 0\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    if self.best_x is None:\n                        x = self._uniform_array(lb, ub)\n                    else:\n                        x = self.best_x + self.rng.normal(0, 1.0, size=self.dim) * (scale_vec * (0.6 * self.gscale * (1.0 + self.rng.rand())))\n                        x = self._reflect_bounds(x, lb, ub)\n                    f = safe_eval(x)\n                    if np.isfinite(f):\n                        self.archive.append((x.copy(), f))\n                    added += 1\n                # some global randoms to regain diversity\n                for _ in range(min(10, self.budget - self.evals)):\n                    x = self._uniform_array(lb, ub)\n                    f = safe_eval(x)\n                    if np.isfinite(f):\n                        self.archive.append((x.copy(), f))\n                # after restart, slightly increase exploration scales\n                self.gscale = min(5.0, self.gscale * (1.0 + 0.3 * self.rng.rand()))\n                # reset small success window and stagnation timer\n                self.success_window.clear()\n                self.iter_since_improve = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive) > 0:\n            X = np.array([a[0] for a in self.archive])\n            F = np.array([a[1] for a in self.archive])\n            idx = np.nanargmin(F)\n            best_x = X[idx].copy()\n            best_f = float(F[idx])\n            return best_f, best_x\n        else:\n            # fallback: random if budget still allowed (unlikely)\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x) if self.evals < self.budget else np.inf\n            return f, x", "configspace": "", "generation": 0, "feedback": "In the code, line 31, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=50)  # recent success flags", "error": "In the code, line 31, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=50)  # recent success flags", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "355eff06-2a01-41bd-be46-c6afa7df89f2", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The code will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # internal adaptive global scale (multiplier for per-dim scales)\n        self.gscale = 0.12\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows & counters\n        self.last_improve_at = 0\n        self.success_window = []\n        self.window_len = max(30, self.budget // 100)\n        self.stagnation_threshold = max(200, self.budget // 10)\n\n        # archive of (x,f)\n        self.archive = []\n\n        # evaluations done\n        self.evals = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # if tuple-like (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    # object with lb/ub attributes\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n                    if lb is not None:\n                        lb = np.asarray(lb, dtype=float)\n                    if ub is not None:\n                        ub = np.asarray(ub, dtype=float)\n            # also try direct attributes on func\n            if lb is None:\n                val = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n                if val is not None:\n                    lb = np.asarray(val, dtype=float)\n            if ub is None:\n                val = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n                if val is not None:\n                    ub = np.asarray(val, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # Normalize scalars to vectors or fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            if np.asarray(lb).size == 1:\n                lb = np.full(self.dim, float(np.asarray(lb).item()))\n                ub = np.full(self.dim, float(np.asarray(ub).item()))\n            else:\n                lb = np.asarray(lb, dtype=float)\n                ub = np.asarray(ub, dtype=float)\n                if lb.size != self.dim:\n                    lb = np.resize(lb, self.dim)\n                if ub.size != self.dim:\n                    ub = np.resize(ub, self.dim)\n\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            # if all inside break\n            if not (np.any(x < lb) or np.any(x > ub)):\n                break\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        points = np.empty((n, dim), dtype=float)\n        for d in range(dim):\n            perm = self.rng.permutation(n)\n            points[:, d] = (perm + self.rng.rand(n)) / float(n)\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(lb.size) * (ub - lb) + lb\n        else:\n            shape = (n, lb.size)\n            samples = self.rng.rand(*shape) * (ub - lb) + lb\n            return samples\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with softening and rare heavy tails\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        dim = scale_vec.size\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # temper extreme tails softly but keep heavy tail rare\n        heavy_mask = (self.rng.rand(dim) < 0.03)  # 3% very heavy\n        # soft tempering: log1p of absolute value preserves sign and compresses extremes\n        tempered = np.sign(c) * np.log1p(np.abs(c))\n        tempered = tempered * (0.8 + 0.4 * self.rng.rand(dim))\n        # re-inflate a rare subset to allow escapes\n        tempered[heavy_mask] = tempered[heavy_mask] * (2.0 + 4.0 * self.rng.rand(heavy_mask.sum()))\n        # scale and cap per-dim\n        res = scale_vec * tempered\n        cap = np.abs(scale_vec) * cap_multiplier\n        res = np.maximum(np.minimum(res, cap), -cap)\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n        domain_width = np.maximum(domain_width, 1e-12)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(12, self.dim * 4), max(1, self.budget // 20))\n        n_init = max(n_init, 6)\n        n_init = min(n_init, self.budget)\n        lhs = self._lhs01(n_init, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        adds = min(6, max(0, self.budget - self.evals))\n        for _ in range(adds):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        adapt_every = max(10, min(200, self.window_len // 3))\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample random if no archive\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            try:\n                iqr = np.subtract(*np.percentile(arr_X, [75, 25], axis=0))\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n                per_dim_scale = np.where(per_dim_scale <= 1e-12, np.std(arr_X, axis=0) + 1e-12, per_dim_scale)\n            except Exception:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12)\n            # scale_vec is a fraction of domain and informed by population spread\n            scale_vec = per_dim_scale * (domain_width / (domain_width + 1e-12))\n\n            # choose anchor: bias to elites\n            finite_mask = np.isfinite(arr_f)\n            if np.any(finite_mask):\n                finite_idxs = np.where(finite_mask)[0]\n                sorted_finite = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                topk = max(2, int(max(2, len(sorted_finite) // 6)))\n                candidates = sorted_finite[:topk]\n                # exponential bias over candidate ranks (best more likely)\n                ranks = np.arange(len(candidates), dtype=float)\n                weights = np.exp(-ranks)\n                weights = weights / weights.sum()\n                anchor_idx = int(self.rng.choice(candidates, p=weights))\n            else:\n                anchor_idx = int(self.rng.randint(arr_X.shape[0]))\n            anchor = arr_X[anchor_idx].copy()\n\n            # parameters for elites / PCA\n            elite_count = max(3, min(20, max(3, arr_X.shape[0] // 6)))\n\n            # propose a candidate via a mixture of strategies\n            x_new = None\n            r = self.rng.rand()\n\n            # Strategy 1: DE-style differential injection (around 25%)\n            if r < 0.25 and arr_X.shape[0] >= 3:\n                others = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                b, c = self.rng.choice(others, size=2, replace=False)\n                xb = arr_X[b]\n                xc = arr_X[c]\n                F = 0.4 + 0.6 * self.rng.rand()  # differential factor [0.4,1.0]\n                diff = F * (xb - xc)\n                # anisotropic gaussian jitter\n                jitter = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                candidate = anchor + diff + jitter * domain_width\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation (~25%)\n            elif r < 0.5 and arr_X.shape[0] >= 5:\n                if np.any(finite_mask):\n                    if finite_mask.sum() >= elite_count:\n                        elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])][:elite_count]\n                    else:\n                        elite_idxs = finite_idxs\n                else:\n                    # if no finite values, just random elites\n                    elite_idxs = self.rng.choice(range(arr_X.shape[0]), size=min(elite_count, arr_X.shape[0]), replace=False)\n                elites = arr_X[np.asarray(elite_idxs, dtype=int)]\n                mu = np.mean(elites, axis=0)\n                cov = np.cov(elites.T) if elites.shape[0] > 1 else np.diag(per_dim_scale**2 + 1e-12)\n                # stable eigen-decomposition\n                try:\n                    vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                    # sample coefficients along principal axes, scaled by eigenvals and global scale\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(np.maximum(vals, 0.0))\n                    scale = self.gscale * (0.6 + 0.8 * self.rng.rand())\n                    perturb = vecs.dot(coeffs) * scale\n                    candidate = mu + perturb * (domain_width / (domain_width + 1e-12))\n                    x_new = self._reflect_bounds(anchor + 0.6 * (candidate - anchor), lb, ub)\n                except Exception:\n                    # fallback to small local jitter\n                    x_new = self._reflect_bounds(anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale * domain_width), lb, ub)\n\n            # Strategy 3: Local anisotropic gaussian (~25%)\n            elif r < 0.75:\n                prob_coord = 0.25 + 0.6 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not np.any(coords):\n                    coords[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise[coords] = self.rng.randn(coords.sum()) * (per_dim_scale[coords] * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                candidate = anchor + noise * domain_width\n                # occasionally mix in a few uniform coords\n                if self.rng.rand() < 0.12:\n                    mix_coords = self.rng.rand(self.dim) < 0.08\n                    if np.any(mix_coords):\n                        uni = self._uniform_array(lb, ub)\n                        candidate[mix_coords] = uni[mix_coords]\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 4: tempered Cauchy global escape (rest)\n            else:\n                candidate = anchor + self._tempered_cauchy(scale_vec * (1.0 + 4.0 * self.gscale))\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # sometimes add a small differential jump for diversity\n            if arr_X.shape[0] >= 2 and self.rng.rand() < 0.18:\n                i1, i2 = self.rng.choice([i for i in range(arr_X.shape[0]) if i != anchor_idx], size=2, replace=False)\n                x_new = x_new + 0.08 * (arr_X[i1] - arr_X[i2])\n                x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.035:\n                tiny = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.04)\n                x_new = self._reflect_bounds(self.best_x + tiny * domain_width, lb, ub)\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale * domain_width)\n                x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if np.any(bad_coords):\n                uni = self._uniform_array(lb, ub)\n                x_new[bad_coords] = uni[bad_coords]\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n            # update best\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                improved = True\n\n            # success window measure: treat improvement as success; also compare to current 25% threshold\n            finite_fs = np.array([a[1] for a in self.archive if np.isfinite(a[1])], dtype=float)\n            if improved:\n                self.success_window.append(1)\n            else:\n                # count as success if in top 25% of finite archive\n                if finite_fs.size > 0:\n                    try:\n                        threshold = np.percentile(finite_fs, 25)\n                        self.success_window.append(1 if (np.isfinite(f_new) and f_new <= threshold) else 0)\n                    except Exception:\n                        self.success_window.append(0)\n                else:\n                    self.success_window.append(0)\n\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window = self.success_window[-self.window_len:]\n\n            # adaptation of gscale every now and then\n            if (self.evals % adapt_every) == 0 and len(self.success_window) > 0:\n                succ_rate = float(np.mean(self.success_window))\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    self.gscale *= 0.88\n                elif succ_rate < 0.12:\n                    self.gscale *= 1.14\n                else:\n                    # mild jitter to escape stagnation\n                    self.gscale *= (0.97 + 0.06 * self.rng.rand())\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1.5))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                # keep best ones and some randoms\n                arr_f_all = np.array([a[1] for a in self.archive], dtype=float)\n                finite_mask_all = np.isfinite(arr_f_all)\n                keep_best = max(6, int(0.2 * self.archive_max))\n                kept_ix = set()\n                if finite_mask_all.any():\n                    finite_idxs_all = np.where(finite_mask_all)[0]\n                    sorted_finite_all = finite_idxs_all[np.argsort(arr_f_all[finite_idxs_all])]\n                    for i in sorted_finite_all[:keep_best]:\n                        kept_ix.add(int(i))\n                # keep current best if present\n                if self.best_x is not None:\n                    diffs = np.linalg.norm(np.vstack([a[0] for a in self.archive]) - self.best_x, axis=1)\n                    best_idx = int(np.argmin(diffs))\n                    kept_ix.add(best_idx)\n                # keep some random ones for diversity\n                keep_rand = max(10, self.archive_max - len(kept_ix))\n                candidates = [i for i in range(len(self.archive)) if i not in kept_ix]\n                self.rng.shuffle(candidates)\n                for i in candidates[:keep_rand]:\n                    kept_ix.add(int(i))\n                new_archive = [self.archive[i] for i in sorted(kept_ix)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_threshold and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                if self.best_x is None:\n                    base = self._uniform_array(lb, ub)\n                else:\n                    base = self.best_x.copy()\n                n_local = min(30, max(6, self.dim * 4))\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * max(0.8, self.gscale))\n                    cand = self._reflect_bounds(base + jitter * domain_width, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.25, 1e-6, 2.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [t for t in self.archive if np.isfinite(t[1])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda t: t[1])\n            self.best_x, self.best_f = best[0].copy(), float(best[1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed\n            if self.best_x is None:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x) if self.evals < self.budget else np.inf\n                self.best_x = x.copy()\n                self.best_f = f\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0a73e31c-3a60-46a9-893a-e047a0d02ae6", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The code will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # internal adaptive global scale (multiplier for per-dim scales)\n        self.gscale = 0.12\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows & counters\n        self.evals = 0\n        self.last_improve_at = 0\n        self.window_len = max(30, self.budget // 100)\n        self.stagnation_threshold = max(200, self.budget // 10)\n\n        # archive of (x,f)\n        self.archive = []\n\n        # success history for short-term adaptation\n        self.success_window = []\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if isinstance(b, (tuple, list)) and len(b) == 2:\n                lb, ub = b\n            elif b is not None:\n                # object with lb/ub attributes\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n            # also try direct attributes on func\n            if lb is None:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            if ub is None:\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n            if lb is not None:\n                lb = np.asarray(lb, dtype=float)\n            if ub is not None:\n                ub = np.asarray(ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # Normalize scalars to vectors\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            # if scalars broadcast\n            if np.isscalar(lb) or getattr(lb, \"size\", 1) == 1:\n                try:\n                    lb = np.full(self.dim, float(np.asarray(lb).item()))\n                except Exception:\n                    lb = np.full(self.dim, -5.0)\n            if np.isscalar(ub) or getattr(ub, \"size\", 1) == 1:\n                try:\n                    ub = np.full(self.dim, float(np.asarray(ub).item()))\n                except Exception:\n                    ub = np.full(self.dim, 5.0)\n            # ensure shapes\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            if lb.size != self.dim:\n                lb = np.full(self.dim, lb.flatten()[0])\n            if ub.size != self.dim:\n                ub = np.full(self.dim, ub.flatten()[0])\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            # if all inside break\n            if not (np.any(x < lb) or np.any(x > ub)):\n                break\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        points = np.empty((n, dim), dtype=float)\n        for d in range(dim):\n            perm = np.arange(n)\n            self.rng.shuffle(perm)\n            # stratified samples in each cell\n            points[:, d] = (perm + self.rng.rand(n)) / n\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size)\n        samples = self.rng.rand(*shape) * (ub - lb) + lb\n        if n == 1:\n            return samples[0]\n        return samples\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        dim = scale_vec.size\n        # sample standard Cauchy via tan(pi*(u-0.5))\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper large values with tanh factor (keeps moderate tails), but occasionally allow heavier tails\n        heavy = (self.rng.rand(dim) < 0.02)  # small chance to allow extreme tail\n        factor = np.tanh(np.abs(c)) / (np.abs(c) + 1e-12)\n        factor = np.where(heavy, 1.0, factor)\n        tempered = np.sign(c) * (np.abs(c) * factor)\n        # scale per-dim and cap\n        cap = cap_multiplier * scale_vec\n        res = np.clip(tempered * scale_vec, -cap, cap)\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n        domain_width = np.maximum(domain_width, 1e-12)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: LHS samples ----\n        n_init = max(6, min(30, int(4 + 2 * self.dim)))  # modest initial cloud relative to dim\n        n_init = min(n_init, self.budget)\n        lhs = self._lhs01(n_init, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            x = self._reflect_bounds(x, lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        n_extra = min(6, max(0, self.budget - self.evals))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        adapt_every = max(10, min(200, self.window_len // 3))\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample random if no archive\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                self.archive.append((x_new.copy(), f_new))\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            try:\n                iqr = np.subtract(*np.percentile(arr_X, [75, 25], axis=0))\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n                per_dim_scale = np.where(per_dim_scale <= 1e-12, np.std(arr_X, axis=0) + 1e-12, per_dim_scale)\n            except Exception:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12)\n            scale_vec = per_dim_scale * (domain_width / (domain_width + 1e-12))  # normalize to domain fraction\n\n            # choose anchor: bias to elites if available\n            finite_mask = np.isfinite(arr_f)\n            finite_idxs = np.where(finite_mask)[0]\n            anchor_idx = None\n            if finite_mask.any():\n                # rank finite by fitness\n                sorted_finite = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                topk = max(2, int(0.15 * arr_X.shape[0]))\n                topk = min(topk, sorted_finite.size)\n                candidates = sorted_finite[:topk] if topk > 0 else sorted_finite\n                # exponential bias over candidate ranks\n                ranks = np.arange(len(candidates), dtype=float)\n                weights = np.exp(-ranks)\n                weights = weights / (weights.sum() + 1e-12)\n                cho = self.rng.choice(len(candidates), p=weights)\n                anchor_idx = int(candidates[cho])\n            else:\n                anchor_idx = self.rng.randint(arr_X.shape[0])\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            x_new = None\n            r = self.rng.rand()\n\n            # Strategy 1: DE-style differential injection (around 25%)\n            if r < 0.25 and arr_X.shape[0] >= 3:\n                # pick two distinct others\n                others = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                if len(others) >= 2:\n                    b, c = self.rng.choice(others, size=2, replace=False)\n                    xb = arr_X[b]\n                    xc = arr_X[c]\n                    diff = xb - xc\n                    # anisotropic gaussian jitter + scaled differential\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                    factor = 0.6 + 0.9 * self.rng.rand()\n                    candidate = anchor + factor * diff + jitter * domain_width\n                    x_new = candidate\n\n            # Strategy 2: PCA-guided elite perturbation (~25%)\n            elif r < 0.5 and arr_X.shape[0] >= 5:\n                elite_count = max(3, min(arr_X.shape[0], int(0.18 * arr_X.shape[0])))\n                if finite_mask.any() and finite_idxs.size >= elite_count:\n                    elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])][:elite_count]\n                else:\n                    # fallback: random elites\n                    elite_idxs = self.rng.choice(range(arr_X.shape[0]), size=elite_count, replace=False)\n                elites = arr_X[elite_idxs]\n                mu = np.mean(elites, axis=0)\n                cov = np.cov(elites.T) if elite_count > 1 else np.eye(self.dim) * 1e-6\n                # stable eigen-decomposition\n                try:\n                    vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                    vals = np.maximum(vals, 0.0)\n                    coeffs = self.rng.randn(self.dim) * np.sqrt(vals + 1e-12)\n                    # scale by gscale and per-dim scales\n                    perturb = vecs.dot(coeffs) * (self.gscale * 0.8)\n                    candidate = mu + perturb * (domain_width / (domain_width + 1e-12))\n                    # jitter around anchor if needed\n                    if self.rng.rand() < 0.4:\n                        candidate = anchor + (candidate - anchor) * (0.6 + 0.8 * self.rng.rand())\n                except Exception:\n                    candidate = anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale)\n                x_new = candidate\n\n            # Strategy 3: Local anisotropic gaussian (~25%)\n            elif r < 0.75:\n                prob_coord = 0.25 + 0.6 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not np.any(coords):\n                    coords[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise_vals = self.rng.randn(coords.sum()) * (per_dim_scale[coords] * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                noise[coords] = noise_vals\n                candidate = anchor + noise * domain_width\n                # occasionally mix in a few uniform coords\n                mix_coords = self.rng.rand(self.dim) < 0.08\n                if np.any(mix_coords):\n                    u = self._uniform_array(lb, ub)\n                    candidate[mix_coords] = u[mix_coords]\n                x_new = candidate\n\n            # Strategy 4: tempered Cauchy global escape (rest)\n            else:\n                candidate = anchor + self._tempered_cauchy(scale_vec * (1.0 + 4.0 * self.gscale))\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if arr_X.shape[0] >= 2 and self.rng.rand() < 0.15:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.08 * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.035:\n                tiny = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * 0.04)\n                x_new = self.best_x + tiny * domain_width\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if np.any(bad_coords):\n                u = self._uniform_array(lb, ub)\n                x_new[bad_coords] = u[bad_coords]\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                improved = True\n                self.success_window.append(1)\n            else:\n                # measure success relative to current archive: consider success if in top 25% finite\n                try:\n                    finite_fs = arr_f[np.isfinite(arr_f)]\n                    if finite_fs.size > 0 and np.isfinite(f_new):\n                        threshold = np.percentile(finite_fs, 25)\n                        self.success_window.append(1 if f_new <= threshold else 0)\n                    else:\n                        self.success_window.append(0)\n                except Exception:\n                    self.success_window.append(0)\n\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window = self.success_window[-self.window_len:]\n\n            # adaptation of gscale every now and then\n            if (self.evals % adapt_every) == 0 and len(self.success_window) > 0:\n                succ_rate = float(np.sum(self.success_window)) / (len(self.success_window) + 1e-12)\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    self.gscale *= 0.88\n                elif succ_rate < 0.12:\n                    self.gscale *= 1.14\n                else:\n                    # mild jitter to escape stagnation\n                    self.gscale *= (0.97 + 0.06 * self.rng.rand())\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1.5))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                # keep best ones and some randoms\n                keep_best = max(6, int(0.2 * self.archive_max))\n                kept_ix = set()\n                # gather finite indices\n                arr_f_all = np.array([a[1] for a in self.archive], dtype=float)\n                finite_idxs_all = np.where(np.isfinite(arr_f_all))[0]\n                if finite_idxs_all.size > 0:\n                    sorted_finite_all = finite_idxs_all[np.argsort(arr_f_all[finite_idxs_all])]\n                    for i in sorted_finite_all[:keep_best]:\n                        kept_ix.add(int(i))\n                # keep current best if present\n                if self.best_x is not None:\n                    diffs = np.linalg.norm(np.vstack([a[0] for a in self.archive]) - self.best_x, axis=1)\n                    best_idx = int(np.argmin(diffs))\n                    kept_ix.add(best_idx)\n                # keep some random ones for diversity\n                keep_rand = max(10, self.archive_max - len(kept_ix))\n                candidates = [i for i in range(len(self.archive)) if i not in kept_ix]\n                self.rng.shuffle(candidates)\n                for i in candidates[:keep_rand]:\n                    kept_ix.add(int(i))\n                new_archive = [self.archive[i] for i in sorted(kept_ix)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > self.stagnation_threshold and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(30, max(6, self.dim * 4))\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    if self.best_x is not None:\n                        jitter = self.rng.randn(self.dim) * (per_dim_scale * max(0.8, self.gscale) * 0.8)\n                        cand = self.best_x + jitter * domain_width\n                    else:\n                        cand = self._uniform_array(lb, ub)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.25, 1e-6, 2.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [t for t in self.archive if np.isfinite(t[1])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda t: t[1])\n            self.best_x, self.best_f = best[0].copy(), float(best[1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.best_x is None:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x) if self.evals < self.budget else np.inf\n                self.best_x = x.copy()\n                self.best_f = f\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3e33c423-26d2-4a12-9ce0-aed28b6ef046", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-like donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The code will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple or func.lb/ub) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400, gscale_init=0.12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # internal adaptive global scale (multiplier for per-dim scales)\n        self.gscale = float(gscale_init)\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n\n        # adaptation windows & counters\n        self.last_improve_at = 0\n        self.window_len = max(30, max(1, self.budget // 100))\n\n        # archive of (x,f)\n        self.archive = []\n\n        # success history for short-term adaptation\n        self.success_window = []\n\n        # evaluation counter\n        self.evals = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb, ub = b\n                else:\n                    # object with lb/ub attributes may exist\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n            # direct attributes on func\n            if lb is None:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            if ub is None:\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        # Normalize scalars to vectors\n        if lb is None or ub is None:\n            # default box\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            # convert to numpy arrays or broadcast scalars\n            lb_arr = np.asarray(lb, dtype=float)\n            ub_arr = np.asarray(ub, dtype=float)\n            if lb_arr.size == 1:\n                lb = np.full(self.dim, float(lb_arr.item()))\n            else:\n                lb = lb_arr.copy()\n            if ub_arr.size == 1:\n                ub = np.full(self.dim, float(ub_arr.item()))\n            else:\n                ub = ub_arr.copy()\n\n            # ensure lengths\n            lb = np.asarray(lb, dtype=float).reshape(-1)\n            ub = np.asarray(ub, dtype=float).reshape(-1)\n            if lb.size != self.dim:\n                lb = np.full(self.dim, float(lb.flatten()[0]))\n            if ub.size != self.dim:\n                ub = np.full(self.dim, float(ub.flatten()[0]))\n\n        return np.asarray(lb, dtype=float), np.asarray(ub, dtype=float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        x = x.copy()\n        # reflect iteratively a couple times to bounce back in\n        for _ in range(5):\n            low_mask = x < lb\n            if np.any(low_mask):\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            if np.any(high_mask):\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n            if not (np.any(x < lb) or np.any(x > ub)):\n                break\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        points = np.empty((n, dim), dtype=float)\n        for d in range(dim):\n            perm = np.arange(n)\n            self.rng.shuffle(perm)\n            points[:, d] = (perm + self.rng.rand(n)) / float(n)\n            # then shuffle within the dimension to ensure mixing\n            self.rng.shuffle(points[:, d])\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size)\n        samples = self.rng.rand(*shape) * (ub - lb) + lb\n        if n == 1:\n            return samples[0]\n        return samples\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        dim = scale_vec.size\n        # sample standard Cauchy via tan(pi*(u-0.5))\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper large values with tanh factor (keeps moderate tails), but occasionally allow heavier tails\n        heavy = (self.rng.rand(dim) < 0.02)  # small chance to allow extreme tail\n        factor = np.tanh(np.abs(c)) / (np.abs(c) + 1e-12)\n        factor = np.where(heavy, 1.0, factor)\n        tempered = np.sign(c) * (np.abs(c) * factor)\n        # scale per-dim and cap\n        cap = cap_multiplier * scale_vec + 1e-12\n        res = np.clip(tempered * scale_vec, -cap, cap)\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x.copy())\n            except Exception:\n                f = np.inf\n            # count this evaluation\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: LHS samples ----\n        n_init = max(6, min(30, int(4 + 2 * self.dim)))  # modest initial cloud relative to dim\n        n_init = min(n_init, self.budget)\n        lhs = self._lhs01(n_init, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        n_extra = min(6, max(0, self.budget - self.evals))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_x = x.copy()\n                self.best_f = f\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        adapt_every = max(10, min(200, max(1, self.window_len // 3)))\n        prob_coord = 0.4  # per-dimension coordinate update probability for local moves\n        stagnation_threshold = max(50, self.window_len * 2)\n\n        while self.evals < self.budget:\n            # convert archive to arrays\n            if len(self.archive) == 0:\n                # sample random if no archive\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                self.archive.append((x_new.copy(), f_new))\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            try:\n                q75 = np.percentile(arr_X, 75, axis=0)\n                q25 = np.percentile(arr_X, 25, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approx std from iqr\n                # fallback to std where too small\n                stds = np.std(arr_X, axis=0)\n                small = per_dim_scale <= 1e-12\n                per_dim_scale[small] = stds[small] + 1e-12\n            except Exception:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n            per_dim_scale = np.maximum(per_dim_scale, 1e-12)\n\n            # finite mask and ranking\n            finite_mask = np.isfinite(arr_f)\n            finite_idxs = np.where(finite_mask)[0]\n\n            # choose anchor: bias to elites if available\n            if finite_mask.any():\n                sorted_finite = finite_idxs[np.argsort(arr_f[finite_idxs])]\n                topk = max(2, int(0.15 * arr_X.shape[0]))\n                topk = min(topk, sorted_finite.size)\n                if topk > 0:\n                    candidates = sorted_finite[:topk]\n                else:\n                    candidates = sorted_finite\n                # exponential bias over candidate ranks\n                ranks = np.arange(len(candidates))\n                weights = np.exp(-0.6 * ranks)\n                weights = weights / (weights.sum() + 1e-12)\n                pick = self.rng.choice(len(candidates), p=weights)\n                anchor_idx = int(candidates[pick])\n            else:\n                anchor_idx = int(self.rng.randint(arr_X.shape[0]))\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            x_new = None\n            r = self.rng.rand()\n\n            # Strategy 1: DE-style differential injection (~30%)\n            if r < 0.30 and arr_X.shape[0] >= 3:\n                idxs = list(range(arr_X.shape[0]))\n                idxs.remove(anchor_idx)\n                if len(idxs) >= 2:\n                    b, c = self.rng.choice(idxs, size=2, replace=False)\n                    xb = arr_X[b]\n                    xc = arr_X[c]\n                    F = 0.6 * (1.0 + 0.5 * (self.rng.rand() - 0.5)) * (1.0 + self.gscale)\n                    diff = xb - xc\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * 0.12 * (0.8 + self.rng.rand()))\n                    candidate = anchor + F * diff + jitter\n                    x_new = candidate\n\n            # Strategy 2: PCA-guided elite perturbation (~25%)\n            elif r < 0.55 and arr_X.shape[0] >= 5:\n                elite_count = max(3, min(arr_X.shape[0], int(max(3, 0.18 * arr_X.shape[0]))))\n                if finite_mask.any() and finite_idxs.size >= elite_count:\n                    elite_idxs = finite_idxs[np.argsort(arr_f[finite_idxs])][:elite_count]\n                else:\n                    elite_idxs = self.rng.choice(range(arr_X.shape[0]), size=elite_count, replace=False)\n                elites = arr_X[elite_idxs]\n                mu = np.mean(elites, axis=0)\n                cov = np.cov(elites.T)\n                # stabilize covariance\n                cov = cov + np.eye(self.dim) * 1e-8\n                try:\n                    vals, vecs = np.linalg.eigh(cov)\n                    vals = np.maximum(vals, 1e-12)\n                    # sample coefficients along eigenvectors with stronger weight on leading directions\n                    coeffs = (self.rng.randn(self.dim) * np.sqrt(vals)) * (0.8 + 0.6 * self.rng.rand(self.dim))\n                    perturb = (vecs @ coeffs) * (self.gscale * 0.9)\n                    # mix between anchor and mu to diversify\n                    blend = 0.3 + 0.7 * self.rng.rand()\n                    candidate = anchor + blend * (mu + perturb - anchor)\n                except Exception:\n                    candidate = anchor + self.rng.randn(self.dim) * (per_dim_scale * self.gscale)\n                x_new = candidate\n\n            # Strategy 3: Local anisotropic gaussian (~25%)\n            elif r < 0.80:\n                coords = (self.rng.rand(self.dim) < prob_coord)\n                if not np.any(coords):\n                    # if none selected, pick at least one\n                    coords[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise[coords] = self.rng.randn(np.count_nonzero(coords)) * (per_dim_scale[coords] * self.gscale)\n                candidate = anchor + noise\n                # occasionally mix in a few uniform coords for diversity\n                mix_coords = (self.rng.rand(self.dim) < 0.08)\n                if np.any(mix_coords):\n                    u = self._uniform_array(lb, ub)\n                    candidate[mix_coords] = u[mix_coords]\n                x_new = candidate\n\n            # Strategy 4: tempered Cauchy global escape (rest)\n            else:\n                scale_vec = per_dim_scale * (1.0 + 3.0 * self.gscale)\n                candidate = anchor + self._tempered_cauchy(scale_vec, cap_multiplier=6.0)\n                # occasionally allow totally random global jump\n                if self.rng.rand() < 0.06:\n                    candidate = self._uniform_array(lb, ub)\n                x_new = candidate\n\n            # sometimes add a small differential jump for diversity\n            if arr_X.shape[0] >= 2 and self.rng.rand() < 0.15:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.08 * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.035:\n                x_new = self.best_x + self.rng.randn(self.dim) * (per_dim_scale * (0.08 + 0.25 * self.gscale))\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = self._uniform_array(lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if np.any(bad_coords):\n                u = self._uniform_array(lb, ub)\n                x_new[bad_coords] = u[bad_coords]\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                improved = True\n                self.last_improve_at = self.evals\n                self.success_window.append(1)\n            else:\n                # measure success relative to current archive: consider success if in top 25% finite\n                try:\n                    finite_fs = arr_f[np.isfinite(arr_f)]\n                    if finite_fs.size > 0:\n                        threshold = np.percentile(finite_fs, 25.0)\n                        self.success_window.append(1 if f_new <= threshold else 0)\n                    else:\n                        self.success_window.append(0)\n                except Exception:\n                    self.success_window.append(0)\n\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window = self.success_window[-self.window_len:]\n\n            # adaptation of gscale every now and then\n            if (self.evals % adapt_every) == 0 and len(self.success_window) > 0:\n                succ_rate = float(np.sum(self.success_window)) / (len(self.success_window) + 1e-12)\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    self.gscale *= 0.88\n                elif succ_rate < 0.12:\n                    self.gscale *= 1.14\n                else:\n                    # mild jitter to escape stagnation\n                    self.gscale *= (0.97 + 0.06 * self.rng.rand())\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-3, 5.0))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                keep_best = max(6, int(0.2 * self.archive_max))\n                # select best finite\n                arr_f_all = np.array([a[1] for a in self.archive], dtype=float)\n                finite_idxs_all = np.where(np.isfinite(arr_f_all))[0]\n                kept_ix = set()\n                if finite_idxs_all.size > 0:\n                    sorted_finite_all = finite_idxs_all[np.argsort(arr_f_all[finite_idxs_all])]\n                    for i in sorted_finite_all[:keep_best]:\n                        kept_ix.add(int(i))\n                # keep current best if present\n                if self.best_x is not None:\n                    diffs = np.linalg.norm(np.vstack([a[0] for a in self.archive]) - self.best_x, axis=1)\n                    best_idx = int(np.argmin(diffs))\n                    kept_ix.add(best_idx)\n                # keep some random ones for diversity\n                keep_rand = max(10, self.archive_max - len(kept_ix))\n                candidates = [i for i in range(len(self.archive)) if i not in kept_ix]\n                self.rng.shuffle(candidates)\n                for i in candidates[:keep_rand]:\n                    kept_ix.add(i)\n                new_archive = [self.archive[i] for i in sorted(kept_ix)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.last_improve_at + stagnation_threshold) < self.evals and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(30, max(6, self.dim * 4))\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    if self.best_x is not None and self.rng.rand() < 0.75:\n                        jitter = self.rng.randn(self.dim) * (per_dim_scale * max(0.6, self.gscale) * 0.6)\n                        cand = self.best_x + jitter\n                    else:\n                        cand = self._uniform_array(lb, ub)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, int(0.05 * self.budget)))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale *= 1.12\n                self.gscale = float(np.clip(self.gscale, 1e-3, 5.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [t for t in self.archive if np.isfinite(t[1])]\n        if len(finite_archive) > 0:\n            # pick best\n            best_idx = int(np.argmin([t[1] for t in finite_archive]))\n            self.best_x = finite_archive[best_idx][0].copy()\n            self.best_f = float(finite_archive[best_idx][1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.best_x is None:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x) if self.evals < self.budget else np.inf\n                self.best_x = x.copy()\n                self.best_f = f\n\n        return float(self.best_f), np.asarray(self.best_x, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0ffa30d3-1f3a-4967-92df-63f8117177ab", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n        self.evals = 0\n\n        # adaptation windows & counters\n        self.last_improve_at = 0\n        self.window_len = max(30, max(50, self.budget // 100))  # adapt window in evaluations\n        self.success_window = []\n\n        # archive of (x,f)\n        self.archive = []\n\n        # global exploration multiplier (adapted)\n        self.gscale = 0.5\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        b = getattr(func, \"bounds\", None)\n        try:\n            if b is not None:\n                # if bounds is tuple (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb, ub = b[0], b[1]\n                else:\n                    # object with lb/ub or lower/upper attributes\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        # also try attributes directly on func\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # Normalize to numpy arrays; fallback to default\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n            return lb, ub\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # scalars broadcast\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.flatten()[0]), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.flatten()[0]), dtype=float)\n\n        # if shapes mismatch or wrong size, fall back\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that are out of bounds (mirror back)\n        x = x.copy()\n        for _ in range(max_iter):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            # reflect low: x = lb + (lb - x)\n            x[low] = lb[low] + (lb[low] - x[low])\n            # reflect high: x = ub - (x - ub)\n            x[high] = ub[high] - (x[high] - ub[high])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        points = np.empty((n, dim), dtype=float)\n        for d in range(dim):\n            perm = np.arange(n)\n            self.rng.shuffle(perm)\n            points[:, d] = (perm + self.rng.rand(n)) / float(n)\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size)\n        samples = self.rng.rand(*shape) * (ub - lb) + lb\n        if n == 1:\n            return samples.reshape(-1)[:]\n        return samples\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        dim = scale_vec.size\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        heavy = (self.rng.rand(dim) < 0.02)\n        factor = np.tanh(np.abs(c)) / (np.abs(c) + 1e-12)\n        factor = np.where(heavy, 1.0, factor)\n        tempered = np.sign(c) * (np.abs(c) * factor)\n        # scale per-dim and cap\n        cap = cap_multiplier * scale_vec + 1e-12\n        res = tempered * scale_vec\n        # cap extreme values coordinate-wise\n        res = np.clip(res, -cap, cap)\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n        domain_width = np.maximum(domain_width, 1e-12)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(np.asarray(x, dtype=float))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(12, self.dim * 3), max(1, self.budget // 20))\n        n_init = min(n_init, self.budget)\n        lhs = self._lhs01(n_init, self.dim)\n        X0 = lb + lhs * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        n_extra = min(6, max(0, self.budget - self.evals))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        adapt_every = max(10, min(200, self.window_len // 3))\n        iter_since_adapt = 0\n\n        # mixing probabilities\n        # DE ~ 0.22, PCA ~ 0.23, local gaussian ~ 0.45, tempered cauchy ~ 0.10 (weights adapt implicitly)\n        while self.evals < self.budget:\n            # ensure archive arrays\n            if len(self.archive) == 0:\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                self.archive.append((x_new.copy(), f_new))\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            if arr_X.shape[0] >= 2:\n                iqr = np.subtract(*np.percentile(arr_X, [75, 25], axis=0))\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n                # final fallback\n                per_dim_scale = np.maximum(per_dim_scale, np.std(arr_X, axis=0) + 1e-12)\n            else:\n                per_dim_scale = np.maximum(np.std(arr_X, axis=0, ddof=0), 1e-12)\n\n            scale_vec = per_dim_scale * (domain_width / (domain_width + 1e-12))  # normalize to domain fraction\n\n            # choose anchor: bias to elites if available\n            finite_mask = np.isfinite(arr_f)\n            candidates = np.where(finite_mask)[0] if finite_mask.any() else np.arange(len(self.archive))\n            if candidates.size > 0:\n                # create a ranking and pick with exponential bias towards top\n                ranks = np.argsort(arr_f[candidates])\n                # top fraction\n                topk = max(2, int(0.15 * arr_X.shape[0]))\n                topk = min(topk, candidates.size)\n                top_candidates = candidates[ranks[:topk]]\n                # choose either top or general candidate\n                if self.rng.rand() < 0.75:\n                    anchor_idx = int(self.rng.choice(top_candidates))\n                else:\n                    anchor_idx = int(self.rng.choice(candidates))\n            else:\n                anchor_idx = self.rng.randint(arr_X.shape[0])\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = self.rng.rand()\n\n            x_new = None\n\n            # Strategy 1: DE-style differential injection (~22%)\n            if r < 0.22 and arr_X.shape[0] >= 3:\n                # pick two distinct others\n                others = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                if len(others) >= 2:\n                    b, c = self.rng.choice(others, size=2, replace=False)\n                    xb = arr_X[b]\n                    xc = arr_X[c]\n                    diff = xb - xc\n                    # anisotropic gaussian jitter + scaled differential\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                    factor = 0.6 + 0.9 * self.rng.rand()\n                    x_new = anchor + factor * (diff * (self.gscale)) + jitter\n                    x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation (~23%)\n            elif r < 0.45:\n                elite_count = max(3, min(arr_X.shape[0], int(0.12 * arr_X.shape[0]) or 3))\n                # if not enough elites, pick best ones by fitness\n                finite_exists = np.isfinite(arr_f)\n                if finite_exists.any():\n                    sorted_idx = np.argsort(arr_f[finite_exists])\n                    finite_idx_list = np.where(finite_exists)[0]\n                    elite_idxs = finite_idx_list[sorted_idx[:elite_count]]\n                else:\n                    elite_idxs = self.rng.choice(range(arr_X.shape[0]), size=elite_count, replace=False)\n                elites = arr_X[elite_idxs]\n                mu = np.mean(elites, axis=0)\n                cov = np.cov(elites.T) if elites.shape[0] > 1 else np.diag(np.maximum(scale_vec, 1e-12))\n                # stable eigen-decomposition\n                try:\n                    vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                except Exception:\n                    vals = np.maximum(np.diag(cov), 0.0)\n                    vecs = np.eye(self.dim)\n                vals = np.maximum(vals, 0.0)\n                # sample in principal subspace, scaled by gscale\n                coeffs = self.rng.randn(self.dim) * (np.sqrt(vals + 1e-12) * self.gscale)\n                pca_step = vecs.dot(coeffs)\n                # bias to anchor or mean occasionally\n                if self.rng.rand() < 0.5:\n                    candidate = anchor + pca_step\n                else:\n                    candidate = mu + pca_step * (0.6 + 0.8 * self.rng.rand())\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 3: Local anisotropic gaussian (~45%)\n            elif r < 0.9:\n                prob_coord = 0.22 + 0.08 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                # ensure at least one coord\n                coords[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise_vals = self.rng.randn(coords.sum()) * (per_dim_scale[coords] * self.gscale * (0.15 + 1.0 * self.rng.rand()))\n                noise[coords] = noise_vals\n                candidate = anchor + noise\n                # occasionally mix in a few uniform coords for diversity\n                if self.rng.rand() < 0.08:\n                    mix_coords = self.rng.rand(self.dim) < 0.08\n                    u = self._uniform_array(lb, ub)\n                    candidate[mix_coords] = u[mix_coords]\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 4: tempered Cauchy global escape (~10%)\n            else:\n                # large jumps occasionally allow escapes\n                jump = self._tempered_cauchy(scale_vec * (1.0 + 6.0 * self.gscale))\n                candidate = anchor + jump\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # sometimes add a small differential jump for added diversity\n            if self.rng.rand() < 0.12 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                x_new = x_new + 0.06 * (arr_X[i1] - arr_X[i2])\n                x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.rand() < 0.06:\n                x_new = 0.5 * x_new + 0.5 * self.best_x\n                x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = self._uniform_array(lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if bad_coords.any():\n                x_new[bad_coords] = self._uniform_array(lb[bad_coords], ub[bad_coords], n=1)\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                improved = True\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            try:\n                finite_fs = arr_f[np.isfinite(arr_f)]\n                if finite_fs.size > 0 and np.isfinite(f_new):\n                    threshold = np.percentile(finite_fs, 25)\n                    self.success_window.append(1 if f_new <= threshold else 0)\n                else:\n                    self.success_window.append(1 if improved else 0)\n            except Exception:\n                self.success_window.append(1 if improved else 0)\n\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window = self.success_window[-self.window_len:]\n\n            iter_since_adapt += 1\n            # adaptation of gscale every now and then\n            if iter_since_adapt >= adapt_every:\n                iter_since_adapt = 0\n                if len(self.success_window) > 5:\n                    succ_rate = float(np.mean(self.success_window))\n                    # if too easy (high success) -> contract slightly, if too hard -> expand\n                    if succ_rate > 0.45:\n                        self.gscale *= 0.88\n                    elif succ_rate < 0.12:\n                        self.gscale *= 1.18\n                    else:\n                        # mild jitter to escape stagnation\n                        self.gscale *= (0.98 + 0.04 * self.rng.rand())\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-4, 3.0))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max * 1.5:\n                # keep best ones and some randoms\n                arr_f_all = np.array([a[1] for a in self.archive], dtype=float)\n                # gather finite indices\n                finite_idxs_all = np.where(np.isfinite(arr_f_all))[0]\n                kept_ix = set()\n                # keep best ones\n                if finite_idxs_all.size > 0:\n                    best_idxs = finite_idxs_all[np.argsort(arr_f_all[finite_idxs_all])[:max(6, int(0.2 * self.archive_max))]]\n                    for ii in best_idxs:\n                        kept_ix.add(int(ii))\n                # keep current best if present\n                if self.best_x is not None:\n                    # find a matching archive index (closest)\n                    dists = np.linalg.norm(np.vstack([a[0] for a in self.archive]) - self.best_x, axis=1)\n                    best_idx = int(np.argmin(dists))\n                    kept_ix.add(best_idx)\n                # keep some random ones for diversity\n                keep_rand = max(10, self.archive_max - len(kept_ix))\n                candidates_idx = [i for i in range(len(self.archive)) if i not in kept_ix]\n                self.rng.shuffle(candidates_idx)\n                for i in candidates_idx[:keep_rand]:\n                    kept_ix.add(int(i))\n                new_archive = [self.archive[i] for i in sorted(kept_ix)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > max(100, self.window_len * 2):\n                # micro-restart: generate a small local cloud around best and some randoms\n                if self.best_x is not None:\n                    n_local = min(30, max(6, self.dim * 4))\n                    for _ in range(n_local):\n                        if self.evals >= self.budget:\n                            break\n                        local_noise = self.rng.randn(self.dim) * (scale_vec * (0.5 + self.gscale * 0.8) * self.rng.rand())\n                        cand = self._reflect_bounds(self.best_x + local_noise, lb, ub)\n                        f = safe_eval(cand)\n                        self.archive.append((cand.copy(), f))\n                        if np.isfinite(f) and f < self.best_f:\n                            self.best_x = cand.copy()\n                            self.best_f = f\n                            self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_x = cand.copy()\n                        self.best_f = f\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.25, 1e-4, 3.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_archive) > 0:\n            best_pair = min(finite_archive, key=lambda t: t[1])\n            self.best_f, self.best_x = best_pair[1], best_pair[0].copy()\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.best_x is None:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x) if self.evals < self.budget else np.inf\n                self.best_x = x\n                self.best_f = f\n\n        return float(self.best_f), np.asarray(self.best_x, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "57635ce4-6520-4b0d-bb4d-2459d9d99bc0", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n        self.evals = 0\n\n        # adaptation windows & counters\n        self.window_len = max(30, max(50, self.budget // 100))  # adapt window in evaluations\n        self.success_window = []\n        self.last_improve_at = 0\n\n        # archive of (x,f)\n        self.archive = []\n\n        # global exploration multiplier (adapted)\n        self.gscale = 0.5\n\n        # archive capacity\n        self.archive_max = int(archive_max)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # try func.bounds as tuple/list or object with lb/ub\n        b = getattr(func, \"bounds\", None)\n        try:\n            if b is not None:\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb, ub = b[0], b[1]\n                else:\n                    # object with lb/ub or lower/upper attributes\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n        except Exception:\n            lb = ub = None\n\n        # also try attributes directly on func\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # if still None, fallback to defaults\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n            return lb, ub\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # scalars broadcast\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # if shapes mismatch or wrong size, fall back\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that are out of bounds (mirror back)\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_iter):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            # reflect low: x = lb + (lb - x)\n            x[low] = lb[low] + (lb[low] - x[low])\n            # reflect high: x = ub - (x - ub)\n            x[high] = ub[high] - (x[high] - ub[high])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        points = np.zeros((n, dim), dtype=float)\n        for d in range(dim):\n            perm = np.arange(n)\n            self.rng.shuffle(perm)\n            points[:, d] = (perm + self.rng.rand(n)) / float(n)\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        scale_vec = np.maximum(np.asarray(scale_vec, dtype=float), 1e-12)\n        c = self.rng.standard_cauchy(self.dim)\n        heavy = self.rng.rand(self.dim) < 0.02\n        factor = np.tanh(np.abs(c)) / (np.abs(c) + 1e-12)\n        factor = np.where(heavy, 1.0, factor)\n        tempered = np.sign(c) * (np.abs(c) * factor)\n        # scale per-dim and cap\n        res = tempered * scale_vec\n        cap = cap_multiplier * scale_vec + 1e-12\n        res = np.clip(res, -cap, cap)\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = np.maximum(ub - lb, 1e-12)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x_ = np.asarray(x, dtype=float).reshape(self.dim)\n            try:\n                f = func(x_)\n            except Exception:\n                # if function raises, treat as invalid\n                f = np.inf\n            self.evals += 1\n            # ensure numeric\n            try:\n                if np.isnan(f) or np.isinf(f):\n                    return np.inf\n            except Exception:\n                return np.inf\n            return float(f)\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(12, self.dim * 3), max(1, self.budget // 20))\n        n_init = min(n_init, max(1, self.budget))\n        lhs = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = lb + lhs[i] * domain_width\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        n_extra = min(6, max(0, self.budget - self.evals))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        finite_vals = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_vals) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        adapt_every = max(10, min(200, self.window_len // 3))\n        iter_since_adapt = 0\n\n        while self.evals < self.budget:\n            # ensure archive arrays\n            if len(self.archive) == 0:\n                # unlikely, but sample random\n                x_rand = self._uniform_array(lb, ub)\n                f_rand = safe_eval(x_rand)\n                self.archive.append((x_rand.copy(), f_rand))\n                if np.isfinite(f_rand) and f_rand < self.best_f:\n                    self.best_f = f_rand\n                    self.best_x = x_rand.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            if arr_X.shape[0] >= 4:\n                iqr = np.subtract(*np.percentile(arr_X, [75, 25], axis=0))\n                per_dim_scale = iqr / 1.349  # approx std from iqr\n                zero_mask = per_dim_scale <= 1e-12\n                if np.any(zero_mask):\n                    per_dim_scale[zero_mask] = np.std(arr_X[:, zero_mask], axis=0) + 1e-12\n            else:\n                per_dim_scale = np.std(arr_X, axis=0) + 1e-12\n            # final fallback: ensure not too tiny\n            per_dim_scale = np.maximum(per_dim_scale, domain_width * 1e-6)\n\n            # choose anchor: bias to elites if available\n            finite_mask = np.isfinite(arr_f)\n            candidates = np.where(finite_mask)[0] if finite_mask.any() else np.arange(arr_X.shape[0])\n            if candidates.size == 0:\n                anchor_idx = int(self.rng.randint(arr_X.shape[0]))\n            else:\n                # exponential bias toward the best\n                sorted_idx = candidates[np.argsort(arr_f[candidates])]\n                ranks = np.arange(sorted_idx.size)\n                lam = max(1.0, sorted_idx.size / 6.0)\n                weights = np.exp(-ranks / lam)\n                weights = weights / weights.sum()\n                pick_pos = self.rng.choice(np.arange(sorted_idx.size), p=weights)\n                anchor_idx = sorted_idx[pick_pos]\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = self.rng.rand()\n            x_new = None\n\n            # Strategy 1: DE-style differential injection (~22%)\n            if r < 0.22 and arr_X.shape[0] >= 3:\n                # pick two distinct others\n                others = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                if len(others) >= 2:\n                    b, c = self.rng.choice(others, size=2, replace=False)\n                    xb = arr_X[b]\n                    xc = arr_X[c]\n                    diff = xb - xc\n                    jitter = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                    scale_diff = (0.6 + 0.8 * self.rng.rand())\n                    candidate = anchor + scale_diff * diff + jitter\n                    x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation (~23%)\n            elif r < 0.45:\n                finite_exists = finite_mask\n                if finite_exists.any():\n                    sorted_idx_full = np.argsort(arr_f[finite_exists])\n                    finite_idx_list = np.where(finite_exists)[0]\n                    elite_count = min(max(3, self.dim), finite_idx_list.size)\n                    elite_idxs = finite_idx_list[sorted_idx_full[:elite_count]]\n                else:\n                    elite_count = min(max(3, self.dim), arr_X.shape[0])\n                    elite_idxs = self.rng.choice(range(arr_X.shape[0]), size=elite_count, replace=False)\n                elites = arr_X[elite_idxs]\n                mu = elites.mean(axis=0)\n                if elites.shape[0] >= 2:\n                    C = np.cov((elites - mu).T)\n                    # stable eigen-decomposition\n                    try:\n                        vals, vecs = np.linalg.eigh(C)\n                        # sort descending\n                        idxs = np.argsort(-vals)\n                        vals = vals[idxs]\n                        vecs = vecs[:, idxs]\n                    except Exception:\n                        vals = np.maximum(np.var(elites, axis=0), 1e-12)\n                        vecs = np.eye(self.dim)\n                        vals = np.concatenate([vals, np.zeros(max(0, self.dim - vals.size))])\n                else:\n                    vals = np.ones(self.dim) * 1e-6\n                    vecs = np.eye(self.dim)\n                vals = np.maximum(vals, 0.0)\n                # choose principal k dimensions (energy-based)\n                energy = np.cumsum(vals) / (vals.sum() + 1e-12)\n                k = max(1, min(self.dim, np.searchsorted(energy, 0.8) + 1))\n                k = min(k, vals.size)\n                # sample in principal subspace, scaled by gscale\n                coeffs = self.rng.randn(k) * (np.sqrt(vals[:k] + 1e-12)) * (0.6 + 1.2 * self.rng.rand()) * self.gscale\n                pca_step = vecs[:, :k].dot(coeffs)\n                # bias occasionally to anchor or to elite mean\n                if self.rng.rand() < 0.5:\n                    candidate = anchor + pca_step\n                else:\n                    candidate = mu + pca_step * (0.6 + 0.8 * self.rng.rand())\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 3: Local anisotropic gaussian (~45%)\n            else:\n                prob_coord = 0.22 + 0.08 * self.rng.rand()\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not coords.any():\n                    coords[self.rng.randint(self.dim)] = True\n                sigma = per_dim_scale * (0.8 + 0.6 * self.rng.rand()) * self.gscale\n                noise = self.rng.randn(self.dim) * sigma\n                candidate = anchor.copy()\n                candidate[coords] += noise[coords]\n                # occasionally mix in a few uniform coords for diversity\n                if self.rng.rand() < 0.08:\n                    u = self._uniform_array(lb, ub)\n                    candidate[coords & (self.rng.rand(self.dim) < 0.3)] = u[coords & (self.rng.rand(self.dim) < 0.3)]\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 4 fallback (tempered Cauchy) - implemented as occasional global escape\n            if x_new is None:\n                jump = self._tempered_cauchy(domain_width * (0.4 + self.gscale * 1.6))\n                candidate = anchor + jump\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # sometimes add a small differential jump for added diversity\n            if self.rng.rand() < 0.12 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(arr_X.shape[0], size=2, replace=False)\n                small = 0.25 * (arr_X[i1] - arr_X[i2]) * (0.2 + 0.8 * self.rng.rand())\n                x_new = self._reflect_bounds(x_new + small, lb, ub)\n\n            # occasional direct small jump to current best to refine\n            if (self.best_x is not None) and (self.rng.rand() < 0.06):\n                small_noise = self.rng.randn(self.dim) * (per_dim_scale * 0.12 * (0.5 + self.rng.rand()))\n                x_new = self._reflect_bounds(self.best_x + small_noise, lb, ub)\n\n            # ensure candidate exists\n            if x_new is None:\n                x_new = self._uniform_array(lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad_coords = ~np.isfinite(x_new)\n            if np.any(bad_coords):\n                u = self._uniform_array(lb, ub)\n                x_new[bad_coords] = u[bad_coords]\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n            self.archive.append((x_new.copy(), f_new))\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                improved = True\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            try:\n                finite_fs = arr_f[np.isfinite(arr_f)]\n                # include current if finite\n                if np.isfinite(f_new):\n                    combined = np.concatenate([finite_fs, np.array([f_new])])\n                    rank = (combined < f_new).sum()  # number of strictly better\n                    frac_better = rank / max(1, combined.size)\n                    success = (frac_better <= 0.25)\n                else:\n                    success = False\n            except Exception:\n                success = improved\n\n            self.success_window.append(1 if success else 0)\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            iter_since_adapt += 1\n            # adaptation of gscale every now and then\n            if iter_since_adapt >= adapt_every:\n                iter_since_adapt = 0\n                if len(self.success_window) > 5:\n                    succ_rate = float(np.mean(self.success_window))\n                    # if too easy -> contract slightly, if too hard -> expand\n                    if succ_rate > 0.45:\n                        self.gscale *= 0.85\n                    elif succ_rate < 0.15:\n                        self.gscale *= 1.18\n                    else:\n                        # mild jitter to escape stagnation\n                        self.gscale *= (0.98 + 0.04 * self.rng.rand())\n                    # clamp gscale to reasonable bounds\n                    self.gscale = float(np.clip(self.gscale, 1e-3, 8.0))\n\n            # prune archive if too large\n            if len(self.archive) > self.archive_max:\n                arr_f_all = np.array([a[1] for a in self.archive], dtype=float)\n                finite_idxs_all = np.where(np.isfinite(arr_f_all))[0]\n                kept_ix = set()\n                # keep best ones\n                if finite_idxs_all.size > 0:\n                    best_n = max(6, int(0.2 * self.archive_max))\n                    best_idxs = finite_idxs_all[np.argsort(arr_f_all[finite_idxs_all])[:best_n]]\n                    kept_ix.update(int(i) for i in best_idxs)\n                # keep current best if present\n                if self.best_x is not None:\n                    # find closest archive entry (euclidean)\n                    dists = np.linalg.norm(arr_X - self.best_x.reshape(1, -1), axis=1)\n                    best_idx = int(np.argmin(dists))\n                    kept_ix.add(best_idx)\n                # keep some random ones for diversity\n                keep_rand = max(10, self.archive_max - len(kept_ix))\n                candidates_idx = [i for i in range(len(self.archive)) if i not in kept_ix]\n                if len(candidates_idx) > 0:\n                    choice = self.rng.choice(candidates_idx, size=min(len(candidates_idx), keep_rand), replace=False)\n                    kept_ix.update(int(i) for i in choice)\n                # build new archive\n                kept_list = sorted(list(kept_ix))\n                new_archive = [self.archive[i] for i in kept_list]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > int(1.5 * self.window_len) and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                if self.best_x is not None:\n                    n_local = min(30, max(6, self.dim * 4))\n                    for _ in range(n_local):\n                        if self.evals >= self.budget:\n                            break\n                        local_noise = self.rng.randn(self.dim) * (per_dim_scale * (0.5 + self.gscale * 0.8) * self.rng.rand())\n                        cand = self._reflect_bounds(self.best_x + local_noise, lb, ub)\n                        f = safe_eval(cand)\n                        self.archive.append((cand.copy(), f))\n                        if np.isfinite(f) and f < self.best_f:\n                            self.best_f = f\n                            self.best_x = cand.copy()\n                            self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_x = cand.copy()\n                        self.best_f = f\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale *= 1.12\n                self.gscale = float(np.clip(self.gscale, 1e-3, 8.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [a for a in self.archive if np.isfinite(a[1])]\n        if len(finite_archive) > 0:\n            best_pair = min(finite_archive, key=lambda af: af[1])\n            self.best_f, self.best_x = best_pair[1], best_pair[0].copy()\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.best_x is None and self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.best_f = f\n                self.best_x = x.copy()\n\n        return self.best_f, (self.best_x.copy() if self.best_x is not None else None)", "configspace": "", "generation": 0, "feedback": "In the code, line 317, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 0 input values to the 1 output values where the mask is true\nOn line: candidate[coords & (self.rng.rand(self.dim) < 0.3)] = u[coords & (self.rng.rand(self.dim) < 0.3)]", "error": "In the code, line 317, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 0 input values to the 1 output values where the mask is true\nOn line: candidate[coords & (self.rng.rand(self.dim) < 0.3)] = u[coords & (self.rng.rand(self.dim) < 0.3)]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "04848905-66a9-4f2a-b0c3-77fa693bf640", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style differential injections, PCA-guided elite moves, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.archive = []  # list of (x (np.array), f (float))\n        self.archive_max = int(archive_max)\n        # adaptive global multiplier (gscale): multiplies per-dim scales inferred from archive\n        self.gscale = 0.18\n        self.success_window = []  # binary list: 1 for success, 0 for fail (short-term)\n        self.window_len = 30\n        self.evals = 0\n        # bookkeeping best\n        self.best_x = None\n        self.best_f = np.inf\n        self.last_improve_at = 0\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # tries several common patterns, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            if isinstance(b, (tuple, list)) and len(b) == 2:\n                # could be (lb_arr, ub_arr)\n                try:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                except Exception:\n                    lb = None\n                    ub = None\n            else:\n                # object with .lb/.ub or .lower/.upper\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or lb\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or ub\n\n        # also try direct attributes on func\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # default fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            # scalars -> broadcast\n            if lb.ndim == 0:\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if ub.ndim == 0:\n                ub = np.full(self.dim, float(ub), dtype=float)\n            # final shape check\n            if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n                lb = np.full(self.dim, -5.0, dtype=float)\n                ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that are out of bounds (mirror back) and clamp finally\n        x = np.asarray(x, dtype=float)\n        for _ in range(max_iter):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            # reflect low: x = lb + (lb - x)\n            x[low] = lb[low] + (lb[low] - x[low])\n            # reflect high: x = ub - (x - ub)\n            x[high] = ub[high] - (x[high] - ub[high])\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        # each dimension: pick a random permutation of n strata\n        points = np.empty((n, dim), dtype=float)\n        for j in range(dim):\n            perm = np.arange(n)\n            self.rng.shuffle(perm)\n            # stratified samples within each stratum\n            points[:, j] = (perm + self.rng.random(n)) / n\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.random(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.random((n, self.dim)) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        # sample standard Cauchy via tan(pi*(u-0.5))\n        u = self.rng.random(self.dim)\n        z = np.tan(np.pi * (u - 0.5))\n        # heavy tail occasionally\n        heavy = self.rng.random(self.dim) < 0.12\n        factor = np.where(heavy, 1.0 + 5.0 * self.rng.random(self.dim), 0.5 + 0.5 * self.rng.random(self.dim))\n        step = np.tanh(z * 0.8) * factor  # temper extremes slightly\n        cap = cap_multiplier * (scale_vec + 1e-12)\n        step = np.sign(step) * np.minimum(np.abs(step) * scale_vec, cap)\n        return step\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                raise RuntimeError(\"Budget exhausted\")\n            try:\n                f = func(np.asarray(x, dtype=float))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return float(f)\n\n        # initialization: LHS sampling (small initial design)\n        n_init = min(max(20, self.dim * 6), int(self.budget // 10 + 1))\n        n_init = max(3, n_init)\n        n_init = min(n_init, self.budget)\n        X0 = lb + self._lhs01(n_init, self.dim) * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniform samples for diversity (bounded by budget)\n        n_extra = min(10, max(3, int(self.budget * 0.02)))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, sample one more random if budget allows\n        if not np.isfinite(self.best_f) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), f))\n            if np.isfinite(f):\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # main optimization loop\n        stagnation_thresh = max(100, int(0.06 * self.budget))\n        while self.evals < self.budget:\n            # ensure archive arrays\n            arr_X = np.array([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive])\n\n            # if archive is tiny, do uniform proposals\n            if arr_X.shape[0] < 4:\n                if self.evals >= self.budget:\n                    break\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), f_new))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            try:\n                finite_mask = np.isfinite(arr_f)\n                arr_X_finite = arr_X[finite_mask] if finite_mask.any() else arr_X\n                if arr_X_finite.shape[0] >= 2:\n                    iqr = np.subtract(*np.percentile(arr_X_finite, [75, 25], axis=0))\n                    per_dim_scale = iqr / 1.349\n                    zero_mask = per_dim_scale <= 1e-12\n                    if zero_mask.any():\n                        # fallback to std for those coords\n                        std_f = np.std(arr_X_finite, axis=0, ddof=0)\n                        per_dim_scale[zero_mask] = std_f[zero_mask]\n                    per_dim_scale = np.maximum(per_dim_scale, 1e-12)\n                else:\n                    per_dim_scale = np.maximum(np.std(arr_X, axis=0, ddof=0), 1e-12)\n            except Exception:\n                per_dim_scale = np.maximum(np.std(arr_X, axis=0, ddof=0), 1e-12)\n\n            # normalize scale to domain fraction so that gscale remains interpretable\n            scale_vec = per_dim_scale / (domain_width + 1e-12) * domain_width\n            scale_vec = np.maximum(scale_vec, 1e-12)\n\n            # choose anchor: bias to elites if available\n            finite_mask = np.isfinite(arr_f)\n            if finite_mask.any():\n                finite_idx = np.where(finite_mask)[0]\n                finite_fs = arr_f[finite_idx]\n                sorted_idx = finite_idx[np.argsort(finite_fs)]\n                topk = max(1, int(round(0.18 * len(sorted_idx))))\n                top_candidates = sorted_idx[:topk]\n                if self.rng.random() < 0.75:\n                    anchor_idx = int(self.rng.choice(top_candidates))\n                else:\n                    anchor_idx = int(self.rng.choice(np.arange(arr_X.shape[0])))\n            else:\n                anchor_idx = int(self.rng.integers(0, arr_X.shape[0]))\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = self.rng.random()\n\n            x_new = None\n\n            # Strategy 1: DE-style differential injection (~22%)\n            if r < 0.22:\n                others = [i for i in range(arr_X.shape[0]) if i != anchor_idx]\n                if len(others) >= 2:\n                    b, c = self.rng.choice(others, size=2, replace=False)\n                    F = 0.8 * (1.0 + 0.5 * (self.rng.random() - 0.5)) * self.gscale\n                    diff = arr_X[b] - arr_X[c]\n                    jitter = self.rng.normal(0.0, scale_vec * 0.6 * self.gscale)\n                    candidate = anchor + F * diff + jitter\n                    x_new = candidate\n\n            # Strategy 2: PCA-guided elite perturbation (~23%)\n            elif r < 0.45:\n                # use top elites if possible\n                finite_exists = np.isfinite(arr_f)\n                if finite_exists.any():\n                    idxs = np.where(finite_exists)[0]\n                    best_idxs = idxs[np.argsort(arr_f[idxs])]\n                    elite_count = min(max(3, self.dim // 2), len(best_idxs))\n                    elites = arr_X[best_idxs[:elite_count]]\n                else:\n                    elites = arr_X[np.argsort(arr_f)[:min(6, len(arr_X))]]\n                mu = np.mean(elites, axis=0)\n                if elites.shape[0] > 1:\n                    cov = np.cov(elites.T) + np.diag(np.maximum(scale_vec * 1e-3, 1e-12))\n                    try:\n                        evals, vecs = np.linalg.eigh(cov)\n                        # pick principal subspace dimension\n                        pc_dim = min(max(1, int(round(self.dim * 0.25))), np.sum(evals > 1e-12))\n                        if pc_dim < 1:\n                            pc_dim = 1\n                        # sample along a few principal components\n                        chosen = np.argsort(evals)[-pc_dim:]\n                        coeffs = self.rng.normal(0.0, np.sqrt(np.maximum(evals[chosen], 1e-12))) * (self.gscale * 0.9)\n                        pca_step = vecs[:, chosen].dot(coeffs)\n                        if self.rng.random() < 0.6:\n                            candidate = mu + pca_step\n                        else:\n                            candidate = anchor + pca_step\n                    except Exception:\n                        candidate = anchor + self.rng.normal(0.0, scale_vec * self.gscale)\n                else:\n                    candidate = anchor + self.rng.normal(0.0, scale_vec * self.gscale)\n                x_new = candidate\n\n            # Strategy 3: Local anisotropic gaussian (~45%)\n            elif r < 0.90:\n                prob_coord = 0.28 + 0.2 * (self.dim / max(1, 20))\n                coords = self.rng.random(self.dim) < prob_coord\n                if not coords.any():\n                    coords[self.rng.integers(0, self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise[coords] = self.rng.normal(0.0, scale_vec[coords] * self.gscale)\n                candidate = anchor + noise\n                # occasionally mix in a few uniform coords for diversity\n                if self.rng.random() < 0.08:\n                    u_idx = self.rng.choice(self.dim, size=max(1, self.dim//10), replace=False)\n                    candidate[u_idx] = lb[u_idx] + self.rng.random(len(u_idx)) * (ub[u_idx] - lb[u_idx])\n                x_new = candidate\n\n            # Strategy 4: tempered Cauchy global escape (~10%)\n            else:\n                step = self._tempered_cauchy(scale_vec * self.gscale, cap_multiplier=8.0)\n                if self.best_x is not None and self.rng.random() < 0.4:\n                    candidate = self.best_x + step\n                else:\n                    candidate = anchor + step\n                x_new = candidate\n\n            # sometimes add a small differential jump for added diversity\n            if self.rng.random() < 0.12 and arr_X.shape[0] >= 2:\n                i1, i2 = self.rng.choice(range(arr_X.shape[0]), size=2, replace=False)\n                smallF = 0.28 * self.gscale\n                x_new = x_new + smallF * (arr_X[i1] - arr_X[i2])\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.random() < 0.06:\n                towards = (self.best_x - (x_new if x_new is not None else anchor))\n                x_new = (x_new if x_new is not None else anchor) + 0.18 * self.rng.random() * towards + self.rng.normal(0.0, scale_vec * 0.08)\n\n            if x_new is None:\n                x_new = self._uniform_array(lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad = ~np.isfinite(x_new)\n            if bad.any():\n                x_new[bad] = lb[bad] + self.rng.random(np.sum(bad)) * (ub[bad] - lb[bad])\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), f_new))\n\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = f_new\n                self.best_x = x_new.copy()\n                improved = True\n                self.last_improve_at = self.evals\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            finite_fs = arr_f[np.isfinite(arr_f)]\n            if finite_fs.size > 0 and np.isfinite(f_new):\n                threshold = np.percentile(finite_fs, 25.0)\n                success = 1 if f_new <= threshold else 0\n            else:\n                success = 1 if np.isfinite(f_new) else 0\n            self.success_window.append(success)\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every now and then\n            if len(self.success_window) == self.window_len and (self.evals % max(10, self.window_len) == 0):\n                succ_rate = float(np.mean(self.success_window))\n                if succ_rate > 0.45:\n                    self.gscale *= 0.88  # contract if easy\n                elif succ_rate < 0.15:\n                    self.gscale *= 1.22  # expand if too hard\n                else:\n                    # small jitter to avoid stagnation\n                    self.gscale *= (1.0 + (self.rng.random() - 0.5) * 0.06)\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-4, 3.0))\n\n            # prune archive if too large (periodically)\n            if len(self.archive) > max(10, self.archive_max * 2):\n                # keep best ones and some randoms\n                arr_X = np.array([a[0] for a in self.archive])\n                arr_f = np.array([a[1] for a in self.archive])\n                finite_idx = np.where(np.isfinite(arr_f))[0]\n                kept_ix = set()\n                if finite_idx.size > 0:\n                    best_fin = finite_idx[np.argsort(arr_f[finite_idx])]\n                    n_keep_best = min(len(best_fin), int(self.archive_max * 0.6))\n                    for ii in best_fin[:n_keep_best]:\n                        kept_ix.add(int(ii))\n                # keep current best if present\n                if self.best_x is not None:\n                    # find closest in archive\n                    dists = np.linalg.norm(arr_X - self.best_x, axis=1)\n                    best_idx = int(np.argmin(dists))\n                    kept_ix.add(best_idx)\n                # keep some random ones for diversity\n                candidates_idx = [i for i in range(len(self.archive)) if i not in kept_ix]\n                self.rng.shuffle(candidates_idx)\n                need = max(10, self.archive_max - len(kept_ix))\n                for ii in candidates_idx[:need]:\n                    kept_ix.add(int(ii))\n                # rebuild archive\n                new_archive = [self.archive[i] for i in sorted(list(kept_ix))]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > stagnation_thresh and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(40, max(8, self.dim * 5))\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    if self.best_x is not None:\n                        cand = self.best_x + self.rng.normal(0.0, scale_vec * (0.6 * self.gscale))\n                        cand = self._reflect_bounds(cand, lb, ub)\n                    else:\n                        cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # add some global randoms to regain diversity\n                n_global = min(10, max(3, int(0.02 * self.budget)))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), f))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.25, 1e-4, 3.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [(x, f) for (x, f) in self.archive if np.isfinite(f)]\n        if len(finite_archive) > 0:\n            best_pair = min(finite_archive, key=lambda p: p[1])\n            self.best_x = best_pair[0].copy()\n            self.best_f = float(best_pair[1])\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                try:\n                    f = safe_eval(x)\n                except RuntimeError:\n                    f = np.inf\n                self.best_x = x\n                self.best_f = f\n            else:\n                # cannot do anything: return previous best (maybe inf)\n                pass\n\n        return float(self.best_f), np.asarray(self.best_x, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 49, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or lb", "error": "In the code, line 49, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or lb", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0ea0620e-03dd-4ee3-8bb9-5ebe7dc5195b", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS)", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        f_best, x_best = opt(func)\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n\n        # bookkeeping\n        self.best_x = None\n        self.best_f = np.inf\n        self.evals = 0\n        self.last_improve_at = 0\n\n        # adaptation windows & counters\n        self.success_window = []\n        self.window_len = 120\n        self.gscale = 0.6  # global exploration multiplier (adapted)\n        self.archive = []\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        lb, ub = None, None\n        # Try common patterns for bounds on the function object\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # tuple (lb, ub)\n                if isinstance(b, tuple) and len(b) == 2:\n                    lb, ub = b[0], b[1]\n                # array shape (dim,2)\n                elif isinstance(b, np.ndarray) and b.ndim == 2 and b.shape[1] == 2 and b.shape[0] == self.dim:\n                    lb = b[:, 0]\n                    ub = b[:, 1]\n                # object with lb/ub\n                elif hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = getattr(b, \"lb\")\n                    ub = getattr(b, \"ub\")\n                elif hasattr(b, \"lower\") and hasattr(b, \"upper\"):\n                    lb = getattr(b, \"lower\")\n                    ub = getattr(b, \"upper\")\n        except Exception:\n            pass\n\n        # try attributes directly on func\n        if lb is None:\n            lb = getattr(func, \"lb\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None)\n\n        # fallback to scalar or per-dim defaults\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # final safety\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(max_iter):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            # mirror out-of-bounds coordinates\n            x[low] = lb[low] + (lb[low] - x[low])\n            x[high] = ub[high] - (x[high] - ub[high])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # Simple Latin Hypercube in [0,1)\n        pts = np.empty((n, dim), dtype=float)\n        for j in range(dim):\n            perm = self.rng.permutation(n)\n            pts[:, j] = (perm + self.rng.rand(n)) / float(n)\n        return pts\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.rand(n, self.dim) * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        - sample standard Cauchy via tan(pi*(u-0.5))\n        - temper with tanh to reduce extremely large tails mostly but keep occasional heavy jumps\n        - scale per-dim and cap extremes\n        \"\"\"\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        dim = self.dim\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy variates\n        # occasional heavy-tail amplification\n        heavy = (self.rng.rand(dim) < 0.03)\n        amplified = c * (1.0 + heavy.astype(float) * (2.5 + 5.0 * self.rng.rand(dim)))\n        tempered = np.tanh(amplified)  # compress extreme tails\n        # gentle factor to scale down very small coordinates\n        factor = np.abs(tempered) / (np.abs(tempered) + 1e-12)\n        res = tempered * scale_vec * (0.8 + 0.4 * factor)\n        cap = cap_multiplier * scale_vec\n        res = np.clip(res, -cap, cap)\n        return res\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # ---- initialization: LHS samples ----\n        # start with a modest initial sampling using LHS for coverage\n        n_init = min(max(8, 4 * self.dim), max(2, self.budget // 10))\n        pts = lb + (ub - lb) * self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = pts[i]\n            f = safe_eval(x)\n            self.archive.append((x.copy(), float(f)))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = float(f)\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        n_extra = min(6, max(0, (self.budget - self.evals) // 50))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), float(f)))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = float(f)\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # if still no finite best, try one more random if budget allows\n        if (self.best_x is None or not np.isfinite(self.best_f)) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive.append((x.copy(), float(f)))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = float(f)\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        iter_since_adapt = 0\n        adapt_every = 40\n        stagnation_thresh = max(200, 20 * self.dim)\n\n        while self.evals < self.budget:\n            iter_since_adapt += 1\n\n            # Ensure archive arrays\n            if len(self.archive) == 0:\n                # no archive entries yet: sample random\n                x_new = self._uniform_array(lb, ub)\n                f_new = safe_eval(x_new)\n                self.archive.append((x_new.copy(), float(f_new)))\n                if np.isfinite(f_new) and f_new < self.best_f:\n                    self.best_f = float(f_new)\n                    self.best_x = x_new.copy()\n                    self.last_improve_at = self.evals\n                continue\n\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive], dtype=float)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            try:\n                q75, q25 = np.percentile(arr_X, [75, 25], axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approximate std from IQR\n            except Exception:\n                per_dim_scale = np.std(arr_X, axis=0, ddof=0)\n\n            # fallback for near-zero scales: use domain fraction\n            small_mask = per_dim_scale <= 1e-12\n            per_dim_scale[small_mask] = domain_width[small_mask] * 0.08 + 1e-8\n            scale_vec = per_dim_scale * (domain_width / (domain_width + 1e-12))\n\n            # choose anchor: bias to elites if available\n            finite_mask = np.isfinite(arr_f)\n            if finite_mask.any():\n                candidates = np.where(finite_mask)[0]\n            else:\n                candidates = np.arange(len(self.archive))\n            ranks = np.argsort(arr_f[candidates])\n            topk = max(2, int(0.15 * arr_X.shape[0]))\n            top_candidates = candidates[ranks[:min(topk, len(ranks))]]\n            if top_candidates.size > 0 and self.rng.rand() < 0.85:\n                anchor_idx = int(self.rng.choice(top_candidates))\n            else:\n                anchor_idx = int(self.rng.randint(len(self.archive)))\n            anchor = arr_X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = self.rng.rand()\n\n            # Strategy 1: DE-style differential injection (~22%)\n            if r < 0.22:\n                # pick two distinct others\n                if len(self.archive) >= 3:\n                    idxs = list(range(len(self.archive)))\n                    idxs.remove(anchor_idx)\n                    i1, i2 = self.rng.choice(idxs, size=2, replace=False)\n                    diff = arr_X[i1] - arr_X[i2]\n                else:\n                    # fallback to small gaussian jitter\n                    diff = self.rng.randn(self.dim) * (per_dim_scale * 0.5)\n                jitter = self.rng.randn(self.dim) * (per_dim_scale * self.gscale * (0.2 + 0.8 * self.rng.rand()))\n                factor = 0.6 + 0.9 * self.rng.rand()\n                x_new = anchor + factor * diff * (self.gscale) + jitter\n\n            # Strategy 2: PCA-guided elite perturbation (~23%)\n            elif r < 0.45:\n                elite_count = min(max(2, int(0.12 * arr_X.shape[0])), arr_X.shape[0])\n                # pick best elites by fitness\n                elite_idxs = np.argsort(arr_f)[:elite_count]\n                elites = arr_X[elite_idxs]\n                mu = elites.mean(axis=0)\n                try:\n                    cov = np.cov(elites, rowvar=False)\n                    # small regularization\n                    cov += np.eye(self.dim) * 1e-8\n                    vals, vecs = np.linalg.eigh(cov)\n                except Exception:\n                    # fallback: diagonal from variance\n                    vals = np.maximum(np.diag(np.cov(elites, rowvar=False)), 0.0)\n                    vecs = np.eye(self.dim)\n                vals = np.maximum(vals, 0.0)\n                # sample in principal subspace (use top k components)\n                k = min(self.dim, max(1, int(0.3 * self.dim)))\n                top_idx = np.argsort(vals)[-k:]\n                vecs_k = vecs[:, top_idx]\n                vals_k = vals[top_idx]\n                coeffs = self.rng.randn(k) * (np.sqrt(vals_k + 1e-12) * (0.4 + 1.2 * self.rng.rand(k)))\n                pca_step = vecs_k.dot(coeffs)\n                if self.rng.rand() < 0.5:\n                    candidate = anchor + pca_step * (self.gscale * (0.6 + 0.8 * self.rng.rand()))\n                else:\n                    candidate = mu + pca_step * (self.gscale * (0.6 + 0.8 * self.rng.rand()))\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 3: Local anisotropic gaussian (~45%)\n            elif r < 0.9:\n                candidate = anchor.copy()\n                # choose coordinates to mutate (make sparse for large dims)\n                prob_coord = 0.12 + 0.5 * min(1.0, self.gscale)\n                coords = self.rng.rand(self.dim) < prob_coord\n                if not coords.any():\n                    coords[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim, dtype=float)\n                noise_vals = self.rng.randn(coords.sum()) * (per_dim_scale[coords] * self.gscale * (0.15 + 1.0 * self.rng.rand()))\n                noise[coords] = noise_vals\n                candidate = candidate + noise\n                # occasionally mix in a few uniform coords for diversity\n                mix_coords = (self.rng.rand(self.dim) < 0.06)\n                if mix_coords.any():\n                    candidate[mix_coords] = self._uniform_array(lb[mix_coords], ub[mix_coords])\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 4: tempered Cauchy global escape (~10%)\n            else:\n                jump = self._tempered_cauchy(scale_vec * (1.0 + 6.0 * self.gscale))\n                x_new = anchor + jump\n\n            # sometimes add a small differential jump for added diversity\n            if self.rng.rand() < 0.14 and len(self.archive) >= 2:\n                i1, i2 = self.rng.choice(range(len(self.archive)), size=2, replace=False)\n                extra = 0.15 * self.gscale * (arr_X[i1] - arr_X[i2])\n                x_new = x_new + extra\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            x_new = np.asarray(x_new, dtype=float).copy()\n            bad_coords = ~np.isfinite(x_new)\n            if bad_coords.any():\n                x_new[bad_coords] = self._uniform_array(lb[bad_coords], ub[bad_coords])\n\n            # reflect to bounds & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_new = safe_eval(x_new)\n\n            # record into archive\n            self.archive.append((x_new.copy(), float(f_new)))\n            improved = False\n            if np.isfinite(f_new) and f_new < self.best_f:\n                self.best_f = float(f_new)\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                improved = True\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            finite_fs = arr_f[np.isfinite(arr_f)] if arr_f.size > 0 else np.array([])\n            top25_thresh = np.nan\n            if finite_fs.size > 0:\n                top25_thresh = np.percentile(finite_fs, 25.0)\n            success_flag = 1 if (np.isfinite(f_new) and (finite_fs.size == 0 or f_new <= top25_thresh)) else 0\n            self.success_window.append(success_flag)\n            # cap success window length\n            if len(self.success_window) > self.window_len:\n                self.success_window = self.success_window[-self.window_len:]\n\n            # adaptation of gscale every now and then\n            if iter_since_adapt >= adapt_every:\n                iter_since_adapt = 0\n                if len(self.success_window) > 0:\n                    succ_rate = float(np.mean(self.success_window))\n                else:\n                    succ_rate = 0.0\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.22:\n                    self.gscale *= 0.88\n                elif succ_rate < 0.06:\n                    self.gscale *= 1.18\n                # mild jitter to escape stagnation\n                if self.rng.rand() < 0.08:\n                    self.gscale *= (1.0 + 0.06 * (self.rng.rand() - 0.5))\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-4, 5.0))\n\n            # prune archive if too large\n            if len(self.archive) > int(self.archive_max * 1.5):\n                # keep best ones and some randoms\n                arr_f_full = np.array([a[1] for a in self.archive], dtype=float)\n                finite_idx = np.where(np.isfinite(arr_f_full))[0]\n                keep = set()\n                if finite_idx.size > 0:\n                    best_idxs = finite_idx[np.argsort(arr_f_full[finite_idx])[: self.archive_max // 2]]\n                    for ii in best_idxs:\n                        keep.add(int(ii))\n                # keep current best if present\n                if self.best_x is not None:\n                    # find closest archive entry to best_x\n                    dists = np.sum((arr_X - self.best_x) ** 2, axis=1)\n                    keep.add(int(np.argmin(dists)))\n                # keep some random ones for diversity\n                remaining = [i for i in range(len(self.archive)) if i not in keep]\n                n_random_keep = max(0, self.archive_max - len(keep))\n                if remaining and n_random_keep > 0:\n                    chosen = self.rng.choice(remaining, size=min(n_random_keep, len(remaining)), replace=False)\n                    for ii in chosen:\n                        keep.add(int(ii))\n                # rebuild archive\n                new_archive = [self.archive[i] for i in sorted(list(keep))]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.evals - self.last_improve_at) > stagnation_thresh and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(30, max(6, self.dim * 4))\n                n_local = int(n_local)\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    if self.best_x is None:\n                        local_noise = self.rng.randn(self.dim) * (scale_vec * (0.6 * self.gscale))\n                        cand = self._reflect_bounds(self._uniform_array(lb, ub), lb, ub)\n                    else:\n                        local_noise = self.rng.randn(self.dim) * (scale_vec * (0.6 * self.gscale) * (0.5 + self.rng.rand()))\n                        cand = self._reflect_bounds(self.best_x + local_noise, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), float(f)))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = float(f)\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # some global randoms to regain diversity\n                n_global = min(8, max(2, n_local // 6))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive.append((cand.copy(), float(f)))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = float(f)\n                        self.best_x = cand.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.15, 1e-4, 5.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_archive = [af for af in self.archive if np.isfinite(af[1])]\n        if len(finite_archive) > 0:\n            best_pair = min(finite_archive, key=lambda t: t[1])\n            self.best_f, self.best_x = float(best_pair[1]), best_pair[0].copy()\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.best_f = float(f)\n                self.best_x = x.copy()\n            else:\n                # return last-known best (maybe inf)\n                if self.best_x is None:\n                    self.best_x = self._uniform_array(lb, ub)\n                    self.best_f = np.inf\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 0, "feedback": "In the code, line 306, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 2 input values to the 1 output values where the mask is true\nOn line: candidate[mix_coords] = self._uniform_array(lb[mix_coords], ub[mix_coords])", "error": "In the code, line 306, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 2 input values to the 1 output values where the mask is true\nOn line: candidate[mix_coords] = self._uniform_array(lb[mix_coords], ub[mix_coords])", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "08d13207-49a7-4266-83e8-5fe0465d2173", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS)", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive = []  # list of (x (ndarray), f)\n        self.archive_max = int(archive_max)\n        # running best\n        self.best_f = np.inf\n        self.best_x = None\n\n        # adaptation parameters\n        self.gscale = 0.2  # global scale multiplier (adapted)\n        self.window_len = max(30, min(200, self.budget // 50))\n        self.success_window = []\n        self.eval_count = 0\n\n        # stagnation control\n        self.no_improve_iters = 0\n        self.last_improve_at = 0\n        self.stagnation_threshold = max(200, min(2000, self.budget // 10))\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        b = getattr(func, \"bounds\", None)\n        try:\n            if b is not None:\n                # if bounds is a tuple (lb, ub)\n                if isinstance(b, tuple) and len(b) == 2:\n                    lb, ub = b\n                else:\n                    # object with lb/ub or lower/upper attributes\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        # also try attributes directly on func\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # Normalize to numpy arrays; fallback to default\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            # broadcast scalars\n            if lb.ndim == 0:\n                lb = np.full(self.dim, float(lb), dtype=float)\n            if ub.ndim == 0:\n                ub = np.full(self.dim, float(ub), dtype=float)\n            # if shapes mismatch or wrong size, fall back\n            if lb.size != self.dim or ub.size != self.dim:\n                lb = np.full(self.dim, -5.0, dtype=float)\n                ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that are out of bounds (mirror back) for robustness\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            # reflect low: x = lb + (lb - x)\n            x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            # reflect high: x = ub - (x - ub)\n            x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = rng.rand(n, dim)\n        bins = cut[:-1, None] + (cut[1:] - cut[:-1])[:, None] * u\n        # shuffle per dimension\n        result = np.zeros_like(bins)\n        for j in range(dim):\n            perm = rng.permutation(n)\n            result[:, j] = bins[perm, j]\n        return result\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        shape = (n, lb.size)\n        samples = self.rng.rand(*shape) * (ub - lb)[None, :] + lb[None, :]\n        if n == 1:\n            return samples[0]\n        return samples\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise tempered Cauchy: standard Cauchy with tanh tempering + occasional heavy tail\n        dim = scale_vec.size\n        u = self.rng.rand(dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # temper extremes with tanh and small factor\n        factor = np.tanh(np.abs(c) / 3.0)  # in (0,1)\n        # occasionally allow heavy tail (2% dims)\n        heavy = (self.rng.rand(dim) < 0.02)\n        factor = np.where(heavy, 1.0, factor)\n        vals = np.sign(c) * factor * (np.abs(c) ** 0.9)  # slightly compress extreme tails\n        cap = cap_multiplier * (scale_vec.copy() + 1e-12)\n        coords = vals * scale_vec\n        # cap extreme values coordinate-wise\n        coords = np.clip(coords, -cap, cap)\n        return coords\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.eval_count >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.eval_count += 1\n            return f\n\n        lb, ub = self._get_bounds(func)\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(10 * self.dim, 40), max(4, self.budget // 10))\n        lhs = self._lhs01(n_init, self.dim)\n        for i in range(n_init):\n            if self.eval_count >= self.budget:\n                break\n            x = lb + lhs[i] * (ub - lb)\n            x = self._reflect_bounds(x, lb, ub)\n            f = safe_eval(x)\n            if np.isfinite(f):\n                self.archive.append((x.copy(), f))\n                if f < self.best_f:\n                    self.best_f = f\n                    self.best_x = x.copy()\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        add_uniform = min(10, max(2, self.dim // 2))\n        for _ in range(add_uniform):\n            if self.eval_count >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            if np.isfinite(f):\n                self.archive.append((x.copy(), f))\n                if f < self.best_f:\n                    self.best_f = f\n                    self.best_x = x.copy()\n\n        # if still no finite best, sample random until one finite or budget done\n        while (self.best_x is None or not np.isfinite(self.best_f)) and self.eval_count < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            if np.isfinite(f):\n                self.archive.append((x.copy(), f))\n                self.best_f = f\n                self.best_x = x.copy()\n\n        # If still none (very unlikely), return inf\n        if self.best_x is None:\n            return np.inf, None\n\n        # Main optimization loop\n        iter_since_adapt = 0\n        adapt_every = max(10, self.window_len // 4)\n\n        while self.eval_count < self.budget:\n            # Build arrays\n            arr_X = np.vstack([a[0] for a in self.archive])\n            arr_f = np.array([a[1] for a in self.archive])\n            # Finite indices\n            finite_idx = np.isfinite(arr_f)\n            if finite_idx.sum() == 0:\n                # fallback random\n                cand = self._uniform_array(lb, ub)\n                f_new = safe_eval(cand)\n                if np.isfinite(f_new):\n                    self.archive.append((cand.copy(), f_new))\n                    if f_new < self.best_f:\n                        self.best_f = f_new\n                        self.best_x = cand.copy()\n                continue\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            Xf = arr_X[finite_idx]\n            if Xf.shape[0] >= 4:\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                per_dim_scale = (q75 - q25) / 1.349  # approx std\n            else:\n                per_dim_scale = Xf.std(axis=0, ddof=0)\n            # fallback\n            zero_mask = per_dim_scale <= 1e-12\n            if zero_mask.any():\n                per_dim_scale[zero_mask] = np.maximum(1e-6, Xf.std(axis=0)[zero_mask])\n            # overall scale floor\n            per_dim_scale = np.maximum(per_dim_scale, 1e-8)\n\n            # choose anchor: bias to elites if available\n            candidates = np.flatnonzero(finite_idx)\n            sorted_idx = np.argsort(arr_f[candidates])\n            topk = max(1, min(len(sorted_idx), int(0.2 * len(sorted_idx))))\n            top_candidates = candidates[sorted_idx[:topk]]\n            # exponential bias: choose an index from top or overall\n            if self.rng.rand() < 0.6 and top_candidates.size > 0:\n                # choose within elites with exponential bias\n                exps = np.exp(-np.arange(top_candidates.size) / max(1.0, top_candidates.size / 5.0))\n                probs = exps / exps.sum()\n                pick = self.rng.choice(top_candidates, p=probs)\n            else:\n                pick = self.rng.choice(candidates)\n            anchor = arr_X[int(pick)].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = self.rng.rand()\n            candidate = None\n\n            # Strategy 1: DE-style differential injection (~22%)\n            if r < 0.22 and arr_X.shape[0] >= 3:\n                # pick two distinct others\n                idxs = list(range(arr_X.shape[0]))\n                idxs.remove(int(pick))\n                a, b = self.rng.choice(idxs, size=2, replace=False)\n                donor = arr_X[a] - arr_X[b]\n                # anisotropic gaussian jitter + scaled differential\n                jitter = self.rng.randn(self.dim) * (per_dim_scale * 0.4) * self.gscale\n                factor = 0.8 + 0.4 * self.rng.rand()  # differential scaling\n                candidate = anchor + factor * donor * (0.5 + 0.5 * self.rng.rand()) + jitter\n\n            # Strategy 2: PCA-guided elite perturbation (~23%)\n            elif r < 0.45 and arr_X.shape[0] >= 3:\n                elite_count = max(3, min(arr_X.shape[0], int(0.12 * arr_X.shape[0]) or 3))\n                # choose elites by fitness\n                finite_idxs = np.flatnonzero(finite_idx)\n                sorted_all = np.argsort(arr_f[finite_idxs])\n                elite_idxs = finite_idxs[sorted_all[:elite_count]]\n                elites = arr_X[elite_idxs]\n                # stable eigen-decomposition\n                if elites.shape[0] > 1:\n                    mean_el = elites.mean(axis=0)\n                    cov = np.cov((elites - mean_el).T)\n                    # regularize\n                    cov = cov + np.eye(self.dim) * 1e-8\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        order = np.argsort(-eigvals)\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # sample in principal subspace: pick top components proportionally\n                        ncomp = max(1, min(self.dim, int(np.sum(eigvals > eigvals.max() * 1e-3))))\n                        coeffs = np.zeros(self.dim)\n                        # sample principal coords\n                        coeffs[:ncomp] = self.rng.randn(ncomp) * (np.sqrt(np.maximum(eigvals[:ncomp], 1e-12))) * self.gscale\n                        # the rest get small isotropic noise\n                        if ncomp < self.dim:\n                            coeffs[ncomp:] = self.rng.randn(self.dim - ncomp) * (per_dim_scale.mean() * 0.08) * self.gscale\n                        step = eigvecs.dot(coeffs)\n                        # bias towards anchor or elite mean occasionally\n                        if self.rng.rand() < 0.5:\n                            base = anchor\n                        else:\n                            base = mean_el\n                        candidate = base + step\n                    except Exception:\n                        candidate = anchor + self.rng.randn(self.dim) * per_dim_scale * self.gscale * 0.5\n                else:\n                    candidate = anchor + self.rng.randn(self.dim) * per_dim_scale * self.gscale * 0.6\n\n            # Strategy 3: Local anisotropic gaussian (~45%)\n            elif r < 0.9:\n                # ensure at least one coord\n                noise = self.rng.randn(self.dim) * (per_dim_scale * (0.6 + 0.8 * self.rng.rand())) * self.gscale\n                candidate = anchor + noise\n                # occasionally mix in a few uniform coords for diversity\n                if self.rng.rand() < 0.08:\n                    mix_k = max(1, int(0.02 * self.dim))\n                    idxs = self.rng.choice(self.dim, size=mix_k, replace=False)\n                    candidate[idxs] = self._uniform_array(lb, ub, n=1)[idxs]\n\n            # Strategy 4: tempered Cauchy global escape (~10%)\n            else:\n                # large jumps occasionally allow escapes\n                scale_vec = per_dim_scale * (4.0 + 10.0 * self.rng.rand()) * self.gscale\n                jump = self._tempered_cauchy(scale_vec, cap_multiplier=8.0)\n                candidate = anchor + jump\n                # with small chance jump from random point\n                if self.rng.rand() < 0.2:\n                    candidate = self._uniform_array(lb, ub)\n\n            # sometimes add a small differential jump for added diversity\n            if self.rng.rand() < 0.12 and arr_X.shape[0] >= 2:\n                a, b = self.rng.choice(arr_X.shape[0], size=2, replace=False)\n                candidate += 0.15 * (arr_X[a] - arr_X[b]) * self.gscale\n\n            # occasional direct small jump to current best to refine\n            if self.rng.rand() < 0.06 and self.best_x is not None:\n                candidate = self.best_x + self.rng.randn(self.dim) * per_dim_scale * (0.2 * self.gscale)\n\n            # ensure candidate exists\n            if candidate is None:\n                candidate = self._uniform_array(lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            mask_bad = ~np.isfinite(candidate)\n            if mask_bad.any():\n                candidate[mask_bad] = self._uniform_array(lb, ub, n=1)[mask_bad]\n\n            # reflect to bounds & clamp\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.eval_count >= self.budget:\n                break\n            f_new = safe_eval(candidate)\n\n            # record into archive\n            if np.isfinite(f_new):\n                self.archive.append((candidate.copy(), f_new))\n                # update best\n                if f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = candidate.copy()\n                    self.no_improve_iters = 0\n                    self.last_improve_at = self.eval_count\n                    improved = True\n                else:\n                    improved = False\n            else:\n                improved = False\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            finite_fs = arr_f.copy()\n            if finite_fs.size > 0:\n                # include new candidate's f for percentile evaluation\n                finite_fs = np.concatenate([finite_fs, np.array([f_new])])\n                if np.isfinite(f_new):\n                    threshold = np.percentile(finite_fs[np.isfinite(finite_fs)], 25)\n                    is_success = f_new <= threshold\n                else:\n                    is_success = False\n            else:\n                is_success = np.isfinite(f_new)\n\n            self.success_window.append(1 if is_success else 0)\n            if len(self.success_window) > self.window_len:\n                self.success_window.pop(0)\n\n            iter_since_adapt += 1\n            self.no_improve_iters += 1\n\n            # adaptation of gscale every now and then\n            if iter_since_adapt >= adapt_every:\n                success_rate = float(np.mean(self.success_window)) if len(self.success_window) > 0 else 0.0\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if success_rate > 0.35:\n                    self.gscale *= 0.92\n                elif success_rate < 0.08:\n                    self.gscale *= 1.12\n                else:\n                    # mild jitter to escape stagnation\n                    self.gscale *= 1.0 + (self.rng.rand() - 0.5) * 0.06\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-5, 4.0))\n                iter_since_adapt = 0\n\n            # prune archive if too large\n            if len(self.archive) > int(self.archive_max * 1.5):\n                # keep best ones and some randoms\n                arr_f_all = np.array([a[1] for a in self.archive])\n                finite_idx_all = np.isfinite(arr_f_all)\n                candidates_idx = np.arange(len(self.archive))\n                kept_ix = set()\n                # keep best half (finite)\n                fin_idxs = np.flatnonzero(finite_idx_all)\n                if fin_idxs.size > 0:\n                    sorted_fin = fin_idxs[np.argsort(arr_f_all[fin_idxs])]\n                    keep_best_n = max(1, min(len(sorted_fin), self.archive_max // 2))\n                    for i in sorted_fin[:keep_best_n]:\n                        kept_ix.add(int(i))\n                # keep current best if present\n                if self.best_x is not None:\n                    # find closest archive index\n                    dists = np.linalg.norm(np.vstack([a[0] for a in self.archive]) - self.best_x[None, :], axis=1)\n                    best_idx = int(np.argmin(dists))\n                    kept_ix.add(best_idx)\n                # keep some random ones for diversity\n                n_random = max(1, min(self.archive_max // 4, len(self.archive) - len(kept_ix)))\n                idxs = list(set(range(len(self.archive))) - kept_ix)\n                if len(idxs) > 0:\n                    pick_rand = self.rng.choice(idxs, size=min(n_random, len(idxs)), replace=False)\n                    for i in pick_rand:\n                        kept_ix.add(int(i))\n                # finalize new archive\n                new_archive = [self.archive[i] for i in sorted(kept_ix)]\n                self.archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if (self.eval_count < self.budget) and (self.no_improve_iters > self.stagnation_threshold):\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(12, max(4, self.dim))\n                if self.best_x is not None:\n                    for _ in range(n_local):\n                        cand = self.best_x + self.rng.randn(self.dim) * per_dim_scale * (0.6 * self.gscale)\n                        cand = self._reflect_bounds(cand, lb, ub)\n                        if self.eval_count >= self.budget:\n                            break\n                        f = safe_eval(cand)\n                        if np.isfinite(f):\n                            self.archive.append((cand.copy(), f))\n                            if f < self.best_f:\n                                self.best_f = f\n                                self.best_x = cand.copy()\n                                self.no_improve_iters = 0\n                # some global randoms to regain diversity\n                n_global = min(12, max(4, self.dim))\n                for _ in range(n_global):\n                    if self.eval_count >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    if np.isfinite(f):\n                        self.archive.append((cand.copy(), f))\n                        if f < self.best_f:\n                            self.best_f = f\n                            self.best_x = cand.copy()\n                            self.no_improve_iters = 0\n                # after restart, slightly increase exploration scales\n                self.gscale = float(np.clip(self.gscale * 1.25, 1e-5, 4.0))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.no_improve_iters = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive) > 0:\n            finite_pairs = [(x, f) for (x, f) in self.archive if np.isfinite(f)]\n            if len(finite_pairs) > 0:\n                best_pair = min(finite_pairs, key=lambda p: p[1])\n                self.best_x = best_pair[0].copy()\n                self.best_f = float(best_pair[1])\n            else:\n                # all infinite/unusable: produce random point if budget still allowed (unlikely)\n                if self.best_x is None and self.eval_count < self.budget:\n                    x = self._uniform_array(lb, ub)\n                    f = safe_eval(x)\n                    if np.isfinite(f):\n                        self.best_x = x.copy()\n                        self.best_f = f\n\n        return float(self.best_f), None if self.best_x is None else self.best_x.copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "793591e2-1024-4ae0-ac1c-30e6ebe18034", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed if seed is not None else np.random.randint(2**31 - 1)\n        self.rng = np.random.default_rng(self.seed)\n\n        # archive\n        self.archive_max = int(archive_max)\n        self.arr_X = np.empty((0, self.dim), dtype=float)\n        self.arr_f = np.empty((0,), dtype=float)\n\n        # adaptation\n        self.gscale = 0.5  # global exploration multiplier\n        self.success_window = []  # recent success (1/0)\n        self.success_window_max = 60\n        self.evals = 0\n\n        # bookkeeping best\n        self.best_x = None\n        self.best_f = np.inf\n        self.last_improve_at = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # try common patterns\n        lb = None; ub = None\n        # func.bounds as tuple (lb, ub)\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # tuple pattern\n            if isinstance(b, tuple) and len(b) == 2:\n                try:\n                    lb = np.array(b[0], dtype=float).flatten()\n                    ub = np.array(b[1], dtype=float).flatten()\n                except Exception:\n                    lb = ub = None\n            else:\n                # object with attributes\n                for low_name, high_name in ((\"lb\", \"ub\"), (\"lower\", \"upper\"), (\"min\", \"max\")):\n                    if hasattr(b, low_name) and hasattr(b, high_name):\n                        try:\n                            lb = np.array(getattr(b, low_name), dtype=float).flatten()\n                            ub = np.array(getattr(b, high_name), dtype=float).flatten()\n                        except Exception:\n                            lb = ub = None\n                        break\n        # direct attributes on func\n        for a_low, a_high in ((\"lb\", \"ub\"), (\"lower\", \"upper\")):\n            if lb is None and hasattr(func, a_low) and hasattr(func, a_high):\n                try:\n                    lb = np.array(getattr(func, a_low), dtype=float).flatten()\n                    ub = np.array(getattr(func, a_high), dtype=float).flatten()\n                except Exception:\n                    lb = ub = None\n                break\n\n        # fallback to scalars or default\n        if lb is None or ub is None:\n            # fallback to default [-5,5]^dim\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast scalars if needed\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb.flatten()[0]), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub.flatten()[0]), dtype=float)\n            # if shapes mismatch or wrong size, fallback to default\n            if lb.size != self.dim or ub.size != self.dim:\n                lb = np.full(self.dim, -5.0, dtype=float)\n                ub = np.full(self.dim, 5.0, dtype=float)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # mirrored reflection for components outside bounds\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            if not low_mask.any():\n                break\n            x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            high_mask = x > ub\n            x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        rng = self.rng\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = rng.uniform(size=(n, dim))\n        a = cut[:n]\n        b = cut[1:n + 1]\n        points = np.zeros((n, dim))\n        for j in range(dim):\n            perm = rng.permutation(n)\n            points[:, j] = u[:, j] * (b - a) + a\n            points[:, j] = np.interp(np.arange(n), np.arange(n), points[perm, j])[np.argsort(perm)]\n        return points\n\n    def _uniform_array(self, lb, ub, n=1):\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            return self.rng.uniform(lb, ub, size=(n, self.dim))\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        # draw standard Cauchy via tan(pi*(u-0.5))\n        u = self.rng.uniform(size=self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # temper heavy tails: soft-sign transform\n        factor = np.tanh(np.abs(c) / 4.0) / (np.abs(c) / 4.0 + 1e-12)\n        tempered = np.sign(c) * (np.abs(c) * factor)\n        # scale and cap\n        scaled = tempered * scale_vec\n        cap = cap_multiplier * scale_vec\n        scaled = np.clip(scaled, -cap, cap)\n        return scaled\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float).flatten()\n            self.evals += 1\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            return float(f) if np.isfinite(f) else np.inf\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(8, 4 * self.dim), self.budget // 10 + 3)\n        n_init = max(1, n_init)\n        # ensure don't exceed budget\n        n_init = min(n_init, self.budget)\n        X0 = self._lhs01(n_init, self.dim) * (ub - lb) + lb\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.arr_X = np.vstack((self.arr_X, x.reshape(1, -1)))\n            self.arr_f = np.concatenate((self.arr_f, [f]))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # add a few pure uniforms for diversity if budget remains\n        for _ in range(min(5, self.budget - self.evals)):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.arr_X = np.vstack((self.arr_X, x.reshape(1, -1)))\n            self.arr_f = np.concatenate((self.arr_f, [f]))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # If still no finite best, try one more random\n        if not np.isfinite(self.best_f) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.arr_X = np.vstack((self.arr_X, x.reshape(1, -1)))\n            self.arr_f = np.concatenate((self.arr_f, [f]))\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x.copy()\n                self.last_improve_at = self.evals\n\n        # Main optimization loop\n        while self.evals < self.budget:\n            n_archive = self.arr_X.shape[0]\n            # Make robust per-dim scales using IQR->std approx, fallback to std or global span\n            if n_archive >= 3:\n                q75 = np.percentile(self.arr_X, 75, axis=0)\n                q25 = np.percentile(self.arr_X, 25, axis=0)\n                iqr = q75 - q25\n                # IQR to std approximate factor\n                scale_est = iqr / 1.349\n                # fallback to std\n                std = np.std(self.arr_X, axis=0, ddof=1)\n                scale_vec = np.where(np.isfinite(scale_est) & (scale_est > 1e-12), scale_est, std)\n                # if still zeros, use global range\n                range_vec = np.maximum(ub - lb, 1e-6)\n                scale_vec = np.where((scale_vec > 1e-12), scale_vec, range_vec * 0.25)\n            else:\n                scale_vec = np.maximum(ub - lb, 1e-6) * 0.25\n\n            # normalize exploration scale by gscale\n            scale_vec = scale_vec * self.gscale\n\n            # choose an anchor: bias to elites if available\n            idxs = np.arange(n_archive)\n            if n_archive > 0:\n                order = np.argsort(self.arr_f)\n                top_frac = max(1, int(0.2 * n_archive))\n                topk = order[:top_frac]\n                if self.rng.random() < 0.75:\n                    anchor_idx = self.rng.choice(topk)\n                else:\n                    anchor_idx = self.rng.choice(idxs)\n                anchor = self.arr_X[anchor_idx].copy()\n            else:\n                anchor = self._uniform_array(lb, ub)\n\n            # propose candidate via mixture\n            r = self.rng.random()\n            x_new = None\n\n            # Strategy 1: DE-style differential injection (~22%)\n            if r < 0.22 and n_archive >= 3:\n                i1, i2, i3 = self.rng.choice(range(n_archive), size=3, replace=False)\n                donor = self.arr_X[i1]\n                base = self.arr_X[i2]\n                diff = self.arr_X[i3] - base\n                F = 0.6 + 0.6 * self.rng.random()  # scaling\n                jitter = self.rng.normal(0, 0.02, size=self.dim) * scale_vec\n                x_new = donor + F * diff + jitter\n                x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation (~23%)\n            elif r < 0.45 and n_archive >= 4:\n                elite_count = max(3, min(n_archive, int(max(3, 0.12 * n_archive))))\n                elites_idx = np.argsort(self.arr_f)[:elite_count]\n                X_el = self.arr_X[elites_idx]\n                # stable PCA using SVD on centered elites\n                mu = X_el.mean(axis=0)\n                Xc = X_el - mu\n                try:\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    # pick subspace dim where singular values significant\n                    sv_norm = S / (S[0] + 1e-12)\n                    subdim = max(1, np.sum(sv_norm > 1e-3))\n                    comps = Vt[:subdim]\n                    # sample step in principal subspace\n                    coeffs = self.rng.normal(0, 1.0, size=subdim) * (S[:subdim] / (elite_count + 1))\n                    pca_step = coeffs @ comps\n                except Exception:\n                    pca_step = self.rng.normal(0, 1.0, size=self.dim) * scale_vec * 0.5\n                # bias to anchor or mean\n                if self.rng.random() < 0.6:\n                    candidate = anchor + pca_step * (0.5 + self.rng.random())\n                else:\n                    candidate = mu + pca_step * (0.5 + self.rng.random())\n                # small gaussian jitter\n                candidate = candidate + self.rng.normal(0, 0.02, size=self.dim) * scale_vec\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 3: Local anisotropic gaussian (~45%)\n            elif r < 0.9:\n                # pick base point (anchor) and perturb subset of coordinates\n                base = anchor.copy()\n                # pick coords to perturb\n                ncoords = max(1, int(self.dim * (0.15 + 0.65 * self.rng.random())))\n                coords = self.rng.choice(self.dim, size=ncoords, replace=False)\n                candidate = base.copy()\n                # anisotropic gaussian per-dim\n                candidate[coords] += self.rng.normal(0, 1.0, size=ncoords) * scale_vec[coords]\n                # occasionally mix in a few uniform coords for exploration\n                if self.rng.random() < 0.08:\n                    mixk = max(1, int(0.03 * self.dim))\n                    mix_coords = self.rng.choice(self.dim, size=mixk, replace=False)\n                    candidate[mix_coords] = self._uniform_array(lb[mix_coords], ub[mix_coords])\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # Strategy 4: tempered Cauchy global escape (~10%)\n            else:\n                tc = self._tempered_cauchy(scale_vec)\n                # jump from anchor or random, sometimes from best\n                base = anchor if self.rng.random() < 0.6 else (self.best_x if self.best_x is not None and self.rng.random() < 0.6 else self._uniform_array(lb, ub))\n                candidate = base + tc\n                x_new = self._reflect_bounds(candidate, lb, ub)\n\n            # sometimes add a small differential jump for added diversity\n            if n_archive >= 2 and self.rng.random() < 0.12:\n                i1, i2 = self.rng.choice(range(n_archive), size=2, replace=False)\n                x_new = x_new + 0.06 * (self.arr_X[i1] - self.arr_X[i2])\n                x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # occasional direct small jump to current best to refine\n            if self.best_x is not None and self.rng.random() < 0.06:\n                x_new = 0.5 * x_new + 0.5 * self.best_x\n                x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad = (~np.isfinite(x_new))\n            if np.any(bad):\n                x_new[bad] = self._uniform_array(lb[bad], ub[bad])\n\n            # final reflect & clamp\n            x_new = self._reflect_bounds(x_new, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(x_new)\n\n            # record into archive\n            self.arr_X = np.vstack((self.arr_X, x_new.reshape(1, -1)))\n            self.arr_f = np.concatenate((self.arr_f, [f]))\n\n            improved = False\n            if np.isfinite(f) and f < self.best_f:\n                self.best_f = f\n                self.best_x = x_new.copy()\n                self.last_improve_at = self.evals\n                improved = True\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            finite_mask = np.isfinite(self.arr_f)\n            finite_fs = self.arr_f[finite_mask]\n            if finite_fs.size > 0 and np.isfinite(f):\n                cutoff = np.percentile(finite_fs, 25)\n                self.success_window.append(1 if f <= cutoff else 0)\n            else:\n                self.success_window.append(0)\n            # cap success window length\n            if len(self.success_window) > self.success_window_max:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every now and then\n            if len(self.success_window) >= 12 and (self.evals % 12 == 0):\n                succ_rate = np.mean(self.success_window)\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    self.gscale *= 0.92\n                elif succ_rate < 0.12:\n                    self.gscale *= 1.12\n                    # mild jitter to escape stagnation\n                    if self.rng.random() < 0.12:\n                        self.gscale *= (1.0 + 0.3 * (self.rng.random() - 0.5))\n                # clamp gscale to reasonable bounds\n                self.gscale = float(np.clip(self.gscale, 1e-4, 3.0))\n\n            # prune archive if too large\n            if self.arr_X.shape[0] > self.archive_max:\n                finite_idxs = np.where(np.isfinite(self.arr_f))[0]\n                # keep best ones and some randoms\n                kbest = max(6, int(0.2 * self.archive_max))\n                if finite_idxs.size > 0:\n                    best_idxs = finite_idxs[np.argsort(self.arr_f[finite_idxs])[:kbest]]\n                else:\n                    best_idxs = np.array([], dtype=int)\n                # keep current best if present (closest in archive)\n                keep = set(best_idxs.tolist())\n                # find index closest to current best_x\n                if self.best_x is not None and self.arr_X.shape[0] > 0:\n                    dists = np.linalg.norm(self.arr_X - self.best_x.reshape(1, -1), axis=1)\n                    keep.add(int(np.argmin(dists)))\n                # add some random survivors\n                all_idxs = set(range(self.arr_X.shape[0]))\n                remaining = list(all_idxs - keep)\n                nrand_keep = max(0, self.archive_max - len(keep))\n                if nrand_keep > 0 and len(remaining) > 0:\n                    chosen = list(self.rng.choice(remaining, size=min(len(remaining), nrand_keep), replace=False))\n                    keep.update(chosen)\n                keep = sorted(list(keep))\n                self.arr_X = self.arr_X[keep]\n                self.arr_f = self.arr_f[keep]\n\n            # stagnation detection & micro-restarts\n            stagnation_len = self.evals - self.last_improve_at\n            if stagnation_len > max(60, self.dim * 15):\n                # micro-restart: generate a small local cloud around best and some randoms\n                n_local = min(20, max(6, int(3 * self.dim)))\n                cloud = []\n                if self.best_x is None:\n                    # randomize if no best\n                    cloud = [self._uniform_array(lb, ub) for _ in range(n_local)]\n                else:\n                    # local perturbations around best with decreasing scales\n                    for k in range(n_local):\n                        scale_fac = 0.4 * (1.0 + 0.7 * (k / (n_local + 1)))\n                        perturb = self.rng.normal(0, 1.0, size=self.dim) * (0.5 * scale_vec * scale_fac)\n                        p = self._reflect_bounds(self.best_x + perturb, lb, ub)\n                        cloud.append(p)\n                    # some global randoms\n                    for _ in range(min(8, self.budget - self.evals)):\n                        cloud.append(self._uniform_array(lb, ub))\n                # evaluate cloud until budget or finished\n                for x in cloud:\n                    if self.evals >= self.budget:\n                        break\n                    f = safe_eval(x)\n                    self.arr_X = np.vstack((self.arr_X, x.reshape(1, -1)))\n                    self.arr_f = np.concatenate((self.arr_f, [f]))\n                    if np.isfinite(f) and f < self.best_f:\n                        self.best_f = f\n                        self.best_x = x.copy()\n                        self.last_improve_at = self.evals\n                # after restart, slightly increase exploration scales\n                self.gscale = min(2.0, self.gscale * 1.08)\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.last_improve_at = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        finite_idxs = np.where(np.isfinite(self.arr_f))[0]\n        if finite_idxs.size > 0:\n            best_idx = finite_idxs[np.argmin(self.arr_f[finite_idxs])]\n            best_pair_x = self.arr_X[best_idx]\n            best_pair_f = float(self.arr_f[best_idx])\n            return best_pair_f, best_pair_x\n        else:\n            # fallback produce a random point (unlikely)\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a30c608f-6e09-4e05-a23a-43db69778992", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        if self.seed is not None:\n            np.random.seed(self.seed)\n        self.archive_max = int(archive_max)\n\n        # bookkeeping\n        self.evals = 0\n        self.archive_x = []  # list of arrays\n        self.archive_f = []  # list of floats\n\n        # adaptation windows & counters\n        self.gscale = 0.2  # global exploration multiplier (adapted)\n        self.success_window = []\n        self.success_win_len = 60  # track recent successes\n        self.stagnation_counter = 0\n        self.last_prune = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # Try func.bounds as tuple (lb, ub)\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            try:\n                if isinstance(b, tuple) and len(b) == 2:\n                    lb, ub = b\n                else:\n                    # Try object with lb/ub or lower/upper attributes\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n            except Exception:\n                lb = ub = None\n\n        # also try attributes directly on func\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # Normalize to numpy arrays; fallback to default\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.array(lb, dtype=float)\n            ub = np.array(ub, dtype=float)\n            # scalars broadcast\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n\n        # if shapes mismatch or wrong size, fall back\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that are out of bounds (mirror back)\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            if low_mask.any():\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if high_mask.any():\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        cut = np.linspace(0, 1, n + 1)\n        u = np.random.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        rdpoints = u * (b - a)[:, None] + a[:, None]\n        # shuffle dims\n        for j in range(dim):\n            np.random.shuffle(rdpoints[:, j])\n        return rdpoints\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        # generate standard cauchy and temper via tanh to avoid extreme blowups\n        # scale per-dim and cap\n        scale_vec = np.maximum(scale_vec, 1e-12)\n        c = np.random.standard_cauchy(size=self.dim)\n        # occasionally allow heavy tails: multiply some coords by random factor\n        heavy_mask = (np.random.rand(self.dim) < 0.05)\n        if heavy_mask.any():\n            c[heavy_mask] *= (1 + np.random.rand(heavy_mask.sum()) * 10.0)\n        step = scale_vec * c\n        # temper extreme values\n        capped = np.tanh(step / (scale_vec * cap_multiplier + 1e-12)) * (scale_vec * cap_multiplier)\n        return capped\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        self.evals = 0\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.stagnation_counter = 0\n        self.gscale = max(1e-6, self.gscale)\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            self.evals += 1\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            return float(f)\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 8))\n        n_init = max(2, n_init)\n        n_init = min(n_init, self.budget)\n        X0 = self._lhs01(n_init, self.dim) * (ub - lb) + lb\n        for i in range(X0.shape[0]):\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        add_uni = min(6, max(0, self.budget - self.evals))\n        if add_uni > 0:\n            U = self._uniform_array(lb, ub, n=add_uni)\n            for i in range(U.shape[0]):\n                f = safe_eval(U[i])\n                self.archive_x.append(U[i].copy())\n                self.archive_f.append(f)\n\n        # if still no finite best, sample one more random if budget allows\n        finite_mask = [np.isfinite(v) for v in self.archive_f]\n        if not any(finite_mask) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Main optimization loop\n        # mixing probabilities (normalized)\n        probs = np.array([0.22, 0.23, 0.45, 0.10])\n        probs = probs / probs.sum()\n        # small differential occasional\n        diff_chance = 0.12\n\n        # main iterations until budget exhausted\n        while self.evals < self.budget:\n            # ensure archive arrays\n            X = np.vstack(self.archive_x)\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            if finite_idx.any():\n                # robust per-dim scale estimate using IQR->std approximation and fallback to std\n                try:\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    iqr = np.maximum(q75 - q25, 1e-12)\n                    scale_est = iqr / 1.349  # approximate std\n                    # fallback to std where iqr zero\n                    zero_mask = (scale_est <= 1e-12)\n                    if zero_mask.any():\n                        sd = np.std(X[finite_idx], axis=0)\n                        scale_est[zero_mask] = np.maximum(sd[zero_mask], 1e-6)\n                except Exception:\n                    scale_est = np.maximum(np.std(X[finite_idx], axis=0), 1e-6)\n            else:\n                scale_est = np.full(self.dim, (ub - lb) / 6.0)\n\n            # choose anchor: bias to elites if available\n            best_indices = np.argsort(F)\n            top_frac = max(1, int(0.15 * len(F)))\n            if len(F) >= 3:\n                if np.random.rand() < 0.75:\n                    anchor_idx = np.random.choice(best_indices[:top_frac])\n                else:\n                    anchor_idx = np.random.randint(len(F))\n            else:\n                anchor_idx = np.random.randint(len(F))\n            anchor = X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = np.random.rand()\n            if r < probs[0]:\n                # Strategy 1: DE-style differential injection (~22%)\n                # pick two distinct others\n                if len(F) >= 3:\n                    ids = list(range(len(F)))\n                    ids.remove(anchor_idx)\n                    a, b = np.random.choice(ids, size=2, replace=False)\n                    diff = X[a] - X[b]\n                    # anisotropic gaussian jitter + scaled differential\n                    jitter = np.random.randn(self.dim) * (scale_est * self.gscale * 0.4)\n                    gamma = 0.6 + 0.6 * np.random.rand()  # between 0.6 and 1.2\n                    cand = anchor + gamma * diff * (self.gscale) + jitter\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n            elif r < probs[0] + probs[1]:\n                # Strategy 2: PCA-guided elite perturbation (~23%)\n                # pick elites\n                n_elite = max(2, min(10, int(0.12 * len(F) + 2)))\n                if len(F) >= n_elite and finite_idx.sum() >= 2:\n                    elite_idx = best_indices[:n_elite]\n                    eliteX = X[elite_idx]\n                    # stable eigen-decomposition\n                    cov = np.cov(eliteX.T)\n                    try:\n                        vals, vecs = np.linalg.eigh(cov)\n                        vals = np.maximum(vals, 1e-12)\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                    except Exception:\n                        vals = np.ones(self.dim)\n                        vecs = np.eye(self.dim)\n                    # choose subspace dimension\n                    k = max(1, min(self.dim, int(1 + np.sum(np.cumsum(vals) / vals.sum() < 0.85))))\n                    coeffs = np.random.randn(k) * np.sqrt(vals[:k]) * (self.gscale * 0.9)\n                    perturb = vecs[:, :k].dot(coeffs)\n                    # bias sometimes toward anchor or mean of elites\n                    if np.random.rand() < 0.5:\n                        base = anchor\n                    else:\n                        base = eliteX.mean(axis=0)\n                    cand = base + perturb\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n            elif r < probs[0] + probs[1] + probs[2]:\n                # Strategy 3: Local anisotropic gaussian (~45%)\n                # ensure at least one coord\n                mask = (np.random.rand(self.dim) < (0.25 + 0.5 * self.gscale))\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise = np.random.randn(self.dim) * (scale_est * (0.5 + np.random.rand() * 0.5) * self.gscale)\n                # occasionally mix in a few uniform coords for diversity\n                if np.random.rand() < 0.12:\n                    uni_coords = np.random.rand(self.dim) < 0.08\n                    if uni_coords.any():\n                        noise[uni_coords] = (np.random.rand(uni_coords.sum()) - 0.5) * (ub[uni_coords] - lb[uni_coords])\n                cand = anchor + noise * mask.astype(float)\n            else:\n                # Strategy 4: tempered Cauchy global escape (~10%)\n                cap = 8.0\n                cand = anchor + self._tempered_cauchy(scale_est * (self.gscale * 2.2 + 1e-12), cap_multiplier=cap)\n\n            # sometimes add a small differential jump for added diversity\n            if np.random.rand() < diff_chance and len(F) >= 3:\n                ids = np.random.choice(len(F), size=2, replace=False)\n                cand += 0.08 * self.gscale * (X[ids[0]] - X[ids[1]])\n\n            # occasional direct small jump to current best to refine\n            if np.random.rand() < 0.06 and finite_idx.any():\n                best_idx = np.argmin(F[finite_idx])\n                # find global index of best among finite ones\n                best_indices_finite = np.where(finite_idx)[0]\n                global_best_idx = best_indices_finite[best_idx]\n                cand = X[global_best_idx] + np.random.randn(self.dim) * (scale_est * 0.06 * (1 + self.gscale))\n\n            # ensure candidate exists\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad = (~np.isfinite(cand)) | (np.isnan(cand))\n            if bad.any():\n                cand[bad] = self._uniform_array(lb[bad], ub[bad])\n\n            # reflect to bounds & clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            if finite_idx.any():\n                rank = (F[finite_idx] <= f_c).sum()  # how many finite are <= f_c\n                prop = rank / finite_idx.sum()\n                success = (prop <= 0.25)\n            else:\n                success = True\n            self.success_window.append(1 if success else 0)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every now and then\n            if len(self.success_window) >= 8 and (len(self.success_window) % 6 == 0):\n                succ_rate = np.mean(self.success_window)\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    self.gscale *= 0.92 * (0.95 + 0.1 * np.random.rand())\n                elif succ_rate < 0.06:\n                    self.gscale *= 1.14 * (0.95 + 0.1 * np.random.rand())\n                else:\n                    # mild jitter to escape stagnation\n                    self.gscale *= (0.98 + 0.06 * np.random.rand())\n                # clamp\n                self.gscale = np.minimum(np.maximum(self.gscale, 1e-5), 5.0)\n\n            # prune archive if too large\n            if len(self.archive_f) > self.archive_max and (len(self.archive_f) - self.last_prune) > (self.archive_max // 6):\n                self.last_prune = len(self.archive_f)\n                F = np.array(self.archive_f)\n                finite_idx = np.isfinite(F)\n                if finite_idx.any():\n                    best_k = max(8, int(0.2 * self.archive_max))\n                    idx_sorted = np.argsort(F)\n                    keep_best = list(idx_sorted[:best_k])\n                else:\n                    keep_best = []\n                # keep current best if present\n                if len(F) > 0:\n                    best_global = int(np.nanargmin(F))\n                    if best_global not in keep_best:\n                        keep_best.append(best_global)\n                # keep some random ones for diversity\n                remaining = list(set(range(len(F))) - set(keep_best))\n                nrand = max(0, self.archive_max - len(keep_best))\n                if remaining and nrand > 0:\n                    keep_rand = list(np.random.choice(remaining, size=min(nrand, len(remaining)), replace=False))\n                else:\n                    keep_rand = []\n                keep_idx = np.array(keep_best + keep_rand, dtype=int)\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if len(self.success_window) == self.success_win_len:\n                recent = np.mean(self.success_window)\n                if recent < 0.06:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n            # micro restart when stagnated\n            if self.stagnation_counter > 6 and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                F = np.array(self.archive_f)\n                finite_idx = np.isfinite(F)\n                if finite_idx.any():\n                    best_idx = int(np.argmin(F))\n                    best_x = self.archive_x[best_idx].copy()\n                else:\n                    best_x = self._uniform_array(lb, ub)\n                # create local cloud\n                n_cloud = min(12, max(4, (self.budget - self.evals) // 20))\n                for i in range(n_cloud):\n                    sigma = (0.03 + 0.06 * np.random.rand()) * (1 + self.gscale)\n                    cand = best_x + np.random.randn(self.dim) * (scale_est * sigma)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    if self.evals < self.budget:\n                        f_c = safe_eval(cand)\n                        self.archive_x.append(cand.copy())\n                        self.archive_f.append(f_c)\n                # some global randoms to regain diversity\n                n_global = min(6, max(2, (self.budget - self.evals) // 40))\n                for i in range(n_global):\n                    if self.evals < self.budget:\n                        cand = self._uniform_array(lb, ub)\n                        f_c = safe_eval(cand)\n                        self.archive_x.append(cand.copy())\n                        self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales\n                self.gscale = min(3.0, self.gscale * (1.2 + 0.2 * np.random.rand()))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            # no evaluations? fallback to uniform random\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x\n        F = np.array(self.archive_f)\n        finite_idx = np.isfinite(F)\n        if finite_idx.any():\n            best_idx = int(np.argmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a1960ed4-206f-43b6-9ade-54c483e9a064", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS)", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        if self.seed is not None:\n            np.random.seed(self.seed)\n        self.archive_max = int(archive_max)\n\n        # bookkeeping\n        self.evals = 0\n        self.archive_x = []\n        self.archive_f = []\n\n        # adaptation windows & counters\n        self.gscale = 0.2  # global exploration multiplier (adapted)\n        self.success_window = []\n        self.success_win_len = 60  # track recent successes\n        self.stagnation_counter = 0\n        self.last_prune = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # if bounds is a tuple/list (lb, ub)\n            if isinstance(b, (list, tuple)) and len(b) == 2:\n                lb, ub = b\n            else:\n                # try object with lb/ub or lower/upper/min/max\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n\n        # also try attributes directly on func\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # Normalize to numpy arrays; fallback to default\n        try:\n            if lb is None or ub is None:\n                raise ValueError\n            lb_arr = np.array(lb, dtype=float)\n            ub_arr = np.array(ub, dtype=float)\n            # handle scalars\n            if lb_arr.shape == ():\n                lb_arr = np.full(self.dim, float(lb_arr))\n            if ub_arr.shape == ():\n                ub_arr = np.full(self.dim, float(ub_arr))\n            # ensure shapes match dim\n            if lb_arr.size != self.dim or ub_arr.size != self.dim:\n                raise ValueError\n            return lb_arr, ub_arr\n        except Exception:\n            return np.full(self.dim, -5.0), np.full(self.dim, 5.0)\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that are out of bounds (mirror back)\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            if low_mask.any():\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if high_mask.any():\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        cut = np.linspace(0, 1, n + 1)\n        u = np.random.rand(n, dim)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        rdpoints = u * (b - a)[:, None] + a[:, None]\n        # shuffle dims\n        for j in range(dim):\n            np.random.shuffle(rdpoints[:, j])\n        return rdpoints\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        scale_vec = np.maximum(scale_vec, 1e-12)\n        c = np.random.standard_cauchy(size=self.dim)\n        heavy_mask = (np.random.rand(self.dim) < 0.05)\n        if heavy_mask.any():\n            c[heavy_mask] *= (1 + np.random.rand(heavy_mask.sum()) * 10.0)\n        step = scale_vec * c\n        capped = np.tanh(step / (scale_vec * cap_multiplier + 1e-12)) * (scale_vec * cap_multiplier)\n        return capped\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset bookkeeping for each call\n        self.evals = 0\n        self.archive_x = []\n        self.archive_f = []\n        self.stagnation_counter = 0\n        self.gscale = max(1e-6, self.gscale)\n        self.success_window = []\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            # ensure x is numpy array of right shape\n            x = np.array(x, dtype=float).reshape(self.dim)\n            self.evals += 1\n            try:\n                f = func(x)\n                # convert to float if possible\n                f = float(f)\n            except Exception:\n                f = np.inf\n            return f\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 8))\n        n_init = max(2, n_init)\n        n_init = min(n_init, max(1, self.budget))\n        X0 = self._lhs01(n_init, self.dim) * (ub - lb) + lb\n        for i in range(X0.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        add_uni = min(6, max(0, self.budget - self.evals))\n        if add_uni > 0:\n            U = self._uniform_array(lb, ub, n=add_uni)\n            for i in range(U.shape[0]):\n                if self.evals >= self.budget:\n                    break\n                f = safe_eval(U[i])\n                self.archive_x.append(U[i].copy())\n                self.archive_f.append(f)\n\n        # if still no finite best, sample one more random if budget allows\n        finite_mask = [np.isfinite(v) for v in self.archive_f]\n        if not any(finite_mask) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Main optimization loop\n        # mixing probabilities (normalized)\n        probs = np.array([0.22, 0.23, 0.45, 0.10])\n        # small differential occasional\n        diff_chance = 0.12\n        top_frac = max(1, int(0.15 * max(3, len(self.archive_f))))  # used later, but updated each iter\n        n_elite = max(2, int(0.12 * max(3, len(self.archive_f))))\n\n        # iterate until budget exhausted\n        while self.evals < self.budget:\n            # ensure archive arrays consistent\n            if len(self.archive_x) == 0:\n                # pick a random point if nothing present\n                x0 = self._uniform_array(lb, ub, n=1)\n                f0 = safe_eval(x0)\n                self.archive_x.append(x0.copy())\n                self.archive_f.append(f0)\n                if self.evals >= self.budget:\n                    break\n\n            X = np.vstack(self.archive_x)\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            if finite_idx.any():\n                try:\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    iqr = np.maximum(q75 - q25, 1e-12)\n                    scale_est = iqr / 1.349  # approximate std\n                    zero_mask = (scale_est <= 1e-12)\n                    if zero_mask.any():\n                        sd = np.std(X[finite_idx], axis=0)\n                        scale_est[zero_mask] = np.maximum(sd[zero_mask], 1e-6)\n                    # final clamp\n                    scale_est = np.maximum(scale_est, 1e-6)\n                except Exception:\n                    scale_est = np.maximum(np.std(X[finite_idx], axis=0), 1e-6)\n            else:\n                scale_est = np.full(self.dim, (ub - lb) / 6.0)\n\n            # choose anchor: bias to elites if available\n            idx_sorted = np.argsort(F)\n            top_frac = max(1, int(0.2 * max(3, len(F))))\n            n_elite = max(2, int(0.12 * max(3, len(F))))\n            if len(F) >= 3:\n                if np.random.rand() < 0.75 and np.isfinite(F).any():\n                    # pick one of top fraction\n                    top_k = max(1, top_frac)\n                    top_k = min(top_k, len(F))\n                    anchor_idx = np.random.choice(idx_sorted[:top_k])\n                else:\n                    anchor_idx = np.random.randint(len(F))\n            else:\n                anchor_idx = np.random.randint(len(F))\n            anchor = X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = np.random.rand()\n            cand = None\n\n            if r < probs[0]:\n                # Strategy 1: DE-style differential injection (~22%)\n                if len(F) >= 3:\n                    ids = list(range(len(F)))\n                    if anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                    else:\n                        a = b = ids[0]\n                    diff = X[a] - X[b]\n                    # anisotropic gaussian jitter + scaled differential\n                    jitter = np.random.randn(self.dim) * (scale_est * self.gscale * 0.4)\n                    gamma = 0.6 + 0.6 * np.random.rand()  # between 0.6 and 1.2\n                    cand = anchor + gamma * diff * (self.gscale) + jitter\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n            elif r < probs[0] + probs[1]:\n                # Strategy 2: PCA-guided elite perturbation (~23%)\n                if len(F) >= n_elite and finite_idx.sum() >= 2:\n                    elite_idx = idx_sorted[:n_elite]\n                    eliteX = X[elite_idx]\n                    cov = np.cov(eliteX.T)\n                    try:\n                        vals, vecs = np.linalg.eigh(cov)\n                        vals = np.maximum(vals, 1e-12)\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                    except Exception:\n                        vals = np.ones(self.dim)\n                        vecs = np.eye(self.dim)\n                    # choose subspace dimension: keep components that explain up to ~85% or at least 1\n                    cum = np.cumsum(vals)\n                    total = cum[-1] if cum[-1] > 0 else 1.0\n                    k = max(1, int(np.searchsorted(cum / total, 0.85) + 1))\n                    k = min(k, self.dim)\n                    coeffs = np.random.randn(k) * np.sqrt(vals[:k]) * (self.gscale * 0.9)\n                    perturb = vecs[:, :k].dot(coeffs)\n                    # bias sometimes toward anchor or mean of elites\n                    if np.random.rand() < 0.5:\n                        base = anchor\n                    else:\n                        base = eliteX.mean(axis=0)\n                    cand = base + perturb\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n            elif r < probs[0] + probs[1] + probs[2]:\n                # Strategy 3: Local anisotropic gaussian (~45%)\n                mask = (np.random.rand(self.dim) < (0.25 + 0.5 * min(1.0, self.gscale)))\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise = np.random.randn(self.dim) * (scale_est * (0.5 + np.random.rand() * 0.5) * self.gscale)\n                # occasionally mix in a few uniform coords for diversity\n                if np.random.rand() < 0.12:\n                    uni_coords = np.random.rand(self.dim) < 0.08\n                    if uni_coords.any():\n                        noise[uni_coords] = (np.random.rand(uni_coords.sum()) - 0.5) * (ub[uni_coords] - lb[uni_coords])\n                cand = anchor + noise * mask.astype(float)\n            else:\n                # Strategy 4: tempered Cauchy global escape (~10%)\n                cap = 8.0\n                base = anchor if np.random.rand() < 0.5 else (np.mean(X[finite_idx], axis=0) if finite_idx.any() else anchor)\n                cand = base + self._tempered_cauchy(scale_est * (1.2 + 0.6 * self.gscale), cap_multiplier=cap)\n\n            # sometimes add a small differential jump for added diversity\n            if np.random.rand() < diff_chance and len(F) >= 3:\n                ids = np.random.choice(len(F), size=2, replace=False)\n                cand = cand + 0.08 * self.gscale * (X[ids[0]] - X[ids[1]])\n\n            # occasional direct small jump to current best to refine\n            if np.random.rand() < 0.06 and finite_idx.any():\n                best_idx = int(np.nanargmin(F))\n                cand = X[best_idx] + np.random.randn(self.dim) * (scale_est * 0.06 * (1 + self.gscale))\n\n            # ensure candidate exists\n            if cand is None:\n                cand = self._uniform_array(lb, ub, n=1)\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            bad = ~(np.isfinite(cand))\n            if bad.any():\n                # fill individually\n                cand = np.array(cand, dtype=float).reshape(self.dim)\n                for j in np.where(bad)[0]:\n                    cand[j] = lb[j] + np.random.rand() * (ub[j] - lb[j])\n\n            # reflect to bounds & clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            if finite_idx.any():\n                # rank among finite values (how many are <= f_c)\n                rank = (F[finite_idx] <= f_c).sum()\n                prop = rank / finite_idx.sum()\n                success = (prop <= 0.25)\n            else:\n                success = False\n            self.success_window.append(1 if success else 0)\n            if len(self.success_window) > self.success_win_len:\n                # keep recent window only\n                self.success_window = self.success_window[-self.success_win_len:]\n\n            # adaptation of gscale every time we have enough window\n            if len(self.success_window) == self.success_win_len:\n                succ_rate = np.mean(self.success_window)\n                if succ_rate > 0.35:\n                    self.gscale *= 0.92 * (0.95 + 0.1 * np.random.rand())\n                elif succ_rate < 0.06:\n                    self.gscale *= 1.14 * (0.95 + 0.1 * np.random.rand())\n                else:\n                    # mild jitter to escape local traps\n                    self.gscale *= (0.98 + 0.06 * np.random.rand())\n                # clamp\n                self.gscale = np.minimum(np.maximum(self.gscale, 1e-5), 5.0)\n\n            # prune archive if too large (occasionally)\n            if len(self.archive_f) > self.archive_max:\n                F = np.array(self.archive_f)\n                finite_idx = np.isfinite(F)\n                if finite_idx.any():\n                    best_k = max(8, int(0.2 * self.archive_max))\n                    idx_sorted = np.argsort(F)\n                    keep_best = list(idx_sorted[:best_k])\n                else:\n                    keep_best = []\n                # keep current best if present\n                if len(F) > 0:\n                    try:\n                        best_global = int(np.nanargmin(F))\n                        if best_global not in keep_best:\n                            keep_best.append(best_global)\n                    except Exception:\n                        pass\n                # keep some random ones for diversity\n                remaining = list(set(range(len(F))) - set(keep_best))\n                nrand = max(0, self.archive_max - len(keep_best))\n                if remaining and nrand > 0:\n                    keep_rand = list(np.random.choice(remaining, size=min(nrand, len(remaining)), replace=False))\n                else:\n                    keep_rand = []\n                keep_idx = np.array(keep_best + keep_rand, dtype=int)\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n                self.last_prune = len(self.archive_f)\n\n            # stagnation detection\n            if len(self.success_window) == self.success_win_len:\n                recent = np.mean(self.success_window)\n                if recent < 0.06:\n                    self.stagnation_counter += 1\n                else:\n                    # occasional decay of stagnation counter if improving\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n\n            # micro restart when stagnated\n            if self.stagnation_counter > 6 and self.evals < self.budget:\n                # micro-restart: generate a small local cloud around best and some randoms\n                F = np.array(self.archive_f)\n                finite_idx = np.isfinite(F)\n                if finite_idx.any():\n                    try:\n                        best_idx = int(np.argmin(F))\n                        best_x = self.archive_x[best_idx].copy()\n                    except Exception:\n                        best_x = self._uniform_array(lb, ub)\n                else:\n                    best_x = self._uniform_array(lb, ub)\n                # create local cloud\n                remaining_budget = max(0, self.budget - self.evals)\n                n_cloud = min(12, max(4, remaining_budget // 20)) if remaining_budget > 0 else 0\n                for i in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    sigma = (0.03 + 0.06 * np.random.rand()) * (1 + self.gscale)\n                    cand = best_x + np.random.randn(self.dim) * (scale_est * sigma)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # some global randoms to regain diversity\n                remaining_budget = max(0, self.budget - self.evals)\n                n_global = min(6, max(2, remaining_budget // 40)) if remaining_budget > 0 else 0\n                for i in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales\n                self.gscale = min(3.0, self.gscale * (1.2 + 0.2 * np.random.rand()))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        F = np.array(self.archive_f)\n        finite_idx = np.isfinite(F)\n        if finite_idx.any():\n            best_idx = int(np.argmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                return f, x\n            # fallback\n            return float(np.inf), self._uniform_array(lb, ub)", "configspace": "", "generation": 0, "feedback": "In the code, line 48, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "error": "In the code, line 48, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3f81b66f-af9c-491c-8a10-08110d19d51e", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage: instantiate with budget and dim, then call with a black-box func(x) that accepts a 1D numpy array.\n    The optimizer will attempt to read bounds from func (func.bounds.lb/ub or func.bounds tuple) but otherwise\n    uses [-5,5]^dim as default.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        if self.seed is not None:\n            np.random.seed(self.seed)\n        self.archive_max = int(archive_max)\n\n        # bookkeeping\n        self.evals = 0\n        self.archive_x = []\n        self.archive_f = []\n\n        # adaptation windows & counters\n        self.gscale = 0.2  # global exploration multiplier (adapted)\n        self.success_window = []\n        self.success_win_len = 60  # track recent successes\n        self.stagnation_counter = 0\n        self.last_prune = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # try attribute func.bounds\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            if isinstance(b, tuple) and len(b) == 2:\n                lb, ub = b\n            else:\n                # object with lb/ub-like attributes\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n        # also try func.lb / func.ub directly\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None) or None\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None) or None\n\n        # Normalize to numpy arrays; fallback to default\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.array(lb, dtype=float)\n            ub = np.array(ub, dtype=float)\n            # scalars broadcast\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n        # final sanity\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that are out of bounds (mirror back)\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            if low_mask.any():\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if high_mask.any():\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        # For each dimension, create n stratified samples and permute\n        rd = np.empty((n, dim))\n        cut = np.linspace(0, 1, n + 1)\n        for j in range(dim):\n            a = cut[:n]\n            b = cut[1:n+1]\n            rd[:, j] = (a + np.random.rand(n) * (b - a))\n            np.random.shuffle(rd[:, j])\n        return rd\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        scale_vec = np.maximum(scale_vec, 1e-12)\n        c = np.random.standard_cauchy(size=self.dim)\n        # occasionally allow heavier tails: multiply some coords by random factor\n        heavy_mask = (np.random.rand(self.dim) < 0.05)\n        if heavy_mask.any():\n            c[heavy_mask] *= (1.0 + np.random.rand(heavy_mask.sum()) * 10.0)\n        # temper extreme values with tanh to cap influence\n        # scale cap_multiplier by median of scale_vec to keep units reasonable\n        # avoid dividing by zero\n        denom = max(1.0, float(np.median(scale_vec)) * 0.0 + 1.0)\n        tempered = np.tanh(c / cap_multiplier) * cap_multiplier\n        step = scale_vec * tempered\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset archives\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.gscale = max(1e-6, self.gscale)\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(np.asarray(x, dtype=float))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            try:\n                return float(f)\n            except Exception:\n                return float(np.inf)\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 8))\n        n_init = max(2, n_init)\n        n_init = min(n_init, self.budget)\n        X0 = self._lhs01(n_init, self.dim) * (ub - lb) + lb\n        for i in range(X0.shape[0]):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        add_uni = min(6, max(0, self.budget - self.evals))\n        if add_uni > 0:\n            U = self._uniform_array(lb, ub, n=add_uni)\n            for i in range(U.shape[0]):\n                if self.evals >= self.budget:\n                    break\n                f = safe_eval(U[i])\n                self.archive_x.append(U[i].copy())\n                self.archive_f.append(f)\n\n        # if still no finite best, sample one more random if budget allows\n        F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n        finite_mask = np.isfinite(F)\n        if (not finite_mask.any()) if finite_mask.size > 0 else True:\n            if self.evals < self.budget:\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.archive_x.append(x.copy())\n                self.archive_f.append(f)\n                F = np.array(self.archive_f)\n                finite_mask = np.isfinite(F)\n\n        # Main optimization loop\n        # mixing probabilities (normalized)\n        probs = np.array([0.22, 0.23, 0.45, 0.10])\n        probs = probs / probs.sum()\n        # small differential occasional\n        diff_chance = 0.12\n\n        # main iterations until budget exhausted\n        while self.evals < self.budget:\n            # ensure archive arrays\n            if len(self.archive_x) == 0:\n                # sample random if archive empty\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.archive_x.append(x.copy())\n                self.archive_f.append(f)\n            X = np.vstack(self.archive_x)\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            if finite_idx.any():\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    iqr = np.maximum(q75 - q25, 1e-12)\n                    scale_est = iqr / 1.349  # approximate std from IQR\n                    # fallback to std where iqr zero\n                    zero_mask = (scale_est <= 1e-12)\n                    if zero_mask.any():\n                        sd = np.std(X[finite_idx], axis=0)\n                        scale_est[zero_mask] = np.maximum(sd[zero_mask], 1e-6)\n                except Exception:\n                    scale_est = np.maximum(np.std(X[finite_idx], axis=0), 1e-6)\n            else:\n                # no finite history, use broad scales\n                scale_est = np.full(self.dim, 1.0)\n\n            # choose anchor: bias to elites if available\n            if len(F) == 0:\n                anchor_idx = 0\n            else:\n                best_indices = np.argsort(F)\n                if len(F) >= 3:\n                    top_frac = max(1, int(0.15 * len(F)))\n                    if np.random.rand() < 0.75:\n                        anchor_idx = np.random.choice(best_indices[:top_frac])\n                    else:\n                        anchor_idx = np.random.randint(len(F))\n                else:\n                    anchor_idx = np.random.randint(len(F))\n            anchor = X[anchor_idx].copy()\n\n            # propose a candidate via a mixture of strategies\n            r = np.random.rand()\n            cand = None\n            if r < probs[0]:\n                # Strategy 1: DE-style differential injection (~22%)\n                if len(F) >= 3:\n                    ids = list(range(len(F)))\n                    ids.remove(anchor_idx)\n                    a, b = np.random.choice(ids, size=2, replace=False)\n                    diff = X[a] - X[b]\n                    jitter = np.random.randn(self.dim) * (scale_est * self.gscale * 0.4)\n                    gamma = 0.6 + 0.6 * np.random.rand()  # between 0.6 and 1.2\n                    cand = anchor + gamma * diff * (self.gscale) + jitter\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n            elif r < probs[0] + probs[1]:\n                # Strategy 2: PCA-guided elite perturbation (~23%)\n                if len(F) >= 3:\n                    n_elite = max(2, min(10, int(0.12 * len(F) + 2)))\n                    elite_idx = best_indices[:n_elite]\n                    eliteX = X[elite_idx]\n                    cov = np.cov(eliteX.T)\n                    try:\n                        vals, vecs = np.linalg.eigh(cov)\n                        vals = np.maximum(vals, 1e-12)\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                    except Exception:\n                        vals = np.ones(self.dim)\n                        vecs = np.eye(self.dim)\n                    # choose subspace dimension capturing ~85% variance or at least 1\n                    cumsum = np.cumsum(vals)\n                    total = cumsum[-1] if cumsum[-1] > 0 else 1.0\n                    k = max(1, min(self.dim, int(1 + np.searchsorted(cumsum / total, 0.85))))\n                    coeffs = np.random.randn(k) * np.sqrt(vals[:k]) * (self.gscale * 0.9)\n                    if np.random.rand() < 0.5:\n                        base = anchor\n                    else:\n                        base = eliteX.mean(axis=0)\n                    perturb = vecs[:, :k].dot(coeffs)\n                    cand = base + perturb\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n            elif r < probs[0] + probs[1] + probs[2]:\n                # Strategy 3: Local anisotropic gaussian (~45%)\n                mask = np.random.rand(self.dim) < 0.25\n                # ensure at least one coord is perturbed\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise = np.random.randn(self.dim) * (scale_est * (0.5 + np.random.rand() * 0.5) * self.gscale)\n                # occasionally mix in a few uniform coords for diversity\n                if np.random.rand() < 0.12:\n                    uni_coords = np.random.rand(self.dim) < 0.12\n                    noise[uni_coords] = (np.random.rand(uni_coords.sum()) - 0.5) * (ub[uni_coords] - lb[uni_coords])\n                cand = anchor + noise * mask\n            else:\n                # Strategy 4: tempered Cauchy global escape (~10%)\n                cap = 8.0\n                cand = anchor + self._tempered_cauchy(scale_est * (self.gscale * 2.2 + 1e-12), cap_multiplier=cap)\n\n            # sometimes add a small differential jump for added diversity\n            if np.random.rand() < diff_chance and len(F) >= 3:\n                ids = np.random.choice(len(F), size=2, replace=False)\n                cand += 0.08 * self.gscale * (X[ids[0]] - X[ids[1]])\n\n            # occasional direct small jump to current best to refine\n            if np.random.rand() < 0.06 and finite_idx.any():\n                try:\n                    best_global = int(np.nanargmin(F))\n                    cand = X[best_global] + np.random.randn(self.dim) * (scale_est * 0.06 * (1 + self.gscale))\n                except Exception:\n                    pass\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            cand = np.asarray(cand, dtype=float)\n            bad = (~np.isfinite(cand)) | (np.isnan(cand))\n            if bad.any():\n                # fill bad coords individually\n                for j in np.where(bad)[0]:\n                    cand[j] = lb[j] + np.random.rand() * (ub[j] - lb[j])\n\n            # reflect to bounds & clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            if finite_idx.any():\n                rank = (F[finite_idx] <= f_c).sum()  # how many finite are <= f_c\n                prop = rank / finite_idx.sum()\n                success = (prop <= 0.25)\n            else:\n                success = True\n            self.success_window.append(1 if success else 0)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adaptation of gscale every now and then\n            if len(self.success_window) >= 8 and (len(self.success_window) % 6 == 0):\n                succ_rate = np.mean(self.success_window[-8:])\n                if succ_rate > 0.35:\n                    self.gscale *= 0.92 * (0.95 + 0.1 * np.random.rand())\n                elif succ_rate < 0.06:\n                    self.gscale *= 1.14 * (0.95 + 0.1 * np.random.rand())\n                    # mild jitter to escape stagnation\n                    self.gscale *= (0.98 + 0.06 * np.random.rand())\n                # clamp gscale to reasonable bounds\n                self.gscale = float(max(1e-6, min(5.0, self.gscale)))\n\n            # prune archive if too large\n            if len(self.archive_f) > self.archive_max and (len(self.archive_f) - self.last_prune) > max(1, (self.archive_max // 6)):\n                self.last_prune = len(self.archive_f)\n                F = np.array(self.archive_f)\n                finite_idx = np.isfinite(F)\n                keep_best = []\n                if finite_idx.any():\n                    # keep several best finite and spread out sampling\n                    best_idxs = np.argsort(F)[:max(1, self.archive_max // 6)]\n                    keep_best = list(map(int, best_idxs))\n                else:\n                    keep_best = []\n                # keep current best if present\n                if len(F) > 0:\n                    best_global = int(np.nanargmin(F))\n                    if best_global not in keep_best:\n                        keep_best.append(best_global)\n                # keep some random ones for diversity\n                remaining = list(set(range(len(F))) - set(keep_best))\n                nrand = max(0, self.archive_max - len(keep_best))\n                if remaining and nrand > 0:\n                    keep_rand = list(np.random.choice(remaining, size=min(nrand, len(remaining)), replace=False))\n                else:\n                    keep_rand = []\n                keep_idx = np.array(keep_best + keep_rand, dtype=int)\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if len(self.success_window) == self.success_win_len:\n                recent = np.mean(self.success_window)\n                if recent < 0.06:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n            # micro restart when stagnated\n            if self.stagnation_counter > 6 and self.evals < self.budget:\n                F = np.array(self.archive_f)\n                finite_idx = np.isfinite(F)\n                if finite_idx.any():\n                    best_idx = int(np.nanargmin(F))\n                    best_x = self.archive_x[best_idx].copy()\n                else:\n                    best_x = self._uniform_array(lb, ub)\n                # create local cloud around best and evaluate a few\n                n_local = min(6, max(2, int(0.08 * (self.budget - self.evals))))\n                sigma = (0.03 + 0.06 * np.random.rand()) * (1 + self.gscale)\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    cand = best_x + np.random.randn(self.dim) * (scale_est * sigma)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # some global randoms to regain diversity\n                n_global = min(6, max(1, int(0.03 * (self.budget - self.evals))))\n                for i in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales\n                self.gscale = min(3.0, self.gscale * (1.2 + 0.2 * np.random.rand()))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            # no evaluations? fallback to uniform random\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x\n        F = np.array(self.archive_f)\n        finite_idx = np.isfinite(F)\n        if finite_idx.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x", "configspace": "", "generation": 0, "feedback": "In the code, line 46, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "error": "In the code, line 46, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "773db274-71c8-429f-b760-df4cf61767c0", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing differential injections, PCA-guided elite perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400, success_win_len=24):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        if self.seed is not None:\n            np.random.seed(self.seed)\n        self.archive_max = int(archive_max)\n\n        # bookkeeping\n        self.archive_x = []  # list of arrays\n        self.archive_f = []\n\n        # adaptation windows & counters\n        self.gscale = 0.25  # global exploration multiplier (adapted)\n        self.stagnation_counter = 0\n        self.last_prune = 0\n        self.success_window = []\n        self.success_win_len = int(success_win_len)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # tuple style (lb, ub)\n            if isinstance(b, tuple) and len(b) == 2:\n                lb, ub = b\n            else:\n                # object with attributes\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n\n        # direct attributes on func\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # if still None, fallback to [-5,5]\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.array(lb, dtype=float)\n            ub = np.array(ub, dtype=float)\n            # broadcast scalars\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n            # if shapes mismatch or wrong size, fallback\n            if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=5):\n        # reflect coordinates that are out of bounds (mirror back) and finally clamp\n        x = x.copy()\n        for _ in range(max_iter):\n            low_mask = x < lb\n            high_mask = x > ub\n            if not (low_mask.any() or high_mask.any()):\n                break\n            if low_mask.any():\n                x[low_mask] = lb[low_mask] + (lb[low_mask] - x[low_mask])\n            if high_mask.any():\n                x[high_mask] = ub[high_mask] - (x[high_mask] - ub[high_mask])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _lhs01(self, n, dim):\n        # simple Latin hypercube in [0,1)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        rd = np.random.rand(n, dim)\n        rdpoints = a[:, None] + rd * (b - a)[:, None]\n        for j in range(dim):\n            np.random.shuffle(rdpoints[:, j])\n        return rdpoints\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise Cauchy tempered with tanh and occasional heavy tail\n        # standard cauchy via inverse CDF\n        u = np.random.rand(self.dim)\n        std_cauchy = np.tan(np.pi * (u - 0.5))\n        # scale per-dim\n        step = std_cauchy * (scale_vec + 1e-12)\n        # occasionally allow heavy tails\n        heavy_mask = (np.random.rand(self.dim) < 0.05)\n        if heavy_mask.any():\n            step[heavy_mask] *= (1.0 + np.random.rand(heavy_mask.sum()) * 10.0)\n        # temper extreme values and cap\n        capped = np.tanh(step / (scale_vec * cap_multiplier + 1e-12)) * (scale_vec * cap_multiplier)\n        return capped\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        self.evals = 0\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.stagnation_counter = 0\n        self.last_prune = 0\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator that respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: LHS samples ----\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 8))\n        n_init = max(2, n_init)\n        n_init = min(n_init, self.budget)\n        X0 = self._lhs01(n_init, self.dim)\n        X0 = lb + X0 * (ub - lb)\n        for i in range(X0.shape[0]):\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        add_uni = min(6, max(0, self.budget - self.evals))\n        if add_uni > 0:\n            U = self._uniform_array(lb, ub, n=add_uni)\n            for i in range(U.shape[0]):\n                if self.evals >= self.budget:\n                    break\n                f = safe_eval(U[i])\n                self.archive_x.append(U[i].copy())\n                self.archive_f.append(f)\n\n        # if still no finite best, sample one more random if budget allows\n        finite_mask = [np.isfinite(v) for v in self.archive_f]\n        if not any(finite_mask) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Main optimization loop\n        probs = np.array([0.22, 0.23, 0.45, 0.10])  # mixing: DE, PCA, Local, Cauchy\n        diff_chance = 0.12\n\n        while self.evals < self.budget:\n            # ensure archive arrays\n            X = np.vstack(self.archive_x)\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            n_points = len(F)\n\n            # robust per-dim scale estimate using IQR->std approximation and fallback to std\n            if finite_idx.any() and finite_idx.sum() >= 2:\n                try:\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    iqr = q75 - q25\n                    scale_est = iqr / 1.349  # approximate std\n                    zero_mask = (scale_est <= 1e-12)\n                    if zero_mask.any():\n                        fallback = np.std(X[finite_idx], axis=0)\n                        scale_est[zero_mask] = np.maximum(fallback[zero_mask], 1e-6)\n                except Exception:\n                    scale_est = np.maximum(np.std(X[finite_idx], axis=0), 1e-6)\n            else:\n                # if no finite or single, use wide scale\n                scale_est = np.full(self.dim, 0.5)\n\n            # sort indices by fitness (finite first)\n            sorted_idx = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            best_indices = sorted_idx.tolist()\n            # choose anchor: bias to elites if available\n            if len(F) >= 3 and any(finite_idx):\n                top_frac = max(2, int(0.12 * len(F)) )\n                if np.random.rand() < 0.75:\n                    anchor_idx = int(np.random.choice(best_indices[:top_frac]))\n                else:\n                    anchor_idx = int(np.random.choice(len(F)))\n            else:\n                anchor_idx = int(np.random.randint(len(F)))\n            anchor = X[anchor_idx].copy()\n\n            cand = None\n            r = np.random.rand()\n            # Strategy 1: DE-style differential injection (~22%)\n            if r < probs[0]:\n                if len(F) >= 3:\n                    ids = list(range(len(F)))\n                    if anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        diff = X[a] - X[b]\n                        gamma = 0.6 + np.random.rand() * 0.9  # scale diff\n                        jitter = np.random.randn(self.dim) * (scale_est * self.gscale * 0.4)\n                        cand = anchor + gamma * diff * (self.gscale) + jitter\n                    else:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n\n            # Strategy 2: PCA-guided elite perturbation (~23%)\n            elif r < probs[0] + probs[1]:\n                n_elite = max(2, min(10, int(0.12 * len(F) + 2)))\n                if len(F) >= n_elite and finite_idx.sum() >= 2:\n                    elite_idx = best_indices[:n_elite]\n                    eliteX = X[elite_idx]\n                    cov = np.cov(eliteX.T)\n                    try:\n                        vals, vecs = np.linalg.eigh(cov)\n                        vals = np.maximum(vals, 1e-12)\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                    except Exception:\n                        vals = np.ones(self.dim)\n                        vecs = np.eye(self.dim)\n                    # choose subspace dimension adaptively\n                    var_explained = np.cumsum(vals) / np.sum(vals)\n                    k = max(1, np.searchsorted(var_explained, 0.85) + 1)\n                    k = min(k, self.dim)\n                    # sample coefficients scaled by eigenvalues\n                    coeffs = np.random.randn(k) * (np.sqrt(vals[:k]) * (0.6 * self.gscale))\n                    perturb = vecs[:, :k].dot(coeffs)\n                    # bias sometimes toward anchor or mean of elites\n                    if np.random.rand() < 0.5:\n                        base = anchor\n                    else:\n                        base = eliteX.mean(axis=0)\n                    cand = base + perturb\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n\n            # Strategy 3: Local anisotropic gaussian (~45%)\n            elif r < probs[0] + probs[1] + probs[2]:\n                mask = (np.random.rand(self.dim) < (0.25 + 0.5 * min(1.0, self.gscale)))\n                # ensure at least one coord\n                mask[np.random.randint(self.dim)] = True\n                noise_scale = scale_est * (0.5 + np.random.rand() * 0.5) * (self.gscale + 1e-6)\n                noise = np.random.randn(self.dim) * noise_scale\n                # occasionally mix in a few uniform coords for diversity\n                if np.random.rand() < 0.12:\n                    uni_coords = (np.random.rand(self.dim) < 0.08)\n                    if uni_coords.any():\n                        noise[uni_coords] = (np.random.rand(uni_coords.sum()) - 0.5) * (ub[uni_coords] - lb[uni_coords])\n                cand = anchor + noise * mask.astype(float)\n\n            # Strategy 4: tempered Cauchy global escape (~10%)\n            else:\n                step = self._tempered_cauchy(scale_est * (1.0 + self.gscale * 0.5))\n                cand = anchor + step\n\n            # sometimes add a small differential jump for added diversity\n            if np.random.rand() < diff_chance and len(F) >= 3:\n                ids = np.random.choice(len(F), size=2, replace=False)\n                cand += 0.08 * self.gscale * (X[ids[0]] - X[ids[1]])\n\n            # occasional direct small jump to current best to refine\n            if np.random.rand() < 0.06 and finite_idx.any():\n                finite_indices = np.where(finite_idx)[0]\n                local_best_idx = finite_indices[np.argmin(F[finite_indices])]\n                cand = X[local_best_idx] + np.random.randn(self.dim) * (scale_est * 0.06 * (1 + self.gscale))\n\n            # safety: replace NaN/inf coords with uniform random within bounds\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            bad = (~np.isfinite(cand)) | (np.isnan(cand))\n            if np.any(bad):\n                # replace bad coords independently\n                rand_coords = np.random.rand(bad.sum())\n                cand[bad] = lb[bad] + rand_coords * (ub[bad] - lb[bad])\n\n            # reflect to bounds & clamp\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success relative to current archive: consider success if in top 25% finite\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            success = False\n            if finite_idx.any():\n                ranks = np.argsort(np.where(finite_idx, F, np.inf))\n                # find rank of latest (last appended)\n                latest_idx = len(F) - 1\n                # compute rank among finite entries\n                finite_indices = [i for i in ranks if np.isfinite(F[i])]\n                if latest_idx in finite_indices:\n                    rank = finite_indices.index(latest_idx)  # 0-based best\n                    success = (rank < max(1, int(0.25 * len(finite_indices))))\n                else:\n                    success = False\n            else:\n                success = True\n            self.success_window.append(1 if success else 0)\n            # trim window\n            if len(self.success_window) > self.success_win_len:\n                self.success_window = self.success_window[-self.success_win_len:]\n\n            # adaptation of gscale every now and then\n            if len(self.success_window) >= 8 and (len(self.success_window) % 6 == 0):\n                succ_rate = np.mean(self.success_window)\n                # if too easy (high success) -> contract slightly, if too hard -> expand\n                if succ_rate > 0.35:\n                    # contract\n                    self.gscale *= (0.90 + 0.08 * np.random.rand())\n                elif succ_rate < 0.10:\n                    # expand\n                    self.gscale *= (1.10 + 0.25 * np.random.rand())\n                else:\n                    # slight jitter\n                    self.gscale *= (0.98 + 0.06 * np.random.rand())\n                # clamp\n                self.gscale = float(np.minimum(np.maximum(self.gscale, 1e-5), 5.0))\n\n            # prune archive if too large\n            if len(self.archive_f) > self.archive_max and (len(self.archive_f) - self.last_prune) > (self.archive_max // 6):\n                self.last_prune = len(self.archive_f)\n                F = np.array(self.archive_f)\n                finite_idx = np.isfinite(F)\n                keep_idx = []\n                if finite_idx.any():\n                    best_k = max(8, int(0.2 * self.archive_max))\n                    idx_sorted = np.argsort(F)\n                    keep_best = list(idx_sorted[:best_k])\n                else:\n                    keep_best = []\n                keep_idx = keep_best.copy()\n                # keep current best if present\n                try:\n                    best_global = int(np.nanargmin(F))\n                    if best_global not in keep_idx:\n                        keep_idx.append(best_global)\n                except Exception:\n                    pass\n                # keep some random ones for diversity\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                nrand = max(0, self.archive_max - len(keep_idx))\n                if len(remaining) > 0 and nrand > 0:\n                    take = min(nrand, len(remaining))\n                    keep_rand = list(np.random.choice(remaining, size=take, replace=False))\n                    keep_idx.extend(keep_rand)\n                # finalize kept\n                keep_idx = sorted(set(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if len(self.success_window) == self.success_win_len:\n                recent = np.mean(self.success_window)\n                if recent < 0.06:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n\n            # micro restart when stagnated\n            if self.stagnation_counter > 6 and self.evals < self.budget:\n                finite_idx = np.isfinite(F)\n                if finite_idx.any():\n                    best_idx = int(np.argmin(F))\n                    best_x = self.archive_x[best_idx].copy()\n                else:\n                    best_x = self._uniform_array(lb, ub)\n                # create local cloud\n                n_cloud = min(12, max(4, (self.budget - self.evals) // 20))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    sigma = 0.6 + 0.6 * np.random.rand()\n                    cand = best_x + np.random.randn(self.dim) * (scale_est * sigma)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # some global randoms to regain diversity\n                n_global = min(6, max(2, (self.budget - self.evals) // 40))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales\n                self.gscale = float(min(3.0, self.gscale * (1.2 + 0.2 * np.random.rand())))\n                # reset small success window and stagnation timer\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            # no evaluations? fallback to uniform random\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x\n        F = np.array(self.archive_f)\n        finite_idx = np.isfinite(F)\n        if finite_idx.any():\n            best_idx = int(np.argmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()\n        else:\n            # all infinite/unusable: produce random point if budget still allowed (unlikely)\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x", "configspace": "", "generation": 0, "feedback": "In the code, line 46, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "error": "In the code, line 46, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "dcf3c193-166b-4d06-880b-0a176ade4ab9", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations,\n    anisotropic Gaussian local moves, tempered Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_max=400, success_win_len=24):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = int(archive_max)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # internal archive\n        self.archive_x = []  # list of numpy arrays\n        self.archive_f = []  # list of floats\n\n        # adaptation windows & counters\n        self.success_win_len = int(success_win_len)\n        self.success_window = []\n        self.stagnation_counter = 0\n\n        # global exploration scale (multiplicative)\n        self.gscale = 1.0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try common patterns, fallback to [-5,5]^dim\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # tuple style (lb, ub)\n                if isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb, ub = b\n                else:\n                    # object with attributes\n                    ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n                    lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n            else:\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None) or None\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None) or None\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.array(lb, dtype=float)\n            ub = np.array(ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n            if lb.size != self.dim or ub.size != self.dim:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # mirror reflections until in bounds, then clamp final\n        x = x.copy()\n        for _ in range(4):\n            low = x < lb\n            high = x > ub\n            if not (low.any() or high.any()):\n                break\n            x[low] = lb[low] + (lb[low] - x[low])\n            x[high] = ub[high] - (x[high] - ub[high])\n        # final clamp\n        np.minimum(np.maximum(x, lb), ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # sample coordinate-wise tempered cauchy with occasional heavy tail\n        u = np.random.rand(self.dim)\n        std_cauchy = np.tan(np.pi * (u - 0.5))\n        # allow occasional heavy tail multiplier\n        heavy = (np.random.rand(self.dim) < 0.05)\n        mult = np.ones(self.dim)\n        if heavy.any():\n            mult[heavy] += np.random.rand(heavy.sum()) * 10.0\n        step = std_cauchy * (scale_vec * (0.9 + 0.2 * np.random.rand(self.dim))) * mult\n        # temper extreme values and cap but keep sign\n        cap = np.maximum(1e-12, scale_vec * cap_multiplier)\n        tempered = np.tanh(step / (cap + 1e-12)) * cap\n        return tempered\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        self.evals = 0\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.stagnation_counter = 0\n        self.gscale = 1.0\n\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # evaluate once, count and enforce budget\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n                if not np.isfinite(f):\n                    f = float(np.inf)\n                else:\n                    f = float(f)\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # ---- initialization: a small latin-hypercube-ish + uniform mix ----\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 12))\n        n_init = max(2, int(n_init))\n        # simple stratified sampling per-dim\n        X0 = np.zeros((n_init, self.dim))\n        for j in range(self.dim):\n            perm = np.random.permutation(n_init)\n            strata = (perm + np.random.rand(n_init)) / n_init\n            X0[:, j] = lb[j] + strata * (ub[j] - lb[j])\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # add a few pure uniforms for diversity (bounded by budget)\n        add_uni = min(6, max(0, (self.budget - self.evals) // 30))\n        for _ in range(add_uni):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # if still no finite best, sample one more random if budget allows\n        finite_mask = [np.isfinite(v) for v in self.archive_f]\n        if not any(finite_mask) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # mixing probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.22, 0.23, 0.45, 0.10])\n        probs = probs / probs.sum()\n\n        # some hyper-parameters\n        diff_chance = 0.14\n        noise_base = 0.08\n        prune_every = 40\n        iter_since_prune = 0\n\n        # main loop\n        while self.evals < self.budget:\n            # ensure arrays\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n\n            finite_idx = np.isfinite(F)\n            n_pop = len(F)\n\n            # compute robust per-dim scale estimate\n            if finite_idx.any() and finite_idx.sum() >= 2:\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    iqr = q75 - q25\n                    scale_est = iqr / 1.349  # approx std from IQR\n                    # fallback to std where zero\n                    zero_mask = (scale_est <= 1e-12)\n                    if zero_mask.any():\n                        fallback = np.std(X[finite_idx], axis=0)\n                        scale_est[zero_mask] = np.maximum(fallback[zero_mask], 1e-6)\n                    scale_est = np.maximum(scale_est, 1e-6)\n                except Exception:\n                    scale_est = np.maximum(np.std(X[finite_idx], axis=0), 1e-6)\n            else:\n                # wide scale if not enough points\n                scale_est = np.full(self.dim, 2.5)\n\n            # choose anchor\n            if n_pop == 0:\n                anchor = self._uniform_array(lb, ub)\n                anchor_idx = None\n            else:\n                # sort indices by fitness (finite first)\n                if finite_idx.any():\n                    finite_indices = np.where(finite_idx)[0]\n                    sorted_finite = finite_indices[np.argsort(F[finite_indices])]\n                    best_indices = list(sorted_finite) + [i for i in range(n_pop) if i not in sorted_finite]\n                else:\n                    best_indices = list(range(n_pop))\n                # bias to elites\n                topk = max(1, min(n_pop, int(max(2, 0.12 * n_pop))))\n                if np.random.rand() < 0.75 and n_pop >= topk:\n                    anchor_idx = int(np.random.choice(best_indices[:topk]))\n                else:\n                    anchor_idx = int(np.random.choice(n_pop))\n                anchor = X[anchor_idx].copy()\n\n            # choose a strategy\n            r = np.random.rand()\n            cum = np.cumsum(probs)\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r < cum[0]:\n                if n_pop >= 3:\n                    # pick three distinct indices not equal to anchor_idx\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        diff = X[a] - X[b]\n                        gamma = 0.6 + 0.6 * np.random.rand()\n                        jitter = np.random.randn(self.dim) * (scale_est * self.gscale * 0.35)\n                        cand = anchor + gamma * diff * (0.9 * self.gscale) + jitter\n                    else:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n                else:\n                    cand = self._uniform_array(lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r < cum[1]:\n                n_elite = max(2, min(12, int(0.12 * max(3, n_pop)) + 1))\n                if n_pop >= n_elite and finite_idx.sum() >= 2:\n                    elite_idx = np.array(best_indices[:n_elite])\n                    eliteX = X[elite_idx]\n                    cov = np.cov(eliteX.T)\n                    try:\n                        vals, vecs = np.linalg.eigh(cov)\n                        # order descending\n                        order = np.argsort(vals)[::-1]\n                        vals = np.maximum(vals[order], 1e-12)\n                        vecs = vecs[:, order]\n                    except Exception:\n                        vals = np.ones(self.dim)\n                        vecs = np.eye(self.dim)\n                    # choose subspace dimension adaptively to explain ~85% variance\n                    explained = np.cumsum(vals) / np.sum(vals)\n                    k = int(np.searchsorted(explained, 0.85) + 1)\n                    k = max(1, min(self.dim, k))\n                    coeffs = np.random.randn(k) * (np.sqrt(vals[:k]) * (0.55 * self.gscale))\n                    perturb = vecs[:, :k].dot(coeffs)\n                    # sometimes bias base toward the elite mean\n                    if np.random.rand() < 0.6:\n                        base = eliteX.mean(axis=0)\n                    else:\n                        base = anchor\n                    cand = base + perturb\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r < cum[2]:\n                # coordinate mask to create anisotropy (sparse changes)\n                prob_change = 0.25 + 0.5 * min(1.0, self.gscale)\n                mask = (np.random.rand(self.dim) < prob_change)\n                # ensure at least one coordinate mutates\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise_scale = noise_base * self.gscale * (1.0 + np.random.rand(self.dim) * 0.6)\n                noise = np.random.randn(self.dim) * (scale_est * noise_scale)\n                cand = anchor + noise * mask.astype(float)\n                # occasional small uniform mix-in for diversity\n                if np.random.rand() < 0.12:\n                    uni_coords = (np.random.rand(self.dim) < 0.08)\n                    cand[uni_coords] = lb[uni_coords] + np.random.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                cand = anchor + self._tempered_cauchy(scale_est * (1.8 * self.gscale), cap_multiplier=5.0)\n                # sometimes center the Cauchy at global best to escape more aggressively\n                if np.random.rand() < 0.12 and finite_idx.any():\n                    best_idx = int(np.nanargmin(F))\n                    cand = X[best_idx] + self._tempered_cauchy(scale_est * (2.2 * self.gscale), cap_multiplier=6.0)\n\n            # sometimes add an extra small differential perturbation\n            if np.random.rand() < diff_chance and n_pop >= 3:\n                ids = np.random.choice(n_pop, size=2, replace=False)\n                cand += 0.05 * self.gscale * (X[ids[0]] - X[ids[1]])\n\n            # occasional directed small jump to current best for refinement\n            if np.random.rand() < 0.06 and finite_idx.any():\n                best_idx = int(np.nanargmin(F))\n                cand = 0.6 * cand + 0.4 * X[best_idx] + np.random.randn(self.dim) * (scale_est * 0.08 * self.gscale)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            bad = (~np.isfinite(cand))\n            if bad.any():\n                cand[bad] = lb[bad] + np.random.rand(bad.sum()) * (ub[bad] - lb[bad])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            if finite_idx.any():\n                finite_indices = np.where(finite_idx)[0]\n                ranks = np.argsort(F[finite_indices])\n                latest_idx = len(F) - 1\n                if np.isfinite(F[latest_idx]):\n                    # rank of latest in finite_indices\n                    position = np.where(finite_indices[ranks] == latest_idx)[0]\n                    if len(position) == 1:\n                        rank0 = int(position[0])  # 0-based best\n                        success = (rank0 < max(1, int(0.25 * len(finite_indices))))\n                    else:\n                        success = False\n                else:\n                    success = False\n            else:\n                success = False\n\n            # also count as success if it improves absolute best\n            if finite_idx.any():\n                if F[latest_idx] < np.min(F[finite_idx]):\n                    success = True\n\n            self.success_window.append(1.0 if success else 0.0)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) == self.success_win_len and (len(self.archive_f) % 7 == 0):\n                recent = np.mean(self.success_window)\n                # if too many successes -> contract slightly\n                if recent > 0.35:\n                    self.gscale *= (0.85 + 0.08 * np.random.rand())\n                # if too few successes -> expand a bit\n                elif recent < 0.08:\n                    self.gscale *= (1.10 + 0.12 * np.random.rand())\n                else:\n                    # slight jitter around current\n                    self.gscale *= (0.98 + 0.04 * np.random.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 0.06, 6.0))\n\n            # prune archive when too large\n            iter_since_prune += 1\n            if len(self.archive_f) > self.archive_max or (iter_since_prune >= prune_every and len(self.archive_f) > 60):\n                iter_since_prune = 0\n                F = np.array(self.archive_f)\n                finite_idx = np.isfinite(F)\n                keep_idx = set()\n                if finite_idx.any():\n                    # keep some best\n                    best_k = max(8, int(0.18 * len(F)))\n                    idx_sorted = np.argsort(F)\n                    keep_best = list(idx_sorted[:best_k])\n                    keep_idx.update(keep_best)\n                    # always keep the very best\n                    try:\n                        best_global = int(np.nanargmin(F))\n                        keep_idx.add(best_global)\n                    except Exception:\n                        pass\n                # keep some randoms for diversity\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                nrand = min(max(8, int(0.08 * len(F))), len(remaining))\n                if nrand > 0 and len(remaining) > 0:\n                    take = min(nrand, len(remaining))\n                    keep_rand = list(np.random.choice(remaining, size=take, replace=False))\n                    keep_idx.update(keep_rand)\n                # finalize kept\n                keep_idx = sorted(set(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection\n            if len(self.success_window) == self.success_win_len:\n                recent = np.mean(self.success_window)\n                if recent < 0.06:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n\n            # micro-restart when stagnated\n            if self.stagnation_counter >= 4 and self.evals < self.budget:\n                self.stagnation_counter = 0\n                # pick center for cloud: current best if exists else random\n                if len(self.archive_f) > 0 and np.isfinite(np.array(self.archive_f)).any():\n                    F = np.array(self.archive_f)\n                    best_idx = int(np.nanargmin(F))\n                    center = self.archive_x[best_idx].copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                # generate a local cloud around center\n                n_cloud = min(18, max(6, (self.budget - self.evals) // 8))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    sigma = 0.4 + 0.8 * np.random.rand()\n                    cand = center + np.random.randn(self.dim) * (scale_est * sigma * self.gscale * (0.5 + np.random.rand()))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                n_global = min(6, max(2, (self.budget - self.evals) // 60))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = float(np.clip(self.gscale * (1.12 + 0.08 * np.random.rand()), 0.06, 8.0))\n                self.success_window = []\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x\n\n        F = np.array(self.archive_f)\n        finite_idx = np.isfinite(F)\n        if finite_idx.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()\n        else:\n            # worst case: return the least-inf (may all be inf)\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "19a5ff8b-0466-4b13-8da9-65ee115185b3", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\nimport math\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=400, success_win_len=24, prune_every=120):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = int(archive_max)\n        self.success_win_len = int(success_win_len)\n        self.prune_every = int(prune_every)\n\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # internal state\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.evals = 0\n        self.gscale = 1.0  # global multiplicative exploration scale\n        self.stagnation_counter = 0\n\n    # --------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to recover bounds from common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            # func.bounds.lb / .ub (common in some frameworks)\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n            # direct attributes on func\n            if lb is None:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None) or None\n            if ub is None:\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None) or None\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Mirror reflections until in bounds, then final clamp\n        x = np.array(x, dtype=float)\n        for _ in range(8):\n            below = x < lb\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (below.any() or above.any()):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling and cap extremes.\n        \"\"\"\n        u = np.random.rand(self.dim)\n        std_c = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplier per-coordinate\n        heavy = (np.random.rand(self.dim) < 0.02).astype(float) * (1.0 + 10.0 * np.random.rand(self.dim))\n        mult = 1.0 + heavy\n        raw = std_c * mult\n        # cap each coordinate magnitude to cap_multiplier * scale_vec (but allow sign)\n        cap = cap_multiplier * (np.maximum(scale_vec, 1e-12))\n        raw = np.sign(raw) * np.minimum(np.abs(raw), 40.0)  # absolute safety\n        step = raw * scale_vec * (0.9 + 0.2 * np.random.rand(self.dim))\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # initialize counters and archive\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.evals = 0\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # Evaluate once, count and enforce budget\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                f = func(x)\n                f = float(f)\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # ---- initialization: stratified LHS-ish + uniform mix ----\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 12))\n        n_init = max(2, int(n_init))\n\n        # Basic LHS: each dimension stratified, shuffle per-dim\n        base = np.zeros((n_init, self.dim))\n        jitter = np.random.rand(n_init, self.dim)\n        for j in range(self.dim):\n            perm = np.arange(n_init)\n            np.random.shuffle(perm)\n            # stratified positions for dimension j\n            base[:, j] = (perm + jitter[:, j]) / float(n_init)\n        X0 = lb + base * (ub - lb)\n\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for diversity (bounded by budget)\n        add_uni = min(6, max(0, (self.budget - self.evals) // 30))\n        for _ in range(add_uni):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # If still no finite best, sample a few more randoms\n        finite_vals = [np.isfinite(v) for v in self.archive_f]\n        if not any(finite_vals) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.28, 0.28, 0.30, 0.14])\n        probs = probs / probs.sum()\n\n        # hyperparameters\n        diff_chance = 0.14  # chance to add small differential\n        noise_base = 0.08\n        iter_since_prune = 0\n        prune_every = max(20, self.prune_every)\n        n_elite = max(3, int(0.10 * max(10, len(self.archive_f))))\n        topk = max(3, int(0.15 * max(10, len(self.archive_f))))\n\n        # main loop\n        while self.evals < self.budget:\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n\n            n_pop = len(F)\n            finite_idx = np.array([np.isfinite(v) for v in F]) if n_pop > 0 else np.array([], dtype=bool)\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if finite_idx.any() and finite_idx.sum() >= 2:\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    iqr = q75 - q25\n                    scale_est = iqr / 1.349  # approx std from IQR\n                    # fallback to std where zero\n                    stds = np.std(X[finite_idx], axis=0)\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = stds[zero_mask]\n                    # fallback to overall width where still zero\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = (ub[zero_mask] - lb[zero_mask]) * 0.25\n                except Exception:\n                    scale_est = np.maximum(0.2 * (ub - lb), 1e-6)\n            else:\n                # wide scale if not enough points\n                scale_est = 0.4 * (ub - lb)\n\n            # choose anchor (center point for generating candidate)\n            if n_pop == 0:\n                anchor_idx = None\n                anchor = self._uniform_array(lb, ub)\n            else:\n                # bias toward elites if we have finite evaluations\n                if finite_idx.any():\n                    finite_indices = np.where(finite_idx)[0]\n                    sorted_idx = np.argsort(F[finite_indices])\n                    best_indices = finite_indices[sorted_idx]\n                else:\n                    best_indices = np.arange(n_pop)\n\n                if len(best_indices) == 0:\n                    anchor_idx = np.random.randint(n_pop)\n                else:\n                    topk = max(1, int(min(len(best_indices), max(2, int(0.12 * n_pop)))))\n                    if np.random.rand() < 0.7:\n                        anchor_idx = int(np.random.choice(best_indices[:topk]))\n                    else:\n                        anchor_idx = int(np.random.choice(best_indices))\n                anchor = X[anchor_idx].copy()\n\n            # choose a strategy\n            r = np.random.rand()\n            cum = np.cumsum(probs)\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0]:\n                if n_pop >= 3:\n                    # choose three distinct indices not equal to anchor\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        gamma = 0.6 + 0.6 * np.random.rand()\n                        base = anchor.copy()\n                        diff = X[a] - X[b]\n                        cand = base + gamma * diff + 0.04 * np.random.randn(self.dim) * (scale_est * self.gscale)\n                    else:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n                else:\n                    cand = self._uniform_array(lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1]:\n                # pick elites\n                if n_pop >= 4 and finite_idx.any():\n                    n_elite = max(2, min(8, int(0.08 * max(10, n_pop)) + 2))\n                    # ensure we have indices\n                    if finite_idx.any():\n                        sorted_idx = np.argsort(F[finite_idx])\n                        elite_idx = np.where(finite_idx)[0][sorted_idx[:n_elite]]\n                    else:\n                        elite_idx = np.argsort(F)[:n_elite]\n                    eliteX = X[elite_idx]\n                    try:\n                        cov = np.cov(eliteX.T)\n                        vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                        explained = np.cumsum(vals) / np.sum(vals)\n                        k = int(np.searchsorted(explained, 0.85) + 1)\n                        k = max(1, min(self.dim, k))\n                        coeffs = np.random.randn(k) * np.sqrt(np.maximum(vals[:k], 1e-12)) * (0.6 * self.gscale)\n                        perturb = vecs[:, :k].dot(coeffs)\n                    except Exception:\n                        # fallback to small gaussian\n                        perturb = np.random.randn(self.dim) * (scale_est * 0.6 * self.gscale)\n                    # sometimes bias base toward elite mean\n                    if np.random.rand() < 0.6:\n                        base = eliteX.mean(axis=0)\n                    else:\n                        base = anchor\n                    cand = base + perturb\n\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                prob_change = 0.25 + 0.5 * min(1.0, self.gscale)\n                mask = (np.random.rand(self.dim) < prob_change)\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise_scale = noise_base * self.gscale * (1.0 + np.random.rand(self.dim) * 0.6)\n                noise = np.random.randn(self.dim) * noise_scale\n                cand = anchor + noise * mask.astype(float)\n                # occasional small uniform mix-in for diversity\n                if np.random.rand() < 0.08:\n                    uni_coords = (np.random.rand(self.dim) < 0.12)\n                    if uni_coords.any():\n                        cand[uni_coords] = lb[uni_coords] + np.random.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                base = anchor.copy()\n                if np.random.rand() < 0.12 and finite_idx.any():\n                    # center at global best sometimes\n                    best_idx = int(np.nanargmin(F))\n                    base = X[best_idx].copy()\n                step = self._tempered_cauchy(scale_est * self.gscale, cap_multiplier=6.0)\n                cand = base + step\n                # occasionally mix small differential\n                if np.random.rand() < diff_chance and n_pop >= 2:\n                    ids = list(range(n_pop))\n                    if anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        cand += 0.08 * np.random.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if np.random.rand() < 0.06 and n_pop >= 1 and finite_idx.any():\n                best_idx = int(np.nanargmin(F))\n                cand = 0.6 * cand + 0.4 * X[best_idx] + np.random.randn(self.dim) * (scale_est * 0.08 * self.gscale)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.array(cand, dtype=float)\n            bad = (~np.isfinite(cand)) | (cand < lb - 1e6) | (cand > ub + 1e6)\n            if bad.any():\n                cand[bad] = lb[bad] + np.random.rand(bad.sum()) * (ub[bad] - lb[bad])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f)\n            finite_idx = np.array([np.isfinite(v) for v in F])\n            latest_idx = len(F) - 1\n            success = False\n            if finite_idx.any():\n                finite_indices = np.where(finite_idx)[0]\n                if len(finite_indices) > 0:\n                    # rank of latest among finite\n                    fin_vals = F[finite_indices]\n                    pos = (fin_vals <= F[latest_idx]).sum()  # how many finite are <= latest\n                    # if latest in top 25% (best means smaller)\n                    if pos <= max(1, int(0.25 * len(fin_vals))):\n                        success = True\n                # also count as success if it improves absolute best\n                if F[latest_idx] < np.min(F[finite_indices]):\n                    success = True\n            else:\n                # if all infinite, any finite is success\n                if np.isfinite(F[latest_idx]):\n                    success = True\n\n            self.success_window.append(1.0 if success else 0.0)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= 6 and (self.evals % 6 == 0):\n                recent = float(np.mean(self.success_window))\n                if recent > 0.28:\n                    # many successes -> contract\n                    self.gscale *= (0.88 + 0.08 * np.random.rand())\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n                elif recent < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= (1.10 + 0.12 * np.random.rand())\n                    self.stagnation_counter += 1\n                else:\n                    # slight jitter\n                    self.gscale *= (0.98 + 0.04 * np.random.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n\n            # prune archive when too large\n            iter_since_prune += 1\n            if len(self.archive_f) > self.archive_max or (iter_since_prune >= prune_every and len(self.archive_f) > 60):\n                iter_since_prune = 0\n                F = np.array(self.archive_f)\n                keep_idx = set()\n                try:\n                    # keep some best\n                    best_k = max(8, int(0.18 * len(F)))\n                    idx_sorted = np.argsort(F)\n                    for i in range(min(best_k, len(idx_sorted))):\n                        keep_idx.add(int(idx_sorted[i]))\n                    # always keep the very best\n                    best_global = int(np.nanargmin(F))\n                    keep_idx.add(best_global)\n                except Exception:\n                    pass\n\n                # keep some randoms for diversity\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                if len(remaining) > 0:\n                    nrand = min(max(8, int(0.08 * len(F))), len(remaining))\n                    take = min(nrand, len(remaining))\n                    if take > 0:\n                        rnd = np.random.choice(remaining, size=take, replace=False)\n                        for r_idx in rnd:\n                            keep_idx.add(int(r_idx))\n\n                # finalize kept\n                keep_idx = sorted(set(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            recent_rate = float(np.mean(self.success_window)) if len(self.success_window) > 0 else 0.0\n            if recent_rate < 0.06 and self.stagnation_counter >= 6 and self.evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random if none)\n                if len(self.archive_f) == 0:\n                    center = self._uniform_array(lb, ub)\n                else:\n                    best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                    center = self.archive_x[best_idx].copy()\n                n_cloud = min(18, max(6, (self.budget - self.evals) // 8))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    sigma = 0.4 + 0.8 * np.random.rand()\n                    cand = center + np.random.randn(self.dim) * (scale_est * sigma * self.gscale * (0.5 + np.random.rand()))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                n_global = min(6, max(2, (self.budget - self.evals) // 20))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.06 + 0.14 * np.random.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            # worst case: sample one uniform if budget 0 or nothing evaluated\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n\n        F = np.array(self.archive_f)\n        finite_idx = np.array([np.isfinite(v) for v in F])\n        if finite_idx.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()\n        else:\n            # all infinite, return the least-inf (first)\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8c997fe6-5508-4dda-b759-90efc8af509c", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style recombination, PCA-guided elite perturbations, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import math\nimport numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=400, success_win_len=24, prune_every=120):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n        self.success_win_len = int(success_win_len)\n        self.prune_every = int(prune_every)\n\n        # internal state\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.gscale = 1.0\n        self.success_window = deque(maxlen=self.success_win_len)\n        self.stagnation_counter = 0\n\n    # --------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to recover bounds from common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # try attributes .lb and .ub\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"lbound\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"ubound\", None)\n            if lb is None:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            if ub is None:\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == ():  # scalars\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        # shape check\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Mirror reflection until in bounds, then final clamp\n        x = np.array(x, dtype=float)\n        for _ in range(10):\n            below = x < lb\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (below.any() or above.any()):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        return lb + self.rng.rand(self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling and cap extremes.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        std_c = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplier per-coordinate\n        heavy = (self.rng.rand(self.dim) < 0.02).astype(float) * (1.0 + 10.0 * self.rng.rand(self.dim))\n        mult = 1.0 + heavy\n        raw = std_c * mult * (scale_vec + 1e-12)\n        cap = cap_multiplier * (scale_vec + 1e-12)\n        step = np.sign(raw) * np.minimum(np.abs(raw), cap)\n        # small chance of pure large uniform jump\n        if self.rng.rand() < 0.01:\n            step += (self.rng.rand(self.dim) - 0.5) * 2.0 * cap\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # limit sanity\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n        self.success_window.clear()\n\n        def safe_eval(x):\n            # Evaluate once, count and enforce budget\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # ---- initialization: stratified LHS-ish + uniform mix ----\n        n_init = min(max(10, int(0.05 * self.budget)), max(10, self.budget // 20))\n        n_init = max(2, n_init)\n        # simple LHS-like\n        X0 = np.zeros((n_init, self.dim))\n        jitter = self.rng.rand(n_init, self.dim) * (1.0 / n_init)\n        for j in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            X0[:, j] = (perm + jitter[:, j]) / float(n_init)\n        X0 = lb + X0 * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for diversity (bounded by budget)\n        add_uni = min(6, max(0, self.budget - self.evals))\n        for _ in range(add_uni):\n            if self.evals >= self.budget: break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # If still no finite best, sample a few more randoms\n        finite_vals = [np.isfinite(v) for v in self.archive_f]\n        if not any(finite_vals):\n            extra = min(8, max(0, self.budget - self.evals))\n            for _ in range(extra):\n                if self.evals >= self.budget: break\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.archive_x.append(x.copy())\n                self.archive_f.append(f)\n\n        # main loop\n        iter_since_prune = 0\n        prune_every = self.prune_every\n        while self.evals < self.budget:\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f, dtype=float) if len(self.archive_f) > 0 else np.array([])\n            n_pop = len(F)\n\n            # compute per-dim robust scale (IQR-based) where possible\n            if n_pop >= 4:\n                finite_idx = np.isfinite(F)\n                if finite_idx.sum() >= 4:\n                    try:\n                        Xf = X[finite_idx]\n                        q75 = np.percentile(Xf, 75, axis=0)\n                        q25 = np.percentile(Xf, 25, axis=0)\n                        iqr = q75 - q25\n                        scale_est = iqr / 1.349  # approx std from IQR\n                        stds = np.std(Xf, axis=0)\n                        zero_mask = (scale_est < 1e-12)\n                        scale_est[zero_mask] = stds[zero_mask]\n                        zero_mask = (scale_est < 1e-12)\n                        scale_est[zero_mask] = (ub[zero_mask] - lb[zero_mask]) * 0.25\n                    except Exception:\n                        scale_est = np.maximum(0.2 * (ub - lb), 1e-6)\n                else:\n                    scale_est = np.maximum(0.2 * (ub - lb), 1e-6)\n            else:\n                scale_est = np.maximum(0.25 * (ub - lb), 1e-6)\n\n            # choose anchor (center point for generating candidate)\n            anchor = None\n            if n_pop >= 3 and np.isfinite(F).any():\n                best_indices = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                elite_prob = 0.7\n                if self.rng.rand() < elite_prob:\n                    # pick from top 15% or top 3\n                    topk = max(3, int(0.15 * max(10, n_pop)))\n                    topk = min(topk, n_pop)\n                    pick_idx = best_indices[self.rng.randint(0, topk)]\n                else:\n                    pick_idx = self.rng.randint(0, n_pop)\n                anchor = X[pick_idx].copy()\n            else:\n                anchor = self._uniform_array(lb, ub)\n\n            # choose strategy\n            r = self.rng.rand()\n            # probabilities: DE 35%, PCA 25%, Local 30%, Cauchy 10%\n            cum = [0.35, 0.60, 0.90, 1.0]\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0] and n_pop >= 4:\n                # pick three distinct indices\n                ids = list(range(n_pop))\n                # prefer finite entries\n                finite_ids = [i for i in ids if np.isfinite(F[i])]\n                if len(finite_ids) >= 3:\n                    a, b, c = self.rng.choice(finite_ids, size=3, replace=False)\n                else:\n                    a, b, c = self.rng.choice(ids, size=3, replace=False)\n                base = X[a].copy()\n                diff = X[b] - X[c]\n                gamma = 0.6 * (0.6 + 0.8 * self.rng.rand())  # variable scaling ~ [0.36, 0.96]\n                cand = base + gamma * diff + 0.06 * self.rng.randn(self.dim) * (scale_est * self.gscale)\n                # small chance to bias toward anchor\n                if self.rng.rand() < 0.12:\n                    cand = anchor + 0.5 * (cand - anchor)\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1] and n_pop >= 4:\n                finite_idx = np.isfinite(F)\n                if finite_idx.sum() >= 4:\n                    sorted_idx = np.argsort(F[finite_idx])\n                    idxs = np.where(finite_idx)[0][sorted_idx]\n                    n_elite = max(3, min(len(idxs), int(0.10 * len(idxs)) + 3))\n                    elite_idx = idxs[:n_elite]\n                    elite_X = X[elite_idx]\n                    # compute covariance\n                    cov = np.cov(elite_X.T) if elite_X.shape[0] > 1 else np.eye(self.dim) * 1e-6\n                    try:\n                        vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                        # sample along top-k principal axes\n                        explained = np.cumsum(vals) / np.sum(vals + 1e-12)\n                        k = max(1, int(np.searchsorted(explained, 0.85) + 1))\n                        k = min(k, self.dim)\n                        coeffs = self.rng.randn(k) * np.sqrt(np.maximum(vals[:k], 1e-12)) * (0.6 * self.gscale)\n                        perturb = vecs[:, :k].dot(coeffs)\n                        base = elite_X[self.rng.randint(0, elite_X.shape[0])].copy()\n                        cand = base + perturb + 0.04 * self.rng.randn(self.dim) * (scale_est * self.gscale)\n                    except Exception:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * 0.6 * self.gscale)\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * 0.6 * self.gscale)\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                noise_base = 0.9\n                prob_change = 0.25 + 0.5 * min(1.0, self.gscale)\n                noise_scale = noise_base * self.gscale * (1.0 + self.rng.rand(self.dim) * 0.6)\n                mask = self.rng.rand(self.dim) < prob_change\n                cand = anchor.copy()\n                cand[mask] += self.rng.randn(mask.sum()) * (scale_est[mask] * noise_scale[mask])\n                # occasional small uniform mix-in for diversity\n                if self.rng.rand() < 0.08:\n                    uni_coords = self.rng.rand(self.dim) < 0.07\n                    if uni_coords.any():\n                        cand[uni_coords] = lb[uni_coords] + self.rng.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                # center at global best sometimes\n                if n_pop >= 1 and np.isfinite(F).any() and self.rng.rand() < 0.6:\n                    best_idx = int(np.nanargmin(F))\n                    base = X[best_idx].copy()\n                else:\n                    base = self._uniform_array(lb, ub)\n                step = self._tempered_cauchy(scale_est * self.gscale, cap_multiplier=6.0)\n                cand = base + step\n                # occasionally mix small differential\n                if n_pop >= 3 and self.rng.rand() < 0.18:\n                    ids = list(range(n_pop))\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    cand += 0.08 * self.rng.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.06 and n_pop >= 1 and np.isfinite(F).any():\n                best_idx = int(np.nanargmin(F))\n                cand += 0.12 * self.rng.rand() * (X[best_idx] - anchor)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            if cand is None or not np.isfinite(cand).all():\n                cand = self._uniform_array(lb, ub)\n            bad = (~np.isfinite(cand)) | (cand < lb - 1e6) | (cand > ub + 1e6)\n            if bad.any():\n                # replace bad coords\n                bad_idx = np.where(bad)[0]\n                cand[bad_idx] = lb[bad_idx] + self.rng.rand(bad_idx.size) * (ub[bad_idx] - lb[bad_idx])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f, dtype=float)\n            latest_idx = len(F) - 1\n            success = False\n            finite_idx = np.isfinite(F)\n            if finite_idx.any():\n                finite_vals = F[finite_idx]\n                rank = (finite_vals < F[latest_idx]).sum()\n                if rank <= max(0, int(0.25 * finite_vals.size)):\n                    success = True\n                # also success if improves absolute best\n                if F[latest_idx] <= np.nanmin(F):\n                    success = True\n            else:\n                # if all infinite, any finite is success\n                if np.isfinite(F[latest_idx]):\n                    success = True\n\n            self.success_window.append(1 if success else 0)\n            if success:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= 6 and (self.evals % 6 == 0):\n                recent_rate = float(np.mean(self.success_window))\n                if recent_rate > 0.4:\n                    # many successes -> contract\n                    self.gscale *= (0.86 + 0.06 * self.rng.rand())\n                elif recent_rate < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= (1.06 + 0.18 * self.rng.rand())\n                else:\n                    # slight jitter\n                    self.gscale *= (0.96 + 0.08 * self.rng.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-6, 1e3))\n\n            # prune archive when too large\n            iter_since_prune += 1\n            if (len(self.archive_f) > self.archive_max) or (iter_since_prune >= prune_every and len(self.archive_f) > 120):\n                iter_since_prune = 0\n                F = np.array(self.archive_f, dtype=float)\n                # keep some best and randoms\n                idx_sorted = np.argsort(np.where(np.isfinite(F), F, np.inf))\n                keep_best = int(max(20, 0.35 * len(F)))\n                keep_idx = set(int(i) for i in idx_sorted[:keep_best])\n                # random keep\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                nrand = min(max(12, int(0.10 * len(F))), len(remaining))\n                if nrand > 0:\n                    r_idx = self.rng.choice(remaining, size=nrand, replace=False)\n                    for rr in r_idx:\n                        keep_idx.add(int(rr))\n                # finalize kept\n                keep_list = sorted(list(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_list]\n                self.archive_f = [self.archive_f[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            recent_rate = float(np.mean(self.success_window)) if len(self.success_window) > 0 else 0.0\n            remaining_budget = self.budget - self.evals\n            if recent_rate < 0.06 and self.stagnation_counter >= 20 and remaining_budget > 6:\n                # perform micro-restart: local cloud around current best (or random if none)\n                if len(self.archive_f) > 0 and np.isfinite(self.archive_f).any():\n                    best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                    center = self.archive_x[best_idx].copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                n_cloud = min(18, max(6, remaining_budget // 6))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    sigma = 0.8 + 1.6 * self.rng.rand()\n                    cand = center + self.rng.randn(self.dim) * (scale_est * sigma * self.gscale * (0.4 + 0.6 * self.rng.rand()))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                for _ in range(min(4, max(0, self.budget - self.evals))):\n                    if self.evals >= self.budget:\n                        break\n                    x = self._uniform_array(lb, ub)\n                    f = safe_eval(x)\n                    self.archive_x.append(x.copy())\n                    self.archive_f.append(f)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.06 + 0.14 * self.rng.rand())\n                self.success_window.clear()\n                self.stagnation_counter = 0\n                # continue main loop\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        F = np.array(self.archive_f, dtype=float) if len(self.archive_f) > 0 else np.array([])\n        if len(F) == 0:\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n        if np.isfinite(F).any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx], dtype=float)\n        else:\n            # all infinite, return the least-inf (first)\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx], dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=self.success_win_len)", "error": "In the code, line 33, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=self.success_win_len)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7a773c1b-df05-42de-8d46-b65ec19154a7", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_frac=0.06,    # fraction of budget used for stratified init\n                 max_archive=400):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # adaptive global scale for many strategies\n        self.gscale = 0.6\n        # memory / archive\n        self.archive_x = []\n        self.archive_f = []\n        # bookkeeping\n        self.evals = 0\n        # short-term success window (1/6 of budget steps or small cap)\n        self.success_window = deque(maxlen=60)\n        self.stagnation_counter = 0\n        self.max_archive = int(max_archive)\n        # parameters that control mixes\n        self.init_frac = float(init_frac)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common patterns; fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # common style: func.bounds.lb / func.bounds.ub (BBOB wrappers)\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        # other common attributes\n        if lb is None:\n            for name in (\"lower_bounds\", \"lower_bound\", \"lb\", \"lower\"):\n                if hasattr(func, name):\n                    try:\n                        lb = np.asarray(getattr(func, name), dtype=float)\n                        break\n                    except Exception:\n                        lb = None\n        if ub is None:\n            for name in (\"upper_bounds\", \"upper_bound\", \"ub\", \"upper\"):\n                if hasattr(func, name):\n                    try:\n                        ub = np.asarray(getattr(func, name), dtype=float)\n                        break\n                    except Exception:\n                        ub = None\n\n        # fallback to scalar attributes maybe\n        if lb is None and hasattr(func, \"bound\") and hasattr(func.bound, \"lower\"):\n            try:\n                lb = np.asarray(func.bound.lower, dtype=float)\n            except Exception:\n                lb = None\n\n        # final fallback: standard BBOB search box: [-5,5]\n        if lb is None:\n            lb = np.full(self.dim, -5.0)\n        elif lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        else:\n            lb = np.asarray(lb, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb))\n\n        if ub is None:\n            ub = np.full(self.dim, 5.0)\n        elif ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        else:\n            ub = np.asarray(ub, dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub))\n\n        # ensure shapes\n        if lb.shape[0] != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.shape[0] != self.dim:\n            ub = np.resize(ub, self.dim)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Mirror reflections until in bounds, then final clamp\n        x = np.array(x, dtype=float)\n        for _ in range(8):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            out = lb + self.rng.rand(n, self.dim) * (ub - lb)\n            return out\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling and cap extremes.\n        scale_vec: vector of per-dim scale (non-negative)\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        # standard cauchy\n        step = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplier (per-coordinate)\n        heavy = (self.rng.rand(self.dim) < 0.06).astype(float) * (2.0 * self.rng.rand(self.dim))\n        mult = 1.0 + heavy\n        step = step * mult\n        # scale\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        # fallback for zeros\n        small = (scale_vec <= 0) | (~np.isfinite(scale_vec))\n        if small.any():\n            # use a small global scale for zero components which will be set by caller\n            scale_vec = scale_vec.copy()\n            # caller should set reasonable baseline; here we set a tiny positive to keep numerical stability\n            scale_vec[small] = 1e-6\n        step = step * scale_vec\n        cap = cap_multiplier * scale_vec\n        # cap each coordinate magnitude to cap_multiplier * scale_vec (but allow sign)\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # initialize counters and archive\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.success_window.clear()\n\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # Evaluate once, count and enforce budget\n            if self.evals >= self.budget:\n                return np.nan\n            # ensure 1d array\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            try:\n                f = float(func(x))\n            except Exception:\n                # in case function raises, count the eval and mark as inf\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # ---- initialization: stratified LHS-ish + uniform mix ----\n        n_init = int(min(max(8, int(self.budget * self.init_frac)), max(12, self.budget)))\n        n_init = max(8, min(n_init, self.budget))\n        base = np.zeros((n_init, self.dim))\n        # stratified positions for dimension j\n        for j in range(self.dim):\n            strata = (np.arange(n_init) + self.rng.rand(n_init)) / float(n_init)\n            self.rng.shuffle(strata)\n            base[:, j] = lb[j] + strata * (ub[j] - lb[j])\n        # evaluate initial population\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = base[i]\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n            # treat first successes\n            self.success_window.append(1.0 if np.isfinite(f) else 0.0)\n        # add a few pure uniform draws for diversity (bounded by budget)\n        n_uniform = min(8, max(0, self.budget - self.evals))\n        for _ in range(n_uniform):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n            self.success_window.append(1.0 if np.isfinite(f) else 0.0)\n\n        # If still nothing finite, sample a few more randoms\n        finite_vals = [np.isfinite(v) for v in self.archive_f]\n        if not any(finite_vals):\n            extra = min(16, max(4, self.budget - self.evals))\n            for _ in range(extra):\n                if self.evals >= self.budget:\n                    break\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.archive_x.append(x.copy())\n                self.archive_f.append(f)\n                self.success_window.append(1.0 if np.isfinite(f) else 0.0)\n\n        # main loop variables\n        iter_since_prune = 0\n        adapt_interval = 6\n\n        # mixture probabilities for strategies\n        p_de = 0.30\n        p_pca = 0.20\n        p_local = 0.38\n        p_cauchy = 0.12\n        cum_probs = np.cumsum([p_de, p_pca, p_local, p_cauchy])\n\n        # hyperparameters\n        topk = max(3, int(0.12 * max(10, len(self.archive_f))))\n        n_elite_default = max(3, topk)\n\n        # main loop\n        while self.evals < self.budget:\n            # prepare population arrays\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n\n            n_pop = len(F)\n            finite_idx = np.array([np.isfinite(v) for v in F]) if n_pop > 0 else np.array([], dtype=bool)\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if n_pop >= 4 and finite_idx.any():\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    iqr = q75 - q25\n                    scale_est = iqr / 1.349  # approx std from IQR\n                    # fallback to std where zero\n                    zeros = (scale_est <= 0) | (~np.isfinite(scale_est))\n                    if zeros.any():\n                        stds = np.std(X[finite_idx], axis=0)\n                        stds[~np.isfinite(stds)] = 0.0\n                        scale_est[zeros] = stds[zeros]\n                        zeros2 = (scale_est <= 0) | (~np.isfinite(scale_est))\n                        if zeros2.any():\n                            # fallback to overall width\n                            width = ub - lb\n                            scale_est[zeros2] = 0.25 * width[zeros2]\n                except Exception:\n                    # wide scale if not enough points\n                    scale_est = 0.25 * (ub - lb)\n            else:\n                # use a broad scale\n                scale_est = 0.25 * (ub - lb)\n\n            # choose anchor (center point for generating candidate)\n            anchor_idx = None\n            if n_pop == 0:\n                anchor = self._uniform_array(lb, ub)\n                anchor_idx = None\n            else:\n                # bias toward elites if we have finite evaluations\n                finite_inds = np.where(finite_idx)[0]\n                if finite_inds.size == 0:\n                    # no finite values: pick random\n                    anchor_idx = self.rng.randint(0, n_pop)\n                else:\n                    # select among topk best (lower is better)\n                    n_elite = max(1, min(n_elite_default, finite_inds.size))\n                    best_indices = np.argsort(F[finite_inds])[:n_elite]\n                    # map back to original indices\n                    elite_indices = finite_inds[best_indices]\n                    # choose anchor by tournament among elites with bias\n                    if self.rng.rand() < 0.7:\n                        # pick a random elite\n                        anchor_idx = int(self.rng.choice(elite_indices))\n                    else:\n                        # pick best\n                        anchor_idx = int(elite_indices[0])\n                anchor = X[anchor_idx] if anchor_idx is not None else self._uniform_array(lb, ub)\n\n            # choose a strategy\n            r = self.rng.rand()\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum_probs[0]:\n                if n_pop >= 3:\n                    # choose three distinct indices not equal to anchor\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = self.rng.choice(ids, size=2, replace=False)\n                        # use anchor as base\n                        base = anchor.copy()\n                        gamma = 0.8 * (0.8 + 0.4 * self.rng.rand())  # differential factor ~0.64..1.0\n                        diff = X[a] - X[b]\n                        cand = base + gamma * diff + 0.04 * self.rng.randn(self.dim) * (scale_est * self.gscale)\n                    else:\n                        cand = anchor + 0.06 * self.rng.randn(self.dim) * (scale_est * self.gscale)\n                else:\n                    # fallback small gaussian\n                    cand = anchor + 0.06 * self.rng.randn(self.dim) * (scale_est * self.gscale)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum_probs[1]:\n                if n_pop >= 4 and finite_idx.any():\n                    finite_inds = np.where(finite_idx)[0]\n                    n_elite = max(3, min(len(finite_inds), n_elite_default))\n                    elite_idx = finite_inds[np.argsort(F[finite_inds])[:n_elite]]\n                    eliteX = X[elite_idx]\n                    # center\n                    base = eliteX.mean(axis=0)\n                    # compute PCA (covariance)\n                    try:\n                        Xc = eliteX - base\n                        cov = np.cov(Xc.T)\n                        # small regularization\n                        cov += np.eye(self.dim) * 1e-9\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort largest first\n                        order = np.argsort(-eigvals)\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # decide number of components to use\n                        explained = np.cumsum(eigvals) / (eigvals.sum() + 1e-12)\n                        k = int(np.searchsorted(explained, 0.85) + 1)\n                        k = max(1, min(self.dim, k))\n                        # sample coefficients along top components\n                        coeffs = self.rng.randn(k) * (np.sqrt(np.maximum(eigvals[:k], 1e-12)) * (0.8 + 0.6 * self.rng.rand(k)))\n                        perturb = eigvecs[:, :k].dot(coeffs)\n                        cand = base + perturb * self.gscale\n                        # occasionally bias base toward anchor\n                        if self.rng.rand() < 0.3:\n                            cand = 0.5 * cand + 0.5 * anchor\n                    except Exception:\n                        cand = base + 0.08 * self.rng.randn(self.dim) * (scale_est * self.gscale)\n                else:\n                    # fallback to small gaussian around anchor\n                    cand = anchor + 0.06 * self.rng.randn(self.dim) * (scale_est * self.gscale)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum_probs[2]:\n                # anisotropic noise scaled by per-dim scale_est\n                noise_base = 0.12 + 0.06 * self.rng.rand()\n                noise_scale = noise_base * self.gscale * (1.0 + self.rng.rand(self.dim) * 0.6)\n                per_dim = (self.rng.randn(self.dim) * (scale_est * noise_scale))\n                cand = anchor + per_dim\n                # occasional small uniform mix-in for diversity\n                if self.rng.rand() < 0.08:\n                    uni_coords = self.rng.rand(self.dim) < 0.12\n                    if uni_coords.any():\n                        cand[uni_coords] = lb[uni_coords] + self.rng.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                base = anchor.copy()\n                # sometimes center at global best\n                if n_pop > 0 and self.rng.rand() < 0.18:\n                    best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                    base = self.archive_x[best_idx].copy()\n                # tempered cauchy step\n                step = self._tempered_cauchy(scale_est * (0.9 + 0.6 * self.gscale))\n                cand = base + step\n                # occasionally mix small differential\n                if self.rng.rand() < 0.2 and n_pop >= 2:\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = self.rng.choice(ids, size=2, replace=False)\n                        cand = 0.9 * cand + 0.1 * (X[a] - X[b]) * 0.6\n\n            # occasional directed small jump to current best for refinement\n            if n_pop >= 1 and self.rng.rand() < 0.06:\n                best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                cand = 0.6 * cand + 0.4 * X[best_idx] + self.rng.randn(self.dim) * (scale_est * 0.08 * self.gscale)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            cand = np.asarray(cand, dtype=float)\n            bad = (~np.isfinite(cand))\n            if bad.any():\n                cand[bad] = self._uniform_array(lb[bad], ub[bad])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            finite_idx = np.array([np.isfinite(v) for v in self.archive_f])\n            latest_idx = len(self.archive_f) - 1\n            success = 0.0\n            if np.isfinite(f_c):\n                finite_indices = np.where(finite_idx)[0]\n                if finite_indices.size > 0:\n                    # rank among finite (smaller is better)\n                    ranks = np.argsort(np.array(self.archive_f)[finite_indices])\n                    # index of latest in rank order\n                    # find position of latest in sorted finite indices\n                    pos = np.where(finite_indices[ranks] == latest_idx)[0]\n                    if pos.size > 0:\n                        pos = pos[0]\n                        if pos < max(1, int(0.25 * finite_indices.size)):\n                            success = 1.0\n                # also count if it improves absolute best\n                best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                if latest_idx == best_idx:\n                    success = 1.0\n            else:\n                # if all infinite, any finite would have been success above; here f_c is inf -> no success\n                success = 0.0\n\n            self.success_window.append(success)\n            if success > 0:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= 6 and (self.evals % adapt_interval == 0):\n                recent = float(np.mean(self.success_window))\n                if recent > 0.28:\n                    # many successes -> contract exploration\n                    self.gscale *= (0.92 - 0.04 * self.rng.rand())\n                elif recent < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= (1.10 + 0.28 * self.rng.rand())\n                else:\n                    # slight jitter\n                    self.gscale *= (0.96 + 0.08 * self.rng.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n\n            # prune archive when too large\n            iter_since_prune += 1\n            if len(self.archive_f) > self.max_archive and iter_since_prune > 12:\n                iter_since_prune = 0\n                keep_idx = set()\n                Farr = np.array(self.archive_f)\n                # keep some best\n                idx_sorted = np.argsort(Farr)\n                n_keep_best = min(30, max(5, int(0.08 * len(Farr))))\n                for i in idx_sorted[:n_keep_best]:\n                    keep_idx.add(int(i))\n                # keep some randoms for diversity\n                remaining = [i for i in range(len(Farr)) if i not in keep_idx]\n                n_keep_rand = min(60, max(10, int(0.12 * len(Farr))))\n                if len(remaining) > 0 and n_keep_rand > 0:\n                    rnd = self.rng.choice(remaining, size=min(len(remaining), n_keep_rand), replace=False)\n                    for r_idx in rnd:\n                        keep_idx.add(int(r_idx))\n                # always keep the very best\n                if len(idx_sorted) > 0:\n                    keep_idx.add(int(idx_sorted[0]))\n                # finalize kept in sorted order for determinism\n                keep_idx = sorted(list(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            recent_rate = float(np.mean(self.success_window)) if len(self.success_window) > 0 else 0.0\n            if recent_rate < 0.06 and self.stagnation_counter >= 8 and self.evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random if none)\n                try:\n                    best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                    center = self.archive_x[best_idx].copy()\n                except Exception:\n                    center = self._uniform_array(lb, ub)\n                n_cloud = min(12, max(6, int(0.06 * (self.budget - self.evals))))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    # sample around center with increasing variance\n                    rad = max(0.02, 0.12 * (1.0 + self.rng.rand()))\n                    x = center + self.rng.randn(self.dim) * (scale_est * rad * (1.0 + 0.6 * self.rng.rand()))\n                    x = self._reflect_bounds(x, lb, ub)\n                    f = safe_eval(x)\n                    self.archive_x.append(x.copy())\n                    self.archive_f.append(f)\n                    self.success_window.append(1.0 if np.isfinite(f) else 0.0)\n                # also add a few global randoms to regain diversity\n                n_global = min(8, max(3, int(0.03 * (self.budget - self.evals))))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    x = self._uniform_array(lb, ub)\n                    f = safe_eval(x)\n                    self.archive_x.append(x.copy())\n                    self.archive_f.append(f)\n                    self.success_window.append(1.0 if np.isfinite(f) else 0.0)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.04 + 0.18 * self.rng.rand())\n                self.stagnation_counter = 0\n                # keep some memory of recent successes but not too many\n                while len(self.success_window) > 20:\n                    self.success_window.popleft()\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        Farr = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n        Xarr = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n        if Farr.size == 0:\n            # worst case: sample one uniform if budget 0 or nothing evaluated\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x\n        finite_mask = np.isfinite(Farr)\n        if finite_mask.any():\n            idx = int(np.nanargmin(np.where(finite_mask, Farr, np.inf)))\n            return float(Farr[idx]), np.asarray(Xarr[idx], dtype=float)\n        else:\n            # all infinite, return the least-inf (first)\n            idx = int(np.nanargmin(Farr))\n            return float(Farr[idx]), np.asarray(Xarr[idx], dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 32, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=60)", "error": "In the code, line 32, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=60)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6d113637-995d-48f2-9f45-b09b03cd94b9", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=400, success_win_len=24, prune_every=120):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed) if seed is not None else np.random\n        self.archive_max = int(archive_max)\n        self.prune_every = int(prune_every)\n        self.success_win_len = int(success_win_len)\n\n        # internal state\n        self.gscale = 1.0  # global multiplicative exploration scale\n        self.evals = 0\n        self.archive_x = []  # list of np arrays\n        self.archive_f = []  # list of floats\n        self.success_window = []\n        self.stagnation_counter = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to recover bounds from common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # Try func.bounds\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n            ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n        # Try func.lower / upper directly\n        if lb is None:\n            lb = getattr(func, \"lower\", None) or getattr(func, \"min\", None)\n        if ub is None:\n            ub = getattr(func, \"upper\", None) or getattr(func, \"max\", None)\n\n        if lb is None or ub is None:\n            # fallback to typical BBOB bound [-5,5]\n            lb = -5.0\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # if scalars, expand\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Mirror reflection until in bounds, then final clamp\n        x = x.copy()\n        # while any out of bound try reflecting; limit iterations\n        for _ in range(4):\n            below = x < lb\n            if not below.any():\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if not above.any():\n                break\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling and cap extremes.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        std_c = np.tan(np.pi * (u - 0.5))\n        # occasional per-coordinate multipliers to allow heavier jumps occasionally\n        mult = 1.0 + 2.0 * (self.rng.rand(self.dim) < 0.12) * self.rng.rand(self.dim)\n        raw = std_c * mult\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        step = raw * scale_vec\n        # cap magnitude but preserve sign\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluation that counts budget and returns +inf on overshoot\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                f = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # initialization: stratified LHS-ish + uniform mix\n        # choose initial population size relative to dim and budget\n        max_init = max(20, 4 * self.dim)\n        n_init = int(min(max_init, max(8, self.budget // 20)))\n        n_init = max(8, n_init)\n        n_init = min(n_init, self.budget)  # can't exceed budget\n        # LHS-like sampling\n        perm = np.arange(n_init)\n        lhs = np.empty((n_init, self.dim), dtype=float)\n        for j in range(self.dim):\n            self.rng.shuffle(perm)\n            # stratified positions with small jitter\n            lhs[:, j] = (perm + self.rng.rand(n_init)) / float(n_init)\n        # scale to bounds\n        X0 = lb + lhs * (ub - lb)\n\n        # evaluate initial set\n        for i in range(n_init):\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n            if self.evals >= self.budget:\n                break\n\n        # add a few pure uniform draws for diversity (bounded by budget)\n        add_uni = min(6, max(0, self.budget - self.evals))\n        for _ in range(add_uni):\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n            if self.evals >= self.budget:\n                break\n\n        # if still no finite best, sample a few more randoms\n        finite_fs = [np.isfinite(fv) for fv in self.archive_f]\n        if not any(finite_fs):\n            extra = min(10, max(1, self.budget - self.evals))\n            for _ in range(extra):\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.archive_x.append(x.copy())\n                self.archive_f.append(f)\n                if self.evals >= self.budget:\n                    break\n\n        # mixing probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.28, 0.32, 0.30, 0.10])\n        probs = probs / probs.sum()\n        # hyperparameters\n        noise_base = 0.08\n        prune_every = max(20, self.prune_every)\n\n        # main loop\n        last_prune_at = 0\n        while self.evals < self.budget:\n            n_pop = len(self.archive_f)\n            F = np.array(self.archive_f) if n_pop > 0 else np.array([])\n            X = np.array(self.archive_x) if n_pop > 0 else np.zeros((0, self.dim))\n\n            finite_idx = np.array([np.isfinite(v) for v in F]) if n_pop > 0 else np.array([], dtype=bool)\n            n_finite = int(np.sum(finite_idx)) if n_pop > 0 else 0\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if n_finite >= 4:\n                try:\n                    Xf = X[finite_idx]\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    iqr = np.maximum(q75 - q25, 1e-9)\n                    # fallback to std where zero\n                    stds = np.std(Xf, axis=0)\n                    scale_est = np.where(iqr > 1e-9, iqr, stds)\n                    # fallback to overall width\n                    width = ub - lb\n                    scale_est = np.where(scale_est > 1e-12, scale_est, 0.5 * width)\n                except Exception:\n                    scale_est = 0.5 * (ub - lb)\n            else:\n                # too few points, use wide scale proportional to domain\n                scale_est = 0.5 * (ub - lb)\n\n            # choose anchor (center point for generating candidate)\n            if n_finite >= 3:\n                # bias toward elites\n                finite_indices = np.where(finite_idx)[0]\n                best_indices = finite_indices[np.argsort(F[finite_indices])]\n                topk = max(1, int(min(len(best_indices), max(2, int(0.12 * max(10, n_pop))))))\n                anchor_idx = int(self.rng.choice(best_indices[:topk]))\n                anchor = X[anchor_idx].copy()\n            else:\n                anchor = self._uniform_array(lb, ub)\n\n            # choose a strategy\n            r = self.rng.rand()\n            cum = np.cumsum(probs)\n\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0] and n_pop >= 4:\n                # pick three distinct indices not equal to anchor_idx when possible\n                ids = list(range(n_pop))\n                # try to favor finite points if available\n                if n_finite >= 3:\n                    pool = list(np.where(finite_idx)[0])\n                    if len(pool) >= 3:\n                        a, b, c = self.rng.choice(pool, size=3, replace=False)\n                    else:\n                        a, b, c = self.rng.choice(ids, size=3, replace=False)\n                else:\n                    a, b, c = self.rng.choice(ids, size=3, replace=False)\n                Fscale = 0.8 * (1.2 + 0.6 * self.rng.rand())  # differential factor\n                donor = X[a] + Fscale * (X[b] - X[c])\n                # small gaussian jitter scaled by scale_est\n                jitter = self.rng.randn(self.dim) * (0.12 * scale_est * self.gscale)\n                cand = donor + jitter\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1]:\n                if n_finite >= 4:\n                    try:\n                        elite_k = max(3, min(n_finite, int(0.20 * n_pop)))\n                        idxs = np.argsort(F)[:elite_k]\n                        elite = X[idxs]\n                        base = elite.mean(axis=0)\n                        cov = np.cov(elite, rowvar=False)\n                        # regularize cov\n                        cov += 1e-8 * np.eye(self.dim)\n                        vals, vecs = np.linalg.eigh(cov)\n                        # sort descending\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                        explained = np.cumsum(vals) / np.sum(vals)\n                        k = int(np.searchsorted(explained, 0.85) + 1)\n                        k = min(k, self.dim)\n                        # sample coefficients along top components\n                        coeffs = self.rng.randn(k) * np.sqrt(np.maximum(vals[:k], 1e-12)) * (0.6 * self.gscale)\n                        perturb = vecs[:, :k] @ coeffs\n                        cand = base + perturb\n                        # occasionally bias toward anchor a bit\n                        if self.rng.rand() < 0.2:\n                            cand = 0.6 * cand + 0.4 * anchor\n                    except Exception:\n                        cand = anchor + self.rng.randn(self.dim) * (scale_est * 0.6 * self.gscale)\n                else:\n                    cand = anchor + self.rng.randn(self.dim) * (scale_est * 0.6 * self.gscale)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                # choose a random anisotropic mask to perturb a subset of dims more\n                mask = (self.rng.rand(self.dim) < 0.6)\n                if not mask.any():\n                    mask[self.rng.randint(self.dim)] = True\n                noise = self.rng.randn(self.dim) * (noise_base * (0.8 + 0.6 * self.rng.rand()) * scale_est * self.gscale)\n                cand = anchor + noise * mask.astype(float)\n                # occasional small uniform mix-in for diversity\n                if self.rng.rand() < 0.12:\n                    cand = 0.85 * cand + 0.15 * self._uniform_array(lb, ub)\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                # center at global best sometimes\n                if n_finite >= 1 and self.rng.rand() < 0.6:\n                    best_idx = int(np.nanargmin(F))\n                    center = X[best_idx]\n                else:\n                    center = self._uniform_array(lb, ub)\n                step = self._tempered_cauchy(scale_est, cap_multiplier=6.0) * (1.2 * self.gscale)\n                cand = center + step\n                # occasionally mix small differential from two archive points\n                if n_pop >= 2 and self.rng.rand() < 0.15:\n                    a, b = self.rng.choice(range(n_pop), size=2, replace=False)\n                    cand += 0.06 * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if n_finite >= 1 and self.rng.rand() < 0.05:\n                best_idx = int(np.nanargmin(F))\n                cand = 0.7 * cand + 0.3 * X[best_idx]\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = self._uniform_array(lb, ub)[bad]\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            if np.isfinite(f_c) and n_finite >= 1:\n                current_best = np.nanmin(F) if n_pop > 0 else np.inf\n                # recompute finite flags including the new point temporarily\n                Fnew = np.array(self.archive_f)\n                finite_new = np.array([np.isfinite(v) for v in Fnew])\n                # rank of latest among finite (smaller is better)\n                ranks = np.argsort(Fnew[finite_new])\n                # find position of last index among those\n                finite_idxs = np.where(finite_new)[0]\n                last_idx = len(Fnew) - 1\n                pos = np.where(finite_idxs[ranks] == last_idx)[0]\n                if pos.size > 0:\n                    rankpos = pos[0]\n                    success = (rankpos < max(1, int(0.25 * sum(finite_new)))) or (f_c < current_best)\n                else:\n                    success = (f_c < current_best)\n            elif np.isfinite(f_c) and n_finite == 0:\n                # if all previous were infinite, any finite is success\n                success = True\n            else:\n                success = False\n\n            self.success_window.append(1 if success else 0)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if (self.evals % max(3, int(0.07 * max(20, self.budget)))) == 0:\n                successes = sum(self.success_window)\n                if successes >= max(1, int(0.48 * len(self.success_window))):\n                    # many successes -> contract slightly (focus)\n                    self.gscale *= (0.88 + 0.08 * self.rng.rand())\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n                elif successes <= max(0, int(0.12 * len(self.success_window))):\n                    # too few successes -> expand\n                    self.gscale *= (1.05 + 0.20 * self.rng.rand())\n                    self.stagnation_counter += 1\n                else:\n                    # slight jitter\n                    self.gscale *= (0.95 + 0.1 * self.rng.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-3, 1e3))\n\n            # prune archive when too large\n            if len(self.archive_f) > self.archive_max and (self.evals - last_prune_at) >= prune_every:\n                last_prune_at = self.evals\n                try:\n                    Farr = np.array(self.archive_f)\n                    finite_mask = np.isfinite(Farr)\n                    # always keep some best\n                    keep_idx = set()\n                    if finite_mask.any():\n                        best_idxs = np.argsort(Farr[finite_mask])[:max(3, int(0.06 * np.sum(finite_mask)))]\n                        # map back to indices\n                        fin_indices = np.where(finite_mask)[0]\n                        for bi in best_idxs:\n                            keep_idx.add(int(fin_indices[bi]))\n                        # always keep very best\n                        keep_idx.add(int(np.where(finite_mask)[0][np.argmin(Farr[finite_mask])]))\n                    # keep some recent ones\n                    recent = list(range(max(0, len(Farr) - 40), len(Farr)))\n                    for idx in recent:\n                        if len(keep_idx) >= 30:\n                            break\n                        keep_idx.add(int(idx))\n                    # keep some randoms for diversity\n                    remaining = [i for i in range(len(Farr)) if i not in keep_idx]\n                    nrand = min(max(8, int(0.08 * len(Farr))), len(remaining))\n                    if nrand > 0:\n                        rand_keep = self.rng.choice(remaining, size=nrand, replace=False)\n                        for r_idx in rand_keep:\n                            keep_idx.add(int(r_idx))\n                    # finalize kept: sort and reconstruct lists\n                    keep_list = sorted(list(keep_idx))\n                    new_x = [self.archive_x[i] for i in keep_list]\n                    new_f = [self.archive_f[i] for i in keep_list]\n                    self.archive_x = new_x\n                    self.archive_f = new_f\n                except Exception:\n                    # fallback: keep the best half\n                    order = np.argsort(np.array(self.archive_f))\n                    keep = order[: max(2, len(order)//2)]\n                    self.archive_x = [self.archive_x[i] for i in keep]\n                    self.archive_f = [self.archive_f[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            if self.stagnation_counter >= 12:\n                # micro-restart: local cloud around current best (or random if none)\n                if len(self.archive_f) > 0:\n                    try:\n                        best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                        center = self.archive_x[best_idx].copy()\n                    except Exception:\n                        center = self._uniform_array(lb, ub)\n                else:\n                    center = self._uniform_array(lb, ub)\n                # create a few concentrated samples around center\n                n_restart = min(12, max(4, self.dim + 2))\n                for _ in range(n_restart):\n                    sigma = 0.12 + 0.5 * self.rng.rand()\n                    cand = center + self.rng.randn(self.dim) * (sigma * scale_est * (1.0 + 0.5 * self.rng.rand()))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                    if self.evals >= self.budget:\n                        break\n                # also add a few global randoms to regain diversity\n                for _ in range(6):\n                    x = self._uniform_array(lb, ub)\n                    f = safe_eval(x)\n                    self.archive_x.append(x.copy())\n                    self.archive_f.append(f)\n                    if self.evals >= self.budget:\n                        break\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.2 + 0.5 * self.rng.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-3, 1e3))\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n\n        F = np.array(self.archive_f)\n        if np.isfinite(F).any():\n            best_idx = int(np.nanargmin(F))\n        else:\n            # all infinite, return first\n            best_idx = 0\n\n        return float(F[best_idx]), self.archive_x[best_idx].copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 42, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "error": "In the code, line 42, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ddc6fb09-363d-4b7d-afed-b91ee1717bdb", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 prune_every=200, n_init=20, success_window_len=50):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # internal adaptive parameters\n        self.gscale = 1.0      # global exploration scaling\n        self.success_window_len = int(success_window_len)\n        self.success_window = [0] * self.success_window_len\n\n        # bookkeeping\n        self.evals = 0\n        self.archive_x = []   # list of np.arrays (shape dim,)\n        self.archive_f = []   # list of floats\n\n        # housekeeping params\n        self.prune_every = int(prune_every)\n        self.n_init = int(n_init)\n\n        # safety\n        if self.n_init < 2:\n            self.n_init = 2\n\n    # --------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to read common attributes. Fallback to [-5,5]^dim.\n        lb = None\n        ub = None\n\n        # common patterns\n        # func.bounds.lb / func.bounds.ub\n        try:\n            b = getattr(func, 'bounds', None)\n            if b is not None:\n                lb = np.array(getattr(b, 'lb', None) if getattr(b, 'lb', None) is not None else getattr(b, 'lower', None))\n                ub = np.array(getattr(b, 'ub', None) if getattr(b, 'ub', None) is not None else getattr(b, 'upper', None))\n        except Exception:\n            lb = None\n            ub = None\n\n        # direct attributes on func\n        if lb is None or ub is None:\n            for attr in ('lb', 'ub', 'lower_bounds', 'upper_bounds', 'lower', 'upper'):\n                if lb is None and hasattr(func, attr):\n                    try:\n                        val = getattr(func, attr)\n                        lb = np.array(val)\n                    except Exception:\n                        pass\n                if ub is None and hasattr(func, attr) and attr != 'lb':\n                    try:\n                        val = getattr(func, attr)\n                        ub = np.array(val)\n                    except Exception:\n                        pass\n\n        # if any scalar to expand\n        try:\n            if lb is not None and lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub is not None and ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n        except Exception:\n            lb = None\n            ub = None\n\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        else:\n            # ensure arrays of correct length\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            if lb.shape != (self.dim,):\n                lb = np.full(self.dim, float(lb.flatten()[0]) if lb.size else -5.0)\n            if ub.shape != (self.dim,):\n                ub = np.full(self.dim, float(ub.flatten()[0]) if ub.size else 5.0)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Mirror reflection until in bounds, then final clamp\n        x = x.copy()\n        # reflect potentially multiple times if far outside\n        for _ in range(4):\n            below = x < lb\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (below.any() or above.any()):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            return self.rng.uniform(lb, ub, size=(n, self.dim))\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling and cap extremes.\n        Returns a vector of length dim.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))  # Cauchy\n        # occasional heavy multiplier\n        heavy = (self.rng.rand(self.dim) < 0.08).astype(float) * (cap_multiplier)\n        mult = 1.0 + heavy\n        step = z * (scale_vec * 0.7) * mult * self.gscale\n        # cap extremes\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12) * self.gscale\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        # small gaussian jitter\n        step += 1e-2 * self.rng.randn(self.dim) * (scale_vec + 1e-6)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # initialize counters and archive\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.success_window = [0] * self.success_window_len\n        rng = self.rng\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator: respects budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x.copy()))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # ---- initialization: stratified LHS-ish + uniform mix ----\n        n_init = min(self.n_init, max(2, self.budget // 10))\n        # Basic LHS: each dimension stratified, shuffle per-dim\n        base = np.zeros((n_init, self.dim))\n        strata = (np.arange(n_init) + rng.rand(n_init)) / n_init\n        for j in range(self.dim):\n            perm = rng.permutation(n_init)\n            base[:, j] = lb[j] + (ub[j] - lb[j]) * strata[perm]\n\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = base[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for diversity (bounded by budget)\n        for _ in range(min(5, max(0, self.budget - self.evals))):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # If still no finite best, sample a few more randoms\n        finite_mask = np.isfinite(np.array(self.archive_f, dtype=float))\n        if not finite_mask.any():\n            extra = min(10, self.budget - self.evals)\n            for _ in range(extra):\n                if self.evals >= self.budget:\n                    break\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.archive_x.append(x.copy())\n                self.archive_f.append(f)\n\n        # ensure we have at least one entry\n        if len(self.archive_x) == 0:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.30, 0.25, 0.35, 0.10])\n        probs = probs / probs.sum()\n\n        # hyperparameters\n        adapt_every = 25\n        gamma = 0.6  # differential weight baseline\n        elite_frac = 0.20\n        stagnation_tolerance = max(50, min(500, self.budget // 20))\n        last_improve_eval = self.evals\n\n        # main loop\n        loop_ctr = 0\n        while self.evals < self.budget:\n            loop_ctr += 1\n\n            N = len(self.archive_x)\n            X = np.array(self.archive_x)\n            F = np.array(self.archive_f, dtype=float)\n\n            # compute robust per-dim scale estimate from IQR where possible\n            scale_est = np.zeros(self.dim)\n            if N >= 4:\n                q75 = np.percentile(X, 75, axis=0)\n                q25 = np.percentile(X, 25, axis=0)\n                iqr = q75 - q25\n                # fallback to std where zero\n                std = np.std(X, axis=0)\n                scale_est = np.where(iqr > 1e-12, iqr, std)\n            else:\n                scale_est = np.std(X, axis=0) if N >= 2 else np.ones(self.dim) * 0.4 * (ub - lb)\n\n            # where still zero, fallback to overall width\n            zero_mask = (scale_est <= 1e-12)\n            if zero_mask.any():\n                scale_est[zero_mask] = 0.4 * (ub - lb)[zero_mask]\n\n            # wide scale if not enough points\n            if N < 6:\n                scale_est = np.maximum(scale_est, 0.4 * (ub - lb))\n\n            # choose anchor (center point for generating candidate)\n            finite_idx = np.isfinite(F)\n            if finite_idx.any():\n                # bias toward elites with softmax on negative fitness\n                ranks = np.argsort(F)\n                # choose from top fraction more often\n                n_elite = max(1, int(np.ceil(elite_frac * N)))\n                if rng.rand() < 0.6:\n                    anchor_idx = int(ranks[rng.randint(0, n_elite)])\n                else:\n                    # mixture: some chance to pick uniformly\n                    anchor_idx = int(rng.randint(0, N))\n            else:\n                anchor_idx = int(rng.randint(0, N))\n\n            base_x = X[anchor_idx].copy()\n\n            # choose a strategy\n            strat = rng.choice(4, p=probs)\n\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if strat == 0 and N >= 4:\n                # choose three distinct indices not equal to anchor\n                ids = list(range(N))\n                ids.remove(anchor_idx)\n                a, b, c = rng.choice(ids, size=3, replace=False)\n                xa, xb, xc = X[a], X[b], X[c]\n                # adaptive gamma\n                gamma_eff = gamma * (0.7 + 0.6 * rng.rand())\n                diff = (xb - xc)\n                cand = xa + gamma_eff * diff + 0.04 * rng.randn(self.dim) * (scale_est * self.gscale)\n                # small chance to mix with anchor\n                if rng.rand() < 0.12:\n                    cand = 0.5 * cand + 0.5 * base_x\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif strat == 1 and N >= 4:\n                n_elite = max(2, int(np.ceil(elite_frac * N)))\n                elites_idx = np.argsort(F)[:n_elite]\n                elites = X[elites_idx]\n                # center\n                mean_elite = elites.mean(axis=0)\n                # PCA\n                C = elites - mean_elite\n                try:\n                    U, S, Vt = np.linalg.svd(C, full_matrices=False)\n                except Exception:\n                    U = None\n                if U is not None and Vt.shape[0] > 0:\n                    # choose number of principal directions to perturb\n                    k = min(max(1, Vt.shape[0] // 2), Vt.shape[0])\n                    # combine top-k with scaled gaussian\n                    coeffs = rng.randn(k) * (0.5 + rng.rand(k) * 1.5) * self.gscale\n                    perturb = np.zeros(self.dim)\n                    for ii in range(k):\n                        dirv = Vt[ii]\n                        perturb += coeffs[ii] * dirv * (S[ii] + 1e-6)\n                    # sometimes bias base toward elite mean\n                    if rng.rand() < 0.4:\n                        base_shift = 0.6 * base_x + 0.4 * mean_elite\n                    else:\n                        base_shift = base_x\n                    cand = base_shift + perturb * 0.6\n                else:\n                    # fallback to small gaussian\n                    cand = base_x + 0.2 * rng.randn(self.dim) * (scale_est * self.gscale)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif strat == 2:\n                # anisotropic per-dim sigma derived from scale_est\n                sigma = 0.12 * (0.3 + rng.rand(self.dim) * 1.7) * (scale_est + 1e-8) * self.gscale\n                cand = base_x + rng.randn(self.dim) * sigma\n                # occasional small uniform mix-in for diversity\n                if rng.rand() < 0.06:\n                    coords = rng.rand(self.dim) < 0.08\n                    if coords.any():\n                        cand[coords] = rng.uniform(lb[coords], ub[coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                # center at global best sometimes\n                if finite_idx.any() and rng.rand() < 0.6:\n                    best_idx = int(np.nanargmin(F))\n                    center = X[best_idx]\n                else:\n                    center = base_x\n                cand = center + self._tempered_cauchy(scale_est)\n                # occasionally mix small differential for directionality\n                if N >= 3 and rng.rand() < 0.25:\n                    ids = list(range(N))\n                    ids.remove(anchor_idx)\n                    b, c = rng.choice(ids, size=2, replace=False)\n                    cand += 0.06 * (X[b] - X[c]) * self.gscale\n\n            # occasional directed small jump to current best for refinement\n            if finite_idx.any() and rng.rand() < 0.08:\n                best_idx = int(np.nanargmin(F))\n                cand = 0.6 * cand + 0.4 * X[best_idx] + rng.randn(self.dim) * (scale_est * 0.08 * self.gscale)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.asarray(cand, dtype=float)\n            bad = (~np.isfinite(cand))\n            if bad.any():\n                cand[bad] = self._uniform_array(lb[bad], ub[bad])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_cand = safe_eval(cand)\n\n            # append to archive\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_cand)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f, dtype=float)\n            finite_idx = np.isfinite(F)\n            success = 0\n            if finite_idx.any():\n                finite_vals = F[finite_idx]\n                # rank of latest among finite (lower is better)\n                # consider top 25% as success\n                threshold = np.percentile(finite_vals, 25)\n                if np.isfinite(f_cand) and f_cand <= threshold:\n                    success = 1\n                # also success if improves absolute best\n                if np.isfinite(f_cand) and f_cand <= np.nanmin(finite_vals):\n                    success = 1\n            else:\n                # if all infinite, any finite is success\n                if np.isfinite(f_cand):\n                    success = 1\n\n            # update success window\n            self.success_window.pop(0)\n            self.success_window.append(success)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if loop_ctr % adapt_every == 0:\n                recent = np.mean(self.success_window)\n                # many successes -> contract\n                if recent > 0.20:\n                    self.gscale *= 0.85\n                elif recent < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= 1.12\n                else:\n                    # slight jitter\n                    self.gscale *= (0.95 + 0.1 * rng.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n\n            # prune archive when too large\n            if len(self.archive_x) > max(200, 6 * self.dim) and (self.evals % self.prune_every == 0):\n                # keep some best and some randoms for diversity\n                F_arr = np.array(self.archive_f, dtype=float)\n                idx_sorted = np.argsort(F_arr)\n                keep = set()\n                # keep best few\n                n_keep_best = max(10, int(0.1 * len(idx_sorted)))\n                for i, idx in enumerate(idx_sorted[:n_keep_best]):\n                    keep.add(int(idx))\n                # keep some randoms\n                n_rand = max(10, int(0.05 * len(idx_sorted)))\n                rand_idx = rng.choice(idx_sorted[n_keep_best:], size=min(n_rand, max(0, len(idx_sorted) - n_keep_best)), replace=False) if len(idx_sorted) - n_keep_best > 0 else []\n                for idx in rand_idx:\n                    keep.add(int(idx))\n                # ensure we keep at least some points\n                keep = set(int(i) for i in keep)\n                if len(keep) < 20:\n                    keep.update(int(i) for i in idx_sorted[:min(len(idx_sorted), 20)])\n                self.archive_x = [self.archive_x[i] for i in sorted(keep)]\n                self.archive_f = [self.archive_f[i] for i in sorted(keep)]\n\n            # stagnation detection & micro-restarts\n            current_best_idx = int(np.nanargmin(np.array(self.archive_f, dtype=float)))\n            current_best = self.archive_x[current_best_idx]\n            # if no improvement for many evals -> micro-restart\n            if (self.evals - last_improve_eval) > stagnation_tolerance:\n                # perform micro-restart: local cloud around current best (or random if none)\n                n_local = min(30, max(6, self.dim * 3))\n                for _ in range(n_local):\n                    if self.evals >= self.budget:\n                        break\n                    # local cloud with moderate spread\n                    x_new = current_best + rng.randn(self.dim) * (scale_est * (0.2 + rng.rand() * 0.8) * self.gscale)\n                    x_new = self._reflect_bounds(x_new, lb, ub)\n                    f_new = safe_eval(x_new)\n                    self.archive_x.append(x_new.copy())\n                    self.archive_f.append(f_new)\n                # add a few global randoms to regain diversity\n                for _ in range(min(10, max(2, self.dim))):\n                    if self.evals >= self.budget:\n                        break\n                    xg = self._uniform_array(lb, ub)\n                    fg = safe_eval(xg)\n                    self.archive_x.append(xg.copy())\n                    self.archive_f.append(fg)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= 1.3\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n                self.success_window = [0] * self.success_window_len\n                last_improve_eval = self.evals\n\n            # update last_improve_eval if we improved global best\n            if np.isfinite(f_cand):\n                prev_best = np.nanmin(F[:-1]) if len(F) > 1 else np.inf\n                if f_cand < prev_best:\n                    last_improve_eval = self.evals\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        F = np.array(self.archive_f, dtype=float)\n        X = np.array(self.archive_x)\n        finite_mask = np.isfinite(F)\n        if finite_mask.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), X[best_idx].copy()\n        else:\n            # worst case: return the least-inf (first) or a random sample\n            if len(F) > 0:\n                best_idx = int(np.nanargmin(F))\n                return float(F[best_idx]), X[best_idx].copy()\n            else:\n                # no evaluations made - sample uniform fallback\n                x = self._uniform_array(lb, ub)\n                return float(np.inf), x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "721813e9-7be3-4db3-97fd-56965c2a9ca1", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-like recombination, PCA-guided elite perturbations, anisotropic local Gaussians, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_size=1000, init_scale=0.1):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.archive_size = int(archive_size)\n        self.init_scale = float(init_scale)\n\n        # internal run state (initialized in __call__)\n        self.evals = 0\n        self.X = []   # list of np arrays\n        self.F = []   # list of floats\n        self.lb = np.full(self.dim, -5.0)\n        self.ub = np.full(self.dim, 5.0)\n\n    # --------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to recover bounds from common patterns, fallback to [-5,5]^dim\n        try:\n            if hasattr(func, \"bounds\"):\n                b = getattr(func, \"bounds\")\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                    if lb.size == 1:\n                        lb = np.full(self.dim, lb.item())\n                        ub = np.full(self.dim, ub.item())\n                    return lb, ub\n            # sometimes func has attributes .lb and .ub\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n                if lb.size == 1:\n                    lb = np.full(self.dim, lb.item())\n                    ub = np.full(self.dim, ub.item())\n                return lb, ub\n        except Exception:\n            pass\n        # default BBOB domain\n        return np.full(self.dim, -5.0), np.full(self.dim, 5.0)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Mirror reflections until in bounds, then final clamp\n        x = x.copy()\n        # perform a few iterations of reflection to handle far excursions\n        for _ in range(4):\n            below = x < lb\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n            # break early if inside\n            if not below.any() and not above.any():\n                break\n        # final clamp (defensive)\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            return self.rng.uniform(0.0, 1.0, size=(n, self.dim)) * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling and cap extremes.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # occasional heavy multiplier per-coordinate\n        heavy = (self.rng.rand(self.dim) < 0.02).astype(float)\n        heavy_mult = 1.0 + heavy * (1.0 + self.rng.exponential(scale=2.0, size=self.dim))\n        raw = c * scale_vec * heavy_mult\n        cap = cap_multiplier * scale_vec\n        # avoid zero caps: if cap==0 then use small epsilon\n        cap = np.where(cap <= 0, 1e-8, cap)\n        clipped = np.sign(raw) * np.minimum(np.abs(raw), cap)\n        return clipped\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # initialize counters and archive\n        self.evals = 0\n        self.X = []\n        self.F = []\n\n        # bounds\n        lb, ub = self._get_bounds(func)\n        if lb.size != self.dim:\n            lb = np.full(self.dim, lb.item() if lb.size == 1 else -5.0)\n        if ub.size != self.dim:\n            ub = np.full(self.dim, ub.item() if ub.size == 1 else 5.0)\n        self.lb = lb\n        self.ub = ub\n        width = ub - lb\n        # safe eval wrapper\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return None\n            try:\n                f = func(np.asarray(x, dtype=float))\n            except Exception:\n                # if evaluation fails, treat as +inf\n                f = float(\"inf\")\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: stratified LHS-ish + uniform mix ----\n        # Choose initial size with respect to dim and budget\n        n_init = int(min(max(8, 4 * self.dim), max(1, self.budget // 10)))\n        n_init = max(n_init, 4)\n        # stratified LHS-like\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        centers = (strata[:-1] + strata[1:]) / 2.0\n        # generate per-dim shuffled centers\n        S = np.empty((n_init, self.dim))\n        for j in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            # jitter within each stratum a little\n            jitter = (self.rng.rand(n_init) - 0.5) * (1.0 / n_init)\n            vals = centers + jitter\n            vals = np.minimum(np.maximum(vals, 0.0), 1.0)\n            S[:, j] = vals[perm]\n        X0 = lb + S * (ub - lb)\n        # evaluate initial points or as many as budget allows\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            f = safe_eval(X0[i])\n            if f is None:\n                break\n            self.X.append(np.asarray(X0[i], dtype=float))\n            self.F.append(f)\n\n        # Add a few pure uniform draws for diversity (bounded by budget)\n        n_uniform = min(max(2, self.dim // 2), max(0, (self.budget - self.evals) // 50 + 1))\n        for _ in range(n_uniform):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            if f is None:\n                break\n            self.X.append(np.asarray(x, dtype=float))\n            self.F.append(f)\n\n        # If still no finite best, sample a few more randoms (guard)\n        finite_mask = [np.isfinite(ff) for ff in self.F]\n        if len(self.F) == 0 or (not any(finite_mask) and self.evals < self.budget):\n            extra = min(8, max(1, self.budget - self.evals))\n            for _ in range(extra):\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                if f is None:\n                    break\n                self.X.append(np.asarray(x, dtype=float))\n                self.F.append(f)\n                if np.isfinite(f):\n                    break\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        p_de = 0.35\n        p_pca = 0.25\n        p_local = 0.30\n        p_cauchy = 0.10\n\n        # hyperparameters\n        F_de = 0.7  # differential weight\n        p_elite = 0.2  # elite fraction for anchors / PCA\n        gscale = np.mean(width) * self.init_scale  # global exploratory scale\n        success_window = []  # recent boolean successes\n        success_win_len = 20\n        stagnation_counter = 0\n        best_so_far = None\n        best_f_so_far = float(\"inf\")\n\n        # main loop\n        while self.evals < self.budget:\n            N = len(self.F)\n            # compute robust per-dim scale estimate from IQR where possible\n            if N >= 4:\n                Xarr = np.vstack(self.X)\n                q75 = np.percentile(Xarr, 75, axis=0)\n                q25 = np.percentile(Xarr, 25, axis=0)\n                iqr = (q75 - q25) / 1.349  # approx robust std\n                per_dim_scale = iqr\n                # fallback to std where zero\n                std = np.std(Xarr, axis=0)\n                per_dim_scale = np.where(per_dim_scale <= 0, std, per_dim_scale)\n            elif N >= 2:\n                Xarr = np.vstack(self.X)\n                std = np.std(Xarr, axis=0)\n                per_dim_scale = std\n            else:\n                per_dim_scale = np.maximum(width * 0.05, 1e-8)\n\n            # finally fallback to overall width wherever zero\n            per_dim_scale = np.where(per_dim_scale <= 0, width * 0.05 + 1e-8, per_dim_scale)\n            # wide scale if not enough points\n            if N < 6:\n                per_dim_scale = np.maximum(per_dim_scale, width * 0.15)\n\n            # choose anchor (center point for generating candidate)\n            healthy_finite = [np.isfinite(f) for f in self.F]\n            finite_idxs = [i for i, ff in enumerate(healthy_finite) if ff]\n            if len(finite_idxs) == 0:\n                # no finite evaluation yet: pick a random anchor\n                anchor_idx = None\n                anchor = self._uniform_array(lb, ub)\n            else:\n                # bias toward elites\n                K = max(1, int(np.ceil(len(finite_idxs) * p_elite)))\n                # get indices sorted by f\n                finite_vals = [(self.F[i], i) for i in finite_idxs]\n                finite_vals.sort(key=lambda t: t[0])\n                elite_idxs = [t[1] for t in finite_vals[:K]]\n                # choose anchor: mostly from elites, sometimes random finite\n                if self.rng.rand() < 0.75:\n                    anchor_idx = self.rng.choice(elite_idxs)\n                else:\n                    anchor_idx = self.rng.choice(finite_idxs)\n                anchor = np.array(self.X[anchor_idx], dtype=float)\n\n            # choose a strategy\n            r = self.rng.rand()\n            if r < p_de:\n                strategy = \"de\"\n            elif r < p_de + p_pca:\n                strategy = \"pca\"\n            elif r < p_de + p_pca + p_local:\n                strategy = \"local\"\n            else:\n                strategy = \"cauchy\"\n\n            xnew = None\n\n            # Strategy 1: DE-style differential injection\n            if strategy == \"de\":\n                if N >= 3:\n                    # choose three distinct indices not equal to anchor (if anchor is an index)\n                    indices = list(range(N))\n                    if anchor_idx is not None and anchor_idx in indices:\n                        indices.remove(anchor_idx)\n                    if len(indices) >= 3:\n                        a, b, c = self.rng.choice(indices, size=3, replace=False)\n                        xa = np.asarray(self.X[a])\n                        xb = np.asarray(self.X[b])\n                        xc = np.asarray(self.X[c])\n                    else:\n                        # fallback: choose with replacement\n                        sel = self.rng.randint(0, N, size=3)\n                        xa = np.asarray(self.X[sel[0]])\n                        xb = np.asarray(self.X[sel[1]])\n                        xc = np.asarray(self.X[sel[2]])\n                    donor = xa + F_de * (xb - xc)\n                    # mix with anchor\n                    mix_mask = self.rng.rand(self.dim) < (0.6 + 0.3 * self.rng.rand())\n                    xnew = anchor.copy()\n                    xnew[mix_mask] = donor[mix_mask]\n                    # small jitter anisotropic\n                    jitter = (self.rng.randn(self.dim) * per_dim_scale) * (gscale / (np.mean(per_dim_scale) + 1e-12))\n                    # with some probability, complement with uniform noise\n                    if self.rng.rand() < 0.08:\n                        jitter += (self._uniform_array(self.lb, self.ub) - (self.lb + self.ub) / 2.0) * 0.01\n                    xnew = xnew + jitter\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif strategy == \"pca\":\n                if len(finite_idxs) >= 4:\n                    Xarr = np.vstack([self.X[i] for i in finite_idxs])\n                    # choose elites\n                    vals_idx = [(self.F[i], i) for i in finite_idxs]\n                    vals_idx.sort(key=lambda t: t[0])\n                    n_elite = max(2, min(len(vals_idx), int(max(2, np.ceil(len(vals_idx) * p_elite)))))\n                    elite_indices = [t[1] for t in vals_idx[:n_elite]]\n                    X_elite = np.vstack([self.X[i] for i in elite_indices])\n                    # PCA\n                    Xm = X_elite - X_elite.mean(axis=0)\n                    cov = np.dot(Xm.T, Xm) / max(1, Xm.shape[0] - 1)\n                    # small regularization\n                    cov += np.eye(self.dim) * (1e-8 * np.mean(np.diag(cov) + 1e-12))\n                    try:\n                        w, v = np.linalg.eigh(cov)\n                        order = np.argsort(-w)\n                        w = w[order]\n                        v = v[:, order]\n                        # sample along top components\n                        comp_count = max(1, min(self.dim, int(1 + np.searchsorted(np.cumsum(w) / (w.sum() + 1e-12), 0.9))))\n                        # produce a sample by mixing components\n                        coeffs = self.rng.randn(comp_count) * np.sqrt(np.maximum(w[:comp_count], 1e-12)) * (gscale / (np.mean(per_dim_scale) + 1e-12))\n                        perturb = np.zeros(self.dim)\n                        for j in range(comp_count):\n                            perturb += coeffs[j] * v[:, j]\n                        # base sometimes is elite mean\n                        if self.rng.rand() < 0.6:\n                            base = X_elite.mean(axis=0)\n                        else:\n                            base = anchor\n                        xnew = base + perturb\n                    except Exception:\n                        # fallback to small gaussian\n                        xnew = anchor + self.rng.randn(self.dim) * per_dim_scale * 0.5 * (gscale / (np.mean(per_dim_scale) + 1e-12))\n                else:\n                    # not enough points for PCA -> gaussian perturb\n                    xnew = anchor + self.rng.randn(self.dim) * per_dim_scale * 0.6 * (gscale / (np.mean(per_dim_scale) + 1e-12))\n\n            # Strategy 3: Local anisotropic gaussian\n            elif strategy == \"local\":\n                # anisotropic Gaussian with per-dim scale times gscale\n                sigma = per_dim_scale * (gscale / (np.mean(per_dim_scale) + 1e-12))\n                # occasionally do a very small move\n                scale_mul = 0.6 + 0.9 * self.rng.rand()\n                xnew = anchor + self.rng.randn(self.dim) * sigma * scale_mul\n                # occasional small uniform mix-in for diversity\n                if self.rng.rand() < 0.05:\n                    mask = self.rng.rand(self.dim) < 0.07\n                    rand_add = (self.rng.rand(self.dim) - 0.5) * width * 0.02\n                    xnew[mask] += rand_add[mask]\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                scale_vec = np.maximum(per_dim_scale, width * 0.02)\n                step = self._tempered_cauchy(scale_vec * (gscale / (np.mean(per_dim_scale) + 1e-12)), cap_multiplier=6.0)\n                # center at global best sometimes\n                if best_so_far is not None and self.rng.rand() < 0.6:\n                    center = best_so_far\n                elif np.isfinite(best_f_so_far) and self.rng.rand() < 0.2:\n                    center = np.asarray(self.X[np.argmin(self.F)])\n                else:\n                    center = anchor\n                xnew = center + step\n                # occasionally mix small differential\n                if self.rng.rand() < 0.08 and N >= 3:\n                    sel = self.rng.choice(max(1, N), size=2, replace=False)\n                    xnew += F_de * (np.asarray(self.X[sel[0]]) - np.asarray(self.X[sel[1]])) * 0.2\n\n            # occasional directed small jump to current best for refinement\n            if best_so_far is not None and self.rng.rand() < 0.03:\n                xnew = best_so_far + (self.rng.randn(self.dim) * per_dim_scale * 0.2)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            xnew = np.asarray(xnew, dtype=float)\n            bad = (~np.isfinite(xnew)) | np.isnan(xnew)\n            if bad.any():\n                xnew[bad] = self._uniform_array(lb[bad], ub[bad])\n\n            # reflect and clamp to bounds\n            xnew = self._reflect_bounds(xnew, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            fnew = safe_eval(xnew)\n            if fnew is None:\n                break\n            self.X.append(xnew)\n            self.F.append(fnew)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            finite_vals = [(ff, ii) for ii, ff in enumerate(self.F) if np.isfinite(ff)]\n            if len(finite_vals) == 0:\n                is_success = np.isfinite(fnew)\n            else:\n                finite_vals.sort(key=lambda t: t[0])\n                # rank among finite (0 is best)\n                fin_indices = [t[1] for t in finite_vals]\n                rank = fin_indices.index(len(self.F) - 1)  # index of latest\n                is_top_q = rank < max(1, int(np.ceil(0.25 * len(finite_vals))))\n                is_improve = np.isfinite(fnew) and (fnew < best_f_so_far)\n                is_success = is_top_q or is_improve\n\n            # update best trackers\n            if np.isfinite(fnew) and fnew < best_f_so_far:\n                best_f_so_far = fnew\n                best_so_far = xnew.copy()\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # also count success window\n            success_window.append(1 if is_success else 0)\n            if len(success_window) > success_win_len:\n                success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(success_window) >= success_win_len:\n                srate = sum(success_window) / len(success_window)\n                if srate > 0.45:\n                    gscale *= 0.85  # contract on many successes\n                elif srate < 0.15:\n                    gscale *= 1.12  # expand when failing\n                # slight jitter\n                gscale *= np.exp(self.rng.randn() * 0.02)\n                # clamp gscale to reasonable range\n                gscale = float(np.clip(gscale, np.min(width) * 1e-4, np.max(width) * 4.0))\n\n            # prune archive when too large\n            if len(self.F) > self.archive_size:\n                # keep some best\n                idxs = np.arange(len(self.F))\n                sorted_idx = np.argsort(self.F)\n                keep_best = sorted_idx[: max(1, int(self.archive_size * 0.6))]\n                # keep some randoms for diversity\n                remaining = np.setdiff1d(idxs, keep_best)\n                n_random = max(0, int(self.archive_size * 0.3))\n                if remaining.size <= n_random:\n                    keep_random = remaining\n                else:\n                    keep_random = self.rng.choice(remaining, size=n_random, replace=False)\n                keep = np.unique(np.concatenate([keep_best, keep_random]))\n                # ensure keep best (absolute best)\n                best_idx = sorted_idx[0]\n                if best_idx not in keep:\n                    if keep.size >= 1:\n                        keep[-1] = best_idx\n                    else:\n                        keep = np.array([best_idx], dtype=int)\n                # rebuild archive\n                self.X = [self.X[i] for i in keep]\n                self.F = [self.F[i] for i in keep]\n\n            # stagnation detection & micro-restarts\n            # if no new best for a while, do a micro-restart (local cloud around current best or random if none)\n            if stagnation_counter > max(20, 10 + self.dim * 2) and self.evals < self.budget:\n                # perform micro-restart\n                stagnation_counter = 0\n                # slightly increase exploration\n                gscale *= 1.6\n                gscale = float(np.clip(gscale, np.min(width) * 1e-4, np.max(width) * 8.0))\n                # pick center: current best if exists else random uniform\n                if best_so_far is not None:\n                    center = best_so_far\n                else:\n                    center = self._uniform_array(lb, ub)\n                n_cloud = min(12 + self.dim, max(4, (self.budget - self.evals) // 10))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    # local cloud samples around center\n                    scale = per_dim_scale * (gscale / (np.mean(per_dim_scale) + 1e-12)) * (0.6 + self.rng.rand() * 0.8)\n                    x = center + self.rng.randn(self.dim) * scale\n                    # more diversity in restarts: sometimes global\n                    if self.rng.rand() < 0.12:\n                        x = self._uniform_array(lb, ub)\n                    x = self._reflect_bounds(x, lb, ub)\n                    f = safe_eval(x)\n                    if f is None:\n                        break\n                    self.X.append(x)\n                    self.F.append(f)\n                # also add a few global randoms\n                n_rand = min(6, max(2, (self.budget - self.evals) // 20))\n                for _ in range(n_rand):\n                    if self.evals >= self.budget:\n                        break\n                    x = self._uniform_array(lb, ub)\n                    f = safe_eval(x)\n                    if f is None:\n                        break\n                    self.X.append(x)\n                    self.F.append(f)\n                # after restart, reset short window a bit\n                success_window = [0] * min(len(success_window), success_win_len // 2)\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.F) == 0:\n            # worst case: sample one uniform if budget 0 or nothing evaluated\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            if f is None:\n                return float(\"inf\"), np.asarray(x)\n            return f, np.asarray(x)\n        else:\n            finite_idx = [i for i, f in enumerate(self.F) if np.isfinite(f)]\n            if len(finite_idx) == 0:\n                # all infinite, return the least-inf (first)\n                best_idx = int(np.argmin(self.F))\n                return float(self.F[best_idx]), np.asarray(self.X[best_idx])\n            else:\n                best_idx = finite_idx[np.argmin([self.F[i] for i in finite_idx])]\n                return float(self.F[best_idx]), np.asarray(self.X[best_idx])", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e9b4f0a5-bba7-4fb6-b47a-3f8a303b241f", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=400, success_win_len=24, prune_every=120):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = int(archive_max)\n        self.success_win_len = int(success_win_len)\n        self.prune_every = int(prune_every)\n\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # internal state\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.evals = 0\n        self.gscale = 1.0  # global multiplicative exploration scale\n        self.stagnation_counter = 0\n\n    # --------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to recover bounds from common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n            if lb is None:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None) or None\n            if ub is None:\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None) or None\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Mirror reflections until in bounds, then final clamp\n        x = np.array(x, dtype=float)\n        for _ in range(8):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling and cap extremes.\n        \"\"\"\n        u = np.random.rand(self.dim)\n        std_c = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplier per-coordinate\n        heavy = (np.random.rand(self.dim) < 0.02).astype(float) * (1.0 + 10.0 * np.random.rand(self.dim))\n        mult = 1.0 + heavy\n        raw = std_c * mult\n        # safety cap\n        raw = np.sign(raw) * np.minimum(np.abs(raw), 40.0)\n        cap = cap_multiplier * (np.maximum(scale_vec, 1e-12))\n        step = raw * scale_vec * (0.9 + 0.2 * np.random.rand(self.dim))\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # initialize counters and archive\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n        self.success_window = []\n\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # Evaluate once, count and enforce budget\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                f = func(x)\n                f = float(f)\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # ---- initialization: stratified LHS-ish + uniform mix ----\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 12))\n        n_init = max(2, int(n_init))\n\n        base = np.zeros((n_init, self.dim))\n        jitter = np.random.rand(n_init, self.dim)\n        for j in range(self.dim):\n            perm = np.arange(n_init)\n            np.random.shuffle(perm)\n            base[:, j] = (perm + jitter[:, j]) / float(n_init)\n        X0 = lb + base * (ub - lb)\n\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for diversity (bounded by budget)\n        add_uni = min(6, max(0, (self.budget - self.evals) // 30))\n        for _ in range(add_uni):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # If still no finite best, sample a few more randoms\n        finite_vals = [np.isfinite(v) for v in self.archive_f]\n        if not any(finite_vals) and self.evals < self.budget:\n            for _ in range(min(6, (self.budget - self.evals))):\n                x = self._uniform_array(lb, ub)\n                f = safe_eval(x)\n                self.archive_x.append(x.copy())\n                self.archive_f.append(f)\n                if any(np.isfinite(v) for v in self.archive_f):\n                    break\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.28, 0.28, 0.30, 0.14])\n        probs = probs / probs.sum()\n\n        # hyperparameters\n        diff_chance = 0.14  # chance to add small differential\n        noise_base = 0.08\n        iter_since_prune = 0\n        prune_every = max(20, self.prune_every)\n\n        # main loop\n        while self.evals < self.budget:\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n\n            n_pop = len(F)\n            finite_idx = np.array([np.isfinite(v) for v in F]) if n_pop > 0 else np.array([], dtype=bool)\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if n_pop > 1 and finite_idx.any() and finite_idx.sum() >= 2:\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    scale_est = (q75 - q25) / 1.349  # approximate std from IQR\n                    stds = np.std(X[finite_idx], axis=0, ddof=0)\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = stds[zero_mask]\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = (ub[zero_mask] - lb[zero_mask]) * 0.25\n                except Exception:\n                    scale_est = 0.4 * (ub - lb)\n            else:\n                # wide scale if not enough points\n                scale_est = 0.4 * (ub - lb)\n\n            # choose anchor (center point for generating candidate)\n            anchor_idx = None\n            anchor = None\n            if n_pop == 0:\n                anchor_idx = None\n                anchor = self._uniform_array(lb, ub)\n            else:\n                # bias toward elites if we have finite evaluations\n                finite_indices = np.where(finite_idx)[0] if finite_idx.any() else np.arange(n_pop)\n                if len(finite_indices) == 0:\n                    # none finite, pick random\n                    anchor_idx = int(np.random.randint(n_pop))\n                else:\n                    fin_vals = F[finite_indices]\n                    sorted_rel = np.argsort(fin_vals)\n                    best_indices = finite_indices[sorted_rel]\n                    topk = max(1, int(min(len(best_indices), max(2, int(0.12 * n_pop)))))\n                    if np.random.rand() < 0.7:\n                        anchor_idx = int(np.random.choice(best_indices[:topk]))\n                    else:\n                        anchor_idx = int(np.random.choice(best_indices))\n                anchor = X[anchor_idx].copy()\n\n            # choose a strategy\n            r = np.random.rand()\n            cum = np.cumsum(probs)\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0]:\n                if n_pop >= 3:\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        gamma = 0.6 + 0.6 * np.random.rand()\n                        base = anchor.copy()\n                        diff = X[a] - X[b]\n                        cand = base + gamma * diff + 0.04 * np.random.randn(self.dim) * (scale_est * self.gscale)\n                    else:\n                        cand = self._uniform_array(lb, ub)\n                else:\n                    cand = self._uniform_array(lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1]:\n                if n_pop >= 4 and finite_idx.any():\n                    try:\n                        # determine elites\n                        fin_idx = np.where(finite_idx)[0]\n                        fin_vals = F[fin_idx]\n                        sorted_rel = np.argsort(fin_vals)\n                        n_elite = max(3, int(0.10 * max(10, len(fin_vals))))\n                        elite_idx = fin_idx[sorted_rel[:n_elite]]\n                        eliteX = X[elite_idx]\n                        cov = np.cov(eliteX.T) + 1e-12 * np.eye(self.dim)\n                        vals, vecs = np.linalg.eigh(cov)\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                        explained = np.cumsum(vals) / (np.sum(vals) + 1e-20)\n                        k = int(np.searchsorted(explained, 0.85) + 1)\n                        k = max(1, min(self.dim, k))\n                        coeffs = np.random.randn(k) * np.sqrt(np.maximum(vals[:k], 1e-12)) * (0.6 * self.gscale)\n                        perturb = vecs[:, :k].dot(coeffs)\n                    except Exception:\n                        perturb = np.random.randn(self.dim) * (scale_est * 0.6 * self.gscale)\n                    # sometimes bias base toward elite mean\n                    if np.random.rand() < 0.6 and 'eliteX' in locals():\n                        base = eliteX.mean(axis=0)\n                    else:\n                        base = anchor\n                    cand = base + perturb\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                prob_change = 0.25 + 0.5 * min(1.0, self.gscale)\n                mask = (np.random.rand(self.dim) < prob_change)\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise_scale = noise_base * (scale_est * self.gscale)\n                noise = np.random.randn(self.dim) * noise_scale\n                cand = anchor + noise * mask.astype(float)\n                # occasional small uniform mix-in for diversity\n                if np.random.rand() < 0.08:\n                    uni_coords = (np.random.rand(self.dim) < 0.12)\n                    if uni_coords.any():\n                        cand[uni_coords] = lb[uni_coords] + np.random.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                base = anchor.copy()\n                step = self._tempered_cauchy(scale_est * self.gscale, cap_multiplier=6.0)\n                # sometimes center at global best\n                if np.random.rand() < 0.12 and finite_idx.any():\n                    try:\n                        best_idx = int(np.nanargmin(F))\n                        base = X[best_idx].copy()\n                    except Exception:\n                        pass\n                cand = base + step\n                # occasionally mix small differential\n                if np.random.rand() < diff_chance and n_pop >= 2:\n                    ids = list(range(n_pop))\n                    if anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        cand += 0.08 * np.random.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if np.random.rand() < 0.06 and n_pop >= 1 and finite_idx.any():\n                try:\n                    best_idx = int(np.nanargmin(F))\n                    cand = 0.6 * cand + 0.4 * X[best_idx] + np.random.randn(self.dim) * (scale_est * 0.08 * self.gscale)\n                except Exception:\n                    pass\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            bad = (~np.isfinite(cand)) | (cand < lb - 1e6) | (cand > ub + 1e6)\n            if np.any(bad):\n                idxs = np.where(bad)[0]\n                cand[idxs] = lb[idxs] + np.random.rand(len(idxs)) * (ub[idxs] - lb[idxs])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f)\n            finite_idx = np.array([np.isfinite(v) for v in F])\n            latest_idx = len(F) - 1\n            success = False\n            if finite_idx.any():\n                finite_indices = np.where(finite_idx)[0]\n                fin_vals = F[finite_indices]\n                # how many finite are <= latest\n                pos = int((fin_vals <= F[latest_idx]).sum())\n                if pos <= max(1, int(0.25 * len(fin_vals))):\n                    success = True\n                # also count as success if it improves absolute best\n                try:\n                    if F[latest_idx] < np.min(fin_vals):\n                        success = True\n                except Exception:\n                    pass\n            else:\n                if np.isfinite(F[latest_idx]):\n                    success = True\n\n            self.success_window.append(1.0 if success else 0.0)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= 6 and (self.evals % 6 == 0):\n                recent = float(np.mean(self.success_window[-6:]))\n                if recent > 0.28:\n                    # many successes -> contract a bit\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n                    self.gscale *= (0.92 + 0.08 * np.random.rand())\n                elif recent < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= (1.10 + 0.12 * np.random.rand())\n                    self.stagnation_counter += 1\n                else:\n                    # slight jitter\n                    self.gscale *= (0.98 + 0.04 * np.random.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n\n            # prune archive when too large\n            iter_since_prune += 1\n            if len(self.archive_f) > self.archive_max or (iter_since_prune >= prune_every and len(self.archive_f) > 60):\n                iter_since_prune = 0\n                F = np.array(self.archive_f)\n                keep_idx = set()\n                try:\n                    best_k = max(8, int(0.18 * len(F)))\n                    idx_sorted = np.argsort(F)\n                    for i in range(min(best_k, len(idx_sorted))):\n                        keep_idx.add(int(idx_sorted[i]))\n                    # always keep the very best\n                    best_global = int(np.nanargmin(F))\n                    keep_idx.add(best_global)\n                except Exception:\n                    pass\n\n                # keep some randoms for diversity\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                if len(remaining) > 0:\n                    nrand = min(max(8, int(0.08 * len(F))), len(remaining))\n                    take = min(nrand, len(remaining))\n                    if take > 0:\n                        rnd = np.random.choice(remaining, size=take, replace=False)\n                        for r_idx in rnd:\n                            keep_idx.add(int(r_idx))\n\n                # finalize kept\n                keep_idx = sorted(set(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            recent_rate = float(np.mean(self.success_window)) if len(self.success_window) > 0 else 0.0\n            if recent_rate < 0.06 and self.stagnation_counter >= 6 and self.evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random if none)\n                if len(self.archive_f) == 0:\n                    center = self._uniform_array(lb, ub)\n                else:\n                    try:\n                        best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                        center = self.archive_x[best_idx].copy()\n                    except Exception:\n                        center = self._uniform_array(lb, ub)\n                n_cloud = min(18, max(6, (self.budget - self.evals) // 8))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    sigma = 0.4 + 0.8 * np.random.rand()\n                    cand = center + np.random.randn(self.dim) * (scale_est * sigma * self.gscale * (0.5 + np.random.rand()))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                n_global = min(8, max(2, (self.budget - self.evals) // 30))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.06 + 0.14 * np.random.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            # worst case: sample one uniform if nothing evaluated\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n\n        F = np.array(self.archive_f)\n        finite_idx = np.array([np.isfinite(v) for v in F])\n        if finite_idx.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()\n        else:\n            # all infinite, return the least-inf (first)\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3cc4ebaf-55c9-448c-9139-59102d663eb3", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-guided ADS).", "code": "import numpy as np\nimport math\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=400, success_win_len=24, prune_every=120):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = int(archive_max)\n        self.prune_every = int(prune_every)\n        self.success_win_len = int(success_win_len)\n\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # internal state\n        self.archive_x = []  # list of numpy arrays\n        self.archive_f = []  # list of floats\n        self.success_window = []\n        self.evals = 0\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        # Try to extract bounds from func if available, else default [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # try several common names\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None) or getattr(b, \"left\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None) or getattr(b, \"right\", None)\n            # also direct attributes on func\n            if lb is None:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            if ub is None:\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_iter=8):\n        x = x.copy()\n        for _ in range(max_iter):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # reflect below: x = lb + (lb - x) => x' = 2*lb - x\n            x[below] = 2.0 * lb[below] - x[below]\n            # reflect above: x' = 2*ub - x\n            x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise tempered Cauchy with occasional heavy tails\n        u = np.random.rand(self.dim)\n        std_c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        heavy_mask = (np.random.rand(self.dim) < 0.02)\n        heavy_mult = 1.0 + 10.0 * np.random.rand(self.dim) * heavy_mask.astype(float)\n        mult = 1.0 + heavy_mult * heavy_mask.astype(float)\n        raw = std_c * mult\n        cap = cap_multiplier * (np.maximum(scale_vec, 1e-12))\n        step = raw * scale_vec * (0.9 + 0.2 * np.random.rand(self.dim))\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        return step\n\n    # ---------- main call ----------\n    def __call__(self, func):\n        # reset state\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.evals = 0\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # Evaluate x if budget allows, count and return float\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                val = func(x)\n                val = float(val)\n            except Exception:\n                val = float(np.inf)\n            self.evals += 1\n            return val\n\n        # ---- initialization: LHS-ish + uniform mix ----\n        # choose n_init depending on budget and dim\n        n_init = int(max(8, min(64, math.ceil(math.sqrt(self.budget) * 2))))\n        n_init = min(n_init, max(2, self.budget // 10))\n        if n_init < 2:\n            n_init = 2\n        n_init = int(n_init)\n\n        # Latin-hypercube style initialization with jitter\n        perm_base = np.vstack([np.random.permutation(n_init) for _ in range(self.dim)]).T\n        jitter = np.random.rand(n_init, self.dim) * 0.9999\n        base = (perm_base + jitter) / float(n_init)\n        X0 = lb + base * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # add a few pure randoms for diversity\n        n_extra = min(8, max(0, (self.budget - self.evals) // 50))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # strategy mix probabilities (DE, PCA, Local, Cauchy)\n        probs = np.array([0.28, 0.28, 0.30, 0.14])\n        cum_probs = np.cumsum(probs)\n\n        noise_base = 0.08\n        diff_chance = 0.14\n        iter_since_prune = 0\n        prune_every = max(20, self.prune_every)\n\n        # main optimization loop\n        while self.evals < self.budget:\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n\n            n_pop = len(F)\n            finite_idx = np.isfinite(F) if n_pop > 0 else np.array([], dtype=bool)\n\n            # compute per-dim scale estimate (IQR fallback)\n            if n_pop >= 3 and finite_idx.sum() >= 2:\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    iqr = q75 - q25\n                    scale_est = np.maximum(iqr / 1.349, 1e-12)  # approx std from IQR\n                    # fallback to std where zero\n                    stds = np.std(X[finite_idx], axis=0)\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = stds[zero_mask]\n                    # fallback to range-based where still tiny\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = 0.2 * (ub - lb)[zero_mask]\n                except Exception:\n                    scale_est = np.maximum(0.2 * (ub - lb), 1e-6)\n            else:\n                scale_est = 0.4 * (ub - lb)\n\n            # choose anchor (bias to elites)\n            if n_pop == 0:\n                anchor = self._uniform_array(lb, ub)\n                anchor_idx = None\n            else:\n                if finite_idx.any():\n                    fin_indices = np.where(finite_idx)[0]\n                    sorted_fin = fin_indices[np.argsort(F[fin_indices])]\n                    best_indices = sorted_fin\n                else:\n                    best_indices = np.arange(n_pop)\n                # pick anchor: usually from topk elites else random\n                topk = max(2, min(len(best_indices), int(0.12 * max(10, n_pop)) + 1))\n                if np.random.rand() < 0.7 and len(best_indices) >= topk:\n                    anchor_idx = int(np.random.choice(best_indices[:topk]))\n                else:\n                    anchor_idx = int(np.random.randint(0, n_pop))\n                anchor = X[anchor_idx].copy()\n\n            # select strategy\n            r = np.random.rand()\n            cand = None\n\n            # Strategy 1: DE-style (differential injection)\n            if r <= cum_probs[0]:\n                if n_pop >= 3:\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        gamma = 0.5 + 0.9 * np.random.rand()  # difference weight\n                        diff = X[a] - X[b]\n                        base_point = anchor.copy()\n                        cand = base_point + gamma * diff + 0.04 * np.random.randn(self.dim) * (scale_est * self.gscale)\n                    else:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum_probs[1]:\n                if n_pop >= 4 and finite_idx.sum() >= 3:\n                    # select elites\n                    n_elite = max(2, min(12, int(0.08 * max(10, n_pop)) + 2))\n                    elite_idx = np.where(finite_idx)[0][np.argsort(F[finite_idx])[:n_elite]]\n                    eliteX = X[elite_idx]\n                    try:\n                        cov = np.cov(eliteX.T) + 1e-12 * np.eye(self.dim)\n                        vals, vecs = np.linalg.eigh(cov)\n                        order = np.argsort(vals)[::-1]\n                        vals = vals[order]\n                        vecs = vecs[:, order]\n                        # sample in PCA subspace: larger steps along top components\n                        comp_count = max(1, min(self.dim, int(1 + np.searchsorted(np.cumsum(vals)/np.sum(vals), 0.8))))\n                        coeffs = np.random.randn(comp_count) * (np.sqrt(vals[:comp_count]) + 1e-12)\n                        perturb = vecs[:, :comp_count].dot(coeffs)\n                        # small gaussian orthogonal component\n                        orth = 0.15 * np.random.randn(self.dim) * (scale_est * self.gscale)\n                        base_point = eliteX.mean(axis=0)\n                        cand = base_point + perturb * (0.9 + 0.4 * np.random.rand()) * self.gscale + orth\n                    except Exception:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n                else:\n                    # fallback to moderate gaussian around anchor\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum_probs[2]:\n                prob_change = 0.25 + 0.5 * min(1.0, max(0.0, self.gscale))\n                mask = (np.random.rand(self.dim) < prob_change)\n                if not mask.any():\n                    mask[np.random.randint(0, self.dim)] = True\n                noise_scale = noise_base * self.gscale * (1.0 + 0.6 * np.random.rand(self.dim))\n                noise = np.random.randn(self.dim) * (scale_est * noise_scale)\n                cand = anchor + noise * mask.astype(float)\n                # occasional small uniform mix-in for diversity\n                if np.random.rand() < 0.08:\n                    uni_coords = (np.random.rand(self.dim) < 0.12)\n                    if uni_coords.any():\n                        cand[uni_coords] = lb[uni_coords] + np.random.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                base_point = anchor.copy()\n                if np.random.rand() < 0.12 and n_pop >= 1 and finite_idx.any():\n                    best_idx = int(np.nanargmin(F))\n                    base_point = X[best_idx].copy()\n                step = self._tempered_cauchy(scale_est * self.gscale, cap_multiplier=6.0)\n                cand = base_point + step\n                # occasionally mix small differential\n                if np.random.rand() < diff_chance and n_pop >= 2:\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        cand += 0.08 * np.random.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if np.random.rand() < 0.06 and n_pop >= 1 and finite_idx.any():\n                best_idx = int(np.nanargmin(F))\n                cand = cand * 0.6 + X[best_idx] * 0.4 + 0.02 * np.random.randn(self.dim) * (scale_est * self.gscale)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            bad = (~np.isfinite(cand)) | (cand < lb - 1e6) | (cand > ub + 1e6)\n            if bad.any():\n                cand = cand.copy()\n                coords = np.where(bad)[0]\n                cand[coords] = lb[coords] + np.random.rand(coords.size) * (ub[coords] - lb[coords])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # compute success: among top 25% of finite or improves global best\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            latest_idx = len(F) - 1\n            success = False\n            if finite_idx.any():\n                fin_indices = np.where(finite_idx)[0]\n                fin_vals = F[fin_indices]\n                # rank of latest among finite (smaller is better)\n                rank_pos = (fin_vals <= F[latest_idx]).sum()\n                if rank_pos <= max(1, int(0.25 * len(fin_vals))):\n                    success = True\n                if F[latest_idx] < np.min(fin_vals):\n                    success = True\n            else:\n                # first finite is success\n                if np.isfinite(F[latest_idx]):\n                    success = True\n\n            # record success window\n            self.success_window.append(1.0 if success else 0.0)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= 6 and (self.evals % 6 == 0):\n                recent = float(np.mean(self.success_window))\n                if recent > 0.28:\n                    # many successes -> contract\n                    self.gscale *= (0.88 + 0.08 * np.random.rand())\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n                elif recent < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= (1.10 + 0.12 * np.random.rand())\n                    self.stagnation_counter += 1\n                else:\n                    # slight jitter\n                    self.gscale *= (0.98 + 0.04 * np.random.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n\n            # prune archive when too large or periodically\n            iter_since_prune += 1\n            if len(self.archive_f) > self.archive_max or (iter_since_prune >= prune_every and len(self.archive_f) > 60):\n                iter_since_prune = 0\n                F = np.array(self.archive_f)\n                keep_idx = set()\n                try:\n                    # keep some best\n                    best_k = max(8, int(0.18 * len(F)))\n                    idx_sorted = np.argsort(F)\n                    for i in range(min(best_k, len(idx_sorted))):\n                        keep_idx.add(int(idx_sorted[i]))\n                    # always keep the very best\n                    best_global = int(np.nanargmin(F))\n                    keep_idx.add(best_global)\n                except Exception:\n                    pass\n                # keep some randoms for diversity\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                if len(remaining) > 0:\n                    nrand = min(max(8, int(0.08 * len(F))), len(remaining))\n                    rnd = np.random.choice(remaining, size=nrand, replace=False)\n                    for r_idx in rnd:\n                        keep_idx.add(int(r_idx))\n                # finalize kept (sorted to preserve some determinism)\n                keep_idx = sorted(set(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            recent_rate = float(np.mean(self.success_window)) if len(self.success_window) > 0 else 0.0\n            if recent_rate < 0.06 and self.stagnation_counter >= 6 and self.evals < self.budget:\n                # micro-restart: local cloud around current best (or random center)\n                if len(self.archive_f) == 0 or not np.isfinite(self.archive_f).any():\n                    center = self._uniform_array(lb, ub)\n                else:\n                    best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                    center = self.archive_x[best_idx].copy()\n                n_cloud = min(18, max(6, (self.budget - self.evals) // 8))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    sigma = 0.6 + 0.6 * np.random.rand()\n                    cand = center + np.random.randn(self.dim) * (scale_est * sigma * self.gscale * (0.5 + np.random.rand()))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # add a few global randoms\n                n_global = min(6, max(2, (self.budget - self.evals) // 20))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales and reset small window\n                self.success_window = []\n                self.stagnation_counter = 0\n                self.gscale *= (1.05 + 0.2 * np.random.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n\n        # final best selection\n        F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n        if F.size == 0:\n            # worst-case: return a uniform random point\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n        finite_idx = np.isfinite(F)\n        if finite_idx.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx]).copy()\n        else:\n            # all infinite, return the least-inf (first)\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx]).copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 131, in __call__, the following error occurred:\nNameError: name 'math' is not defined\nOn line: n_init = int(max(8, min(64, math.ceil(math.sqrt(self.budget) * 2))))", "error": "In the code, line 131, in __call__, the following error occurred:\nNameError: name 'math' is not defined\nOn line: n_init = int(max(8, min(64, math.ceil(math.sqrt(self.budget) * 2))))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8ccc72df-e678-4482-99d6-76569c463630", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=400, success_win_len=24, prune_every=120):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_max = int(archive_max)\n        self.success_win_len = int(success_win_len)\n        self.prune_every = int(prune_every)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # internal state\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.evals = 0\n        # global scale factor for the sampler\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n\n    # --------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to recover bounds from common patterns, fallback to [-5,5]^dim\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n            else:\n                lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n                ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # Mirror reflections until in bounds, then final clamp\n        x = np.array(x, dtype=float)\n        for _ in range(8):\n            below = x < lb\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling,\n        per-coordinate multipliers, and cap extremes.\n        \"\"\"\n        scale_vec = np.maximum(np.array(scale_vec, dtype=float), 1e-12)\n        u = np.random.rand(self.dim)\n        std_c = np.tan(np.pi * (u - 0.5))  # standard Cauchy per coord\n        # mild per-coordinate occasional multiplier to allow heavier tails sometimes\n        mult = 1.0 + 0.5 * np.random.rand(self.dim)\n        raw = std_c * mult\n        # map to step using scale_vec and cap\n        cap = cap_multiplier * scale_vec\n        step = np.sign(raw) * np.minimum(np.abs(raw) * scale_vec, cap)\n        # absolute safety cap\n        abs_cap = 40.0 * np.maximum(scale_vec, 1e-3)\n        step = np.sign(step) * np.minimum(np.abs(step), abs_cap)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset state\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.evals = 0\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n\n        lb, ub = self._get_bounds(func)\n\n        def safe_eval(x):\n            # Evaluate once, count and enforce budget: do NOT call func if budget exhausted\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # ---- initialization: stratified LHS-ish + uniform mix ----\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 12))\n        n_init = max(2, int(n_init))\n\n        # Basic LHS: each dimension stratified, shuffle per-dim\n        base = np.zeros((n_init, self.dim))\n        for j in range(self.dim):\n            perm = np.arange(n_init)\n            np.random.shuffle(perm)\n            # stratified positions along j\n            slots = (np.arange(n_init) + np.random.rand(n_init)) / float(n_init)\n            base[:, j] = slots[perm]\n        X0 = lb + base * (ub - lb)\n\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for diversity (bounded by budget)\n        add_uni = min(6, max(0, (self.budget - self.evals) // 30))\n        for _ in range(add_uni):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # If still no finite best, sample a few more randoms\n        finite_vals = [np.isfinite(v) for v in self.archive_f]\n        if (len(finite_vals) == 0 or not any(finite_vals)) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.34, 0.28, 0.28, 0.10])\n        probs = probs / probs.sum()\n\n        # hyperparameters\n        noise_base = 0.08\n        iter_since_prune = 0\n\n        # main loop\n        while self.evals < self.budget:\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n\n            n_pop = len(F)\n            finite_idx = np.array([np.isfinite(v) for v in F]) if n_pop > 0 else np.array([], dtype=bool)\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if finite_idx.any() and finite_idx.sum() >= 2:\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    iqr = q75 - q25\n                    scale_est = iqr / 1.349  # approx std from IQR\n                    # fallback to std where zero\n                    stds = np.std(X[finite_idx], axis=0)\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = stds[zero_mask]\n                    # fallback to overall width where still zero\n                    zero_mask = (scale_est < 1e-12)\n                    if zero_mask.any():\n                        scale_est[zero_mask] = 0.2 * (ub - lb)[zero_mask]\n                except Exception:\n                    scale_est = 0.2 * (ub - lb)\n            else:\n                # wide scale if not enough points\n                scale_est = 0.5 * (ub - lb)\n\n            # choose anchor (center point for generating candidate)\n            if n_pop == 0 or not finite_idx.any():\n                anchor_idx = None\n                anchor = self._uniform_array(lb, ub)\n            else:\n                finite_indices = np.where(finite_idx)[0]\n                sorted_idx = finite_indices[np.argsort(F[finite_indices])]\n                best_indices = sorted_idx  # ordered from best to worse among finite\n                # choose anchor with bias toward elites\n                topk = max(1, int(min(len(best_indices), max(2, int(0.12 * n_pop)))))\n                if np.random.rand() < 0.6:\n                    # pick from topk\n                    anchor_idx = int(np.random.choice(best_indices[:topk]))\n                else:\n                    anchor_idx = int(np.random.choice(np.arange(n_pop)))\n                anchor = X[anchor_idx].copy()\n\n            # choose a strategy\n            r = np.random.rand()\n            cum = np.cumsum(probs)\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0]:\n                if n_pop >= 3:\n                    # choose two distinct indices not equal to anchor\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        gamma = 0.6 + 0.6 * np.random.rand()\n                        diff = X[a] - X[b]\n                        cand = anchor + gamma * diff + 0.04 * np.random.randn(self.dim) * (scale_est * self.gscale)\n                    else:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n                else:\n                    cand = self._uniform_array(lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1]:\n                if n_pop >= 4 and finite_idx.any():\n                    try:\n                        elite_k = max(3, int(min(len(F), max(3, int(0.12 * len(F))))))\n                        elite_idx = np.argsort(F)[:elite_k]\n                        eliteX = X[elite_idx]\n                        mean_elite = eliteX.mean(axis=0)\n                        Z = eliteX - mean_elite\n                        # PCA via SVD\n                        U, S, Vt = np.linalg.svd(Z, full_matrices=False)\n                        vecs = Vt.T  # columns are principal directions\n                        vals = S**2\n                        # choose number of components explaining ~85%\n                        explained = np.cumsum(vals) / np.sum(vals) if vals.sum() > 0 else np.ones_like(vals)\n                        k = int(np.searchsorted(explained, 0.85) + 1) if vals.sum() > 0 else 1\n                        k = max(1, min(self.dim, k))\n                        # sample coefficients along top-k PCs\n                        coeffs = np.random.randn(k) * (0.6 * self.gscale)\n                        perturb = vecs[:, :k].dot(coeffs * np.sqrt(vals[:k] + 1e-12))\n                        # sometimes bias base toward elite mean\n                        if np.random.rand() < 0.7:\n                            base = mean_elite\n                        else:\n                            base = anchor\n                        cand = base + perturb\n                    except Exception:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                prob_change = 0.25 + 0.5 * min(1.0, self.gscale)\n                mask = (np.random.rand(self.dim) < prob_change)\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise_scale = noise_base * self.gscale * (1.0 + np.random.rand(self.dim) * 0.6)\n                noise = np.random.randn(self.dim) * (scale_est * noise_scale)\n                cand = anchor + noise * mask.astype(float)\n                # occasional small uniform mix-in for diversity\n                if np.random.rand() < 0.08:\n                    uni_coords = (np.random.rand(self.dim) < 0.12)\n                    if uni_coords.any():\n                        cand[uni_coords] = lb[uni_coords] + np.random.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                base = anchor.copy()\n                if np.random.rand() < 0.12 and finite_idx.any():\n                    best_idx = int(np.nanargmin(F))\n                    base = X[best_idx].copy()\n                step = self._tempered_cauchy(scale_est * self.gscale, cap_multiplier=6.0)\n                cand = base + step\n                # occasionally mix small differential\n                if n_pop >= 3 and np.random.rand() < 0.18:\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        cand += 0.08 * np.random.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if np.random.rand() < 0.06 and n_pop >= 1 and finite_idx.any():\n                best_idx = int(np.nanargmin(F))\n                cand = 0.6 * cand + 0.4 * X[best_idx] + np.random.randn(self.dim) * (scale_est * 0.08 * self.gscale)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.array(cand, dtype=float)\n            bad = (~np.isfinite(cand)) | (cand < lb - 1e6) | (cand > ub + 1e6)\n            if bad.any():\n                cand[bad] = lb[bad] + np.random.rand(bad.sum()) * (ub[bad] - lb[bad])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f)\n            latest_idx = len(F) - 1\n            success = False\n            finite_indices = np.where(np.isfinite(F))[0]\n            if len(finite_indices) > 0:\n                fin_vals = F[finite_indices]\n                rank_pos = np.sum(fin_vals < F[latest_idx])\n                # if latest in top 25% (best means smaller)\n                if rank_pos <= max(1, int(0.25 * len(finite_indices))):\n                    success = True\n                # also count as success if it improves absolute best\n                if F[latest_idx] <= np.min(fin_vals):\n                    success = True\n            else:\n                # if all infinite, any finite is success\n                if np.isfinite(F[latest_idx]):\n                    success = True\n\n            # push into success window\n            self.success_window.append(1.0 if success else 0.0)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= 6 and (self.evals % 6 == 0):\n                recent = float(np.mean(self.success_window))\n                if recent > 0.28:\n                    # many successes -> contract exploration slightly\n                    self.gscale *= (0.9 + 0.08 * np.random.rand())\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n                elif recent < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= (1.10 + 0.12 * np.random.rand())\n                    self.stagnation_counter += 1\n                else:\n                    # small jitter\n                    self.gscale *= (0.98 + 0.04 * np.random.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n\n            # prune archive when too large\n            iter_since_prune += 1\n            if len(self.archive_f) > self.archive_max or (iter_since_prune >= self.prune_every and len(self.archive_f) > 60):\n                iter_since_prune = 0\n                F = np.array(self.archive_f)\n                keep_idx = set()\n                try:\n                    # keep some best\n                    best_k = max(8, int(0.18 * len(F)))\n                    idx_sorted = np.argsort(F)\n                    for i in range(min(best_k, len(idx_sorted))):\n                        keep_idx.add(int(idx_sorted[i]))\n                    # always keep the very best\n                    best_global = int(idx_sorted[0])\n                    keep_idx.add(best_global)\n                except Exception:\n                    pass\n\n                # keep some randoms for diversity\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                if len(remaining) > 0:\n                    take = max(6, int(0.06 * len(F)))\n                    take = min(take, len(remaining))\n                    rnd = np.random.choice(remaining, size=take, replace=False)\n                    for r_idx in rnd:\n                        keep_idx.add(int(r_idx))\n\n                # finalize kept (keep order of indices sorted)\n                keep_list = sorted(list(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_list]\n                self.archive_f = [self.archive_f[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if self.stagnation_counter >= 12:\n                # perform micro-restart: local cloud around current best (or random if none)\n                if len(self.archive_f) == 0 or not any(np.isfinite(self.archive_f)):\n                    center = self._uniform_array(lb, ub)\n                else:\n                    best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                    center = np.array(self.archive_x[best_idx]).copy()\n                n_cloud = min(18, max(6, (self.budget - self.evals) // 8))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    sigma = 0.4 + 0.8 * np.random.rand()\n                    cand = center + np.random.randn(self.dim) * (sigma * scale_est * self.gscale)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                n_global = min(6, max(2, (self.budget - self.evals) // 20))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.06 + 0.14 * np.random.rand())\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            # worst case: sample one uniform if nothing evaluated\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n\n        F = np.array(self.archive_f)\n        finite_idx = np.array([np.isfinite(v) for v in F])\n        if finite_idx.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()\n        else:\n            # all infinite, return the least-inf (first)\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), self.archive_x[best_idx].copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7bace763-0eb2-4917-91d2-7ba55b51314f", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # internal state\n        self.evals = 0\n        self.gscale = 1.0  # global multiplier for step sizes\n        self.stagnation_counter = 0\n\n    # ---- utilities ----\n    def _reflect_bounds(self, x, lb, ub):\n        # Reflect out-of-bounds coordinates until within [lb,ub]\n        x = np.array(x, dtype=float)\n        # For any coordinate < lb: reflect as lb + (lb - x)\n        # For > ub: reflect as ub - (x - ub)\n        # Do until all inside (but break after a few to avoid infinite loop)\n        for _ in range(4):\n            below = x < lb\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n            if not (below.any() or above.any()):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        # coordinate-wise tempered Cauchy\n        scale_vec = np.maximum(np.array(scale_vec, dtype=float), 1e-12)\n        u = np.random.rand(self.dim)\n        std_c = np.tan(np.pi * (u - 0.5))  # standard Cauchy per coord\n        mult = 1.0 + 0.5 * np.random.rand(self.dim)\n        raw = std_c * mult\n        cap = cap_multiplier * scale_vec\n        step = np.sign(raw) * np.minimum(np.abs(raw) * scale_vec, cap)\n        abs_cap = 40.0 * np.maximum(scale_vec, 1e-3)\n        step = np.sign(step) * np.minimum(np.abs(step), abs_cap)\n        return step\n\n    # ---- main call ----\n    def __call__(self, func):\n        # Use canonical bounds for BBOB-like problems\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n\n        # safe evaluation wrapper enforcing budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # init archive\n        self.archive_x = []\n        self.archive_f = []\n\n        # Initialization: stratified LHS-ish + a few uniform draws\n        n_init = min(max(8 + 2 * self.dim, 20), max(1, self.budget // 12))\n        n_init = max(2, int(n_init))\n        # Create LHS-like base: for each dim, permute stratified fractions\n        base = np.zeros((n_init, self.dim))\n        for j in range(self.dim):\n            perm = np.random.permutation(n_init)\n            frac = (np.arange(n_init) + np.random.rand(n_init)) / float(n_init)\n            base[:, j] = frac[perm]\n        X0 = lb + base * (ub - lb)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for diversity\n        add_uni = min(6, max(1, self.budget // 200))\n        for _ in range(add_uni):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # If still no finite best, sample a few more randoms\n        finite_vals = [np.isfinite(v) for v in self.archive_f]\n        if (len(finite_vals) == 0 or not any(finite_vals)) and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Strategy probabilities: order [DE, PCA, Local, Cauchy]\n        probs = np.array([0.30, 0.25, 0.30, 0.15])\n        probs = probs / probs.sum()\n\n        noise_base = 0.08\n        success_win_len = 24\n        self.success_window = deque(maxlen=success_win_len)\n        iter_since_prune = 0\n        max_archive = max(200, 20 * self.dim)\n        prune_every = 60 + self.dim * 2\n\n        # main loop\n        while self.evals < self.budget:\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n\n            n_pop = X.shape[0]\n            finite_idx = np.array([np.isfinite(v) for v in F]) if n_pop > 0 else np.array([], dtype=bool)\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if finite_idx.any() and finite_idx.sum() >= 3:\n                try:\n                    q25 = np.percentile(X[finite_idx], 25, axis=0)\n                    q75 = np.percentile(X[finite_idx], 75, axis=0)\n                    iqr = q75 - q25\n                    # robust estimate convert IQR to approx std (IQR ~ 1.349*std for normal)\n                    scale_est = iqr / 1.349\n                    # fallback to std where zero\n                    stds = np.std(X[finite_idx], axis=0)\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = stds[zero_mask]\n                    # fallback to overall width/6\n                    width = ub - lb\n                    scale_est[scale_est < 1e-12] = width[scale_est < 1e-12] / 6.0\n                except Exception:\n                    scale_est = (ub - lb) / 6.0\n            else:\n                # wide scale if not enough points\n                scale_est = (ub - lb) / 3.0\n\n            # choose anchor (center point for generating candidate)\n            if n_pop == 0 or not finite_idx.any():\n                anchor = self._uniform_array(lb, ub)\n                anchor_idx = None\n            else:\n                finite_indices = np.where(finite_idx)[0]\n                sorted_idx = finite_indices[np.argsort(F[finite_indices])]\n                best_indices = sorted_idx  # ordered from best to worse among finite\n                topk = max(1, int(min(len(best_indices), max(2, int(0.12 * n_pop)))))\n                if np.random.rand() < 0.6:\n                    anchor_idx = int(np.random.choice(best_indices[:topk]))\n                else:\n                    anchor_idx = int(np.random.choice(np.arange(n_pop)))\n                anchor = X[anchor_idx].copy()\n\n            # pick a strategy\n            r = np.random.rand()\n            cum = np.cumsum(probs)\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0]:\n                if n_pop >= 3 and anchor_idx is not None:\n                    ids = list(range(n_pop))\n                    ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        gamma = 0.5 + 0.8 * np.random.rand()\n                        diff = X[a] - X[b]\n                        cand = anchor + gamma * diff + np.random.randn(self.dim) * (0.3 * scale_est * self.gscale)\n                    else:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale)\n                else:\n                    cand = self._uniform_array(lb, ub)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1]:\n                if n_pop >= 4 and finite_idx.any():\n                    # pick elites\n                    elite_k = max(3, int(min(n_pop, max(3, int(0.12 * n_pop)))))\n                    elite_idx = sorted_idx[:elite_k]\n                    eliteX = X[elite_idx]\n                    mean_elite = eliteX.mean(axis=0)\n                    # PCA via SVD on centered elite\n                    Z = eliteX - mean_elite\n                    try:\n                        U, S, Vt = np.linalg.svd(Z, full_matrices=False)\n                        vecs = Vt.T\n                        vals = S ** 2\n                        explained = np.cumsum(vals) / np.sum(vals) if vals.sum() > 0 else np.ones_like(vals)\n                        k = int(np.searchsorted(explained, 0.85) + 1) if vals.sum() > 0 else 1\n                        k = max(1, min(self.dim, k))\n                        # sample coefficients along top-k PCs\n                        coeffs = np.random.randn(k) * (0.9 + 0.6 * np.random.rand(k))\n                        perturb = vecs[:, :k].dot(coeffs * np.sqrt(vals[:k] + 1e-12))\n                        # sometimes bias base toward elite mean\n                        base = mean_elite if np.random.rand() < 0.7 else anchor\n                        cand = base + (perturb * (0.6 * self.gscale))\n                    except Exception:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.9)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                mask = np.random.rand(self.dim) < (0.18 + 0.06 * np.random.rand())\n                # ensure at least one dimension changes\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise_scale = noise_base * self.gscale * (scale_est * (1.0 + 0.6 * np.random.rand(self.dim)))\n                noise = np.random.randn(self.dim) * noise_scale\n                cand = anchor + noise * mask.astype(float)\n                # occasional small uniform mix-in for diversity\n                uni_coords = (np.random.rand(self.dim) < 0.12)\n                if uni_coords.any():\n                    cand[uni_coords] = lb[uni_coords] + np.random.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                base = anchor.copy()\n                if np.random.rand() < 0.12 and finite_idx.any():\n                    best_idx = int(np.nanargmin(F))\n                    base = X[best_idx].copy()\n                step = self._tempered_cauchy(scale_est * self.gscale, cap_multiplier=6.0)\n                cand = base + step\n                # occasionally mix small differential\n                if n_pop >= 3 and anchor_idx is not None and np.random.rand() < 0.18:\n                    ids = list(range(n_pop))\n                    ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        cand += 0.08 * np.random.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if np.random.rand() < 0.035 and finite_idx.any():\n                best_idx = int(np.nanargmin(F))\n                cand = cand * 0.4 + X[best_idx] * 0.6\n\n            # sanitize candidate\n            if cand is None:\n                cand = self._uniform_array(lb, ub)\n            cand = np.array(cand, dtype=float)\n            bad = (~np.isfinite(cand)) | (cand < lb - 1e6) | (cand > ub + 1e6)\n            if bad.any():\n                # replace bad coords with uniform\n                coords = np.where(bad)[0]\n                cand[coords] = lb[coords] + np.random.rand(coords.size) * (ub[coords] - lb[coords])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f)\n            success = False\n            finite_indices = np.where(np.isfinite(F))[0]\n            if len(finite_indices) > 0:\n                fin_vals = F[finite_indices]\n                latest_idx = len(F) - 1\n                # top 25% means having rank within 25% of finite count (lower is better)\n                rank = np.sum(fin_vals < F[latest_idx])\n                if rank <= max(0, int(0.25 * len(fin_vals))):\n                    success = True\n                if F[latest_idx] <= np.min(fin_vals):\n                    success = True\n            else:\n                # if all infinite, any finite is success\n                if np.isfinite(F[-1]):\n                    success = True\n\n            self.success_window.append(1.0 if success else 0.0)\n            if success:\n                self.stagnation_counter = max(0, self.stagnation_counter - 1)\n            else:\n                self.stagnation_counter += 1\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= 6 and (self.evals % 6 == 0):\n                recent = float(np.mean(self.success_window))\n                if recent > 0.4:\n                    # many successes -> contract exploration slightly\n                    self.gscale *= (0.9 + 0.08 * np.random.rand())\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n                elif recent < 0.12:\n                    # too few successes -> expand\n                    self.gscale *= (1.10 + 0.12 * np.random.rand())\n                    self.stagnation_counter += 1\n                else:\n                    # small jitter\n                    self.gscale *= (0.98 + 0.04 * np.random.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-4, 1e2))\n\n            # prune archive when too large\n            iter_since_prune += 1\n            if iter_since_prune >= prune_every or len(self.archive_x) > max_archive:\n                iter_since_prune = 0\n                F = np.array(self.archive_f)\n                idx_sorted = np.argsort(F)  # smaller is better; nan/inf go to end\n                keep_idx = set()\n                # keep some best\n                best_k = max(8, int(0.18 * len(F)))\n                for i in range(min(best_k, len(idx_sorted))):\n                    keep_idx.add(int(idx_sorted[i]))\n                # always keep the very best\n                if len(idx_sorted) > 0:\n                    keep_idx.add(int(idx_sorted[0]))\n                # keep some randoms for diversity\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                if len(remaining) > 0:\n                    take = max(6, int(0.06 * len(F)))\n                    take = min(take, len(remaining))\n                    rnd = list(np.random.choice(remaining, size=take, replace=False))\n                    for rdx in rnd:\n                        keep_idx.add(int(rdx))\n                # finalize kept (keep order of indices sorted)\n                keep_list = sorted(list(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_list]\n                self.archive_f = [self.archive_f[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if self.stagnation_counter >= 12 and self.evals < self.budget:\n                # micro-restart: local cloud around current best (or random if none)\n                if len(self.archive_f) == 0 or not any(np.isfinite(self.archive_f)):\n                    center = self._uniform_array(lb, ub)\n                else:\n                    best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                    center = self.archive_x[best_idx].copy()\n                n_cloud = max(6, 6 + self.dim // 2)\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    spread = (ub - lb) * (0.06 + 0.08 * np.random.rand(self.dim)) * (1.0 + 0.8 * (self.gscale - 1.0))\n                    cand = center + np.random.randn(self.dim) * spread\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                n_global = min(6, max(2, (self.budget - self.evals) // 20))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.06 + 0.14 * np.random.rand())\n                self.stagnation_counter = 0\n                self.success_window.clear()\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            # worst case: sample one uniform if nothing evaluated\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            return f, x\n\n        F = np.array(self.archive_f)\n        finite_mask = np.isfinite(F)\n        if finite_mask.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx], dtype=float)\n        else:\n            # all infinite, return the least-inf (first)\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx], dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 135, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=success_win_len)", "error": "In the code, line 135, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=success_win_len)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5b571421-1673-462b-aea9-50659dc8dfa8", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=400, success_win_len=24, prune_every=120):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.archive_max = int(archive_max)\n        self.success_win_len = int(success_win_len)\n        self.prune_every = int(prune_every)\n\n        # internal state\n        self.evals = 0\n        # global scale factor for the sampler (multiplicative)\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n\n        # RNG\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # try several common patterns, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        # check for bounds object\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # try a few common shapes\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = lb or b.lb\n                    ub = ub or b.ub\n                elif hasattr(b, \"lower\") and hasattr(b, \"upper\"):\n                    lb = lb or b.lower\n                    ub = ub or b.upper\n        except Exception:\n            pass\n\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n\n        # broadcast scalars\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # mirror-reflection: reflect until in bounds then clamp\n        x = np.array(x, dtype=float)\n        # iterate limited times to avoid infinite loops\n        for _ in range(4):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = lb[below] + (lb[below] - x[below])\n            if above.any():\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n        if n == 1:\n            return lb + np.random.rand(self.dim) * (ub - lb)\n        else:\n            return lb + np.random.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=6.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling,\n        occasional per-coordinate multipliers, and cap extremes.\n        \"\"\"\n        scale_vec = np.maximum(np.array(scale_vec, dtype=float), 1e-12)\n        u = np.random.rand(self.dim)\n        raw = np.tan(np.pi * (u - 0.5))  # standard Cauchy per coord\n        # mild per-coordinate occasional multiplier to allow heavier tails sometimes\n        mult = 1.0 + 0.5 * np.random.rand(self.dim)\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        step = np.sign(raw) * np.minimum(np.abs(raw) * scale_vec * mult, cap)\n        # absolute safety cap\n        abs_cap = 40.0 * np.maximum(scale_vec, 1e-3)\n        step = np.sign(step) * np.minimum(np.abs(step), abs_cap)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset state\n        self.evals = 0\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.gscale = 1.0\n        self.stagnation_counter = 0\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluation wrapper: increments eval counter and prevents calls beyond budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return float(np.inf)\n            try:\n                f = float(func(np.array(x, dtype=float)))\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return f\n\n        # ---- initialization: stratified-ish LHS + uniform mix ----\n        n_init = max(4, min( max(6, 2*self.dim), self.budget//10 if self.budget>=10 else 2 ))\n        # simple per-dimension stratified sampling\n        X0 = np.empty((n_init, self.dim))\n        for j in range(self.dim):\n            perm = np.random.permutation(n_init)\n            strata = (perm + np.random.rand(n_init)) / float(n_init)\n            X0[:, j] = lb[j] + strata * (ub[j] - lb[j])\n        # Evaluate initial points (respect budget)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i].copy()\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for diversity (bounded by budget)\n        add_uni = min(6, max(0, (self.budget - self.evals) // 30))\n        for _ in range(add_uni):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # ensure we have at least something\n        if len(self.archive_x) == 0 and self.evals < self.budget:\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.35, 0.25, 0.30, 0.10], dtype=float)\n        probs = probs / probs.sum()\n        cum = np.cumsum(probs)\n\n        # hyperparameters\n        noise_base = 0.08\n        iter_since_prune = 0\n        last_prune_at = 0\n\n        # main loop\n        while self.evals < self.budget:\n            X = np.array(self.archive_x) if len(self.archive_x) > 0 else np.zeros((0, self.dim))\n            F = np.array(self.archive_f) if len(self.archive_f) > 0 else np.array([])\n\n            n_pop = len(F)\n            finite_idx = np.array([np.isfinite(v) for v in F]) if n_pop > 0 else np.array([], dtype=bool)\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if n_pop >= 4 and finite_idx.any():\n                try:\n                    Xf = X[finite_idx]\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    iqr = q75 - q25\n                    scale_est = iqr / 1.349  # approx std from IQR\n                    # fallback to std where zero\n                    stds = np.std(Xf, axis=0)\n                    zero_mask = (scale_est < 1e-12)\n                    scale_est[zero_mask] = stds[zero_mask]\n                    # fallback to overall width where still zero\n                    zero_mask = (scale_est < 1e-12)\n                    if zero_mask.any():\n                        scale_est[zero_mask] = 0.5 * (ub - lb)[zero_mask]\n                except Exception:\n                    scale_est = 0.5 * (ub - lb)\n            else:\n                # wide scale if not enough points\n                scale_est = 0.5 * (ub - lb)\n            # guard\n            scale_est = np.maximum(scale_est, 1e-12)\n\n            # choose anchor (center point for generating candidate)\n            if n_pop == 0 or not finite_idx.any():\n                anchor = self._uniform_array(lb, ub)\n                anchor_idx = None\n            else:\n                finite_indices = np.where(finite_idx)[0]\n                sorted_idx = finite_indices[np.argsort(F[finite_indices])]\n                # bias sampling towards elites: sample from top-k with exponential bias\n                topk = max(2, min(len(sorted_idx), int(max(3, 0.12 * len(sorted_idx)))))\n                probs_elite = np.exp(-np.arange(topk) / max(1.0, topk / 3.0))\n                probs_elite = probs_elite / probs_elite.sum()\n                pick = np.random.choice(topk, p=probs_elite)\n                anchor_idx = int(sorted_idx[pick])\n                anchor = X[anchor_idx].copy()\n\n            # choose a strategy\n            r = np.random.rand()\n            cand = None\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0]:\n                if n_pop >= 3:\n                    ids = list(range(n_pop))\n                    if anchor_idx is not None and anchor_idx in ids:\n                        ids.remove(anchor_idx)\n                    if len(ids) >= 2:\n                        a, b = np.random.choice(ids, size=2, replace=False)\n                        Fdiff = X[a] - X[b]\n                        # differential scale relative to problem width and gscale\n                        de_scale = 0.8 * (0.8 + 0.4 * np.random.rand()) * self.gscale\n                        cand = anchor + de_scale * Fdiff + np.random.randn(self.dim) * (0.06 * scale_est * self.gscale)\n                    else:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1]:\n                if n_pop >= 4 and finite_idx.any():\n                    try:\n                        elite_k = max(3, min(len(F), int(max(3, 0.12 * len(F)))))\n                        elite_idx = np.argsort(F)[:elite_k]\n                        eliteX = X[elite_idx]\n                        mean_elite = np.mean(eliteX, axis=0)\n                        Z = eliteX - mean_elite\n                        # PCA via SVD of centered elite matrix\n                        U, svals, Vt = np.linalg.svd(Z, full_matrices=False)\n                        vals = (svals ** 2) / max(1, eliteX.shape[0] - 1)  # approx eigenvalues\n                        explained = np.cumsum(vals) / (np.sum(vals) + 1e-18)\n                        k = int(np.searchsorted(explained, 0.85) + 1) if vals.sum() > 0 else 1\n                        k = max(1, min(self.dim, k))\n                        vecs = Vt.T  # columns are principal directions\n                        # sample coefficients along top-k PCs\n                        coeffs = np.random.randn(k) * (0.6 * self.gscale)\n                        perturb = vecs[:, :k].dot(coeffs * np.sqrt(vals[:k] + 1e-12))\n                        # sometimes bias base toward elite mean, sometimes toward anchor\n                        if np.random.rand() < 0.65:\n                            base = mean_elite\n                        else:\n                            base = anchor\n                        cand = base + perturb + np.random.randn(self.dim) * (0.03 * scale_est * self.gscale)\n                    except Exception:\n                        cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n                else:\n                    cand = anchor + np.random.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                # pick subset of coordinates to perturb\n                mask = (np.random.rand(self.dim) < 0.28)\n                if not mask.any():\n                    mask[np.random.randint(self.dim)] = True\n                noise_scale = noise_base * self.gscale * (1.0 + np.random.rand(self.dim) * 0.6)\n                local_noise = np.random.randn(self.dim) * (scale_est * noise_scale)\n                cand = anchor.copy()\n                cand[mask] += local_noise[mask]\n                # occasional small uniform mix-in for diversity\n                if np.random.rand() < 0.08:\n                    uni_coords = (np.random.rand(self.dim) < 0.12)\n                    cand[uni_coords] = lb[uni_coords] + np.random.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                # base near best occasionally\n                if n_pop >= 1 and finite_idx.any() and np.random.rand() < 0.6:\n                    best_idx = int(np.nanargmin(F))\n                    base = X[best_idx].copy()\n                else:\n                    base = self._uniform_array(lb, ub)\n                # cauchy scale uses scale_est but also relative width\n                scale_vec = np.maximum(scale_est, 0.05 * (ub - lb)) * (0.8 + 0.6 * np.random.rand())\n                cand = base + self._tempered_cauchy(scale_vec * self.gscale, cap_multiplier=6.0)\n                # occasionally add small differential to introduce directed escapes\n                if n_pop >= 3 and np.random.rand() < 0.16:\n                    ids = list(range(n_pop))\n                    a, b = np.random.choice(ids, size=2, replace=False)\n                    cand += 0.08 * np.random.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if np.random.rand() < 0.06 and n_pop >= 1 and finite_idx.any():\n                best_idx = int(np.nanargmin(F))\n                cand = 0.6 * cand + 0.4 * X[best_idx] + np.random.randn(self.dim) * (scale_est * 0.08 * self.gscale)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            cand = np.array(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = lb[bad] + np.random.rand(bad.sum()) * (ub[bad] - lb[bad])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f)\n            finite_indices = np.where(np.isfinite(F))[0] if len(F) > 0 else np.array([], dtype=int)\n            success = 0\n            if len(finite_indices) > 0:\n                latest_idx = len(F) - 1\n                fin_vals = F[finite_indices]\n                # rank position among finite (0 is best)\n                rank_pos = np.sum(fin_vals < F[latest_idx])\n                if rank_pos <= max(1, int(0.25 * len(finite_indices))):\n                    success = 1\n                # also count as success if it improves absolute best\n                if F[latest_idx] <= np.min(fin_vals):\n                    success = 1\n            else:\n                # if all infinite, any finite is success\n                if np.isfinite(F[-1]):\n                    success = 1\n\n            self.success_window.append(success)\n            if len(self.success_window) > self.success_win_len:\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= max(3, int(self.success_win_len/3)):\n                recent = float(np.mean(self.success_window))\n                if recent > 0.28:\n                    # many successes -> contract exploration slightly\n                    self.gscale *= (0.9 - 0.06 * np.random.rand())\n                elif recent < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= (1.06 + 0.12 * np.random.rand())\n                    self.stagnation_counter += 1\n                else:\n                    # small jitter\n                    self.gscale *= (0.98 + 0.04 * np.random.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-6, 12.0))\n\n            # prune archive when too large\n            iter_since_prune += 1\n            if len(self.archive_f) > self.archive_max or (iter_since_prune >= self.prune_every and len(self.archive_f) > 60):\n                iter_since_prune = 0\n                F = np.array(self.archive_f)\n                idx_sorted = np.argsort(F)\n                keep_idx = set()\n                # keep some best\n                best_k = max(8, int(0.18 * len(F)))\n                for i in range(min(best_k, len(idx_sorted))):\n                    keep_idx.add(int(idx_sorted[i]))\n                # always keep very best\n                if len(idx_sorted) > 0:\n                    keep_idx.add(int(idx_sorted[0]))\n                # keep some randoms for diversity\n                remaining = [i for i in range(len(F)) if i not in keep_idx]\n                if len(remaining) > 0:\n                    take = max(6, int(0.06 * len(F)))\n                    take = min(take, len(remaining))\n                    rnd = list(np.random.choice(remaining, size=take, replace=False))\n                    for r_idx in rnd:\n                        keep_idx.add(int(r_idx))\n                # finalize kept (sorted)\n                keep_list = sorted(list(keep_idx))\n                self.archive_x = [self.archive_x[i] for i in keep_list]\n                self.archive_f = [self.archive_f[i] for i in keep_list]\n\n            # stagnation detection & micro-restarts\n            if self.stagnation_counter >= 10 and self.evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random if none)\n                if len(self.archive_f) == 0 or not any(np.isfinite(self.archive_f)):\n                    center = self._uniform_array(lb, ub)\n                else:\n                    best_idx = int(np.nanargmin(np.array(self.archive_f)))\n                    center = np.array(self.archive_x[best_idx]).copy()\n                n_cloud = min(18, max(6, (self.budget - self.evals) // 8))\n                sigma = 1.2 + 0.6 * np.random.rand()\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    cand = center + np.random.randn(self.dim) * (sigma * scale_est * self.gscale)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                n_global = min(6, max(2, (self.budget - self.evals) // 20))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.06 + 0.14 * np.random.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-6, 12.0))\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        if len(self.archive_f) == 0:\n            # worst case: sample one uniform if nothing evaluated\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n\n        F = np.array(self.archive_f)\n        finite_mask = np.isfinite(F)\n        if finite_mask.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx], dtype=float).copy()\n        else:\n            # all infinite: return the least-inf (first)\n            best_idx = int(np.argmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx], dtype=float).copy()", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "26d0292c-b6be-4264-bf03-d06760a2a4a6", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 success_win_len=50, prune_every=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # adaptation / bookkeeping\n        self.gscale = 1.0              # global multiplicative exploration scale\n        self.stagnation_counter = 0\n        self.success_win_len = int(success_win_len)\n        self.prune_every = int(prune_every)\n\n        # archive\n        self.archive_x = []   # list of numpy arrays\n        self.archive_f = []   # list of scalars\n        self.evals = 0\n\n        # sliding success window (0/1)\n        self.success_window = []\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # allow func to optionally expose bounds through common attributes\n        try:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        # if bounds given as object with .lb/.ub or .lower/.upper\n        try:\n            if lb is None or ub is None:\n                b = getattr(func, \"bounds\", None)\n                if b is not None:\n                    lb = lb or getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                    ub = ub or getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n        except Exception:\n            pass\n\n        if lb is None or ub is None:\n            # default for BBOB tasks\n            lb = -5.0\n            ub = 5.0\n\n        lb = np.array(lb, dtype=float)\n        ub = np.array(ub, dtype=float)\n\n        # broadcast scalars to vectors\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        # mirror-reflection: reflect until in bounds then clamp\n        x = x.copy()\n        for _ in range(6):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            U = self.rng.rand(n, self.dim)\n            return lb + U * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling,\n        occasional per-coordinate multipliers, and cap extremes.\n        \"\"\"\n        scale_vec = np.maximum(np.array(scale_vec, dtype=float), 1e-12)\n        u = self.rng.rand(self.dim)\n        step = np.tan(np.pi * (u - 0.5)) * scale_vec\n        # occasional heavy-tail multiplier per coordinate:\n        heavy_mask = (self.rng.rand(self.dim) < 0.06)\n        step[heavy_mask] *= (1.0 + 2.5 * self.rng.rand(heavy_mask.sum()))\n        # absolute safety cap\n        abs_cap = cap_multiplier * np.maximum(scale_vec, 1e-6)\n        step = np.sign(step) * np.minimum(np.abs(step), abs_cap)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset state for new run\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.success_window = []\n        self.stagnation_counter = 0\n        self.gscale = 1.0\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluation wrapper: increments eval counter and prevents calls beyond budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                # do not call black-box beyond budget\n                return float(np.inf)\n            # ensure finite numeric array\n            x = np.array(x, dtype=float)\n            self.evals += 1\n            try:\n                f = float(func(x))\n            except Exception:\n                # if evaluation fails, treat as infinite (invalid)\n                f = float(np.inf)\n            return f\n\n        # ---- initialization: stratified-ish LHS + uniform mix ----\n        if self.budget < 6:\n            n_init = max(1, self.budget)\n        else:\n            n_init = max(4, min(max(6, 2 * self.dim), max(2, self.budget // 10)))\n\n        # Latin-hypercube-ish: divide [0,1] into n_init strata for each dim\n        strata = np.linspace(0, 1, n_init + 1)\n        points = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            # one sample per stratum, then shuffle rows\n            u = self.rng.rand(n_init)\n            pts = strata[:-1] + u * (1.0 / n_init)\n            self.rng.shuffle(pts)\n            points[:, d] = pts\n\n        # scale to bounds\n        X0 = lb + points * (ub - lb)\n\n        # Evaluate initial points (respect budget)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X0[i]\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for diversity (bounded by budget)\n        add_uni = min(4, max(0, (self.budget - self.evals) // max(1, self.dim)))\n        for _ in range(add_uni):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # If nothing evaluated (budget 0), return fallback\n        if self.evals == 0:\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.32, 0.28, 0.28, 0.12])\n        probs = probs / probs.sum()\n        cum = np.cumsum(probs)\n\n        # main loop\n        last_prune_at = len(self.archive_f)\n        while self.evals < self.budget:\n            # convert archive to numpy arrays for convenience\n            X = np.array(self.archive_x, dtype=float)\n            F = np.array(self.archive_f, dtype=float)\n            n_pop = X.shape[0]\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if n_pop >= 4 and np.isfinite(F).any():\n                # use coordinates of finite points\n                finite_idx = np.isfinite(F)\n                if finite_idx.sum() >= 3:\n                    Xf = X[finite_idx]\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    iqr = q75 - q25\n                    # fallback to std where zero-ish\n                    stds = np.std(Xf, axis=0)\n                    scale_est = np.maximum(iqr, stds * 0.5)\n                    zero_mask = scale_est < 1e-8\n                    if zero_mask.any():\n                        scale_est[zero_mask] = 0.5 * (ub - lb)[zero_mask]\n                else:\n                    scale_est = 0.5 * (ub - lb)\n            else:\n                # wide scale if not enough points\n                scale_est = 0.5 * (ub - lb)\n            # guard lower\n            scale_est = np.maximum(scale_est, 1e-8)\n\n            # choose anchor (center point for generating candidate)\n            finite_mask = np.isfinite(F)\n            if finite_mask.any():\n                # bias sampling towards elites: sample from top-k with exponential bias\n                sorted_idx = np.argsort(F)\n                topk = min(max(3, int(0.15 * n_pop)), n_pop)\n                topk = max(1, topk)\n                elite_idx = sorted_idx[:topk]\n                probs_elite = np.exp(-np.arange(topk) / max(1.0, topk / 3.0))\n                probs_elite = probs_elite / probs_elite.sum()\n                pick = self.rng.choice(topk, p=probs_elite)\n                anchor_idx = int(elite_idx[pick])\n                anchor = X[anchor_idx].copy()\n            else:\n                anchor_idx = None\n                anchor = self._uniform_array(lb, ub)\n\n            # choose a strategy\n            r = self.rng.rand()\n\n            # Start candidate with anchor as base\n            cand = anchor.copy()\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0]:\n                if n_pop >= 3:\n                    ids = list(range(n_pop))\n                    # ensure unique a,b different from anchor_idx if possible\n                    choices = [i for i in ids if i != anchor_idx]\n                    if len(choices) >= 2:\n                        a, b = self.rng.choice(choices, size=2, replace=False)\n                    else:\n                        a, b = self.rng.choice(ids, size=2, replace=False)\n                    de_scale = 0.6 * (0.8 + 0.6 * self.rng.rand()) * self.gscale\n                    diff = X[a] - X[b]\n                    cand = anchor + de_scale * diff\n                    # add small coordinate-wise gaussian noise\n                    cand += self.rng.randn(self.dim) * (scale_est * self.gscale * 0.12)\n                else:\n                    # fallback to gaussian near anchor\n                    cand = anchor + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1]:\n                # use top-k elites\n                if n_pop >= 3 and finite_mask.any():\n                    sorted_idx = np.argsort(F)\n                    topk = min(max(3, int(0.15 * n_pop)), n_pop)\n                    topk = max(1, topk)\n                    eliteX = X[sorted_idx[:topk]]\n                    mean_elite = eliteX.mean(axis=0)\n                    Z = eliteX - mean_elite\n                    try:\n                        U, S, Vt = np.linalg.svd(Z, full_matrices=False)\n                        explained = S**2\n                        if explained.sum() > 0:\n                            explained = explained / explained.sum()\n                            cum_expl = np.cumsum(explained)\n                            k = int(np.searchsorted(cum_expl, 0.85) + 1)\n                        else:\n                            k = 1\n                    except Exception:\n                        Vt = np.eye(self.dim)\n                        S = np.ones(self.dim)\n                        k = 1\n                    k = max(1, min(k, Vt.shape[0]))\n                    pcs = Vt[:k]\n                    # sample coefficients along top-k PCs\n                    coeffs = self.rng.randn(k) * (0.6 * self.gscale) * (S[:k] / (S[:k].mean() + 1e-12))\n                    delta = (coeffs.reshape(1, -1) @ pcs).ravel()\n                    base = (0.6 * mean_elite + 0.4 * anchor)\n                    cand = base + delta + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.08)\n                else:\n                    cand = anchor + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.6)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                mask = (self.rng.rand(self.dim) < 0.28)\n                if not mask.any():\n                    # force at least one coordinate\n                    mask[self.rng.randint(self.dim)] = True\n                local_noise = self.rng.randn(self.dim) * (scale_est * self.gscale * 0.18)\n                cand = anchor.copy()\n                cand[mask] += local_noise[mask]\n                # occasional small uniform mix-in for diversity\n                uni_coords = (self.rng.rand(self.dim) < 0.12)\n                if uni_coords.any():\n                    cand[uni_coords] = lb[uni_coords] + self.rng.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                scale_vec = np.maximum(scale_est, 0.05 * (ub - lb)) * (0.8 + 0.6 * self.rng.rand())\n                cand = anchor + self._tempered_cauchy(scale_vec, cap_multiplier=40.0) * self.gscale\n                # occasionally add small directed differential to introduce guided escapes\n                if n_pop >= 3 and (self.rng.rand() < 0.18):\n                    ids = list(range(n_pop))\n                    a, b = self.rng.choice(ids, size=2, replace=False)\n                    cand += 0.08 * self.rng.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.06 and finite_mask.any():\n                best_idx = int(np.nanargmin(F))\n                cand = 0.85 * cand + 0.15 * X[best_idx] + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.05)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = lb[bad] + self.rng.rand(bad.sum()) * (ub[bad] - lb[bad])\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f)\n            finite_idx = np.isfinite(F)\n            success = 0\n            if finite_idx.any():\n                fin_vals = F[finite_idx]\n                # rank position among finite (0 is best)\n                latest_val = F[-1]\n                rank_pos = np.sum(fin_vals < latest_val)\n                # success if within top 25% or improves absolute best\n                if rank_pos <= max(0, int(0.25 * fin_vals.size)):\n                    success = 1\n                best_now = np.nanmin(F)\n                if latest_val <= best_now:\n                    success = 1\n            else:\n                # if all infinite, any finite is success\n                if np.isfinite(F[-1]):\n                    success = 1\n\n            self.success_window.append(success)\n            if len(self.success_window) > self.success_win_len:\n                # trim\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= max(3, int(self.success_win_len / 3)):\n                recent = float(np.mean(self.success_window))\n                # increase stagnation counter if few successes\n                if recent > 0.28:\n                    # many successes -> contract exploration slightly\n                    self.gscale *= 0.92 * (0.98 + 0.04 * self.rng.rand())\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n                elif recent < 0.12:\n                    # too few successes -> expand\n                    self.gscale *= 1.08 * (1.0 + 0.04 * (self.rng.rand() - 0.5))\n                    self.stagnation_counter += 1\n                else:\n                    # mild jitter around current scale\n                    self.gscale *= 1.0 + 0.02 * (self.rng.rand() - 0.5)\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-6, 8.0))\n\n            # prune archive when too large\n            if len(self.archive_f) - last_prune_at >= self.prune_every:\n                last_prune_at = len(self.archive_f)\n                F = np.array(self.archive_f)\n                finite_mask = np.isfinite(F)\n                idxs = np.arange(len(F))\n                # always keep best few\n                n_keep_best = max(10, 2 * self.dim + 5)\n                sorted_idx = np.argsort(F)\n                keep_idx = set(int(i) for i in sorted_idx[:min(n_keep_best, len(sorted_idx))])\n                # keep some randoms for diversity\n                remaining = [i for i in idxs if i not in keep_idx]\n                take = max(0, min(40, int(0.5 * len(remaining))))\n                if remaining and take > 0:\n                    rnd = list(self.rng.choice(remaining, size=min(take, len(remaining)), replace=False))\n                    for rr in rnd:\n                        keep_idx.add(int(rr))\n                # if archive still too large, keep a spaced subset to keep history\n                if len(keep_idx) < min(200, max(80, 8 * self.dim)):\n                    # add earliest and latest\n                    keep_idx.add(0)\n                    keep_idx.add(len(F) - 1)\n                # finalize kept (sorted by original order)\n                keep_idx = sorted(list(keep_idx))\n                X_new = [self.archive_x[i] for i in keep_idx]\n                F_new = [self.archive_f[i] for i in keep_idx]\n                self.archive_x = X_new\n                self.archive_f = F_new\n\n            # stagnation detection & micro-restarts\n            if self.stagnation_counter >= 10 and self.evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random if none)\n                F = np.array(self.archive_f)\n                finite_mask = np.isfinite(F)\n                if finite_mask.any():\n                    best_idx = int(np.nanargmin(F))\n                    center = np.array(self.archive_x[best_idx], dtype=float)\n                else:\n                    center = self._uniform_array(lb, ub)\n                # local cloud\n                n_cloud = min(18, max(6, (self.budget - self.evals) // 8))\n                sigma = 1.2 + 0.6 * self.rng.rand()\n                # recompute scale_est if needed\n                if len(self.archive_x) > 0:\n                    scale_est_local = scale_est\n                else:\n                    scale_est_local = 0.5 * (ub - lb)\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    cand = center + self.rng.randn(self.dim) * (sigma * scale_est_local * self.gscale)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                n_glob = min(6, (self.budget - self.evals))\n                for _ in range(n_glob):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.06 + 0.14 * self.rng.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-6, 8.0))\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        F = np.array(self.archive_f)\n        if F.size == 0:\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n\n        finite_mask = np.isfinite(F)\n        if finite_mask.any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx], dtype=float).copy()\n        else:\n            # all infinite: return a uniform fallback\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "bf1fe5cb-a478-43fa-8fb2-01f0b8d2a1bf", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 prune_every=200, n_init_scale=6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # global exploration scale (adapted)\n        self.gscale = 1.0\n        # archive\n        self.archive_x = []\n        self.archive_f = []\n        # bookkeeping\n        self.evals = 0\n        self.prune_every = int(prune_every)\n        # initial sampling heuristic (n_init = min(budget, n_init_scale*dim + 4))\n        self.n_init_scale = int(n_init_scale)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func attributes, otherwise default [-5,5]^dim.\n        Returns lb, ub as numpy arrays of length self.dim.\n        \"\"\"\n        lb = None\n        ub = None\n        try:\n            # try common attribute names\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n            # sometimes bounds are in func.bounds.lb etc.\n            if (lb is None or ub is None) and hasattr(func, \"bounds\"):\n                b = getattr(func, \"bounds\")\n                lb = lb or getattr(b, \"lb\", None) or getattr(b, \"lower\", None)\n                ub = ub or getattr(b, \"ub\", None) or getattr(b, \"upper\", None)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            # If sizes mismatch, fallback to default\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(8):\n            below = x < lb\n            if not below.any():\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            return lb.reshape(1, -1) + self.rng.rand(n, self.dim) * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling,\n        occasional per-coordinate heavy multipliers, and cap extremes.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        # raw cauchy\n        z = np.tan(np.pi * (u - 0.5))\n        # occasional heavy-tail multiplier per coordinate:\n        heavy_mask = (self.rng.rand(self.dim) < 0.06)\n        heavy_mult = np.ones(self.dim)\n        heavy_mult[heavy_mask] = 4.0 + 6.0 * self.rng.rand(heavy_mask.sum())\n        # apply scale\n        s = np.maximum(scale_vec, 1e-9)\n        vals = z * s * heavy_mult\n        # absolute safety cap\n        abs_cap = cap_multiplier * s\n        vals = np.sign(vals) * np.minimum(np.abs(vals), abs_cap)\n        # small gaussian jitter to break symmetry\n        vals += 1e-3 * self.rng.randn(self.dim) * s\n        return vals\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset state for new run\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.gscale = 1.0\n        self.success_window = []\n        self.stagnation_counter = 0\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluation wrapper: increments eval counter and prevents calls beyond budget\n        def safe_eval(x):\n            # ensure numeric array\n            x = np.array(x, dtype=float)\n            if self.evals >= self.budget:\n                return np.inf\n            try:\n                f = func(x)\n                # Ensure numeric scalar\n                if not np.isfinite(f):\n                    f = float(np.inf)\n            except Exception:\n                f = float(np.inf)\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: stratified-ish LHS + uniform mix ----\n        n_init = min(self.budget, max(4, self.n_init_scale * self.dim))\n        # Latin-hypercube-ish: strata per dimension, random pairing\n        # We create n_init points by sampling strata per dimension and shuffling rows\n        strata = (np.arange(n_init) / float(n_init))\n        pts = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            perm = np.arange(n_init)\n            self.rng.shuffle(perm)\n            u = self.rng.rand(n_init)\n            pts[:, d] = (perm + u) / float(n_init)\n        # scale to bounds\n        X_init = lb + pts * (ub - lb)\n\n        # Evaluate initial points (respect budget)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X_init[i]\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for added diversity if budget remains\n        n_extra = min(6, max(0, self.budget - self.evals))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        if len(self.archive_f) == 0:\n            # Nothing evaluated (budget 0). Return fallback.\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        probs = np.array([0.32, 0.28, 0.28, 0.12])\n        cum = np.cumsum(probs)\n\n        last_prune_at = 0\n\n        # main loop\n        while self.evals < self.budget:\n            # convert archive to numpy arrays for convenience\n            X = np.array(self.archive_x, dtype=float)\n            F = np.array(self.archive_f, dtype=float)\n            finite_mask = np.isfinite(F)\n            fin_idx = np.where(finite_mask)[0]\n\n            n_pop = X.shape[0]\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if fin_idx.size >= 4:\n                Xf = X[fin_idx]\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                scale_est = (q75 - q25) / 1.349  # approx std from IQR\n                stds = np.std(Xf, axis=0)\n                small_mask = scale_est < 1e-8\n                scale_est[small_mask] = stds[small_mask]\n            elif n_pop >= 2:\n                scale_est = np.std(X, axis=0)\n            else:\n                scale_est = 0.5 * (ub - lb)\n            # guard lower bound on scale\n            scale_est = np.maximum(scale_est, 1e-9 * (ub - lb + 1e-9))\n\n            # choose anchor (center point for generating candidate)\n            if fin_idx.size > 0:\n                fin_vals = F[fin_idx]\n                sorted_idx_fin = fin_idx[np.argsort(fin_vals)]\n                # bias sampling towards elites: sample from top-k with exponential bias\n                topk = max(1, min(len(sorted_idx_fin), int(max(2, 0.2 * len(sorted_idx_fin)))))\n                elites = sorted_idx_fin[:topk]\n                # exponential weights favoring best\n                r_expo = self.rng.rand()\n                weight = np.exp(-2.5 * np.arange(topk) / max(1, topk - 1))\n                weight = weight / weight.sum()\n                # sample an elite index\n                try:\n                    sel = self.rng.choice(np.arange(topk), p=weight)\n                    anchor_idx = elites[int(sel)]\n                except Exception:\n                    anchor_idx = int(elites[0])\n                anchor = X[anchor_idx].copy()\n            else:\n                # no finite evaluations yet, pick uniform anchor\n                anchor_idx = None\n                anchor = self._uniform_array(lb, ub)\n\n            # choose donor indices for DE-style moves\n            choices = list(range(n_pop))\n            if anchor_idx is not None and anchor_idx in choices:\n                choices.remove(anchor_idx)\n\n            # choose a strategy\n            r = self.rng.rand()\n            cand = anchor.copy()\n\n            # Strategy 1: DE-style differential injection\n            if r <= cum[0]:\n                if len(choices) >= 2:\n                    a, b = self.rng.choice(choices, size=2, replace=False)\n                    diff = X[a] - X[b]\n                    # differential weight scales with gscale and global scale_est median\n                    F_weight = 0.6 * self.gscale * (np.median(scale_est) / (np.median(scale_est) + 1e-12))\n                    cand = anchor + F_weight * diff\n                    # small gaussian perturbation\n                    cand += self.rng.randn(self.dim) * (scale_est * self.gscale * 0.06)\n                else:\n                    # fallback to gaussian near anchor\n                    cand = anchor + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.2)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif r <= cum[1]:\n                if fin_idx.size >= 3:\n                    topk = max(1, min(4, fin_idx.size))\n                    elite_idx = sorted_idx_fin[:topk]\n                    eliteX = X[elite_idx]\n                    mean_elite = eliteX.mean(axis=0)\n                    Z = eliteX - mean_elite\n                    try:\n                        U, S, Vt = np.linalg.svd(Z, full_matrices=False)\n                        pcs = Vt[:topk]\n                        # sample coefficients along top-k PCs\n                        coeffs = self.rng.randn(topk) * (0.6 * self.gscale) * (S[:topk] / (S[:topk].mean() + 1e-12))\n                        delta = (coeffs.reshape(1, -1) @ pcs).ravel()\n                        base = anchor if anchor_idx is not None else mean_elite\n                        cand = base + delta + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.08)\n                    except Exception:\n                        cand = anchor + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.2)\n                else:\n                    # insufficient elites, random gaussian\n                    cand = anchor + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.25)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif r <= cum[2]:\n                # anisotropic per-dim gaussian centered at anchor\n                sigma = scale_est * (0.9 * self.gscale)\n                # Force at least one coordinate to change\n                shifts = self.rng.randn(self.dim) * sigma\n                if np.allclose(shifts, 0.0):\n                    shifts[self.rng.randint(0, self.dim)] = sigma.max() * 0.1\n                cand = anchor + shifts\n                # occasional small uniform mix-in for diversity\n                uni_coords = (self.rng.rand(self.dim) < 0.12)\n                if uni_coords.any():\n                    cand[uni_coords] = lb[uni_coords] + self.rng.rand(uni_coords.sum()) * (ub[uni_coords] - lb[uni_coords])\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                # tempered Cauchy centered on anchor but can be large\n                cauch = self._tempered_cauchy(scale_est, cap_multiplier=60.0)\n                cand = anchor + cauch * (1.0 * self.gscale)\n                # occasionally add small directed differential to introduce guided escapes\n                if len(choices) >= 2 and self.rng.rand() < 0.18:\n                    a, b = self.rng.choice(choices, size=2, replace=False)\n                    cand += 0.08 * self.rng.rand() * (X[a] - X[b])\n\n            # occasional directed small jump to current best for refinement\n            if fin_idx.size > 0 and self.rng.rand() < 0.06:\n                best_idx = sorted_idx_fin[0]\n                # small step towards best\n                cand += 0.12 * self.rng.rand() * (X[best_idx] - cand)\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = self._uniform_array(lb[bad], ub[bad]) if bad.sum() == self.dim else (lb[bad] + self.rng.rand(bad.sum()) * (ub[bad] - lb[bad]))\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            F = np.array(self.archive_f)\n            finite_mask = np.isfinite(F)\n            success = 0\n            if finite_mask.any():\n                fin_vals = F[finite_mask]\n                # rank position among finite (0 is best)\n                rank_pos = np.sum(fin_vals < f_c)\n                if rank_pos <= max(0, int(0.25 * fin_vals.size)):\n                    success = 1\n                # improvement over global best\n                if f_c <= fin_vals.min():\n                    success = 1\n            else:\n                # if all infinite, any finite is success\n                if np.isfinite(f_c):\n                    success = 1\n\n            self.success_window.append(success)\n            # keep a rolling window of last 64 outcomes\n            if len(self.success_window) > 64:\n                self.success_window = self.success_window[-64:]\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= 8 and (self.evals % 8 == 0):\n                recent = float(np.mean(self.success_window[-32:])) if len(self.success_window) >= 32 else float(np.mean(self.success_window))\n                if recent > 0.28:\n                    # many successes -> contract exploration slightly\n                    self.gscale *= (0.92 + 0.06 * self.rng.rand())\n                    self.stagnation_counter = max(0, self.stagnation_counter - 1)\n                elif recent < 0.12:\n                    # too few successes -> expand\n                    self.gscale *= (1.08 + 0.12 * self.rng.rand())\n                    self.stagnation_counter += 1\n                else:\n                    # slight jitter\n                    self.gscale *= (0.98 + 0.06 * self.rng.rand())\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-6, 8.0))\n\n            # prune archive when too large\n            if len(self.archive_f) - last_prune_at >= self.prune_every:\n                F = np.array(self.archive_f)\n                finite_mask = np.isfinite(F)\n                if finite_mask.any():\n                    sorted_idx = np.argsort(F)  # includes inf at end\n                else:\n                    sorted_idx = np.arange(len(F))\n                n_keep_best = min(20, max(3, 4 * self.dim))\n                keep_idx = set(int(i) for i in sorted_idx[:min(n_keep_best, len(sorted_idx))])\n                # keep some randoms for diversity\n                n_random_keep = min(40, max(5, len(self.archive_x) // 20))\n                for rr in self.rng.choice(len(self.archive_x), size=n_random_keep, replace=False):\n                    keep_idx.add(int(rr))\n                # if archive still too large, keep a spaced subset to keep history\n                target_keep = min(200, max(80, 8 * self.dim))\n                if len(keep_idx) < target_keep:\n                    # add earliest and latest and spaced indices\n                    keep_idx.add(0)\n                    keep_idx.add(len(self.archive_x) - 1)\n                    # space-add\n                    step = max(1, len(self.archive_x) // (target_keep - len(keep_idx) + 1))\n                    for i in range(0, len(self.archive_x), step):\n                        keep_idx.add(i)\n                        if len(keep_idx) >= target_keep:\n                            break\n                # finalize kept (sorted by original order)\n                keep_list = sorted(keep_idx)\n                X_new = [self.archive_x[i] for i in keep_list]\n                F_new = [self.archive_f[i] for i in keep_list]\n                self.archive_x = X_new\n                self.archive_f = F_new\n                last_prune_at = len(self.archive_f)\n\n            # stagnation detection & micro-restarts\n            if self.stagnation_counter > max(8, 6 + self.dim // 2):\n                # perform micro-restart: local cloud around current best (or random if none)\n                F = np.array(self.archive_f)\n                finite_mask = np.isfinite(F)\n                if finite_mask.any():\n                    best_idx = int(np.nanargmin(F))\n                    center = self.archive_x[best_idx].copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                # create a local cloud\n                n_cloud = min(10 + self.dim, max(6, self.budget // 200))\n                if self.evals + n_cloud > self.budget:\n                    n_cloud = max(0, self.budget - self.evals)\n                # recompute local scale\n                scale_local = 0.5 * (ub - lb)\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    cand = center + self.rng.randn(self.dim) * (scale_local * (0.2 + 0.6 * self.gscale))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f)\n                # also add a few global randoms to regain diversity\n                n_glob = min(6, max(3, self.dim // 2))\n                for _ in range(n_glob):\n                    if self.evals >= self.budget:\n                        break\n                    cand = self._uniform_array(lb, ub)\n                    f = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= (1.06 + 0.14 * self.rng.rand())\n                self.gscale = float(np.clip(self.gscale, 1e-6, 8.0))\n                self.success_window = []\n                self.stagnation_counter = 0\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        F = np.array(self.archive_f, dtype=float)\n        finite_mask = np.isfinite(F)\n        if finite_mask.any():\n            best_i = int(np.nanargmin(F))\n            return float(F[best_i]), np.array(self.archive_x[best_i], dtype=float)\n        else:\n            # all infinite: return a uniform fallback\n            x = self._uniform_array(lb, ub)\n            return float(np.inf), x", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d060bf5a-8b4e-48c8-a82a-a368790000d5", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nimport math\nimport collections\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 n_init_scale=4, gscale_init=0.5, max_archive=2000):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.n_init_scale = int(n_init_scale)\n        self.gscale = float(gscale_init)\n        self.max_archive = int(max_archive)\n\n        # bookkeeping\n        self.evals = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func attributes, otherwise default [-5,5]^dim.\n        Returns lb, ub as numpy arrays of length self.dim.\n        \"\"\"\n        lb = None\n        ub = None\n        # try common attribute names\n        for name in (\"bounds\",):\n            if hasattr(func, name):\n                b = getattr(func, name)\n                # try b.lb, b.ub, b.lower, b.upper, or b[0], b[1]\n                lb = lb or getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                ub = ub or getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n                # maybe b is a tuple/list\n                if lb is None and isinstance(b, (tuple, list)) and len(b) >= 2:\n                    lb, ub = b[0], b[1]\n        # try func.bounds_lb / bounds_ub\n        lb = lb or getattr(func, \"lb\", None) or getattr(func, \"lower\", None) or getattr(func, \"bounds_lb\", None)\n        ub = ub or getattr(func, \"ub\", None) or getattr(func, \"upper\", None) or getattr(func, \"bounds_ub\", None)\n\n        # default\n        if lb is None or ub is None:\n            lb = -5.0\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        # broadcast scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # final guard\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = x.copy()\n        for _ in range(niter):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            # mirror reflection\n            x[below] = lb[below] + (lb[below] - x[below])\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            return self.rng.rand(n, self.dim) * (ub - lb) + lb\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling,\n        occasional per-coordinate heavy multipliers, and cap extremes.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))\n        # occasional heavy-tail multiplier per coordinate:\n        heavy_mask = (self.rng.rand(self.dim) < 0.06)\n        heavy_mult = np.ones(self.dim)\n        heavy_mult[heavy_mask] = 2.0 + self.rng.rand(heavy_mask.sum()) * 8.0  # 2..10x\n        raw = z * heavy_mult * scale_vec\n        # absolute safety cap\n        cap = np.maximum(scale_vec * cap_multiplier, 1e-12)\n        raw = np.clip(raw, -cap, cap)\n        # small gaussian jitter to break symmetry\n        raw = raw + 1e-6 * scale_vec * self.rng.randn(self.dim)\n        return raw\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset state for new run\n        self.archive_x = []\n        self.archive_f = []\n        self.evals = 0\n        self.gscale = float(self.gscale)  # maybe reset\n\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n\n        # safe evaluation wrapper: increments eval counter and prevents calls beyond budget\n        def safe_eval(x):\n            x = np.asarray(x, dtype=float)\n            if self.evals >= self.budget:\n                raise RuntimeError(\"Budget exceeded\")\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f\n\n        # ---- initialization: stratified-ish LHS + uniform mix ----\n        n_init = min(self.budget, max(4, self.n_init_scale * self.dim))\n        pts = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            perm = np.arange(n_init)\n            self.rng.shuffle(perm)\n            # sample in strata\n            pts[:, d] = (perm + self.rng.rand(n_init)) / float(n_init)\n        # scale to bounds\n        X_init = lb + pts * span\n\n        # Evaluate initial points (respect budget)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X_init[i]\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for added diversity if budget remains\n        n_extra = min(8, max(0, self.budget - self.evals))\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            f = safe_eval(x)\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Nothing evaluated (budget 0). Return fallback.\n        if len(self.archive_f) == 0:\n            # return a uniform fallback\n            x0 = self._uniform_array(lb, ub)\n            try:\n                f0 = float(func(x0))\n            except Exception:\n                f0 = np.inf\n            return f0, x0\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        strat_probs = np.array([0.35, 0.25, 0.30, 0.10])  # sum 1\n\n        recent_window = collections.deque(maxlen=64)\n        last_prune_at = 0\n        no_improve_counter = 0\n        best_so_far = np.nanmin(np.array(self.archive_f, dtype=float))\n        best_x = self.archive_x[int(np.nanargmin(np.array(self.archive_f, dtype=float)))].copy()\n\n        # main loop\n        while self.evals < self.budget:\n            # convert archive to numpy arrays for convenience\n            X = np.array(self.archive_x, dtype=float)\n            F = np.array(self.archive_f, dtype=float)\n            finite_mask = np.isfinite(F)\n            fin_idx = np.where(finite_mask)[0]\n\n            # compute robust per-dim scale estimate from IQR where possible\n            if fin_idx.size >= 4:\n                Xf = X[fin_idx]\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                iqr = q75 - q25\n                scale_est = iqr / 1.349  # approx std\n                # fallback to std where IQR is zero\n                small_mask = (scale_est <= 0)\n                if small_mask.any():\n                    stds = np.std(Xf, axis=0)\n                    scale_est[small_mask] = stds[small_mask]\n            else:\n                # base on global span\n                scale_est = span * 0.5\n\n            # guard lower bound on scale\n            scale_est = np.maximum(scale_est, 1e-9 * (ub - lb + 1e-9))\n\n            # choose anchor (center point for generating candidate)\n            anchor_idx = None\n            if fin_idx.size > 0:\n                # bias sampling towards elites: sample from top-k with exponential bias\n                sorted_idx_fin = fin_idx[np.argsort(F[fin_idx])]\n                topk = max(1, min(len(sorted_idx_fin), int(max(2, 0.2 * len(sorted_idx_fin)))))\n                # exponential weights favoring best\n                weights = np.exp(-np.arange(topk) / max(1.0, 0.5 * topk))\n                weights = weights / weights.sum()\n                choice_pos = self.rng.choice(topk, p=weights)\n                anchor_idx = sorted_idx_fin[choice_pos]\n                anchor = X[anchor_idx].copy()\n            else:\n                anchor = self._uniform_array(lb, ub)\n\n            # choose donor indices for DE-style moves\n            choices = list(range(len(self.archive_x)))\n            if anchor_idx is not None and anchor_idx in choices:\n                choices.remove(anchor_idx)\n\n            # choose a strategy\n            s = self.rng.choice(4, p=strat_probs)\n\n            if s == 0 and len(choices) >= 2:\n                # Strategy 1: DE-style differential injection\n                r1, r2 = self.rng.choice(choices, size=2, replace=False)\n                Xr1 = np.array(self.archive_x[r1], dtype=float)\n                Xr2 = np.array(self.archive_x[r2], dtype=float)\n                # differential weight scales with gscale and global scale_est median\n                med_scale = np.median(scale_est)\n                F_diff = (Xr1 - Xr2)\n                diff_weight = self.gscale * (0.8 + 0.6 * self.rng.rand()) * med_scale / (np.linalg.norm(F_diff) + 1e-12)\n                cand = anchor + diff_weight * F_diff + 0.02 * med_scale * self.rng.randn(self.dim)\n                # if candidate is too close, fallback to gaussian\n                if np.linalg.norm(cand - anchor) < 1e-12:\n                    cand = anchor + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.2)\n            elif s == 1:\n                # Strategy 2: PCA-guided elite perturbation\n                if fin_idx.size >= 3:\n                    topk_pca = min(8, fin_idx.size)\n                    elite_idx = fin_idx[np.argsort(F[fin_idx])[:topk_pca]]\n                    eliteX = X[elite_idx]\n                    mean_elite = eliteX.mean(axis=0)\n                    Z = eliteX - mean_elite\n                    try:\n                        # compute top PCs via SVD\n                        U, Svals, Vt = np.linalg.svd(Z, full_matrices=False)\n                        pcs = Vt[:min(self.dim, topk_pca)]\n                        # sample coefficients along top PCs\n                        coeffs = (self.rng.randn(pcs.shape[0]) * (Svals[:pcs.shape[0]] / (topk_pca + 1.0)))\n                        direction = (coeffs @ pcs).ravel()\n                        base = anchor if anchor is not None else mean_elite\n                        cand = base + direction * self.gscale + 0.02 * scale_est * self.rng.randn(self.dim)\n                    except Exception:\n                        cand = anchor + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.25)\n                else:\n                    cand = anchor + self.rng.randn(self.dim) * (scale_est * self.gscale * 0.25)\n            elif s == 2:\n                # Strategy 3: Local anisotropic gaussian\n                anis = scale_est * (0.4 + 0.6 * self.rng.rand(self.dim))\n                cand = anchor + anis * (self.gscale * self.rng.randn(self.dim))\n                # Force at least one coordinate to change\n                idx_change = self.rng.randint(0, self.dim)\n                cand[idx_change] = anchor[idx_change] + 0.5 * anis[idx_change] * (1.0 if self.rng.rand() < 0.5 else -1.0)\n                # occasional small uniform mix-in for diversity\n                if self.rng.rand() < 0.08:\n                    mix_idx = self.rng.choice(self.dim, size=max(1, self.dim // 8), replace=False)\n                    cand[mix_idx] = self._uniform_array(lb[mix_idx], ub[mix_idx], n=1)\n            else:\n                # Strategy 4: tempered Cauchy global escape\n                cand = anchor + self._tempered_cauchy(scale_est * (self.gscale * 1.5))\n                # occasionally add small directed differential to introduce guided escapes\n                if fin_idx.size >= 1 and self.rng.rand() < 0.25:\n                    best_idx = int(np.nanargmin(F))\n                    delta = X[best_idx] - anchor\n                    cand = cand + 0.05 * self.gscale * delta\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.04 and fin_idx.size > 0:\n                best_idx = int(np.nanargmin(F))\n                small_step = 0.05 * self.gscale * (X[best_idx] - anchor)\n                cand = cand + small_step\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            cand = np.asarray(cand, dtype=float)\n            bad = ~np.isfinite(cand)\n            if bad.any():\n                cand[bad] = self._uniform_array(lb[bad], ub[bad], n=1) if bad.sum() == self.dim else (lb + self.rng.rand(bad.sum()) * (ub - lb))[0:bad.sum()]\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            f_c = safe_eval(cand)\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            success = 0\n            if np.isfinite(f_c):\n                if fin_idx.size > 0:\n                    rank_pos = 1 + np.sum(F[finite_mask] < f_c)  # 1-based rank among finites\n                    if rank_pos <= max(1, int(max(1, 0.25 * finite_mask.sum()))):\n                        success = 1\n                if f_c < best_so_far:\n                    success = 1\n                    best_so_far = f_c\n                    best_x = cand.copy()\n                    no_improve_counter = 0\n                else:\n                    no_improve_counter += 1\n            else:\n                # if all infinite so far, any finite is success, else not\n                if finite_mask.sum() == 0 and np.isfinite(f_c):\n                    success = 1\n                    best_so_far = f_c\n                    best_x = cand.copy()\n                    no_improve_counter = 0\n                else:\n                    no_improve_counter += 1\n\n            recent_window.append(success)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(recent_window) == recent_window.maxlen and (self.evals % 16 == 0):\n                succ_frac = sum(recent_window) / float(len(recent_window))\n                if succ_frac > 0.35:\n                    # many successes -> contract exploration slightly\n                    self.gscale *= (0.92 + 0.06 * self.rng.rand())\n                elif succ_frac < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= (1.08 + 0.12 * self.rng.rand())\n                else:\n                    # slight jitter\n                    self.gscale *= (0.985 + 0.03 * self.rng.rand())\n                # clamp gscale\n                self.gscale = np.clip(self.gscale, 1e-6, 8.0)\n\n            # prune archive when too large (keep diversity)\n            if len(self.archive_x) > self.max_archive and (len(self.archive_f) - last_prune_at) > (self.max_archive // 4):\n                target_keep = int(self.max_archive * 0.9)\n                # keep best 40%, some random, and spaced history\n                X = np.array(self.archive_x)\n                F = np.array(self.archive_f)\n                finite_mask = np.isfinite(F)\n                keep_idx = set()\n                # keep best\n                fin_idx = np.where(finite_mask)[0]\n                if fin_idx.size > 0:\n                    best_sorted = fin_idx[np.argsort(F[fin_idx])]\n                    n_best = max(1, int(0.4 * target_keep))\n                    for i in best_sorted[:n_best]:\n                        keep_idx.add(int(i))\n                # keep some randoms\n                n_random_keep = max(1, int(0.1 * target_keep))\n                all_idx = np.arange(len(self.archive_x))\n                if len(all_idx) > 0:\n                    for rr in self.rng.choice(all_idx, size=min(n_random_keep, len(all_idx)), replace=False):\n                        keep_idx.add(int(rr))\n                # keep last few\n                for i in range(len(self.archive_x)-min(50, len(self.archive_x)), len(self.archive_x)):\n                    if i >= 0:\n                        keep_idx.add(int(i))\n                # if archive still too large, keep a spaced subset\n                if len(keep_idx) < target_keep:\n                    step = max(1, len(self.archive_x) // (target_keep - len(keep_idx)))\n                    for i in range(0, len(self.archive_x), step):\n                        keep_idx.add(i)\n                        if len(keep_idx) >= target_keep:\n                            break\n                keep_idx = sorted(keep_idx)\n                X_new = [self.archive_x[i] for i in keep_idx]\n                F_new = [self.archive_f[i] for i in keep_idx]\n                self.archive_x = X_new\n                self.archive_f = F_new\n                last_prune_at = len(self.archive_f)\n\n            # stagnation detection & micro-restarts\n            if no_improve_counter > max(200, 20 * self.dim):\n                # perform micro-restart: local cloud around current best (or random if none)\n                if np.isfinite(best_so_far):\n                    center = best_x.copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                n_cloud = min(10 + self.dim, max(6, self.budget // 200))\n                local_scale = np.maximum(1e-6, scale_est * (0.5 + self.rng.rand(self.dim) * 1.5))\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    cand = center + (self.rng.randn(self.dim) * local_scale * (1.0 + 0.5 * self.rng.rand()))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_c = safe_eval(cand)\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                    if np.isfinite(f_c) and f_c < best_so_far:\n                        best_so_far = f_c\n                        best_x = cand.copy()\n                # also add a few global randoms to regain diversity\n                n_glob = min(6, max(2, self.dim // 2))\n                for _ in range(n_glob):\n                    if self.evals >= self.budget:\n                        break\n                    xg = self._uniform_array(lb, ub)\n                    fg = safe_eval(xg)\n                    self.archive_x.append(xg.copy())\n                    self.archive_f.append(fg)\n                    if np.isfinite(fg) and fg < best_so_far:\n                        best_so_far = fg\n                        best_x = xg.copy()\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(8.0, self.gscale * (1.05 + 0.2 * self.rng.rand()))\n                no_improve_counter = 0\n                recent_window.clear()\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        F = np.array(self.archive_f, dtype=float)\n        if np.isfinite(F).any():\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), np.array(self.archive_x[best_idx], dtype=float)\n        else:\n            # all infinite: return a uniform fallback\n            x0 = self._uniform_array(lb, ub)\n            try:\n                f0 = float(func(x0))\n            except Exception:\n                f0 = np.inf\n            return f0, x0", "configspace": "", "generation": 0, "feedback": "In the code, line 44, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = lb or getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "error": "In the code, line 44, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = lb or getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "729274f9-310c-4b39-82b8-d75e7c8b7a51", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_factor=10,  # initial pts per dimension (stratified-ish)\n                 max_archive=1000,\n                 adapt_window=50,\n                 stagnation_limit=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # algorithm state\n        self.archive_x = []\n        self.archive_f = []\n        self.max_archive = int(max_archive)\n        self.gscale = 0.5  # global exploration scale\n        self.adapt_window = int(adapt_window)\n        self.success_window = []\n        # initial sampling configuration\n        self.init_factor = int(init_factor)\n        # stagnation detection\n        if stagnation_limit is None:\n            self.stagnation_limit = max(200, 20 * self.dim)\n        else:\n            self.stagnation_limit = int(stagnation_limit)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func attributes, otherwise default [-5,5]^dim.\n        Returns lb, ub as numpy arrays of length self.dim.\n        \"\"\"\n        lb = None\n        ub = None\n\n        # common patterns\n        # func.bounds = (lb, ub) or object with .lb/.ub\n        b = getattr(func, \"bounds\", None)\n        if b is not None:\n            # tuple/list style\n            if isinstance(b, (tuple, list)) and len(b) == 2:\n                lb, ub = b[0], b[1]\n            else:\n                # object with lb/ub attrs\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None)\n\n        # direct attributes\n        if lb is None:\n            lb = getattr(func, \"bounds_lb\", None) or getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"bounds_ub\", None) or getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        # if bounds come as scalars or length-2 tuple inside\n        try:\n            lb_a = np.asarray(lb, dtype=float)\n            ub_a = np.asarray(ub, dtype=float)\n        except Exception:\n            lb_a = None\n            ub_a = None\n\n        # fallback default\n        if lb_a is None or ub_a is None:\n            lb = -5.0\n            ub = 5.0\n            lb_a = np.full(self.dim, lb, dtype=float)\n            ub_a = np.full(self.dim, ub, dtype=float)\n        else:\n            # if scalar broadcast\n            if lb_a.size == 1:\n                lb_a = np.full(self.dim, float(lb_a.ravel()[0]), dtype=float)\n            if ub_a.size == 1:\n                ub_a = np.full(self.dim, float(ub_a.ravel()[0]), dtype=float)\n            # final guard sizes\n            if lb_a.shape[0] != self.dim or ub_a.shape[0] != self.dim:\n                # try to broadcast by repeating first element\n                lb_a = np.resize(lb_a, (self.dim,))\n                ub_a = np.resize(ub_a, (self.dim,))\n\n        return lb_a.astype(float), ub_a.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = np.asarray(x, dtype=float)\n        for _ in range(niter):\n            # below lower\n            below = x < lb\n            if not np.any(below) and not np.any(x > ub):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            r = self.rng.rand(n, self.dim)\n            return lb + r * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling,\n        occasional per-coordinate heavy multipliers, and cap extremes.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))\n        # occasional heavy-tail multiplier per coordinate:\n        heavy_mult = np.ones(self.dim)\n        heavy_mask = self.rng.rand(self.dim) < 0.05\n        heavy_mult[heavy_mask] = 4.0 + 6.0 * self.rng.rand(np.sum(heavy_mask))\n        # absolute safety cap\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        sample = scale_vec * z * heavy_mult\n        sample = np.clip(sample, -cap, cap)\n        # small gaussian jitter to break symmetry\n        sample += 1e-3 * scale_vec * self.rng.randn(self.dim)\n        return sample\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset state for new run\n        self.evals = 0\n        self.archive_x = []\n        self.archive_f = []\n        self.success_window = []\n        self.gscale = max(1e-3, min(2.0, self.gscale))\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluation wrapper: increments eval counter and prevents calls beyond budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                raise RuntimeError(\"Budget exceeded\")\n            f = func(np.asarray(x, dtype=float))\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: stratified-ish LHS + uniform mix ----\n        n_init = min(max(2 * self.dim, int(self.init_factor * self.dim)), max(1, self.budget // 10))\n        # stratified (simple LHS-like)\n        strata = np.arange(n_init)\n        X_init = np.empty((n_init, self.dim), dtype=float)\n        for d in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            u = self.rng.rand(n_init)\n            X_init[:, d] = (perm + u) / float(n_init)\n        # scale to bounds\n        X_init = lb + X_init * (ub - lb)\n\n        # Evaluate initial points (respect budget)\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = X_init[i]\n            try:\n                f = safe_eval(x)\n            except RuntimeError:\n                break\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Add a few pure uniform draws for added diversity if budget remains\n        n_extra = min(10 + self.dim, self.budget - self.evals)\n        for _ in range(n_extra):\n            if self.evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = safe_eval(x)\n            except RuntimeError:\n                break\n            self.archive_x.append(x.copy())\n            self.archive_f.append(f)\n\n        # Nothing evaluated (budget 0). Return fallback.\n        if len(self.archive_f) == 0:\n            fallback = self._uniform_array(lb, ub)\n            return np.inf, fallback\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        p_de = 0.35\n        p_pca = 0.25\n        p_local = 0.30\n        p_cauchy = 0.10\n\n        last_prune_at = 0\n        no_improve_counter = 0\n        best_so_far = np.nanmin(self.archive_f)\n\n        # main loop\n        while self.evals < self.budget:\n            # convert archive to numpy arrays for convenience\n            X = np.array(self.archive_x, dtype=float)\n            F = np.array(self.archive_f, dtype=float)\n\n            # compute robust per-dim scale estimate from IQR where possible\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask):\n                Xf = X[finite_mask]\n                q25 = np.percentile(Xf, 25, axis=0)\n                q75 = np.percentile(Xf, 75, axis=0)\n                scale_est = np.maximum(1e-8, (q75 - q25) / 1.349)  # approximate std from IQR\n                zero_mask = scale_est <= 1e-12\n                if np.any(zero_mask):\n                    # fallback to std\n                    std_est = np.std(Xf, axis=0)\n                    scale_est[zero_mask] = np.maximum(std_est[zero_mask], 1e-6)\n            else:\n                # base on global span\n                scale_est = (ub - lb) * 0.5\n\n            # guard lower bound on scale\n            scale_est = np.maximum(scale_est, 1e-8)\n\n            # choose anchor (center point for generating candidate)\n            fin_idx = np.where(np.isfinite(F))[0]\n            if fin_idx.size > 0:\n                # bias sampling towards elites: sample from top-k with exponential bias\n                topk = max(2, int(min(30, 0.25 * fin_idx.size)))\n                sorted_idx_fin = fin_idx[np.argsort(F[fin_idx])]\n                elite_idx = sorted_idx_fin[:topk]\n                # exponential weights favoring best\n                ranks = np.arange(len(elite_idx))\n                w = np.exp(-0.3 * ranks)\n                w = w / np.sum(w)\n                anchor_idx = self.rng.choice(elite_idx, p=w)\n                anchor = X[anchor_idx].copy()\n            else:\n                # choose random archive or uniform fallback\n                if X.shape[0] > 0:\n                    anchor = X[self.rng.randint(0, X.shape[0])].copy()\n                else:\n                    anchor = self._uniform_array(lb, ub)\n\n            # choose donor indices for DE-style moves\n            n_arch = X.shape[0]\n            def pick_distinct(k):\n                if n_arch <= k:\n                    # fallback to random uniform noise centers\n                    return [self.rng.randint(0, n_arch) for _ in range(k)]\n                else:\n                    return self.rng.choice(n_arch, size=k, replace=False)\n\n            # choose a strategy\n            r = self.rng.rand()\n            if r < p_de:\n                # Strategy 1: DE-style differential injection\n                donors = pick_distinct(3)\n                x1 = X[donors[0]].copy()\n                x2 = X[donors[1]].copy()\n                x3 = X[donors[2]].copy()\n                de_scale = 0.8 * self.gscale * np.median(scale_est)\n                cand = anchor + de_scale * (x1 - x2) + 0.3 * de_scale * (x3 - anchor)\n                # if candidate too close, fallback to gaussian perturb\n                if np.linalg.norm(cand - anchor) < 1e-8:\n                    cand = anchor + (self.gscale * self.rng.randn(self.dim) * scale_est)\n            elif r < p_de + p_pca and fin_idx.size >= max(3, 2*self.dim):\n                # Strategy 2: PCA-guided elite perturbation\n                k_pc = min(5, max(1, self.dim // 2))\n                elite_count = max(2 * self.dim, min(100, fin_idx.size))\n                elite_count = min(elite_count, fin_idx.size)\n                elite_idx2 = sorted_idx_fin[:elite_count]\n                Xe = X[elite_idx2] - np.mean(X[elite_idx2], axis=0)\n                try:\n                    U, S, Vt = np.linalg.svd(Xe, full_matrices=False)\n                    pcs = Vt[:k_pc]\n                    # sample coefficients along top PCs\n                    coeffs = (self.rng.randn(k_pc) * (S[:k_pc] / (1.0 + S[:k_pc])) * self.gscale)\n                    perturb = np.dot(coeffs, pcs)\n                    cand = anchor + perturb\n                except Exception:\n                    # fallback\n                    cand = anchor + (self.gscale * self.rng.randn(self.dim) * scale_est)\n            elif r < p_de + p_pca + p_local:\n                # Strategy 3: Local anisotropic gaussian\n                anis = scale_est + 1e-12\n                cand = anchor + anis * (self.gscale * self.rng.randn(self.dim))\n                # Force at least one coordinate to change\n                if np.allclose(cand, anchor):\n                    idx = self.rng.randint(0, self.dim)\n                    cand[idx] += self.gscale * scale_est[idx] * (0.5 + self.rng.rand())\n                # occasional small uniform mix-in for diversity\n                if self.rng.rand() < 0.05:\n                    mix = self._uniform_array(lb, ub)\n                    m = 0.1 * self.rng.rand()\n                    cand = (1.0 - m) * cand + m * mix\n            else:\n                # Strategy 4: tempered Cauchy global escape\n                cand = anchor + self._tempered_cauchy(scale_est)\n                # occasionally add small directed differential to introduce guided escapes\n                if fin_idx.size >= 1 and self.rng.rand() < 0.25:\n                    best_idx = int(np.nanargmin(F))\n                    cand += 0.1 * (X[best_idx] - anchor) * self.gscale\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.03 and np.any(np.isfinite(F)):\n                best_idx = int(np.nanargmin(F))\n                cand += 0.02 * (X[best_idx] - anchor) * self.rng.rand()\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            bad = (~np.isfinite(cand))\n            if np.any(bad):\n                cand[bad] = self._uniform_array(lb[bad], ub[bad]).ravel()\n\n            # reflect and clamp to bounds\n            cand = self._reflect_bounds(cand, lb, ub, niter=5)\n\n            # evaluate candidate if budget allows\n            if self.evals >= self.budget:\n                break\n            try:\n                f_c = safe_eval(cand)\n            except RuntimeError:\n                break\n\n            self.archive_x.append(cand.copy())\n            self.archive_f.append(f_c)\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            cur_best = np.nanmin(self.archive_f)\n            finite_fs = np.asarray(self.archive_f)[np.isfinite(self.archive_f)]\n            if finite_fs.size > 0:\n                cutoff = np.percentile(finite_fs, 25.0)\n            else:\n                cutoff = np.inf\n            is_success = False\n            if np.isfinite(f_c) and f_c <= cutoff:\n                is_success = True\n            if f_c < best_so_far:\n                best_so_far = f_c\n                is_success = True\n                no_improve_counter = 0\n            else:\n                no_improve_counter += 1\n\n            # track recent successes\n            self.success_window.append(1 if is_success else 0)\n            if len(self.success_window) > self.adapt_window:\n                self.success_window.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(self.success_window) >= max(5, self.adapt_window // 2):\n                succ_rate = np.mean(self.success_window)\n                if succ_rate > 0.45:\n                    # many successes -> contract exploration slightly\n                    self.gscale *= 0.9\n                elif succ_rate < 0.08:\n                    # too few successes -> expand\n                    self.gscale *= 1.12\n                # slight jitter\n                self.gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, 1e-4, 8.0))\n\n            # prune archive when too large (keep diversity)\n            if len(self.archive_x) > self.max_archive and (len(self.archive_x) - last_prune_at) > (self.max_archive // 10):\n                last_prune_at = len(self.archive_x)\n                target_keep = int(self.max_archive * 0.9)\n                X = np.array(self.archive_x)\n                F = np.array(self.archive_f)\n                fin_idx = np.where(np.isfinite(F))[0]\n                keep_idx = set()\n\n                # keep best 40%\n                if fin_idx.size > 0:\n                    best_sorted = fin_idx[np.argsort(F[fin_idx])]\n                    nbest = max(1, int(0.4 * target_keep))\n                    keep_idx.update(best_sorted[:nbest].tolist())\n\n                # keep some randoms\n                nrand = max(1, int(0.2 * target_keep))\n                cand_rand = list(set(self.rng.randint(0, len(self.archive_x), size=2*nrand)))\n                keep_idx.update(cand_rand[:nrand])\n\n                # keep some from history (spaced)\n                last_keep = list(range(max(0, len(self.archive_x) - 10), len(self.archive_x)))\n                keep_idx.update(last_keep)\n\n                # if still too small, fill with spaced subset\n                if len(keep_idx) < target_keep:\n                    step = max(1, len(self.archive_x) // (target_keep - len(keep_idx)))\n                    spaced = list(range(0, len(self.archive_x), step))[:(target_keep - len(keep_idx))]\n                    keep_idx.update(spaced)\n\n                # finalize keep indices\n                keep_idx = sorted(list(keep_idx))[:target_keep]\n                self.archive_x = [self.archive_x[i] for i in keep_idx]\n                self.archive_f = [self.archive_f[i] for i in keep_idx]\n\n            # stagnation detection & micro-restarts\n            if no_improve_counter > self.stagnation_limit:\n                # perform micro-restart: local cloud around current best (or random if none)\n                if np.any(np.isfinite(self.archive_f)):\n                    best_idx = int(np.nanargmin(self.archive_f))\n                    center = np.array(self.archive_x[best_idx])\n                else:\n                    center = self._uniform_array(lb, ub)\n                local_scale = max(1e-6, 0.05 * (ub - lb) * (1.0 + 0.5 * self.gscale))\n                n_cloud = min(20, 5 + 2 * self.dim)\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    cand = center + (self.rng.randn(self.dim) * local_scale * (1.0 + 0.5 * self.rng.rand()))\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    try:\n                        f_c = safe_eval(cand)\n                    except RuntimeError:\n                        break\n                    self.archive_x.append(cand.copy())\n                    self.archive_f.append(f_c)\n                # also add a few global randoms to regain diversity\n                n_global = min(10, max(2, self.dim))\n                for _ in range(n_global):\n                    if self.evals >= self.budget:\n                        break\n                    x = self._uniform_array(lb, ub)\n                    try:\n                        f = safe_eval(x)\n                    except RuntimeError:\n                        break\n                    self.archive_x.append(x.copy())\n                    self.archive_f.append(f)\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale *= 1.3\n                self.gscale = float(np.clip(self.gscale, 1e-4, 8.0))\n                no_improve_counter = 0\n                self.success_window = []\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        F = np.array(self.archive_f, dtype=float)\n        X = np.array(self.archive_x, dtype=float)\n        if np.any(np.isfinite(F)):\n            best_idx = int(np.nanargmin(F))\n            return float(F[best_idx]), X[best_idx].copy()\n        else:\n            # all infinite: return a uniform fallback\n            fallback = self._uniform_array(lb, ub)\n            return np.inf, fallback", "configspace": "", "generation": 0, "feedback": "In the code, line 58, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "error": "In the code, line 58, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "dcfb40a9-c500-4e6d-aaa6-5413408b7e70", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style recombination, PCA-guided elite moves, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts for robust continuous optimization.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_frac=0.08,      # fraction of budget used for initial sampling\n                 archive_max=400,     # maximum archive size\n                 elite_frac=0.25,     # fraction considered elite\n                 gscale_init=0.5,     # global scale multiplier\n                 gscale_min=1e-6,\n                 gscale_max=8.0,\n                 adapt_window=50,     # short-term adaptation window\n                 stagnation_window=200,# micro-restart trigger window\n                 verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.init_frac = float(init_frac)\n        self.archive_max = int(archive_max)\n        self.elite_frac = float(elite_frac)\n        self.gscale = float(gscale_init)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.adapt_window = int(adapt_window)\n        self.stagnation_window = int(stagnation_window)\n        self.verbose = verbose\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func attributes, otherwise default [-5,5]^dim.\n        Returns lb, ub as numpy arrays of length self.dim.\n        \"\"\"\n        # default\n        lb = None\n        ub = None\n\n        # common patterns: func.bounds = (lb, ub) or object with .lb/.ub or .lower/.upper\n        bnd = getattr(func, 'bounds', None)\n        if bnd is not None:\n            # tuple/list style\n            if isinstance(bnd, (list, tuple)) and len(bnd) == 2:\n                lb, ub = bnd\n            else:\n                # maybe object with lb/ub\n                lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None)\n                ub = getattr(bnd, 'ub', None) or getattr(bnd, 'upper', None)\n\n        # direct attributes\n        if lb is None:\n            lb = getattr(func, 'lb', None) or getattr(func, 'lower', None)\n        if ub is None:\n            ub = getattr(func, 'ub', None) or getattr(func, 'upper', None)\n\n        # if bounds provided as scalars, broadcast\n        def _to_array(v):\n            if v is None:\n                return None\n            v = np.array(v)\n            if v.size == 1:\n                return np.repeat(float(v.item()), self.dim)\n            if v.size == 2 and self.dim == 1:\n                return v.astype(float)\n            if v.size == self.dim:\n                return v.astype(float)\n            # if it's shape (dim,2) or list of pairs -> try to take first column as lb, second as ub outside\n            return None\n\n        lb_a = _to_array(lb)\n        ub_a = _to_array(ub)\n\n        # special-case: bounds given as list of (lb_i,ub_i) pairs\n        if lb_a is None or ub_a is None:\n            if isinstance(bnd, (list, tuple)) and len(bnd) == self.dim:\n                # each element might be (low, high)\n                try:\n                    arr = np.array(bnd, dtype=float)\n                    if arr.shape == (self.dim, 2):\n                        lb_a = arr[:, 0].astype(float)\n                        ub_a = arr[:, 1].astype(float)\n                except Exception:\n                    pass\n\n        if lb_a is None or ub_a is None:\n            # fallback to default [-5,5]\n            lb_a = np.repeat(-5.0, self.dim)\n            ub_a = np.repeat(5.0, self.dim)\n\n        # final guard: ensure shapes\n        lb_a = np.asarray(lb_a, dtype=float).reshape(self.dim)\n        ub_a = np.asarray(ub_a, dtype=float).reshape(self.dim)\n        # guard that lb < ub elementwise\n        swap = lb_a > ub_a\n        if np.any(swap):\n            tmp = lb_a.copy()\n            lb_a[swap] = ub_a[swap]\n            ub_a[swap] = tmp[swap]\n        return lb_a, ub_a\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            out = self.rng.uniform(0.0, 1.0, size=(n, self.dim))\n            return lb[None, :] + out * (ub - lb)[None, :]\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling,\n        occasional per-coordinate heavy multipliers, and cap extremes.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        # standard Cauchy\n        z = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplier per coordinate\n        heavy = (self.rng.rand(self.dim) < 0.07).astype(float) * (1.0 + 4.0 * self.rng.rand(self.dim))\n        z = z * (1.0 + heavy)\n        # scale\n        step = z * scale_vec\n        # cap extremes\n        cap = cap_multiplier * scale_vec\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        # small gaussian jitter to break symmetry\n        step += 1e-6 * self.rng.randn(self.dim) * np.maximum(1.0, scale_vec)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset rng if seed provided (deterministic per-run)\n        self.rng = np.random.RandomState(self.seed)\n\n        # read bounds\n        lb, ub = self._get_bounds(func)\n\n        # init counters and archive\n        evals = 0\n        archive = []  # list of dicts: {'x':x, 'f':f, 'age': age}\n        age_counter = 0\n\n        # safe evaluation wrapper: increments eval counter and prevents calls beyond budget\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                raise RuntimeError(\"Evaluation budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            age_counter += 1\n            return f\n\n        # ---- initialization: stratified-ish LHS + uniform mix ----\n        n_init = max(3, int(self.budget * self.init_frac))\n        n_init = min(n_init, max(3, self.budget // 10))\n        # build simple Latin-hypercube like sampling by permuting strata per dim\n        init_points = np.zeros((n_init, self.dim))\n        strata = np.linspace(0.0, 1.0, n_init+1)\n        for d in range(self.dim):\n            order = self.rng.permutation(n_init)\n            # uniformly sample within each stratum\n            pts = (strata[:-1] + self.rng.rand(n_init) * (1.0 / n_init))\n            init_points[:, d] = pts[order]\n        # scale to bounds\n        init_points = lb[None, :] + init_points * (ub - lb)[None, :]\n\n        # Evaluate initial points (respect budget)\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = init_points[i]\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n        # Add a few pure uniform draws for added diversity if budget remains\n        n_uniform = min(10, max(0, (self.budget - evals) // 50))\n        for _ in range(n_uniform):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # Nothing evaluated (budget 0). Return fallback.\n        if len(archive) == 0:\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb))\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        # Initially favor exploration; probabilities adapt a little with gscale\n        base_probs = np.array([0.35, 0.20, 0.30, 0.15])\n\n        # track success history\n        success_history = []\n        best_f = min([a['f'] for a in archive if np.isfinite(a['f'])]) if any(np.isfinite([a['f'] for a in archive])) else np.inf\n        best_x = None\n        for a in archive:\n            if np.isfinite(a['f']) and a['f'] <= best_f:\n                best_f = a['f']\n                best_x = a['x'].copy()\n\n        # main loop\n        recent_best_age = 0\n        while evals < self.budget:\n            # convert archive to numpy arrays for convenience\n            X = np.vstack([a['x'] for a in archive])\n            F = np.array([a['f'] for a in archive])\n            ages = np.array([a['age'] for a in archive])\n\n            finite_mask = np.isfinite(F)\n            finite_F = F[finite_mask]\n            finite_X = X[finite_mask] if np.any(finite_mask) else X\n            # compute robust per-dim scale estimate from IQR where possible\n            if finite_X.shape[0] >= 4:\n                q75 = np.percentile(finite_X, 75, axis=0)\n                q25 = np.percentile(finite_X, 25, axis=0)\n                scale_vec = 0.74 * (q75 - q25)  # approx to std for normal\n                # fallback to std where iqr = 0\n                zero_mask = scale_vec <= 1e-12\n                if np.any(zero_mask):\n                    scale_vec[zero_mask] = np.std(finite_X[:, zero_mask], axis=0) + 1e-12\n            else:\n                scale_vec = np.std(X, axis=0) + 1e-12\n            # base on global span as additional guard\n            span = (ub - lb)\n            scale_vec = np.maximum(scale_vec, 1e-3 * span)\n\n            # guard lower bound on scale\n            scale_vec = np.maximum(scale_vec, 1e-12)\n\n            # choose anchor (center point for generating candidate)\n            n_elite = max(1, int(np.ceil(self.elite_frac * len(archive))))\n            # rank archive by f ascending, NaNs/infs at end\n            idx_sorted = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elite_idx = idx_sorted[:n_elite]\n            # exponential bias favoring best\n            weights = np.exp(-np.linspace(0, 2.0, n_elite))\n            weights = weights / np.sum(weights)\n            # sample anchor index among elites\n            if len(elite_idx) > 0 and self.rng.rand() < 0.9:\n                e_choice = self.rng.choice(elite_idx, p=weights)\n                anchor = archive[e_choice]['x'].copy()\n            else:\n                # uniform from archive\n                anchor = archive[self.rng.randint(len(archive))]['x'].copy()\n\n            # choose donor indices for DE-style moves\n            def pick_distinct(k):\n                # pick k distinct indices from archive excluding a given anchor if possible\n                ids = list(range(len(archive)))\n                # shuffle\n                self.rng.shuffle(ids)\n                # just return first k distinct points (allow duplicates if small archive)\n                out = []\n                for idd in ids:\n                    out.append(idd)\n                    if len(out) >= k:\n                        break\n                # if not enough, fill with random noise centers\n                while len(out) < k:\n                    out.append(self.rng.randint(len(archive)))\n                return out\n\n            # choose a strategy\n            probs = base_probs.copy()\n            # slightly favor global moves when gscale large\n            probs = probs * np.array([1.0, 0.9, 1.0, 1.0 + 0.5 * min(1.0, self.gscale)])\n            probs = probs / np.sum(probs)\n            strat = self.rng.choice(4, p=probs)\n\n            xcand = anchor.copy()\n            # Strategy 1: DE-style differential injection\n            if strat == 0:\n                # choose donors\n                ids = pick_distinct(3)\n                xa = archive[ids[0]]['x']\n                xb = archive[ids[1]]['x']\n                xc = archive[ids[2]]['x']\n                # differential factor scales with gscale\n                Fcoeff = 0.8 * (1.0 + 0.5 * (self.gscale - 0.5))\n                diff = xa + Fcoeff * (xb - xc)\n                # mix with anchor and small anisotropic gaussian\n                alpha = 0.6\n                xcand = alpha * anchor + (1.0 - alpha) * diff\n                xcand += (self.gscale * 0.5) * (self.rng.randn(self.dim) * scale_vec)\n                # if too close, fallback to gaussian\n                if np.linalg.norm(xcand - anchor) < 1e-12:\n                    xcand = anchor + 0.1 * self.gscale * (self.rng.randn(self.dim) * scale_vec)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif strat == 1:\n                # attempt PCA on elites\n                if len(elite_idx) >= 2:\n                    mat = np.vstack([archive[i]['x'] for i in elite_idx])\n                    # center\n                    c = mat.mean(axis=0)\n                    M = mat - c\n                    try:\n                        cov = np.cov(M, rowvar=False)\n                        # small regularization\n                        cov += np.eye(self.dim) * 1e-8\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort descending\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # sample coefficients along top PCs\n                        n_pcs = max(1, min(self.dim, int(1 + np.sum(eigvals > 1e-12))))\n                        coeffs = self.rng.randn(n_pcs) * (np.sqrt(np.maximum(eigvals[:n_pcs], 1e-12)) * self.gscale)\n                        step = eigvecs[:, :n_pcs].dot(coeffs)\n                        xcand = c + step\n                    except Exception:\n                        # fallback small gaussian\n                        xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n                else:\n                    xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif strat == 2:\n                # anisotropy per-dim\n                anis = 0.2 + 0.8 * self.rng.rand(self.dim)\n                sigma = scale_vec * (0.6 * anis + 0.4) * self.gscale\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # Force at least one coordinate to change\n                idx_change = self.rng.randint(self.dim)\n                if abs(xcand[idx_change] - anchor[idx_change]) < 1e-12:\n                    xcand[idx_change] += sigma[idx_change] * (0.5 + self.rng.rand())\n                # occasional small uniform mix-in for diversity\n                if self.rng.rand() < 0.06:\n                    mix = self.rng.rand(self.dim) < 0.08\n                    xcand[mix] = self._uniform_array(lb[mix], ub[mix], n=1) if np.sum(mix) > 0 else xcand[mix]\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                xcand = anchor + self._tempered_cauchy(scale_vec * max(1.0, 6.0 * self.gscale))\n                # occasionally add small directed differential to introduce guided escapes\n                if self.rng.rand() < 0.3:\n                    ids = pick_distinct(2)\n                    xcand += 0.2 * self.gscale * (archive[ids[0]]['x'] - archive[ids[1]]['x'])\n\n            # occasional directed small jump to current best for refinement\n            if best_x is not None and self.rng.rand() < 0.08:\n                xcand = 0.7 * xcand + 0.3 * best_x\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            if not np.all(np.isfinite(xcand)):\n                bad = ~np.isfinite(xcand)\n                xcand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect and clamp to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub, niter=4)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            fnew = safe_eval(xcand)\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            finiteF = F[np.isfinite(F)] if np.any(np.isfinite(F)) else np.array([])\n            threshold = np.percentile(np.where(np.isfinite(F), F, np.inf), 25) if finiteF.size > 0 else np.inf\n            is_success = False\n            if np.isfinite(fnew) and fnew <= threshold:\n                is_success = True\n            if np.isfinite(fnew) and fnew < best_f:\n                best_f = fnew\n                best_x = xcand.copy()\n                is_success = True\n                recent_best_age = age_counter\n\n            success_history.append(1 if is_success else 0)\n            if len(success_history) > self.adapt_window:\n                success_history.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(success_history) >= self.adapt_window and (age_counter % max(1, (self.adapt_window // 4)) == 0):\n                succ_rate = sum(success_history) / len(success_history)\n                # many successes -> contract exploration slightly\n                if succ_rate > 0.3:\n                    self.gscale *= 0.92\n                # too few successes -> expand\n                elif succ_rate < 0.06:\n                    self.gscale *= 1.12\n                # slight jitter\n                self.gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n                # clamp gscale\n                self.gscale = np.clip(self.gscale, self.gscale_min, self.gscale_max)\n\n            # prune archive when too large (keep diversity)\n            if len(archive) > self.archive_max:\n                # keep best 40%\n                kbest = max(1, int(0.4 * self.archive_max))\n                idx_sorted = np.argsort(np.where(np.isfinite(np.array([a['f'] for a in archive])), np.array([a['f'] for a in archive]), np.inf))\n                keep = set(idx_sorted[:kbest].tolist())\n                # keep some randoms\n                nrand = int(0.1 * self.archive_max)\n                rand_idx = self.rng.choice([i for i in range(len(archive)) if i not in keep], size=min(nrand, len(archive)-len(keep)), replace=False)\n                keep.update(rand_idx.tolist())\n                # keep some from history (spaced)\n                n_hist = int(0.15 * self.archive_max)\n                if len(archive) > 0:\n                    spaced = list(range(0, len(archive), max(1, len(archive)//(n_hist+1))))\n                    spaced = [i for i in spaced if i not in keep]\n                    spaced = spaced[:n_hist]\n                    keep.update(spaced)\n                # if still too small, fill with spaced subset\n                if len(keep) < self.archive_max:\n                    all_idx = list(range(len(archive)))\n                    self.rng.shuffle(all_idx)\n                    for ii in all_idx:\n                        if ii not in keep:\n                            keep.add(ii)\n                        if len(keep) >= self.archive_max:\n                            break\n                # finalize keep indices\n                new_archive = [archive[i] for i in sorted(keep)]\n                archive = new_archive\n\n            # stagnation detection & micro-restarts\n            no_improve_steps = age_counter - recent_best_age\n            if no_improve_steps > self.stagnation_window and evals < self.budget:\n                if self.verbose:\n                    print(\"Micro-restart: no improvement for\", no_improve_steps, \"steps; evals:\", evals)\n                # perform micro-restart: local cloud around current best (or random if none)\n                cloud_size = min(40, max(6, int(0.05 * self.budget)))\n                center = best_x.copy() if best_x is not None else self._uniform_array(lb, ub, n=1)\n                for i in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    perturb = (self.gscale * 0.6) * (self.rng.randn(self.dim) * scale_vec)\n                    xc = center + perturb\n                    xc = self._reflect_bounds(xc, lb, ub)\n                    fxc = safe_eval(xc)\n                    archive.append({'x': xc.copy(), 'f': fxc, 'age': age_counter})\n                    if np.isfinite(fxc) and fxc < best_f:\n                        best_f = fxc\n                        best_x = xc.copy()\n                # also add a few global randoms to regain diversity\n                for _ in range(min(10, self.budget - evals)):\n                    if evals >= self.budget:\n                        break\n                    xr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.3, self.gscale_max)\n                success_history = []\n                recent_best_age = age_counter\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        Finite = [a for a in archive if np.isfinite(a['f'])]\n        if len(Finite) > 0:\n            best = min(Finite, key=lambda a: a['f'])\n            return best['f'], best['x'].copy()\n        else:\n            # all infinite: return a uniform fallback\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb))\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb", "configspace": "", "generation": 0, "feedback": "In the code, line 58, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None)", "error": "In the code, line 58, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "67f9f24a-06c5-4d1c-afbf-b116fdd2d1a9", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style donor recombination, PCA-guided elite perturbations, anisotropic gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (MemoryGuidedADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_frac=0.08,      # fraction of budget used for initial sampling\n                 archive_max=400,     # maximum archive size\n                 elite_frac=0.25,     # fraction considered elite\n                 gscale_init=0.5,     # global scale multiplier\n                 gscale_min=1e-6,\n                 gscale_max=8.0,\n                 adapt_window=50,     # short-term adaptation window\n                 stagnation_window=200,# micro-restart trigger window\n                 verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_frac = float(init_frac)\n        self.archive_max = int(archive_max)\n        self.elite_frac = float(elite_frac)\n        self.gscale = float(gscale_init)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.adapt_window = int(adapt_window)\n        self.stagnation_window = int(stagnation_window)\n        self.verbose = verbose\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func attributes, otherwise default [-5,5]^dim.\n        Returns lb, ub as numpy arrays of length self.dim.\n        \"\"\"\n        # default\n        lb = None\n        ub = None\n\n        # common patterns: func.bounds = (lb, ub) or object with .lb/.ub or .lower/.upper\n        bnd = getattr(func, 'bounds', None)\n        if bnd is not None:\n            # tuple/list style\n            if isinstance(bnd, (list, tuple)) and len(bnd) == 2:\n                lb, ub = bnd\n            else:\n                # maybe object with lb/ub or lower/upper\n                lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None) or lb\n                ub = getattr(bnd, 'ub', None) or getattr(bnd, 'upper', None) or ub\n\n        # direct attributes on func\n        if lb is None:\n            lb = getattr(func, 'lb', None) or getattr(func, 'lower', None)\n        if ub is None:\n            ub = getattr(func, 'ub', None) or getattr(func, 'upper', None)\n\n        # helper to broadcast scalars or 1-d arrays\n        def _to_array(v):\n            if v is None:\n                return None\n            a = np.array(v, dtype=float)\n            if a.size == 1:\n                return np.repeat(float(a.item()), self.dim)\n            if a.size == self.dim:\n                return a.astype(float)\n            # allow if shape (dim,2)\n            if a.ndim == 2 and a.shape[0] == self.dim and a.shape[1] == 2:\n                return None  # will be handled below\n            # fallback\n            return None\n\n        lb_a = _to_array(lb)\n        ub_a = _to_array(ub)\n\n        # special-case: bounds given as list of (lb_i,ub_i) pairs\n        if (lb_a is None or ub_a is None) and bnd is not None:\n            try:\n                arr = np.array(bnd, dtype=float)\n                if arr.shape == (self.dim, 2):\n                    lb_a = arr[:, 0].astype(float)\n                    ub_a = arr[:, 1].astype(float)\n            except Exception:\n                pass\n\n        if lb_a is None or ub_a is None:\n            # fallback to default [-5,5]\n            lb_a = np.repeat(-5.0, self.dim)\n            ub_a = np.repeat(5.0, self.dim)\n\n        # final guard: ensure shapes\n        lb_a = np.asarray(lb_a, dtype=float).reshape((self.dim,))\n        ub_a = np.asarray(ub_a, dtype=float).reshape((self.dim,))\n        # guard that lb <= ub elementwise\n        swap = lb_a > ub_a\n        if np.any(swap):\n            tmp = lb_a.copy()\n            lb_a[swap] = ub_a[swap]\n            ub_a[swap] = tmp[swap]\n        return lb_a, ub_a\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            out = self.rng.rand(n, self.dim)\n            return lb[None, :] + out * (ub - lb)[None, :]\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy:\n        tan(pi*(u-0.5)) gives standard Cauchy samples; we temper by scaling,\n        occasional per-coordinate heavy multipliers, and cap extremes.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        # standard Cauchy\n        z = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplier per coordinate\n        heavy = (self.rng.rand(self.dim) < 0.07).astype(float) * (1.0 + 4.0 * self.rng.rand(self.dim))\n        z = z * (1.0 + heavy)\n        # scale\n        step = z * scale_vec\n        # cap extremes\n        cap = cap_multiplier * scale_vec\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        # small gaussian jitter to break symmetry\n        step += 1e-6 * self.rng.randn(self.dim) * np.maximum(1.0, scale_vec)\n        return step\n\n    # ---------------- main call ----------------\n    def __call__(self, func):\n        # reset rng if seed provided (deterministic per-run)\n        self.rng = np.random.RandomState(self.seed)\n\n        # read bounds\n        lb, ub = self._get_bounds(func)\n\n        # init counters and archive\n        evals = 0\n        archive = []  # list of dicts: {'x':x, 'f':f, 'age': age}\n        age_counter = 0\n\n        # safe evaluation wrapper: increments eval counter and prevents calls beyond budget\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                raise RuntimeError(\"Evaluation budget exhausted\")\n            try:\n                f = float(func(x))\n            except Exception:\n                # if func errors, treat as bad (infinite)\n                f = np.inf\n            evals += 1\n            age_counter += 1\n            return f\n\n        # ---- initialization: stratified-ish LHS + uniform mix ----\n        n_init = max(3, int(self.budget * self.init_frac))\n        n_init = min(n_init, max(3, self.budget))  # cannot exceed budget\n        # build simple Latin-hypercube like sampling by permuting strata per dim\n        init_points = np.zeros((n_init, self.dim))\n        strata = np.linspace(0.0, 1.0, n_init+1)\n        for d in range(self.dim):\n            order = self.rng.permutation(n_init)\n            # uniformly sample within each stratum\n            pts = (strata[:-1] + self.rng.rand(n_init) * (1.0 / n_init))\n            init_points[:, d] = pts[order]\n        # scale to bounds\n        init_points = lb[None, :] + init_points * (ub - lb)[None, :]\n\n        # Evaluate initial points (respect budget)\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = init_points[i]\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # Add a few pure uniform draws for added diversity if budget remains\n        n_uniform = min(10, max(0, (self.budget - evals) // 50))\n        for _ in range(n_uniform):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # Nothing evaluated (budget 0). Return fallback.\n        if len(archive) == 0:\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb))\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb\n\n        # Mix probabilities for strategies: DE, PCA, Local, Cauchy\n        # Initially favor exploration; probabilities adapt a little with gscale\n        base_probs = np.array([0.35, 0.20, 0.30, 0.15])\n\n        # track success history\n        success_history = []\n        best_f = np.inf\n        best_x = None\n        for a in archive:\n            if np.isfinite(a['f']) and a['f'] < best_f:\n                best_f = a['f']\n                best_x = a['x'].copy()\n\n        recent_best_age = age_counter\n\n        # main loop\n        while evals < self.budget:\n            # convert archive to numpy arrays for convenience\n            X = np.vstack([a['x'] for a in archive])\n            F = np.array([a['f'] for a in archive])\n            ages = np.array([a['age'] for a in archive])\n\n            finite_mask = np.isfinite(F)\n            finite_X = X[finite_mask] if np.any(finite_mask) else X\n            # compute robust per-dim scale estimate from IQR where possible\n            if finite_X.shape[0] >= 4:\n                q25 = np.percentile(finite_X, 25, axis=0)\n                q75 = np.percentile(finite_X, 75, axis=0)\n                scale_vec = 0.74 * (q75 - q25)  # approx to std for normal\n                # fallback to std where iqr = 0\n                zero_mask = scale_vec <= 1e-12\n                if np.any(zero_mask):\n                    stds = np.std(finite_X[:, zero_mask], axis=0)\n                    stds[stds <= 1e-12] = 1e-12\n                    scale_vec[zero_mask] = stds\n            else:\n                scale_vec = np.std(X, axis=0) + 1e-12\n            # base on global span as additional guard\n            span = (ub - lb)\n            scale_vec = np.maximum(scale_vec, 1e-3 * span)\n            # guard lower bound on scale\n            scale_vec = np.maximum(scale_vec, 1e-12)\n\n            # choose anchor (center point for generating candidate)\n            n_elite = max(1, int(np.ceil(self.elite_frac * len(archive))))\n            # rank archive by f ascending, NaNs/infs at end\n            finite_indices = np.where(np.isfinite(F))[0]\n            if finite_indices.size > 0:\n                sorted_idx = finite_indices[np.argsort(F[finite_indices])]\n                idx_sorted = list(sorted_idx) + [i for i in range(len(archive)) if i not in sorted_idx]\n            else:\n                idx_sorted = list(range(len(archive)))\n\n            elite_idx = idx_sorted[:n_elite]\n            # exponential bias favoring best\n            weights = np.exp(-np.linspace(0, 2.0, n_elite))\n            weights = weights / np.sum(weights)\n\n            # sample anchor index among elites\n            if len(elite_idx) > 0 and self.rng.rand() < 0.9:\n                e_choice = self.rng.choice(elite_idx, p=weights)\n                anchor = archive[e_choice]['x'].copy()\n                anchor_idx = e_choice\n            else:\n                anchor_idx = self.rng.randint(len(archive))\n                anchor = archive[anchor_idx]['x'].copy()\n\n            # choose donor indices for DE-style moves\n            def pick_distinct(k, exclude=None):\n                ids = list(range(len(archive)))\n                if exclude is not None and exclude in ids:\n                    ids.remove(exclude)\n                if len(ids) >= k:\n                    return list(self.rng.choice(ids, size=k, replace=False))\n                else:\n                    # allow duplicates if archive small\n                    return [self.rng.randint(len(archive)) for _ in range(k)]\n\n            # choose a strategy\n            probs = base_probs.copy()\n            # slightly favor global moves when gscale large\n            probs = probs * np.array([1.0, 0.9, 1.0, 1.0 + 0.5 * min(1.0, self.gscale)])\n            probs = probs / np.sum(probs)\n            strat = self.rng.choice(4, p=probs)\n\n            xcand = anchor.copy()\n\n            # Strategy 1: DE-style differential injection\n            if strat == 0:\n                ids = pick_distinct(3, exclude=anchor_idx)\n                xa = archive[ids[0]]['x']\n                xb = archive[ids[1]]['x']\n                xc = archive[ids[2]]['x']\n                # differential factor scales with gscale\n                Fcoeff = 0.8 * (1.0 + 0.5 * (self.gscale - 0.5))\n                diff = xa + Fcoeff * (xb - xc)\n                # mix with anchor and small anisotropic gaussian\n                alpha = 0.6\n                xcand = alpha * anchor + (1.0 - alpha) * diff\n                xcand += (self.gscale * 0.5) * (self.rng.randn(self.dim) * scale_vec)\n                # if too close, fallback to gaussian\n                if np.linalg.norm(xcand - anchor) < 1e-12:\n                    xcand = anchor + 0.1 * self.gscale * (self.rng.randn(self.dim) * scale_vec)\n\n            # Strategy 2: PCA-guided elite perturbation\n            elif strat == 1:\n                mat = np.vstack([archive[i]['x'] for i in elite_idx])\n                if mat.shape[0] >= 2:\n                    # center\n                    c = np.mean(mat, axis=0)\n                    M = mat - c\n                    try:\n                        cov = np.cov(M, rowvar=False)\n                        # small regularization\n                        cov += np.eye(self.dim) * 1e-8\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        # sort descending\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # choose number of pcs (at least 1)\n                        n_pcs = max(1, min(self.dim, int(np.ceil(0.3 * self.dim))))\n                        # sample coefficients along top PCs\n                        coeffs = self.rng.randn(n_pcs) * (np.sqrt(np.maximum(eigvals[:n_pcs], 1e-12)) * self.gscale)\n                        step = eigvecs[:, :n_pcs].dot(coeffs)\n                        xcand = c + step\n                    except Exception:\n                        # fallback small gaussian\n                        xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n                else:\n                    xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n\n            # Strategy 3: Local anisotropic gaussian\n            elif strat == 2:\n                # anisotropy per-dim\n                anis = 0.2 + 0.8 * self.rng.rand(self.dim)\n                sigma = scale_vec * (0.6 * anis + 0.4) * self.gscale\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # Force at least one coordinate to change\n                idx_change = self.rng.randint(self.dim)\n                if abs(xcand[idx_change] - anchor[idx_change]) < 1e-12:\n                    xcand[idx_change] += sigma[idx_change] * (0.5 + self.rng.rand())\n                # occasional small uniform mix-in for diversity\n                if self.rng.rand() < 0.06:\n                    mix = self.rng.rand(self.dim) < 0.08\n                    if np.any(mix):\n                        uni = self._uniform_array(lb[mix], ub[mix], n=1)\n                        xcand[mix] = uni\n\n            # Strategy 4: tempered Cauchy global escape\n            else:\n                xcand = anchor + self._tempered_cauchy(scale_vec * max(1.0, 6.0 * self.gscale))\n                # occasionally add small directed differential to introduce guided escapes\n                if self.rng.rand() < 0.3:\n                    ids = pick_distinct(2, exclude=anchor_idx)\n                    xcand += 0.2 * self.gscale * (archive[ids[0]]['x'] - archive[ids[1]]['x'])\n\n            # occasional directed small jump to current best for refinement\n            if best_x is not None and self.rng.rand() < 0.08:\n                xcand = 0.7 * xcand + 0.3 * best_x\n\n            # sanitize candidate: replace NaN/inf coords with uniform draws\n            bad = ~np.isfinite(xcand)\n            if np.any(bad):\n                # draw uniformly for bad coordinates\n                xcand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect and clamp to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub, niter=4)\n\n            # evaluate candidate if budget allows\n            if evals >= self.budget:\n                break\n            fnew = safe_eval(xcand)\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # measure success: new sample is success if it is among top 25% finite or improves global best\n            finiteF = F[np.isfinite(F)] if np.any(np.isfinite(F)) else np.array([])\n            if finiteF.size > 0:\n                threshold = np.percentile(np.where(np.isfinite(F), F, np.inf), 25)\n            else:\n                threshold = np.inf\n            is_success = False\n            if np.isfinite(fnew) and (fnew <= threshold or fnew < best_f):\n                is_success = True\n            if np.isfinite(fnew) and fnew < best_f:\n                best_f = fnew\n                best_x = xcand.copy()\n                recent_best_age = age_counter\n\n            success_history.append(1 if is_success else 0)\n            # keep success_history length bounded\n            if len(success_history) > self.adapt_window:\n                success_history.pop(0)\n\n            # adapt gscale occasionally (short-term adaptation)\n            if len(success_history) >= self.adapt_window and (age_counter % max(1, (self.adapt_window // 4)) == 0):\n                succ_rate = sum(success_history) / len(success_history)\n                # many successes -> contract exploration slightly\n                if succ_rate > 0.3:\n                    self.gscale *= 0.92\n                # too few successes -> expand\n                elif succ_rate < 0.06:\n                    self.gscale *= 1.12\n                # slight jitter\n                self.gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n                # clamp gscale\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep diversity)\n            if len(archive) > self.archive_max:\n                # keep best 40%\n                kbest = max(1, int(0.4 * self.archive_max))\n                finite_idx = [i for i in range(len(archive)) if np.isfinite(archive[i]['f'])]\n                finite_sorted = sorted(finite_idx, key=lambda i: archive[i]['f'])\n                keep = set(finite_sorted[:kbest])\n                # keep some randoms\n                nrand = int(0.1 * self.archive_max)\n                remaining = [i for i in range(len(archive)) if i not in keep]\n                if len(remaining) > 0 and nrand > 0:\n                    rand_idx = self.rng.choice(remaining, size=min(nrand, len(remaining)), replace=False)\n                    keep.update(rand_idx.tolist())\n                # keep some from history (spaced)\n                n_hist = int(0.15 * self.archive_max)\n                if len(archive) > 0 and n_hist > 0:\n                    spaced = list(range(0, len(archive), max(1, len(archive)//(n_hist+1))))\n                    spaced = [i for i in spaced if i not in keep]\n                    spaced = spaced[:n_hist]\n                    keep.update(spaced)\n                # if still too small, fill with random picks\n                if len(keep) < self.archive_max:\n                    all_idx = list(range(len(archive)))\n                    self.rng.shuffle(all_idx)\n                    for ii in all_idx:\n                        if ii not in keep:\n                            keep.add(ii)\n                        if len(keep) >= self.archive_max:\n                            break\n                # finalize keep indices\n                new_archive = [archive[i] for i in sorted(keep)]\n                archive = new_archive\n\n            # stagnation detection & micro-restarts\n            no_improve_steps = age_counter - recent_best_age\n            if no_improve_steps > self.stagnation_window and evals < self.budget:\n                if self.verbose:\n                    print(\"Micro-restart: no improvement for\", no_improve_steps, \"steps; evals:\", evals)\n                # perform micro-restart: local cloud around current best (or random if none)\n                cloud_size = min(40, max(6, int(0.05 * self.budget)))\n                center = best_x.copy() if best_x is not None else self._uniform_array(lb, ub, n=1)\n                for i in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    perturb = (self.gscale * 0.6) * (self.rng.randn(self.dim) * scale_vec)\n                    xc = self._reflect_bounds(center + perturb, lb, ub, niter=2)\n                    fxc = safe_eval(xc)\n                    archive.append({'x': xc.copy(), 'f': fxc, 'age': age_counter})\n                    if np.isfinite(fxc) and fxc < best_f:\n                        best_f = fxc\n                        best_x = xc.copy()\n                        recent_best_age = age_counter\n                # also add a few global randoms to regain diversity\n                for _ in range(min(10, self.budget - evals)):\n                    if evals >= self.budget:\n                        break\n                    xr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        recent_best_age = age_counter\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.3, self.gscale_max)\n                success_history = []\n                recent_best_age = age_counter\n\n        # final best: ensure we return the best finite from archive (or fallback)\n        Finite = [a for a in archive if np.isfinite(a['f'])]\n        if len(Finite) > 0:\n            best = min(Finite, key=lambda a: a['f'])\n            return best['f'], best['x'].copy()\n        else:\n            # all infinite: return a uniform fallback\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb))\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb", "configspace": "", "generation": 0, "feedback": "In the code, line 57, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None) or lb", "error": "In the code, line 57, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None) or lb", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6b552a15-ed36-46f2-bf7a-7547bfc6ed1c", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style recombination, PCA-guided elite moves, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-Guided Adaptive Directional Sampling).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_frac=0.08,      # fraction of budget used for initial sampling\n                 archive_max=400,     # maximum archive size\n                 elite_frac=0.25,     # fraction considered elite\n                 gscale_min=1e-6,\n                 gscale_max=8.0,\n                 adapt_window=50,     # short-term adaptation window (success history)\n                 stagnation_window=200,# micro-restart trigger window (no improvements)\n                 verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.init_frac = float(init_frac)\n        self.archive_max = int(archive_max)\n        self.elite_frac = float(elite_frac)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.adapt_window = int(adapt_window)\n        self.stagnation_window = int(stagnation_window)\n        self.verbose = verbose\n\n        # dynamic state\n        self.gscale = 0.8  # global scale multiplier (adapted)\n        # Base probabilities for strategies: DE, PCA, Local, Cauchy\n        self.base_probs = np.array([0.35, 0.20, 0.30, 0.15])\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func attributes, otherwise default [-5,5]^dim.\n        Returns lb, ub as numpy arrays of length self.dim.\n        \"\"\"\n        # defaults\n        lb = None\n        ub = None\n\n        # common pattern: func.bounds = (lb, ub) where lb/ub can be scalars, arrays, or list of pairs\n        bnd = getattr(func, 'bounds', None)\n        if bnd is not None:\n            if isinstance(bnd, (list, tuple)) and len(bnd) == 2:\n                lb, ub = bnd\n            else:\n                # maybe object with .lb/.ub or .lower/.upper\n                lb = getattr(bnd, 'lb', lb) or getattr(bnd, 'lower', lb)\n                ub = getattr(bnd, 'ub', ub) or getattr(bnd, 'upper', ub)\n\n        # direct attributes on func\n        if lb is None:\n            lb = getattr(func, 'lb', None) or getattr(func, 'lower', None)\n        if ub is None:\n            ub = getattr(func, 'ub', None) or getattr(func, 'upper', None)\n\n        # helper to convert to shape (dim,)\n        def to_arr(v):\n            if v is None:\n                return None\n            arr = np.array(v, dtype=float)\n            if arr.size == 1:\n                return np.repeat(arr.item(), self.dim)\n            if arr.size == self.dim:\n                return arr.reshape(self.dim)\n            # sometimes bounds provided as shape (dim,2) or list of pairs -> handled outside\n            return None\n\n        lb_a = to_arr(lb)\n        ub_a = to_arr(ub)\n\n        # special case: bounds provided as list of (low,high) per-dim\n        if (lb_a is None or ub_a is None) and isinstance(bnd, (list, tuple)) and len(bnd) == self.dim:\n            try:\n                arr = np.array(bnd, dtype=float)\n                if arr.shape == (self.dim, 2):\n                    lb_a = arr[:, 0].astype(float)\n                    ub_a = arr[:, 1].astype(float)\n            except Exception:\n                pass\n\n        # final fallback\n        if lb_a is None:\n            lb_a = np.repeat(-5.0, self.dim)\n        if ub_a is None:\n            ub_a = np.repeat(5.0, self.dim)\n\n        lb_a = np.asarray(lb_a, dtype=float).reshape(self.dim)\n        ub_a = np.asarray(ub_a, dtype=float).reshape(self.dim)\n\n        # ensure lb <= ub elementwise (swap if provided reverse)\n        swap = lb_a > ub_a\n        if np.any(swap):\n            tmp = lb_a.copy()\n            lb_a[swap] = ub_a[swap]\n            ub_a[swap] = tmp[swap]\n\n        return lb_a, ub_a\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = x.copy()\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            out = self.rng.rand(n, self.dim)\n            return lb[None, :] + out * (ub - lb)[None, :]\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy: returns vector of length dim.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # occasional per-coordinate heavy multiplier\n        heavy_mask = (self.rng.rand(self.dim) < 0.07)\n        heavy_mult = 1.0 + 4.0 * self.rng.rand(self.dim) * heavy_mask.astype(float)\n        z = z * heavy_mult\n        step = z * scale_vec\n        cap = cap_multiplier * scale_vec\n        step = np.sign(step) * np.minimum(np.abs(step), cap)\n        # small gaussian jitter to break pure symmetry\n        step = step + 0.01 * (scale_vec * self.rng.randn(self.dim))\n        return step\n\n    # ---------- main entry ----------\n    def __call__(self, func):\n        # reset RNG for reproducibility\n        self.rng = np.random.RandomState(self.seed)\n\n        # read bounds\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        # evaluate budget guard\n        evals = 0\n\n        # archive: list of dicts with {'x':..., 'f':..., 'age':...}\n        archive = []\n        age_counter = 0\n\n        # safe evaluation wrapper: never call func if budget exhausted\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                # do not call function; return inf\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            age_counter += 1\n            return f\n\n        # ---- initialization (LHS-like + some pure randoms) ----\n        n_init = int(max(3, min(self.budget - 1, max(5, int(self.init_frac * self.budget)))))\n        # LHS-like: permute strata per-dim\n        init_points = np.zeros((n_init, self.dim))\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        for d in range(self.dim):\n            order = self.rng.permutation(n_init)\n            pts = strata[:-1] + self.rng.rand(n_init) * (1.0 / n_init)\n            init_points[:, d] = pts[order]\n        init_points = lb[None, :] + init_points * (ub - lb)[None, :]\n\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = init_points[i]\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # Add a few pure uniform draws\n        n_uniform = min(10, max(0, (self.budget - evals) // 50))\n        for _ in range(n_uniform):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # If nothing evaluated (budget 0), return fallback uniform\n        if len(archive) == 0:\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb)) if self.budget > 0 else np.inf\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb\n\n        # initial best\n        all_f = np.array([a['f'] for a in archive], dtype=float)\n        finite_mask = np.isfinite(all_f)\n        if np.any(finite_mask):\n            idx_best = np.nanargmin(np.where(finite_mask, all_f, np.inf))\n            best_f = all_f[idx_best]\n            best_x = archive[idx_best]['x'].copy()\n            last_improve_eval = evals\n        else:\n            best_f = np.inf\n            best_x = None\n            last_improve_eval = 0\n\n        # history for adaptation\n        success_history = []\n\n        # main loop\n        while evals < self.budget:\n            # convert archive arrays\n            X = np.vstack([a['x'] for a in archive])\n            F = np.array([a['f'] for a in archive], dtype=float)\n            ages = np.array([a['age'] for a in archive], dtype=int)\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            finite_rows = np.isfinite(F)\n            if np.any(finite_rows) and np.sum(finite_rows) >= 2:\n                finite_X = X[finite_rows]\n                q75 = np.percentile(finite_X, 75, axis=0)\n                q25 = np.percentile(finite_X, 25, axis=0)\n                scale_vec = 0.74 * (q75 - q25)\n                zero_mask = scale_vec <= 1e-12\n                if np.any(zero_mask):\n                    stds = np.std(finite_X[:, zero_mask], axis=0)\n                    stds = np.where(stds <= 1e-12, 1e-12, stds)\n                    scale_vec[zero_mask] = stds\n            else:\n                scale_vec = np.std(X, axis=0)\n                scale_vec[scale_vec <= 1e-12] = 1e-12\n\n            scale_vec = np.maximum(scale_vec, 1e-3 * span)\n            scale_vec = np.maximum(scale_vec, 1e-12)\n\n            # elites\n            n_elite = max(1, int(np.ceil(self.elite_frac * len(archive))))\n            finite_sorted_idx = np.argsort(np.where(np.isfinite(F), F, np.inf))\n            elite_idx = finite_sorted_idx[:n_elite]\n\n            # sample anchor among elites with exponential bias\n            anchor = archive[self.rng.randint(len(archive))]['x'].copy()\n            if len(elite_idx) > 0:\n                weights = np.exp(-np.linspace(0, 2.0, len(elite_idx)))\n                weights = weights / np.sum(weights)\n                if self.rng.rand() < 0.9:\n                    choice = self.rng.choice(elite_idx, p=weights)\n                    anchor = archive[choice]['x'].copy()\n\n            # helper to pick distinct donors\n            def pick_distinct(k, exclude_idx=None):\n                ids = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in ids:\n                    ids.remove(exclude_idx)\n                self.rng.shuffle(ids)\n                out = ids[:k]\n                # allow duplicates if not enough\n                while len(out) < k:\n                    out.append(self.rng.randint(len(archive)))\n                return out\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            probs = probs * np.array([1.0, 0.95, 1.0, 1.0 + 0.6 * min(1.0, self.gscale)])\n            probs = probs / np.sum(probs)\n            strat = self.rng.choice(4, p=probs)\n\n            xcand = anchor.copy()\n            # prepare donors for DE-like if needed\n            donors = pick_distinct(3)\n            xa = archive[donors[0]]['x']\n            xb = archive[donors[1]]['x']\n            xc = archive[donors[2]]['x']\n\n            # Strategy 0: DE-style differential injection\n            if strat == 0:\n                # differential factor scale depends on gscale and local spread\n                Fcoeff = 0.8 * (1.0 + 0.5 * (self.gscale - 1.0))\n                diff = xa + Fcoeff * (xb - xc)\n                # blend with anchor\n                alpha = 0.6\n                noise = (self.rng.randn(self.dim) * (0.08 + 0.6 * self.rng.rand(self.dim))) * scale_vec * self.gscale\n                xcand = alpha * anchor + (1.0 - alpha) * diff + noise\n                # occasionally inject a bit of Cauchy for escape\n                if self.rng.rand() < 0.06:\n                    xcand += 0.6 * self._tempered_cauchy(scale_vec * self.gscale)\n\n            # Strategy 1: PCA-guided elite perturbation\n            elif strat == 1:\n                if len(elite_idx) >= 2:\n                    mat = np.vstack([archive[i]['x'] for i in elite_idx])\n                    c = mat.mean(axis=0)\n                    M = mat - c\n                    cov = np.cov(M, rowvar=False)\n                    # regularize\n                    cov += np.eye(self.dim) * (1e-8 * np.mean(np.diag(cov) + 1e-8))\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # sample along top pcs (random number)\n                        n_pcs = max(1, min(self.dim, int(1 + np.sum(eigvals > 1e-12))))\n                        coeffs = self.rng.randn(n_pcs) * (np.sqrt(np.maximum(eigvals[:n_pcs], 1e-12)) * self.gscale)\n                        step = eigvecs[:, :n_pcs].dot(coeffs)\n                        xcand = c + step\n                        # small blend towards anchor\n                        if self.rng.rand() < 0.7:\n                            xcand = 0.7 * anchor + 0.3 * xcand\n                    except Exception:\n                        # fallback gaussian around anchor\n                        xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n                else:\n                    xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n\n            # Strategy 2: Local anisotropic gaussian\n            elif strat == 2:\n                anis = 0.2 + 0.8 * self.rng.rand(self.dim)\n                sigma = scale_vec * (0.5 * anis + 0.5) * self.gscale\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # ensure at least one coordinate changes\n                idx_change = self.rng.randint(self.dim)\n                if abs(xcand[idx_change] - anchor[idx_change]) < 1e-14:\n                    xcand[idx_change] += sigma[idx_change] * (0.5 + self.rng.rand())\n                # occasional mixed uniform per-dim\n                if self.rng.rand() < 0.06:\n                    mix = self.rng.rand(self.dim) < 0.08\n                    if np.any(mix):\n                        xcand[mix] = self._uniform_array(lb[mix], ub[mix], n=1)\n\n            # Strategy 3: tempered Cauchy global escape\n            else:\n                xcand = anchor.copy()\n                cauch = self._tempered_cauchy(scale_vec * (1.0 + 0.5 * self.gscale))\n                xcand += cauch\n                # sometimes add directed differential\n                if self.rng.rand() < 0.3:\n                    xcand += 0.2 * self.gscale * (archive[donors[0]]['x'] - archive[donors[1]]['x'])\n\n            # occasional directed small jump to current best for refinement\n            if best_x is not None and self.rng.rand() < 0.08:\n                xcand = 0.7 * xcand + 0.3 * best_x\n\n            # sanitize candidate\n            if not np.all(np.isfinite(xcand)):\n                bad = ~np.isfinite(xcand)\n                xcand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub, niter=4)\n\n            # evaluate candidate\n            fnew = safe_eval(xcand)\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # update best if improved\n            is_improved = np.isfinite(fnew) and (fnew < best_f)\n            if is_improved:\n                best_f = fnew\n                best_x = xcand.copy()\n                last_improve_eval = evals\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_F = F[np.isfinite(F)] if np.any(np.isfinite(F)) else np.array([])\n            if finite_F.size > 0:\n                try:\n                    thresh = np.percentile(np.where(np.isfinite(np.concatenate(([fnew], F))), np.concatenate(([fnew], F)), np.inf), 25)\n                except Exception:\n                    thresh = np.inf\n            else:\n                thresh = np.inf\n            is_success = (np.isfinite(fnew) and fnew <= thresh) or is_improved\n            success_history.append(1 if is_success else 0)\n            if len(success_history) > self.adapt_window:\n                success_history.pop(0)\n\n            # short-term adaptation of global scale\n            if len(success_history) >= max(5, min(self.adapt_window, 8)) and (age_counter % max(1, (self.adapt_window // 4)) == 0):\n                succ_rate = sum(success_history) / len(success_history)\n                if succ_rate > 0.35:\n                    self.gscale *= 0.92\n                elif succ_rate < 0.06:\n                    self.gscale *= 1.12\n                # tiny jitter\n                self.gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep best + some random + spaced)\n            if len(archive) > self.archive_max:\n                Farr = np.array([a['f'] for a in archive], dtype=float)\n                idx_sorted = np.argsort(np.where(np.isfinite(Farr), Farr, np.inf))\n                keep = set()\n                kbest = max(1, int(0.4 * self.archive_max))\n                keep.update(idx_sorted[:kbest].tolist())\n                # keep some randoms\n                nrand = int(0.1 * self.archive_max)\n                rand_idx = self.rng.choice(len(archive), size=min(nrand, len(archive)), replace=False)\n                keep.update(rand_idx.tolist())\n                # keep some spaced-in-history\n                n_hist = int(0.15 * self.archive_max)\n                if len(archive) > 0 and n_hist > 0:\n                    spaced = list(range(0, len(archive), max(1, len(archive) // (n_hist + 1))))\n                    spaced = [i for i in spaced if i not in keep][:n_hist]\n                    keep.update(spaced)\n                # fill remaining randomly\n                all_idx = list(range(len(archive)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= self.archive_max:\n                        break\n                    keep.add(ii)\n                new_archive = [archive[i] for i in sorted(keep)]\n                archive = new_archive\n\n            # stagnation detection & micro-restarts\n            no_improve_steps = evals - last_improve_eval\n            if no_improve_steps > self.stagnation_window and evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random center)\n                cloud_size = min(40, max(6, int(0.05 * self.budget)))\n                center = best_x.copy() if (best_x is not None) else self._uniform_array(lb, ub, n=1)\n                for i in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    perturb = (self.rng.randn(self.dim) * (0.2 * span * self.gscale + 0.02 * scale_vec))\n                    xc = center + perturb\n                    xc = self._reflect_bounds(xc, lb, ub)\n                    fxc = safe_eval(xc)\n                    archive.append({'x': xc.copy(), 'f': fxc, 'age': age_counter})\n                    if np.isfinite(fxc) and fxc < best_f:\n                        best_f = fxc\n                        best_x = xc.copy()\n                        last_improve_eval = evals\n                # also add some global randoms\n                for _ in range(min(10, self.budget - evals)):\n                    if evals >= self.budget:\n                        break\n                    xr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        last_improve_eval = evals\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = float(np.clip(self.gscale * 1.3, self.gscale_min, self.gscale_max))\n                success_history = []\n                last_improve_eval = evals\n\n            # optionally verbose\n            if self.verbose and (evals % max(1, int(self.budget // 10)) == 0):\n                print(f\"[MG-ADS] evals={evals}, best_f={best_f:.6g}, gscale={self.gscale:.4g}, archive={len(archive)}\")\n\n        # final best\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda a: a['f'])\n            return float(best['f']), best['x'].copy()\n        else:\n            # all infinite: return a uniform fallback\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb)) if evals < self.budget else np.inf\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb", "configspace": "", "generation": 0, "feedback": "In the code, line 60, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(bnd, 'lb', lb) or getattr(bnd, 'lower', lb)", "error": "In the code, line 60, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(bnd, 'lb', lb) or getattr(bnd, 'lower', lb)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9e60f609-4bfe-4a0a-994a-5ed84a9f0a1f", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style recombination, PCA-guided elite moves, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-Guided Adaptive Directional Sampling).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_frac=0.08,      # fraction of budget used for initial sampling\n                 archive_max=400,     # maximum archive size\n                 elite_frac=0.12,     # fraction considered elite\n                 adapt_window=40,     # short-term success window for adaptation\n                 gscale_min=0.05,\n                 gscale_max=8.0,\n                 stagnation_window=200,# micro-restart trigger window (no improvements)\n                 verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.init_frac = float(init_frac)\n        self.archive_max = int(archive_max)\n        self.elite_frac = float(elite_frac)\n        self.adapt_window = int(adapt_window)\n        self.stagnation_window = int(stagnation_window)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.verbose = bool(verbose)\n\n        # dynamic state\n        self.gscale = 0.8  # global scale multiplier (adapted)\n        # Base probabilities for strategies: DE, PCA, Local, Cauchy\n        self.base_probs = np.array([0.35, 0.20, 0.30, 0.15])\n\n    # ---------- utilities ----------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func attributes, otherwise default [-5,5]^dim.\n        Supports patterns:\n          - func.bounds = (lb, ub) where lb/ub can be arrays or scalars\n          - func.bounds = list/array of shape (dim,2)\n          - func.lb/func.ub or func.lower/func.upper\n        Returns (lb, ub) as numpy arrays shape (dim,).\n        \"\"\"\n        # default\n        lb = None\n        ub = None\n\n        bnd = getattr(func, 'bounds', None)\n        if bnd is not None:\n            # tuple (lb, ub)\n            try:\n                if isinstance(bnd, (list, tuple)) and len(bnd) == 2:\n                    lb, ub = bnd\n            except Exception:\n                pass\n            # maybe object with attributes\n            if lb is None:\n                lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None)\n            if ub is None:\n                ub = getattr(bnd, 'ub', None) or getattr(bnd, 'upper', None)\n\n        # direct attributes on func\n        if lb is None:\n            lb = getattr(func, 'lb', None) or getattr(func, 'lower', None)\n        if ub is None:\n            ub = getattr(func, 'ub', None) or getattr(func, 'upper', None)\n\n        def to_arr(v):\n            if v is None:\n                return None\n            arr = np.array(v, dtype=float)\n            if arr.size == 1:\n                return np.repeat(arr.item(), self.dim)\n            if arr.size == self.dim:\n                return arr.reshape(self.dim)\n            # maybe shape (dim,2)\n            if arr.ndim == 2 and arr.shape == (self.dim, 2):\n                return None  # handled outside\n            return None\n\n        lb_a = to_arr(lb)\n        ub_a = to_arr(ub)\n\n        # special case: bounds provided as list of (low,high) per-dim\n        if (lb_a is None or ub_a is None) and isinstance(bnd, (list, tuple, np.ndarray)):\n            try:\n                arr = np.array(bnd, dtype=float)\n                if arr.shape == (self.dim, 2):\n                    lb_a = arr[:, 0].astype(float)\n                    ub_a = arr[:, 1].astype(float)\n            except Exception:\n                pass\n\n        # final fallback\n        if lb_a is None:\n            lb_a = np.repeat(-5.0, self.dim)\n        if ub_a is None:\n            ub_a = np.repeat(5.0, self.dim)\n\n        lb_a = np.asarray(lb_a, dtype=float).reshape(self.dim)\n        ub_a = np.asarray(ub_a, dtype=float).reshape(self.dim)\n\n        # ensure lb <= ub elementwise (swap if provided reverse)\n        swap = lb_a > ub_a\n        if np.any(swap):\n            tmp = lb_a.copy()\n            lb_a[swap] = ub_a[swap]\n            ub_a[swap] = tmp[swap]\n\n        return lb_a, ub_a\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = np.array(x, dtype=float).copy()\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        \"\"\"\n        Return a single vector of length dim if n==1, otherwise an (n,dim) array.\n        lb,ub can be arrays length dim or scalars.\n        \"\"\"\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            out = self.rng.rand(n, self.dim) * (ub - lb)[None, :] + lb[None, :]\n            return out\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy: returns vector of length dim.\n        - scale_vec: per-dim scale\n        - occasionally apply heavy multiplicative factors\n        - cap extremes to avoid numerical issues\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # occasional per-coordinate heavy multiplier\n        heavy_mask = (self.rng.rand(self.dim) < 0.07)\n        heavy_mult = 1.0 + 4.0 * self.rng.rand(self.dim) * heavy_mask.astype(float)\n        step = z * scale_vec * heavy_mult\n        # small gaussian jitter to break pure symmetry\n        step += 0.01 * (self.rng.randn(self.dim) * scale_vec)\n        # cap extreme values coordinate-wise\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        step = np.clip(step, -cap, cap)\n        return step\n\n    # ---------- main entry ----------\n    def __call__(self, func):\n        # reset RNG for reproducibility (keep same RandomState)\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        evals = 0\n\n        # archive: list of dicts with {'x':..., 'f':..., 'age':...}\n        archive = []\n        age_counter = 0\n\n        # safe evaluation wrapper: never call func if budget exhausted\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            age_counter += 1\n            return f\n\n        # ---- initialization (LHS-like + some pure randoms) ----\n        n_init = int(max(3, min(self.budget - 1, max(5, int(self.init_frac * self.budget)))))\n        # Latin-hypercube like sampling\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        base = strata[:-1]\n        init_points = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            order = self.rng.permutation(n_init)\n            pts = base + self.rng.rand(n_init) * (1.0 / n_init)\n            init_points[:, d] = pts[order]\n        init_points = lb[None, :] + init_points * (ub - lb)[None, :]\n\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = init_points[i]\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # Add a few pure uniform draws (cheap exploration)\n        n_uniform = min(10, max(0, (self.budget - evals) // 50))\n        for _ in range(n_uniform):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # If nothing evaluated (budget 0), return fallback uniform\n        if len(archive) == 0:\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb)) if evals < self.budget else np.inf\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb\n\n        # initial best\n        all_f = np.array([a['f'] for a in archive], dtype=float)\n        finite_mask = np.isfinite(all_f)\n        if np.any(finite_mask):\n            idx_best = np.nanargmin(np.where(finite_mask, all_f, np.inf))\n            best_f = archive[idx_best]['f']\n            best_x = archive[idx_best]['x'].copy()\n            last_improve_eval = evals\n        else:\n            best_f = np.inf\n            best_x = None\n            last_improve_eval = 0\n\n        # history for adaptation\n        success_history = []\n\n        # main loop\n        while evals < self.budget:\n            # convert archive arrays\n            X = np.array([a['x'] for a in archive], dtype=float)\n            F = np.array([a['f'] for a in archive], dtype=float)\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            finite_rows = np.isfinite(F)\n            if np.any(finite_rows) and np.sum(finite_rows) >= 2:\n                finite_X = X[finite_rows]\n                q75 = np.percentile(finite_X, 75, axis=0)\n                q25 = np.percentile(finite_X, 25, axis=0)\n                scale_vec = (q75 - q25) / 1.349  # approximate std from IQR\n                # fallback to std where IQR is tiny\n                stds = np.std(finite_X, axis=0)\n                small = scale_vec <= 1e-12\n                scale_vec[small] = stds[small]\n            else:\n                scale_vec = np.std(X, axis=0)\n            scale_vec = np.maximum(scale_vec, 1e-12)\n            scale_vec = np.maximum(scale_vec, 1e-3 * span)  # ensure not too small relative to bounds\n\n            # determine elites\n            finite_idx = np.where(np.isfinite(F))[0]\n            n_elite = max(1, int(np.ceil(self.elite_frac * max(1, len(archive)))))\n            if finite_idx.size > 0:\n                sorted_idx = finite_idx[np.argsort(F[finite_idx])]\n                elite_idx = sorted_idx[:min(n_elite, sorted_idx.size)].tolist()\n            else:\n                elite_idx = list(range(min(n_elite, len(archive))))\n\n            # pick anchor among elites with exponential bias\n            if len(elite_idx) > 0:\n                weights = np.exp(-np.linspace(0, 2.0, len(elite_idx)))\n                weights = weights / np.sum(weights)\n                # mostly pick elites, occasionally uniform from archive\n                if self.rng.rand() < 0.92:\n                    choice = self.rng.choice(len(elite_idx), p=weights)\n                    anchor = archive[elite_idx[choice]]['x'].copy()\n                else:\n                    anchor = archive[self.rng.randint(len(archive))]['x'].copy()\n            else:\n                anchor = self._uniform_array(lb, ub, n=1)\n\n            # helper to pick distinct donors\n            def pick_distinct(k, exclude_idx=None):\n                ids = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in ids:\n                    ids.remove(exclude_idx)\n                if len(ids) >= k:\n                    return self.rng.choice(ids, size=k, replace=False).tolist()\n                else:\n                    # allow duplicates if not enough\n                    return [self.rng.randint(len(archive)) for _ in range(k)]\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            probs = probs * np.array([1.0, 0.95, 1.0, 1.0 + 0.6 * min(1.0, self.gscale)])\n            probs = probs / probs.sum()\n            strat = int(self.rng.choice(4, p=probs))\n\n            xcand = anchor.copy()\n\n            # prepare donors for DE-like if needed\n            donors = pick_distinct(3)\n            xa = archive[donors[0]]['x']\n            xb = archive[donors[1]]['x']\n            xc = archive[donors[2]]['x']\n\n            # Strategy 0: DE-style differential injection\n            if strat == 0:\n                # differential factor scale depends on gscale and local spread\n                Fdiff = xb - xc\n                diff = Fdiff\n                alpha = 0.55 * (0.7 + 0.6 * self.rng.rand())  # blend factor randomized\n                noise = (self.rng.randn(self.dim) * (0.06 + 0.5 * self.rng.rand(self.dim))) * scale_vec * self.gscale\n                xcand = alpha * anchor + (1.0 - alpha) * (anchor + 0.9 * diff) + noise\n                # occasionally inject a bit of Cauchy for escape\n                if self.rng.rand() < 0.12:\n                    xcand += 0.6 * self._tempered_cauchy(scale_vec * self.gscale)\n\n            # Strategy 1: PCA-guided elite perturbation\n            elif strat == 1:\n                # build matrix of elites (or top portion)\n                top_k = max(2, min(len(elite_idx), min(20, int(0.6 * len(archive))))) if len(elite_idx) > 0 else min(5, len(archive))\n                if len(elite_idx) >= 2:\n                    mat_idx = elite_idx[:top_k]\n                else:\n                    mat_idx = list(range(min(len(archive), top_k)))\n                mat = np.array([archive[i]['x'] for i in mat_idx], dtype=float)\n                # robust mean\n                c = np.mean(mat, axis=0)\n                M = mat - c\n                if M.shape[0] >= 2:\n                    cov = np.cov(M, rowvar=False)\n                    cov += np.eye(self.dim) * (1e-8 * (np.mean(np.diag(cov)) + 1e-8))\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(cov)\n                        order = np.argsort(eigvals)[::-1]\n                        eigvecs = eigvecs[:, order]\n                        # sample along a few top PCs\n                        n_pcs = max(1, min(self.dim, 1 + int(self.rng.rand() * min(self.dim, 4))))\n                        coeffs = (0.6 * self.gscale) * (self.rng.randn(n_pcs) * np.sqrt(np.maximum(eigvals[order][:n_pcs], 1e-12)))\n                        step = eigvecs[:, :n_pcs].dot(coeffs)\n                        xcand = c + step\n                        # small blend towards anchor\n                        if self.rng.rand() < 0.7:\n                            xcand = 0.6 * anchor + 0.4 * xcand\n                    except Exception:\n                        xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n                else:\n                    # fallback gaussian around anchor\n                    xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n\n            # Strategy 2: Local anisotropic gaussian\n            elif strat == 2:\n                anis = 0.2 + 0.8 * self.rng.rand(self.dim)\n                sigma = scale_vec * (0.5 * anis + 0.5) * self.gscale\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # ensure at least one coordinate changes\n                idx_change = int(self.rng.randint(self.dim))\n                if abs(xcand[idx_change] - anchor[idx_change]) < 1e-16:\n                    xcand[idx_change] += sigma[idx_change] * (0.5 + self.rng.rand())\n                # occasional mixed uniform per-dim\n                if self.rng.rand() < 0.06:\n                    mix = self.rng.rand(self.dim) < 0.08\n                    if np.any(mix):\n                        xcand[mix] = self._uniform_array(lb[mix], ub[mix], n=1)\n\n            # Strategy 3: tempered Cauchy global escape\n            else:\n                cauch = self._tempered_cauchy(scale_vec * (1.0 + 0.5 * self.gscale))\n                xcand = anchor + cauch\n                # sometimes add directed differential\n                if self.rng.rand() < 0.3:\n                    xcand += 0.2 * self.gscale * (archive[donors[0]]['x'] - archive[donors[1]]['x'])\n\n            # occasional directed small jump to current best for refinement\n            if best_x is not None and self.rng.rand() < 0.08:\n                dir_to_best = best_x - xcand\n                xcand += 0.2 * self.gscale * dir_to_best * (0.5 + self.rng.rand())\n\n            # sanitize candidate\n            if not np.all(np.isfinite(xcand)):\n                bad = ~np.isfinite(xcand)\n                xcand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub, niter=4)\n\n            # evaluate candidate\n            fnew = safe_eval(xcand)\n\n            # update archive\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # update best if improved\n            is_improved = False\n            if np.isfinite(fnew) and fnew < best_f:\n                best_f = fnew\n                best_x = xcand.copy()\n                last_improve_eval = evals\n                is_improved = True\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_F = F[np.isfinite(F)] if np.any(np.isfinite(F)) else np.array([])\n            is_success = False\n            if np.isfinite(fnew):\n                if finite_F.size > 0:\n                    combined = np.concatenate(([fnew], finite_F))\n                    thresh = np.percentile(combined, 25)\n                    if fnew <= thresh:\n                        is_success = True\n                else:\n                    is_success = True  # first finite\n                if is_improved:\n                    is_success = True\n\n            success_history.append(1 if is_success else 0)\n            # keep success_history bounded\n            if len(success_history) > self.adapt_window:\n                success_history.pop(0)\n\n            # short-term adaptation of global scale\n            if len(success_history) >= max(6, min(self.adapt_window, 12)) and (age_counter % max(1, (self.adapt_window // 4)) == 0):\n                succ_rate = sum(success_history) / len(success_history)\n                # if success rate too low -> increase exploration\n                if succ_rate < 0.06:\n                    self.gscale = float(min(self.gscale * 1.12, self.gscale_max))\n                # if success rate high -> exploit more (shrink)\n                elif succ_rate > 0.28:\n                    self.gscale = float(max(self.gscale * 0.90, self.gscale_min))\n                # tiny jitter\n                self.gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep best + some random + spaced)\n            if len(archive) > self.archive_max:\n                Farr = np.array([a['f'] for a in archive], dtype=float)\n                finite_mask_all = np.isfinite(Farr)\n                idx_sorted = np.argsort(np.where(finite_mask_all, Farr, np.inf))\n                keep = set()\n                kbest = max(1, int(0.4 * self.archive_max))\n                keep.update(idx_sorted[:kbest].tolist())\n                # keep some randoms\n                n_rand = max(1, int(0.15 * self.archive_max))\n                rand_idx = self.rng.choice(len(archive), size=min(n_rand, len(archive)), replace=False)\n                keep.update(rand_idx.tolist())\n                # keep some spaced-in-history\n                n_hist = int(0.15 * self.archive_max)\n                if len(archive) > 0 and n_hist > 0:\n                    spaced = list(range(0, len(archive), max(1, len(archive) // (n_hist + 1))))\n                    spaced = [i for i in spaced if i not in keep][:n_hist]\n                    keep.update(spaced)\n                # fill remaining randomly\n                all_idx = list(range(len(archive)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= self.archive_max:\n                        break\n                    keep.add(ii)\n                # rebuild archive\n                archive = [archive[i] for i in sorted(keep)]\n                # reset ages to compact order\n                for idx, a in enumerate(archive):\n                    a['age'] = idx\n\n            # stagnation detection & micro-restarts\n            no_improve_steps = evals - last_improve_eval\n            if no_improve_steps > self.stagnation_window and evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random center)\n                cloud_size = min(40, max(6, int(0.05 * self.budget)))\n                center = best_x.copy() if (best_x is not None) else self._uniform_array(lb, ub, n=1)\n                for _ in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    perturb = (self.rng.randn(self.dim) * (0.2 * span * self.gscale + 0.02 * scale_vec))\n                    xc = self._reflect_bounds(center + perturb, lb, ub, niter=2)\n                    fxc = safe_eval(xc)\n                    archive.append({'x': xc.copy(), 'f': fxc, 'age': age_counter})\n                    if np.isfinite(fxc) and fxc < best_f:\n                        best_x = xc.copy()\n                        best_f = fxc\n                        last_improve_eval = evals\n                # also add some global randoms\n                for _ in range(min(10, max(0, self.budget - evals))):\n                    xr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        last_improve_eval = evals\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = float(np.clip(self.gscale * 1.3, self.gscale_min, self.gscale_max))\n                success_history = []\n                last_improve_eval = evals\n\n            # optionally verbose once in a while\n            if self.verbose and (evals % max(1, int(max(1, self.budget // 10))) == 0):\n                print(f\"[MG-ADS] evals={evals}, best_f={best_f:.6g}, gscale={self.gscale:.4g}, archive={len(archive)}\")\n\n        # final best\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda a: a['f'])\n            return float(best['f']), best['x'].copy()\n        else:\n            # all infinite: return a uniform fallback\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb)) if evals < self.budget else np.inf\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb", "configspace": "", "generation": 0, "feedback": "In the code, line 66, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None)", "error": "In the code, line 66, in _get_bounds, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: lb = getattr(bnd, 'lb', None) or getattr(bnd, 'lower', None)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d9c7278d-5163-4cc9-bfa2-0b5a59dacdf2", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style recombination, PCA-guided elite moves, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-Guided Adaptive Directional Sampling).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler mixing DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_max=400,     # maximum archive size\n                 elite_frac=0.12,     # fraction considered elite\n                 adapt_window=40,     # short-term success window for adaptation\n                 gscale_min=0.2,\n                 gscale_max=8.0,\n                 init_frac=0.06,      # fraction of budget used for initialization\n                 stagnation_window=300,\n                 micro_cloud_size=20):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.archive_max = int(archive_max)\n        self.elite_frac = float(elite_frac)\n        self.adapt_window = int(adapt_window)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.init_frac = float(init_frac)\n        self.stagnation_window = int(stagnation_window)\n        self.micro_cloud_size = int(micro_cloud_size)\n\n        # dynamic state\n        self.gscale = 0.8  # global scale multiplier (adapted)\n        # base probabilities for strategies: DE, PCA, Local, Cauchy\n        self.base_probs = np.array([0.30, 0.20, 0.35, 0.15], dtype=float)\n        self.base_probs /= np.sum(self.base_probs)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"Try to read bounds from func attributes, otherwise default [-5,5]^dim.\"\"\"\n        lb = None\n        ub = None\n        # common patterns\n        bnd = getattr(func, 'bounds', None)\n        if bnd is not None:\n            # bounds as tuple (lb, ub)\n            try:\n                if isinstance(bnd, (list, tuple)) and len(bnd) == 2:\n                    lb, ub = bnd\n                elif isinstance(bnd, np.ndarray) and bnd.shape == (self.dim, 2):\n                    arr = np.asarray(bnd, dtype=float)\n                    return arr[:, 0].copy(), arr[:, 1].copy()\n            except Exception:\n                lb = ub = None\n\n        # try separate attrs\n        if lb is None:\n            lb = getattr(func, 'lb', None) or getattr(func, 'lower', None)\n        if ub is None:\n            ub = getattr(func, 'ub', None) or getattr(func, 'upper', None)\n\n        def to_arr(v):\n            if v is None:\n                return None\n            arr = np.array(v, dtype=float)\n            if arr.size == 1:\n                return np.repeat(arr.item(), self.dim)\n            if arr.ndim == 1 and arr.size == self.dim:\n                return arr\n            # maybe shape (dim,2)\n            if arr.ndim == 2 and arr.shape == (self.dim, 2):\n                return None  # handled earlier\n            # fallback: try reshape\n            try:\n                return arr.reshape(self.dim)\n            except Exception:\n                return None\n\n        lb_a = to_arr(lb)\n        ub_a = to_arr(ub)\n\n        if lb_a is None:\n            lb_a = np.repeat(-5.0, self.dim)\n        if ub_a is None:\n            ub_a = np.repeat(5.0, self.dim)\n\n        lb_a = np.asarray(lb_a, dtype=float).reshape(self.dim)\n        ub_a = np.asarray(ub_a, dtype=float).reshape(self.dim)\n\n        # ensure lb <= ub elementwise (swap if provided reversed)\n        swap = lb_a > ub_a\n        if np.any(swap):\n            tmp = lb_a.copy()\n            lb_a[swap] = ub_a[swap]\n            ub_a[swap] = tmp[swap]\n\n        return lb_a, ub_a\n\n    def _reflect_bounds(self, x, lb, ub, niter=4):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = np.array(x, dtype=float).copy()\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        \"\"\"\n        Return a single vector of length dim if n==1, otherwise an (n,dim) array.\n        lb,ub can be arrays length dim or scalars.\n        \"\"\"\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.repeat(lb.item(), self.dim)\n        if ub.size == 1:\n            ub = np.repeat(ub.item(), self.dim)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            out = self.rng.rand(n, self.dim) * (ub[None, :] - lb[None, :]) + lb[None, :]\n            return out\n\n    def _tempered_cauchy(self, scale_vec, heavy_prob=0.06, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy: returns vector of length dim.\n        - scale_vec: per-dim scale\n        - occasionally apply heavy multiplicative factors\n        - cap extremes to avoid numerical issues\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # heavy mask\n        heavy_mask = self.rng.rand(self.dim) < heavy_prob\n        heavy_mult = 1.0 + 6.0 * self.rng.rand(self.dim) * heavy_mask.astype(float)\n        step = z * scale_vec * heavy_mult\n        # small gaussian jitter to break pure symmetry\n        step += 0.01 * self.rng.randn(self.dim) * np.maximum(scale_vec, 1e-12)\n        # cap extreme values coordinate-wise\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        step = np.clip(step, -cap, cap)\n        return step\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        # prepare bounds\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        # counters and storage\n        evals = 0\n        age_counter = 0\n\n        # archive: list of dicts with {'x':..., 'f':..., 'age':...}\n        archive = []\n\n        # safe evaluation wrapper: never call func if budget exhausted\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            age_counter += 1\n            return f\n\n        # ---- initialization (LHS-like + some pure randoms) ----\n        n_init = int(max(3, min(self.budget - 1, max(5, int(self.init_frac * self.budget)))))\n        # Latin-hypercube like sampling (simple)\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        base = strata[:-1]\n        init_points = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            order = self.rng.permutation(n_init)\n            pts = base + self.rng.rand(n_init) * (1.0 / n_init)\n            init_points[:, d] = pts[order]\n        init_points = lb[None, :] + init_points * (ub - lb)[None, :]\n\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = init_points[i]\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # Add a few pure uniform draws (cheap exploration)\n        n_uniform = min(10, max(0, (self.budget - evals) // 50))\n        for _ in range(n_uniform):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # If nothing evaluated (budget 0), return fallback uniform\n        if len(archive) == 0:\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb)) if evals < self.budget else np.inf\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb\n\n        # initial best\n        all_f = np.array([a['f'] for a in archive], dtype=float)\n        finite_mask = np.isfinite(all_f)\n        if np.any(finite_mask):\n            idx_best = np.nanargmin(np.where(finite_mask, all_f, np.inf))\n            best_x = archive[idx_best]['x'].copy()\n            best_f = float(all_f[idx_best])\n            last_improve_eval = evals\n        else:\n            best_f = np.inf\n            best_x = None\n            last_improve_eval = 0\n\n        # history for adaptation\n        success_history = []\n\n        # main loop\n        while evals < self.budget:\n            # convert archive arrays\n            X = np.array([a['x'] for a in archive], dtype=float)\n            F = np.array([a['f'] for a in archive], dtype=float)\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            finite_rows = np.isfinite(F)\n            if np.any(finite_rows) and np.sum(finite_rows) >= 2:\n                finite_X = X[finite_rows]\n                q25 = np.percentile(finite_X, 25, axis=0)\n                q75 = np.percentile(finite_X, 75, axis=0)\n                scale_vec = (q75 - q25) / 1.349  # approximate std from IQR\n                # fallback to std where IQR is tiny\n                stds = np.std(finite_X, axis=0)\n                small = scale_vec <= 1e-12\n                scale_vec[small] = stds[small]\n            else:\n                # weak fallback\n                scale_vec = np.maximum(1e-6, 0.05 * span)\n\n            scale_vec = np.maximum(scale_vec, 1e-12)\n            scale_vec = np.maximum(scale_vec, 1e-3 * span)  # ensure not too small relative to bounds\n\n            # determine elites\n            finite_idx = np.where(np.isfinite(F))[0]\n            n_elite = max(1, int(np.ceil(self.elite_frac * max(1, len(archive)))))\n            if finite_idx.size > 0:\n                sorted_idx = finite_idx[np.argsort(F[finite_idx])]\n                elite_idx = sorted_idx[:min(n_elite, sorted_idx.size)].tolist()\n            else:\n                elite_idx = list(range(min(n_elite, len(archive))))\n\n            # pick anchor among elites with exponential bias\n            if len(elite_idx) > 0:\n                weights = np.exp(-np.linspace(0, 2.0, len(elite_idx)))\n                weights = weights / np.sum(weights)\n                if self.rng.rand() < 0.92:\n                    # pick with bias among elites\n                    choice = self.rng.choice(len(elite_idx), p=weights)\n                    anchor = archive[elite_idx[choice]]['x'].copy()\n                else:\n                    # uniform from archive\n                    anchor = archive[self.rng.randint(len(archive))]['x'].copy()\n            else:\n                anchor = archive[self.rng.randint(len(archive))]['x'].copy()\n\n            # helper to pick distinct donors\n            def pick_distinct(k, exclude_idx=None):\n                ids = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in ids:\n                    ids.remove(exclude_idx)\n                if len(ids) >= k:\n                    return self.rng.choice(ids, size=k, replace=False).tolist()\n                else:\n                    # allow duplicates if not enough\n                    return [self.rng.randint(len(archive)) for _ in range(k)]\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            # modest tilt toward Cauchy if gscale large\n            probs[3] *= (1.0 + 0.2 * (self.gscale - 1.0))\n            probs = np.maximum(probs, 1e-6)\n            probs = probs / np.sum(probs)\n\n            strat = int(self.rng.choice(4, p=probs))\n\n            # prepare donors for DE-like if needed\n            donors = pick_distinct(3)\n            xb = archive[donors[1]]['x']\n            xc = archive[donors[2]]['x']\n            xa = archive[donors[0]]['x']\n\n            # propose candidate\n            xcand = None\n\n            # Strategy 0: DE-style differential injection\n            if strat == 0:\n                # differential factor scale depends on gscale and local spread\n                Fdiff = xb - xc\n                diff = Fdiff * (0.6 + 0.8 * self.rng.rand()) * self.gscale\n                alpha = 0.2 + 0.6 * self.rng.rand()\n                noise = 0.02 * (np.rint(self.rng.randn(self.dim)) * 0.0 + self.rng.randn(self.dim)) * np.maximum(scale_vec, 1e-12)\n                xcand = anchor + alpha * diff + noise\n                # occasionally inject a bit of Cauchy for escape\n                if self.rng.rand() < 0.08:\n                    xcand += 0.5 * self._tempered_cauchy(scale_vec * self.gscale)\n            # Strategy 1: PCA-guided elite perturbation\n            elif strat == 1:\n                top_k = max(2, min(len(elite_idx), min(20, int(0.6 * len(archive))))) if len(elite_idx) > 0 else min(5, len(archive))\n                if len(elite_idx) >= 2:\n                    mat_idx = elite_idx[:top_k]\n                    mat = X[mat_idx]\n                    c = np.mean(mat, axis=0)\n                    M = mat - c\n                    if M.shape[0] >= 2:\n                        cov = np.cov(M, rowvar=False)\n                        try:\n                            eigvals, eigvecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                            order = np.argsort(eigvals)[::-1]\n                            eigvals = eigvals[order]\n                            eigvecs = eigvecs[:, order]\n                            # sample along a few top PCs\n                            n_pcs = max(1, min(self.dim, 1 + int(self.rng.rand() * min(self.dim, 4))))\n                            coeffs = (0.6 * self.gscale) * (self.rng.randn(n_pcs) * np.sqrt(np.maximum(eigvals[:n_pcs], 1e-12)))\n                            step = eigvecs[:, :n_pcs].dot(coeffs)\n                            xcand = anchor + step\n                        except Exception:\n                            xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n                    else:\n                        # fallback gaussian around anchor\n                        xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n                else:\n                    xcand = anchor + self.gscale * (self.rng.randn(self.dim) * scale_vec)\n            # Strategy 2: Local anisotropic gaussian\n            elif strat == 2:\n                # anisotropy: sample a random anisotropy between 0 and 2\n                anis = 0.5 + 1.5 * self.rng.rand()\n                sigma = scale_vec * (0.5 * anis + 0.5) * self.gscale\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # ensure at least one coordinate changes\n                idx_change = int(self.rng.randint(self.dim))\n                if abs(xcand[idx_change] - anchor[idx_change]) < 1e-16:\n                    xcand[idx_change] += sigma[idx_change] * (0.5 + self.rng.rand())\n                # occasional mixed uniform per-dim\n                if self.rng.rand() < 0.06:\n                    mix_mask = self.rng.rand(self.dim) < 0.15\n                    if np.any(mix_mask):\n                        xcand[mix_mask] = self._uniform_array(lb[mix_mask], ub[mix_mask], n=1)[mix_mask]\n            # Strategy 3: tempered Cauchy global escape\n            else:\n                cauch = self._tempered_cauchy(scale_vec * (1.0 + 0.6 * self.gscale))\n                xcand = anchor + cauch\n                # sometimes add directed differential\n                if self.rng.rand() < 0.3:\n                    xcand += 0.25 * self.gscale * (archive[donors[0]]['x'] - archive[donors[1]]['x'])\n\n            # occasional directed small jump to current best for refinement\n            if (best_x is not None) and (self.rng.rand() < 0.12):\n                dir_to_best = best_x - xcand\n                xcand = xcand + 0.18 * self.gscale * dir_to_best * (0.5 + self.rng.rand())\n\n            # sanitize candidate\n            # if NaN or inf in coords, replace with uniform\n            if not np.all(np.isfinite(xcand)):\n                xcand = self._uniform_array(lb, ub, n=1)\n\n            # reflect to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub)\n\n            # evaluate candidate (safe)\n            fnew = safe_eval(xcand)\n\n            # update archive\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # update best if improved\n            is_improved = False\n            if np.isfinite(fnew) and fnew < best_f:\n                best_f = float(fnew)\n                best_x = xcand.copy()\n                last_improve_eval = evals\n                is_improved = True\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_F = F[np.isfinite(F)] if np.any(np.isfinite(F)) else np.array([])\n            is_success = False\n            if np.isfinite(fnew):\n                if finite_F.size > 0:\n                    thresh = np.percentile(finite_F, 25)\n                    if fnew <= thresh:\n                        is_success = True\n                else:\n                    is_success = True  # first finite\n                if is_improved:\n                    is_success = True\n\n            success_history.append(1 if is_success else 0)\n            # keep success_history bounded\n            if len(success_history) > self.adapt_window:\n                success_history.pop(0)\n\n            # short-term adaptation of global scale\n            if len(success_history) >= max(6, min(self.adapt_window, 12)) and (age_counter % max(1, (self.adapt_window // 4)) == 0):\n                succ_rate = sum(success_history) / len(success_history)\n                # if success rate too low -> increase exploration\n                if succ_rate < 0.06:\n                    self.gscale *= 1.18 + 0.06 * (self.rng.rand() - 0.5)\n                # if success rate high -> exploit more (shrink)\n                elif succ_rate > 0.3:\n                    self.gscale *= 0.88 + 0.06 * (self.rng.rand() - 0.5)\n                # tiny jitter\n                self.gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep best + some random + spaced)\n            if len(archive) > self.archive_max:\n                Farr = np.array([a['f'] for a in archive], dtype=float)\n                finite_mask_all = np.isfinite(Farr)\n                idx_sorted = np.argsort(np.where(finite_mask_all, Farr, np.inf))\n                keep = set()\n                kbest = max(1, int(0.4 * self.archive_max))\n                keep.update(idx_sorted[:kbest].tolist())\n                # keep some randoms\n                n_rand = max(1, int(0.15 * self.archive_max))\n                rand_idx = self.rng.choice(len(archive), size=n_rand, replace=False)\n                keep.update(rand_idx.tolist())\n                # keep some spaced-in-history\n                n_hist = max(1, int(0.15 * self.archive_max))\n                if len(archive) > 0 and n_hist > 0:\n                    spaced = list(range(0, len(archive), max(1, len(archive) // (n_hist + 1))))\n                    spaced = [i for i in spaced if i not in keep][:n_hist]\n                    keep.update(spaced)\n                # fill remaining randomly\n                all_idx = list(range(len(archive)))\n                remaining = [i for i in all_idx if i not in keep]\n                self.rng.shuffle(remaining)\n                target_keep = min(self.archive_max, len(all_idx))\n                while len(keep) < target_keep and remaining:\n                    keep.add(remaining.pop())\n                # rebuild archive\n                new_archive = []\n                keep_list = sorted(keep)\n                for new_i, old_i in enumerate(keep_list):\n                    a = archive[old_i]\n                    a2 = {'x': a['x'].copy(), 'f': a['f'], 'age': new_i}  # reset ages compactly\n                    new_archive.append(a2)\n                archive = new_archive\n\n            # stagnation detection & micro-restarts\n            no_improve_steps = evals - last_improve_eval\n            if no_improve_steps > self.stagnation_window and evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random center)\n                center = best_x.copy() if (best_x is not None) else self._uniform_array(lb, ub, n=1)\n                cloud_size = min(self.micro_cloud_size, max(4, (self.budget - evals) // 10))\n                for _ in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    perturb = (self.rng.randn(self.dim) * (0.2 * span * (0.6 + 0.8 * self.gscale) + 0.02 * scale_vec))\n                    xc = self._reflect_bounds(center + perturb, lb, ub)\n                    fxc = safe_eval(xc)\n                    archive.append({'x': xc.copy(), 'f': fxc, 'age': age_counter})\n                    if np.isfinite(fxc) and fxc < best_f:\n                        best_f = fxc\n                        best_x = xc.copy()\n                        last_improve_eval = evals\n                # also add some global randoms\n                nglob = min(6, max(1, (self.budget - evals) // 20))\n                for _ in range(nglob):\n                    if evals >= self.budget:\n                        break\n                    xr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        last_improve_eval = evals\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = float(np.clip(self.gscale * 1.25, self.gscale_min, self.gscale_max))\n                success_history = []\n                last_improve_eval = evals\n\n            # optional small termination early if perfect (common BBOB target)\n            if best_f == 0.0:\n                break\n\n        # final best\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda a: a['f'])\n            return float(best['f']), best['x'].copy()\n        else:\n            # all infinite: return a uniform fallback\n            xfb = self._uniform_array(lb, ub, n=1)\n            try:\n                ffb = float(func(xfb)) if evals < self.budget else np.inf\n            except Exception:\n                ffb = np.inf\n            return ffb, xfb", "configspace": "", "generation": 0, "feedback": "In the code, line 364, in __call__, the following error occurred:\nIndexError: boolean index did not match indexed array along axis 0; size of axis is 2 but size of corresponding boolean axis is 10\nOn line: xcand[mix_mask] = self._uniform_array(lb[mix_mask], ub[mix_mask], n=1)[mix_mask]", "error": "In the code, line 364, in __call__, the following error occurred:\nIndexError: boolean index did not match indexed array along axis 0; size of axis is 2 but size of corresponding boolean axis is 10\nOn line: xcand[mix_mask] = self._uniform_array(lb[mix_mask], ub[mix_mask], n=1)[mix_mask]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7680b3ab-4891-4be0-a552-b09fce4ab992", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style recombination, PCA-guided elite moves, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 elite_frac=0.12,     # fraction considered elite\n                 adapt_window=40,     # short-term success window for adaptation\n                 gscale_min=1e-3,\n                 gscale_max=8.0,\n                 stagnation_window=200,# micro-restart trigger window (no improvements)\n                 archive_max=None,\n                 verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.elite_frac = float(elite_frac)\n        self.adapt_window = int(adapt_window)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.gscale = 1.0  # global multiplicative exploration scale (adapted)\n        self.stagnation_window = int(stagnation_window)\n        self.verbose = bool(verbose)\n        self.archive_max = int(archive_max) if archive_max is not None else max(40, 6 * self.dim + 40)\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.36, 0.24, 0.30, 0.10], dtype=float)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to read common convention attributes; otherwise default [-5,5]^dim\n        lb = None\n        ub = None\n        # common patterns\n        bnd = getattr(func, \"bounds\", None)\n        if bnd is not None:\n            # (lb, ub) tuple\n            if isinstance(bnd, (list, tuple)) and len(bnd) == 2:\n                lb, ub = bnd\n            else:\n                try:\n                    arr = np.asarray(bnd, dtype=float)\n                    if arr.shape == (self.dim, 2):\n                        lb = arr[:, 0]\n                        ub = arr[:, 1]\n                except Exception:\n                    pass\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        def to_arr(v):\n            if v is None:\n                return None\n            a = np.array(v, dtype=float)\n            if a.size == 1:\n                return np.repeat(a.item(), self.dim)\n            if a.shape == (self.dim,):\n                return a\n            # if shape (dim,2) handled earlier\n            return None\n\n        lb_a = to_arr(lb)\n        ub_a = to_arr(ub)\n\n        if lb_a is None or ub_a is None:\n            # fallback to [-5,5]^dim\n            lb_a = np.repeat(-5.0, self.dim)\n            ub_a = np.repeat(5.0, self.dim)\n\n        lb_a = np.asarray(lb_a, dtype=float).reshape(self.dim)\n        ub_a = np.asarray(ub_a, dtype=float).reshape(self.dim)\n\n        # ensure lb <= ub elementwise\n        swap = lb_a > ub_a\n        if np.any(swap):\n            tmp = lb_a.copy()\n            lb_a[swap] = ub_a[swap]\n            ub_a[swap] = tmp[swap]\n\n        return lb_a, ub_a\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = np.array(x, dtype=float).copy()\n        for _ in range(niter):\n            below = x < lb\n            above = x > ub\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        \"\"\"\n        Return a single vector of length dim if n==1, otherwise an (n,dim) array.\n        lb,ub can be arrays length dim or scalars.\n        \"\"\"\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.repeat(lb.item(), self.dim)\n        if ub.size == 1:\n            ub = np.repeat(ub.item(), self.dim)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            r = self.rng.rand(n, self.dim) * (ub - lb) + lb\n            return r\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy: returns vector of length dim.\n        - scale_vec: per-dim scale\n        - occasionally apply heavy multiplicative factors\n        - cap extremes to avoid numerical issues\n        \"\"\"\n        scale_vec = np.maximum(scale_vec, 1e-12)\n        # basic standard Cauchy via inv CDF: tan(pi*(u-0.5))\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplier per-coordinate\n        heavy = (self.rng.rand(self.dim) < 0.02).astype(float)\n        heavy_mult = 1.0 + heavy * (5.0 + 20.0 * self.rng.rand(self.dim))\n        step = z * scale_vec * heavy_mult * (0.6 + 0.8 * self.rng.rand(self.dim))\n        # small gaussian jitter\n        step += 0.02 * self.rng.randn(self.dim) * scale_vec\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        step = np.clip(step, -cap, cap)\n        return step\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        # read bounds\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        if np.any(span <= 0):\n            span = np.maximum(span, 1e-6)\n\n        evals = 0\n        age_counter = 0\n\n        # archive: list of dicts {'x':..., 'f':..., 'age':...}\n        archive = []\n\n        # safe evaluation enforcing budget and catching exceptions\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            age_counter += 1\n            return f\n\n        # ---- initialization: LHS-like + a few pure randoms ----\n        n_init = min(max(20, 4 * self.dim), max(2, int(0.12 * self.budget)))\n        n_init = min(n_init, self.budget)\n        # Latin-hypercube like: per-dim strata\n        # create n_init points using simple LHS permutations\n        pts = np.zeros((n_init, self.dim), dtype=float)\n        for d in range(self.dim):\n            perm = np.arange(n_init)\n            self.rng.shuffle(perm)\n            # stratified within each slice\n            a = (perm + self.rng.rand(n_init)) / float(n_init)\n            pts[:, d] = lb[d] + a * (ub[d] - lb[d])\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = pts[i]\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # a few uniform extra draws\n        n_uniform = min(8, max(0, (self.budget - evals) // 50))\n        for _ in range(n_uniform):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # If nothing evaluated (e.g. budget==0) return fallback\n        if len(archive) == 0:\n            xfb = self._uniform_array(lb, ub, n=1)\n            ffb = safe_eval(xfb)  # will be inf if budget exhausted\n            return float(ffb), xfb.copy()\n\n        # initial best\n        finite_fs = [a['f'] for a in archive if np.isfinite(a['f'])]\n        if len(finite_fs) > 0:\n            best_idx = np.argmin([a['f'] for a in archive])\n            best_f = float(archive[best_idx]['f'])\n            best_x = archive[best_idx]['x'].copy()\n            last_improve_eval = evals\n        else:\n            best_f = np.inf\n            best_x = None\n            last_improve_eval = 0\n\n        # history for adaptation\n        success_history = []\n\n        # main loop\n        while evals < self.budget:\n            # convert archive arrays\n            X = np.array([a['x'] for a in archive], dtype=float)\n            F = np.array([a['f'] for a in archive], dtype=float)\n            ages = np.array([a['age'] for a in archive], dtype=int)\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            finite_mask = np.isfinite(F)\n            if np.any(finite_mask) and np.sum(finite_mask) >= 2:\n                finite_X = X[finite_mask]\n                q25 = np.percentile(finite_X, 25, axis=0)\n                q75 = np.percentile(finite_X, 75, axis=0)\n                scale_vec = (q75 - q25) / 1.349  # approx std\n                # fallback to std where IQR tiny\n                stds = np.std(finite_X, axis=0)\n                small = scale_vec <= 1e-12\n                scale_vec[small] = stds[small]\n            else:\n                # no good statistics: use proportion of bound span\n                scale_vec = 0.2 * span\n            scale_vec = np.maximum(scale_vec, 1e-12)\n            scale_vec = np.maximum(scale_vec, 1e-3 * span)  # ensure not too small relative to bounds\n\n            # determine elites\n            finite_idx = np.where(np.isfinite(F))[0]\n            n_elite = max(1, int(np.ceil(self.elite_frac * max(1, len(archive)))))\n            if finite_idx.size > 0:\n                sorted_idx = finite_idx[np.argsort(F[finite_idx])]\n                elite_idx = sorted_idx[:min(n_elite, sorted_idx.size)].tolist()\n            else:\n                elite_idx = list(range(min(n_elite, len(archive))))\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            if len(elite_idx) > 0 and self.rng.rand() < 0.92:\n                weights = np.exp(-np.linspace(0, 2.0, len(elite_idx)))\n                weights = weights / np.sum(weights)\n                pick = self.rng.choice(len(elite_idx), p=weights)\n                anchor = archive[elite_idx[pick]]['x'].copy()\n            else:\n                anchor = archive[self.rng.randint(len(archive))]['x'].copy()\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                ids = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in ids:\n                    ids.remove(exclude_idx)\n                if len(ids) >= k:\n                    return self.rng.choice(ids, size=k, replace=False).tolist()\n                else:\n                    # allow duplicates if not enough\n                    return [self.rng.randint(len(archive)) for _ in range(k)]\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            probs[3] *= (1.0 + 0.35 * min(2.0, self.gscale))  # boost Cauchy when gscale large\n            probs = np.maximum(probs, 1e-8)\n            probs = probs / np.sum(probs)\n            strat = int(self.rng.choice(4, p=probs))\n\n            xcand = anchor.copy()\n\n            # prepare donors for DE-like if needed\n            donors = pick_distinct(3)\n            xa = archive[donors[0]]['x']\n            xb = archive[donors[1]]['x']\n            xc = archive[donors[2]]['x']\n\n            # Strategy 0: DE-style differential injection\n            if strat == 0:\n                # differential factor scale depends on gscale and local spread\n                Fdiff = 0.6 * (0.8 + 0.6 * self.rng.rand())\n                diff = xa - xb\n                # normalize by typical coordinate scales to avoid insane steps\n                norm = np.maximum(np.std(X, axis=0), 1e-12)\n                diff_scaled = diff * (Fdiff * (scale_vec / norm))\n                # base injection towards anchor + diff\n                xcand = anchor + diff_scaled * self.gscale\n                # add anisotropic noise\n                noise = (self.rng.randn(self.dim) * (0.06 + 0.5 * self.rng.rand(self.dim))) * scale_vec * self.gscale\n                xcand += noise\n                # occasionally inject a bit of tempered cauchy for escape\n                if self.rng.rand() < 0.06:\n                    xcand += 0.6 * self._tempered_cauchy(scale_vec * self.gscale)\n\n            # Strategy 1: PCA-guided elite perturbation\n            elif strat == 1:\n                if len(elite_idx) >= 2:\n                    top_k = min(len(elite_idx), max(2, int(0.25 * len(elite_idx))))\n                    mat = np.array([archive[i]['x'] for i in elite_idx[:top_k]])\n                    c = np.mean(mat, axis=0)\n                    M = mat - c\n                    try:\n                        cov = np.cov(M, rowvar=False)\n                        eigvals, eigvecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                        order = np.argsort(eigvals)[::-1]\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # sample along a few top PCs with coefficients proportional to sqrt(eigvals)\n                        n_pcs = min( max(1, int(np.ceil(self.dim * 0.25))), self.dim)\n                        coeffs = (np.sqrt(np.maximum(eigvals[:n_pcs], 1e-12)) *\n                                  (self.rng.randn(n_pcs) * (0.6 + 0.8 * self.rng.rand(n_pcs))))\n                        step = eigvecs[:, :n_pcs].dot(coeffs) * (0.6 * self.gscale)\n                        xcand = c + step\n                        # blend slightly towards anchor to maintain diversity\n                        blend = 0.12 + 0.18 * self.rng.rand()\n                        xcand = blend * anchor + (1.0 - blend) * xcand\n                    except Exception:\n                        # fallback gaussian around anchor\n                        sigma = 0.5 * scale_vec * self.gscale\n                        xcand = anchor + self.rng.randn(self.dim) * sigma\n                else:\n                    # not enough elites: gaussian around anchor\n                    sigma = 0.6 * scale_vec * self.gscale\n                    xcand = anchor + self.rng.randn(self.dim) * sigma\n\n            # Strategy 2: Local anisotropic gaussian\n            elif strat == 2:\n                sigma = 0.45 * scale_vec * (0.8 + 0.9 * self.gscale)\n                # anisotropic per-dim multipliers\n                multipliers = 0.5 + 1.5 * self.rng.rand(self.dim)\n                sigma *= multipliers\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # ensure at least one coordinate changes (numerical safety)\n                if np.allclose(xcand, anchor, atol=1e-16):\n                    idx_change = self.rng.randint(self.dim)\n                    xcand[idx_change] += 1e-6 * span[idx_change] + 1e-3 * sigma[idx_change]\n                # occasional mixed uniform per-dim to break local traps\n                if self.rng.rand() < 0.06:\n                    mix = self.rng.rand(self.dim) < 0.08\n                    if np.any(mix):\n                        xcand[mix] = self._uniform_array(lb[mix], ub[mix], n=1)\n\n            # Strategy 3: tempered Cauchy global escape\n            else:\n                cauch = self._tempered_cauchy(scale_vec * (1.0 + 0.6 * self.gscale))\n                xcand = anchor + cauch\n                # sometimes add directed differential component\n                if self.rng.rand() < 0.3:\n                    xcand += 0.4 * (xa - xb) * (0.4 + 0.8 * self.rng.rand()) * self.gscale\n\n            # occasional directed small jump to current best for refinement\n            if best_x is not None and self.rng.rand() < 0.08:\n                dir_to_best = best_x - xcand\n                # small step towards best scaled by coordinate-wise influence\n                xcand += 0.08 * dir_to_best * (0.4 + 0.6 * self.rng.rand())\n\n            # sanitize candidate\n            bad = ~np.isfinite(xcand)\n            if np.any(bad):\n                # replace bad coords by uniform draws\n                xrep = self._uniform_array(lb, ub, n=1)\n                xcand[bad] = xrep[bad]\n\n            # reflect to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub, niter=4)\n\n            # evaluate candidate\n            fnew = safe_eval(xcand)\n\n            # update archive: append new candidate\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # update best if improved\n            is_improved = False\n            if np.isfinite(fnew) and fnew < best_f:\n                best_f = fnew\n                best_x = xcand.copy()\n                last_improve_eval = evals\n                is_improved = True\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_F = F[np.isfinite(F)] if F.size > 0 else np.array([])\n            is_success = False\n            if np.isfinite(fnew):\n                if finite_F.size == 0:\n                    is_success = True\n                else:\n                    combined = np.concatenate((finite_F, np.array([fnew])))\n                    thresh = np.percentile(combined, 25)\n                    if fnew <= thresh:\n                        is_success = True\n            if is_improved:\n                is_success = True\n\n            success_history.append(1 if is_success else 0)\n            # keep success_history bounded\n            if len(success_history) > self.adapt_window:\n                success_history = success_history[-self.adapt_window:]\n\n            # short-term adaptation of global scale\n            if len(success_history) >= max(4, int(0.25 * self.adapt_window)):\n                succ_rate = float(np.mean(success_history))\n                # if success rate too low -> increase exploration\n                if succ_rate < 0.12:\n                    self.gscale = float(np.clip(self.gscale * (1.0 + 0.18 * self.rng.rand()), self.gscale_min, self.gscale_max))\n                # if success rate very high -> exploit more (shrink)\n                elif succ_rate > 0.36:\n                    self.gscale = float(np.clip(self.gscale * (0.8 - 0.12 * self.rng.rand()), self.gscale_min, self.gscale_max))\n                # small jitter to avoid exact repeats\n                self.gscale = float(np.clip(self.gscale * (1.0 + 0.02 * (self.rng.rand() - 0.5)), self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep best + some random + spaced)\n            if len(archive) > self.archive_max:\n                Farr = np.array([a['f'] for a in archive], dtype=float)\n                finite_mask_all = np.isfinite(Farr)\n                idx_sorted = np.argsort(np.where(finite_mask_all, Farr, np.inf))\n                keep = set()\n                # keep best kbest\n                kbest = max(3, int(0.1 * self.archive_max))\n                for ii in idx_sorted[:kbest]:\n                    keep.add(ii)\n                # keep some randoms\n                n_rand = max(3, int(0.06 * self.archive_max))\n                all_idx = list(range(len(archive)))\n                self.rng.shuffle(all_idx)\n                for ii in all_idx:\n                    if len(keep) >= (kbest + n_rand):\n                        break\n                    keep.add(ii)\n                # keep some spaced in history (by age)\n                n_hist = max(3, int(0.06 * self.archive_max))\n                spaced = list(range(0, len(archive), max(1, len(archive) // (n_hist + 1))))\n                for i in spaced:\n                    if len(keep) >= self.archive_max:\n                        break\n                    keep.add(i)\n                # fill remaining random\n                ii = 0\n                while len(keep) < self.archive_max and ii < len(archive):\n                    keep.add(ii)\n                    ii += 1\n                archive = [archive[i] for i in sorted(keep)]\n                # compact ages\n                for idx, a in enumerate(archive):\n                    a['age'] = idx\n\n            # stagnation detection & micro-restarts\n            no_improve_steps = evals - last_improve_eval\n            if no_improve_steps > self.stagnation_window and evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random center)\n                cloud_size = min(40, max(6, int(0.05 * self.budget)))\n                center = best_x.copy() if (best_x is not None) else self._uniform_array(lb, ub, n=1)\n                # sample cloud points, evaluate some of them (respect budget)\n                for _ in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    perturb = (0.8 * self._tempered_cauchy(0.5 * scale_vec * max(1.0, self.gscale)))\n                    xr = self._reflect_bounds(center + perturb, lb, ub, niter=2)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        last_improve_eval = evals\n                # also add some global randoms\n                n_global = min(6, max(2, int(0.01 * self.budget)))\n                for _ in range(n_global):\n                    if evals >= self.budget:\n                        break\n                    xr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        last_improve_eval = evals\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = float(np.clip(self.gscale * 1.25, self.gscale_min, self.gscale_max))\n                success_history = []\n\n            # optionally verbose occasionally\n            if self.verbose and (evals % max(1, int(max(1, self.budget // 10))) == 0):\n                print(f\"[MG-ADS] evals={evals}, best_f={best_f:.6g}, gscale={self.gscale:.4g}, archive={len(archive)}\")\n\n        # final best from finite archive if any\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda a: a['f'])\n            return float(best['f']), best['x'].copy()\n        else:\n            # all infinite: return a uniform fallback (if budget left will evaluate once)\n            xfb = self._uniform_array(lb, ub, n=1)\n            ffb = safe_eval(xfb)\n            return float(ffb), xfb.copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 357, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 10 input values to the 1 output values where the mask is true\nOn line: xcand[mix] = self._uniform_array(lb[mix], ub[mix], n=1)", "error": "In the code, line 357, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 10 input values to the 1 output values where the mask is true\nOn line: xcand[mix] = self._uniform_array(lb[mix], ub[mix], n=1)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9e200f95-1430-415c-bb04-ba27c829ccbe", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style recombination, PCA-guided elite moves, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 elite_frac=0.12,     # fraction considered elite\n                 adapt_window=40,     # short-term success window for adaptation\n                 gscale_init=0.6,     # initial global multiplier for step sizes\n                 gscale_min=1e-3,\n                 gscale_max=8.0,\n                 archive_max=None,\n                 verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.elite_frac = float(elite_frac)\n        self.adapt_window = int(adapt_window)\n        self.gscale = float(gscale_init)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        if archive_max is None:\n            self.archive_max = max(40, 6 * self.dim + 40)\n        else:\n            self.archive_max = int(archive_max)\n        self.verbose = bool(verbose)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Prefer function-provided bounds if available, otherwise [-5,5]^dim\n        try:\n            # widely used patterns: func.bounds.lb / ub or func.bounds or func.lb/ub\n            if hasattr(func, \"bounds\"):\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                elif isinstance(b, (tuple, list)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n                else:\n                    lb = None; ub = None\n            elif hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                lb = np.asarray(func.lb, dtype=float)\n                ub = np.asarray(func.ub, dtype=float)\n            else:\n                lb = None; ub = None\n        except Exception:\n            lb = None; ub = None\n\n        if lb is None or ub is None:\n            lb_a = np.repeat(-5.0, self.dim)\n            ub_a = np.repeat(5.0, self.dim)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            # accept scalar, vector length dim, or shape (dim,2)\n            if lb.size == 1:\n                lb_a = np.repeat(lb.item(), self.dim)\n            elif lb.size == self.dim:\n                lb_a = lb.reshape(self.dim)\n            elif lb.size == self.dim * 2 and lb.shape == (self.dim, 2):\n                lb_a = lb[:, 0]\n            else:\n                # fallback\n                lb_a = np.repeat(-5.0, self.dim)\n\n            if ub.size == 1:\n                ub_a = np.repeat(ub.item(), self.dim)\n            elif ub.size == self.dim:\n                ub_a = ub.reshape(self.dim)\n            elif ub.size == self.dim * 2 and ub.shape == (self.dim, 2):\n                ub_a = ub[:, 1]\n            else:\n                ub_a = np.repeat(5.0, self.dim)\n\n        # ensure lb <= ub elementwise\n        swap = lb_a > ub_a\n        if np.any(swap):\n            tmp = lb_a.copy()\n            lb_a[swap] = ub_a[swap]\n            ub_a[swap] = tmp[swap]\n\n        return lb_a, ub_a\n\n    def _reflect_bounds(self, x, lb, ub, niter=5):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = x.copy()\n        for _ in range(niter):\n            below = x < lb\n            above = x > ub\n            if not np.any(below) and not np.any(above):\n                break\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        \"\"\"\n        Return a single vector of length dim if n==1, otherwise an (n,dim) array.\n        lb,ub can be arrays length dim or scalars.\n        \"\"\"\n        lb_a = np.asarray(lb, dtype=float)\n        ub_a = np.asarray(ub, dtype=float)\n        if lb_a.size == 1:\n            lb_a = np.repeat(lb_a.item(), self.dim)\n        if ub_a.size == 1:\n            ub_a = np.repeat(ub_a.item(), self.dim)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub_a - lb_a) + lb_a\n        else:\n            r = self.rng.rand(n, self.dim) * (ub_a - lb_a) + lb_a\n            return r\n\n    def _tempered_cauchy(self, scale_vec, heavy_prob=0.06, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy: returns vector of length dim.\n        - scale_vec: per-dim scale (array-like)\n        - occasionally apply heavy multiplicative factors\n        - cap extremes to avoid numerical issues\n        \"\"\"\n        s = np.asarray(scale_vec, dtype=float).reshape(self.dim)\n        # avoid zeros\n        s = np.maximum(s, 1e-12)\n        # standard Cauchy via inverse CDF\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplier per-coordinate\n        heavy = (self.rng.rand(self.dim) < heavy_prob).astype(float)\n        heavy_mult = 1.0 + heavy * (2.0 + self.rng.rand(self.dim) * (cap_multiplier - 2.0))\n        step = z * s * heavy_mult\n        # small gaussian jitter to avoid pure Cauchy extremes\n        step += 0.02 * s * self.rng.randn(self.dim)\n        # cap extreme values elementwise relative to scale\n        cap = cap_multiplier * np.maximum(s, 1e-12)\n        step = np.clip(step, -cap, cap)\n        return step\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n\n        evals = 0\n        age_counter = 0\n\n        archive = []  # list of dicts {'x':..., 'f':..., 'age':...}\n\n        # safe evaluation enforcing budget and catching exceptions\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                return np.inf\n            try:\n                fx = float(func(x))\n            except Exception:\n                fx = np.inf\n            evals += 1\n            age_counter += 1\n            return fx\n\n        # ---- initialization: LHS-like + a few pure randoms ----\n        n_init = min(max(20, 4 * self.dim), max(2, int(0.12 * max(1, self.budget))))\n        n_init = min(n_init, max(1, self.budget))  # cannot exceed budget\n        pts = np.zeros((n_init, self.dim), dtype=float)\n        # Latin-hypercube like: per-dim strata\n        perms = [self.rng.permutation(n_init) for _ in range(self.dim)]\n        for d in range(self.dim):\n            for i in range(n_init):\n                base = (perms[d][i] + self.rng.rand()) / n_init\n                pts[i, d] = lb[d] + base * (ub[d] - lb[d])\n        # Evaluate initial\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = pts[i]\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # a few uniform extra draws (diversify)\n        n_uniform = min(3, max(0, self.budget - evals))\n        for _ in range(n_uniform):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # If nothing evaluated (e.g. budget==0) return fallback\n        if len(archive) == 0:\n            return np.inf, self._uniform_array(lb, ub, n=1)\n\n        # initial best\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best_idx = np.argmin([a['f'] for a in finite_archive])\n            best_x = finite_archive[best_idx]['x'].copy()\n            best_f = finite_archive[best_idx]['f']\n            last_improve_eval = evals\n        else:\n            best_x = None\n            best_f = np.inf\n            last_improve_eval = 0\n\n        # history for adaptation\n        success_history = []\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        base_probs = np.array([0.32, 0.18, 0.38, 0.12], dtype=float)\n\n        # main loop\n        while evals < self.budget:\n            # convert archive arrays\n            X = np.array([a['x'] for a in archive], dtype=float)\n            F = np.array([a['f'] for a in archive], dtype=float)\n\n            finite_mask = np.isfinite(F)\n            finite_X = X[finite_mask] if np.any(finite_mask) else np.zeros((0, self.dim))\n            finite_F = F[finite_mask] if np.any(finite_mask) else np.array([])\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            if finite_X.shape[0] >= 2:\n                q75 = np.percentile(finite_X, 75, axis=0)\n                q25 = np.percentile(finite_X, 25, axis=0)\n                iqr = (q75 - q25) / 1.349  # approximate standard deviation\n                scale_vec = iqr\n                # fallback to std where IQR tiny\n                stds = np.std(finite_X, axis=0)\n                small = scale_vec <= 1e-12\n                scale_vec[small] = stds[small]\n                # no good statistics: use proportion of bound span\n                tiny = scale_vec <= 1e-12\n                scale_vec[tiny] = 0.08 * span[tiny]\n            else:\n                scale_vec = 0.08 * span\n\n            # determine elites\n            n_elite = max(2, int(max(2, self.elite_frac * max(1, len(archive)))))\n            finite_idx = np.where(finite_mask)[0]\n            if finite_idx.size > 0:\n                sorted_idx = finite_idx[np.argsort(finite_F)]\n                elite_idx = sorted_idx[:min(n_elite, sorted_idx.size)].tolist()\n            else:\n                elite_idx = list(range(min(n_elite, len(archive))))\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            if len(elite_idx) >= 1:\n                weights = np.exp(-np.linspace(0, 2.0, len(elite_idx)))\n                weights = weights / np.sum(weights)\n                pick = self.rng.choice(len(elite_idx), p=weights)\n                anchor_idx = elite_idx[pick]\n                anchor = archive[anchor_idx]['x'].copy()\n            else:\n                # fallback random anchor\n                anchor_idx = self.rng.randint(len(archive))\n                anchor = archive[anchor_idx]['x'].copy()\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = base_probs.copy()\n            probs[3] *= (1.0 + 0.35 * min(2.0, self.gscale))\n            probs = np.maximum(probs, 1e-8)\n            probs = probs / np.sum(probs)\n\n            strat = self.rng.choice(4, p=probs)\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                idxs = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in idxs:\n                    idxs.remove(exclude_idx)\n                if len(idxs) >= k:\n                    return self.rng.choice(idxs, size=k, replace=False).tolist()\n                else:\n                    # allow repeats if not enough\n                    return self.rng.choice(range(len(archive)), size=k, replace=True).tolist()\n\n            # candidate generation\n            if strat == 0:\n                # Strategy 0: DE-style differential injection\n                donors = pick_distinct(3, exclude_idx=anchor_idx)\n                xa = archive[donors[0]]['x']\n                xb = archive[donors[1]]['x']\n                xc = archive[donors[2]]['x']\n                diff = xb - xc\n                # normalize differential by typical coordinate scale\n                denom = 1.0 + np.abs(diff) / (scale_vec + 1e-12)\n                Fcoef = 0.6 * self.gscale\n                step = Fcoef * diff / denom\n                anis = 0.12 * scale_vec * self.gscale * self.rng.randn(self.dim)\n                xcand = xa + step + anis\n                # occasional tempered escape\n                if self.rng.rand() < 0.06:\n                    xcand += 0.6 * self._tempered_cauchy(scale_vec * self.gscale)\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                if len(elite_idx) >= 2:\n                    top_k = min(len(elite_idx), max(2, int(max(2, 0.25 * len(elite_idx)))))\n                    mat = np.array([archive[i]['x'] for i in elite_idx[:top_k]])\n                    c = np.mean(mat, axis=0)\n                    M = mat - c\n                    try:\n                        cov = np.cov(M, rowvar=False)\n                        eigvals, eigvecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                        order = np.argsort(-eigvals)\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        n_pcs = min(max(1, int(np.ceil(self.dim * 0.25))), self.dim)\n                        coeffs = self.rng.randn(n_pcs) * np.sqrt(np.maximum(eigvals[:n_pcs], 1e-12))\n                        step = eigvecs[:, :n_pcs].dot(coeffs) * (0.6 * self.gscale)\n                        # blend with anchor\n                        xcand = anchor + 0.8 * step + 0.2 * (c - anchor)\n                    except Exception:\n                        sigma = 0.6 * scale_vec * self.gscale\n                        xcand = anchor + self.rng.randn(self.dim) * sigma\n                else:\n                    sigma = 0.6 * scale_vec * self.gscale\n                    xcand = anchor + self.rng.randn(self.dim) * sigma\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                sigma = 0.45 * scale_vec * (0.8 + 0.9 * self.gscale)\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # occasional mixed uniform per-dim to break local traps\n                if self.rng.rand() < 0.06:\n                    mix = self.rng.rand(self.dim) < 0.12\n                    if np.any(mix):\n                        u = self._uniform_array(lb[mix], ub[mix], n=1)\n                        xcand[mix] = u\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                cauch = self._tempered_cauchy(scale_vec * max(0.8, self.gscale))\n                xcand = anchor + cauch\n                # sometimes add directed differential component\n                if self.rng.rand() < 0.25 and len(archive) >= 2:\n                    ids = pick_distinct(2, exclude_idx=anchor_idx)\n                    xa = archive[ids[0]]['x']\n                    xb = archive[ids[1]]['x']\n                    xcand += 0.4 * (xa - xb) * (0.4 + 0.8 * self.rng.rand()) * self.gscale\n\n            # occasional directed small jump to current best for refinement\n            if best_x is not None and self.rng.rand() < 0.08:\n                dir_to_best = best_x - anchor\n                xcand += 0.08 * dir_to_best * (0.4 + 0.6 * self.rng.rand())\n\n            # sanitize candidate: replace NaNs/Infs by uniform draws\n            bad = ~np.isfinite(xcand)\n            if np.any(bad):\n                xcand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub)\n\n            # evaluate candidate\n            fnew = safe_eval(xcand)\n\n            # update archive: append new candidate\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # update best if improved\n            is_improved = False\n            if np.isfinite(fnew) and fnew < best_f:\n                best_f = fnew\n                best_x = xcand.copy()\n                last_improve_eval = evals\n                is_improved = True\n\n            # measure success: success if improved global best or in top 25% of finite archive\n            is_success = False\n            finite_F = np.array([a['f'] for a in archive if np.isfinite(a['f'])])\n            if np.isfinite(fnew):\n                if is_improved:\n                    is_success = True\n                elif finite_F.size > 0:\n                    combined = np.concatenate((finite_F, np.array([fnew])))\n                    thresh = np.percentile(combined, 25)\n                    if fnew <= thresh:\n                        is_success = True\n            success_history.append(1 if is_success else 0)\n            # keep success_history bounded\n            if len(success_history) > self.adapt_window:\n                success_history = success_history[-self.adapt_window:]\n\n            # short-term adaptation of global scale\n            if len(success_history) >= 4:\n                succ_rate = float(np.mean(success_history))\n                if succ_rate < 0.18:\n                    # increase exploration\n                    self.gscale = float(np.clip(self.gscale * (1.0 + 0.12 * (1.0 + self.rng.rand())), self.gscale_min, self.gscale_max))\n                elif succ_rate > 0.60:\n                    # exploit more (shrink)\n                    self.gscale = float(np.clip(self.gscale * (0.85 - 0.08 * self.rng.rand()), self.gscale_min, self.gscale_max))\n                else:\n                    # small jitter\n                    self.gscale = float(np.clip(self.gscale * (1.0 + 0.02 * (self.rng.rand() - 0.5)), self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep best + some random + spaced by age)\n            if len(archive) > self.archive_max and len(archive) > 5:\n                kbest = max(3, int(0.12 * self.archive_max))\n                n_rand = max(3, int(0.06 * self.archive_max))\n                n_hist = max(3, int(0.06 * self.archive_max))\n                # best indices (finite only)\n                fin_idx = np.where(np.isfinite(np.array([a['f'] for a in archive])))[0]\n                if fin_idx.size > 0:\n                    sorted_fin = fin_idx[np.argsort([archive[i]['f'] for i in fin_idx])]\n                else:\n                    sorted_fin = np.arange(len(archive))\n                keep_idx = set(sorted_fin[:kbest].tolist())\n                # some randoms\n                all_idx = list(range(len(archive)))\n                rand_choice = self.rng.choice(all_idx, size=min(n_rand, len(all_idx)), replace=False).tolist()\n                keep_idx.update(rand_choice)\n                # some spaced by age\n                ages = np.array([a['age'] for a in archive])\n                oldest_idx = np.argsort(ages)[-n_hist:].tolist()\n                keep_idx.update(oldest_idx)\n                # always keep current anchor and best\n                keep_idx.add(anchor_idx)\n                if best_x is not None:\n                    # find index of a point equal to best_x (approx)\n                    dists = np.linalg.norm(np.array([a['x'] for a in archive]) - best_x.reshape(1, -1), axis=1)\n                    keep_idx.add(int(np.argmin(dists)))\n                # build pruned archive\n                new_archive = [archive[i] for i in sorted(list(keep_idx))]\n                archive = new_archive\n\n            # stagnation detection & micro-restarts\n            stagnation_threshold = max(60, 6 * self.dim)\n            if evals - last_improve_eval > stagnation_threshold and evals < self.budget:\n                # micro-restart: local cloud around current best (or random center)\n                cloud_center = best_x.copy() if best_x is not None else self._uniform_array(lb, ub, n=1)\n                cloud_size = min(12, max(6, int(3 + 0.5 * self.dim)))\n                # sample cloud points but respect budget\n                for _ in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    perturb = 0.5 * self._tempered_cauchy(0.5 * scale_vec * max(1.0, self.gscale))\n                    xr = cloud_center + perturb\n                    xr = self._reflect_bounds(xr, lb, ub)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        last_improve_eval = evals\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = float(np.clip(self.gscale * 1.25, self.gscale_min, self.gscale_max))\n                success_history = []\n\n            # optionally verbose occasionally\n            if self.verbose and (evals % max(1, int(max(1, self.budget // 10))) == 0):\n                print(f\"[MG-ADS] evals={evals}, best_f={best_f:.6g}, gscale={self.gscale:.3g}, archive={len(archive)}\")\n\n        # final best from finite archive if any\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda a: a['f'])\n            return float(best['f']), best['x'].copy()\n        else:\n            # all infinite: return a uniform fallback (if budget left will evaluate once)\n            return np.inf, self._uniform_array(lb, ub, n=1)", "configspace": "", "generation": 0, "feedback": "In the code, line 127, in _uniform_array, the following error occurred:\nValueError: operands could not be broadcast together with shapes (10,) (2,) \nOn line: return self.rng.rand(self.dim) * (ub_a - lb_a) + lb_a", "error": "In the code, line 127, in _uniform_array, the following error occurred:\nValueError: operands could not be broadcast together with shapes (10,) (2,) \nOn line: return self.rng.rand(self.dim) * (ub_a - lb_a) + lb_a", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "35dca236-c3c8-4f73-9452-f601b631cfab", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid continuous optimizer mixing DE-style recombination, PCA-elite moves, anisotropic local Gaussians and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 elite_frac=0.16,\n                 gscale=0.6, gscale_min=1e-3, gscale_max=6.0,\n                 adapt_window=40, stagnation_window=500,\n                 archive_max=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.elite_frac = float(elite_frac)\n        self.gscale = float(gscale)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.adapt_window = int(adapt_window)\n        self.stagnation_window = int(stagnation_window)\n        self.archive_max = int(archive_max) if archive_max is not None else max(40, 6 * self.dim + 40)\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.28, 0.22, 0.38, 0.12], dtype=float)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to read common attributes; otherwise default [-5,5]^dim\n        lb = None\n        ub = None\n        # func may expose .bounds with .lb and .ub or .lower/.upper or attributes lb/ub\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, 0, None)\n                ub = getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, 1, None)\n        except Exception:\n            lb = ub = None\n\n        if lb is None:\n            lb = getattr(func, \"lb\", None) or getattr(func, \"lower\", None)\n        if ub is None:\n            ub = getattr(func, \"ub\", None) or getattr(func, \"upper\", None)\n\n        def to_arr(v):\n            if v is None:\n                return None\n            a = np.asarray(v, dtype=float)\n            # scalar -> broadcast\n            if a.size == 1:\n                return np.repeat(float(a), self.dim)\n            # shape (dim,) or (dim,1)\n            return a.reshape(self.dim)\n\n        lb_a = to_arr(lb)\n        ub_a = to_arr(ub)\n\n        if lb_a is None or ub_a is None:\n            # fallback to [-5,5]^dim\n            lb_a = np.repeat(-5.0, self.dim)\n            ub_a = np.repeat(5.0, self.dim)\n\n        lb_a = np.asarray(lb_a, dtype=float).reshape(self.dim)\n        ub_a = np.asarray(ub_a, dtype=float).reshape(self.dim)\n\n        # ensure lb <= ub\n        mask = lb_a > ub_a\n        if np.any(mask):\n            # swap where necessary\n            tmp = lb_a[mask].copy()\n            lb_a[mask] = ub_a[mask]\n            ub_a[mask] = tmp\n\n        return lb_a, ub_a\n\n    def _reflect_bounds(self, x, lb, ub, niter=5):\n        # Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        # then final clamp.\n        x = x.copy()\n        for _ in range(niter):\n            below = x < lb\n            if not np.any(below) and not np.any(x > ub):\n                break\n            # mirror\n            x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        \"\"\"\n        Return a single vector of length dim if n==1, otherwise an (n,dim) array.\n        lb,ub can be arrays length dim or scalars.\n        \"\"\"\n        lb_a = np.asarray(lb, dtype=float)\n        ub_a = np.asarray(ub, dtype=float)\n        if lb_a.size == 1:\n            lb_a = np.repeat(lb_a.item(), self.dim)\n        if ub_a.size == 1:\n            ub_a = np.repeat(ub_a.item(), self.dim)\n        if n == 1:\n            return lb_a + (ub_a - lb_a) * self.rng.rand(self.dim)\n        else:\n            return lb_a + (ub_a - lb_a) * self.rng.rand(n, self.dim)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy: returns vector of length dim.\n        - scale_vec: per-dim scale\n        - occasionally apply heavy multiplicative factors\n        - cap extremes to avoid numerical issues\n        \"\"\"\n        u = self.rng.rand(self.dim)  # (0,1)\n        z = np.tan(np.pi * (u - 0.5))\n        heavy = (self.rng.rand(self.dim) < 0.06).astype(float)  # occasional heavy per-dim\n        heavy_mult = 1.0 + heavy * (5.0 + 20.0 * self.rng.rand(self.dim))\n        step = z * scale_vec * heavy_mult * (0.6 + 0.8 * self.rng.rand(self.dim))\n        # small gaussian jitter\n        step += 0.04 * scale_vec * self.rng.randn(self.dim)\n        # cap extreme values\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        step = np.clip(step, -cap, cap)\n        return step\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span = np.maximum(span, 1e-8)\n\n        archive = []  # list of dicts {'x':..., 'f':..., 'age':...}\n        evals = 0\n        age_counter = 0\n\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                return np.inf\n            try:\n                f = float(func(np.asarray(x, dtype=float)))\n            except Exception:\n                f = np.inf\n            evals += 1\n            age_counter += 1\n            return f\n\n        # ---- initialization: LHS-like + a few pure randoms ----\n        n_init = min(max(20, 4 * self.dim), max(2, int(0.12 * max(1, self.budget))))\n        n_init = min(n_init, max(1, self.budget))\n        # simple Latin-hypercube like: per-dim permutations\n        perm_matrix = np.zeros((n_init, self.dim), dtype=int)\n        for d in range(self.dim):\n            perm = np.arange(n_init)\n            self.rng.shuffle(perm)\n            perm_matrix[:, d] = perm\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            x = np.empty(self.dim, dtype=float)\n            for d in range(self.dim):\n                a = (perm_matrix[i, d] + self.rng.rand()) / float(n_init)\n                x[d] = lb[d] + a * (ub[d] - lb[d])\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n\n        # a few extra uniform draws if budget allows\n        for _ in range(min(6, max(0, self.budget - evals))):\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n            if evals >= self.budget:\n                break\n\n        if len(archive) == 0:\n            # budget 0\n            return float(np.inf), np.repeat(np.nan, self.dim)\n\n        # initial best\n        best_idx = int(np.argmin([a['f'] for a in archive]))\n        best_f = float(archive[best_idx]['f'])\n        best_x = archive[best_idx]['x'].copy()\n\n        # adaptation history\n        success_history = deque(maxlen=self.adapt_window)\n        last_improve_eval = evals if np.isfinite(best_f) else 0\n\n        # working probs\n        probs = self.base_probs.copy()\n\n        # main loop\n        while evals < self.budget:\n            # convert archive arrays\n            X = np.array([a['x'] for a in archive], dtype=float)\n            F = np.array([a['f'] for a in archive], dtype=float)\n            ages = np.array([a['age'] for a in archive], dtype=float)\n\n            finite_mask = np.isfinite(F)\n            finite_idx = np.where(finite_mask)[0]\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            if finite_idx.size >= 2:\n                Xf = X[finite_idx]\n                q75 = np.percentile(Xf, 75, axis=0)\n                q25 = np.percentile(Xf, 25, axis=0)\n                iqr = q75 - q25\n                stds = np.std(Xf, axis=0)\n                scale_vec = np.where(iqr > 1e-12, iqr / 1.349, stds)\n                scale_vec = np.maximum(scale_vec, 1e-12 * span)\n            else:\n                # no good statistics: use proportion of bound span\n                scale_vec = 0.12 * span\n\n            # determine elites\n            n_elite = max(1, int(np.ceil(self.elite_frac * max(1, len(archive)))))\n            if finite_idx.size >= n_elite:\n                order = np.argsort(F[finite_idx])\n                elite_idx = finite_idx[order[:n_elite]].tolist()\n            else:\n                elite_idx = list(range(min(n_elite, len(archive))))\n\n            # choose strategy (adaptive biased by gscale)\n            # favor Cauchy when gscale large\n            probs_mod = probs.copy()\n            probs_mod[3] *= (1.0 + 0.8 * (self.gscale - self.gscale_min) / max(1e-6, self.gscale_max - self.gscale_min))\n            probs_mod = np.maximum(probs_mod, 1e-8)\n            probs_mod = probs_mod / np.sum(probs_mod)\n            strat = self.rng.choice(4, p=probs_mod)\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            if len(elite_idx) > 0 and self.rng.rand() < 0.96:\n                # weight by rank exponential preference\n                ranks = np.arange(len(elite_idx))\n                # better ones (earlier in list) get higher weight\n                weights = np.exp(-0.8 * ranks.astype(float))\n                weights = weights / np.sum(weights)\n                pick = self.rng.choice(len(elite_idx), p=weights)\n                anchor = archive[elite_idx[pick]]['x'].copy()\n            else:\n                anchor = self._uniform_array(lb, ub, n=1)\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                ids = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in ids:\n                    ids.remove(exclude_idx)\n                if len(ids) >= k:\n                    return self.rng.choice(ids, size=k, replace=False).tolist()\n                else:\n                    # allow duplicates if not enough\n                    return [self.rng.randint(0, len(archive)) for _ in range(k)]\n\n            # prepare candidate\n            if strat == 0 and len(archive) >= 3:\n                # Strategy 0: DE-style differential injection\n                i_anchor = None\n                if len(elite_idx) > 0 and self.rng.rand() < 0.7:\n                    base = anchor.copy()\n                else:\n                    base = archive[self.rng.randint(0, len(archive))]['x'].copy()\n                donors = pick_distinct(2)\n                xa = archive[donors[0]]['x']\n                xb = archive[donors[1]]['x']\n                Fcoef = 0.6 * (0.6 + 0.8 * self.rng.rand()) * self.gscale\n                diff = (xa - xb)\n                # normalize diff by per-dim std to avoid huge steps\n                denom = np.maximum(np.std(X, axis=0), 1e-12 * span)\n                diff = diff * (scale_vec / denom)\n                xcand = base + Fcoef * diff\n                # anisotropic noise\n                noise = (0.02 + 0.12 * self.rng.rand(self.dim)) * scale_vec * self.rng.randn(self.dim)\n                xcand += noise\n                # small tempered cauchy chance\n                if self.rng.rand() < 0.06:\n                    xcand += 0.15 * self._tempered_cauchy(scale_vec)\n            elif strat == 1 and len(elite_idx) >= 2:\n                # Strategy 1: PCA-guided elite perturbation\n                Xel = np.array([archive[i]['x'] for i in elite_idx])\n                # center at mean of elites\n                mu = np.mean(Xel, axis=0)\n                # PCA on elite covariance\n                cov = np.cov((Xel - mu).T) if Xel.shape[0] > 1 else np.diag(scale_vec**2)\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                except Exception:\n                    eigvals = np.maximum(np.var(Xel, axis=0), 1e-12)\n                    eigvecs = np.eye(self.dim)\n                    eigvals = np.ones(self.dim) * np.mean(eigvals)\n                order = np.argsort(-eigvals)\n                eigvals = eigvals[order]\n                eigvecs = eigvecs[:, order]\n                n_pcs = max(1, min(self.dim, 1 + int(np.sum(eigvals) / (np.max(eigvals) + 1e-12))))\n                coeffs = (np.sqrt(np.maximum(eigvals[:n_pcs], 1e-12)) *\n                          (0.6 + 0.8 * self.rng.rand(n_pcs)) *\n                          self.rng.randn(n_pcs))\n                step = eigvecs[:, :n_pcs].dot(coeffs)\n                blend = 0.12 + 0.18 * self.rng.rand()\n                xcand = (1.0 - blend) * mu + blend * anchor + (0.6 + 0.6 * self.rng.rand()) * self.gscale * step\n                # fallback gaussian if numerical issues\n                if not np.all(np.isfinite(xcand)):\n                    xcand = anchor + 0.6 * self.gscale * (scale_vec * self.rng.randn(self.dim))\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                center = anchor.copy()\n                multipliers = 0.5 + 1.5 * self.rng.rand(self.dim)\n                sigma = multipliers * 0.12 * span * (0.6 + 0.8 * self.rng.rand())\n                xcand = center + self.gscale * (sigma * self.rng.randn(self.dim))\n                # ensure at least one coordinate changes\n                if np.allclose(xcand, center):\n                    i = self.rng.randint(0, self.dim)\n                    xcand[i] += 0.02 * span[i] * (1.0 if self.rng.rand() < 0.5 else -1.0)\n                # occasional mixed uniform per-dim to break local traps\n                mix = self.rng.rand(self.dim) < 0.06\n                if np.any(mix):\n                    xcand[mix] = self._uniform_array(lb[mix], ub[mix], n=1)\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                xcand = anchor.copy()\n                cauch = self._tempered_cauchy(scale_vec * (1.0 + 0.6 * self.gscale))\n                xcand += cauch\n                # sometimes add directed differential component\n                if len(archive) >= 2 and self.rng.rand() < 0.3:\n                    xa = archive[self.rng.randint(0, len(archive))]['x']\n                    xb = archive[self.rng.randint(0, len(archive))]['x']\n                    xcand += 0.4 * (xa - xb) * (0.4 + 0.8 * self.rng.rand()) * self.gscale\n\n            # occasional small directed jump to current best for refinement\n            if np.isfinite(best_f) and self.rng.rand() < 0.06:\n                direction = best_x - xcand\n                frac = 0.08 * (0.4 + 0.6 * self.rng.rand())\n                xcand += frac * direction * (0.5 + 1.2 * self.rng.rand())\n\n            # sanitize NaNs/Infs\n            bad = ~np.isfinite(xcand)\n            if np.any(bad):\n                xcand[bad] = self._uniform_array(lb[bad], ub[bad], n=1)\n\n            # reflect to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub)\n\n            # evaluate candidate\n            if evals >= self.budget:\n                break\n            fnew = safe_eval(xcand)\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # update best if improved\n            is_improved = False\n            if np.isfinite(fnew) and (not np.isfinite(best_f) or fnew < best_f):\n                best_f = fnew\n                best_x = xcand.copy()\n                last_improve_eval = evals\n                is_improved = True\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            is_success = False\n            finite_F = F[finite_mask] if finite_mask.any() else np.array([])\n            if is_improved:\n                is_success = True\n            elif finite_F.size > 0 and np.isfinite(fnew):\n                combined = np.concatenate((finite_F, np.array([fnew])))\n                thr = np.percentile(combined, 25)\n                if fnew <= thr:\n                    is_success = True\n\n            success_history.append(1.0 if is_success else 0.0)\n\n            # short-term adaptation of global scale\n            if len(success_history) >= max(4, int(0.25 * self.adapt_window)):\n                succ_rate = float(np.mean(success_history))\n                # if success rate too low -> increase exploration\n                if succ_rate < 0.15:\n                    self.gscale = min(self.gscale * (1.02 + 0.02 * self.rng.rand()), self.gscale_max)\n                elif succ_rate > 0.5:\n                    self.gscale = max(self.gscale * (0.98 - 0.02 * self.rng.rand()), self.gscale_min)\n                # small jitter\n                self.gscale = float(np.clip(self.gscale * (1.0 + 0.012 * (self.rng.rand() - 0.5)),\n                                            self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep best + some random + spaced)\n            if len(archive) > self.archive_max:\n                Farr = np.array([a['f'] for a in archive], dtype=float)\n                idxs = np.arange(len(archive))\n                # always keep best kbest\n                kbest = max(3, int(0.06 * self.archive_max))\n                order = np.argsort(Farr)\n                keep = set(order[:kbest].tolist())\n                # keep some randoms\n                n_rand = max(3, int(0.08 * self.archive_max))\n                rnds = self.rng.choice(idxs.tolist(), size=min(n_rand, len(idxs)), replace=False)\n                keep.update(rnds.tolist())\n                # keep some by spacing in age\n                n_hist = max(3, int(0.06 * self.archive_max))\n                age_order = np.argsort(-ages)\n                for ii in age_order[:n_hist]:\n                    keep.add(int(ii))\n                    if len(keep) >= self.archive_max:\n                        break\n                # fill remaining by best\n                if len(keep) < self.archive_max:\n                    for ii in order:\n                        keep.add(int(ii))\n                        if len(keep) >= self.archive_max:\n                            break\n                # compact archive\n                new_archive = [archive[i] for i in sorted(list(keep))]\n                archive = new_archive\n\n            # stagnation detection & micro-restarts\n            no_improve_steps = evals - last_improve_eval\n            if no_improve_steps > self.stagnation_window and evals < self.budget:\n                # micro-restart: local cloud around current best (or random center)\n                cloud_size = min(40, max(6, int(0.05 * max(1, self.budget))))\n                center = best_x.copy() if (best_x is not None and np.isfinite(best_f)) else self._uniform_array(lb, ub, n=1)\n                # sample cloud points, evaluate some of them (respect budget)\n                for _ in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    rad = 0.05 * span * (1.0 + 4.0 * self.rng.rand(self.dim))\n                    xcloud = center + rad * self.rng.randn(self.dim)\n                    xcloud = self._reflect_bounds(xcloud, lb, ub)\n                    fr = safe_eval(xcloud)\n                    archive.append({'x': xcloud.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xcloud.copy()\n                        last_improve_eval = evals\n                # also add a few global randoms\n                for _ in range(6):\n                    if evals >= self.budget:\n                        break\n                    xr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        last_improve_eval = evals\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.12, self.gscale_max)\n                success_history.clear()\n\n            # Occasionally nudge strategy probabilities based on recent successes\n            if len(success_history) >= 8 and self.rng.rand() < 0.08:\n                # if many successes -> favor local strategies, else favor global\n                mean_succ = np.mean(success_history)\n                if mean_succ > 0.35:\n                    probs = 0.9 * probs + 0.1 * np.array([0.12, 0.18, 0.56, 0.14])\n                else:\n                    probs = 0.9 * probs + 0.1 * np.array([0.22, 0.18, 0.26, 0.34])\n                probs = np.maximum(probs, 1e-8)\n\n        # final best from finite archive if any\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda a: a['f'])\n            return float(best['f']), best['x'].copy()\n        else:\n            # all infinite: return random fallback center\n            xfb = self._uniform_array(lb, ub, n=1)\n            return float(np.inf), xfb.copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 196, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_history = deque(maxlen=self.adapt_window)", "error": "In the code, line 196, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: success_history = deque(maxlen=self.adapt_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "14268c39-0687-4983-9737-e04bfe1b092b", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 elite_frac=0.12,\n                 archive_max=200,\n                 gscale_init=1.0,\n                 gscale_min=1e-3,\n                 gscale_max=8.0,\n                 stagnation_window=None,\n                 verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.elite_frac = float(elite_frac)\n        self.archive_max = int(archive_max)\n        self.gscale = float(gscale_init)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.verbose = bool(verbose)\n        # default stagnation window\n        self.stagnation_window = (int(stagnation_window)\n                                  if stagnation_window is not None\n                                  else max(40, min(400, self.budget // 20)))\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read commonly used bound attributes from func, otherwise default.\n        Returns (lb_array, ub_array)\n        \"\"\"\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # try common shapes\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                elif hasattr(b, \"__len__\") and len(b) == 2:\n                    # (lb, ub) tuple (scalars or arrays)\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            pass\n\n        # try attributes on func\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        if lb is None or ub is None:\n            lb = np.repeat(-5.0, self.dim)\n            ub = np.repeat(5.0, self.dim)\n        else:\n            # broadcast scalars\n            if lb.ndim == 0:\n                lb = np.repeat(float(lb), self.dim)\n            if ub.ndim == 0:\n                ub = np.repeat(float(ub), self.dim)\n            lb = lb.astype(float)\n            ub = ub.astype(float)\n            if lb.size != self.dim or ub.size != self.dim:\n                # fallback\n                lb = np.repeat(-5.0, self.dim)\n                ub = np.repeat(5.0, self.dim)\n\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection into [lb,ub], repeated a few times to handle large violations,\n        then final clamp.\n        \"\"\"\n        x = np.asarray(x, dtype=float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        # if NaNs, replace by mid point\n        x = np.where(np.isfinite(x), x, 0.5 * (lb + ub))\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        \"\"\"\n        Return a single vector of length dim if n==1, otherwise an (n,dim) array.\n        lb,ub can be arrays length dim or scalars.\n        \"\"\"\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.ndim == 0:\n            lb = np.repeat(lb, self.dim)\n        if ub.ndim == 0:\n            ub = np.repeat(ub, self.dim)\n        if n == 1:\n            return self.rng.rand(self.dim) * (ub - lb) + lb\n        else:\n            r = self.rng.rand(n, self.dim) * (ub - lb)[None, :] + lb[None, :]\n            return r\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy: returns vector of length dim.\n        - scale_vec: per-dim scale\n        - occasionally apply heavy multiplicative factors\n        - cap extremes to avoid numerical issues\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # occasional heavy multiplier per-coordinate\n        heavy = (self.rng.rand(self.dim) < 0.02).astype(float)\n        mul = 1.0 + heavy * (5.0 + 20.0 * self.rng.rand(self.dim))\n        step = z * scale_vec * mul\n        # small gaussian jitter\n        step += 0.02 * self.rng.randn(self.dim) * np.maximum(scale_vec, 1e-12)\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        step = np.clip(step, -cap, cap)\n        # if any NaN/inf, replace by gaussian\n        bad = ~np.isfinite(step)\n        if np.any(bad):\n            step[bad] = self.rng.randn(np.sum(bad)) * np.maximum(scale_vec[bad], 1e-6)\n        return step\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure arrays\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        span = ub - lb\n        if np.any(span <= 0):\n            span = np.maximum(span, 1.0)\n\n        archive = []  # list of dicts {'x':..., 'f':..., 'age':...}\n\n        evals = 0\n        age_counter = 0\n\n        # local state for adaptation\n        probs = np.array([0.25, 0.20, 0.45, 0.10], dtype=float)  # DE, PCA, Local, Cauchy\n        success_history = []\n        no_improve_steps = 0\n        best_f = np.inf\n        best_x = None\n\n        def safe_eval(x):\n            nonlocal evals, age_counter\n            if evals >= self.budget:\n                return float('inf')\n            try:\n                fx = func(np.asarray(x, dtype=float))\n            except Exception:\n                fx = float('inf')\n            evals += 1\n            age_counter += 1\n            return float(fx)\n\n        # ---- initialization: LHS-like + a few pure randoms ----\n        if self.budget <= 0:\n            return float('inf'), None\n\n        n_init = min(max(6, self.dim * 6), max(1, self.budget // 6))\n        # Latin-hypercube like: per-dim strata\n        Xinit = np.empty((n_init, self.dim), dtype=float)\n        for d in range(self.dim):\n            perm = np.arange(n_init)\n            self.rng.shuffle(perm)\n            # stratified within each slice\n            cut = (perm + self.rng.rand(n_init)) / n_init\n            Xinit[:, d] = lb[d] + cut * (ub[d] - lb[d])\n        # evaluate initial\n        for i in range(n_init):\n            x = Xinit[i]\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': age_counter})\n            if np.isfinite(f) and f < best_f:\n                best_f = f\n                best_x = x.copy()\n\n        # a few uniform extra draws\n        n_uniform = min(8, max(0, (self.budget - evals) // 50))\n        for _ in range(n_uniform):\n            xr = self._uniform_array(lb, ub, n=1)\n            fr = safe_eval(xr)\n            archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n            if np.isfinite(fr) and fr < best_f:\n                best_f = fr\n                best_x = xr.copy()\n\n        # if no evaluation succeeded, evaluate a single fallback\n        if len(archive) == 0:\n            xfb = self._uniform_array(lb, ub, n=1)\n            ffb = safe_eval(xfb)\n            archive.append({'x': xfb.copy(), 'f': ffb, 'age': age_counter})\n            if np.isfinite(ffb):\n                best_f = ffb\n                best_x = xfb.copy()\n\n        # main loop\n        while evals < self.budget:\n            # convert archive arrays\n            X = np.array([a['x'] for a in archive], dtype=float)\n            F = np.array([a['f'] for a in archive], dtype=float)\n\n            finite_idx = np.where(np.isfinite(F))[0]\n            finite_X = X[finite_idx] if finite_idx.size > 0 else np.empty((0, self.dim))\n            finite_F = F[finite_idx] if finite_idx.size > 0 else np.empty((0,))\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            if finite_X.shape[0] >= 2:\n                q75 = np.percentile(finite_X, 75, axis=0)\n                q25 = np.percentile(finite_X, 25, axis=0)\n                iqr = q75 - q25\n                small = iqr < 1e-9\n                scale_vec = iqr.copy()\n                if np.any(small):\n                    stdv = np.std(finite_X, axis=0)\n                    scale_vec[small] = stdv[small]\n                # if still zero, fallback to fraction of span\n                zero_mask = scale_vec <= 1e-12\n                scale_vec[zero_mask] = 0.2 * span[zero_mask]\n            else:\n                scale_vec = 0.2 * span\n\n            # determine elites\n            if finite_F.size > 0:\n                n_elite = max(1, int(np.ceil(self.elite_frac * finite_F.size)))\n                sorted_idx = np.argsort(finite_F)\n                elite_idx = finite_idx[sorted_idx[:n_elite]]\n            else:\n                elite_idx = np.array([], dtype=int)\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            if elite_idx.size > 0 and self.rng.rand() < 0.92:\n                # bias towards better ones\n                ranks = np.arange(elite_idx.size)\n                weights = np.exp(-ranks / max(1.0, 0.2 * elite_idx.size))\n                weights = weights / np.sum(weights)\n                chosen = self.rng.choice(elite_idx, p=weights)\n                anchor = archive[int(chosen)]['x'].copy()\n            else:\n                anchor = archive[self.rng.randint(len(archive))]['x'].copy()\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                idxs = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in idxs:\n                    idxs.remove(exclude_idx)\n                if len(idxs) >= k:\n                    return self.rng.choice(idxs, size=k, replace=False)\n                else:\n                    return self.rng.choice(idxs, size=k, replace=True)\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = np.maximum(probs, 1e-8)\n            mixed = probs / np.sum(probs)\n\n            strat = self.rng.choice(4, p=mixed)\n            xcand = None\n\n            # prepare donors for DE-like if needed\n            donors = pick_distinct(3)\n            xa = archive[donors[0]]['x']\n            xb = archive[donors[1]]['x']\n            xc = archive[donors[2]]['x']\n\n            if strat == 0:\n                # Strategy 0: DE-style differential injection\n                Fdiff = 0.6 * (0.6 + 0.8 * self.gscale) * self.rng.rand()\n                diff = (xb - xc)\n                norm = np.maximum(np.linalg.norm(diff) / np.sqrt(self.dim), 1e-12)\n                diff_scaled = diff * (Fdiff * (scale_vec / norm))\n                sigma = 0.06 * (0.8 + self.gscale) * scale_vec\n                xcand = anchor + diff_scaled + self.rng.randn(self.dim) * sigma\n                if self.rng.rand() < 0.03:\n                    xcand += self._tempered_cauchy(scale_vec * (0.5 + self.gscale))\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                if elite_idx.size >= 2:\n                    E = np.array([archive[i]['x'] for i in elite_idx])\n                    # center at elite mean\n                    mu = np.mean(E, axis=0)\n                    C = np.cov(E, rowvar=False)\n                    # regularize\n                    C += np.eye(self.dim) * 1e-8 * np.mean(np.diag(C))\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                    except Exception:\n                        eigvals = np.abs(np.diag(C))\n                        eigvecs = np.eye(self.dim)\n                    # sample along top PCs\n                    order = np.argsort(eigvals)[::-1]\n                    eigvals = eigvals[order]\n                    eigvecs = eigvecs[:, order]\n                    kpc = max(1, min(self.dim, int(1 + np.round(self.dim * 0.15))))\n                    coeffs = self.rng.randn(kpc) * np.sqrt(np.maximum(eigvals[:kpc], 0.0))\n                    pc_step = (eigvecs[:, :kpc] @ coeffs)\n                    # blend towards anchor\n                    blend = 0.4 + 0.6 * self.rng.rand()\n                    xcand = blend * anchor + (1.0 - blend) * (mu + 0.8 * pc_step)\n                    # add small anisotropic noise\n                    xcand += self.rng.randn(self.dim) * 0.03 * scale_vec * (0.8 + self.gscale)\n                else:\n                    sigma = 0.2 * scale_vec * (0.8 + self.gscale)\n                    xcand = anchor + self.rng.randn(self.dim) * sigma\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                sigma = 0.45 * scale_vec * (0.8 + 0.9 * self.gscale)\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # ensure at least one coordinate changes\n                if np.allclose(xcand, anchor, atol=1e-16):\n                    idx = self.rng.randint(self.dim)\n                    xcand[idx] += 1e-6 + self.rng.randn() * sigma[idx]\n                # occasional mixed uniform per-dim to break local traps\n                if self.rng.rand() < 0.06:\n                    mask = self.rng.rand(self.dim) < 0.12\n                    if np.any(mask):\n                        xcand[mask] = self._uniform_array(lb[mask], ub[mask], n=1)\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                xcand = anchor + self._tempered_cauchy(scale_vec * (1.2 + 1.6 * self.gscale))\n                # sometimes add directed differential component\n                if self.rng.rand() < 0.2:\n                    directed = (xa - anchor) * (0.2 * self.rng.rand())\n                    xcand += directed\n\n            # occasional directed small jump to current best for refinement\n            if best_x is not None and self.rng.rand() < 0.07:\n                toward = (best_x - anchor)\n                # coordinate-wise influence\n                fac = 0.02 * (0.5 + self.rng.rand())\n                xcand = xcand + fac * toward * (self.rng.rand(self.dim) > 0.3)\n\n            # sanitize candidate for NaN/inf\n            bad = (~np.isfinite(xcand)) | (xcand < -1e300) | (xcand > 1e300)\n            if np.any(bad):\n                # replace bad coords by uniform draws\n                xr = self._uniform_array(lb, ub, n=1)\n                xcand[bad] = xr[bad]\n\n            # reflect to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub)\n\n            # evaluate candidate\n            fnew = safe_eval(xcand)\n            archive.append({'x': xcand.copy(), 'f': fnew, 'age': age_counter})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(fnew) and fnew < best_f:\n                best_f = fnew\n                best_x = xcand.copy()\n                improved = True\n                no_improve_steps = 0\n            else:\n                no_improve_steps += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            success = False\n            if finite_F.size > 0 and np.isfinite(fnew):\n                # recompute finite_F including this candidate\n                all_finite = np.sort(np.concatenate([finite_F, [fnew]]))\n                thresh = np.percentile(all_finite, 25.0)\n                if (fnew <= thresh) or improved:\n                    success = True\n            elif np.isfinite(fnew):\n                success = True\n\n            success_history.append(1 if success else 0)\n            if len(success_history) > 200:\n                success_history.pop(0)\n\n            # short-term adaptation of global scale\n            if len(success_history) >= 8:\n                sr = np.mean(success_history[-40:]) if len(success_history) >= 40 else np.mean(success_history)\n                if sr < 0.12:\n                    # low success -> increase exploration\n                    self.gscale = min(self.gscale * (1.0 + 0.12 * (0.12 - sr)), self.gscale_max)\n                elif sr > 0.4:\n                    # high success -> shrink to exploit\n                    self.gscale = max(self.gscale * (1.0 - 0.06 * (sr - 0.4)), self.gscale_min)\n                # small jitter\n                self.gscale = float(np.clip(self.gscale * (1.0 + 0.02 * (self.rng.rand() - 0.5)),\n                                            self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep best + some random + spaced)\n            if len(archive) > self.archive_max:\n                Farr = np.array([a['f'] for a in archive], dtype=float)\n                # keep best kbest\n                kbest = max(6, int(0.06 * self.archive_max))\n                best_idxs = np.argsort(Farr)[:kbest].tolist()\n                keep = set(best_idxs)\n                # keep some randoms\n                n_rand = max(3, int(0.08 * self.archive_max))\n                all_idxs = list(range(len(archive)))\n                remaining = [i for i in all_idxs if i not in keep]\n                if len(remaining) > 0:\n                    rand_keep = list(self.rng.choice(remaining, size=min(n_rand, len(remaining)), replace=False))\n                    keep.update(rand_keep)\n                # keep some spaced by age (older points)\n                ages = np.array([a['age'] for a in archive], dtype=float)\n                order_by_age = np.argsort(-ages)  # newest first\n                ii = 0\n                while len(keep) < self.archive_max and ii < len(order_by_age):\n                    keep.add(int(order_by_age[ii]))\n                    ii += 1\n                # compact\n                new_archive = [archive[i] for i in sorted(keep)]\n                archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if no_improve_steps > self.stagnation_window and evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random center)\n                cloud_size = min(40, max(6, int(0.05 * self.budget)))\n                center = best_x.copy() if best_x is not None and self.rng.rand() < 0.9 else self._uniform_array(lb, ub, n=1)\n                # create local cloud\n                for i in range(cloud_size):\n                    if evals >= self.budget:\n                        break\n                    radius = 0.4 * (1.0 + self.gscale) * scale_vec\n                    xr = center + self.rng.randn(self.dim) * radius * (0.5 + self.rng.rand())\n                    xr = self._reflect_bounds(xr, lb, ub)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        no_improve_steps = 0\n                # add some global randoms\n                for _ in range(min(6, (self.budget - evals))):\n                    xr = self._uniform_array(lb, ub, n=1)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': fr, 'age': age_counter})\n                    if np.isfinite(fr) and fr < best_f:\n                        best_f = fr\n                        best_x = xr.copy()\n                        no_improve_steps = 0\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.3, self.gscale_max)\n                no_improve_steps = 0\n\n            # optionally verbose occasionally\n            if self.verbose and (evals % max(1, self.budget // 10) == 0):\n                print(f\"[MG-ADS] evals={evals}/{self.budget}, best_f={best_f:.6g}, gscale={self.gscale:.4g}, archive_size={len(archive)}\")\n\n        # final best from finite archive if any\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda a: a['f'])\n            return float(best['f']), best['x'].copy()\n        else:\n            # all infinite: return a uniform fallback (if budget left will evaluate once)\n            fallback = self._uniform_array(lb, ub, n=1)\n            return float('inf'), fallback", "configspace": "", "generation": 0, "feedback": "In the code, line 347, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 10 input values to the 1 output values where the mask is true\nOn line: xcand[mask] = self._uniform_array(lb[mask], ub[mask], n=1)", "error": "In the code, line 347, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 10 input values to the 1 output values where the mask is true\nOn line: xcand[mask] = self._uniform_array(lb[mask], ub[mask], n=1)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8ed531d8-5954-4813-a6b1-2c3ea2cad185", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 gscale_init=0.25, gscale_min=1e-4, gscale_max=4.0,\n                 adapt_window=50, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.gscale = float(gscale_init)\n        self.gscale_min = float(gscale_min)\n        self.gscale_max = float(gscale_max)\n        self.adapt_window = int(adapt_window)\n        self.verbose = bool(verbose)\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.35, 0.15, 0.40, 0.10], dtype=float)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try several common ways to get bounds, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        if hasattr(func, \"bounds\"):\n            try:\n                b = func.bounds\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n            except Exception:\n                lb = None\n        if lb is None:\n            # attributes like lb, ub\n            if hasattr(func, \"lb\") and hasattr(func, \"ub\"):\n                try:\n                    lb = np.asarray(func.lb, dtype=float)\n                    ub = np.asarray(func.ub, dtype=float)\n                except Exception:\n                    lb = None\n        # fallback to scalar properties\n        if lb is None:\n            lb = np.repeat(-5.0, self.dim)\n            ub = np.repeat(5.0, self.dim)\n        else:\n            # ensure shapes\n            if lb.size == 1:\n                lb = np.repeat(lb.item(), self.dim)\n            if ub.size == 1:\n                ub = np.repeat(ub.item(), self.dim)\n            lb = lb.reshape(self.dim)\n            ub = ub.reshape(self.dim)\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # mirror-reflection for any coordinate outside bounds; repeat a few times\n        x = x.copy().astype(float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            # if all inside, break\n            if np.all((x >= lb) & (x <= ub)):\n                break\n        # final clamp to be safe\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb_a = np.repeat(lb.item(), self.dim)\n        else:\n            lb_a = lb.reshape(self.dim)\n        if ub.size == 1:\n            ub_a = np.repeat(ub.item(), self.dim)\n        else:\n            ub_a = ub.reshape(self.dim)\n        if n == 1:\n            return lb_a + self.rng.rand(self.dim) * (ub_a - lb_a)\n        else:\n            out = lb_a + self.rng.rand(n, self.dim) * (ub_a - lb_a)\n            return out\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplicative factor per-coordinate\n        heavy = (self.rng.rand(self.dim) < 0.02).astype(float)\n        heavy_mul = 1.0 + heavy * (10.0 * self.rng.rand(self.dim))\n        # small gaussian jitter\n        jitter = 0.02 * self.rng.randn(self.dim)\n        out = c * scale_vec * heavy_mul + jitter * scale_vec\n        # cap extremes to avoid overflow\n        cap = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        out = np.sign(out) * np.minimum(np.abs(out), cap)\n        return out\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        # read bounds\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        # archive: list of dicts {'x':..., 'f':..., 'age':...}\n        archive = []\n\n        evals = 0\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            return f\n\n        # ---- initialization: LHS-like + a few pure randoms ----\n        n_init = int(min(max(10, int(0.08 * self.budget)), 150))\n        # Latin-hypercube like: per-dim strata permutations\n        n_init = min(n_init, max(1, self.budget))\n        if n_init > 0:\n            # create matrix (n_init, dim) by stratum sampling\n            perm = np.zeros((self.dim, n_init), dtype=float)\n            for d in range(self.dim):\n                idx = np.arange(n_init)\n                self.rng.shuffle(idx)\n                perm[d, :] = idx\n            # stratified within each slice\n            Xinit = np.zeros((n_init, self.dim), dtype=float)\n            for i in range(n_init):\n                a = (perm[:, i] + self.rng.rand(self.dim)) / float(n_init)\n                Xinit[i, :] = lb + a * span\n            for i in range(n_init):\n                if evals >= self.budget:\n                    break\n                x = Xinit[i, :]\n                f = safe_eval(x)\n                archive.append({'x': x.copy(), 'f': f, 'age': 0})\n        # a few uniform extra draws\n        while evals < min(self.budget, max(2, int(0.01 * self.budget)) + n_init):\n            x = self._uniform_array(lb, ub, n=1)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': f, 'age': 0})\n\n        # If nothing evaluated (e.g. budget==0) return fallback\n        if len(archive) == 0:\n            fallback_x = self._uniform_array(lb, ub, n=1)\n            return float(np.inf), fallback_x\n\n        # initial best\n        finite_fs = [a['f'] for a in archive]\n        best_idx = int(np.argmin(finite_fs))\n        best = {'x': archive[best_idx]['x'].copy(), 'f': float(archive[best_idx]['f'])}\n        last_improve_eval = evals\n\n        # history for adaptation\n        success_history = []\n        max_success_hist = max(20, self.adapt_window)\n        adapt_window = max(8, self.adapt_window)\n\n        # probabilities\n        probs = self.base_probs.copy()\n\n        # main loop\n        stagnation_counter = 0\n        stagnation_threshold = max(40, int(0.06 * self.budget))\n\n        while evals < self.budget:\n            # convert archive arrays\n            X = np.vstack([a['x'] for a in archive])\n            Fs = np.array([a['f'] for a in archive], dtype=float)\n            finite_mask = np.isfinite(Fs)\n            if not np.any(finite_mask):\n                # no finite values -> sample uniform\n                xcand = self._uniform_array(lb, ub, n=1)\n                fcand = safe_eval(xcand)\n                archive.append({'x': xcand.copy(), 'f': fcand, 'age': 0})\n                if fcand < best['f']:\n                    best = {'x': xcand.copy(), 'f': float(fcand)}\n                    last_improve_eval = evals\n                continue\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            scales = np.zeros(self.dim, dtype=float)\n            for d in range(self.dim):\n                col = X[:, d]\n                q75, q25 = np.percentile(col, [75, 25])\n                iqr = q75 - q25\n                if iqr > 1e-9:\n                    scales[d] = max(iqr / 1.349, 1e-12)\n                else:\n                    stdd = np.std(col)\n                    if stdd > 1e-12:\n                        scales[d] = stdd\n                    else:\n                        scales[d] = 0.05 * span[d]  # fallback to fraction of span\n\n            # determine elites (by f)\n            sorted_idx = np.argsort(Fs)\n            nelite = max(2, int(max(2, 0.15 * len(archive))))\n            elite_idx = sorted_idx[:nelite].tolist()\n            # pick anchor among elites with exponential bias (better elites preferred)\n            weights = np.exp(-np.linspace(0, 2.0, len(elite_idx)))\n            weights = weights / np.sum(weights)\n            pick = self.rng.choice(len(elite_idx), p=weights)\n            anchor = archive[elite_idx[pick]]['x'].copy()\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                ids = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in ids:\n                    ids.remove(exclude_idx)\n                if len(ids) >= k:\n                    return list(self.rng.choice(ids, size=k, replace=False))\n                else:\n                    # allow duplicates if not enough\n                    return list(self.rng.choice(ids, size=k, replace=True))\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            probs[3] *= (1.0 + 0.35 * min(2.0, self.gscale))  # boost Cauchy when gscale large\n            probs = np.maximum(probs, 1e-6)\n            probs = probs / np.sum(probs)\n\n            # pick a strategy\n            strategy = self.rng.choice(4, p=probs)\n\n            # prepare donors for DE-like if needed\n            donors = pick_distinct(3, exclude_idx=None)\n            xc = archive[donors[2]]['x']\n\n            # compute a per-dim normalized scale for moves\n            norm_scale = np.maximum(scales, 1e-12)\n\n            # Strategy implementations\n            if strategy == 0:\n                # Strategy 0: DE-style differential injection\n                x1 = archive[donors[0]]['x']\n                x2 = archive[donors[1]]['x']\n                F = 0.6 * (0.5 + 0.5 * self.gscale)  # differential factor\n                diff = (x1 - x2)\n                # normalize by norm_scale to keep coordinates comparable\n                diff_scaled = diff * (F * norm_scale / (norm_scale + 1e-12))\n                noise = (self.rng.randn(self.dim) * (0.06 + 0.5 * self.rng.rand(self.dim))) * norm_scale * self.gscale\n                # occasionally inject tempered cauchy\n                if self.rng.rand() < 0.06:\n                    cauchy_part = self._tempered_cauchy(norm_scale) * 0.4 * self.gscale\n                else:\n                    cauchy_part = 0.0\n                xcand = anchor + diff_scaled + noise + cauchy_part\n            elif strategy == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                if len(elite_idx) >= 2:\n                    elite_X = X[elite_idx, :]\n                    mean_elite = elite_X.mean(axis=0)\n                    C = np.cov((elite_X - mean_elite).T)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        # sort descending\n                        order = np.argsort(-eigvals)\n                        eigvals = eigvals[order]\n                        eigvecs = eigvecs[:, order]\n                        # sample along top components\n                        kpc = min(self.dim, max(1, int(0.3 * self.dim)))\n                        coeffs = np.array([self.rng.randn() * np.sqrt(max(0.0, eigvals[i])) for i in range(kpc)])\n                        step = eigvecs[:, :kpc].dot(coeffs)\n                        step *= (0.8 + 0.6 * self.gscale)\n                        # blend slightly towards anchor\n                        xcand = anchor + 0.9 * step + 0.1 * (mean_elite - anchor) * self.rng.rand()\n                    except Exception:\n                        # fallback gaussian around anchor\n                        xcand = anchor + self.rng.randn(self.dim) * (0.6 * norm_scale * self.gscale)\n                else:\n                    xcand = anchor + self.rng.randn(self.dim) * (0.6 * norm_scale * self.gscale)\n            elif strategy == 2:\n                # Strategy 2: Local anisotropic gaussian\n                sigma = 0.8 * norm_scale * max(0.5, self.gscale)\n                # anisotropic multipliers\n                multipliers = 0.4 + self.rng.rand(self.dim) * 1.6\n                sigma = sigma * multipliers\n                xcand = anchor + self.rng.randn(self.dim) * sigma\n                # ensure at least one coordinate changes\n                if np.allclose(xcand, anchor):\n                    j = self.rng.randint(self.dim)\n                    xcand[j] += sigma[j] * (0.5 + self.rng.rand())\n                # occasional mixed uniform per-dim\n                if self.rng.rand() < 0.03:\n                    mask = self.rng.rand(self.dim) < 0.15\n                    xcand[mask] = self._uniform_array(lb, ub, n=1)[mask]\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                scale_vec = norm_scale * (0.8 + 0.6 * self.gscale)\n                tc = self._tempered_cauchy(scale_vec)\n                xcand = anchor + tc\n                # sometimes add directed differential component\n                if self.rng.rand() < 0.3:\n                    d0 = archive[donors[0]]['x']\n                    d1 = archive[donors[1]]['x']\n                    xcand += 0.2 * (d0 - d1)\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.05:\n                dir_to_best = best['x'] - anchor\n                xcand += 0.12 * (dir_to_best * (self.rng.rand(self.dim) * 0.8 + 0.2))\n\n            # sanitize candidate: replace nan/inf coordinates by uniform draws\n            bad = ~np.isfinite(xcand)\n            if np.any(bad):\n                xcand[bad] = self._uniform_array(lb, ub, n=1)[bad]\n\n            # reflect to bounds\n            xcand = self._reflect_bounds(xcand, lb, ub, niter=4)\n\n            # evaluate candidate\n            fcand = safe_eval(xcand)\n            archive.append({'x': xcand.copy(), 'f': fcand, 'age': 0})\n\n            # increment ages of others\n            for a in archive[:-1]:\n                a['age'] += 1\n\n            # update best if improved\n            improved = False\n            if np.isfinite(fcand) and fcand < best['f']:\n                best = {'x': xcand.copy(), 'f': float(fcand)}\n                last_improve_eval = evals\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs = np.array([a['f'] for a in archive if np.isfinite(a['f'])])\n            if finite_fs.size > 0:\n                thresh = np.percentile(finite_fs, 25)\n                is_success = (np.isfinite(fcand) and fcand <= thresh) or improved\n            else:\n                is_success = improved\n\n            success_history.append(bool(is_success))\n            if len(success_history) > max_success_hist:\n                success_history.pop(0)\n\n            # short-term adaptation of global scale\n            if len(success_history) >= max(4, int(0.25 * adapt_window)):\n                succ_rate = float(np.mean(success_history[-adapt_window:]))\n                # if success rate too low -> increase exploration\n                if succ_rate < 0.18:\n                    self.gscale *= (1.0 + 0.12 + 0.25 * (0.18 - succ_rate))\n                elif succ_rate > 0.55:\n                    self.gscale *= (1.0 - 0.08 * min(1.0, (succ_rate - 0.55) / 0.45))\n                # small jitter\n                self.gscale *= (1.0 + 0.02 * (self.rng.rand() - 0.5))\n                # clamp\n                self.gscale = float(np.clip(self.gscale, self.gscale_min, self.gscale_max))\n\n            # prune archive when too large (keep best + some random + spaced)\n            max_archive = int(min(300, max(40, 4 * self.dim + 40)))\n            if len(archive) > max_archive:\n                # keep best kbest\n                kbest = min(20 + self.dim, max(5, int(0.08 * max_archive)))\n                sorted_idx = np.argsort([a['f'] for a in archive])\n                keep = set(sorted_idx[:kbest].tolist())\n                # keep some randoms\n                n_rand = int(0.12 * max_archive)\n                all_ids = set(range(len(archive)))\n                remaining = list(all_ids - keep)\n                if remaining:\n                    keep_rand = self.rng.choice(remaining, size=min(len(remaining), n_rand), replace=False).tolist()\n                    keep.update(keep_rand)\n                # keep some spaced by age\n                remaining = list(all_ids - keep)\n                remaining_ages = sorted(remaining, key=lambda i: archive[i]['age'], reverse=True)\n                n_age = int(0.10 * max_archive)\n                keep.update(remaining_ages[:n_age])\n                # compact\n                new_archive = [archive[i] for i in sorted(list(keep))]\n                # fill remaining randomly if needed\n                if len(new_archive) < max_archive:\n                    need = max_archive - len(new_archive)\n                    candidates = list(set(range(len(archive))) - set(sorted(list(keep))))\n                    if candidates:\n                        pick_ids = self.rng.choice(candidates, size=min(len(candidates), need), replace=False)\n                        for i in pick_ids:\n                            new_archive.append(archive[i])\n                archive = new_archive\n                # reassign\n                # limit ages to avoid overflow\n                for a in archive:\n                    a['age'] = int(min(a['age'], 1000000))\n\n            # stagnation detection & micro-restarts\n            if (evals - last_improve_eval) > stagnation_threshold and evals < self.budget:\n                # perform micro-restart: local cloud around current best (or random center)\n                cloud_size = min(60, max(8, int(0.04 * self.budget)))\n                center = best['x'].copy() if self.rng.rand() < 0.8 else self._uniform_array(lb, ub, n=1)\n                # make cloud spread proportional to current scales and gscale\n                cloud_sigma = np.maximum(scales * (1.5 + 1.5 * self.gscale), 1e-12)\n                # evaluate some cloud points (but not exceeding budget)\n                n_eval_cloud = min(cloud_size, self.budget - evals)\n                if n_eval_cloud > 0:\n                    for _ in range(n_eval_cloud):\n                        cand = center + self.rng.randn(self.dim) * cloud_sigma * (0.6 + self.rng.rand() * 1.8)\n                        cand = self._reflect_bounds(cand, lb, ub)\n                        fc = safe_eval(cand)\n                        archive.append({'x': cand.copy(), 'f': fc, 'age': 0})\n                        if np.isfinite(fc) and fc < best['f']:\n                            best = {'x': cand.copy(), 'f': float(fc)}\n                            last_improve_eval = evals\n                # also inject some global randoms\n                n_randoms = min(8, self.budget - evals)\n                for _ in range(n_randoms):\n                    cand = self._uniform_array(lb, ub, n=1)\n                    fc = safe_eval(cand)\n                    archive.append({'x': cand.copy(), 'f': fc, 'age': 0})\n                    if np.isfinite(fc) and fc < best['f']:\n                        best = {'x': cand.copy(), 'f': float(fc)}\n                        last_improve_eval = evals\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = float(min(self.gscale_max, self.gscale * (1.0 + 0.45 * (1.0 + self.rng.rand()))))\n                success_history = []\n                last_improve_eval = evals\n                stagnation_counter = 0\n\n            # optionally verbose occasionally\n            if self.verbose and (evals % max(1, int(0.05 * self.budget)) == 0):\n                print(f\"[MG-ADS] evals={evals}/{self.budget}, best_f={best['f']:.6g}, gscale={self.gscale:.4g}, archive={len(archive)}\")\n\n        # final best from finite archive if any\n        finite_entries = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_entries) > 0:\n            best_entry = min(finite_entries, key=lambda a: a['f'])\n            return float(best_entry['f']), best_entry['x'].copy()\n        # all infinite: return a uniform fallback (if budget left will evaluate once)\n        return float(np.inf), self._uniform_array(lb, ub, n=1)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d1108205-17df-418b-a265-22af1091d384", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.verbose = verbose\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.30, 0.20, 0.40, 0.10], dtype=float)\n        self.base_probs /= self.base_probs.sum()\n\n        # some internal hyperparameters\n        self.max_archive = max(200 + 20 * self.dim, 400)\n        self.k_elite_ratio = 0.20\n        self.gscale = 0.2  # global scale factor, adapted online\n        self.gscale_min = 1e-4\n        self.gscale_max = 10.0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to infer bounds from function attributes, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # common attributes pattern\n        for attr in (\"bounds\", \"box\", \"domain\"):\n            if hasattr(func, attr):\n                b = getattr(func, attr)\n                try:\n                    lb = np.array(b.lb if hasattr(b, \"lb\") else b[0])\n                    ub = np.array(b.ub if hasattr(b, \"ub\") else b[1])\n                except Exception:\n                    try:\n                        b = np.asarray(b)\n                        if b.shape == (2, self.dim):\n                            lb, ub = b[0], b[1]\n                    except Exception:\n                        pass\n        # direct attributes lb/ub\n        if lb is None and hasattr(func, \"lb\"):\n            lb = np.array(func.lb)\n        if ub is None and hasattr(func, \"ub\"):\n            ub = np.array(func.ub)\n        # fallback to scalar properties\n        if lb is None or ub is None:\n            try:\n                lb_s = getattr(func, \"lower\", None)\n                ub_s = getattr(func, \"upper\", None)\n                if lb_s is not None and ub_s is not None:\n                    lb = np.asarray(lb_s)\n                    ub = np.asarray(ub_s)\n            except Exception:\n                pass\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        # ensure shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # mirror-reflection for any coordinate outside bounds; repeat a few times\n        x = x.copy()\n        for _ in range(niter):\n            below = x < lb\n            x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            x[above] = ub[above] - (x[above] - ub[above])\n            if np.all((x >= lb) & (x <= ub)):\n                break\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        shape = (n, self.dim) if n > 1 else (self.dim,)\n        draws = self.rng.rand(*(shape)) * (ub - lb) + lb\n        return draws\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF\n        # u in (0,1), cauchy = scale * tan(pi*(u-0.5))\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplicative factor per-coordinate\n        heavy = self.rng.rand(self.dim) < 0.02\n        heavy_factor = np.where(heavy, self.rng.lognormal(mean=0.0, sigma=1.0, size=self.dim), 1.0)\n        vals = scale_vec * c * heavy_factor\n        # small gaussian jitter to break symmetry\n        vals += 0.01 * scale_vec * self.rng.randn(self.dim)\n        # cap extremes to avoid overflow\n        max_abs = cap_multiplier * scale_vec\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator wrapper tracking budget usage\n        evals = 0\n        budget = self.budget\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return np.inf\n            # ensure 1D array\n            x = np.asarray(x, dtype=float)\n            if x.ndim == 2 and x.shape[0] == 1:\n                x = x[0]\n            # Function may expect shape (dim,), handle as is\n            try:\n                y = func(x)\n            except Exception:\n                # attempt to coerce to float\n                y = float(func(x))\n            evals += 1\n            return y\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        # choose initial population size reasonably small but > dim\n        n_init = min(max(10, 3 * self.dim), max(5, budget // 10))\n        # Latin-hypercube-like\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        X_init = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            # sample within strata\n            u = self.rng.rand(n_init)\n            X_init[:, d] = (strata[perm] + u * (1.0 / n_init))[:-0]  # shape ok\n        # map to bounds\n        X_init = lb + X_init * (ub - lb)\n        # a few extra uniform\n        n_extra = min(5, budget - n_init)\n        if n_extra > 0:\n            X_extra = self._uniform_array(lb, ub, n_extra)\n            X = np.vstack([X_init, X_extra])\n        else:\n            X = X_init\n\n        archive = []\n        # evaluate initial population until budget exhausted\n        for xi in X:\n            if evals >= budget:\n                break\n            f = safe_eval(xi)\n            archive.append({'x': xi.copy(), 'f': float(f), 'age': 0})\n        # If nothing evaluated (e.g., budget==0) return fallback\n        if len(archive) == 0:\n            # return uniform fallback without evaluating\n            x_fallback = (lb + ub) / 2.0\n            return np.inf, x_fallback\n\n        # initial best\n        archive = sorted(archive, key=lambda e: e['f'])\n        best = archive[0]['f']\n        xbest = archive[0]['x'].copy()\n\n        # history for adaptation\n        recent_successes = []\n        win_len = 30\n\n        # main loop\n        iter_no = 0\n        stagn_counter = 0\n        last_improve_iter = 0\n\n        while evals < budget:\n            iter_no += 1\n\n            # convert archive arrays\n            Xa = np.array([e['x'] for e in archive])\n            Fa = np.array([e['f'] for e in archive])\n            ages = np.array([e['age'] for e in archive])\n\n            finite_mask = np.isfinite(Fa)\n            if not np.any(finite_mask):\n                # sample uniform candidate if archive has no finite\n                cand = self._uniform_array(lb, ub)\n            else:\n                # compute per-dim robust scale estimate (IQR fallback to std)\n                Xf = Xa[finite_mask]\n                if Xf.shape[0] >= 2:\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    scale = (q75 - q25) / 1.349  # approximate robust std\n                    small = scale < 1e-12\n                    if np.any(small):\n                        sd = np.std(Xf, axis=0)\n                        scale[small] = sd[small]\n                else:\n                    scale = np.std(Xa, axis=0)\n                # fallback to broad scale if still degenerate\n                fallback = 0.2 * (ub - lb)\n                scale = np.maximum(scale, fallback * 1e-3)\n                scale = np.maximum(scale, 1e-12)\n\n                # determine elites (by f)\n                n_elite = max(3, int(np.ceil(self.k_elite_ratio * len(archive))))\n                sorted_idx = np.argsort(Fa)\n                elite_idx = sorted_idx[:n_elite]\n                elites = Xa[elite_idx]\n                elites_f = Fa[elite_idx]\n\n                # pick anchor among elites with exponential bias (better elites preferred)\n                ranks = np.arange(n_elite)\n                weights = np.exp(-ranks / max(1.0, n_elite / 3.0))\n                weights = weights / weights.sum()\n                pick = self.rng.choice(n_elite, p=weights)\n                anchor = elites[pick].copy()\n\n                # helper to pick distinct donors for DE\n                def pick_distinct(k, exclude_idx=None):\n                    idxs = list(range(len(archive)))\n                    if exclude_idx is not None and exclude_idx in idxs:\n                        idxs.remove(exclude_idx)\n                    if len(idxs) >= k:\n                        return self.rng.choice(idxs, size=k, replace=False)\n                    else:\n                        # allow duplicates if not enough\n                        return self.rng.choice(idxs, size=k, replace=True)\n\n                # adapt probabilities slightly with gscale (favor Cauchy when large)\n                probs = self.base_probs.copy()\n                probs[3] *= (1.0 + (self.gscale / (self.gscale + 0.5)))  # boost cauchy slightly\n                probs /= probs.sum()\n                strat = self.rng.choice(4, p=probs)\n\n                # prepare donors for DE-like if needed\n                donor_vals = None\n                if strat == 0 or self.rng.rand() < 0.05:  # DE or occasional differential injection\n                    ids = pick_distinct(3)\n                    xr1 = Xa[ids[0]]\n                    xr2 = Xa[ids[1]]\n                    xr3 = Xa[ids[2]]\n                    donor_vals = (xr1, xr2, xr3)\n\n                # compute a per-dim normalized scale for moves\n                # norm_scale to keep coordinates comparable\n                norm_scale = np.maximum(scale, 1e-12)\n\n                # Strategy implementations\n                if strat == 0:\n                    # Strategy 0: DE-style differential injection\n                    F = 0.6 * self.gscale\n                    r = anchor.copy()\n                    if donor_vals is not None:\n                        xr1, xr2, xr3 = donor_vals\n                        # classical DE/rand/1-like mixed with anchor\n                        diff = xr1 - xr2\n                        norm = np.linalg.norm(diff / (norm_scale + 1e-12))\n                        if norm > 1e-12:\n                            diff = diff / (norm + 1e-12) * (np.mean(norm_scale) * np.clip(norm, 0.5, 10.0))\n                        r = anchor + F * diff + 0.01 * norm_scale * self.rng.randn(self.dim)\n                        # occasionally inject tempered cauchy to some coords\n                        if self.rng.rand() < 0.07:\n                            r += self._tempered_cauchy(norm_scale * self.gscale)\n                    else:\n                        r = anchor + 0.05 * norm_scale * self.rng.randn(self.dim)\n                    cand = r\n\n                elif strat == 1:\n                    # Strategy 1: PCA-guided elite perturbation\n                    # Use elites to compute principal directions\n                    E = elites - np.mean(elites, axis=0)\n                    # If not enough variability, fallback to gaussian\n                    try:\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                    except Exception:\n                        U = None\n                    if E.shape[0] >= 2 and np.any(S > 1e-12):\n                        # sample along top components\n                        kcomp = min(max(1, int(min(self.dim, 1 + np.sum(S > 1e-8)))), min(self.dim, 3 + int(self.dim / 4)))\n                        pcs = Vt[:kcomp]\n                        coeffs = self.rng.randn(kcomp) * (S[:kcomp] / (S[:kcomp].mean() + 1e-12)) * self.gscale\n                        r = anchor.copy()\n                        r += (coeffs @ pcs)  # linear combination along principal components\n                        # blend slightly towards anchor mean of elites\n                        r = 0.85 * r + 0.15 * (np.mean(elites, axis=0))\n                        # small gaussian jitter\n                        r += 0.02 * norm_scale * self.rng.randn(self.dim)\n                    else:\n                        r = anchor + 0.05 * norm_scale * self.rng.randn(self.dim)\n                    cand = r\n\n                elif strat == 2:\n                    # Strategy 2: Local anisotropic gaussian\n                    # anisotropic multipliers: log-uniform multipliers per-dim\n                    logs = self.rng.randn(self.dim) * 0.6\n                    multipliers = np.exp(logs)\n                    sig = norm_scale * self.gscale * multipliers * 0.3\n                    r = anchor + sig * self.rng.randn(self.dim)\n                    # ensure at least one coordinate changes\n                    if np.all(np.abs(r - anchor) < 1e-12):\n                        j = self.rng.randint(self.dim)\n                        r[j] += sig[j] * (0.5 + self.rng.rand())\n                    # occasional mixed uniform per-dim\n                    if self.rng.rand() < 0.03:\n                        mask = self.rng.rand(self.dim) < 0.1\n                        if np.any(mask):\n                            r[mask] = self._uniform_array(lb, ub)[mask]\n                    cand = r\n\n                else:\n                    # Strategy 3: tempered Cauchy global escape\n                    r = anchor.copy()\n                    r += self._tempered_cauchy(norm_scale * self.gscale)\n                    # sometimes add directed differential component\n                    if donor_vals is not None and self.rng.rand() < 0.25:\n                        xr1, xr2, xr3 = donor_vals\n                        r += 0.1 * (xr1 - xr2)\n                    cand = r\n\n                # occasional directed small jump to current best for refinement\n                if self.rng.rand() < 0.02 and len(archive) > 0:\n                    cand = 0.8 * cand + 0.2 * xbest\n\n                # sanitize candidate: replace nan/inf coordinates by uniform draws\n                cand = np.asarray(cand, dtype=float)\n                mask_bad = ~np.isfinite(cand)\n                if np.any(mask_bad):\n                    cand[mask_bad] = self._uniform_array(lb[mask_bad], ub[mask_bad]) if mask_bad.size == 1 else (lb + self.rng.rand(np.sum(mask_bad)) * (ub - lb))[mask_bad]\n\n                # reflect to bounds\n                cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            # increment ages of others\n            for e in archive:\n                e['age'] += 1\n\n            # update archive\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best:\n                best = float(f_cand)\n                xbest = cand.copy()\n                improved = True\n                last_improve_iter = iter_no\n                stagn_counter = 0\n            else:\n                stagn_counter += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n            if finite_fs.size > 0:\n                try:\n                    q25 = np.percentile(finite_fs, 25)\n                except Exception:\n                    q25 = np.min(finite_fs)\n            else:\n                q25 = np.inf\n            success = (np.isfinite(f_cand) and (f_cand <= q25)) or improved\n            recent_successes.append(1 if success else 0)\n            if len(recent_successes) > win_len:\n                recent_successes.pop(0)\n\n            # short-term adaptation of global scale\n            if len(recent_successes) >= win_len:\n                sr = np.mean(recent_successes)\n                if sr < 0.2:\n                    # low success -> increase exploration\n                    self.gscale = min(self.gscale * (1.0 + 0.12), self.gscale_max)\n                else:\n                    # high success -> tighten\n                    self.gscale = max(self.gscale * (0.995), self.gscale_min)\n            else:\n                # small jitter to avoid lock\n                self.gscale *= (1.0 + (self.rng.randn() * 0.001))\n                self.gscale = np.clip(self.gscale, self.gscale_min, self.gscale_max)\n\n            # prune archive when too large (keep best + some random + spaced)\n            if len(archive) > self.max_archive:\n                kbest = max(10, 2 * self.dim)\n                keep = []\n                # keep best\n                archive = sorted(archive, key=lambda e: e['f'])\n                keep.extend(archive[:kbest])\n                # keep some randoms\n                nrand = min(40, len(archive) - kbest)\n                if nrand > 0:\n                    rest_idx = np.arange(kbest, len(archive))\n                    sel = self.rng.choice(rest_idx, size=nrand, replace=False)\n                    keep.extend([archive[i] for i in sel])\n                # keep some spaced by age\n                ages_rest = np.array([e['age'] for e in archive[kbest:]])\n                if ages_rest.size > 0:\n                    sel_age = np.argsort(-ages_rest)[:min(40, ages_rest.size)]\n                    for s in sel_age:\n                        keep.append(archive[kbest + s])\n                # compact and deduplicate by proximity (simple)\n                unique = []\n                seen = []\n                for e in keep:\n                    key = tuple(np.round(e['x'], 8))\n                    if key not in seen:\n                        unique.append(e)\n                        seen.append(key)\n                archive = unique\n                # fill remaining randomly if needed (don't evaluate, just keep some points)\n                archive = archive[: self.max_archive]\n\n            # stagnation detection & micro-restarts\n            # perform micro-restart if long stagnation & budget left\n            if (iter_no - last_improve_iter) > max(200, 20 * self.dim) and evals < budget:\n                # micro-restart: local cloud around current best (or random center)\n                if xbest is None or not np.isfinite(best):\n                    center = self._uniform_array(lb, ub)\n                else:\n                    center = xbest.copy()\n                # make cloud spread proportional to current scales and gscale\n                cloud_size = min(40, max(6, budget - evals))\n                cloud_sigma = np.maximum(np.std(Xa, axis=0), 0.1 * (ub - lb)) * (1.0 + self.gscale)\n                ncloud = min(cloud_size, max(2, (budget - evals) // 3))\n                if ncloud <= 0:\n                    pass\n                else:\n                    for _ in range(ncloud):\n                        if evals >= budget:\n                            break\n                        p = center + cloud_sigma * (0.5 + self.rng.rand(self.dim)) * (self.rng.randn(self.dim))\n                        p = self._reflect_bounds(p, lb, ub)\n                        f_p = safe_eval(p)\n                        archive.append({'x': p.copy(), 'f': float(f_p), 'age': 0})\n                        if np.isfinite(f_p) and f_p < best:\n                            best = float(f_p)\n                            xbest = p.copy()\n                            last_improve_iter = iter_no\n                # also inject some global randoms\n                nrand = min(10, budget - evals)\n                for _ in range(nrand):\n                    if evals >= budget:\n                        break\n                    rx = self._uniform_array(lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < best:\n                        best = float(fr)\n                        xbest = rx.copy()\n                        last_improve_iter = iter_no\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.5, self.gscale_max)\n                recent_successes = []\n                last_improve_iter = iter_no\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or evals == budget):\n                finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n                print(f\"iter {iter_no}, evals {evals}/{budget}, archive {len(archive)}, best {best:.4e}, gscale {self.gscale:.4e}, median {np.median(finite_fs) if finite_fs.size>0 else np.nan}\")\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries = sorted(finite_entries, key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        else:\n            # all infinite: return a uniform fallback (if budget left will evaluate once)\n            x_fallback = self._uniform_array(lb, ub)\n            return np.inf, x_fallback", "configspace": "", "generation": 0, "feedback": "In the code, line 162, in __call__, the following error occurred:\nValueError: could not broadcast input array from shape (0,) into shape (10,)\nOn line: X_init[:, d] = (strata[perm] + u * (1.0 / n_init))[:-0]  # shape ok", "error": "In the code, line 162, in __call__, the following error occurred:\nValueError: could not broadcast input array from shape (0,) into shape (10,)\nOn line: X_init[:, d] = (strata[perm] + u * (1.0 / n_init))[:-0]  # shape ok", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "88d34d8c-366c-427d-97c4-9ef110fbdd2d", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=True)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.verbose = verbose\n        self.rng = np.random.RandomState(seed)\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.30, 0.20, 0.40, 0.10], dtype=float)\n\n        # some internal hyperparameters\n        self.max_archive = max(200 + 20 * self.dim, 400)\n        self.k_elite_ratio = 0.20\n        self.gscale = 0.2  # global scale factor, adapted online\n        self.gscale_min = 1e-4\n        self.gscale_max = 10.0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to infer bounds from function attributes, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # common attributes pattern\n        for attr in (\"bounds\", \"box\", \"domain\"):\n            if hasattr(func, attr):\n                try:\n                    b = getattr(func, attr)\n                    b = np.asarray(b)\n                    if b.ndim == 2 and b.shape[0] >= 2:\n                        lb, ub = np.asarray(b[0], dtype=float), np.asarray(b[1], dtype=float)\n                        break\n                except Exception:\n                    pass\n        # direct attributes lb/ub\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n        # fallback to scalar properties\n        if lb is None or ub is None:\n            try:\n                lb_s = getattr(func, \"lower\", None)\n                ub_s = getattr(func, \"upper\", None)\n                if lb_s is not None and ub_s is not None:\n                    lb = np.asarray(lb_s, dtype=float)\n                    ub = np.asarray(ub_s, dtype=float)\n            except Exception:\n                pass\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # mirror-reflection for any coordinate outside bounds; repeat a few times\n        x = x.copy()\n        for _ in range(niter):\n            below = x < lb\n            above = x > ub\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if np.all((x >= lb) & (x <= ub)):\n                break\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            shape = (n, self.dim)\n            draws = self.rng.rand(*shape) * (ub - lb) + lb\n            return draws\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF\n        # u in (0,1), cauchy = scale * tan(pi*(u-0.5))\n        scale_vec = np.maximum(np.asarray(scale_vec, dtype=float), 1e-12)\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplicative factor per-coordinate\n        heavy = self.rng.rand(self.dim) < 0.02\n        heavy_factor = np.where(heavy, self.rng.lognormal(mean=0.0, sigma=1.0, size=self.dim), 1.0)\n        vals = scale_vec * c * heavy_factor\n        # small gaussian jitter to break symmetry\n        vals += 0.01 * scale_vec * self.rng.randn(self.dim)\n        # cap extremes to avoid overflow\n        max_abs = cap_multiplier * scale_vec\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluator wrapper tracking budget usage\n        evals = 0\n        budget = self.budget\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return np.inf\n            # ensure 1D array\n            x = np.asarray(x, dtype=float)\n            if x.ndim == 2 and x.shape[0] == 1:\n                x = x[0]\n            try:\n                y = func(x)\n            except Exception:\n                # attempt to coerce to float or return inf\n                try:\n                    y = float(func(x))\n                except Exception:\n                    y = np.inf\n            evals += 1\n            return y\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        n_init = int(min(max(10, 3 * self.dim), max(5, budget // 10)))\n        # Latin-hypercube-like sampling\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        X_init = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            u = self.rng.rand(n_init)\n            # points in (i/n, (i+1)/n)\n            X_init[:, d] = (strata[perm][:n_init] + u * (1.0 / n_init))[:n_init]\n        # map to bounds\n        X_init = lb + X_init * (ub - lb)\n\n        # a few extra uniform\n        n_extra = min(5, max(0, budget - n_init))\n        X_extra = self._uniform_array(lb, ub, n_extra) if n_extra > 0 else np.empty((0, self.dim))\n        if n_extra > 0:\n            X = np.vstack([X_init, X_extra])\n        else:\n            X = X_init\n\n        archive = []\n        # evaluate initial population until budget exhausted\n        for xi in X:\n            if evals >= budget:\n                break\n            f = safe_eval(xi)\n            archive.append({'x': xi.copy(), 'f': float(f), 'age': 0})\n\n        # If nothing evaluated (e.g., budget==0) return fallback\n        if len(archive) == 0:\n            x_fallback = (lb + ub) / 2.0\n            return np.inf, x_fallback\n\n        # initial best\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            best = float(min(e['f'] for e in finite_entries))\n            xbest = sorted(finite_entries, key=lambda e: e['f'])[0]['x'].copy()\n        else:\n            best = np.inf\n            xbest = archive[0]['x'].copy()\n\n        # history for adaptation\n        win_len = 30\n        recent_successes = [0] * win_len\n\n        # main loop\n        stagn_counter = 0\n        last_improve_iter = 0\n        iter_no = 0\n\n        # pre-define some DE parameters\n        CR_base = 0.9\n\n        while evals < budget:\n            iter_no += 1\n\n            # convert archive arrays\n            Xa = np.array([e['x'] for e in archive])\n            Fa = np.array([e['f'] for e in archive])\n            ages = np.array([e['age'] for e in archive])\n\n            finite_mask = np.isfinite(Fa)\n            if not np.any(finite_mask):\n                cand = self._uniform_array(lb, ub)\n            else:\n                # compute per-dim robust scale estimate (IQR fallback to std)\n                Xf = Xa[finite_mask]\n                if Xf.shape[0] >= 2:\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    scale = (q75 - q25) / 1.349  # approximate robust std\n                    # if scale too small, fallback to std\n                    small = scale < 1e-12\n                    if np.any(small):\n                        sd = np.std(Xf, axis=0)\n                        scale[small] = sd[small]\n                else:\n                    scale = np.std(Xa, axis=0)\n                    # if still degenerate\n                    if np.all(scale == 0):\n                        scale = 0.2 * (ub - lb)\n                # ensure not zero\n                scale = np.maximum(scale, 1e-12)\n\n                # determine elites (by f)\n                n_elite = max(3, int(np.ceil(self.k_elite_ratio * len(archive))))\n                sorted_idx = np.argsort(Fa)\n                elite_idx = sorted_idx[:n_elite]\n                elites = Xa[elite_idx]\n                elites_f = Fa[elite_idx]\n\n                # pick anchor among elites with exponential bias (better elites preferred)\n                ranks = np.arange(n_elite)\n                weights = np.exp(-ranks / max(1.0, n_elite / 3.0))\n                weights = weights / weights.sum()\n                pick = self.rng.choice(n_elite, p=weights)\n                anchor = elites[pick].copy()\n\n                # helper to pick distinct donors for DE\n                def pick_distinct(k, exclude_idx=None):\n                    idxs = list(range(len(archive)))\n                    if exclude_idx is not None and exclude_idx in idxs:\n                        idxs.remove(exclude_idx)\n                    if len(idxs) >= k:\n                        return self.rng.choice(idxs, size=k, replace=False)\n                    else:\n                        # allow duplicates if not enough\n                        return self.rng.choice(idxs, size=k, replace=True)\n\n                # adapt probabilities slightly with gscale (favor Cauchy when large)\n                probs = self.base_probs.copy()\n                probs[3] *= (1.0 + (self.gscale / (self.gscale + 0.5)))  # boost cauchy slightly\n                probs = np.maximum(probs, 1e-8)\n                probs /= probs.sum()\n                strat = int(self.rng.choice(4, p=probs))\n\n                # prepare donors for DE-like if needed\n                donor_vals = None\n                if strat == 0 or self.rng.rand() < 0.05:  # DE or occasional differential injection\n                    ids = pick_distinct(3)\n                    xr1 = Xa[ids[0]]\n                    xr2 = Xa[ids[1]]\n                    xr3 = Xa[ids[2]]\n                    donor_vals = (xr1, xr2, xr3)\n\n                # compute a per-dim normalized scale for moves\n                norm_scale = np.maximum(scale, 1e-12)\n\n                # Strategy implementations\n                if strat == 0:\n                    # Strategy 0: DE-style differential injection (anchor / current-to-rand variant)\n                    r = anchor.copy()\n                    F = 0.6 + 0.4 * self.rng.rand()  # differential weight in [0.6,1.0)\n                    CR = np.clip(CR_base + 0.1 * (self.rng.randn()), 0.1, 0.95)\n                    if donor_vals is not None:\n                        xr1, xr2, xr3 = donor_vals\n                        mutant = xr1 + F * (xr2 - xr3)\n                        # binomial crossover to produce trial\n                        mask = self.rng.rand(self.dim) < CR\n                        if not np.any(mask):\n                            mask[self.rng.randint(self.dim)] = True\n                        r = np.where(mask, mutant, anchor)\n                        r += 0.01 * norm_scale * self.rng.randn(self.dim)\n                        # occasionally inject tempered cauchy to some coords\n                        if self.rng.rand() < 0.07:\n                            r += self._tempered_cauchy(norm_scale * self.gscale)\n                    else:\n                        r = anchor + 0.05 * norm_scale * self.rng.randn(self.dim)\n                    cand = r\n\n                elif strat == 1:\n                    # Strategy 1: PCA-guided elite perturbation\n                    E = elites - np.mean(elites, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                    except Exception:\n                        U = None\n                        S = np.zeros(self.dim)\n                        Vt = np.eye(self.dim)\n                    if E.shape[0] >= 2 and np.any(S > 1e-12):\n                        kcomp = min(max(1, int(min(self.dim, 1 + np.sum(S > 1e-8)))), min(self.dim, 3 + int(self.dim / 4)))\n                        pcs = Vt[:kcomp]\n                        # scale coefficients by singular values and gscale\n                        coeffs = self.rng.randn(kcomp) * (S[:kcomp] / (S[:kcomp].mean() + 1e-12)) * self.gscale\n                        r = anchor.copy()\n                        r += (coeffs @ pcs)\n                        # blend slightly towards anchor mean of elites\n                        r = 0.85 * r + 0.15 * (np.mean(elites, axis=0))\n                        # small gaussian jitter\n                        r += 0.02 * norm_scale * self.rng.randn(self.dim)\n                    else:\n                        r = anchor + 0.05 * norm_scale * self.rng.randn(self.dim)\n                    cand = r\n\n                elif strat == 2:\n                    # Strategy 2: Local anisotropic gaussian\n                    logs = self.rng.randn(self.dim) * 0.6\n                    multipliers = np.exp(logs)\n                    sig = norm_scale * self.gscale * multipliers * 0.3\n                    r = anchor + sig * self.rng.randn(self.dim)\n                    # ensure at least one coordinate changes\n                    if np.all(np.abs(r - anchor) < 1e-12):\n                        j = self.rng.randint(self.dim)\n                        r[j] += 1e-6\n                    # occasional mixed uniform per-dim\n                    if self.rng.rand() < 0.03:\n                        mask = self.rng.rand(self.dim) < 0.1\n                        if np.any(mask):\n                            r[mask] = self._uniform_array(lb, ub)[mask]\n                    cand = r\n\n                else:\n                    # Strategy 3: tempered Cauchy global escape\n                    r = anchor.copy()\n                    r += self._tempered_cauchy(norm_scale * self.gscale)\n                    # sometimes add directed differential component if donors exist\n                    if donor_vals is not None and self.rng.rand() < 0.5:\n                        xr1, xr2, xr3 = donor_vals\n                        r += 0.12 * (xr1 - xr2)\n                    cand = r\n\n                # occasional directed small jump to current best for refinement\n                if self.rng.rand() < 0.02 and len(archive) > 0:\n                    cand = 0.8 * cand + 0.2 * xbest\n\n                # sanitize candidate: replace nan/inf coordinates by uniform draws\n                cand = np.asarray(cand, dtype=float)\n                mask_bad = ~np.isfinite(cand)\n                if np.any(mask_bad):\n                    # replace bad coordinates individually\n                    for idx in np.where(mask_bad)[0]:\n                        cand[idx] = lb[idx] + self.rng.rand() * (ub[idx] - lb[idx])\n\n                # reflect to bounds\n                cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            # increment ages of others\n            for e in archive:\n                e['age'] += 1\n\n            # update archive\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best:\n                best = float(f_cand)\n                xbest = cand.copy()\n                improved = True\n                last_improve_iter = iter_no\n                stagn_counter = 0\n            else:\n                stagn_counter += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n            if finite_fs.size > 0:\n                try:\n                    q25 = np.percentile(finite_fs, 25)\n                except Exception:\n                    q25 = np.min(finite_fs)\n            else:\n                q25 = np.inf\n            success = (np.isfinite(f_cand) and (f_cand <= q25)) or improved\n            # update sliding window\n            recent_successes.append(1 if success else 0)\n            if len(recent_successes) > win_len:\n                recent_successes.pop(0)\n\n            # short-term adaptation of global scale\n            if len(recent_successes) >= win_len:\n                sr = np.mean(recent_successes)\n                if sr < 0.18:\n                    # low success -> increase exploration\n                    self.gscale = min(self.gscale * (1.0 + 0.12), self.gscale_max)\n                elif sr > 0.5:\n                    # high success -> tighten\n                    self.gscale = max(self.gscale * (0.985), self.gscale_min)\n                else:\n                    # slight decay towards stability\n                    self.gscale = np.clip(self.gscale * (1.0 - 0.002 * (sr - 0.3)), self.gscale_min, self.gscale_max)\n            else:\n                # small jitter to avoid lock\n                self.gscale *= (1.0 + (self.rng.randn() * 0.001))\n                self.gscale = np.clip(self.gscale, self.gscale_min, self.gscale_max)\n\n            # prune archive when too large (keep best + some random + oldest)\n            if len(archive) > self.max_archive:\n                kbest = max(10, 2 * self.dim)\n                archive_sorted = sorted(archive, key=lambda e: e['f'])\n                keep = []\n                keep.extend(archive_sorted[:kbest])  # keep best\n                # keep some randoms from the rest\n                rest = archive_sorted[kbest:]\n                nrand = min(len(rest), max(0, int(0.05 * self.max_archive)))\n                if nrand > 0:\n                    sel_idx = self.rng.choice(len(rest), size=nrand, replace=False)\n                    for i in sel_idx:\n                        keep.append(rest[i])\n                # keep some oldest from the remaining\n                remaining = rest\n                if len(remaining) > 0:\n                    ages_rest = np.array([e['age'] for e in remaining])\n                    sel_age_idx = np.argsort(-ages_rest)[:min(40, ages_rest.size)]\n                    for s in sel_age_idx:\n                        keep.append(remaining[s])\n                # compact and deduplicate by proximity (simple)\n                unique = []\n                seen = set()\n                for e in keep:\n                    key = tuple(np.round(e['x'], 8))\n                    if key not in seen:\n                        unique.append(e)\n                        seen.add(key)\n                archive = unique[: self.max_archive]\n\n            # stagnation detection & micro-restarts\n            if (iter_no - last_improve_iter) > max(200, 20 * self.dim) and evals < budget:\n                # micro-restart: local cloud around current best (or random center)\n                if xbest is None or not np.isfinite(best):\n                    center = self._uniform_array(lb, ub)\n                else:\n                    center = xbest.copy()\n                cloud_size = min(40, max(6, budget - evals))\n                # use robust sigma\n                if Xa.size > 0:\n                    cloud_sigma = np.maximum(np.std(Xa, axis=0), 0.1 * (ub - lb)) * (1.0 + self.gscale)\n                else:\n                    cloud_sigma = 0.2 * (ub - lb) * (1.0 + self.gscale)\n                ncloud = min(cloud_size, max(2, (budget - evals) // 3))\n                # generate cloud\n                for _ in range(ncloud):\n                    if evals >= budget:\n                        break\n                    perturb = (0.5 + self.rng.rand(self.dim)) * self.rng.randn(self.dim)\n                    p = center + cloud_sigma * perturb\n                    p = self._reflect_bounds(p, lb, ub)\n                    f_p = safe_eval(p)\n                    archive.append({'x': p.copy(), 'f': float(f_p), 'age': 0})\n                    if np.isfinite(f_p) and f_p < best:\n                        best = float(f_p)\n                        xbest = p.copy()\n                        last_improve_iter = iter_no\n                # also inject some global randoms\n                nrand = min(10, budget - evals)\n                for _ in range(nrand):\n                    if evals >= budget:\n                        break\n                    rx = self._uniform_array(lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < best:\n                        best = float(fr)\n                        xbest = rx.copy()\n                        last_improve_iter = iter_no\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.5, self.gscale_max)\n                recent_successes = [0] * win_len\n                last_improve_iter = iter_no\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or evals == budget):\n                finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n                median = np.median(finite_fs) if finite_fs.size > 0 else np.nan\n                print(f\"iter {iter_no}, evals {evals}/{budget}, archive {len(archive)}, best {best:.4e}, gscale {self.gscale:.4e}, median {median:.4e}\")\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries = sorted(finite_entries, key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        # all infinite: return a uniform fallback point in the domain center\n        x_fallback = (lb + ub) / 2.0\n        return np.inf, x_fallback", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "c489f4b1-9f42-41a3-9743-3a1febe34c2d", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic Gaussian locals and tempered Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=True)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.verbose = verbose\n        self.rng = np.random.RandomState(seed)\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.30, 0.20, 0.40, 0.10], dtype=float)\n\n        # some internal hyperparameters\n        self.max_archive = max(200 + 20 * self.dim, 400)\n        self.k_elite_ratio = 0.20\n        self.gscale = 0.2  # global scale factor, adapted online\n        self.gscale_min = 1e-6\n        self.gscale_max = 10.0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to infer bounds from function attributes, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # common attributes pattern\n        for attr in (\"bounds\", \"box\", \"domain\"):\n            if hasattr(func, attr):\n                try:\n                    b = getattr(func, attr)\n                    arr = np.asarray(b, dtype=float)\n                    if arr.ndim == 2 and arr.shape[0] >= 2:\n                        lb, ub = arr[0].copy(), arr[1].copy()\n                        break\n                except Exception:\n                    pass\n        # direct attributes lb/ub\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"ub\"):\n            try:\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                ub = None\n        # fallback to scalar properties\n        if (lb is None or ub is None) and hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n            try:\n                lb = np.asarray(getattr(func, \"lower\"), dtype=float)\n                ub = np.asarray(getattr(func, \"upper\"), dtype=float)\n            except Exception:\n                pass\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # mirror-reflection for any coordinate outside bounds; repeat a few times\n        x = x.copy()\n        for _ in range(niter):\n            below = x < lb\n            above = x > ub\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if np.all((x >= lb) & (x <= ub)):\n                break\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            shape = (n, self.dim)\n            draws = self.rng.rand(*shape) * (ub - lb) + lb\n            return draws\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF\n        scale_vec = np.maximum(np.asarray(scale_vec, dtype=float), 1e-12)\n        u = self.rng.rand(self.dim)\n        vals = scale_vec * np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplicative factor per-coordinate\n        heavy = self.rng.rand(self.dim) < 0.02\n        heavy_factor = np.where(heavy, self.rng.lognormal(mean=0.0, sigma=1.0, size=self.dim), 1.0)\n        vals = vals * heavy_factor\n        # small gaussian jitter to break symmetry\n        vals += 0.01 * scale_vec * self.rng.randn(self.dim)\n        # cap extremes to avoid overflow\n        max_abs = cap_multiplier * scale_vec\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n\n        # safe evaluator wrapper tracking budget usage\n        evals = 0\n        budget = self.budget\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            # ensure 1D array if given single sample row\n            if x.ndim == 2 and x.shape[0] == 1:\n                x = x[0]\n            try:\n                y = float(func(x))\n            except Exception:\n                try:\n                    y = float(np.asarray(func(x), dtype=float))\n                except Exception:\n                    y = np.inf\n            evals += 1\n            return y\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        lb, ub = self._get_bounds(func)\n\n        n_init = int(min(max(10, 3 * self.dim), max(5, budget // 10)))\n        n_init = max(2, n_init)\n        # Latin-hypercube-like sampling\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        X_init = np.zeros((n_init, self.dim))\n        for d in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            # pick uniform within each strata interval\n            u = self.rng.rand(n_init)\n            X_init[:, d] = (strata[perm] + u * (1.0 / n_init))\n        # map to bounds\n        X_init = lb + X_init * (ub - lb)\n\n        # a few extra uniform\n        n_extra = min(5, max(0, budget - n_init))\n        X_extra = self._uniform_array(lb, ub, n_extra) if n_extra > 0 else np.empty((0, self.dim))\n        if n_extra > 0:\n            X = np.vstack([X_init, X_extra])\n        else:\n            X = X_init\n\n        archive = []\n        # evaluate initial population until budget exhausted\n        for xi in X:\n            if evals >= budget:\n                break\n            f = safe_eval(xi)\n            archive.append({'x': xi.copy(), 'f': float(f), 'age': 0})\n\n        # If nothing evaluated (e.g., budget==0) return fallback\n        if len(archive) == 0:\n            x_fallback = (lb + ub) / 2.0\n            return np.inf, x_fallback\n\n        # initial best\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            best = float(min(e['f'] for e in finite_entries))\n            xbest = sorted(finite_entries, key=lambda e: e['f'])[0]['x'].copy()\n        else:\n            best = np.inf\n            xbest = None\n\n        # history for adaptation\n        win_len = 30\n        recent_successes = [0] * win_len\n\n        # main loop\n        stagn_counter = 0\n        last_improve_iter = 0\n        iter_no = 0\n\n        # pre-define some DE parameters\n        CR_base = 0.9\n\n        while evals < budget:\n            iter_no += 1\n\n            # convert archive arrays\n            Xa = np.array([e['x'] for e in archive]) if len(archive) > 0 else np.empty((0, self.dim))\n            Fa = np.array([e['f'] for e in archive]) if len(archive) > 0 else np.array([])\n            ages = np.array([e['age'] for e in archive]) if len(archive) > 0 else np.array([])\n\n            finite_mask = np.isfinite(Fa) if Fa.size > 0 else np.array([], dtype=bool)\n\n            # if no finite entries or archive tiny, sample uniform\n            if Fa.size == 0 or not np.any(finite_mask):\n                cand = self._uniform_array(lb, ub)\n            else:\n                # compute per-dim robust scale estimate (IQR fallback to std)\n                Xf = Xa[finite_mask]\n                if Xf.shape[0] >= 2:\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    scale = (q75 - q25) / 1.349  # approximate std from IQR\n                    sd = np.std(Xf, axis=0)\n                    small = scale < 1e-12\n                    if np.any(small):\n                        scale[small] = sd[small]\n                else:\n                    scale = np.std(Xa, axis=0) if Xa.size > 0 else 0.2 * (ub - lb)\n                    if np.all(scale == 0):\n                        scale = 0.2 * (ub - lb)\n                scale = np.maximum(scale, 1e-12)\n\n                # determine elites (by f)\n                n_elite = max(3, int(np.ceil(self.k_elite_ratio * len(archive))))\n                n_elite = min(n_elite, len(archive))\n                sorted_idx = np.argsort(Fa)\n                elite_idx = sorted_idx[:n_elite]\n                elites = Xa[elite_idx]\n                elites_f = Fa[elite_idx]\n\n                # pick anchor among elites with exponential bias (better elites preferred)\n                ranks = np.arange(n_elite)\n                weights = np.exp(-ranks / max(1.0, n_elite / 3.0))\n                weights = weights / weights.sum()\n                pick = self.rng.choice(n_elite, p=weights)\n                anchor = elites[pick].copy()\n                anchor_idx = elite_idx[pick]\n\n                # helper to pick distinct donors for DE\n                def pick_distinct(k, exclude_idx=None):\n                    idxs = list(range(len(archive)))\n                    if exclude_idx is not None and exclude_idx in idxs:\n                        idxs.remove(exclude_idx)\n                    if len(idxs) >= k:\n                        return self.rng.choice(idxs, size=k, replace=False)\n                    else:\n                        # allow duplicates if not enough entries\n                        return self.rng.choice(idxs, size=k, replace=True)\n\n                # adapt probabilities slightly with gscale (favor Cauchy when large)\n                probs = self.base_probs.copy()\n                probs[3] *= (1.0 + (self.gscale / (self.gscale + 0.5)))  # boost cauchy slightly\n                probs = np.maximum(probs, 1e-8)\n                probs /= probs.sum()\n                strat = int(self.rng.choice(4, p=probs))\n\n                # prepare donors for DE-like if needed\n                donor_vals = None\n                if strat == 0 or self.rng.rand() < 0.05:  # DE or occasional differential injection\n                    ids = pick_distinct(3, exclude_idx=anchor_idx)\n                    xr1 = Xa[ids[0]]\n                    xr2 = Xa[ids[1]]\n                    xr3 = Xa[ids[2]]\n                    donor_vals = (xr1, xr2, xr3)\n\n                # compute a per-dim normalized scale for moves\n                norm_scale = np.maximum(scale, 1e-12)\n\n                # Strategy implementations\n                if strat == 0:\n                    # Strategy 0: DE-style differential injection (current-to-rand variant)\n                    r = anchor.copy()\n                    F = 0.6 + 0.4 * self.rng.rand()  # differential weight in [0.6,1.0)\n                    CR = np.clip(CR_base + 0.1 * (self.rng.randn()), 0.1, 0.95)\n                    if donor_vals is not None:\n                        xr1, xr2, xr3 = donor_vals\n                        mutant = xr1 + F * (xr2 - xr3)\n                        # binomial crossover to produce trial\n                        mask = self.rng.rand(self.dim) < CR\n                        if not np.any(mask):\n                            mask[self.rng.randint(self.dim)] = True\n                        r = np.where(mask, mutant, anchor)\n                        # occasionally inject tempered cauchy to some coords\n                        if self.rng.rand() < 0.07:\n                            r = r + self._tempered_cauchy(norm_scale * self.gscale)\n                    else:\n                        r = anchor + 0.05 * norm_scale * self.rng.randn(self.dim)\n                    cand = r\n\n                elif strat == 1:\n                    # Strategy 1: PCA-guided elite perturbation\n                    E = elites - np.mean(elites, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                    except Exception:\n                        U = None\n                        S = np.zeros(self.dim)\n                        Vt = np.eye(self.dim)\n                    if E.shape[0] >= 2 and np.any(S > 1e-12):\n                        kcomp = min(self.dim, max(1, np.sum(S > 1e-12)))\n                        pcs = Vt[:kcomp]\n                        coeffs = (self.rng.randn(kcomp) * (S[:kcomp] / (S[:kcomp].mean() + 1e-12)) * self.gscale)\n                        r = anchor.copy()\n                        r += coeffs @ pcs  # add principal component combination\n                        # blend slightly towards anchor mean of elites\n                        r = 0.85 * r + 0.15 * (np.mean(elites, axis=0))\n                        # small gaussian jitter\n                        r += 0.02 * norm_scale * self.rng.randn(self.dim)\n                    else:\n                        r = anchor + 0.05 * norm_scale * self.rng.randn(self.dim)\n                    cand = r\n\n                elif strat == 2:\n                    # Strategy 2: Local anisotropic gaussian\n                    logs = self.rng.randn(self.dim) * 0.6\n                    multipliers = np.exp(logs)\n                    sig = norm_scale * self.gscale * multipliers * 0.3\n                    r = anchor + sig * self.rng.randn(self.dim)\n                    # ensure at least one coordinate changes\n                    if np.all(np.abs(r - anchor) < 1e-12):\n                        j = self.rng.randint(self.dim)\n                        r[j] += 1e-6\n                    # occasional mixed uniform per-dim\n                    if self.rng.rand() < 0.03:\n                        mask = self.rng.rand(self.dim) < 0.2\n                        if np.any(mask):\n                            r[mask] = self._uniform_array(lb, ub)[mask]\n                    cand = r\n\n                else:\n                    # Strategy 3: tempered Cauchy global escape\n                    r = anchor.copy()\n                    r = r + self._tempered_cauchy(norm_scale * self.gscale)\n                    # sometimes add directed differential component if donors exist\n                    if donor_vals is not None and self.rng.rand() < 0.5:\n                        xr1, xr2, xr3 = donor_vals\n                        r += 0.3 * (xr1 - xr3)\n                    cand = r\n\n                # occasional directed small jump to current best for refinement\n                if self.rng.rand() < 0.02 and xbest is not None and len(archive) > 0:\n                    cand = 0.8 * cand + 0.2 * xbest\n\n                # sanitize candidate: replace nan/inf coordinates by uniform draws\n                cand = np.asarray(cand, dtype=float)\n                mask_bad = ~np.isfinite(cand)\n                if np.any(mask_bad):\n                    # replace bad coordinates individually\n                    for idx in np.where(mask_bad)[0]:\n                        cand[idx] = lb[idx] + self.rng.rand() * (ub[idx] - lb[idx])\n\n                # reflect to bounds\n                cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n            # increment ages of others\n            for e in archive:\n                e['age'] += 1\n\n            # update archive: append candidate (keep as float)\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best:\n                best = float(f_cand)\n                xbest = cand.copy()\n                improved = True\n                last_improve_iter = iter_no\n                stagn_counter = 0\n            else:\n                stagn_counter += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n            if finite_fs.size > 0:\n                try:\n                    q25 = np.percentile(finite_fs, 25)\n                except Exception:\n                    q25 = np.min(finite_fs)\n            else:\n                q25 = np.inf\n            success = (np.isfinite(f_cand) and (f_cand <= q25)) or improved\n            # update sliding window\n            recent_successes.append(1 if success else 0)\n            if len(recent_successes) > win_len:\n                recent_successes.pop(0)\n\n            # short-term adaptation of global scale\n            sr = float(np.mean(recent_successes)) if len(recent_successes) > 0 else 0.0\n            if sr < 0.18:\n                # low success -> increase exploration\n                self.gscale = min(self.gscale * (1.0 + 0.12), self.gscale_max)\n            elif sr > 0.5:\n                # high success -> tighten\n                self.gscale = max(self.gscale * (0.985), self.gscale_min)\n                # slight decay towards stability\n                self.gscale = np.clip(self.gscale * (1.0 - 0.002 * (sr - 0.3)), self.gscale_min, self.gscale_max)\n            else:\n                # small jitter to avoid lock\n                self.gscale *= (1.0 + (self.rng.randn() * 0.001))\n                self.gscale = np.clip(self.gscale, self.gscale_min, self.gscale_max)\n\n            # prune archive when too large (keep best + some random + oldest)\n            if len(archive) > self.max_archive:\n                kbest = max(10, 2 * self.dim)\n                archive_sorted = sorted(archive, key=lambda e: e['f'])\n                keep = []\n                keep.extend(archive_sorted[:kbest])  # keep best\n                # keep some randoms from the rest\n                rest = archive_sorted[kbest:]\n                nrand = min(len(rest), max(0, int(0.05 * self.max_archive)))\n                if nrand > 0 and len(rest) > 0:\n                    sel_idx = self.rng.choice(len(rest), size=nrand, replace=False)\n                    for ii in sel_idx:\n                        keep.append(rest[ii])\n                # keep some oldest from the remaining\n                remaining = rest\n                if len(remaining) > 0:\n                    ages_rest = np.array([e['age'] for e in remaining])\n                    sel_age_idx = np.argsort(-ages_rest)[:min(40, ages_rest.size)]\n                    for s in sel_age_idx:\n                        keep.append(remaining[s])\n                # compact and deduplicate by proximity (simple)\n                unique = []\n                seen = set()\n                for e in keep:\n                    key = tuple(np.round(e['x'], 8))\n                    if key not in seen:\n                        unique.append(e)\n                        seen.add(key)\n                archive = unique[: self.max_archive]\n\n            # stagnation detection & micro-restarts\n            if (iter_no - last_improve_iter) > max(200, 20 * self.dim) and evals < budget:\n                # micro-restart: local cloud around current best (or random center)\n                if xbest is None or not np.isfinite(best):\n                    center = self._uniform_array(lb, ub)\n                else:\n                    center = xbest.copy()\n                cloud_size = min(40, max(6, budget - evals))\n                # use robust sigma\n                if Xa.size > 0:\n                    cloud_sigma = np.maximum(np.std(Xa, axis=0), 0.1 * (ub - lb)) * (1.0 + self.gscale)\n                else:\n                    cloud_sigma = 0.2 * (ub - lb) * (1.0 + self.gscale)\n                ncloud = min(cloud_size, max(2, (budget - evals) // 3))\n                # generate cloud\n                for _ in range(ncloud):\n                    if evals >= budget:\n                        break\n                    perturb = (0.5 + self.rng.rand(self.dim)) * self.rng.randn(self.dim)\n                    p = center + cloud_sigma * perturb\n                    p = self._reflect_bounds(p, lb, ub)\n                    f_p = safe_eval(p)\n                    archive.append({'x': p.copy(), 'f': float(f_p), 'age': 0})\n                    if np.isfinite(f_p) and f_p < best:\n                        best = float(f_p)\n                        xbest = p.copy()\n                        last_improve_iter = iter_no\n                # also inject some global randoms\n                nrand = min(10, budget - evals)\n                for _ in range(nrand):\n                    if evals >= budget:\n                        break\n                    rx = self._uniform_array(lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < best:\n                        best = float(fr)\n                        xbest = rx.copy()\n                        last_improve_iter = iter_no\n                # after restart, slightly increase exploration scales and reset small window\n                recent_successes = [0] * win_len\n                last_improve_iter = iter_no\n                self.gscale = min(self.gscale * 1.2, self.gscale_max)\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or evals == budget):\n                finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n                median = float(np.median(finite_fs)) if finite_fs.size > 0 else np.inf\n                print(f\"iter {iter_no}, evals {evals}/{budget}, archive {len(archive)}, best {best:.4e}, gscale {self.gscale:.4e}, median {median:.4e}\")\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries = sorted(finite_entries, key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        # all infinite: return a uniform fallback point in the domain center\n        x_fallback = (lb + ub) / 2.0\n        return np.inf, x_fallback", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "05b4b53a-35be-4e43-ba8d-bef304ebff88", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donors, PCA-guided elite moves, anisotropic Gaussian locals, tempered Cauchy escapes, short-term scale adaptation and micro-restarts — a memory-guided adaptive directional search for black-box continuous optimization.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=True)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.verbose = bool(verbose)\n        self.rng = np.random.RandomState(seed)\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.30, 0.20, 0.40, 0.10], dtype=float)\n\n        # some internal hyperparameters\n        self.max_archive = max(200 + 20 * self.dim, 400)\n        self.gscale = 0.2  # global scale factor, adapted online\n        self.gscale_min = 1e-4\n        self.gscale_max = 10.0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to infer bounds from function attributes, fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        # common attributes pattern\n        for attr in (\"bounds\", \"box\", \"domain\"):\n            if hasattr(func, attr):\n                try:\n                    b = np.asarray(getattr(func, attr))\n                    if b.ndim == 2 and b.shape[0] >= 2 and b.shape[1] == self.dim:\n                        lb = b[0].astype(float)\n                        ub = b[1].astype(float)\n                        break\n                    if b.ndim == 1 and b.size == 2:\n                        lb = np.full(self.dim, float(b[0]))\n                        ub = np.full(self.dim, float(b[1]))\n                        break\n                except Exception:\n                    pass\n        # direct attributes lb/ub\n        if lb is None and hasattr(func, \"lb\"):\n            try:\n                lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n            except Exception:\n                lb = None\n        if ub is None and hasattr(func, \"ub\"):\n            try:\n                ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n            except Exception:\n                ub = None\n        # fallback to scalar properties\n        if (lb is None or ub is None) and (hasattr(func, \"lower\") and hasattr(func, \"upper\")):\n            try:\n                lb_s = getattr(func, \"lower\")\n                ub_s = getattr(func, \"upper\")\n                lb = np.asarray(lb_s, dtype=float)\n                ub = np.asarray(ub_s, dtype=float)\n            except Exception:\n                pass\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        ub = np.asarray(ub, dtype=float)\n        lb = np.asarray(lb, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n            ub = np.full(self.dim, float(ub))\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # mirror-reflection for any coordinate outside bounds; repeat a few times\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(niter):\n            below = x < lb\n            above = x > ub\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            shape = (n, self.dim)\n            draws = self.rng.rand(*shape) * (ub - lb) + lb\n            return draws\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF\n        u = self.rng.rand(self.dim) * 0.999998 + 1e-6  # avoid exactly 0/1\n        c = np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplicative factor per-coordinate (rare)\n        heavy = (self.rng.rand(self.dim) < 0.02).astype(float) * (1.0 + self.rng.rand(self.dim) * 20.0) + 1.0\n        vals = scale_vec * c * heavy\n        # small gaussian jitter to break symmetry\n        vals += 1e-6 * scale_vec * self.rng.randn(self.dim)\n        # cap extremes to avoid overflow\n        max_abs = cap_multiplier * scale_vec\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        # localize budget variable\n        budget = int(self.budget)\n\n        # safe evaluator wrapper tracking budget usage\n        evals = 0\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return np.inf\n            # ensure 1D array\n            x = np.asarray(x, dtype=float)\n            if x.ndim == 2 and x.shape[0] == 1:\n                x = x[0]\n            try:\n                y = float(func(x))\n            except Exception:\n                try:\n                    y = float(func(np.asarray(x, dtype=float)))\n                except Exception:\n                    y = np.inf\n            evals += 1\n            return y\n\n        # get bounds\n        lb, ub = self._get_bounds(func)\n\n        if budget <= 0:\n            x_fallback = (lb + ub) / 2.0\n            return np.inf, x_fallback\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        n_init = int(min(max(10, 3 * self.dim), max(5, budget // 10)))\n        # Latin-hypercube-like sampling\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        X_init = np.empty((n_init, self.dim), dtype=float)\n        for d in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            u = self.rng.rand(n_init)\n            # points in (i/n, (i+1)/n)\n            coords = (strata[:n_init] + u * (1.0 / n_init))\n            X_init[:, d] = coords[perm]\n        # map to bounds\n        X_init = lb + X_init * (ub - lb)\n\n        # a few extra uniform\n        n_extra = min(5, max(0, budget - n_init))\n        if n_extra > 0:\n            X_extra = self._uniform_array(lb, ub, n_extra)\n            X = np.vstack([X_init, X_extra])\n        else:\n            X = X_init\n\n        archive = []\n        # evaluate initial population until budget exhausted\n        for xi in X:\n            if evals >= budget:\n                break\n            f = safe_eval(xi)\n            archive.append({'x': xi.copy(), 'f': float(f), 'age': 0})\n\n        # If nothing evaluated (e.g., budget==0) return fallback\n        if len(archive) == 0:\n            x_fallback = (lb + ub) / 2.0\n            return np.inf, x_fallback\n\n        # initial best\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries.sort(key=lambda e: e['f'])\n            xbest = finite_entries[0]['x'].copy()\n            best = float(finite_entries[0]['f'])\n        else:\n            xbest = archive[0]['x'].copy()\n            best = float(archive[0]['f'])\n\n        # history for adaptation\n        win_len = 30\n        recent_successes = [0] * win_len\n\n        # main loop\n        last_improve_iter = 0\n        iter_no = 0\n\n        # pre-define some DE parameters\n        CR_mean = 0.5\n\n        while evals < budget:\n            iter_no += 1\n\n            # convert archive arrays\n            Xa = np.array([e['x'] for e in archive])\n            Fa = np.array([e['f'] for e in archive])\n            ages = np.array([e['age'] for e in archive])\n\n            finite_mask = np.isfinite(Fa)\n            if not np.any(finite_mask):\n                cand = self._uniform_array(lb, ub)\n            else:\n                # compute per-dim robust scale estimate (IQR fallback to std)\n                Xf = Xa[finite_mask]\n                if Xf.shape[0] >= 2:\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    scale = (q75 - q25) / 1.349  # approximate robust std\n                    # if scale too small, fallback to std\n                    small = scale < 1e-12\n                    if np.any(small):\n                        std = np.std(Xf, axis=0)\n                        scale[small] = std[small]\n                else:\n                    scale = np.std(Xa, axis=0)\n                    if np.any(np.isnan(scale)) or np.all(scale == 0):\n                        scale = 0.2 * (ub - lb)\n                # ensure not zero\n                scale = np.maximum(scale, 1e-12)\n\n                # determine elites (by f)\n                sorted_idx = np.argsort(Fa)\n                n_elite = max(2, min(12, int(np.ceil(len(archive) * 0.2))))\n                elites_idx = sorted_idx[:n_elite]\n                elites = Xa[elites_idx]\n\n                # pick anchor among elites with exponential bias (better elites preferred)\n                ranks = np.arange(n_elite)\n                weights = np.exp(-ranks / max(1.0, n_elite / 3.0))\n                weights = weights / weights.sum()\n                pick = self.rng.choice(n_elite, p=weights)\n                anchor = elites[pick].copy()\n\n                # helper to pick distinct donors for DE\n                def pick_distinct(k, exclude_idx=None):\n                    idxs = list(range(len(archive)))\n                    if exclude_idx is not None and exclude_idx in idxs:\n                        idxs.remove(exclude_idx)\n                    if len(idxs) >= k:\n                        return self.rng.choice(idxs, size=k, replace=False)\n                    else:\n                        # allow duplicates if not enough\n                        return self.rng.choice(idxs, size=k, replace=True)\n\n                # adapt probabilities slightly with gscale (favor Cauchy when large)\n                probs = self.base_probs.copy()\n                probs[3] *= (1.0 + (self.gscale / (self.gscale + 0.5)))  # boost cauchy slightly\n                probs = np.maximum(probs, 1e-8)\n                probs = probs / probs.sum()\n                strat = int(self.rng.choice(4, p=probs))\n\n                donor_vals = None\n                xr1 = xr2 = xr3 = None\n                if strat == 0 or self.rng.rand() < 0.05:  # DE or occasional differential injection\n                    ids = pick_distinct(3)\n                    xr1 = Xa[ids[0]]\n                    xr2 = Xa[ids[1]]\n                    xr3 = Xa[ids[2]]\n                    donor_vals = (xr1, xr2, xr3)\n\n                # compute a per-dim normalized scale for moves\n                norm_scale = np.maximum(scale, 1e-12)\n\n                # Strategy implementations\n                if strat == 0:\n                    # Strategy 0: DE-style differential injection (anchor / current-to-rand variant)\n                    F = 0.6 + 0.4 * self.rng.rand()  # differential weight in [0.6,1.0)\n                    CR = np.clip(self.rng.randn() * 0.08 + CR_mean, 0.05, 0.9)\n                    if donor_vals is not None:\n                        mutant = anchor + F * (xr1 - xr2)\n                        # binomial crossover to produce trial\n                        mask = self.rng.rand(self.dim) < CR\n                        if not np.any(mask):\n                            mask[self.rng.randint(self.dim)] = True\n                        r = np.where(mask, mutant, anchor)\n                        # occasionally inject tempered cauchy to some coords\n                        if self.rng.rand() < 0.07:\n                            r += self._tempered_cauchy(norm_scale * self.gscale)\n                    else:\n                        r = anchor + 0.05 * norm_scale * self.rng.randn(self.dim)\n                    cand = r\n\n                elif strat == 1:\n                    # Strategy 1: PCA-guided elite perturbation\n                    E = elites - np.mean(elites, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                    except Exception:\n                        U = None\n                        S = np.zeros(self.dim)\n                        Vt = np.eye(self.dim)\n                    # choose number of components\n                    kcomp = min(max(1, int(min(self.dim, 1 + np.sum(S > 1e-8)))), min(self.dim, 3 + int(self.dim / 4)))\n                    pcs = Vt[:kcomp]\n                    # scale coefficients by singular values and gscale\n                    sv = S[:kcomp] if S.size >= kcomp else np.ones(kcomp)\n                    coeffs = self.rng.randn(kcomp) * (sv / (sv.mean() + 1e-12)) * (0.6 + self.gscale)\n                    r = anchor.copy()\n                    r += (coeffs @ pcs)\n                    # blend slightly towards anchor mean of elites\n                    r = 0.85 * r + 0.15 * np.mean(elites, axis=0)\n                    # small gaussian jitter\n                    r += 0.02 * norm_scale * self.rng.randn(self.dim)\n                    cand = r\n\n                elif strat == 2:\n                    # Strategy 2: Local anisotropic gaussian\n                    # anisotropic sigma per-dim\n                    base_sig = 0.08 * (1.0 + self.gscale) * norm_scale\n                    # emphasize dims with larger spread\n                    sig = base_sig * (1.0 + 0.5 * (norm_scale / (norm_scale.mean() + 1e-12)))\n                    r = anchor + sig * self.rng.randn(self.dim)\n                    # ensure at least one coordinate changes\n                    if np.all(np.abs(r - anchor) < 1e-12):\n                        j = self.rng.randint(self.dim)\n                        r[j] += sig[j] * (0.5 + self.rng.rand())\n                    # occasional mixed uniform per-dim\n                    if self.rng.rand() < 0.03:\n                        mask = self.rng.rand(self.dim) < 0.1\n                        if np.any(mask):\n                            r[mask] = self._uniform_array(lb, ub)[mask]\n                    cand = r\n\n                else:\n                    # Strategy 3: tempered Cauchy global escape\n                    r = anchor.copy()\n                    r += self._tempered_cauchy(norm_scale * self.gscale)\n                    # sometimes add directed differential component if donors exist\n                    if donor_vals is not None and self.rng.rand() < 0.5:\n                        r += 0.12 * (xr1 - xr2)\n                    cand = r\n\n                # occasional directed small jump to current best for refinement\n                if self.rng.rand() < 0.06:\n                    cand = 0.8 * cand + 0.2 * xbest\n\n                # sanitize candidate: replace nan/inf coordinates by uniform draws\n                cand = np.asarray(cand, dtype=float)\n                mask_bad = ~np.isfinite(cand)\n                if np.any(mask_bad):\n                    cand[mask_bad] = self._uniform_array(lb, ub)[mask_bad]\n\n                # reflect to bounds\n                cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n\n            # increment ages of others\n            for e in archive:\n                e['age'] += 1\n\n            # update archive\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best:\n                best = float(f_cand)\n                xbest = cand.copy()\n                improved = True\n                last_improve_iter = iter_no\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n            success = False\n            if finite_fs.size > 0:\n                try:\n                    q25 = np.percentile(finite_fs, 25.0)\n                    if np.isfinite(f_cand) and f_cand <= q25:\n                        success = True\n                except Exception:\n                    success = improved\n            else:\n                success = improved\n\n            # update sliding window\n            recent_successes.append(1 if success else 0)\n            if len(recent_successes) > win_len:\n                recent_successes.pop(0)\n\n            # short-term adaptation of global scale\n            if len(recent_successes) >= win_len:\n                sr = np.mean(recent_successes)\n                if sr < 0.18:\n                    # low success -> increase exploration\n                    self.gscale = min(self.gscale * (1.0 + 0.12), self.gscale_max)\n                elif sr > 0.5:\n                    # high success -> tighten\n                    self.gscale = max(self.gscale * (0.985), self.gscale_min)\n                else:\n                    # slight decay towards stability\n                    self.gscale = np.clip(self.gscale * 0.997 + 0.0005, self.gscale_min, self.gscale_max)\n            else:\n                # small jitter to avoid lock\n                self.gscale = np.clip(self.gscale * (1.0 + (self.rng.randn() * 0.001)), self.gscale_min, self.gscale_max)\n\n            # prune archive when too large (keep best + some random + oldest)\n            if len(archive) > self.max_archive:\n                kbest = max(10, 2 * self.dim)\n                archive_sorted = sorted(archive, key=lambda e: e['f'])\n                keep = []\n                keep.extend(archive_sorted[:kbest])  # keep best\n                # keep some randoms from the rest\n                rest = archive_sorted[kbest:]\n                nrand = min(len(rest), max(0, int(0.05 * self.max_archive)))\n                if nrand > 0:\n                    rnd_idx = self.rng.choice(len(rest), size=nrand, replace=False)\n                    for ii in rnd_idx:\n                        keep.append(rest[ii])\n                # keep some oldest from the remaining\n                remaining = [r for i, r in enumerate(rest) if i not in (set(rnd_idx) if nrand > 0 else set())]\n                if len(remaining) > 0:\n                    ages_rest = np.array([e['age'] for e in remaining])\n                    sel_age_idx = np.argsort(-ages_rest)[:min(40, ages_rest.size)]\n                    for s in sel_age_idx:\n                        keep.append(remaining[s])\n                # compact and deduplicate by proximity (simple)\n                unique = []\n                seen = set()\n                for e in keep:\n                    key = tuple(np.round(e['x'], 8))\n                    if key not in seen:\n                        seen.add(key)\n                        unique.append(e)\n                archive = unique\n\n            # stagnation detection & micro-restarts\n            if (iter_no - last_improve_iter) > max(200, 20 * self.dim) and evals < budget:\n                # micro-restart: local cloud around current best (or random center)\n                if np.isfinite(best):\n                    center = xbest.copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                cloud_size = min(40, max(6, budget - evals))\n                # use robust sigma\n                if Xa.size > 0:\n                    cloud_sigma = np.maximum(np.std(Xa, axis=0), 0.1 * (ub - lb)) * (1.0 + self.gscale)\n                else:\n                    cloud_sigma = 0.2 * (ub - lb) * (1.0 + self.gscale)\n                ncloud = int(cloud_size)\n                for _ in range(ncloud):\n                    if evals >= budget:\n                        break\n                    perturb = cloud_sigma * (0.5 + self.rng.rand(self.dim)) * self.rng.randn(self.dim)\n                    p = center + perturb\n                    p = self._reflect_bounds(p, lb, ub)\n                    f_p = safe_eval(p)\n                    archive.append({'x': p.copy(), 'f': float(f_p), 'age': 0})\n                    if np.isfinite(f_p) and f_p < best:\n                        best = float(f_p)\n                        xbest = p.copy()\n                        last_improve_iter = iter_no\n                # also inject some global randoms\n                nrand_inject = min(6, budget - evals)\n                for _ in range(nrand_inject):\n                    if evals >= budget:\n                        break\n                    rx = self._uniform_array(lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < best:\n                        best = float(fr)\n                        xbest = rx.copy()\n                        last_improve_iter = iter_no\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.5, self.gscale_max)\n                recent_successes = [0] * win_len\n                last_improve_iter = iter_no\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or evals == budget):\n                finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n                median = np.median(finite_fs) if finite_fs.size > 0 else np.nan\n                print(f\"iter {iter_no}, evals {evals}/{budget}, archive {len(archive)}, best {best:.4e}, gscale {self.gscale:.4e}, median {median:.4e}\")\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries.sort(key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        # all infinite: return a uniform fallback point in the domain center\n        x_fallback = (lb + ub) / 2.0\n        return np.inf, x_fallback", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "f7030bf7-3a4c-4f60-ab72-cfe221f6a868", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style differential proposals, PCA-guided elite exploration, anisotropic Gaussian local searches, tempered Cauchy escapes, short-term scale adaptation and micro-restarts for robust continuous black-box optimization.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=False)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.verbose = verbose\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.36, 0.24, 0.28, 0.12], dtype=float)\n\n        # internal hyperparameters\n        self.k_elite_ratio = 0.20\n        self.gscale = 0.18  # global scale factor, adapted online\n        self.gscale_min = 1e-5\n        self.gscale_max = 10.0\n\n        # archive limits\n        self.max_archive = max(60, 20 * self.dim)\n        # adaptation window\n        self.win_len = 28\n        # stagnation threshold for micro-restart (iterations w/o improvement)\n        self.stagn_limit = max(60, 6 * self.dim)\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Many affine BBOB tasks are within [-5,5]^dim; but allow func to provide attributes\n        lb = None\n        ub = None\n        # try common attribute patterns\n        for attr in (\"bounds\", \"box\", \"domain\"):\n            if hasattr(func, attr):\n                try:\n                    b = getattr(func, attr)\n                    b = np.asarray(b, dtype=float)\n                    if b.ndim == 2 and b.shape[0] >= 2:\n                        lb, ub = b[0].copy(), b[1].copy()\n                        break\n                except Exception:\n                    pass\n        if lb is None:\n            if hasattr(func, \"lb\"):\n                try:\n                    lb = np.asarray(getattr(func, \"lb\"), dtype=float)\n                except Exception:\n                    lb = None\n        if ub is None:\n            if hasattr(func, \"ub\"):\n                try:\n                    ub = np.asarray(getattr(func, \"ub\"), dtype=float)\n                except Exception:\n                    ub = None\n\n        # fallback to scalar lower/upper\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=4):\n        # mirror-reflection: reflect coordinate across bounds repeatedly\n        x = np.asarray(x, dtype=float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            # if all in bounds, stop early\n            if np.all((x >= lb) & (x <= ub)):\n                break\n        # final clamp to be safe\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if n == 1:\n            return lb + self.rng.rand(self.dim) * (ub - lb)\n        else:\n            r = self.rng.rand(n, self.dim)\n            return lb + r * (ub - lb)\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF\n        scale_vec = np.maximum(np.asarray(scale_vec, dtype=float), 1e-12)\n        u = self.rng.rand(self.dim)\n        c = np.tan(np.pi * (u - 0.5))  # standard Cauchy variates\n        vals = scale_vec * c\n        # occasional heavy multiplicative factor per-coordinate\n        heavy = self.rng.rand(self.dim) < 0.02\n        if np.any(heavy):\n            vals[heavy] *= self.rng.lognormal(mean=0.0, sigma=1.0, size=heavy.sum())\n        # small gaussian jitter to break symmetry\n        vals += 1e-3 * scale_vec * self.rng.randn(self.dim)\n        # cap extremes to avoid overflow/nan\n        max_abs = cap_multiplier * scale_vec\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        # safe evaluator wrapper tracking budget usage\n        evals = 0\n        budget = self.budget\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            try:\n                y = func(x)\n            except Exception:\n                # if function throws, treat as infinite\n                y = np.inf\n            evals += 1\n            return float(y)\n\n        # if budget zero, return fallback\n        if budget <= 0:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n            return np.inf, (lb + ub) / 2.0\n\n        # bounds\n        lb, ub = self._get_bounds(func)\n\n        # initial population: LHS-like + a few randoms\n        n_init = int(min(max(12, 4 * self.dim), max(12, budget // 6)))\n        # stratified along each dimension independently then shuffled\n        strata = np.linspace(0.0, 1.0, n_init + 1)\n        X_init = np.empty((n_init, self.dim), dtype=float)\n        for d in range(self.dim):\n            u = self.rng.rand(n_init)\n            pts = (strata[:-1] + u * (1.0 / n_init))\n            self.rng.shuffle(pts)\n            X_init[:, d] = pts\n        X_init = lb + X_init * (ub - lb)\n\n        n_extra = min(6, max(0, budget - n_init))\n        if n_extra > 0:\n            X_extra = self._uniform_array(lb, ub, n_extra)\n            X = np.vstack([X_init, X_extra])\n        else:\n            X = X_init\n\n        # evaluate initial population until budget exhausted\n        archive = []\n        for xi in X:\n            if evals >= budget:\n                break\n            x = self._reflect_bounds(xi, lb, ub)\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': float(f), 'age': 0})\n\n        # If nothing evaluated (e.g., budget==0) return fallback\n        if len(archive) == 0:\n            return np.inf, (lb + ub) / 2.0\n\n        # initial best\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            best = float(min(e['f'] for e in finite_entries))\n            xbest = sorted(finite_entries, key=lambda e: e['f'])[0]['x'].copy()\n        else:\n            best = np.inf\n            xbest = archive[0]['x'].copy()\n\n        # history for adaptation\n        recent_successes = [0] * self.win_len\n        stagn_counter = 0\n        last_improve_iter = 0\n        iter_no = 0\n\n        CR_base = 0.9\n\n        while evals < budget:\n            iter_no += 1\n\n            # convert archive arrays\n            Xa = np.array([e['x'] for e in archive])\n            Fa = np.array([e['f'] for e in archive])\n            ages = np.array([e['age'] for e in archive])\n\n            finite_mask = np.isfinite(Fa)\n            if not np.any(finite_mask):\n                # fallback: uniform around center\n                norm_scale = np.ones(self.dim) * (ub - lb) * 0.2\n            else:\n                # compute per-dim robust scale estimate (IQR fallback to std)\n                Xf = Xa[finite_mask]\n                if Xf.shape[0] >= 3:\n                    q75 = np.percentile(Xf, 75, axis=0)\n                    q25 = np.percentile(Xf, 25, axis=0)\n                    scale = (q75 - q25) / 1.349  # approx sigma from IQR\n                    small = scale < 1e-12\n                    if np.any(small):\n                        sd = np.std(Xf, axis=0)\n                        scale[small] = sd[small]\n                    # if still degenerate, fallback to a fraction of domain\n                    scale[np.isnan(scale)] = 0.2 * (ub - lb)[np.isnan(scale)]\n                    scale[scale <= 0] = 0.2 * (ub - lb)[scale <= 0]\n                else:\n                    scale = 0.2 * (ub - lb)\n                norm_scale = np.maximum(scale, 1e-12)\n\n            # determine elites (by f)\n            sorted_idx = np.argsort(Fa, kind='mergesort')\n            n_elite = max(3, int(np.ceil(self.k_elite_ratio * len(archive))))\n            n_elite = min(n_elite, len(archive))\n            elite_idx = sorted_idx[:n_elite]\n            elites = Xa[elite_idx]\n            elites_f = Fa[elite_idx]\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            ranks = np.arange(n_elite)\n            weights = np.exp(-ranks / max(1.0, n_elite / 3.0))\n            weights = weights / weights.sum()\n            pick = self.rng.choice(n_elite, p=weights)\n            anchor = elites[pick].copy()\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                idxs = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in idxs:\n                    idxs.remove(exclude_idx)\n                if len(idxs) >= k and k > 0:\n                    return self.rng.choice(idxs, size=k, replace=False)\n                elif k == 0:\n                    return []\n                else:\n                    # allow duplicates if not enough\n                    return self.rng.choice(idxs, size=k, replace=True)\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            probs[3] *= (1.0 + (self.gscale / (self.gscale + 0.5)))  # boost cauchy slightly\n            probs = np.maximum(probs, 1e-8)\n            probs /= probs.sum()\n            strat = int(self.rng.choice(4, p=probs))\n\n            # prepare donors for DE-like if needed\n            donor_vals = None\n            if strat == 0 or self.rng.rand() < 0.14:\n                ids = pick_distinct(3)\n                if len(ids) >= 3:\n                    xr1 = Xa[ids[0]].copy()\n                    xr2 = Xa[ids[1]].copy()\n                    xr3 = Xa[ids[2]].copy()\n                    donor_vals = (xr1, xr2, xr3)\n                else:\n                    donor_vals = None\n\n            # compute a per-dim normalized scale for moves\n            # scale vector normalized to domain\n            dom = ub - lb\n            norm_scale = np.maximum(norm_scale, 1e-12)\n            # produce candidate depending on strategy\n            cand = None\n\n            if strat == 0:\n                # Strategy 0: DE-style differential injection (anchor / current-to-rand variant)\n                F = 0.6 + 0.4 * self.rng.rand()  # differential weight in [0.6,1.0)\n                CR = CR_base * (0.6 + 0.8 * self.rng.rand())\n                if donor_vals is not None:\n                    xr1, xr2, xr3 = donor_vals\n                    mutant = xr1 + F * (xr2 - xr3)\n                else:\n                    # fallback to gaussian mutant\n                    mutant = anchor + self.gscale * norm_scale * self.rng.randn(self.dim)\n                mask = self.rng.rand(self.dim) < CR\n                # ensure at least one coordinate\n                mask[self.rng.randint(self.dim)] = True\n                r = np.where(mask, mutant, anchor)\n                # occasionally inject tempered cauchy to some coords\n                if self.rng.rand() < 0.07:\n                    r += self._tempered_cauchy(self.gscale * norm_scale)\n                cand = r\n\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                E = elites - np.mean(elites, axis=0)\n                kcomp = min(max(1, self.dim // 4), E.shape[0])\n                try:\n                    U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                    pcs = Vt[:kcomp]\n                    # coefficients scaled by singular values and gscale\n                    coeff = (self.rng.randn(kcomp) * (S[:kcomp] / (S[0] + 1e-12))) * (0.6 + self.rng.rand() * 0.8)\n                    r = anchor.copy()\n                    for i in range(kcomp):\n                        r += self.gscale * norm_scale * coeff[i] * pcs[i]\n                    # blend slightly towards anchor mean of elites\n                    r = 0.85 * r + 0.15 * (np.mean(elites, axis=0))\n                    # small gaussian jitter\n                    r += 0.02 * norm_scale * self.rng.randn(self.dim)\n                except Exception:\n                    # fallback small gaussian\n                    r = anchor + 0.05 * norm_scale * self.rng.randn(self.dim)\n                cand = r\n\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                # anisotropic multipliers: lognormal around 0\n                logs = self.rng.randn(self.dim) * 0.25\n                multipliers = np.exp(logs)\n                sig = self.gscale * norm_scale * multipliers\n                r = anchor + sig * self.rng.randn(self.dim)\n                # ensure at least one coordinate changes\n                if np.all(np.abs(r - anchor) < 1e-12):\n                    j = self.rng.randint(self.dim)\n                    r[j] += 1e-6\n                # occasional mixed uniform per-dim\n                if self.rng.rand() < 0.03:\n                    mask = self.rng.rand(self.dim) < 0.2\n                    if np.any(mask):\n                        r[mask] = self._uniform_array(lb[mask], ub[mask]) if mask.sum() == 1 else (lb + self.rng.rand(mask.sum()) * (ub - lb)[mask])\n                cand = r\n\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                r = anchor.copy()\n                # sometimes add directed differential component if donors exist\n                if donor_vals is not None and self.rng.rand() < 0.6:\n                    xr1, xr2, xr3 = donor_vals\n                    r += 0.12 * (xr1 - xr2)\n                r += self._tempered_cauchy(self.gscale * norm_scale)\n                cand = r\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.02 and len(archive) > 0:\n                cand = 0.9 * cand + 0.1 * (xbest + 0.02 * (ub - lb) * self.rng.randn(self.dim))\n\n            # sanitize candidate: replace nan/inf coordinates by uniform draws\n            cand = np.asarray(cand, dtype=float)\n            mask_bad = ~np.isfinite(cand)\n            if np.any(mask_bad):\n                cand[mask_bad] = self._uniform_array(lb[mask_bad], ub[mask_bad]) if mask_bad.sum() == 1 else (lb[mask_bad] + self.rng.rand(mask_bad.sum()) * (ub - lb)[mask_bad])\n\n            # reflect to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n\n            # increment ages of others\n            for e in archive:\n                e['age'] += 1\n\n            # add to archive\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best:\n                best = float(f_cand)\n                xbest = cand.copy()\n                improved = True\n                stagn_counter = 0\n                last_improve_iter = iter_no\n            else:\n                stagn_counter += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n            if finite_fs.size > 0:\n                try:\n                    q25 = np.percentile(finite_fs, 25)\n                except Exception:\n                    q25 = np.min(finite_fs)\n            else:\n                q25 = np.inf\n            success = (np.isfinite(f_cand) and (f_cand <= q25)) or improved\n            # update sliding window\n            recent_successes.append(1 if success else 0)\n            if len(recent_successes) > self.win_len:\n                recent_successes.pop(0)\n\n            # short-term adaptation of global scale\n            if len(recent_successes) >= self.win_len:\n                sr = float(sum(recent_successes[-self.win_len:]) / float(self.win_len))\n                if sr < 0.18:\n                    # low success -> increase exploration\n                    self.gscale = min(self.gscale * (1.0 + 0.12), self.gscale_max)\n                elif sr > 0.4:\n                    # high success -> tighten\n                    self.gscale = max(self.gscale * 0.985, self.gscale_min)\n                else:\n                    # slight decay towards stability\n                    self.gscale *= (1.0 - 0.003)\n                    self.gscale = np.clip(self.gscale, self.gscale_min, self.gscale_max)\n            else:\n                # small jitter to avoid lock\n                self.gscale *= (1.0 + (self.rng.randn() * 0.001))\n                self.gscale = np.clip(self.gscale, self.gscale_min, self.gscale_max)\n\n            # prune archive when too large (keep best + some random + oldest)\n            if len(archive) > self.max_archive:\n                kbest = max(10, 2 * self.dim)\n                archive_sorted = sorted(archive, key=lambda e: e['f'])\n                keep = []\n                keep.extend(archive_sorted[:kbest])  # keep best\n                rest = archive_sorted[kbest:]\n                nrand = min(len(rest), max(0, int(0.08 * len(archive))))\n                if nrand > 0 and len(rest) > 0:\n                    sel_idx = self.rng.choice(len(rest), size=nrand, replace=False)\n                    for i in sel_idx:\n                        keep.append(rest[i])\n                remaining = [e for e in archive if e not in keep]\n                # keep some oldest from the remaining\n                if len(remaining) > 0:\n                    ages_rest = np.array([e['age'] for e in remaining])\n                    nold = min(6, len(remaining))\n                    sel_age_idx = np.argsort(-ages_rest)[:nold]\n                    for s in sel_age_idx:\n                        keep.append(remaining[s])\n                # compact and deduplicate by proximity (simple hashing)\n                seen = set()\n                unique = []\n                for e in keep:\n                    key = tuple(np.round(e['x'], decimals=6))\n                    if key not in seen:\n                        seen.add(key)\n                        unique.append(e)\n                archive = unique[: self.max_archive]\n\n            # stagnation detection & micro-restarts\n            if stagn_counter >= self.stagn_limit and evals < budget:\n                # micro-restart: local cloud around current best (or random center if none)\n                if np.isfinite(best):\n                    center = xbest.copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                cloud_size = min(60, max(8, (budget - evals) // 6))\n                cloud_sigma = np.maximum(0.6 * self.gscale * norm_scale, 0.02 * (ub - lb))\n                ncloud = min(cloud_size, max(2, (budget - evals) // 3))\n                for _ in range(ncloud):\n                    if evals >= budget:\n                        break\n                    p = center + self.rng.randn(self.dim) * cloud_sigma\n                    p = self._reflect_bounds(p, lb, ub)\n                    f_p = safe_eval(p)\n                    archive.append({'x': p.copy(), 'f': float(f_p), 'age': 0})\n                    if np.isfinite(f_p) and f_p < best:\n                        best = float(f_p)\n                        xbest = p.copy()\n                        last_improve_iter = iter_no\n                # also inject some global randoms\n                for _ in range(min(6, budget - evals)):\n                    if evals >= budget:\n                        break\n                    rx = self._uniform_array(lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < best:\n                        best = float(fr)\n                        xbest = rx.copy()\n                        last_improve_iter = iter_no\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.5, self.gscale_max)\n                stagn_counter = 0\n                recent_successes = [0] * self.win_len\n                if self.verbose:\n                    print(f\"[MG-ADS] micro-restart at iter {iter_no}, evals {evals}, best {best:.4e}\")\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or evals == budget):\n                print(f\"[MG-ADS] iter {iter_no}, evals {evals}, best {best:.4e}, gscale {self.gscale:.3e}\")\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries = sorted(finite_entries, key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        # all infinite: return a uniform fallback point in the domain center\n        x_fallback = (lb + ub) / 2.0\n        return np.inf, x_fallback", "configspace": "", "generation": 0, "feedback": "In the code, line 449, in <listcomp>, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: remaining = [e for e in archive if e not in keep]", "error": "In the code, line 449, in <listcomp>, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: remaining = [e for e in archive if e not in keep]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ad8d91c1-7193-4877-925f-c871977fe780", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style recombination, PCA-guided elite moves, anisotropic local search and tempered-Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=False)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.verbose = verbose\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.30, 0.20, 0.40, 0.10], dtype=float)\n\n        # some internal hyperparameters\n        self.max_archive = max(200 + 20 * self.dim, 400)\n        self.k_elite_ratio = 0.20\n        self.gscale = 0.2  # global exploration multiplier\n        self.gscale_min = 1e-4\n        self.gscale_max = 50.0\n\n        # short-term adaptation window\n        self.win_len = 40\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try common possible attributes from benchmark wrappers, else fallback [-5,5]\n        lb = None\n        ub = None\n\n        # Common patterns: func.bounds.lb/ub, func.lb/ub, func.lower/upper, attributes in dict\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = np.asarray(getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None))\n                ub = np.asarray(getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None))\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None:\n            try:\n                lb = np.asarray(getattr(func, \"lb\", None))\n            except Exception:\n                lb = None\n        if ub is None:\n            try:\n                ub = np.asarray(getattr(func, \"ub\", None))\n            except Exception:\n                ub = None\n\n        # fallback to scalar attributes\n        if lb is None or ub is None:\n            try:\n                lb_s = getattr(func, \"lower\", None) or getattr(func, \"min\", None)\n                ub_s = getattr(func, \"upper\", None) or getattr(func, \"max\", None)\n                if lb_s is not None and ub_s is not None:\n                    lb = np.asarray(lb_s, dtype=float)\n                    ub = np.asarray(ub_s, dtype=float)\n            except Exception:\n                lb = None\n                ub = None\n\n        # final fallback to [-5,5]^dim\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb[:self.dim]\n        ub = ub[:self.dim]\n\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # mirror-reflection for any coordinate outside bounds; repeat a few times\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            if np.all((x >= lb) & (x <= ub)):\n                break\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        shape = (n, self.dim) if n != 1 else (self.dim,)\n        draws = self.rng.rand(*shape) * (ub - lb) + lb\n        return draws\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF\n        scale_vec = np.maximum(np.asarray(scale_vec, dtype=float), 1e-12)\n        u = self.rng.rand(self.dim)\n        vals = scale_vec * np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplicative factor per-coordinate\n        heavy_mask = self.rng.rand(self.dim) < 0.02\n        if np.any(heavy_mask):\n            vals[heavy_mask] *= (1.0 + 10.0 * self.rng.rand(np.sum(heavy_mask)))\n        # small gaussian jitter to break symmetry\n        vals += 0.01 * scale_vec * self.rng.randn(self.dim)\n        # cap extremes to avoid overflow\n        max_abs = cap_multiplier * scale_vec\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n\n        evals = 0\n        budget = int(self.budget)\n\n        def safe_eval(x):\n            nonlocal evals\n            # do not call if budget exhausted\n            if evals >= budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            if x.ndim == 2 and x.shape[0] == 1:\n                x = x[0]\n            try:\n                y = float(func(x))\n            except Exception:\n                # try convert vector to list/tuple\n                try:\n                    y = float(func(np.asarray(x)))\n                except Exception:\n                    y = np.inf\n            evals += 1\n            return y\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        n_init = int(min(max(10, 3 * self.dim), max(5, budget // 10)))\n        X_init = np.zeros((n_init, self.dim))\n        # LHS-like: for each dim shuffle strata\n        for d in range(self.dim):\n            perm = np.arange(n_init)\n            self.rng.shuffle(perm)\n            u = self.rng.rand(n_init)\n            strata = (perm + u) / float(n_init)\n            X_init[:, d] = strata\n\n        # map to bounds\n        X_init = lb + X_init * (ub - lb)\n\n        # a few extra uniform samples\n        n_extra = min(5, max(0, budget - n_init))\n        X_extra = self._uniform_array(lb, ub, n=n_extra) if n_extra > 0 else np.zeros((0, self.dim))\n        if n_extra == 1:\n            X_extra = X_extra.reshape(1, -1)\n        X = np.vstack([X_init, X_extra]) if X_extra.shape[0] > 0 else X_init.copy()\n\n        # evaluate initial population until budget exhausted\n        archive = []\n        for xi in X:\n            if evals >= budget:\n                break\n            fxi = safe_eval(xi)\n            archive.append({'x': np.asarray(xi, dtype=float).copy(), 'f': float(fxi), 'age': 0})\n\n        # If nothing evaluated (e.g., budget==0) return fallback\n        if len(archive) == 0:\n            x_fallback = (lb + ub) / 2.0\n            return np.inf, x_fallback\n\n        # initial best\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            xbest = sorted(finite_entries, key=lambda e: e['f'])[0]['x'].copy()\n            best = float(sorted(finite_entries, key=lambda e: e['f'])[0]['f'])\n        else:\n            best = np.inf\n            xbest = archive[0]['x'].copy()\n\n        # history for adaptation\n        recent_successes = [0] * self.win_len\n        strat_success = np.ones(4, dtype=float)  # successes per strategy\n        strat_tries = np.ones(4, dtype=float)\n\n        stagn_counter = 0\n        last_improve_iter = 0\n\n        iter_no = 0\n\n        # main loop\n        while evals < budget:\n            iter_no += 1\n\n            # convert archive arrays\n            Xa = np.array([e['x'] for e in archive])\n            Fa = np.array([e['f'] for e in archive])\n            ages = np.array([e.get('age', 0) for e in archive])\n\n            finite_mask = np.isfinite(Fa)\n            finite_X = Xa[finite_mask] if np.any(finite_mask) else np.empty((0, self.dim))\n            finite_F = Fa[finite_mask] if np.any(finite_mask) else np.array([])\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            if finite_X.shape[0] >= 2:\n                q25 = np.percentile(finite_X, 25, axis=0)\n                q75 = np.percentile(finite_X, 75, axis=0)\n                scale = (q75 - q25) / 1.349  # approx robust std\n                small = scale < 1e-8\n                if np.any(small):\n                    sd = np.std(finite_X, axis=0)\n                    scale[small] = sd[small]\n                if np.all(scale == 0):\n                    scale = np.maximum(np.std(Xa, axis=0), 1e-6)\n            else:\n                # not enough data: use domain scale\n                scale = 0.3 * (ub - lb)\n\n            # determine elites (by f)\n            n_elite = max(3, int(np.ceil(self.k_elite_ratio * len(archive))))\n            finite_idx = np.where(finite_mask)[0]\n            if finite_idx.size > 0:\n                sorted_idx = finite_idx[np.argsort(finite_F)]\n            else:\n                sorted_idx = np.argsort(Fa)\n            elite_idx = sorted_idx[:min(n_elite, len(sorted_idx))]\n            if elite_idx.size == 0:\n                # pick some random indices\n                elite_idx = np.array(self.rng.choice(len(archive), size=min(3, len(archive)), replace=False))\n\n            elites = Xa[elite_idx]\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            # preference vector\n            ranks = np.arange(len(elite_idx))\n            weights = np.exp(-0.6 * ranks)\n            weights = weights / np.sum(weights)\n            anchor_idx = self.rng.choice(elite_idx, p=weights)\n            anchor = Xa[anchor_idx].copy()\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            probs[3] = probs[3] + 0.15 * (min(self.gscale, 5.0) / 5.0)  # boost Cauchy with gscale\n            probs = probs / np.sum(probs)\n\n            # pick strategy\n            strat = self.rng.choice(4, p=probs)\n            strat_tries[strat] += 1.0\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                idxs = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in idxs:\n                    idxs.remove(exclude_idx)\n                if len(idxs) == 0:\n                    return [0] * k\n                if len(idxs) >= k:\n                    return list(self.rng.choice(idxs, size=k, replace=False))\n                else:\n                    return list(self.rng.choice(idxs, size=k, replace=True))\n\n            # compute a per-dim normalized scale for moves\n            norm_scale = np.maximum(scale, 1e-12)\n\n            cand = None\n            CR = 0.9  # crossover prob for DE-like\n            F = 0.6   # differential weight\n\n            if strat == 0:\n                # Strategy 0: DE-style differential injection (anchor / current-to-rand variant)\n                ids = pick_distinct(3, exclude_idx=anchor_idx)\n                xr1 = Xa[ids[0]].copy()\n                xr2 = Xa[ids[1]].copy()\n                xr3 = Xa[ids[2]].copy()\n                r = anchor.copy()\n                # differential term\n                r += F * (xr1 - xr2)\n                # small gaussian anisotropic jitter\n                r += 0.02 * norm_scale * self.rng.randn(self.dim)\n                # binomial crossover with a random elite\n                mask = self.rng.rand(self.dim) < CR\n                if not np.any(mask):\n                    mask[self.rng.randint(self.dim)] = True\n                trial = anchor.copy()\n                trial[mask] = r[mask]\n                # occasional tempered cauchy injection\n                if self.rng.rand() < 0.07:\n                    trial += 0.6 * self._tempered_cauchy(norm_scale * self.gscale)\n                cand = trial\n\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                E = elites - np.mean(elites, axis=0)\n                if E.shape[0] >= 2:\n                    # compute PCA / SVD\n                    try:\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        pcs = Vt  # principal directions\n                        # sample coefficients scaled by singular values and gscale\n                        kcomp = min(self.dim, max(1, pcs.shape[0]))\n                        coeffs = (self.rng.randn(kcomp) * (S[:kcomp] / (np.max(S[:kcomp]) + 1e-12))) * (1.5 + self.gscale)\n                        delta = coeffs @ pcs[:kcomp, :]\n                        r = anchor.copy() + delta\n                        # blend slightly towards anchor mean of elites\n                        r = 0.85 * r + 0.15 * np.mean(elites, axis=0)\n                        r += 0.02 * norm_scale * self.rng.randn(self.dim)\n                        cand = r\n                    except Exception:\n                        # fallback to Gaussian jitter\n                        cand = anchor + 0.08 * norm_scale * self.rng.randn(self.dim)\n                else:\n                    cand = anchor + 0.08 * norm_scale * self.rng.randn(self.dim)\n\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                r = anchor.copy()\n                # anisotropic per-dim sigma proportional to spread but smaller\n                local_sigma = 0.08 * (norm_scale + 1e-12) * (1.0 + 0.5 * self.gscale)\n                r += local_sigma * self.rng.randn(self.dim)\n                # ensure at least one coord perturbed\n                if np.allclose(r, anchor):\n                    j = self.rng.randint(self.dim)\n                    r[j] += 1e-6\n                # occasional mixed uniform per-dim injection\n                if self.rng.rand() < 0.03:\n                    mix = (self.rng.rand(self.dim) < 0.2)\n                    if np.any(mix):\n                        r[mix] = self._uniform_array(lb[mix], ub[mix])\n                cand = r\n\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                r = anchor.copy()\n                r += self._tempered_cauchy(norm_scale * (1.2 + self.gscale))\n                # sometimes add directed differential component if donors exist\n                if self.rng.rand() < 0.2 and len(archive) >= 3:\n                    ids = pick_distinct(3, exclude_idx=None)\n                    xr1 = Xa[ids[0]].copy()\n                    xr2 = Xa[ids[1]].copy()\n                    r += 0.12 * (xr1 - xr2)\n                cand = r\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.03:\n                cand = 0.9 * cand + 0.1 * xbest\n\n            # sanitize candidate: replace nan/inf coordinates by uniform draws\n            cand = np.asarray(cand, dtype=float)\n            mask_bad = ~np.isfinite(cand)\n            if np.any(mask_bad):\n                cand[mask_bad] = self._uniform_array(lb[mask_bad], ub[mask_bad])\n\n            # reflect to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n\n            # increment ages of others\n            for e in archive:\n                e['age'] = e.get('age', 0) + 1\n\n            # update archive\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best:\n                best = float(f_cand)\n                xbest = cand.copy()\n                stagn_counter = 0\n                last_improve_iter = iter_no\n                improved = True\n            else:\n                stagn_counter += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n            success = False\n            if finite_fs.size > 0 and np.isfinite(f_cand):\n                try:\n                    q25 = np.percentile(finite_fs, 25)\n                except Exception:\n                    q25 = np.min(finite_fs)\n                success = (f_cand <= q25) or improved\n\n            # update sliding window & per-strategy stats\n            recent_successes.pop(0)\n            recent_successes.append(1 if success else 0)\n            if success:\n                strat_success[strat] += 1.0\n\n            # short-term adaptation of global scale based on recent success rate\n            sr = float(np.mean(recent_successes))\n            if sr < 0.15:\n                # low success -> increase exploration\n                self.gscale = min(self.gscale * (1.0 + 0.12), self.gscale_max)\n            elif sr > 0.6:\n                # high success -> tighten\n                self.gscale = max(self.gscale * 0.985, self.gscale_min)\n            else:\n                # small decay to stabilize\n                self.gscale = max(self.gscale * 0.998, self.gscale_min)\n\n            # prune archive when too large (keep best + some random + oldest)\n            if len(archive) > self.max_archive:\n                kbest = max(10, 2 * self.dim)\n                archive_sorted = sorted(archive, key=lambda e: e['f'])\n                keep = []\n                keep.extend(archive_sorted[:kbest])  # keep best\n                rest = archive_sorted[kbest:]\n                if len(rest) > 0:\n                    nrand = min(len(rest), max(0, int(0.05 * self.max_archive)))\n                    if nrand > 0:\n                        sel_idx = self.rng.choice(len(rest), size=nrand, replace=False)\n                        for i in sel_idx:\n                            keep.append(rest[i])\n                    # keep some oldest from the remaining\n                    ages_rest = np.array([e['age'] for e in rest])\n                    if ages_rest.size > 0:\n                        sel_age_idx = np.argsort(-ages_rest)[:min(40, ages_rest.size)]\n                        for s in sel_age_idx:\n                            keep.append(rest[s])\n                # compact and deduplicate by proximity (simple)\n                unique = []\n                seen = set()\n                for e in keep:\n                    key = tuple(np.round(e['x'], 8))\n                    if key not in seen:\n                        unique.append(e)\n                        seen.add(key)\n                archive = unique\n\n            # stagnation detection & micro-restarts\n            if (iter_no - last_improve_iter) > max(200, 20 * self.dim) or stagn_counter > (100 + 10 * self.dim):\n                # micro-restart: local cloud around current best (or random center if best is inf)\n                if np.isfinite(best):\n                    center = xbest.copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                cloud_size = min(40, max(6, budget - evals))\n                # cloud sigma related to gscale and domain\n                cloud_sigma = np.maximum(np.std(Xa, axis=0) if Xa.size else (0.1 * (ub - lb)),\n                                        0.05 * (ub - lb)) * (1.0 + self.gscale)\n                cloud_sigma = np.minimum(cloud_sigma, (ub - lb))\n                ncloud = int(cloud_size)\n                for _ in range(ncloud):\n                    if evals >= budget:\n                        break\n                    perturb = center + (0.5 + self.rng.rand(self.dim)) * cloud_sigma * self.rng.randn(self.dim)\n                    p = self._reflect_bounds(perturb, lb, ub)\n                    f_p = safe_eval(p)\n                    archive.append({'x': p.copy(), 'f': float(f_p), 'age': 0})\n                    if np.isfinite(f_p) and f_p < best:\n                        best = float(f_p)\n                        xbest = p.copy()\n                        last_improve_iter = iter_no\n                # also inject some global randoms\n                nrand = min(8, max(1, int(4 + 2 * self.dim / 10)))\n                for _ in range(nrand):\n                    if evals >= budget:\n                        break\n                    rx = self._uniform_array(lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < best:\n                        best = float(fr)\n                        xbest = rx.copy()\n                        last_improve_iter = iter_no\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.3 + 0.02, self.gscale_max)\n                recent_successes = [0] * self.win_len\n                last_improve_iter = iter_no\n                stagn_counter = 0\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or evals == budget):\n                finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n                if finite_fs.size > 0:\n                    print(f\"[MG-ADS] iter={iter_no} evals={evals} best={np.min(finite_fs):.6g} gscale={self.gscale:.4g}\")\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries = sorted(finite_entries, key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        # all infinite: return a uniform fallback point in the domain center\n        x_fallback = (lb + ub) / 2.0\n        return np.inf, x_fallback", "configspace": "", "generation": 0, "feedback": "In the code, line 357, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 2 input values to the 1 output values where the mask is true\nOn line: r[mix] = self._uniform_array(lb[mix], ub[mix])", "error": "In the code, line 357, in __call__, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 2 input values to the 1 output values where the mask is true\nOn line: r[mix] = self._uniform_array(lb[mix], ub[mix])", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5e591ddf-2e7a-4cc6-8c94-8a415ec3403b", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler mixing DE-style recombination, PCA-guided elite moves, anisotropic local search, tempered-Cauchy escapes, short-term scale adaptation and micro-restarts (Memory-Guided ADS).", "code": "import numpy as np\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=False)\n        fbest, xbest = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False, max_archive=600):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.verbose = bool(verbose)\n\n        # Short-term adaptation window\n        self.win_len = 40\n\n        # global exploration scale (adapted)\n        self.gscale = 1.0\n        self.gscale_min = 0.05\n        self.gscale_max = 8.0\n\n        # archive capacity\n        self.max_archive = int(max_archive)\n\n        # default base strategy probabilities: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.35, 0.20, 0.35, 0.10], dtype=float)\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try common patterns for bounds, else default [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # b might be an object with lb/ub or a tuple/list\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb = np.asarray(b.lb, dtype=float)\n                    ub = np.asarray(b.ub, dtype=float)\n                elif isinstance(b, (list, tuple)) and len(b) == 2:\n                    lb = np.asarray(b[0], dtype=float)\n                    ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            # try other common attr names\n            try:\n                lb_candidate = getattr(func, \"lower\", None) or getattr(func, \"lb\", None) or getattr(func, \"min\", None)\n                ub_candidate = getattr(func, \"upper\", None) or getattr(func, \"ub\", None) or getattr(func, \"max\", None)\n                if lb_candidate is not None and ub_candidate is not None:\n                    lb = np.asarray(lb_candidate, dtype=float)\n                    ub = np.asarray(ub_candidate, dtype=float)\n            except Exception:\n                lb = lb\n                ub = ub\n\n        # final fallback: scalar bounds or [-5,5]\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n\n        # if scalars, expand\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n\n        # ensure correct length\n        if lb.size < self.dim:\n            lb = np.concatenate([lb, np.full(self.dim - lb.size, lb[-1])])\n        if ub.size < self.dim:\n            ub = np.concatenate([ub, np.full(self.dim - ub.size, ub[-1])])\n\n        # ensure lb <= ub per coordinate (swap if needed)\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n\n        return lb[:self.dim].astype(float), ub[:self.dim].astype(float)\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # mirror-reflection for any coordinate outside bounds; repeat a few times\n        x = x.astype(float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp to be safe\n        np.clip(x, lb, ub, out=x)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        draws = self.rng.rand(n, self.dim) * (ub - lb) + lb\n        if n == 1:\n            return draws.reshape(-1)\n        return draws\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF transform\n        u = self.rng.rand(self.dim)\n        vals = scale_vec * np.tan(np.pi * (u - 0.5))\n        # occasional heavy multiplicative factor per-coordinate\n        heavy_mask = self.rng.rand(self.dim) < 0.02\n        if np.any(heavy_mask):\n            vals[heavy_mask] *= (2.0 + 5.0 * self.rng.rand(np.sum(heavy_mask)))\n        # small gaussian jitter to break symmetry\n        vals += 0.01 * scale_vec * self.rng.randn(self.dim)\n        # cap extremes to avoid overflow\n        max_abs = cap_multiplier * scale_vec\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        evals = 0\n        budget = int(self.budget)\n        dim = self.dim\n\n        lb, ub = self._get_bounds(func)\n\n        # safe evaluation closure\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                # do not call function anymore\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            if x.ndim == 2 and x.shape[0] == 1:\n                x = x[0]\n            try:\n                y = float(func(x))\n            except Exception:\n                # try converting to python list\n                try:\n                    y = float(func(np.asarray(x)))\n                except Exception:\n                    y = np.inf\n            evals += 1\n            return y\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        n_init = min(max(12, 4 * dim), budget)\n        # create LHS-like matrix\n        X_init = np.zeros((n_init, dim), dtype=float)\n        for d in range(dim):\n            perm = np.arange(n_init)\n            self.rng.shuffle(perm)\n            u = self.rng.rand(n_init)\n            strata = (perm + u) / float(n_init)\n            X_init[:, d] = strata\n        # map to bounds\n        X_init = lb + X_init * (ub - lb)\n\n        # a few extra uniform samples\n        n_extra = min(5, max(0, budget - n_init))\n        if n_extra > 0:\n            X_extra = self._uniform_array(lb, ub, n_extra)\n            if n_extra == 1:\n                X_extra = X_extra.reshape(1, -1)\n            X = np.vstack([X_init, X_extra])\n        else:\n            X = X_init.copy()\n\n        # evaluate initial population until budget exhausted\n        archive = []\n        for xi in X:\n            if evals >= budget:\n                break\n            fxi = safe_eval(xi)\n            archive.append({'x': xi.copy(), 'f': float(fxi), 'age': 0})\n        if len(archive) == 0:\n            # nothing evaluated; return fallback center\n            x_fallback = 0.5 * (lb + ub)\n            return np.inf, x_fallback\n\n        # initial best\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries.sort(key=lambda e: e['f'])\n            best = float(finite_entries[0]['f'])\n            xbest = finite_entries[0]['x'].copy()\n        else:\n            best = np.inf\n            xbest = 0.5 * (lb + ub)\n\n        # history for adaptation\n        recent_successes = [0] * self.win_len\n        strat_success = np.ones(4, dtype=float)  # successes per strategy\n\n        stagn_counter = 0\n        last_improve_iter = 0\n        iter_no = 0\n\n        # main loop\n        while evals < budget:\n            iter_no += 1\n\n            # convert archive to arrays\n            Xa = np.array([e['x'] for e in archive])\n            Fa = np.array([e['f'] for e in archive])\n\n            finite_mask = np.isfinite(Fa)\n            finite_X = Xa[finite_mask] if np.any(finite_mask) else np.empty((0, dim))\n            finite_F = Fa[finite_mask] if np.any(finite_mask) else np.array([])\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            domain_scale = (ub - lb)\n            if finite_X.shape[0] >= 4:\n                q25 = np.percentile(finite_X, 25, axis=0)\n                q75 = np.percentile(finite_X, 75, axis=0)\n                scale = (q75 - q25) / 1.349  # approx robust std\n                small = scale < 1e-8\n                if np.any(small):\n                    sd = np.std(finite_X, axis=0)\n                    scale[small] = sd[small]\n                # if still very small, fallback to domain scale fraction\n                very_small = scale < (1e-8)\n                if np.any(very_small):\n                    scale[very_small] = 0.05 * domain_scale[very_small]\n            elif finite_X.shape[0] >= 2:\n                scale = np.std(finite_X, axis=0)\n                scale[scale < 1e-8] = 0.05 * domain_scale[scale < 1e-8]\n            else:\n                scale = 0.25 * domain_scale\n\n            # determine elites (by f)\n            finite_idx = np.where(finite_mask)[0]\n            if finite_idx.size > 0:\n                sorted_idx = finite_idx[np.argsort(finite_F)]\n            else:\n                sorted_idx = np.array([], dtype=int)\n            n_elite = max(2, min(20, int(max(2, 0.15 * max(3, len(sorted_idx))))))\n            if sorted_idx.size > 0:\n                elite_idx = sorted_idx[:min(n_elite, len(sorted_idx))]\n            else:\n                # if no finite values, use full archive\n                elite_idx = np.arange(min(len(archive), n_elite), dtype=int)\n\n            # choose an anchor among elites with exponential bias\n            if elite_idx.size > 0:\n                ranks = np.arange(len(elite_idx))[::-1]  # best have higher rank weight\n                weights = np.exp(1.5 * (len(elite_idx) - np.arange(len(elite_idx))))\n                weights = weights / np.sum(weights)\n                try:\n                    anchor_idx = self.rng.choice(elite_idx, p=weights)\n                except Exception:\n                    anchor_idx = int(elite_idx[0])\n            else:\n                anchor_idx = self.rng.randint(len(archive))\n\n            anchor = archive[anchor_idx]['x'].copy()\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            probs[3] = probs[3] + 0.15 * (min(self.gscale, 5.0) / 5.0)\n            probs = np.maximum(probs, 1e-8)\n            probs = probs / np.sum(probs)\n\n            # pick strategy\n            strat = int(self.rng.choice(4, p=probs))\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                idxs = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in idxs:\n                    idxs.remove(exclude_idx)\n                if len(idxs) == 0:\n                    return []\n                pick = []\n                for _ in range(k):\n                    if len(idxs) == 0:\n                        pick.append(self.rng.randint(len(archive)))\n                    else:\n                        sel = self.rng.choice(idxs)\n                        pick.append(int(sel))\n                        idxs.remove(sel)\n                return pick\n\n            # compute a per-dim normalized scale for moves\n            norm_scale = np.maximum(scale, 1e-12)\n\n            # generate candidate\n            cand = anchor.copy()\n\n            if strat == 0:\n                # Strategy 0: DE-style differential injection (anchor / current-to-rand variant)\n                ids = pick_distinct(3, exclude_idx=anchor_idx)\n                if len(ids) < 3:\n                    # fallback: simple jitter\n                    cand = anchor + 0.15 * self.gscale * norm_scale * self.rng.randn(dim)\n                else:\n                    xr1 = archive[ids[0]]['x']\n                    xr2 = archive[ids[1]]['x']\n                    xr3 = archive[ids[2]]['x']\n                    Fscale = 0.8 * (1.0 + 0.4 * (self.gscale - 1.0))\n                    r = anchor + Fscale * (xr2 - xr3)\n                    # small gaussian anisotropic jitter\n                    r += 0.02 * norm_scale * self.rng.randn(dim)\n                    # binomial crossover\n                    CR = 0.7\n                    mask = self.rng.rand(dim) < CR\n                    if not np.any(mask):\n                        mask[self.rng.randint(dim)] = True\n                    trial = anchor.copy()\n                    trial[mask] = r[mask]\n                    # occasional tempered cauchy injection to some coords\n                    if self.rng.rand() < 0.06:\n                        trial += self._tempered_cauchy(0.3 * (ub - lb) * (0.5 + self.gscale))\n                    cand = trial\n\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                if len(elite_idx) >= 2:\n                    elites = np.array([archive[i]['x'] for i in elite_idx])\n                    E = elites - np.mean(elites, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(E, full_matrices=False)\n                        kcomp = max(1, min(dim, int(max(1, min(dim, max(1, E.shape[0]//1)) / 1)))))\n                        # sample coefficients scaled by singular values and global scale\n                        coeffs = (self.rng.randn(kcomp) * (S[:kcomp] / (np.max(S[:kcomp]) + 1e-12))) * (0.8 + 0.6 * self.gscale)\n                        delta = Vt[:kcomp].T.dot(coeffs)\n                        r = anchor.copy() + delta\n                        # blend slightly towards mean of elites\n                        r = 0.85 * r + 0.15 * np.mean(elites, axis=0)\n                        cand = r + 0.02 * norm_scale * self.rng.randn(dim)\n                    except Exception:\n                        cand = anchor + 0.08 * norm_scale * self.rng.randn(dim)\n                else:\n                    cand = anchor + 0.08 * norm_scale * self.rng.randn(dim)\n\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                local_sigma = 0.08 * (0.5 + self.gscale) * norm_scale\n                r = anchor + local_sigma * self.rng.randn(dim)\n                # ensure at least one coord perturbed\n                if not np.any(np.abs(r - anchor) > 1e-12):\n                    j = self.rng.randint(dim)\n                    r[j] += local_sigma[j] * (0.5 + self.rng.randn())\n                # occasional mixed uniform per-dim injection\n                mix = (self.rng.rand(dim) < 0.12)\n                if np.any(mix):\n                    r[mix] = lb[mix] + (ub[mix] - lb[mix]) * self.rng.rand(np.sum(mix))\n                cand = r\n\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                scale_vec = 0.25 * (ub - lb) * (0.8 + 0.8 * min(self.gscale, 4.0))\n                r = anchor + self._tempered_cauchy(scale_vec)\n                # sometimes add directed differential component if donors exist\n                ids = pick_distinct(2, exclude_idx=anchor_idx)\n                if len(ids) >= 2:\n                    xr1 = archive[ids[0]]['x']\n                    xr2 = archive[ids[1]]['x']\n                    r += 0.12 * (xr1 - xr2)\n                cand = r\n\n            # occasional directed small jump to current best for refinement\n            if np.isfinite(best) and self.rng.rand() < 0.03:\n                cand = 0.6 * cand + 0.4 * xbest + 0.02 * norm_scale * self.rng.randn(dim)\n\n            # sanitize candidate: replace nan/inf coordinates by uniform draws\n            cand = np.asarray(cand, dtype=float)\n            mask_bad = ~np.isfinite(cand)\n            if np.any(mask_bad):\n                cand[mask_bad] = self._uniform_array(lb[mask_bad], ub[mask_bad], n=1).reshape(-1)[mask_bad]\n\n            # reflect to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(cand)\n\n            # increment ages of others\n            for e in archive:\n                e['age'] = e.get('age', 0) + 1\n\n            # update archive with candidate\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best:\n                best = float(f_cand)\n                xbest = cand.copy()\n                last_improve_iter = iter_no\n                improved = True\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n            if finite_fs.size > 0:\n                q25 = np.percentile(finite_fs, 25)\n            else:\n                q25 = np.inf\n            success = (np.isfinite(f_cand) and f_cand <= q25) or improved\n\n            # update sliding window & per-strategy stats\n            recent_successes.pop(0)\n            recent_successes.append(1 if success else 0)\n            if success:\n                strat_success[strat] += 1.0\n\n            # short-term adaptation of global scale based on recent success rate\n            sr = float(np.mean(recent_successes))\n            if sr < 0.15:\n                # low success -> increase exploration\n                self.gscale = min(self.gscale * (1.0 + 0.12), self.gscale_max)\n            elif sr > 0.4:\n                # high success -> tighten\n                self.gscale = max(self.gscale * 0.88, self.gscale_min)\n            # small decay to stabilize\n            self.gscale = max(self.gscale * 0.998, self.gscale_min)\n\n            # prune archive when too large (keep best + some random + some oldest)\n            if len(archive) > self.max_archive:\n                kbest = max(10, 4 * dim)\n                archive_sorted = sorted(archive, key=lambda e: e['f'])\n                keep = archive_sorted[:kbest]\n                rest = archive_sorted[kbest:]\n                # keep some random from rest\n                nrand = min(len(rest), max(0, int(0.05 * self.max_archive)))\n                if nrand > 0:\n                    rand_sel = self.rng.choice(len(rest), size=nrand, replace=False)\n                    for rsel in rand_sel:\n                        keep.append(rest[int(rsel)])\n                # keep some oldest\n                ages_rest = np.array([e['age'] for e in rest])\n                if ages_rest.size > 0:\n                    sel_age_idx = np.argsort(-ages_rest)[:min(40, ages_rest.size)]\n                    for s in sel_age_idx:\n                        keep.append(rest[int(s)])\n                # compact and deduplicate by proximity (simple)\n                unique = []\n                seen = []\n                for e in keep:\n                    x = e['x']\n                    is_dup = False\n                    for s in seen:\n                        if np.linalg.norm(x - s) < 1e-8:\n                            is_dup = True\n                            break\n                    if not is_dup:\n                        unique.append(e)\n                        seen.append(x.copy())\n                archive = unique\n\n            # stagnation detection & micro-restarts\n            stagn_counter += 1\n            if (iter_no - last_improve_iter) > max(200, 20 * dim) or stagn_counter > (150 + 10 * dim):\n                stagn_counter = 0\n                # micro-restart: local cloud around current best (or random center if best is inf)\n                if np.isfinite(best):\n                    center = xbest.copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                # cloud sigma related to gscale and domain\n                if Xa.size:\n                    cloud_sigma = np.maximum(np.std(Xa, axis=0), 0.05 * domain_scale) * (0.6 + 0.6 * self.gscale)\n                else:\n                    cloud_sigma = 0.1 * domain_scale * (0.6 + 0.6 * self.gscale)\n                cloud_size = min(40, max(6, int(6 + dim)))\n                ncloud = int(cloud_size)\n                for _ in range(ncloud):\n                    if evals >= budget:\n                        break\n                    perturb = center + (0.5 + self.rng.rand(self.dim)) * cloud_sigma * self.rng.randn(self.dim)\n                    perturb = self._reflect_bounds(perturb, lb, ub)\n                    f_p = safe_eval(perturb)\n                    archive.append({'x': perturb.copy(), 'f': float(f_p), 'age': 0})\n                    if np.isfinite(f_p) and f_p < best:\n                        xbest = perturb.copy()\n                        best = float(f_p)\n                        last_improve_iter = iter_no\n                # also inject some global randoms\n                nrand = min(8, max(1, int(4 + 2 * dim / 10)))\n                for _ in range(nrand):\n                    if evals >= budget:\n                        break\n                    rx = self._uniform_array(lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < best:\n                        xbest = rx.copy()\n                        best = float(fr)\n                        last_improve_iter = iter_no\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.3 + 0.02, self.gscale_max)\n                recent_successes = [0] * self.win_len\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or improved):\n                if np.any(finite_fs):\n                    med = np.median(finite_fs)\n                    if self.verbose:\n                        print(f\"iter={iter_no}, evals={evals}/{budget}, best={best:.6e}, median={med:.6e}, gscale={self.gscale:.3f}\")\n                else:\n                    if self.verbose:\n                        print(f\"iter={iter_no}, evals={evals}/{budget}, best={best:.6e}, gscale={self.gscale:.3f}\")\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries.sort(key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        # all infinite: return a uniform fallback point in the domain center\n        x_fallback = 0.5 * (lb + ub)\n        return np.inf, x_fallback", "configspace": "", "generation": 0, "feedback": "In the code, line 143, in evaluate, the following error occurred:\nSyntaxError: unmatched ')'\nOn line: dim = self.dim", "error": "In the code, line 143, in evaluate, the following error occurred:\nSyntaxError: unmatched ')'\nOn line: dim = self.dim", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ad9c0b56-4ebb-4fda-8407-cc2084391aed", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic local search and tempered-Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=False)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.verbose = verbose\n\n        # base probabilities for strategies: [DE, PCA, Local, Cauchy]\n        self.base_probs = np.array([0.35, 0.25, 0.25, 0.15], dtype=float)\n\n        # some internal hyperparameters\n        self.max_archive = max(200 + 20 * self.dim, 400)\n        self.k_elite_ratio = 0.20\n        self.gscale = 0.2  # global exploration multiplier\n        self.gscale_min = 1e-4\n        self.gscale_max = 20.0\n\n        # short-term adaptation window\n        self.adapt_window = max(30, 10 * self.dim)\n        self.recent_successes = deque(maxlen=self.adapt_window)\n\n        # DE params\n        self.F0 = 0.6\n        self.CR = 0.9\n\n        # stagnation / micro-restart\n        self.stagn_thresh = max(30, 10 * self.dim)\n        self.stagn_counter = 0\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        # Try to extract bounds from function attributes; otherwise default [-5,5]\n        lb = None\n        ub = None\n        # Common patterns\n        # try nested attributes\n        try:\n            b = getattr(func, \"bounds\", None)\n            if b is not None and hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n        except Exception:\n            pass\n\n        # flat attributes\n        if lb is None or ub is None:\n            lb_attr = getattr(func, \"lower\", None) or getattr(func, \"min\", None) or getattr(func, \"lb\", None)\n            ub_attr = getattr(func, \"upper\", None) or getattr(func, \"max\", None) or getattr(func, \"ub\", None)\n            if lb_attr is not None and ub_attr is not None:\n                lb = np.asarray(lb_attr, dtype=float)\n                ub = np.asarray(ub_attr, dtype=float)\n\n        # dict-like attribute\n        if lb is None or ub is None:\n            try:\n                if hasattr(func, \"__dict__\"):\n                    d = func.__dict__\n                    if \"lower\" in d and \"upper\" in d:\n                        lb = np.asarray(d[\"lower\"], dtype=float)\n                        ub = np.asarray(d[\"upper\"], dtype=float)\n            except Exception:\n                pass\n\n        # fallback to scalar bounds\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = np.asarray(lb, dtype=float).reshape(-1)\n        ub = np.asarray(ub, dtype=float).reshape(-1)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb[0]), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub[0]), dtype=float)\n        # ensure correct length\n        if lb.size < self.dim:\n            lb = np.concatenate([lb, np.full(self.dim - lb.size, lb[-1])])\n        if ub.size < self.dim:\n            ub = np.concatenate([ub, np.full(self.dim - ub.size, ub[-1])])\n\n        # ensure lb <= ub per coordinate\n        swap_mask = lb > ub\n        if np.any(swap_mask):\n            tmp = lb.copy()\n            lb[swap_mask] = ub[swap_mask]\n            ub[swap_mask] = tmp[swap_mask]\n\n        return lb[:self.dim], ub[:self.dim]\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        # mirror-reflection for any coordinate outside bounds; repeat a few times\n        x = np.asarray(x, dtype=float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp to be safe\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        shape = (n, self.dim) if n != 1 else (self.dim,)\n        draws = self.rng.rand(*shape) * (ub - lb) + lb\n        if n == 1:\n            return draws.reshape(1, -1)\n        return draws\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        # coordinate-wise tempered Cauchy via inverse CDF\n        # generate uniform and map via tan(pi*(u-0.5)), cap extremes\n        u = self.rng.rand(self.dim)\n        z = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n        # small gaussian jitter to break symmetry\n        z += 0.02 * self.rng.randn(self.dim)\n        max_abs = cap_multiplier * np.maximum(scale_vec, 1e-12)\n        z = np.clip(z * scale_vec, -max_abs, max_abs)\n        return z\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        budget = int(self.budget)\n        evals = 0\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            # try pass 1D vector; some wrappers expect shape (1,d)\n            try:\n                val = float(func(x))\n            except Exception:\n                try:\n                    val = float(func(x.reshape(1, -1)))\n                except Exception:\n                    val = np.inf\n            evals += 1\n            return val\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        n_init = min(max(10, 4 * self.dim), max(5, int(0.15 * max(1, budget))))\n        n_extra = min(5, max(0, budget - n_init))\n        X_init = np.zeros((n_init, self.dim))\n        # LHS-like: for each dim shuffle strata\n        for d in range(self.dim):\n            perm = np.arange(n_init)\n            self.rng.shuffle(perm)\n            u = self.rng.rand(n_init)\n            strata = (perm + u) / float(n_init)\n            X_init[:, d] = strata\n        # map to bounds\n        X_init = lb + X_init * (ub - lb)\n        # a few extra uniform samples\n        X_extra = self._uniform_array(lb, ub, n=n_extra) if n_extra > 0 else np.zeros((0, self.dim))\n\n        # evaluate initial population until budget exhausted\n        archive = []\n        for x in np.vstack([X_init, X_extra]):\n            f = safe_eval(x)\n            archive.append({'x': x.copy(), 'f': float(f), 'age': 0})\n            if evals >= budget:\n                break\n\n        if len(archive) == 0:\n            # budget 0: return center fallback\n            center = 0.5 * (lb + ub)\n            return float(np.inf), center.copy()\n\n        # initial best\n        finite = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite) > 0:\n            best_entry = min(finite, key=lambda e: e['f'])\n            best = float(best_entry['f'])\n            xbest = best_entry['x'].copy()\n        else:\n            best = np.inf\n            xbest = np.copy(0.5 * (lb + ub))\n\n        # history for adaptation\n        strat_success = np.zeros(4, dtype=float)\n        strat_counts = np.zeros(4, dtype=float)\n\n        last_improve_iter = 0\n        iter_no = 0\n\n        # main loop\n        while evals < budget:\n            iter_no += 1\n            # convert archive arrays\n            Xa = np.array([e['x'] for e in archive])\n            Fa = np.array([e['f'] for e in archive])\n            ages = np.array([e.get('age', 0) for e in archive])\n\n            finite_mask = np.isfinite(Fa)\n            finite_idx = np.where(finite_mask)[0]\n            finite_F = Fa[finite_mask] if np.any(finite_mask) else np.array([])\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            if len(finite_idx) >= 4:\n                finite_X = Xa[finite_mask]\n                q25 = np.percentile(finite_X, 25, axis=0)\n                q75 = np.percentile(finite_X, 75, axis=0)\n                iqr = (q75 - q25) / 1.349  # approx to std\n                scale = np.maximum(iqr, 1e-8)\n            else:\n                scale = np.maximum((ub - lb) / 6.0, 1e-8)\n\n            norm_scale = np.maximum(scale, 1e-12)\n\n            # determine elites (by f)\n            n_finite = len(finite_idx)\n            n_elite = max(2, int(np.ceil(self.k_elite_ratio * max(3, n_finite))))\n            if n_finite > 0:\n                sorted_idx = finite_idx[np.argsort(Fa[finite_mask])]\n                elite_idx = sorted_idx[:min(n_elite, len(sorted_idx))]\n                elites = Xa[elite_idx]\n            else:\n                elite_idx = np.arange(len(archive))\n                elites = Xa[elite_idx]\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            if len(elite_idx) > 1:\n                ranks = np.arange(len(elite_idx))\n                weights = np.exp(-0.5 * ranks / max(1, len(elite_idx) * 0.2))\n                weights = weights / np.sum(weights)\n                anchor_idx = self.rng.choice(elite_idx, p=weights)\n            else:\n                anchor_idx = self.rng.choice(len(archive))\n            anchor = archive[anchor_idx]['x'].copy()\n\n            # adapt probabilities slightly with gscale (favor Cauchy when large)\n            probs = self.base_probs.copy()\n            probs[3] = probs[3] + 0.15 * (min(self.gscale, 5.0) / 5.0)  # boost Cauchy with gscale\n            probs = probs / np.sum(probs)\n\n            # pick strategy\n            strat = self.rng.choice(4, p=probs)\n            strat_counts[strat] += 1.0\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                idxs = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in idxs:\n                    idxs.remove(exclude_idx)\n                if len(idxs) < k:\n                    return [self.rng.choice(idxs) for _ in range(k)]\n                sel = self.rng.choice(idxs, size=k, replace=False)\n                return list(sel)\n\n            candidate = anchor.copy()\n\n            # Strategy implementations\n            if strat == 0:\n                # Strategy 0: DE-style differential injection (anchor / current-to-rand variant)\n                donors = pick_distinct(2, exclude_idx=anchor_idx)\n                xr1 = archive[donors[0]]['x']\n                xr2 = archive[donors[1]]['x']\n                F = self.F0 * (1.0 + 0.8 * (self.gscale))  # scale with gscale\n                r = anchor + F * (xr1 - xr2)\n                # small gaussian anisotropic jitter\n                r += 0.06 * self.gscale * (norm_scale * self.rng.randn(self.dim))\n                # binomial crossover with a random elite\n                mask = self.rng.rand(self.dim) < self.CR\n                if not np.any(mask):\n                    mask[self.rng.randint(self.dim)] = True\n                trial = anchor.copy()\n                trial[mask] = r[mask]\n                # occasional tempered cauchy injection\n                if self.rng.rand() < 0.07:\n                    trial += self._tempered_cauchy(norm_scale * self.gscale)\n                candidate = trial\n\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                E = elites.copy()\n                if E.shape[0] >= 2:\n                    # center\n                    Em = E - np.mean(E, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(Em, full_matrices=False)\n                        pcs = Vt  # principal directions\n                        kcomp = max(1, min(self.dim, int(np.ceil(0.25 * self.dim))))\n                        # sample coefficients scaled by singular values and gscale\n                        coeffs = (self.rng.randn(kcomp) * (S[:kcomp] / (1.0 + np.arange(kcomp)))) * (0.8 * self.gscale)\n                        delta = coeffs @ pcs[:kcomp, :]\n                        r = anchor + delta\n                        # blend slightly towards anchor mean of elites\n                        r = 0.85 * r + 0.15 * np.mean(elites, axis=0)\n                        candidate = r\n                    except Exception:\n                        candidate = anchor + 0.05 * (norm_scale * self.rng.randn(self.dim))\n                else:\n                    # fallback to Gaussian jitter\n                    candidate = anchor + 0.12 * (norm_scale * self.gscale * self.rng.randn(self.dim))\n\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                sigma = 0.08 * (1.0 + self.gscale) * norm_scale\n                r = anchor + sigma * self.rng.randn(self.dim)\n                # ensure at least one coord perturbed\n                if np.allclose(r, anchor):\n                    r[self.rng.randint(self.dim)] += sigma[self.rng.randint(self.dim)]\n                # occasional mixed uniform per-dim injection\n                prob_mix = 0.03 + 0.02 * self.gscale\n                mix_mask = self.rng.rand(self.dim) < prob_mix\n                if np.any(mix_mask):\n                    r[mix_mask] = self.rng.rand(np.sum(mix_mask)) * (ub[mix_mask] - lb[mix_mask]) + lb[mix_mask]\n                candidate = r\n\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                r = anchor.copy()\n                # tempering scale uses norm_scale and gscale\n                scale_vec = norm_scale * (0.5 + 2.0 * self.gscale)\n                r += self._tempered_cauchy(scale_vec)\n                # sometimes add directed differential component if donors exist\n                if len(archive) >= 3 and self.rng.rand() < 0.35:\n                    d_ids = pick_distinct(2, exclude_idx=anchor_idx)\n                    r += 0.5 * self.F0 * (archive[d_ids[0]]['x'] - archive[d_ids[1]]['x'])\n                candidate = r\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.03 and np.isfinite(best):\n                candidate = 0.7 * candidate + 0.3 * xbest\n\n            # sanitize candidate: replace nan/inf coordinates by uniform draws\n            mask_bad = ~np.isfinite(candidate)\n            if np.any(mask_bad):\n                candidate[mask_bad] = lb[mask_bad] + self.rng.rand(np.sum(mask_bad)) * (ub[mask_bad] - lb[mask_bad])\n\n            # reflect to bounds\n            candidate = self._reflect_bounds(candidate, lb, ub)\n\n            # evaluate candidate\n            f_cand = safe_eval(candidate)\n\n            # increment ages of others\n            for e in archive:\n                e['age'] = e.get('age', 0) + 1\n\n            # update archive\n            archive.append({'x': candidate.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best:\n                best = float(f_cand)\n                xbest = candidate.copy()\n                improved = True\n                last_improve_iter = iter_no\n                self.stagn_counter = 0\n            else:\n                self.stagn_counter += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            success = False\n            finite_vals = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n            if finite_vals.size > 0:\n                q25 = np.percentile(finite_vals, 25)\n                if np.isfinite(f_cand) and f_cand <= q25:\n                    success = True\n            if improved:\n                success = True\n\n            # update sliding window & per-strategy stats\n            self.recent_successes.append(1.0 if success else 0.0)\n            if success:\n                strat_success[strat] += 1.0\n\n            # short-term adaptation of global scale based on recent success rate\n            if len(self.recent_successes) >= max(5, self.adapt_window // 6):\n                sr = float(np.mean(self.recent_successes))\n                if sr < 0.15:\n                    # low success -> increase exploration\n                    self.gscale = min(self.gscale * 1.12 + 0.01, self.gscale_max)\n                elif sr > 0.35:\n                    # high success -> tighten\n                    self.gscale = max(self.gscale * 0.88 - 0.005, self.gscale_min)\n\n            # prune archive when too large (keep best + some random + oldest)\n            if len(archive) > self.max_archive:\n                archive_sorted = sorted(archive, key=lambda e: (np.isfinite(e['f']) == False, e['f']))\n                kbest = max(10, 2 * self.dim)\n                keep = archive_sorted[:kbest]  # keep best\n                rest = archive_sorted[kbest:]\n                # keep some random from rest\n                nrand = min(len(rest), max(0, int(0.05 * self.max_archive)))\n                if nrand > 0:\n                    sel_idx = self.rng.choice(len(rest), size=nrand, replace=False)\n                    for i in sel_idx:\n                        keep.append(rest[i])\n                # keep some oldest from the remaining\n                rest2 = [e for e in rest if e not in keep]\n                if len(rest2) > 0:\n                    ages_rest = np.array([e['age'] for e in rest2])\n                    oldest_idx = np.argsort(-ages_rest)[:min(len(rest2), max(0, int(0.05 * self.max_archive)))]\n                    for i in oldest_idx:\n                        keep.append(rest2[i])\n                # compact and deduplicate by proximity (simple)\n                unique = []\n                seen = []\n                for e in keep:\n                    x = e['x']\n                    add = True\n                    for s in seen:\n                        if np.linalg.norm(x - s) < 1e-6:\n                            add = False\n                            break\n                    if add:\n                        unique.append(e)\n                        seen.append(x)\n                archive = unique\n\n            # stagnation detection & micro-restarts\n            if self.stagn_counter > self.stagn_thresh or (iter_no % max(1000, 50*self.dim) == 0 and iter_no > 0):\n                # micro-restart: local cloud around current best (or random center if best is inf)\n                if np.isfinite(best):\n                    center = xbest.copy()\n                else:\n                    center = lb + self.rng.rand(self.dim) * (ub - lb)\n                cloud_sigma = np.maximum((ub - lb) * 0.06 * (1.0 + self.gscale), 1e-8)\n                ncloud = min(8 + self.dim, max(4, int(0.05 * self.adapt_window)))\n                for _ in range(ncloud):\n                    perturb = center + (0.5 + self.rng.rand(self.dim)) * cloud_sigma * self.rng.randn(self.dim)\n                    perturb = self._reflect_bounds(perturb, lb, ub)\n                    p_f = safe_eval(perturb)\n                    archive.append({'x': perturb.copy(), 'f': float(p_f), 'age': 0})\n                    if np.isfinite(p_f) and p_f < best:\n                        # instant improvement update\n                        best = float(p_f)\n                        xbest = perturb.copy()\n                # also inject some global randoms\n                for _ in range(3):\n                    rx = lb + self.rng.rand(self.dim) * (ub - lb)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < best:\n                        best = float(fr)\n                        xbest = rx.copy()\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.2 + 0.02, self.gscale_max)\n                self.recent_successes.clear()\n                self.stagn_counter = 0\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or evals == budget):\n                print(\"Iter\", iter_no, \"evals\", evals, \"best\", best, \"gscale\", self.gscale)\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries = sorted(finite_entries, key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        # all infinite: return a uniform fallback point in the domain center\n        center = 0.5 * (lb + ub)\n        return float(np.inf), center.copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 35, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.recent_successes = deque(maxlen=self.adapt_window)", "error": "In the code, line 35, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.recent_successes = deque(maxlen=self.adapt_window)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5d825590-a27d-4bb5-b09b-d124e132a7d6", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic local search and tempered-Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=False)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.verbose = verbose\n\n        # population / archive sizing heuristics\n        self.pop_init = max(12, min(60, 4 * self.dim))  # initial samples\n        self.max_archive = max(200 + 20 * self.dim, 400)  # capacity before pruning\n\n        # short-term adaptation\n        self.gscale = 0.20  # global exploration multiplier (adapted)\n        self.gscale_min = 1e-5\n        self.gscale_max = 5.0\n\n        # sliding window for recent success\n        self.window_len = 60\n        self.recent_success = deque(maxlen=self.window_len)\n\n        # RNG\n        self.rng = np.random.default_rng(self.seed)\n\n        # differential weight default\n        self.F = 0.6\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func (common wrappers). Fall back to [-5,5]^dim.\n        Returns arrays lb, ub shape (dim,).\n        \"\"\"\n        lb = None\n        ub = None\n        # Try common attribute patterns non-exhaustively\n        try:\n            # e.g., func.bounds.lb / func.bounds.ub\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                lb = np.asarray(getattr(b, \"lb\", None) or getattr(b, \"lower\", None) or getattr(b, \"min\", None))\n                ub = np.asarray(getattr(b, \"ub\", None) or getattr(b, \"upper\", None) or getattr(b, \"max\", None))\n        except Exception:\n            pass\n        if lb is None or ub is None:\n            try:\n                lb = np.asarray(getattr(func, \"lb\", None))\n                ub = np.asarray(getattr(func, \"ub\", None))\n            except Exception:\n                lb = None\n                ub = None\n        if lb is None or ub is None:\n            # last resort: scalar attributes lower/upper\n            try:\n                lb_s = getattr(func, \"lower\", None) or getattr(func, \"min\", None)\n                ub_s = getattr(func, \"upper\", None) or getattr(func, \"max\", None)\n                if lb_s is not None and ub_s is not None:\n                    lb = np.asarray(lb_s, dtype=float)\n                    ub = np.asarray(ub_s, dtype=float)\n            except Exception:\n                pass\n        # final fallback\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb.item()))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub.item()))\n            # ensure shapes\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim)\n        # ensure lb <= ub\n        swap_mask = lb > ub\n        if np.any(swap_mask):\n            tmp = lb.copy()\n            lb[swap_mask] = ub[swap_mask]\n            ub[swap_mask] = tmp[swap_mask]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection for coordinates outside bounds. Repeat a few times\n        to ensure the vector is inside. If still outside, clamp.\n        \"\"\"\n        x = np.asarray(x, dtype=float).copy()\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            # after reflection some coords might still be out, proceed to next iteration\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            return self.rng.uniform(lb, ub, size=(n, self.dim))\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy via inverse CDF:\n        x = scale * tan(pi*(u-0.5))\n        Cap extremes to avoid overflow: max_abs = cap_multiplier * scale_vec\n        Add small gaussian jitter to break symmetry.\n        \"\"\"\n        scale_vec = np.asarray(scale_vec, dtype=float)\n        u = self.rng.random(self.dim)\n        vals = scale_vec * np.tan(np.pi * (u - 0.5))\n        max_abs = np.maximum(1e-12, cap_multiplier * np.maximum(np.abs(scale_vec), 1e-12))\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        # small gaussian jitter\n        vals += 1e-6 * (scale_vec + 1e-12) * self.rng.standard_normal(self.dim)\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_scale = ub - lb\n        if np.any(domain_scale <= 0):\n            # if degenerated bounds, fallback to [-5,5]\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n            domain_scale = ub - lb\n\n        evals = 0\n        budget = int(self.budget)\n        rng = self.rng\n\n        # wrapper to safely call func without exceeding budget\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            # if func expects list/tuple convert\n            try:\n                f = func(x)\n            except Exception:\n                try:\n                    f = func(x.tolist())\n                except Exception:\n                    f = np.inf\n            evals += 1\n            return float(f)\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        n_init = min(self.pop_init, budget) if budget > 0 else 0\n        X_init = np.zeros((n_init, self.dim))\n        if n_init > 0:\n            # for each dim create strata and sample randomly within strata\n            strata = np.arange(n_init)\n            for d in range(self.dim):\n                perm = rng.permutation(n_init)\n                # sample uniformly within each stratum [i/n, (i+1)/n)\n                u = rng.random(n_init)\n                X_init[:, d] = (perm + u) / float(n_init)\n            # map to bounds\n            X_init = lb + X_init * (ub - lb)\n\n        # add a few extra pure uniforms (diversity)\n        extras = min(6, max(1, self.dim // 2))\n        X_extra = self._uniform_array(lb, ub, n=extras) if (budget - n_init) > 0 else np.empty((0, self.dim))\n        if X_extra.ndim == 1:\n            X_extra = X_extra.reshape(1, -1)\n\n        # evaluate initial population until budget exhausted\n        archive = []\n        for i in range(X_init.shape[0]):\n            if evals >= budget:\n                break\n            x = X_init[i]\n            f = safe_eval(x)\n            archive.append({'x': np.asarray(x, dtype=float).copy(), 'f': float(f), 'age': 0})\n        for i in range(X_extra.shape[0]):\n            if evals >= budget:\n                break\n            x = X_extra[i]\n            f = safe_eval(x)\n            archive.append({'x': np.asarray(x, dtype=float).copy(), 'f': float(f), 'age': 0})\n\n        # If nothing evaluated (e.g., budget==0) return fallback (center)\n        if len(archive) == 0:\n            center = 0.5 * (lb + ub)\n            return float(np.inf), center\n\n        # initial best\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) == 0:\n            # none finite yet, pick first archive as best\n            xbest = archive[0]['x'].copy()\n            fbest = archive[0]['f']\n        else:\n            finite_entries.sort(key=lambda e: e['f'])\n            xbest = finite_entries[0]['x'].copy()\n            fbest = finite_entries[0]['f']\n\n        # main loop\n        iter_no = 0\n        last_improve_iter = 0\n        stagnation_counter = 0\n        while evals < budget:\n            iter_no += 1\n\n            # convert archive arrays\n            Xa = np.array([e['x'] for e in archive])\n            Fa = np.array([e['f'] for e in archive])\n            ages = np.array([e['age'] for e in archive], dtype=int)\n\n            finite_mask = np.isfinite(Fa)\n            finite_X = Xa[finite_mask] if np.any(finite_mask) else np.empty((0, self.dim))\n            finite_fs = Fa[finite_mask] if np.any(finite_mask) else np.array([])\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            if finite_X.shape[0] >= 3:\n                q75 = np.percentile(finite_X, 75, axis=0)\n                q25 = np.percentile(finite_X, 25, axis=0)\n                iqr = q75 - q25\n                # convert IQR to approx std: iqr / 1.349\n                robust_scale = iqr / 1.349\n                small_mask = robust_scale <= 1e-12\n                if np.any(small_mask):\n                    robust_scale[small_mask] = np.std(finite_X[:, small_mask], axis=0, ddof=0)\n                # ensure not too small relative to domain\n                robust_scale = np.maximum(robust_scale, domain_scale * 1e-6)\n            else:\n                # not enough data: use domain scale / 8\n                robust_scale = domain_scale / 8.0\n\n            # norm_scale used for generating moves (vector)\n            norm_scale = np.maximum(robust_scale * (0.25 + self.gscale), domain_scale * 1e-6)\n\n            # determine elites (by f)\n            n_archive = len(archive)\n            if np.any(finite_mask):\n                # pick elites among finite entries\n                finite_idx = np.where(finite_mask)[0]\n                n_elite = max(2, int(0.2 * max(3, len(finite_idx))))\n                sorted_finite_idx = finite_idx[np.argsort(finite_fs)]\n                elite_idx = sorted_finite_idx[:n_elite].tolist()\n            else:\n                # pick some random indices as elites\n                elite_idx = list(range(min(3, n_archive)))\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            # Compute preference weights decreasing with rank\n            anchor_idx = None\n            if len(elite_idx) == 1:\n                anchor_idx = elite_idx[0]\n            else:\n                # rank within elites by f if available\n                ranks = np.arange(len(elite_idx))\n                # bias more to better ones\n                probs_anchor = np.exp(-0.8 * ranks)\n                probs_anchor = probs_anchor / np.sum(probs_anchor)\n                pick = rng.choice(len(elite_idx), p=probs_anchor)\n                anchor_idx = elite_idx[pick]\n            anchor = archive[anchor_idx]['x'].copy()\n\n            # adapt base probabilities slightly with gscale (favor Cauchy when gscale large)\n            base_probs = np.array([0.36, 0.26, 0.26, 0.12])  # DE, PCA, Local, Cauchy\n            # boost Cauchy when exploration flagged\n            base_probs[3] *= (1.0 + 1.2 * min(2.0, self.gscale * 2.0))\n            probs = base_probs / np.sum(base_probs)\n\n            # pick strategy\n            strat = rng.choice(4, p=probs)\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                pool = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in pool:\n                    pool.remove(exclude_idx)\n                if len(pool) < k:\n                    return None\n                sel = rng.choice(pool, size=k, replace=False).tolist()\n                return sel\n\n            # compute a per-dim normalized scale for moves\n            # norm_scale is per-dim vector; we will multiply by small factors per-strategy\n\n            cand = anchor.copy()\n            if strat == 0:\n                # Strategy 0: DE-style differential injection (current-to-rand variant)\n                ids = pick_distinct(3, exclude_idx=anchor_idx)\n                if ids is None:\n                    # fallback to random Gaussian perturbation\n                    cand = anchor + 0.04 * norm_scale * rng.standard_normal(self.dim)\n                else:\n                    xr0 = archive[ids[0]]['x']\n                    xr1 = archive[ids[1]]['x']\n                    xr2 = archive[ids[2]]['x']\n                    # current-to-rand-like\n                    trial = anchor + self.F * (xr0 - xr1) + 0.5 * self.F * (xr2 - anchor)\n                    # anisotropic jitter\n                    trial += 0.06 * norm_scale * rng.standard_normal(self.dim)\n                    # binomial crossover with an elite\n                    # pick elite partner\n                    elite_partner = archive[rng.choice(elite_idx)]['x']\n                    mask = rng.random(self.dim) < 0.5\n                    if not np.any(mask):\n                        mask[rng.integers(0, self.dim)] = True\n                    cand = np.where(mask, trial, elite_partner)\n                    # occasional tempered cauchy injection\n                    if rng.random() < 0.03:\n                        cand += self._tempered_cauchy(norm_scale * (0.8 + self.gscale * 2.0))\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                if finite_X.shape[0] >= min(3, self.dim):\n                    # compute PCA on elites (centered)\n                    elites_X = finite_X[[np.where(finite_mask)[0].tolist().index(i) if False else 0]]  # dummy to avoid mypy; we'll recompute properly\n                    # simpler: take top few elites\n                    elite_positions = np.array([archive[i]['x'] for i in elite_idx])\n                    Xc = elite_positions - np.mean(elite_positions, axis=0)\n                    try:\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        pcs = Vt  # rows are principal directions\n                        # sample coefficients scaled by singular values and gscale\n                        kcomp = min(self.dim, max(1, pcs.shape[0]))\n                        coeffs = rng.standard_normal(kcomp) * (S[:kcomp] / (np.max(S[:kcomp]) + 1e-12)) * (0.5 + self.gscale)\n                        delta = np.zeros(self.dim)\n                        for j in range(kcomp):\n                            delta += coeffs[j] * pcs[j]\n                        r = anchor + delta\n                        # blend slightly towards mean of elites\n                        r = 0.85 * r + 0.15 * np.mean(elite_positions, axis=0)\n                        cand = r + 0.03 * norm_scale * rng.standard_normal(self.dim)\n                    except Exception:\n                        # fallback\n                        cand = anchor + 0.08 * norm_scale * rng.standard_normal(self.dim)\n                else:\n                    cand = anchor + 0.08 * norm_scale * rng.standard_normal(self.dim)\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                # anisotropic per-dim sigma proportional to spread but smaller\n                local_sigma = norm_scale * (0.25 + 0.5 * rng.random())\n                cand = anchor + local_sigma * rng.standard_normal(self.dim)\n                # ensure at least one coord perturbed\n                if rng.random() < 0.02:\n                    j = rng.integers(0, self.dim)\n                    cand[j] = anchor[j] + 2.0 * local_sigma[j] * rng.standard_normal()\n                # occasional mixed uniform per-dim injection\n                if rng.random() < 0.05:\n                    mix_mask = rng.random(self.dim) < 0.07\n                    if np.any(mix_mask):\n                        cand[mix_mask] = rng.uniform(lb[mix_mask], ub[mix_mask])\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                cand = anchor.copy()\n                cand += self._tempered_cauchy(norm_scale * (1.2 + self.gscale))\n                # sometimes add directed differential component if donors exist\n                ids = pick_distinct(3, exclude_idx=None)\n                if ids is not None:\n                    xr1 = archive[ids[1]]['x']\n                    xr2 = archive[ids[2]]['x']\n                    cand += 0.12 * (xr1 - xr2)\n\n            # occasional directed small jump to current best for refinement\n            if rng.random() < 0.05 and np.isfinite(fbest):\n                cand = 0.85 * cand + 0.15 * xbest\n\n            # sanitize candidate: replace nan/inf coordinates by uniform draws\n            cand = np.asarray(cand, dtype=float)\n            mask_bad = ~np.isfinite(cand)\n            if np.any(mask_bad):\n                cand[mask_bad] = self._uniform_array(lb[mask_bad], ub[mask_bad])\n\n            # reflect to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget remains\n            if evals >= budget:\n                break\n            f_cand = safe_eval(cand)\n\n            # increment ages of others\n            for e in archive:\n                e['age'] += 1\n\n            # update archive\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and (not np.isfinite(fbest) or f_cand < fbest):\n                fbest = float(f_cand)\n                xbest = cand.copy()\n                improved = True\n                last_improve_iter = iter_no\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            success = False\n            if np.isfinite(f_cand):\n                finite_fs_now = np.array([e['f'] for e in archive if np.isfinite(e['f'])])\n                if finite_fs_now.size > 0:\n                    threshold = np.percentile(finite_fs_now, 25)\n                    if f_cand <= threshold:\n                        success = True\n            if improved:\n                success = True\n\n            self.recent_success.append(1 if success else 0)\n            sr = float(np.mean(self.recent_success)) if len(self.recent_success) > 0 else 0.0\n\n            # short-term adaptation of global scale based on recent success rate\n            # low success -> increase exploration, high success -> tighten\n            if sr < 0.12:\n                self.gscale = min(self.gscale * 1.08 + 0.001, self.gscale_max)\n            elif sr > 0.35:\n                self.gscale = max(self.gscale * 0.92 - 0.0005, self.gscale_min)\n            else:\n                # slight decay toward stable region\n                self.gscale = np.clip(self.gscale * (1.0 + 0.01 * (0.25 - sr)), self.gscale_min, self.gscale_max)\n\n            # prune archive when too large (keep best + some random + some oldest)\n            if len(archive) > self.max_archive:\n                # sort by f (finite first)\n                archive_sorted = sorted(archive, key=lambda e: (not np.isfinite(e['f']), e['f'] if np.isfinite(e['f']) else np.inf))\n                kbest = max(10, 3 * self.dim)\n                keep = archive_sorted[:kbest]  # keep best\n                rest = archive_sorted[kbest:]\n                # keep some oldest from remaining\n                if len(rest) > 0:\n                    rest_sorted_by_age = sorted(rest, key=lambda e: -e['age'])\n                    n_old = min(len(rest_sorted_by_age), max(10, len(rest_sorted_by_age)//6))\n                    keep.extend(rest_sorted_by_age[:n_old])\n                    # inject some random subset\n                    rng.shuffle(rest)\n                    n_rand = min(20, len(rest))\n                    keep.extend(rest[:n_rand])\n                # compact and deduplicate by proximity (simple)\n                unique = []\n                seen = set()\n                for e in keep:\n                    key = tuple(np.round(e['x'], decimals=8))\n                    if key not in seen:\n                        unique.append(e)\n                        seen.add(key)\n                archive = unique[:self.max_archive]\n\n            # stagnation detection & micro-restarts\n            # if no improvement for long time relative to dimension, do a micro-restart near best\n            stagnation_limit = max(150, 25 * self.dim)\n            if stagnation_counter > stagnation_limit and evals < budget:\n                # micro-restart: local cloud around current best (or random center if best is inf)\n                if np.isfinite(fbest):\n                    center = xbest.copy()\n                else:\n                    # pick some good finite or random\n                    finite_list = [e for e in archive if np.isfinite(e['f'])]\n                    if len(finite_list) > 0:\n                        center = finite_list[0]['x'].copy()\n                    else:\n                        center = self._uniform_array(lb, ub)\n                # cloud sigma related to gscale and domain\n                cloud_sigma = np.maximum(domain_scale * (0.2 * (1.0 + self.gscale)), 1e-8)\n                n_cloud = min(20 + self.dim, max(6, int(6 + self.dim)))\n                for _ in range(n_cloud):\n                    if evals >= budget:\n                        break\n                    rx = center + cloud_sigma * (0.5 * rng.standard_normal(self.dim))\n                    rx = self._reflect_bounds(rx, lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and (not np.isfinite(fbest) or fr < fbest):\n                        fbest = float(fr)\n                        xbest = rx.copy()\n                # also inject some global randoms\n                nrand = min(8, max(1, int(4 + 2 * self.dim / 10)))\n                for _ in range(nrand):\n                    if evals >= budget:\n                        break\n                    rx = self._uniform_array(lb, ub)\n                    fr = safe_eval(rx)\n                    archive.append({'x': rx.copy(), 'f': float(fr), 'age': 0})\n                    if np.isfinite(fr) and fr < fbest:\n                        fbest = float(fr)\n                        xbest = rx.copy()\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.15 + 0.01, self.gscale_max)\n                self.recent_success.clear()\n                stagnation_counter = 0\n                last_improve_iter = iter_no\n\n            # occasionally verbose\n            if self.verbose and (iter_no % 200 == 0 or improved):\n                finite_fs = [e['f'] for e in archive if np.isfinite(e['f'])]\n                best_now = np.min(finite_fs) if len(finite_fs) > 0 else np.inf\n                print(f\"[MG-ADS] iter={iter_no} evals={evals} best={best_now:.6g} gscale={self.gscale:.4g}\")\n\n        # final best from finite archive if any\n        finite_entries = [e for e in archive if np.isfinite(e['f'])]\n        if len(finite_entries) > 0:\n            finite_entries.sort(key=lambda e: e['f'])\n            return float(finite_entries[0]['f']), finite_entries[0]['x'].copy()\n        else:\n            # all infinite: return a uniform fallback point in the domain center\n            center = 0.5 * (lb + ub)\n            return float(np.inf), center", "configspace": "", "generation": 0, "feedback": "In the code, line 34, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.recent_success = deque(maxlen=self.window_len)", "error": "In the code, line 34, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.recent_success = deque(maxlen=self.window_len)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a3d005c8-eb7f-48c5-b7dd-0e4ea3e7a34d", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic local search and tempered-Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\nimport math\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=False)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed if seed is not None else np.random.randint(1_000_000)\n        self.verbose = verbose\n\n        # short-term adaptation parameters\n        self.gscale = 1.0         # current global scale multiplier\n        self.gscale_max = 5.0     # max allowed scale\n        self.success_window = deque(maxlen=40)  # short-term success history\n\n        # Archive / population heuristics\n        self.init_pop_mult = 4    # initial population per-dim multiplier (LHS strata)\n        self.extra_random = max(3, self.dim // 2)\n        self.archive_max = 20 * max(1, self.dim)  # cap for archive size\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n        # differential weight default (for DE-like moves)\n        self.F = 0.6\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func (common wrappers). Fall back to [-5,5]^dim.\n        Returns arrays lb, ub shape (dim,).\n        \"\"\"\n        # default domain\n        default_lb = -5.0\n        default_ub = 5.0\n        lb = np.full(self.dim, default_lb, dtype=float)\n        ub = np.full(self.dim, default_ub, dtype=float)\n\n        # try a few common attribute patterns\n        try:\n            # e.g., func.bounds.lb / func.bounds.ub (box object)\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                    lb_attr = np.asarray(b.lb)\n                    ub_attr = np.asarray(b.ub)\n                    if lb_attr.size == 1:\n                        lb[:] = float(lb_attr.item())\n                    elif lb_attr.size == self.dim:\n                        lb = lb_attr.astype(float)\n                    if ub_attr.size == 1:\n                        ub[:] = float(ub_attr.item())\n                    elif ub_attr.size == self.dim:\n                        ub = ub_attr.astype(float)\n                    return lb, ub\n        except Exception:\n            pass\n\n        # try scalar attributes\n        try:\n            if hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n                lo = np.asarray(func.lower)\n                hi = np.asarray(func.upper)\n                if lo.size == 1:\n                    lb[:] = float(lo.item())\n                elif lo.size == self.dim:\n                    lb = lo.astype(float)\n                if hi.size == 1:\n                    ub[:] = float(hi.item())\n                elif hi.size == self.dim:\n                    ub = hi.astype(float)\n                return lb, ub\n        except Exception:\n            pass\n\n        # fallback\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection for coordinates outside bounds. Repeat a few times\n        to ensure the vector is inside. If still outside, clamp.\n        \"\"\"\n        x = np.asarray(x).astype(float)\n        for _ in range(niter):\n            below = x < lb\n            above = x > ub\n            if not np.any(below) and not np.any(above):\n                break\n            x[below] = lb[below] + (lb[below] - x[below])  # reflect\n            x[above] = ub[above] - (x[above] - ub[above])\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        if n == 1:\n            return self.rng.uniform(lb, ub)\n        else:\n            return self.rng.uniform(lb, ub, size=(n, self.dim))\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy via inverse CDF:\n        x = scale * tan(pi*(u-0.5))\n        Cap extremes to avoid overflow: max_abs = cap_multiplier * scale_vec\n        Add small gaussian jitter to break symmetry.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        vals = np.tan(np.pi * (u - 0.5))  # standard Cauchy quantiles\n        # scale\n        scale_vec = np.maximum(np.abs(scale_vec), 1e-12)\n        vals = vals * scale_vec\n        max_abs = cap_multiplier * scale_vec\n        too_large = np.abs(vals) > max_abs\n        if np.any(too_large):\n            vals[too_large] = np.sign(vals[too_large]) * max_abs[too_large]\n        # small gaussian jitter\n        vals += 1e-6 * (scale_vec + 1e-12) * self.rng.standard_normal(self.dim)\n        return vals\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        # read bounds\n        lb, ub = self._get_bounds(func)\n        domain_scale = ub - lb\n        domain_scale = np.maximum(domain_scale, 1e-12)\n\n        # enforce shapes\n        lb = np.asarray(lb).reshape(self.dim)\n        ub = np.asarray(ub).reshape(self.dim)\n\n        # wrapper to safely call func without exceeding budget\n        eval_count = 0\n        best_f = float(\"inf\")\n        best_x = None\n\n        def safe_eval(x):\n            nonlocal eval_count, best_f, best_x\n            if eval_count >= self.budget:\n                return float(\"inf\")\n            x_arr = np.asarray(x, dtype=float)\n            # ensure 1D shape\n            if x_arr.ndim > 1:\n                x_eval = x_arr.ravel()\n            else:\n                x_eval = x_arr\n            # try to call function and handle types\n            try:\n                f = func(x_eval)\n            except Exception:\n                # try converting to list for some wrappers\n                try:\n                    f = func(x_eval.tolist())\n                except Exception:\n                    f = float(\"inf\")\n            eval_count += 1\n            # update best seen here for record\n            if np.isfinite(f) and f < best_f:\n                best_f = float(f)\n                best_x = np.array(x_eval, dtype=float)\n            return float(f)\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        n_init = min(max(self.init_pop_mult * self.dim, 8), max(8, self.budget))\n        # create LHS-like initial matrix\n        strata = np.zeros((n_init, self.dim), dtype=float)\n        for d in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            # sample uniformly within each stratum\n            strata[:, d] = (perm + self.rng.rand(n_init)) / float(n_init)\n        X_init = lb + strata * (ub - lb)\n\n        # add a few extra pure uniforms (diversity)\n        X_extra = self._uniform_array(lb, ub, n=self.extra_random)\n\n        # Evaluate initial population until budget exhausted\n        archive = []  # list of dicts: {'x':..., 'f':..., 'age':...}\n        for i in range(X_init.shape[0]):\n            if eval_count >= self.budget:\n                break\n            x = X_init[i]\n            f = safe_eval(x)\n            archive.append({'x': np.array(x, dtype=float), 'f': float(f), 'age': 0})\n        for i in range(X_extra.shape[0]):\n            if eval_count >= self.budget:\n                break\n            x = X_extra[i]\n            f = safe_eval(x)\n            archive.append({'x': np.array(x, dtype=float), 'f': float(f), 'age': 0})\n\n        # If nothing evaluated (e.g., budget==0) return fallback (center)\n        if eval_count == 0:\n            x0 = (lb + ub) / 2.0\n            return float(\"inf\"), x0\n\n        # initial best\n        finite_fs = [a['f'] for a in archive if np.isfinite(a['f'])]\n        if len(finite_fs) > 0:\n            idx_best = np.argmin([a['f'] if np.isfinite(a['f']) else np.inf for a in archive])\n            best_f = float(archive[idx_best]['f'])\n            best_x = archive[idx_best]['x'].copy()\n        else:\n            # choose first as best if all infinite\n            best_f = float(archive[0]['f'])\n            best_x = archive[0]['x'].copy()\n\n        # main loop\n        stagnation_iters = 0\n        no_improve_since = 0\n        max_iters = max(1, self.budget - eval_count)\n\n        iters = 0\n        while eval_count < self.budget:\n            iters += 1\n\n            # convert archive arrays\n            As = np.array([a['x'] for a in archive])\n            Fs = np.array([a['f'] for a in archive])\n            ages = np.array([a['age'] for a in archive])\n\n            finite_mask = np.isfinite(Fs)\n            finite_idx = np.where(finite_mask)[0] if np.any(finite_mask) else np.array([], dtype=int)\n            finite_X = As[finite_mask] if np.any(finite_mask) else np.empty((0, self.dim))\n            finite_fs = Fs[finite_mask] if np.any(finite_mask) else np.array([])\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            if finite_X.shape[0] >= 3:\n                q75 = np.percentile(finite_X, 75, axis=0)\n                q25 = np.percentile(finite_X, 25, axis=0)\n                iqr = q75 - q25\n                # convert IQR to approx std: iqr / 1.349\n                approx_std = iqr / 1.349\n                # fallback to std where iqr is too small\n                stds = np.std(finite_X, axis=0)\n                est_scale = np.where(approx_std > 1e-12, approx_std, stds)\n                # ensure not too small relative to domain\n                est_scale = np.maximum(est_scale, domain_scale / 1000.0)\n            else:\n                # not enough data: use domain scale / 8\n                est_scale = domain_scale / 8.0\n\n            # norm_scale used for generating moves (vector)\n            norm_scale = est_scale * (0.5 + 0.5 * self.gscale)\n\n            # determine elites (by f)\n            elite_ids = []\n            if finite_X.shape[0] > 0:\n                sorted_finite_idx = finite_idx[np.argsort(finite_fs)]\n                n_elite = max(1, min(6 + self.dim // 4, len(sorted_finite_idx)))\n                # pick elites among finite entries: best n_elite and a couple random from top 2*n_elite\n                topk = sorted_finite_idx[:min(len(sorted_finite_idx), max(3, 2 * n_elite))]\n                # choose n_elite among topk with bias to better ones\n                weights = np.exp(-0.5 * np.arange(len(topk)))\n                weights = weights / np.sum(weights)\n                choose_ids = self.rng.choice(topk, size=n_elite, replace=False, p=weights if len(weights) == len(topk) else None)\n                elite_ids = list(choose_ids)\n            else:\n                elite_ids = []\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            if len(elite_ids) > 0:\n                # rank them by f\n                ef = np.array([archive[i]['f'] for i in elite_ids])\n                ord_idx = np.argsort(ef)\n                ranks = np.arange(len(elite_ids))\n                pref = np.exp(-0.8 * ranks)\n                pref = pref / np.sum(pref)\n                anchor_idx = elite_ids[self.rng.choice(len(elite_ids), p=pref)]\n            else:\n                anchor_idx = self.rng.randint(len(archive))\n\n            anchor = archive[anchor_idx]['x'].copy()\n            anchor_f = archive[anchor_idx]['f']\n\n            # adapt base probabilities slightly with gscale (favor Cauchy when gscale large)\n            strat_probs = np.array([0.42, 0.18, 0.30, 0.10])  # DE, PCA, local, Cauchy\n            # boost Cauchy when exploration flagged\n            strat_probs = strat_probs * np.array([1.0, 1.0, 1.0, 1.0 + 0.6 * (self.gscale - 1.0)])\n            strat_probs = np.maximum(strat_probs, 0.001)\n            strat_probs = strat_probs / np.sum(strat_probs)\n\n            strat = self.rng.choice(4, p=strat_probs)\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                pool = list(range(len(archive)))\n                if exclude_idx is not None and exclude_idx in pool:\n                    pool.remove(exclude_idx)\n                if len(pool) < k:\n                    # fallback: random vectors in domain\n                    return None\n                return self.rng.choice(pool, size=k, replace=False)\n\n            # compute a per-dim normalized scale for moves\n            scale_vec = norm_scale * self.gscale\n\n            # Candidate generation\n            cand = anchor.copy()\n\n            if strat == 0:\n                # Strategy 0: DE-style differential injection (current-to-rand variant)\n                donors = pick_distinct(2, exclude_idx=anchor_idx)\n                if donors is None:\n                    # fallback: gaussian jitter\n                    cand = anchor + 0.08 * scale_vec * self.rng.standard_normal(self.dim)\n                else:\n                    r1, r2 = donors\n                    x_r1 = archive[r1]['x']\n                    x_r2 = archive[r2]['x']\n                    # differential step\n                    cand = anchor + self.F * (x_r1 - x_r2)\n                    # anisotropic jitter\n                    cand += 0.04 * scale_vec * self.rng.standard_normal(self.dim)\n                    # binomial crossover with an elite partner occasionally\n                    if len(elite_ids) > 0 and self.rng.rand() < 0.7:\n                        elite_partner = archive[self.rng.choice(elite_ids)]['x']\n                        mask = self.rng.rand(self.dim) < 0.5\n                        cand = np.where(mask, cand, elite_partner)\n                    # occasional tempered cauchy injection\n                    if self.rng.rand() < 0.04:\n                        cand += self._tempered_cauchy(scale_vec * 0.8)\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                if len(elite_ids) >= 2:\n                    E = np.array([archive[i]['x'] for i in elite_ids])\n                    # center\n                    meanE = E.mean(axis=0)\n                    Z = E - meanE\n                    try:\n                        U, S, Vt = np.linalg.svd(Z, full_matrices=False)\n                        # project a random gaussian on top components\n                        kcomp = min(self.dim, max(1, Vt.shape[0]))\n                        coeffs = self.rng.randn(kcomp) * (S[:kcomp] / (np.max(S[:kcomp]) + 1e-12))\n                        if np.any(np.isnan(coeffs)):\n                            coeffs = self.rng.randn(kcomp)\n                        delta = np.dot(coeffs, Vt[:kcomp, :])\n                        cand = meanE + 0.6 * self.gscale * delta\n                        # blend slightly towards anchor (exploit)\n                        cand = 0.6 * cand + 0.4 * anchor\n                    except Exception:\n                        # fallback gaussian around an elite\n                        e = archive[self.rng.choice(elite_ids)]['x']\n                        cand = e + 0.06 * scale_vec * self.rng.randn(self.dim)\n                elif len(elite_ids) == 1:\n                    e = archive[elite_ids[0]]['x']\n                    cand = e + 0.06 * scale_vec * self.rng.randn(self.dim)\n                else:\n                    cand = anchor + 0.06 * scale_vec * self.rng.randn(self.dim)\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                sigma = 0.12 * scale_vec * max(0.6, 1.0 / (1.0 + self.gscale))\n                # ensure at least one coord perturbed\n                mask = self.rng.rand(self.dim) < (0.4 + 0.2 * self.gscale)\n                if not np.any(mask):\n                    mask[self.rng.randint(self.dim)] = True\n                noise = np.zeros(self.dim)\n                noise[mask] = self.rng.randn(np.sum(mask)) * sigma[mask]\n                cand = anchor + noise\n                # occasional mixed uniform per-dim injection\n                if self.rng.rand() < 0.06:\n                    uni_mask = self.rng.rand(self.dim) < 0.08\n                    if np.any(uni_mask):\n                        cand[uni_mask] = self.rng.uniform(lb[uni_mask], ub[uni_mask])\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                cand = anchor + self._tempered_cauchy(scale_vec * (1.5 + 0.5 * self.gscale))\n                # sometimes add directed differential component if donors exist\n                donors = pick_distinct(2, exclude_idx=None)\n                if donors is not None and self.rng.rand() < 0.33:\n                    xr1, xr2 = archive[donors[0]]['x'], archive[donors[1]]['x']\n                    cand += 0.12 * (xr1 - xr2)\n\n            # occasional directed small jump to current best for refinement\n            if best_x is not None and self.rng.rand() < 0.05:\n                cand = 0.85 * cand + 0.15 * best_x\n\n            # sanitize candidate: replace nan/inf coordinates by uniform draws\n            bad = ~np.isfinite(cand)\n            if np.any(bad):\n                cand[bad] = self.rng.uniform(lb[bad], ub[bad])\n\n            # reflect to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget remains\n            if eval_count < self.budget:\n                f_cand = safe_eval(cand)\n            else:\n                f_cand = float(\"inf\")\n\n            # increment ages of others\n            for e in archive:\n                e['age'] += 1\n\n            # update archive: append candidate\n            archive.append({'x': cand.copy(), 'f': float(f_cand), 'age': 0})\n\n            # update best if improved\n            improved = False\n            if np.isfinite(f_cand) and f_cand < best_f:\n                best_f = float(f_cand)\n                best_x = cand.copy()\n                improved = True\n                stagnation_iters = 0\n                no_improve_since = 0\n            else:\n                stagnation_iters += 1\n                no_improve_since += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_fs_now = np.array([a['f'] for a in archive if np.isfinite(a['f'])])\n            is_success = False\n            if np.isfinite(f_cand):\n                if improved:\n                    is_success = True\n                elif finite_fs_now.size > 0:\n                    threshold = np.percentile(finite_fs_now, 25)\n                    if f_cand <= threshold:\n                        is_success = True\n            self.success_window.append(1 if is_success else 0)\n\n            # short-term adaptation of global scale based on recent success rate\n            if len(self.success_window) >= 8:\n                rate = float(np.mean(self.success_window))\n                # low success -> increase exploration, high success -> tighten\n                if rate < 0.15:\n                    self.gscale = min(self.gscale * 1.06 + 0.002, self.gscale_max)\n                elif rate > 0.5:\n                    self.gscale = max(self.gscale * 0.92 - 0.001, 0.2)\n                # slight decay toward 1.0 to avoid runaway\n                self.gscale = self.gscale * 0.995 + 0.005\n\n            # prune archive when too large (keep best + some random + some oldest)\n            if len(archive) > self.archive_max:\n                # sort by f (finite first)\n                def key_fn(a):\n                    return a['f'] if np.isfinite(a['f']) else 1e300\n                archive_sorted = sorted(archive, key=key_fn)\n                kbest = max(3, int(3 + self.dim * 1.5))\n                keep = archive_sorted[:kbest]  # keep best\n                # keep some oldest from remaining\n                remain = archive_sorted[kbest:]\n                # select by age and randomness\n                remain_sorted_by_age = sorted(remain, key=lambda a: -a['age'])\n                keep += remain_sorted_by_age[:max(0, min(len(remain_sorted_by_age), int(self.dim)))]\n                # inject some random subset from remain\n                if len(remain) > 0:\n                    nrand = min(len(remain), max(1, int(self.dim / 2)))\n                    picks = self.rng.choice(len(remain), size=nrand, replace=False)\n                    for p in picks:\n                        keep.append(remain[p])\n                # compact and deduplicate by proximity (simple)\n                new_archive = []\n                seen = []\n                thresh = 1e-6 + np.linalg.norm(domain_scale) * 1e-3\n                for a in keep:\n                    x = a['x']\n                    too_close = False\n                    for s in seen:\n                        if np.linalg.norm(x - s) < thresh:\n                            too_close = True\n                            break\n                    if not too_close:\n                        new_archive.append(a)\n                        seen.append(x)\n                archive = new_archive\n\n            # stagnation detection & micro-restarts\n            if no_improve_since > max(80, 12 * self.dim):\n                # micro-restart: local cloud around current best (or random center if best is inf)\n                if np.isfinite(best_f) and best_x is not None:\n                    center = best_x.copy()\n                else:\n                    center = self._uniform_array(lb, ub)\n                # cloud: generate a few candidates around center\n                cloud_n = max(6, min(40, 6 * self.dim))\n                cloud_sigma = np.maximum(domain_scale * (0.18 * (1.0 + self.gscale)), 1e-8)\n                for _ in range(cloud_n):\n                    if eval_count >= self.budget:\n                        break\n                    cand_local = center + self.rng.randn(self.dim) * cloud_sigma * (0.5 + self.rng.rand(self.dim))\n                    cand_local = self._reflect_bounds(cand_local, lb, ub)\n                    f_local = safe_eval(cand_local)\n                    archive.append({'x': cand_local.copy(), 'f': float(f_local), 'age': 0})\n                    if np.isfinite(f_local) and f_local < best_f:\n                        best_f = float(f_local)\n                        best_x = cand_local.copy()\n                        improved = True\n                # also inject some global randoms\n                for _ in range(3):\n                    if eval_count >= self.budget:\n                        break\n                    xr = self._uniform_array(lb, ub)\n                    fr = safe_eval(xr)\n                    archive.append({'x': xr.copy(), 'f': float(fr), 'age': 0})\n                # after restart, slightly increase exploration scales and reset small window\n                self.gscale = min(self.gscale * 1.3, self.gscale_max)\n                self.success_window.clear()\n                no_improve_since = 0\n\n            # occasionally verbose\n            if self.verbose and (iters % 100 == 0 or improved):\n                print(f\"[MG-ADS] evals={eval_count}/{self.budget} best_f={best_f:.4e} gscale={self.gscale:.3f} archive={len(archive)}\")\n\n        # final best from finite archive if any\n        finite_archive = [a for a in archive if np.isfinite(a['f'])]\n        if len(finite_archive) > 0:\n            best = min(finite_archive, key=lambda a: a['f'])\n            return float(best['f']), best['x'].copy()\n        # all infinite: return a uniform fallback point in the domain center\n        fallback = (lb + ub) / 2.0\n        return float(\"inf\"), fallback", "configspace": "", "generation": 0, "feedback": "In the code, line 27, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=40)  # short-term success history", "error": "In the code, line 27, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.success_window = deque(maxlen=40)  # short-term success history", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "460a7d93-1c95-49be-a1f4-ded328b21eae", "fitness": "-inf", "name": "MemoryGuidedADS", "description": "Archive-driven hybrid sampler combining DE-style donor recombination, PCA-guided elite moves, anisotropic local search and tempered-Cauchy escapes with short-term scale adaptation and micro-restarts.", "code": "import numpy as np\nfrom collections import deque\nimport math\nimport warnings\n\nclass MemoryGuidedADS:\n    \"\"\"\n    Memory-Guided Adaptive Directional Sampling (MG-ADS)\n\n    One-line: Archive-driven hybrid sampler combining DE-style donor recombination,\n    PCA-guided elite perturbations, anisotropic Gaussian local moves, tempered\n    Cauchy global jumps, short-term scale adaptation and micro-restarts.\n\n    Usage:\n        opt = MemoryGuidedADS(budget=10000, dim=10, seed=1, verbose=False)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.verbose = bool(verbose)\n        self.rng = np.random.RandomState(seed)\n        # population / archive heuristics\n        self.popsize = max(8, min(64, 4 * self.dim))  # initial population target\n        self.max_archive = max(100, 6 * self.popsize)\n        # short-term adaptation\n        self.gscale = 0.1  # global exploration scale (relative)\n        self.F = 0.8  # differential weight default\n        self.CR = 0.3  # crossover probability\n        # sliding window for recent success (for adaptation)\n        self.window = deque(maxlen=50)\n        # evaluation counter\n        self.evals = 0\n        # random jitter floor\n        self.eps = 1e-12\n\n    # ---------------- utilities ----------------\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to read bounds from func (common wrappers). Fall back to [-5,5]^dim.\n        Returns arrays lb, ub shape (dim,).\n        \"\"\"\n        lb = None\n        ub = None\n        # try common patterns\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb).astype(float)\n                ub = np.asarray(b.ub).astype(float)\n        if lb is None:\n            # try attributes lower/upper\n            if hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n                lb = np.asarray(func.lower).astype(float)\n                ub = np.asarray(func.upper).astype(float)\n        # last resort: scalar attributes\n        if lb is None:\n            if hasattr(func, \"lower_bound\"):\n                lb = np.asarray(func.lower_bound).astype(float)\n            if hasattr(func, \"upper_bound\"):\n                ub = np.asarray(func.upper_bound).astype(float)\n        # fallback to [-5,5]\n        if lb is None:\n            lb = -5.0 * np.ones(self.dim)\n        if ub is None:\n            ub = 5.0 * np.ones(self.dim)\n        lb = np.broadcast_to(np.asarray(lb).astype(float), (self.dim,))\n        ub = np.broadcast_to(np.asarray(ub).astype(float), (self.dim,))\n        # ensure lb <= ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, niter=3):\n        \"\"\"\n        Mirror-reflection for coordinates outside bounds. Repeat a few times\n        to ensure the vector is inside. If still outside, clamp.\n        \"\"\"\n        x = np.array(x, dtype=float)\n        for _ in range(niter):\n            below = x < lb\n            if np.any(below):\n                x[below] = lb[below] + (lb[below] - x[below])\n            above = x > ub\n            if np.any(above):\n                x[above] = ub[above] - (x[above] - ub[above])\n            # if all inside, break\n            if not (np.any(x < lb) or np.any(x > ub)):\n                break\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub, n=1):\n        lb = np.asarray(lb)\n        ub = np.asarray(ub)\n        shape = (n, self.dim)\n        u = self.rng.rand(*shape)\n        return lb + (ub - lb) * u\n\n    def _tempered_cauchy(self, scale_vec, cap_multiplier=40.0):\n        \"\"\"\n        Coordinate-wise tempered Cauchy via inverse CDF:\n        x = scale * tan(pi*(u-0.5))\n        Cap extremes to avoid overflow: max_abs = cap_multiplier * scale_vec\n        Add small gaussian jitter to break symmetry.\n        \"\"\"\n        u = self.rng.rand(self.dim)\n        t = np.tan(np.pi * (u - 0.5))\n        x = scale_vec * t\n        cap = cap_multiplier * np.maximum(np.abs(scale_vec), 1e-12)\n        x = np.clip(x, -cap, cap)\n        # small gaussian jitter\n        x += 1e-3 * scale_vec * self.rng.randn(self.dim)\n        return x\n\n    # ---------------- main entry ----------------\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure shapes\n        lb = lb.flatten()[: self.dim]\n        ub = ub.flatten()[: self.dim]\n        domain_scale = ub - lb\n        domain_scale[domain_scale <= 0] = 10.0  # safe fallback\n\n        # safe wrapper to call func without exceeding budget\n        def safe_eval(x):\n            if self.evals >= self.budget:\n                return np.inf\n            # convert to list if some wrappers expect list/tuple\n            arg = x.tolist() if hasattr(func, \"__call__\") and isinstance(x, (list, tuple)) else x\n            # call\n            try:\n                f = func(x)\n            except Exception:\n                # some wrappers require python list\n                try:\n                    f = func(arg)\n                except Exception:\n                    # if still failing, return inf\n                    f = np.inf\n            self.evals += 1\n            return float(f)\n\n        # ---- initialization: stratified LHS-like + a few pure randoms ----\n        pop_target = min(self.popsize, max(6, self.budget // 10))\n        # stratified per-dim sampling\n        initial = []\n        n0 = pop_target\n        # strata indices and shuffle\n        strata = np.zeros((n0, self.dim))\n        for d in range(self.dim):\n            perm = np.arange(n0)\n            self.rng.shuffle(perm)\n            u = (perm + self.rng.rand(n0)) / float(n0)\n            strata[:, d] = u\n        pts = lb + (ub - lb) * strata\n        for i in range(n0):\n            initial.append(pts[i].copy())\n        # add a few pure uniforms for diversity\n        extra = min(6, max(2, int(self.dim // 2)))\n        for _ in range(extra):\n            initial.append(self._uniform_array(lb, ub, n=1)[0])\n        # ensure not more than budget\n        archive_X = []\n        archive_f = []\n        archive_age = []\n\n        # evaluate initial population until budget exhausted\n        for x in initial:\n            if self.evals >= self.budget:\n                break\n            x_ref = self._reflect_bounds(x, lb, ub)\n            f = safe_eval(x_ref)\n            archive_X.append(x_ref)\n            archive_f.append(f)\n            archive_age.append(0)\n\n        # If nothing evaluated (e.g., budget==0) return fallback (center)\n        if len(archive_X) == 0:\n            x0 = 0.5 * (lb + ub)\n            return np.inf, x0\n\n        archive_X = np.vstack(archive_X)\n        archive_f = np.array(archive_f, dtype=float)\n        archive_age = np.array(archive_age, dtype=int)\n\n        # initial best\n        finite_mask = np.isfinite(archive_f)\n        if np.any(finite_mask):\n            best_idx = np.argmin(archive_f[finite_mask])\n            # translate index back\n            finite_idxs = np.where(finite_mask)[0]\n            best_idx = finite_idxs[best_idx]\n            f_best = float(archive_f[best_idx])\n            x_best = archive_X[best_idx].copy()\n        else:\n            # no finite values yet, pick first\n            f_best = float(archive_f[0])\n            x_best = archive_X[0].copy()\n\n        # main loop\n        stagnation_since = 0\n        no_improve_iters = 0\n        iter_count = 0\n        while self.evals < self.budget:\n            iter_count += 1\n            N = archive_X.shape[0]\n\n            # compute per-dim robust scale estimate (IQR fallback to std)\n            norm_scale = np.zeros(self.dim)\n            if N >= 3:\n                q75 = np.percentile(archive_X, 75, axis=0)\n                q25 = np.percentile(archive_X, 25, axis=0)\n                iqr = q75 - q25\n                with np.errstate(invalid=\"ignore\"):\n                    est = iqr / 1.349\n                # where iqr is zero, use std\n                zero_mask = (est <= 0) | ~np.isfinite(est)\n                if np.any(zero_mask):\n                    est2 = np.std(archive_X, axis=0, ddof=0)\n                    est[zero_mask] = est2[zero_mask]\n                # ensure not too small relative to domain\n                min_scale = domain_scale / max(8.0, self.dim)\n                est = np.maximum(est, min_scale * 1e-3)\n                norm_scale = est\n            else:\n                norm_scale = domain_scale / 8.0\n\n            # determine elites (by f)\n            finite_inds = np.where(np.isfinite(archive_f))[0]\n            if finite_inds.size == 0:\n                elites = np.arange(min(3, N))\n            else:\n                k_elite = max(2, min( max(2, N // 5), 10))\n                sorted_inds = finite_inds[np.argsort(archive_f[finite_inds])]\n                elites = sorted_inds[:min(k_elite, sorted_inds.size)]\n                # if not enough elites, pad with random indices\n                if elites.size < k_elite:\n                    rest = np.setdiff1d(np.arange(N), elites)\n                    if rest.size > 0:\n                        add = self.rng.choice(rest, size=min(len(rest), k_elite - elites.size), replace=False)\n                        elites = np.concatenate([elites, add])\n\n            # pick anchor among elites with exponential bias (better elites preferred)\n            if elites.size > 1 and np.any(np.isfinite(archive_f[elites])):\n                elite_fs = archive_f[elites]\n                ranks = np.argsort(elite_fs)\n                rank_scores = np.exp(-np.arange(len(ranks)) / max(1.0, len(ranks) / 3.0))\n                # normalized probabilities favoring better ranks\n                p = rank_scores / np.sum(rank_scores)\n                # map probabilities back to actual elite indices by rank order\n                anchor_idx = elites[ranks[self.rng.choice(len(ranks), p=p)]]\n            else:\n                anchor_idx = elites[0] if elites.size > 0 else self.rng.randint(N)\n\n            # adapt base strategy probabilities slightly with gscale\n            # strategies: 0=DE-style, 1=PCA-elite, 2=anisotropic local, 3=tempered Cauchy\n            base_probs = np.array([0.35, 0.25, 0.25, 0.15])\n            # favor cauchy more when gscale large\n            bias = min(0.5, self.gscale * 2.0)\n            base_probs[3] += bias * 0.5\n            base_probs[:3] *= (1.0 - base_probs[3]) / base_probs[:3].sum()\n            strat = self.rng.choice(4, p=base_probs)\n\n            # helper to pick distinct donors for DE\n            def pick_distinct(k, exclude_idx=None):\n                pool = list(range(N))\n                if exclude_idx is not None and exclude_idx in pool:\n                    pool.remove(exclude_idx)\n                if len(pool) < k:\n                    # sample with replacement if not enough\n                    return [self.rng.choice(pool) for _ in range(k)]\n                else:\n                    return list(self.rng.choice(pool, size=k, replace=False))\n\n            # compute per-dim normalized scale for moves\n            move_scale = np.maximum(norm_scale, domain_scale * 1e-6)\n            move_scale *= self.gscale + 1e-12\n\n            # Choose a base vector: pick an \"anchor\" and maybe a current vector\n            cur_idx = self.rng.randint(N)\n            base = archive_X[cur_idx].copy()\n            anchor = archive_X[anchor_idx].copy()\n\n            # Build candidate\n            if strat == 0:\n                # Strategy 0: DE-style differential injection (current-to-rand variant)\n                # pick donors\n                d1, d2 = pick_distinct(2, exclude_idx=cur_idx)\n                donor1 = archive_X[d1]\n                donor2 = archive_X[d2]\n                # current-to-rand like: base + F*(donor1 - donor2)\n                cand = base + self.F * (donor1 - donor2)\n                # anisotropic jitter\n                jitter = (self.rng.randn(self.dim) * move_scale * (0.5 + self.rng.rand()))\n                cand += jitter\n                # binomial crossover with an elite partner (anchor)\n                mask = self.rng.rand(self.dim) < self.CR\n                if not np.any(mask):\n                    mask[self.rng.randint(self.dim)] = True\n                cand[mask] = anchor[mask]\n                # occasional tempered cauchy injection on subset\n                if self.rng.rand() < 0.08 + 0.5 * (self.gscale):\n                    tc = self._tempered_cauchy(move_scale)\n                    mix_mask = self.rng.rand(self.dim) < 0.15\n                    cand[mix_mask] += tc[mix_mask]\n            elif strat == 1:\n                # Strategy 1: PCA-guided elite perturbation\n                # compute PCA on elites (centered)\n                E = archive_X[elites]\n                center = E.mean(axis=0)\n                Xc = E - center\n                try:\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                except Exception:\n                    U, S, Vt = np.zeros_like(Xc), np.zeros(min(Xc.shape)), np.zeros((min(Xc.shape), self.dim))\n                # sample coefficients scaled by singular values and gscale\n                comps = min(3, Vt.shape[0])\n                coeffs = np.zeros(self.dim)\n                for j in range(comps):\n                    step = (self.rng.randn() * S[j] * (0.5 + self.gscale))\n                    coeffs += step * Vt[j]\n                cand = center + coeffs\n                # blend slightly towards a randomly chosen elite\n                partner = archive_X[self.rng.choice(elites)]\n                blend = 0.2 + 0.6 * self.rng.rand()\n                cand = blend * cand + (1 - blend) * partner\n                # small isotropic jitter\n                cand += 0.05 * move_scale * self.rng.randn(self.dim)\n            elif strat == 2:\n                # Strategy 2: Local anisotropic gaussian\n                # anisotropic per-dim sigma proportional to spread but smaller\n                local_sigma = move_scale * (0.2 + 0.8 * self.rng.rand(self.dim))\n                cand = anchor + self.rng.randn(self.dim) * local_sigma\n                # ensure at least one coord perturbed\n                if self.rng.rand() < 0.6:\n                    i = self.rng.randint(self.dim)\n                    cand[i] += (self.rng.randn() * local_sigma[i] * 3.0)\n                # occasional mixed uniform per-dim injection\n                if self.rng.rand() < 0.07:\n                    mix = self.rng.rand(self.dim) < 0.08\n                    if np.any(mix):\n                        cand[mix] = lb[mix] + (ub[mix] - lb[mix]) * self.rng.rand(np.sum(mix))\n            else:\n                # Strategy 3: tempered Cauchy global escape\n                tc = self._tempered_cauchy(domain_scale * (0.5 + self.gscale))\n                cand = anchor + tc\n                # sometimes add directed differential component if donors exist\n                if N >= 3 and self.rng.rand() < 0.4:\n                    d1, d2 = pick_distinct(2, exclude_idx=None)\n                    cand += 0.5 * self.F * (archive_X[d1] - archive_X[d2])\n\n            # occasional directed small jump to current best for refinement\n            if self.rng.rand() < 0.06 and np.isfinite(f_best):\n                cand = 0.6 * cand + 0.4 * x_best + 0.03 * move_scale * self.rng.randn(self.dim)\n\n            # sanitize candidate: replace nan/inf coordinates by uniform draws\n            bad = (~np.isfinite(cand)) | (cand < -1e300) | (cand > 1e300)\n            if np.any(bad):\n                cand[bad] = lb[bad] + (ub[bad] - lb[bad]) * self.rng.rand(np.sum(bad))\n\n            # reflect to bounds\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget remains\n            if self.evals < self.budget:\n                f_cand = safe_eval(cand)\n            else:\n                break\n\n            # increment ages of others\n            archive_age += 1\n\n            # update archive (append)\n            archive_X = np.vstack([archive_X, cand])\n            archive_f = np.concatenate([archive_f, [f_cand]])\n            archive_age = np.concatenate([archive_age, [0]])\n\n            # update best if improved\n            improved = np.isfinite(f_cand) and (f_cand < f_best)\n            if improved:\n                f_best = float(f_cand)\n                x_best = cand.copy()\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # measure success: success if in top 25% of finite archive or improved global best\n            finite_mask = np.isfinite(archive_f)\n            success = False\n            if np.isfinite(f_cand):\n                if improved:\n                    success = True\n                else:\n                    fin_fs = archive_f[finite_mask]\n                    if fin_fs.size > 0:\n                        thresh = np.percentile(fin_fs, 25)\n                        if f_cand <= thresh:\n                            success = True\n            self.window.append(1 if success else 0)\n\n            # short-term adaptation of global scale based on recent success rate\n            if len(self.window) >= 6:\n                sr = float(sum(self.window)) / len(self.window)\n                # low success -> increase exploration, high success -> tighten\n                if sr < 0.15:\n                    self.gscale = min(2.0, self.gscale * 1.08 + 0.005)\n                elif sr > 0.35:\n                    self.gscale = max(1e-4, self.gscale * 0.93 - 0.002)\n                else:\n                    # slight decay toward stable region\n                    self.gscale = max(1e-5, self.gscale * 0.995 + 0.0001)\n\n            # prune archive when too large (keep best + some random + some oldest)\n            if archive_X.shape[0] > self.max_archive:\n                Nnow = archive_X.shape[0]\n                # sort by f (finite first)\n                finite_mask = np.isfinite(archive_f)\n                fin_idxs = np.where(finite_mask)[0]\n                inf_idxs = np.where(~finite_mask)[0]\n                keep = []\n                # keep some best\n                k_best = min( int(0.2 * self.max_archive), max(3, int(0.05 * Nnow)))\n                if fin_idxs.size > 0:\n                    best_fin = fin_idxs[np.argsort(archive_f[fin_idxs])][:k_best]\n                    keep.extend(best_fin.tolist())\n                # keep some oldest\n                k_old = min(int(0.2 * self.max_archive), max(3, int(0.05 * Nnow)))\n                oldest = np.argsort(-archive_age)[:k_old]\n                keep.extend([i for i in oldest.tolist() if i not in keep])\n                # add random subset from remaining\n                remaining = [i for i in range(Nnow) if i not in keep]\n                n_needed = max(0, self.max_archive - len(keep))\n                if len(remaining) > 0 and n_needed > 0:\n                    chosen = self.rng.choice(remaining, size=min(len(remaining), n_needed), replace=False)\n                    keep.extend(chosen.tolist())\n                keep = sorted(set(keep))\n                archive_X = archive_X[keep]\n                archive_f = archive_f[keep]\n                archive_age = archive_age[keep]\n\n                # compact and deduplicate by proximity (simple)\n                # remove duplicates that are extremely close\n                to_keep = []\n                seen = []\n                for i, x in enumerate(archive_X):\n                    close = False\n                    for j in seen:\n                        if np.linalg.norm(x - archive_X[j]) < 1e-9 + 1e-6 * np.mean(domain_scale):\n                            close = True\n                            break\n                    if not close:\n                        to_keep.append(i)\n                        seen.append(i)\n                archive_X = archive_X[to_keep]\n                archive_f = archive_f[to_keep]\n                archive_age = archive_age[to_keep]\n\n            # stagnation detection & micro-restarts\n            if no_improve_iters > max(200, 20 * self.dim):\n                # micro-restart near best or random if best is inf\n                no_improve_iters = 0\n                if np.isfinite(f_best):\n                    center = x_best.copy()\n                else:\n                    # pick a good finite or random\n                    fin_idxs = np.where(np.isfinite(archive_f))[0]\n                    if fin_idxs.size > 0:\n                        center = archive_X[self.rng.choice(fin_idxs)].copy()\n                    else:\n                        center = lb + domain_scale * self.rng.rand(self.dim)\n                # create a local cloud\n                n_cloud = min( max(6, self.popsize // 2), self.budget - self.evals)\n                cloud_sigma = domain_scale * (0.02 + 0.3 * self.gscale)\n                for _ in range(n_cloud):\n                    if self.evals >= self.budget:\n                        break\n                    cand = center + cloud_sigma * self.rng.randn(self.dim)\n                    cand = self._reflect_bounds(cand, lb, ub)\n                    f_cand = safe_eval(cand)\n                    archive_X = np.vstack([archive_X, cand])\n                    archive_f = np.concatenate([archive_f, [f_cand]])\n                    archive_age = np.concatenate([archive_age, [0]])\n                    if np.isfinite(f_cand) and f_cand < f_best:\n                        f_best = float(f_cand)\n                        x_best = cand.copy()\n                # slightly increase exploration scales and reset small window\n                self.gscale = min(2.0, self.gscale * 1.2 + 0.02)\n                self.window.clear()\n\n            # occasionally verbose\n            if self.verbose and (iter_count % 100 == 0 or self.evals >= self.budget):\n                print(\n                    f\"[MG-ADS] evals={self.evals}/{self.budget}, archive={archive_X.shape[0]}, f_best={f_best:.4e}, gscale={self.gscale:.4e}\"\n                )\n\n        # final best from finite archive if any\n        finite_mask = np.isfinite(archive_f)\n        if np.any(finite_mask):\n            idx = np.argmin(archive_f[finite_mask])\n            fin_idxs = np.where(finite_mask)[0]\n            idx = fin_idxs[idx]\n            return float(archive_f[idx]), archive_X[idx].copy()\n        else:\n            # all infinite: return a uniform fallback point in the domain center\n            x0 = 0.5 * (lb + ub)\n            return np.inf, x0", "configspace": "", "generation": 0, "feedback": "In the code, line 32, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.window = deque(maxlen=50)", "error": "In the code, line 32, in __init__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: self.window = deque(maxlen=50)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}

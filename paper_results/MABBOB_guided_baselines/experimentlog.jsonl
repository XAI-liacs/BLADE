{"method_name": "LHNS:vns", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "LHNS:vns", "budget": 200, "kwargs": {"method": "vns", "minimisation": false}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "9f6730b0-60ac-45f0-abbf-1be0e6ad71b1", "fitness": 0.6101040829433475, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality (required)\n    - pop_size: optional population size\n    - seed: RNG seed (optional)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # read bounds and ensure arrays of correct shape\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # If bounds are scalar, expand\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        # ensure shapes match dim\n        if lb.size != self.dim or ub.size != self.dim:\n            raise ValueError(\"Bounds size doesn't match solver dim\")\n\n        # quick fallback if budget very small: pure random search\n        if self.budget <= 2:\n            self.f_opt = np.inf\n            self.x_opt = None\n            remaining = self.budget\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f = func(x)\n                remaining -= 1\n                if f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n            return self.f_opt, self.x_opt\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure numpy array float and correct shape\n            x = np.array(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            nonlocal_update_best(f, x)\n            return f, x\n\n        def nonlocal_update_best(f, x):\n            # use outer scope variables self.f_opt/self.x_opt\n            if f < getattr(self, \"f_opt\", np.inf):\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # Initialize population\n        # choose population based on dim and budget but modest\n        default_pop = max(4, min(self.dim + 2, 20))\n        if self.pop_size is None:\n            pop_size = min(default_pop, max(2, self.budget // 12))\n        else:\n            pop_size = max(2, int(self.pop_size))\n            pop_size = min(pop_size, max(2, self.budget // 4))\n\n        # initial scale relative to bounds\n        base_scale = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        pop = []\n        pop_sigma = []\n        pop_f = []\n\n        # seed initial population around a random center\n        center = np.random.uniform(lb, ub)\n        # ensure at least one global random sample\n        for i in range(pop_size):\n            # add small perturbations around center and randoms\n            if i == 0:\n                x = center.copy()\n            else:\n                if np.random.rand() < 0.5:\n                    x = center + np.random.normal(0, base_scale, size=self.dim)\n                else:\n                    x = np.random.uniform(lb, ub)\n            x = np.clip(x, lb, ub)\n            try:\n                f, x = callf(x)\n            except RuntimeError:\n                break\n            pop.append(x.copy())\n            pop_f.append(f)\n            pop_sigma.append(base_scale * (1.0 + 0.5 * np.random.rand()))\n\n        # If budget used up during init, handle\n        if len(pop) == 0:\n            # fallback: random sampling until exhausted\n            self.f_opt = np.inf\n            self.x_opt = None\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f = func(x)\n                remaining -= 1\n                if f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # main loop\n        stagnation = 0\n        iters = 0\n        while remaining > 0:\n            iters += 1\n            # small tournament selection\n            k = min(3, len(pop))\n            candidates = np.random.choice(len(pop), size=k, replace=False)\n            parent_i = candidates[np.argmin(pop_f[candidates])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction\n            d = np.random.normal(0, 1, size=self.dim)\n            nr = np.linalg.norm(d)\n            if nr <= 0:\n                continue\n            d = d / nr\n\n            # primary directional trial with stochasticized step-length (Cauchy for heavy-tail)\n            # sample step multiplier from truncated Cauchy to avoid infinite steps\n            raw_c = np.random.standard_cauchy()\n            step_len = sigma * (1.0 + 0.8 * np.tanh(raw_c))  # tanh to avoid extremely large\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n            success = False\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i] - 1e-12:\n                # accept and increase sigma a bit\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.18, np.max(ub - lb))\n                success = True\n                stagnation = 0\n            else:\n                # local backtracking: try a few fractional steps along d\n                frac_list = [0.5, 0.25, 0.125, 0.0625]\n                for frac in frac_list:\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + frac * sigma * d, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i] - 1e-12:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n                        success = True\n                        stagnation = 0\n                        break\n\n            # orthogonal perturbation for local diversification\n            if remaining > 0 and nr > 1e-12:\n                r = np.random.normal(0, 1, size=self.dim)\n                # make orthogonal to d: subtract projection\n                r = r - np.dot(r, d) * d\n                rr = np.linalg.norm(r)\n                if rr > 1e-12:\n                    r = r / rr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i] - 1e-12:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                            success = True\n                            stagnation = 0\n                    except RuntimeError:\n                        break\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            jump_prob = 0.035 + 0.0005 * self.dim  # small scaling with dim\n            if (np.random.rand() < jump_prob) and remaining > 0:\n                # Cauchy vector per-dim, but normalize to robust scale\n                raw = np.random.standard_cauchy(size=self.dim)\n                # robust scale to limit extremely large values while preserving heavy-tail\n                med = np.median(np.abs(raw)) + 1e-12\n                levy_vec = raw / med\n                jump_scale = base_scale * (1.0 + 5.0 * np.random.rand())\n                x_jump = np.clip(x_parent + jump_scale * levy_vec, lb, ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                # if it's good, replace the worst in population\n                worst_i = np.argmax(pop_f)\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump.copy()\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(base_scale * 0.5, 1e-12)\n                    success = True\n                    stagnation = 0\n                else:\n                    # sometimes keep as candidate by replacing parent (explorative)\n                    if np.random.rand() < 0.1:\n                        pop[parent_i] = x_jump.copy()\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.7, 1e-12)\n                        success = True\n                        stagnation = 0\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0:\n                # pick two best\n                best_idx = np.argsort(pop_f)[:2]\n                if len(best_idx) == 2:\n                    x_mix = 0.5 * (pop[best_idx[0]] + pop[best_idx[1]])\n                else:\n                    x_mix = pop[best_idx[0]].copy()\n                # small gaussian noise scaled by mean sigma\n                mix_noise = np.random.normal(0, 0.25 * np.mean(pop_sigma), size=self.dim)\n                x_mix = np.clip(x_mix + mix_noise, lb, ub)\n                try:\n                    f_mix, x_mix = callf(x_mix)\n                except RuntimeError:\n                    break\n                # replace parent or worst depending on improvement\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix.copy()\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n                    success = True\n                    stagnation = 0\n                else:\n                    worst_i = np.argmax(pop_f)\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix.copy()\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = max(0.8 * np.mean(pop_sigma), 1e-12)\n                        success = True\n                        stagnation = 0\n\n            # adapt parent sigma on failure\n            if not success:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.88, 1e-12)\n                stagnation += 1\n            else:\n                # mild increase on success for that individual\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * (1.05 + 0.03 * np.random.rand()),\n                                          np.max(ub - lb))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (np.random.rand() < 0.06 or stagnation > 12):\n                worst_i = np.argmax(pop_f)\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_scale * (0.5 + np.random.rand())\n\n            # small population shrink/grow: if some individuals are obsolete (very close), reinitialize one\n            if remaining > 0 and len(pop) > 1:\n                # if two individuals extremely close, re-seed the worse\n                dmat = np.linalg.norm(pop - pop.mean(axis=0), axis=1)\n                if np.min(dmat) < 1e-8:\n                    worst_i = np.argmax(pop_f)\n                    x_new = np.random.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_scale * (0.3 + 0.7 * np.random.rand())\n\n            # safety: keep arrays consistent lengths (in case budget exhausted during steps)\n            if remaining <= 0:\n                break\n\n            # small safeguard: if stagnation extremely long, perform a bigger Lévy jump\n            if stagnation > 40 and remaining > 0:\n                worst_i = np.argmax(pop_f)\n                raw = np.random.standard_cauchy(size=self.dim)\n                med = np.median(np.abs(raw)) + 1e-12\n                levy_vec = raw / med\n                jump_scale = base_scale * (2.0 + 8.0 * np.random.rand())\n                x_jump = np.clip(pop[worst_i] + jump_scale * levy_vec, lb, ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_jump.copy()\n                pop_f[worst_i] = f_jump\n                pop_sigma[worst_i] = base_scale * 0.5\n                stagnation = 0\n\n        # finished budget or exhausted loop\n        # ensure best returned\n        if getattr(self, \"f_opt\", None) is None:\n            # fallback to best in population\n            idx = np.argmin(pop_f)\n            self.f_opt = float(pop_f[idx])\n            self.x_opt = pop[idx].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.610 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09846467886612897, 0.16854999901730205, 0.860801891691015, 0.9598384081587479, 0.8884185246643306, 0.9094708517765246, 0.30166869567260135, 0.855659477905328, 0.880894157760645, 0.17727414392085217]}, "task_prompt": ""}, "log_dir": "run-LHNSvns-MA_BBOB-2", "seed": 2}
{"method_name": "LHNS:vns", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "LHNS:vns", "budget": 200, "kwargs": {"method": "vns", "minimisation": false}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "b86b42f7-8ab4-421c-85e6-8231d5a94e54", "fitness": 0.7060722492679999, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line: Adaptive DE with occasional Lévy global jumps and a shrinking/growing trust-region local search.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # default population scales with dimension but limited by budget\n        if pop_size is None:\n            # try to keep population useful but not too large relative to budget\n            self.pop_size = int(min(max(8, 6 * self.dim), max(4, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.default_rng(self.seed)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            return np.full(self.dim, float(b))\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size != self.dim:\n            # attempt broadcasting\n            return np.resize(b.astype(float), (self.dim,))\n        return b.astype(float)\n\n    def __call__(self, func):\n        # bounds (Many BBOB provides func.bounds.lb / ub)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        norm_range = np.linalg.norm(range_vec)\n        # safety: if range is zero (degenerate) set small positive\n        if norm_range == 0:\n            norm_range = 1.0\n\n        # If budget extremely small: do simple random sampling and return best\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n\n        # If budget smaller than population, sample only budget many points uniformly\n        if self.budget < self.pop_size:\n            # limited evaluations only\n            fbest = np.inf\n            xbest = None\n            for _ in range(self.budget):\n                x = lb + self.rng.random(self.dim) * range_vec\n                f = func(x)\n                if f < fbest:\n                    fbest = f\n                    xbest = x.copy()\n            self.f_opt = fbest\n            self.x_opt = xbest\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = func(pop[i])\n            evals += 1\n\n        # If we ran out of budget during initialization\n        if evals >= self.budget:\n            # find best among evaluated\n            idx = np.argmin(fvals)\n            self.f_opt = float(fvals[idx])\n            self.x_opt = pop[idx].copy()\n            return self.f_opt, self.x_opt\n\n        # initialize strategy/adaptation parameters\n        F_mean = 0.5\n        CR_mean = 0.9\n        tau_F = 0.1  # probability to mutate F in jDE style\n        tau_CR = 0.1\n        trust_radius = 0.2 * norm_range  # scalar trust radius\n        p_levy = 0.05  # initial chance of performing a Lévy jump instead of DE\n        stagnation_counter = 0\n        stagnation_threshold = max(20, 20 * self.dim)\n        gen = 0\n\n        # helpers\n        def levy_step(scale=1.0):\n            # Use Cauchy (heavy-tailed) as a simple Lévy-like step, clipped for stability\n            s = self.rng.standard_cauchy(self.dim)\n            s = np.clip(s, -10.0, 10.0)\n            # normalize to avoid extremely small magnitude and scale by given factor\n            norm = np.linalg.norm(s)\n            if norm == 0:\n                s = self.rng.normal(0, 1.0, self.dim)\n                norm = max(1e-8, np.linalg.norm(s))\n            s = s / norm\n            # add random amplitude drawn from a Pareto-like tail using power law\n            amp = (1.0 + self.rng.pareto(1.5))  # heavy tail amplitude\n            return s * amp * scale\n\n        # global best\n        best_idx = int(np.argmin(fvals))\n        f_best = float(fvals[best_idx])\n        x_best = pop[best_idx].copy()\n\n        # main loop: run generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # one generation: iterate over population members\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n            improved_in_gen = False\n            for i in indices:\n                if evals >= self.budget:\n                    break\n\n                # Adapt individual parameters (jDE-like)\n                if self.rng.random() < tau_F:\n                    Fi = 0.1 + 0.8 * self.rng.random()\n                else:\n                    # normal jitter around mean\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.95)\n                if self.rng.random() < tau_CR:\n                    CRi = self.rng.random()\n                else:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # Decide whether to do Lévy centered exploration (global jump) or DE\n                if self.rng.random() < p_levy:\n                    # Lévy-based candidate around best with some random scaling\n                    scale = trust_radius / (1.0 + gen * 0.05)  # gradually dampen amplitude across generations\n                    step = levy_step(scale=scale)\n                    candidate = x_best + step * (range_vec / max(1.0, np.linalg.norm(range_vec)))\n                    # occasionally combine with small differential vector to keep diversity\n                    if self.rng.random() < 0.3:\n                        r1, r2 = self.rng.choice(self.pop_size, size=2, replace=False)\n                        candidate += Fi * (pop[r1] - pop[r2]) * 0.1\n                else:\n                    # DE/rand/1 with binomial crossover (target is pop[i])\n                    # pick r1,r2,r3 distinct from i\n                    others = [idx for idx in range(self.pop_size) if idx != i]\n                    r1, r2, r3 = self.rng.choice(others, size=3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    jrand = self.rng.integers(0, self.dim)\n                    mask = self.rng.random(self.dim) < CRi\n                    mask[jrand] = True\n                    candidate = np.where(mask, donor, pop[i])\n                    # small local exploitation bias pushing toward best with small prob\n                    if self.rng.random() < 0.15:\n                        candidate = candidate * 0.85 + x_best * 0.15\n\n                # projection/clipping to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = func(candidate)\n                evals += 1\n\n                # Selection: greedy replacement (DE style)\n                if f_candidate < fvals[i]:\n                    # accept candidate\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    # nudge global parameter means toward successful params (simple exponential moving avg)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    improved_in_gen = True\n                    # if improvement was substantial, slightly reduce trust radius to focus\n                    if f_candidate < f_best - 1e-12:\n                        # local improvement\n                        trust_radius = max(1e-8, trust_radius * 0.90)\n                        stagnation_counter = 0\n                    else:\n                        # small move, mild contraction\n                        trust_radius = max(1e-8, trust_radius * 0.97)\n                else:\n                    # unsuccessful -> slightly increase diversity\n                    trust_radius = min(2.0 * norm_range, trust_radius * 1.002)\n\n                # update global best if needed\n                if f_candidate < f_best:\n                    f_best = float(f_candidate)\n                    x_best = candidate.copy()\n                    # successful global improvement reduces Levy chance\n                    p_levy = max(0.01, p_levy * 0.6)\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # clear local Fi/CRi (just let them go out of scope)\n\n            # End of generation: perform trust-region local search near current best\n            # Number of local samples limited by remaining budget and dimension\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            n_local = min( max(1, int(self.dim // 2)), remaining, 8 )\n            # anisotropic sigma: base trust_radius scaled per-dimension\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # per-dim sigma: trust_radius times small random anisotropy / sqrt(dim)\n                per_dim_scale = (0.3 + 0.7 * self.rng.random(self.dim))\n                sigma = (trust_radius / max(1.0, np.sqrt(self.dim))) * per_dim_scale\n                local_candidate = x_best + self.rng.normal(0.0, 1.0, self.dim) * sigma\n                local_candidate = np.clip(local_candidate, lb, ub)\n                f_local = func(local_candidate)\n                evals += 1\n                if f_local < f_best:\n                    # successful local step: accept and shrink trust region\n                    f_best = float(f_local)\n                    x_best = local_candidate.copy()\n                    trust_radius = max(1e-9, trust_radius * 0.85)\n                    # gently reduce Levy frequency since we're exploiting successfully\n                    p_levy = max(0.01, p_levy * 0.7)\n                    stagnation_counter = 0\n                    improved_in_gen = True\n                else:\n                    # unsuccessful local sample: slightly expand trust radius to escape\n                    trust_radius = min(2.0 * norm_range, trust_radius * 1.03)\n                    stagnation_counter += 1\n\n            # Adjust exploration parameters based on stagnation\n            if stagnation_counter > stagnation_threshold:\n                # strong stagnation: increase global exploration and re-seed half pop\n                p_levy = min(0.6, p_levy * 1.5)\n                num_reseed = max(1, self.pop_size // 2)\n                for j in range(num_reseed):\n                    if evals >= self.budget:\n                        break\n                    idx = self.rng.integers(0, self.pop_size)\n                    pop[idx] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[idx] = func(pop[idx])\n                    evals += 1\n                # enlarge trust radius to escape basin\n                trust_radius = min(2.0 * norm_range, trust_radius * 1.5)\n                stagnation_counter = 0\n                # adjust means to encourage exploration\n                F_mean = np.clip(F_mean * 0.9 + 0.1 * self.rng.random(), 0.05, 0.95)\n                CR_mean = np.clip(CR_mean * 0.9 + 0.1 * self.rng.random(), 0.0, 1.0)\n\n            # Insure we keep a record of the best in pop if we found better while reseeding\n            idx = int(np.argmin(fvals))\n            if fvals[idx] < f_best:\n                f_best = float(fvals[idx])\n                x_best = pop[idx].copy()\n\n            # decay/increase p_levy slowly based on recent progress\n            if not improved_in_gen:\n                # no improvement in generation -> encourage more Lévy jumps\n                p_levy = min(0.5, p_levy * 1.05)\n            else:\n                # progress -> slightly favor DE exploitation\n                p_levy = max(0.01, p_levy * 0.98)\n\n        # finished budget or loop\n        self.f_opt = float(f_best)\n        self.x_opt = x_best.copy() if x_best is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.706 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24369199054875612, 0.23684996082603715, 0.505462462130592, 0.9462896583199361, 0.9056584325551437, 0.9090692023771285, 0.8594419287899415, 0.6843223239356291, 0.9015275779325783, 0.8684089552642569]}, "task_prompt": ""}, "log_dir": "run-LHNSvns-MA_BBOB-3", "seed": 3}
{"method_name": "LHNS:vns", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "LHNS:vns", "budget": 200, "kwargs": {"method": "vns", "minimisation": false}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "aa2826d1-aeee-4573-8d9c-42bc53f77d36", "fitness": 0.736765340829983, "name": "AdaptiveSubspaceCovarianceSearch", "description": "Hybrid covariance-adaptive global search with occasional principal-direction 1D probes and adaptive step-size (Adaptive Subspace Covariance Search — ASCS).", "code": "import numpy as np\n\nclass AdaptiveSubspaceCovarianceSearch:\n    \"\"\"\n    Adaptive Subspace Covariance Search (ASCS)\n    - Hybrid global-local sampler that alternates covariance-adapted multivariate\n      sampling with targeted principal-direction line searches and adaptive step-size.\n    - Budget-aware; respects provided function-evaluation budget and bound constraints.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # default population scales with dimension\n            self.pop_size = max(4 * self.dim, 12)\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(None if seed is None else int(seed))\n\n    def __call__(self, func):\n        # Robustly get bounds as arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.ndim == 0:\n            lb = np.full(self.dim, lb.item())\n        if ub.ndim == 0:\n            ub = np.full(self.dim, ub.item())\n\n        # local helpers\n        def reflect_clip(x):\n            # single symmetric reflection then clip to bounds (safe and simple)\n            x = np.array(x, dtype=float)\n            # reflect values above upper bound\n            over = x > ub\n            if np.any(over):\n                x[over] = ub[over] - (x[over] - ub[over])\n            under = x < lb\n            if np.any(under):\n                x[under] = lb[under] - (lb[under] - x[under])\n            # final clip to ensure in bounds\n            return np.clip(x, lb, ub)\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initialize a small random sampling seed (n_init) while respecting budget\n        n_init = min(max(6, 2 * self.dim), max(1, self.budget // 10))\n        n_init = min(n_init, self.budget)\n        samples_x = []\n        samples_f = []\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            samples_x.append(x.copy())\n            samples_f.append(float(f))\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n            if evals >= self.budget:\n                break\n\n        # If no evaluations were possible (budget==0), return\n        if evals == 0:\n            return f_opt, x_opt\n\n        # center and its fitness\n        center = x_opt.copy()\n        f_center = f_opt\n\n        # initial covariance: anisotropic diagonal proportional to (range/4)^2\n        range_vec = ub - lb\n        init_var = (range_vec / 4.0) ** 2\n        init_var = np.maximum(init_var, 1e-12)\n        C = np.diag(init_var)\n\n        # initial global step-size multiplier (sigma) scale ~ quarter of average range\n        sigma = 0.25 * np.mean(range_vec)\n        sigma = max(sigma, 1e-8)\n\n        # adaptation parameters\n        alpha_cov = 0.18   # covariance mixing rate\n        inc_factor = 1.2\n        dec_factor = 0.88\n        min_sigma = 1e-8\n        max_sigma = 5.0 * np.mean(range_vec)\n\n        # stagnation control\n        iter_count = 0\n        last_improve_iter = 0\n        patience = max(10, 5 * self.dim)\n\n        # main loop: sample around center, adapt covariance and sigma, and occasional 1D probes\n        while evals < self.budget:\n            rem = self.budget - evals\n            bs = min(self.pop_size, rem)\n            batch_x = []\n            batch_f = []\n\n            # Sample candidates and evaluate sequentially (respect budget)\n            for k in range(bs):\n                # draw from N(0, C) scaled by sigma\n                try:\n                    # sample zero-mean multivariate normal with covariance C\n                    z = self.rng.multivariate_normal(np.zeros(self.dim), C)\n                except Exception:\n                    # fallback to isotropic/directional using diag of C\n                    diag = np.maximum(np.diag(C), 1e-20)\n                    z = self.rng.randn(self.dim) * np.sqrt(diag)\n                x = reflect_clip(center + sigma * z)\n                f = func(x)\n                evals += 1\n\n                batch_x.append(x.copy())\n                batch_f.append(float(f))\n\n                # immediate global best update\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n\n                if evals >= self.budget:\n                    break\n\n            if len(batch_f) == 0:\n                break\n\n            # find best in batch\n            best_idx = int(np.argmin(batch_f))\n            best_x = batch_x[best_idx]\n            best_f = batch_f[best_idx]\n\n            improved = False\n            # If the batch produced an improvement vs center -> move center toward the best\n            if best_f < f_center:\n                improved = True\n                lr = 0.6  # learning rate toward best\n                prev_center = center.copy()\n                center = lr * best_x + (1 - lr) * center\n                center = reflect_clip(center)\n                f_center = float(best_f)\n                sigma = min(max_sigma, sigma * inc_factor)\n                last_improve_iter = iter_count\n                # small random perturb to escape local saddle\n                center = reflect_clip(center + 0.03 * sigma * self.rng.randn(self.dim))\n            else:\n                # no immediate improvement: modest shrinkage\n                sigma = max(min_sigma, sigma * dec_factor)\n\n            # Build covariance update from top-performing samples in the batch\n            bs_avail = len(batch_x)\n            top_k = max(2, bs_avail // 3)\n            # indices of best top_k samples\n            idxs = np.argsort(batch_f)[:top_k]\n            X = np.vstack([batch_x[i] for i in idxs]) - center  # deviations from new center\n\n            # compute sample covariance (biased estimate ok) and scale to maintain average scale\n            if X.shape[0] == 1:\n                S = np.outer(X[0], X[0])\n            else:\n                # rowvar=False equivalent: each row is an observation\n                # Use covariance = (X^T X) / n_samples\n                S = (X.T @ X) / float(X.shape[0])\n\n            # normalize S so its mean variance equals current average diag(C)\n            mean_var_C = np.mean(np.diag(C))\n            mean_var_S = np.trace(S) / float(self.dim) if np.trace(S) > 0 else 0.0\n            if mean_var_S <= 0:\n                S_scaled = S * 0.0 + np.diag(init_var) * 1e-4\n            else:\n                S_scaled = S * (mean_var_C / (mean_var_S + 1e-16))\n\n            # mix into covariance matrix\n            C = (1 - alpha_cov) * C + alpha_cov * S_scaled\n            # stabilize covariance (small ridge)\n            C += 1e-12 * np.eye(self.dim)\n\n            # Occasionally perform directed 1D line probes along principal axis\n            if (iter_count % max(3, self.dim // 2) == 0) and (evals < self.budget):\n                # principal eigenvector\n                try:\n                    w, v = np.linalg.eigh(C)\n                    pv = v[:, np.argmax(w.real)].real\n                except Exception:\n                    # fallback to random direction\n                    pv = self.rng.randn(self.dim)\n                    pv /= np.linalg.norm(pv) + 1e-12\n\n                # try a set of relative step sizes along pv and -pv, prefer smaller first\n                probe_steps = [0.5, 1.0, 2.0]\n                for s in probe_steps + [-p for p in probe_steps]:\n                    if evals >= self.budget:\n                        break\n                    x_try = reflect_clip(center + sigma * s * pv)\n                    f_try = func(x_try)\n                    evals += 1\n                    if f_try < f_center:\n                        center = x_try.copy()\n                        f_center = float(f_try)\n                        # encourage covariance to align with pv by rank-1 boost\n                        rank1 = np.outer(pv, pv) * (np.mean(np.diag(C)) + 1e-12)\n                        C = (1 - 0.5 * alpha_cov) * C + (0.5 * alpha_cov) * rank1\n                        sigma = min(max_sigma, sigma * (inc_factor ** 1.1))\n                        last_improve_iter = iter_count\n                        # update global best if needed\n                        if f_try < f_opt:\n                            f_opt = float(f_try)\n                            x_opt = x_try.copy()\n                        # after a successful probe, stop other probes to conserve budget\n                        break\n\n            # stagnation handling: if no improvement for long, increase exploratory jitter\n            if (iter_count - last_improve_iter) > patience:\n                # random restart/large jump with low probability\n                if self.rng.rand() < 0.15:\n                    jump = 0.5 * np.mean(range_vec) * (1.0 + self.rng.randn(self.dim) * 0.5)\n                    center = reflect_clip(center + jump)\n                    # reinitialize sigma to encourage global exploration\n                    sigma = min(max_sigma, sigma * 2.0)\n                    last_improve_iter = iter_count\n                else:\n                    # gradually cool down sigma to focus\n                    sigma = max(min_sigma, sigma * dec_factor)\n\n            iter_count += 1\n\n            # safety cap sigma\n            sigma = np.clip(sigma, min_sigma, max_sigma)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSubspaceCovarianceSearch scored 0.737 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1292277143337709, 0.8171395650060508, 0.8854606728619763, 0.935338858764542, 0.8779836457931969, 0.9078507160179624, 0.32601099400848266, 0.7261631001011908, 0.8921865055979407, 0.8702916358147161]}, "task_prompt": ""}, "log_dir": "run-LHNSvns-MA_BBOB-4", "seed": 4}
{"method_name": "LHNS:vns", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "LHNS:vns", "budget": 200, "kwargs": {"method": "vns", "minimisation": false}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "eaaf06ac-5dff-436f-94fa-73357144ffdd", "fitness": 0.4236228314297604, "name": "AdaptiveDirectionalSampling", "description": "Hybrid Adaptive Directional Sampling (ADS) — a memory-guided sampler mixing DE-style donors, PCA-guided elite moves, anisotropic Gaussian local search, heavy-tailed Cauchy jumps and occasional micro-restarts with adaptive global scaling to robustly explore [-5,5]^dim boxes.", "code": "import numpy as np\n\nclass AdaptiveDirectionalSampling:\n    \"\"\"\n    Hybrid Adaptive Directional Sampling (ADS).\n\n    One-line: Hybrid adaptive sampler mixing DE-like donors, principal-direction local moves,\n    heavy-tailed jumps and occasional micro-restarts to robustly explore [-5,5]^dim search spaces.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 elite_frac=0.20, init_scale=0.20,\n                 keep_memory=None, F=0.6, cr=0.9, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.elite_frac = float(elite_frac)\n        self.init_scale = float(init_scale)\n        if keep_memory is None:\n            self.keep_memory = max(200, 20 * self.dim)\n        else:\n            self.keep_memory = int(max(10, keep_memory))\n        self.F = float(F)\n        self.cr = float(cr)\n\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _extract_bounds(self, func):\n        # Try common formats, else fallback to [-5,5]^dim\n        lb = None\n        ub = None\n        try:\n            # If func has .bounds attribute that is an object with lb/ub\n            b = getattr(func, \"bounds\", None)\n            if b is not None:\n                # handle object with attributes\n                lb_attr = getattr(b, \"lb\", None)\n                ub_attr = getattr(b, \"ub\", None)\n                if lb_attr is not None and ub_attr is not None:\n                    lb = np.asarray(lb_attr, dtype=float)\n                    ub = np.asarray(ub_attr, dtype=float)\n                else:\n                    # maybe bounds is a tuple/list (lb, ub)\n                    if isinstance(b, (tuple, list)) and len(b) >= 2:\n                        lb = np.asarray(b[0], dtype=float)\n                        ub = np.asarray(b[1], dtype=float)\n        except Exception:\n            lb = None\n            ub = None\n\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        else:\n            # broadcast/resize to dim\n            try:\n                lb = np.broadcast_to(lb, (self.dim,)).astype(float).copy()\n                ub = np.broadcast_to(ub, (self.dim,)).astype(float).copy()\n            except Exception:\n                lb = np.resize(lb, self.dim).astype(float).copy()\n                ub = np.resize(ub, self.dim).astype(float).copy()\n\n        # ensure valid: if any lb >= ub, fallback to [-5,5]\n        if np.any(lb >= ub):\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        return lb, ub\n\n    def _reflect_bounds(self, x, lb, ub, max_reflect=10):\n        x = np.array(x, dtype=float)\n        for _ in range(max_reflect):\n            below = x < lb\n            above = x > ub\n            if not (below.any() or above.any()):\n                break\n            if below.any():\n                x[below] = 2.0 * lb[below] - x[below]\n            if above.any():\n                x[above] = 2.0 * ub[above] - x[above]\n        # final clamp\n        x = np.minimum(np.maximum(x, lb), ub)\n        return x\n\n    def _uniform_array(self, lb, ub):\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return self.rng.uniform(lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        func(x) must accept a 1D numpy array of length self.dim and return a scalar.\n        \"\"\"\n        lb, ub = self._extract_bounds(func)\n        span = ub - lb\n        base_scale_vec = np.maximum(span * self.init_scale, 1e-12)\n\n        archive_x = []\n        archive_f = []\n\n        # initial sampling budget: ensure some initial diversity\n        evals = 0\n        n_init = min(max(10, 3 * self.dim), max(3, self.budget // 10))\n\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x = self._uniform_array(lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # adaptation & history\n        recent_success = []\n        recent_window = max(50, 5 * self.dim)\n        stagnation_limit = max(40, 10 * self.dim)\n        stagnation_counter = 0\n\n        # move probabilities\n        p_local = 0.45\n        p_de = 0.18\n        p_pca = 0.22\n        # rest goes to heavy-tailed (Cauchy)\n\n        # adaptive global multiplier for PCA coefficients and scales\n        gscale = 1.0\n\n        # bookkeeping for pruning\n        keep_memory = max(self.keep_memory, 50 + 10 * self.dim)\n\n        # main loop\n        while evals < self.budget:\n            # sort archive by fitness\n            if len(archive_f) == 0:\n                sorted_idx = np.array([], dtype=int)\n            else:\n                sorted_idx = np.argsort(archive_f)\n\n            # pick base: mostly best, sometimes random archive or uniform\n            r_base = self.rng.random()\n            if self.x_opt is None or (r_base > 0.8 and len(archive_x) > 0 and self.rng.random() < 0.5):\n                # pick a randomly chosen archive member (bias to better)\n                if len(archive_x) > 0:\n                    idx = sorted_idx[self.rng.integers(low=0, high=len(sorted_idx))]\n                    base = np.array(archive_x[idx], dtype=float)\n                else:\n                    base = self._uniform_array(lb, ub)\n            elif self.x_opt is None:\n                base = self._uniform_array(lb, ub)\n            else:\n                # use best known\n                base = np.array(self.x_opt, dtype=float)\n\n            # adaptive per-dim scale vector (introduce some jitter per-dim)\n            scale = base_scale_vec * gscale\n            scale = np.maximum(scale, 1e-12)\n            # small random per-dim jitter for anisotropy\n            per_dim_jitter = 1.0 + 0.2 * self.rng.normal(0.0, 1.0, size=self.dim)\n            scale = np.maximum(scale * per_dim_jitter, 1e-12)\n\n            r = self.rng.random()\n            cand = None\n\n            # Local anisotropic gaussian around base\n            if r < p_local:\n                # anisotropic multipliers to allow some dimensions larger\n                rand_mult = np.exp(0.5 * self.rng.normal(0.0, 1.0, size=self.dim))\n                jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                cand = base + jitter * (scale * rand_mult)\n\n            # DE-style donor move with crossover\n            elif r < p_local + p_de and len(archive_x) >= 3:\n                # pick three distinct indices\n                idxs = self.rng.choice(len(archive_x), size=3, replace=False)\n                x0 = np.array(archive_x[idxs[0]], dtype=float)\n                x1 = np.array(archive_x[idxs[1]], dtype=float)\n                x2 = np.array(archive_x[idxs[2]], dtype=float)\n                donor = x0 + self.F * (x1 - x2)\n                # crossover mask (ensure at least one dim taken)\n                mask = (self.rng.random(self.dim) < self.cr)\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                cand = np.array(base, dtype=float)\n                cand[mask] = donor[mask]\n                # small gaussian jitter proportional to scale\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # PCA-guided sampling using elites\n            elif r < p_local + p_de + p_pca and len(archive_x) >= 3:\n                n_keep = max(2, int(self.elite_frac * len(archive_x)))\n                n_keep = min(n_keep, len(archive_x))\n                if n_keep < 2:\n                    jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                    cand = base + jitter * scale\n                else:\n                    elite_idx = sorted_idx[:n_keep]\n                    elites = np.array([archive_x[i] for i in elite_idx], dtype=float)\n                    # choose center: either mean or one of elites (randomized)\n                    if self.rng.random() < 0.6:\n                        center = np.mean(elites, axis=0)\n                    else:\n                        center = elites[self.rng.integers(0, elites.shape[0])]\n                    # PCA / SVD\n                    try:\n                        cov = np.cov(elites, rowvar=False)\n                        # small regularization\n                        cov += np.eye(self.dim) * 1e-8 * np.mean(np.diag(cov) if cov.shape[0]>0 else 1.0)\n                        U, Svals, _ = np.linalg.svd(cov)\n                        sqrt_vals = np.sqrt(np.maximum(Svals, 0.0))\n                        # sample coefficients along principal directions (gaussian scaled by eigenvalues)\n                        coeffs = self.rng.normal(0.0, 1.0, size=self.dim) * sqrt_vals\n                        # scale coefficients by adaptive gscale and span\n                        mean_root = max(np.mean(sqrt_vals), 1e-12)\n                        gmult = (gscale * (0.8 + 0.8 * self.rng.random())) / mean_root\n                        coeffs = coeffs * gmult\n                        cand = center + (U @ coeffs)\n                        # small isotropic jitter\n                        cand += self.rng.normal(0.0, 0.05, size=self.dim) * scale\n                    except Exception:\n                        jitter = self.rng.normal(0.0, 1.0, size=self.dim)\n                        cand = base + jitter * scale\n\n            # heavy-tailed per-dim Cauchy jumps (Levy-like)\n            else:\n                u = self.rng.random(self.dim)\n                cauch = np.tan(np.pi * (u - 0.5))\n                # truncate extreme tails by clipping\n                cauch = np.clip(cauch, -1e2, 1e2)\n                mult = (0.6 + 1.4 * self.rng.random(self.dim)) * gscale\n                cand = base + cauch * (scale * 0.6 * mult)\n                cand += self.rng.normal(0.0, 0.02, size=self.dim) * scale\n\n            # Ensure candidate finite and within bounds (reflect)\n            cand = np.array(cand, dtype=float)\n            nonfinite = ~np.isfinite(cand)\n            if nonfinite.any():\n                fill = self._uniform_array(lb, ub)\n                cand[nonfinite] = fill[nonfinite]\n            cand = self._reflect_bounds(cand, lb, ub)\n\n            # evaluate candidate if budget available\n            if evals >= self.budget:\n                break\n            try:\n                f_cand = float(func(cand))\n            except Exception:\n                f_cand = np.inf\n            evals += 1\n\n            # update archives and bests\n            archive_x.append(cand.copy())\n            archive_f.append(f_cand)\n            improved = False\n            if f_cand < self.f_opt - 1e-12:\n                self.x_opt = np.array(cand, dtype=float)\n                self.f_opt = float(f_cand)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # update recent success window\n            recent_success.append(1 if improved else 0)\n            if len(recent_success) > recent_window:\n                recent_success.pop(0)\n            succ_rate = float(np.mean(recent_success)) if len(recent_success) > 0 else 0.0\n\n            # adapt gscale based on short-term success rate\n            # increase when success is good, shrink slowly otherwise\n            if succ_rate > 0.15:\n                gscale *= 1.06\n            else:\n                gscale *= 0.995\n            # small multiplicative jitter for exploration\n            gscale *= (1.0 + 0.02 * (self.rng.random() - 0.5))\n            gscale = float(np.clip(gscale, 1e-6, 1e3))\n\n            # stagnation detection & micro-restart (injections)\n            if (stagnation_counter >= stagnation_limit) or (succ_rate < 0.03 and stagnation_counter > stagnation_limit // 2):\n                # modest injection batch: add a few perturbed elites or randoms\n                injections = min(5, max(1, (self.dim // 4)))\n                old_gscale = gscale\n                gscale = min(gscale * 5.0, 1e2)  # temporarily broaden\n                for _inj in range(injections):\n                    if evals >= self.budget:\n                        break\n                    if len(archive_x) == 0 or self.rng.random() < 0.6:\n                        x_inj = self._uniform_array(lb, ub)\n                    else:\n                        # choose a top elite to perturb\n                        top_k = max(1, int(0.05 * len(archive_x)))\n                        top_k = min(top_k, len(archive_x))\n                        if top_k <= 0:\n                            x_inj = self._uniform_array(lb, ub)\n                        else:\n                            idx_top = sorted_idx[self.rng.integers(0, top_k)]\n                            center = np.array(archive_x[idx_top], dtype=float)\n                            # local perturbation with larger scale\n                            x_inj = center + self.rng.normal(0.0, 1.0, size=self.dim) * (base_scale_vec * gscale * (1.0 + self.rng.random()))\n                    nonfinite_inj = ~np.isfinite(x_inj)\n                    if nonfinite_inj.any():\n                        fill = self._uniform_array(lb, ub)\n                        x_inj[nonfinite_inj] = fill[nonfinite_inj]\n                    x_inj = self._reflect_bounds(x_inj, lb, ub)\n                    try:\n                        f_inj = float(func(x_inj))\n                    except Exception:\n                        f_inj = np.inf\n                    evals += 1\n                    archive_x.append(np.array(x_inj, dtype=float))\n                    archive_f.append(f_inj)\n                    if f_inj < self.f_opt - 1e-12:\n                        self.x_opt = np.array(x_inj, dtype=float)\n                        self.f_opt = float(f_inj)\n                        stagnation_counter = 0\n                # gently reset gscale\n                gscale = max(1e-6, old_gscale * 0.6)\n                # small cooldown\n                stagnation_counter = 0\n\n            # prune memory periodically\n            if len(archive_x) > 2 * keep_memory:\n                sorted_idx = np.argsort(archive_f)\n                n_keep_best = max(int(0.6 * keep_memory), min(len(sorted_idx), keep_memory // 2))\n                best_idx = list(sorted_idx[:n_keep_best])\n                # keep some of most recent entries\n                recent_cnt = keep_memory - len(best_idx)\n                recent_cnt = max(0, recent_cnt)\n                start = max(0, len(archive_x) - recent_cnt)\n                recent_idx = list(range(start, len(archive_x)))\n                keep_idx = sorted(set(best_idx + recent_idx))\n                archive_x = [archive_x[i] for i in keep_idx]\n                archive_f = [archive_f[i] for i in keep_idx]\n\n        # final return\n        if self.x_opt is None:\n            # degenerate: return random point (call function if we still have budget)\n            x_try = self._uniform_array(lb, ub)\n            f_try = np.inf\n            if evals < self.budget:\n                try:\n                    f_try = float(func(x_try))\n                except Exception:\n                    f_try = np.inf\n                evals += 1\n            self.x_opt = np.array(x_try, dtype=float)\n            self.f_opt = float(f_try)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)\n\n\n# Example usage:\n# sampler = AdaptiveDirectionalSampling(budget=1000, dim=10, seed=42)\n# fbest, xbest = sampler(func)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDirectionalSampling scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14343502461109991, 0.16353909776262288, 0.955888640369503, 0.17602939208915003, 0.9623710978763477, 0.9753068558120805, 0.24110355639297132, 0.24603914832870388, 0.2175666679653373, 0.15494883308978824]}, "task_prompt": ""}, "log_dir": "run-LHNSvns-MA_BBOB-5", "seed": 5}
{"method_name": "LHNS:vns", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "LHNS:vns", "budget": 200, "kwargs": {"method": "vns", "minimisation": false}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "3d503217-3d76-43e1-8ad2-a5dad1b9eaed", "fitness": 0.6426089663745771, "name": "LevyAdaptiveTrustRegion", "description": "Lévy-Driven Adaptive Trust-Region (L-DATR) — combine heavy-tailed Cauchy global jumps with an adaptive local trust-region guided by a cheap linear surrogate gradient and PCA-like principal directions, with 1D parabolic refinement and randomized restarts.", "code": "import numpy as np\n\nclass LevyAdaptiveTrustRegion:\n    \"\"\"\n    Lévy-Driven Adaptive Trust-Region (L-DATR)\n\n    Combines:\n      - heavy-tailed elementwise Cauchy jumps for global exploration,\n      - a cheap linear surrogate (least-squares) to get a gradient-like direction\n        for local exploitation inside an adaptive trust region,\n      - PCA-like principal directions from recent moves,\n      - 1D parabolic refinement along promising directions,\n      - randomized restarts (long jumps) on stagnation,\n      - history trimming and duplicate-evaluation avoidance.\n\n    Designed for noiseless, box-bounded problems (defaults to [-5,5]^d).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 stagnation_limit=80, trust_rel=0.08, init_evals=12,\n                 max_history=200, batch_quota=8, max_iter=1000000):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self._rng = np.random.RandomState(seed)\n        self.init_evals = int(max(3, init_evals))\n        self.max_history = int(max_history)\n        self.batch_quota = int(max(1, batch_quota))\n        self.stagnation_limit = int(stagnation_limit)\n        self.trust_rel = float(trust_rel)\n        self.max_iter = int(max_iter)\n\n        # outputs / state after run\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n        # runtime history containers\n        self.history_x = []\n        self.history_f = []\n\n    def _get_bounds(self, func):\n        \"\"\"\n        Try to infer bounds; fallback to [-5,5]^d.\n        Accepts common patterns:\n          - func.bounds as array-like (d,2) or tuple/list (lb,ub)\n          - func.lb / func.ub or func.lower_bounds / func.upper_bounds\n        Returns (lb, ub) as length-d numpy arrays.\n        \"\"\"\n        d = self.dim\n        lb = np.full(d, -5.0, dtype=float)\n        ub = np.full(d, 5.0, dtype=float)\n\n        # try bounds attribute\n        b = getattr(func, 'bounds', None)\n        try:\n            if b is not None:\n                arr = np.asarray(b, dtype=float)\n                if arr.ndim == 2 and arr.shape == (d, 2):\n                    lb = arr[:, 0].copy()\n                    ub = arr[:, 1].copy()\n                    return lb, ub\n                # tuple/list (lb, ub)\n                if isinstance(b, (list, tuple)) and len(b) == 2:\n                    a = np.asarray(b[0], dtype=float).flatten()\n                    bb = np.asarray(b[1], dtype=float).flatten()\n                    if a.size == d and bb.size == d:\n                        return a.copy(), bb.copy()\n        except Exception:\n            pass\n\n        # try various single attributes\n        for name in ('lb', 'lower_bounds', 'lower_bound', 'lbounds'):\n            if hasattr(func, name):\n                try:\n                    a = np.asarray(getattr(func, name), dtype=float).flatten()\n                    if a.size == d:\n                        lb = a.copy()\n                        break\n                except Exception:\n                    pass\n        for name in ('ub', 'upper_bounds', 'upper_bound', 'ubound'):\n            if hasattr(func, name):\n                try:\n                    b = np.asarray(getattr(func, name), dtype=float).flatten()\n                    if b.size == d:\n                        ub = b.copy()\n                        break\n                except Exception:\n                    pass\n\n        return lb, ub\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize callable func: func(x) takes (dim,) numpy array and returns scalar.\n        Returns (f_opt, x_opt).\n        \"\"\"\n        dim = self.dim\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        if np.any(span <= 0):\n            raise ValueError(\"Invalid bounds detected\")\n\n        # reset state\n        self.history_x = []\n        self.history_f = []\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def clip_to_bounds(x):\n            x = np.asarray(x, dtype=float).flatten()\n            if x.size != dim:\n                raise ValueError(\"x has incorrect dimension\")\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # evaluation wrapper: deduplicate (no budget cost if exact duplicate),\n        # count and stop when budget exhausted\n        def eval_and_store(x):\n            x = clip_to_bounds(x)\n            # duplicate detection: if identical within tight tolerance, return stored f\n            if self.history_x:\n                arr = np.vstack(self.history_x)\n                d2 = np.sum((arr - x) ** 2, axis=1)\n                idx = np.argmin(d2)\n                if d2[idx] <= 1e-14:\n                    return float(self.history_f[idx])\n            if self.evals >= self.budget:\n                raise StopIteration\n            try:\n                f = float(func(x))\n            except Exception:\n                # penalize failed evaluations but count them\n                f = 1e300\n            self.history_x.append(x.copy())\n            self.history_f.append(f)\n            self.evals += 1\n            # trim history\n            if len(self.history_x) > self.max_history:\n                self.history_x = self.history_x[-self.max_history:]\n                self.history_f = self.history_f[-self.max_history:]\n            # update best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        def hist_arrays():\n            if not self.history_x:\n                return np.zeros((0, dim)), np.zeros((0,), dtype=float)\n            return np.vstack(self.history_x), np.asarray(self.history_f, dtype=float)\n\n        # initial seeding: center + random samples\n        try:\n            center_seed = (lb + ub) / 2.0\n            # ensure center evaluated\n            eval_and_store(center_seed)\n            for _ in range(self.init_evals - 1):\n                x0 = lb + self._rng.rand(dim) * (ub - lb)\n                eval_and_store(x0)\n        except StopIteration:\n            return float(self.f_opt), (self.x_opt.copy() if self.x_opt is not None else None)\n\n        # initial trust radius\n        trust_radius = max(1e-8, self.trust_rel * float(np.mean(span)))\n        stagnation = 0\n        iter_count = 0\n\n        # helper: cheap linear gradient estimation via least squares on local neighborhood\n        def estimate_gradient(center_pt, k=None):\n            X, F = hist_arrays()\n            n = X.shape[0]\n            if n < 4:\n                return None\n            diffs = X - center_pt.reshape(1, -1)\n            dists = np.linalg.norm(diffs, axis=1)\n            if k is None:\n                k = min(max(6, dim + 2), n)\n            k = min(k, n)\n            idx = np.argsort(dists)[:k]\n            M_offsets = diffs[idx]  # shape (k,dim)\n            y = F[idx].astype(float)\n            # center y to reduce intercept bias\n            y = y - np.median(y)\n            M = np.hstack((np.ones((k, 1)), M_offsets))\n            try:\n                sol, *_ = np.linalg.lstsq(M, y, rcond=None)\n            except Exception:\n                return None\n            g = sol[1:]\n            # reliability check: gradient magnitude relative to function scale\n            scale = np.median(np.maximum(1e-12, np.abs(F[idx])))\n            relg = np.linalg.norm(g) / (scale + 1e-12)\n            if relg < 1e-10:\n                return None\n            return g\n\n        # PCA-like principal direction from recent improving moves\n        def principal_direction(center_pt, window=60):\n            X, F = hist_arrays()\n            n = X.shape[0]\n            if n < 3:\n                return None\n            idx = np.arange(max(0, n - window), n)\n            Xw = X[idx]\n            Fw = F[idx]\n            best_recent = np.min(Fw)\n            # improvements: smaller F -> larger weight\n            improvements = np.maximum(0.0, Fw - best_recent)\n            weights = 1.0 / (1.0 + improvements)\n            wsum = np.sum(weights) + 1e-12\n            offsets = Xw - center_pt.reshape(1, -1)\n            # weighted covariance-like matrix\n            W = weights / wsum\n            cov = (offsets.T * W).dot(offsets)\n            # eigendecomposition\n            try:\n                w, v = np.linalg.eigh(cov)\n            except Exception:\n                return None\n            principal = v[:, np.argmax(w)]\n            if np.linalg.norm(principal) < 1e-12:\n                return None\n            return principal\n\n        # 1D parabolic fit utility: fit y = a x^2 + b x + c\n        def fit_parabola(xs, ys):\n            xs = np.asarray(xs, dtype=float).flatten()\n            ys = np.asarray(ys, dtype=float).flatten()\n            if xs.size < 3:\n                return None\n            if np.max(xs) - np.min(xs) < 1e-12:\n                return None\n            A = np.vstack((xs ** 2, xs, np.ones_like(xs))).T\n            try:\n                sol, *_ = np.linalg.lstsq(A, ys, rcond=None)\n                a, b, c = sol[0], sol[1], sol[2]\n            except Exception:\n                return None\n            # require reasonable curvature\n            if np.abs(a) < 1e-16:\n                return None\n            return a, b, c\n\n        # main optimization loop\n        try:\n            while self.evals < self.budget and iter_count < self.max_iter:\n                iter_count += 1\n                Xhist, Fhist = hist_arrays()\n                n_hist = Xhist.shape[0]\n                if n_hist == 0:\n                    break\n                # choose center: global best so far\n                center = self.x_opt.copy()\n                best_f = float(self.f_opt)\n\n                improved = False\n                candidates = []\n\n                # estimate gradient and principal direction\n                gn = estimate_gradient(center, k=min(40, max(6, n_hist)))\n                if gn is not None:\n                    gnorm = np.linalg.norm(gn)\n                    if gnorm > 1e-16:\n                        gn = gn / (gnorm + 1e-12)\n                    else:\n                        gn = None\n                pd = principal_direction(center, window=60)\n                if pd is not None:\n                    pd = pd / (np.linalg.norm(pd) + 1e-12)\n\n                # Build candidate list\n\n                # 1) Gradient-guided trust-region steps (if gradient available)\n                if gn is not None:\n                    for frac in (0.3, 0.6, 1.0):\n                        candidates.append(center - gn * (frac * trust_radius))\n                    # small orthogonal explorations\n                    for _ in range(3):\n                        r = self._rng.randn(dim)\n                        # remove projection on gn to encourage orthogonality\n                        r = r - np.dot(r, gn) * gn\n                        nr = np.linalg.norm(r)\n                        if nr > 1e-12:\n                            r = r / nr\n                            candidates.append(center + 0.6 * trust_radius * r)\n\n                # 2) Lévy-like heavy-tailed exploration (element-wise Cauchy)\n                for scale_mult in (0.3, 1.0, 2.5, 5.0):\n                    # elementwise Cauchy via inverse CDF\n                    u = self._rng.rand(dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    # scale by trust_radius but also allow directional scaling by span\n                    scale = scale_mult * trust_radius * (1.0 + 0.3 * (np.mean(span) / (trust_radius + 1e-12)))\n                    step = cauch * scale\n                    candidates.append(center + step)\n                    # mean-shifted variant\n                    candidates.append(center + np.mean(step) * np.ones(dim))\n\n                # 3) Principal-direction probes\n                if pd is not None:\n                    for s in (0.5, 1.0, 1.6):\n                        candidates.append(center - s * trust_radius * pd)\n                        candidates.append(center + s * trust_radius * pd)\n\n                # 4) Small local gaussian perturbations\n                for _ in range(4):\n                    r = self._rng.randn(dim)\n                    candidates.append(center + 0.12 * trust_radius * r)\n\n                # 5) occasional long uniform jump (random restart-ish)\n                if self._rng.rand() < 0.06 or stagnation > (self.stagnation_limit // 2):\n                    # uniform long jump across domain biased by center\n                    longp = lb + self._rng.rand(dim) * (ub - lb)\n                    candidates.append(longp)\n                    # also a long Cauchy jump\n                    u = self._rng.rand(dim)\n                    cauch = np.tan(np.pi * (u - 0.5))\n                    longp2 = center + cauch * (2.0 * np.mean(span))\n                    candidates.append(longp2)\n\n                # clamp to bounds and remove duplicates via coarse rounding\n                seen = set()\n                cand_arr = []\n                for c in candidates:\n                    c_cl = clip_to_bounds(c)\n                    key = tuple(np.round(c_cl, decimals=8))  # coarse uniqueness\n                    if key in seen:\n                        continue\n                    seen.add(key)\n                    cand_arr.append(c_cl)\n\n                # deterministic shuffle with RNG state\n                perm = list(range(len(cand_arr)))\n                if len(perm) > 1:\n                    self._rng.shuffle(perm)\n                cand_arr = [cand_arr[i] for i in perm]\n\n                # Evaluate up to batch_quota candidates or until budget\n                evaled_this_batch = 0\n                local_best_x = center.copy()\n                local_best_f = best_f\n                for c in cand_arr:\n                    if evaled_this_batch >= self.batch_quota:\n                        break\n                    try:\n                        f = eval_and_store(c)\n                    except StopIteration:\n                        raise\n                    evaled_this_batch += 1\n                    if f < local_best_f - 1e-12:\n                        local_best_f = f\n                        local_best_x = c.copy()\n                        improved = True\n\n                # Adjust trust radius based on outcome\n                if improved:\n                    stagnation = 0\n                    # modest expansion scaling with improvement\n                    trust_radius = min(np.max(span), trust_radius * (1.0 + 0.25))\n                else:\n                    stagnation += 1\n                    trust_radius = max(1e-8, trust_radius * 0.88)\n\n                # 1D quadratic probing along negative surrogate gradient if available and budget remains\n                if gn is not None and self.evals < self.budget:\n                    alphas = np.array([-1.2, -0.6, -0.3, 0.0, 0.3, 0.6, 1.0])\n                    xs = []\n                    ys = []\n                    # collect a few samples along the direction\n                    for a in alphas:\n                        if self.evals >= self.budget:\n                            break\n                        pt = clip_to_bounds(center + a * trust_radius * gn)\n                        try:\n                            y = eval_and_store(pt)\n                        except StopIteration:\n                            raise\n                        xs.append(a * trust_radius)\n                        ys.append(y)\n                    xs = np.array(xs)\n                    ys = np.array(ys)\n                    parab = fit_parabola(xs, ys)\n                    if parab is not None:\n                        a_coef, b_coef, c_coef = parab\n                        # For minimization parabola a_coef should be > 0 (convex)\n                        if a_coef > 0:\n                            s_opt = -b_coef / (2.0 * a_coef)\n                            # only accept if within a few sampled radii\n                            if np.abs(s_opt) <= 3.0 * trust_radius:\n                                opt_pt = clip_to_bounds(center + s_opt * gn)\n                                try:\n                                    fopt = eval_and_store(opt_pt)\n                                except StopIteration:\n                                    raise\n                                if fopt < local_best_f - 1e-12:\n                                    local_best_f = fopt\n                                    local_best_x = opt_pt.copy()\n                                    improved = True\n                                    stagnation = 0\n                                    trust_radius = min(np.max(span), trust_radius * 1.25)\n\n                # If stagnated, consider restart strategies\n                if stagnation >= self.stagnation_limit:\n                    X, F = hist_arrays()\n                    n = X.shape[0]\n                    if n >= 3:\n                        k = max(3, min(10, max(3, n // 10)))\n                        top_idx = np.argsort(F)[:k]\n                        topX = X[top_idx]\n                        centroid = np.mean(topX, axis=0)\n                        # a few small samples around centroid\n                        tries = 3\n                        for _ in range(tries):\n                            p = clip_to_bounds(centroid + 0.04 * np.mean(span) * self._rng.randn(dim))\n                            try:\n                                eval_and_store(p)\n                            except StopIteration:\n                                break\n                        # long Cauchy jump from centroid\n                        u = self._rng.rand(dim)\n                        cauch = np.tan(np.pi * (u - 0.5))\n                        scale = 2.0 * np.mean(span)\n                        longp = clip_to_bounds(centroid + cauch * scale)\n                        try:\n                            eval_and_store(longp)\n                        except StopIteration:\n                            pass\n                        # a few random global samples\n                        for _ in range(3):\n                            p = lb + self._rng.rand(dim) * (ub - lb)\n                            try:\n                                eval_and_store(p)\n                            except StopIteration:\n                                break\n                    # reset trust and stagnation\n                    trust_radius = max(1e-8, self.trust_rel * np.mean(span))\n                    stagnation = 0\n\n                # safety small random probes when budget low\n                remaining = self.budget - self.evals\n                if remaining > 0 and remaining < max(5, self.batch_quota):\n                    try:\n                        p = clip_to_bounds(self.x_opt + 0.02 * np.mean(span) * self._rng.randn(dim))\n                        eval_and_store(p)\n                    except StopIteration:\n                        pass\n\n                # termination quick-break if very good found\n                if self.f_opt <= 1e-12:\n                    break\n\n                # bound trust radius to reasonable range\n                trust_radius = float(np.clip(trust_radius, 1e-8, np.max(span) * 0.6))\n\n        except StopIteration:\n            # budget exhausted\n            pass\n\n        # finished\n        self.x_opt = np.asarray(self.x_opt, dtype=float) if self.x_opt is not None else None\n        return float(self.f_opt), (self.x_opt.copy() if self.x_opt is not None else None)", "configspace": "", "generation": 0, "feedback": "The algorithm LevyAdaptiveTrustRegion scored 0.643 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1097709012658733, 0.16605432355411942, 0.9272709301160008, 0.9687899669781057, 0.961400589626004, 0.96501083081961, 0.30314853970272326, 0.9193999412213658, 0.9597889177580405, 0.145454722703927]}, "task_prompt": ""}, "log_dir": "run-LHNSvns-MA_BBOB-1", "seed": 1}
